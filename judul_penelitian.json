[
    {
        "title":"Adapting a World Model for Trajectory Following in a 3D Game",
        "abstract":"Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it. In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay. In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge. Additionally, we investigate several future alignment strategies that address the distribution shift caused by the aleatoric uncertainty and imperfections of the agent. We measure both the trajectory deviation distance and the first significant deviation point between the reference and the agent's trajectory and show that the optimal configuration depends on the chosen setting. Our results show that in a diverse data setting, a GPT-style policy head with an encoder trained from scratch performs the best, DINOv2 encoder with the GPT-style policy head gives the best results in the low data regime, and both GPT-style and MLP-style policy heads had comparable results when pre-trained on a diverse setting and fine-tuned for a specific behaviour setting.\n        △ Less",
        "authors":"['Marko Tot', 'Shu Ishida', 'Abdelhak Lemkhenter', 'David Bignell', 'Pallavi Choudhury', 'Chris Lovett', 'Luis França', 'Matheus Ribeiro Furtado de Mendonça', 'Tarun Gupta', 'Darren Gehring', 'Sam Devlin', 'Sergio Valcarcel Macua', 'Raluca Georgescu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians",
        "abstract":"Accurate, real-time 3D reconstruction of human heads from monocular images and videos underlies numerous visual applications. As 3D ground truth data is hard to come by at scale, previous methods have sought to learn from abundant 2D videos in a self-supervised manner. Typically, this involves the use of differentiable mesh rendering, which is effective but faces limitations. To improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a set of Gaussians that are rigged to this mesh. We then reanimate this rigged head avatar to match a target frame, and backpropagate photometric losses to both the 3DMM and Gaussian prediction networks. We find that using Gaussians for rendering substantially improves the effectiveness of this self-supervised approach. Training solely on 2D data, our method surpasses existing self-supervised approaches in geometric evaluations on the NoW benchmark for neutral faces and a new benchmark for non-neutral expressions. Our method also produces highly expressive meshes, outperforming state-of-the-art in emotion classification.\n        △ Less",
        "authors":"['Liam Schoneveld', 'Zhe Chen', 'Davide Davoli', 'Jiapeng Tang', 'Saimon Terazawa', 'Ko Nishino', 'Matthias Nießner']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BitNet b1.58 2B4T Technical Report",
        "abstract":"We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.\n        △ Less",
        "authors":"['Shuming Ma', 'Hongyu Wang', 'Shaohan Huang', 'Xingxing Zhang', 'Ying Hu', 'Ting Song', 'Yan Xia', 'Furu Wei']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions",
        "abstract":"We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input. Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Predictor: a transformer-decoder module to predict the interaction trajectory from test time inputs by using an indexer module to retrieve a latent affordance from the learned codebook. To train our model, we develop a data engine that extracts 3D hand poses and contact trajectories from the diverse HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger than existing works, in terms of diversity of objects and interactions observed, and test for generalization of the model across object categories, action categories, tasks, and scenes. Experimental results show the effectiveness of our approach over transformer & diffusion baselines across all settings.\n        △ Less",
        "authors":"['Aditya Prakash', 'Benjamin Lundell', 'Dmitry Andreychuk', 'David Forsyth', 'Saurabh Gupta', 'Harpreet Sawhney']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dysarthria Normalization via Local Lie Group Transformations for Robust ASR",
        "abstract":"We present a geometry-driven method for normalizing dysarthric speech using local Lie group transformations of spectrograms. Time, frequency, and amplitude distortions are modeled as smooth, invertible deformations, parameterized by scalar fields and applied via exponential maps. A neural network is trained to infer these fields from synthetic distortions of typical speech-without using any pathological data. At test time, the model applies an approximate inverse to real dysarthric inputs. Despite zero-shot generalization, we observe substantial ASR gains, including up to 16 percentage points WER reduction on challenging TORGO samples, with no degradation on clean speech. This work introduces a principled, interpretable approach for robust speech recognition under motor speech disorders\n        △ Less",
        "authors":"['Mikhail Osipov']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Edge Intelligence for Wildlife Conservation: Real-Time Hornbill Call Classification Using TinyML",
        "abstract":"Hornbills, an iconic species of Malaysia's biodiversity, face threats from habi-tat loss, poaching, and environmental changes, necessitating accurate and real-time population monitoring that is traditionally challenging and re-source intensive. The emergence of Tiny Machine Learning (TinyML) offers a chance to transform wildlife monitoring by enabling efficient, real-time da-ta analysis directly on edge devices. Addressing the challenge of wildlife conservation, this research paper explores the pivotal role of machine learn-ing, specifically TinyML, in the classification and monitoring of hornbill calls in Malaysia. Leveraging audio data from the Xeno-canto database, the study aims to develop a speech recognition system capable of identifying and classifying hornbill vocalizations. The proposed methodology involves pre-processing the audio data, extracting features using Mel-Frequency Energy (MFE), and deploying the model on an Arduino Nano 33 BLE, which is adept at edge computing. The research encompasses foundational work, in-cluding a comprehensive introduction, literature review, and methodology. The model is trained using Edge Impulse and validated through real-world tests, achieving high accuracy in hornbill species identification. The project underscores the potential of TinyML for environmental monitoring and its broader application in ecological conservation efforts, contributing to both the field of TinyML and wildlife conservation.\n        △ Less",
        "authors":"['Kong Ka Hing', 'Mehran Behjati']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative analysis of unsupervised clustering techniques using validation metrics: Study on cognitive features from the Canadian Longitudinal Study on Aging (CLSA)",
        "abstract":"Purpose: The primary goal of this study is to explore the application of evaluation metrics to different clustering algorithms using the data provided from the Canadian Longitudinal Study (CLSA), focusing on cognitive features. The objective of our work is to discover potential clinically relevant clusters that contribute to the development of dementia over time-based on cognitive changes. Method: The CLSA dataset includes 18,891 participants with data available at both baseline and follow-up assessments, to which clustering algorithms were applied. The clustering methodologies employed in this analysis are K-means (KM) clustering, Hierarchical Clustering (HC) and Partitioning Around Medoids (PAM). We use multiple evaluation metrics to assess our analysis. For internal evaluation metrics, we use: Average silhouette Width, Within and Between the sum of square Ratio (WB.Ratio), Entropy, Calinski-Harabasz Index (CH Index), and Separation Index. For clustering comparison metrics, we used: Homogeneity, Completeness, Adjusted Rand Index (ARI), Rand Index (RI), and Variation Information. Results: Using evaluation metrics to compare the results of the three clustering techniques, K-means and Partitioning Around Medoids (PAM) produced similar results. In contrast, there are significant differences between K-means clustering and Hierarchical Clustering. Our study highlights the importance of the two internal evaluation metrics: entropy and separation index. In between clustering comparison metrics, the Adjusted Rand Index is a key tool. Conclusion: The study results have the potential to contribute to understanding dementia. Researchers can also benefit by applying the suggested evaluation metrics to other areas of healthcare research. Overall, our study improves the understanding of using clustering techniques and evaluation metrics to reveal complex patterns in medical data.\n        △ Less",
        "authors":"['ChenNingZhi Sheng', 'Rafal Kustra', 'Davide Chicco']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields",
        "abstract":"Spatiotemporal learning is challenging due to the intricate interplay between spatial and temporal dependencies, the high dimensionality of the data, and scalability constraints. These challenges are further amplified in scientific domains, where data is often irregularly distributed (e.g., missing values from sensor failures) and high-volume (e.g., high-fidelity simulations), posing additional computational and modeling difficulties. In this paper, we present SCENT, a novel framework for scalable and continuity-informed spatiotemporal representation learning. SCENT unifies interpolation, reconstruction, and forecasting within a single architecture. Built on a transformer-based encoder-processor-decoder backbone, SCENT introduces learnable queries to enhance generalization and a query-wise cross-attention mechanism to effectively capture multi-scale dependencies. To ensure scalability in both data size and model complexity, we incorporate a sparse attention mechanism, enabling flexible output representations and efficient evaluation at arbitrary resolutions. We validate SCENT through extensive simulations and real-world experiments, demonstrating state-of-the-art performance across multiple challenging tasks while achieving superior scalability.\n        △ Less",
        "authors":"['David Keetae Park', 'Xihaier Luo', 'Guang Zhao', 'Seungjun Lee', 'Miruna Oprescu', 'Shinjae Yoo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FLIP Reasoning Challenge",
        "abstract":"Over the past years, advances in artificial intelligence (AI) have demonstrated how AI can solve many perception and generation tasks, such as image classification and text writing, yet reasoning remains a challenge. This paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning capabilities based on human verification tasks on the Idena blockchain. FLIP challenges present users with two orderings of 4 images, requiring them to identify the logically coherent one. By emphasizing sequential reasoning, visual storytelling, and common sense, FLIP provides a unique testbed for multimodal AI systems. Our experiments evaluate state-of-the-art models, leveraging both vision-language models (VLMs) and large language models (LLMs). Results reveal that even the best open-sourced and closed-sourced models achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot settings, compared to human performance of 95.3%. Captioning models aid reasoning models by providing text descriptions of images, yielding better results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5 Pro. Combining the predictions from 15 models in an ensemble increases the accuracy to 85.2%. These findings highlight the limitations of existing reasoning models and the need for robust multimodal benchmarks like FLIP. The full codebase and dataset will be available at https:\/\/github.com\/aplesner\/FLIP-Reasoning-Challenge.\n        △ Less",
        "authors":"['Andreas Plesner', 'Turlan Kuzhagaliyev', 'Roger Wattenhofer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Human Aligned Compression for Robust Models",
        "abstract":"Adversarial attacks on image models threaten system robustness by introducing imperceptible perturbations that cause incorrect predictions. We investigate human-aligned learned lossy compression as a defense mechanism, comparing two learned models (HiFiC and ELIC) against traditional JPEG across various quality levels. Our experiments on ImageNet subsets demonstrate that learned compression methods outperform JPEG, particularly for Vision Transformer architectures, by preserving semantically meaningful content while removing adversarial noise. Even in white-box settings where attackers can access the defense, these methods maintain substantial effectiveness. We also show that sequential compression--applying rounds of compression\/decompression--significantly enhances defense efficacy while maintaining classification performance. Our findings reveal that human-aligned compression provides an effective, computationally efficient defense that protects the image features most relevant to human and machine understanding. It offers a practical approach to improving model robustness against adversarial threats.\n        △ Less",
        "authors":"['Samuel Räber', 'Andreas Plesner', 'Till Aczel', 'Roger Wattenhofer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning",
        "abstract":"Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach attains state-of-the-art (SOTA) performance, exceeding all previous efforts in the field of Arabic ASR on the standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.\n        △ Less",
        "authors":"['Mahmoud Salhab', 'Marwan Elghitany', 'Shameed Sait', 'Syed Sibghat Ullah', 'Mohammad Abusheikh', 'Hasan Abusheikh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative Evaluation of Radiomics and Deep Learning Models for Disease Detection in Chest Radiography",
        "abstract":"The application of artificial intelligence (AI) in medical imaging has revolutionized diagnostic practices, enabling advanced analysis and interpretation of radiological data. This study presents a comprehensive evaluation of radiomics-based and deep learning-based approaches for disease detection in chest radiography, focusing on COVID-19, lung opacity, and viral pneumonia. While deep learning models, particularly convolutional neural networks (CNNs) and vision transformers (ViTs), learn directly from image data, radiomics-based models extract and analyze quantitative features, potentially providing advantages in data-limited scenarios. This study systematically compares the diagnostic accuracy and robustness of various AI models, including Decision Trees, Gradient Boosting, Random Forests, Support Vector Machines (SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against state-of-the-art computer vision deep learning architectures. Performance metrics across varying sample sizes reveal insights into each model's efficacy, highlighting the contexts in which specific AI approaches may offer enhanced diagnostic capabilities. The results aim to inform the integration of AI-driven diagnostic tools in clinical practice, particularly in automated and high-throughput environments where timely, reliable diagnosis is critical. This comparative study addresses an essential gap, establishing guidance for the selection of AI models based on clinical and operational needs.\n        △ Less",
        "authors":"['Zhijin He', 'Alan B. McMillan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Human-Centered Early Prediction Models for Academic Performance in Real-World Contexts",
        "abstract":"Supporting student success requires collaboration among multiple stakeholders. Researchers have explored machine learning models for academic performance prediction; yet key challenges remain in ensuring these models are interpretable, equitable, and actionable within real-world educational support systems. First, many models prioritize predictive accuracy but overlook human-centered considerations, limiting trust among students and reducing their usefulness for educators and institutional decision-makers. Second, most models require at least a month of data before making reliable predictions, delaying opportunities for early intervention. Third, current models primarily rely on sporadically collected, classroom-derived data, missing broader behavioral patterns that could provide more continuous and actionable insights. To address these gaps, we present three modeling approaches-LR, 1D-CNN, and MTL-1D-CNN-to classify students as low or high academic performers. We evaluate them based on explainability, fairness, and generalizability to assess their alignment with key social values. Using behavioral and self-reported data collected within the first week of two Spring terms, we demonstrate that these models can identify at-risk students as early as week one. However, trade-offs across human-centered considerations highlight the complexity of designing predictive models that effectively support multi-stakeholder decision-making and intervention strategies. We discuss these trade-offs and their implications for different stakeholders, outlining how predictive models can be integrated into student support systems. Finally, we examine broader socio-technical challenges in deploying these models and propose future directions for advancing human-centered, collaborative academic prediction systems.\n        △ Less",
        "authors":"['Han Zhang', 'Yiyi Ren', 'Paula S. Nurius', 'Jennifer Mankoff', 'Anind K. Dey']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Watermarking Needs Input Repetition Masking",
        "abstract":"Recent advancements in Large Language Models (LLMs) raised concerns over potential misuse, such as for spreading misinformation. In response two counter measures emerged: machine learning-based detectors that predict if text is synthetic, and LLM watermarking, which subtly marks generated text for identification and attribution. Meanwhile, humans are known to adjust language to their conversational partners both syntactically and lexically. By implication, it is possible that humans or unwatermarked LLMs could unintentionally mimic properties of LLM generated text, making counter measures unreliable. In this work we investigate the extent to which such conversational adaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that both humans and LLMs end up mimicking, including the watermarking signal even in seemingly improbable settings. This challenges current academic assumptions and suggests that for long-term watermarking to be reliable, the likelihood of false positives needs to be significantly lower, while longer word sequences should be used for seeding watermarking mechanisms.\n        △ Less",
        "authors":"['David Khachaturov', 'Robert Mullins', 'Ilia Shumailov', 'Sumanth Dathathri']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"zkVC: Fast Zero-Knowledge Proof for Private and Verifiable Computing",
        "abstract":"In the context of cloud computing, services are held on cloud servers, where the clients send their data to the server and obtain the results returned by server. However, the computation, data and results are prone to tampering due to the vulnerabilities on the server side. Thus, verifying the integrity of computation is important in the client-server setting. The cryptographic method known as Zero-Knowledge Proof (ZKP) is renowned for facilitating private and verifiable computing. ZKP allows the client to validate that the results from the server are computed correctly without violating the privacy of the server's intellectual property. Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zkSNARKs), in particular, has been widely applied in various applications like blockchain and verifiable machine learning. Despite their popularity, existing zkSNARKs approaches remain highly computationally intensive. For instance, even basic operations like matrix multiplication require an extensive number of constraints, resulting in significant overhead. In addressing this challenge, we introduce \\textit{zkVC}, which optimizes the ZKP computation for matrix multiplication, enabling rapid proof generation on the server side and efficient verification on the client side. zkVC integrates optimized ZKP modules, such as Constraint-reduced Polynomial Circuit (CRPC) and Prefix-Sum Query (PSQ), collectively yielding a more than 12-fold increase in proof speed over prior methods. The code is available at https:\/\/github.com\/UCF-Lou-Lab-PET\/zkformer\n        △ Less",
        "authors":"['Yancheng Zhang', 'Mengxin Zheng', 'Xun Chen', 'Jingtong Hu', 'Weidong Shi', 'Lei Ju', 'Yan Solihin', 'Qian Lou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning",
        "abstract":"Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.\n        △ Less",
        "authors":"['Siyan Zhao', 'Devaansh Gupta', 'Qinqing Zheng', 'Aditya Grover']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks",
        "abstract":"Decentralized federated learning (DFL) is a promising machine learning paradigm for bringing artificial intelligence (AI) capabilities to the network edge. Running DFL on top of edge networks, however, faces severe performance challenges due to the extensive parameter exchanges between agents. Most existing solutions for these challenges were based on simplistic communication models, which cannot capture the case of learning over a multi-hop bandwidth-limited network. In this work, we address this problem by jointly designing the communication scheme for the overlay network formed by the agents and the mixing matrix that controls the communication demands between the agents. By carefully analyzing the properties of our problem, we cast each design problem into a tractable optimization and develop an efficient algorithm with guaranteed performance. Our evaluations based on real topology and data show that the proposed algorithm can reduce the total training time by over $80\\%$ compared to the baseline without sacrificing accuracy, while significantly improving the computational efficiency over the state of the art.\n        △ Less",
        "authors":"['Tingyang Sun', 'Tuan Nguyen', 'Ting He']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leave-One-Out Stable Conformal Prediction",
        "abstract":"Conformal prediction (CP) is an important tool for distribution-free predictive uncertainty quantification. Yet, a major challenge is to balance computational efficiency and prediction accuracy, particularly for multiple predictions. We propose Leave-One-Out Stable Conformal Prediction (LOO-StabCP), a novel method to speed up full conformal using algorithmic stability without sample splitting. By leveraging leave-one-out stability, our method is much faster in handling a large number of prediction requests compared to existing method RO-StabCP based on replace-one stability. We derived stability bounds for several popular machine learning tools: regularized loss minimization (RLM) and stochastic gradient descent (SGD), as well as kernel method, neural networks and bagging. Our method is theoretically justified and demonstrates superior numerical performance on synthetic and real-world data. We applied our method to a screening problem, where its effective exploitation of training data led to improved test power compared to state-of-the-art method based on split conformal.\n        △ Less",
        "authors":"['Kiljae Lee', 'Yuan Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure",
        "abstract":"It is sometimes assumed that Large Language Models (LLMs) know language, or for example that they know that Paris is the capital of France. But what -- if anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior.\n        △ Less",
        "authors":"['Céline Budding']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CoMotion: Concurrent Multi-person 3D Motion",
        "abstract":"We introduce an approach for detecting and tracking detailed 3D poses of multiple people from a single monocular camera stream. Our system maintains temporally coherent predictions in crowded scenes filled with difficult poses and occlusions. Our model performs both strong per-frame detection and a learned pose update to track people from frame to frame. Rather than match detections across time, poses are updated directly from a new input image, which enables online tracking through occlusion. We train on numerous image and video datasets leveraging pseudo-labeled annotations to produce a model that matches state-of-the-art systems in 3D pose estimation accuracy while being faster and more accurate in tracking multiple people through time. Code and weights are provided at https:\/\/github.com\/apple\/ml-comotion\n        △ Less",
        "authors":"['Alejandro Newell', 'Peiyun Hu', 'Lahav Lipson', 'Stephan R. Richter', 'Vladlen Koltun']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning",
        "abstract":"Federated Learning (FL) has emerged as a promising framework for distributed learning, but its growing complexity has led to significant energy consumption, particularly from computations on the client side. This challenge is especially critical in energy-harvesting FL (EHFL) systems, where device availability fluctuates due to limited and time-varying energy resources. We propose FedBacys, a battery-aware FL framework that introduces cyclic client participation based on users' battery levels to cope with these issues. FedBacys enables clients to save energy and strategically perform local training just before their designated transmission time by clustering clients and scheduling their involvement sequentially. This design minimizes redundant computation, reduces system-wide energy usage, and improves learning stability. Our experiments demonstrate that FedBacys outperforms existing approaches in terms of energy efficiency and performance consistency, exhibiting robustness even under non-i.i.d. training data distributions and with very infrequent battery charging. This work presents the first comprehensive evaluation of cyclic client participation in EHFL, incorporating both communication and computation costs into a unified, resource-aware scheduling strategy.\n        △ Less",
        "authors":"['Eunjeong Jeong', 'Nikolaos Pappas']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Approximation Bounds for Transformer Networks with Application to Regression",
        "abstract":"We explore the approximation capabilities of Transformer networks for Hölder and Sobolev functions, and apply these results to address nonparametric regression estimation with dependent observations. First, we establish novel upper bounds for standard Transformer networks approximating sequence-to-sequence mappings whose component functions are Hölder continuous with smoothness index $γ\\in (0,1]$. To achieve an approximation error $\\varepsilon$ under the $L^p$-norm for $p \\in [1, \\infty]$, it suffices to use a fixed-depth Transformer network whose total number of parameters scales as $\\varepsilon^{-d_x n \/ γ}$. This result not only extends existing findings to include the case $p = \\infty$, but also matches the best known upper bounds on number of parameters previously obtained for fixed-depth FNNs and RNNs. Similar bounds are also derived for Sobolev functions. Second, we derive explicit convergence rates for the nonparametric regression problem under various $β$-mixing data assumptions, which allow the dependence between observations to weaken over time. Our bounds on the sample complexity impose no constraints on weight magnitudes. Lastly, we propose a novel proof strategy to establish approximation bounds, inspired by the Kolmogorov-Arnold representation theorem. We show that if the self-attention layer in a Transformer can perform column averaging, the network can approximate sequence-to-sequence Hölder functions, offering new insights into the interpretability of self-attention mechanisms.\n        △ Less",
        "authors":"['Yuling Jiao', 'Yanming Lai', 'Defeng Sun', 'Yang Wang', 'Bokai Yan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning",
        "abstract":"Semantic 3D city models are worldwide easy-accessible, providing accurate, object-oriented, and semantic-rich 3D priors. To date, their potential to mitigate the noise impact on radar object detection remains under-explored. In this paper, we first introduce a unique dataset, RadarCity, comprising 54K synchronized radar-image pairs and semantic 3D city models. Moreover, we propose a novel neural network, RADLER, leveraging the effectiveness of contrastive self-supervised learning (SSL) and semantic 3D city models to enhance radar object detection of pedestrians, cyclists, and cars. Specifically, we first obtain the robust radar features via a SSL network in the radar-image pretext task. We then use a simple yet effective feature fusion strategy to incorporate semantic-depth features from semantic 3D city models. Having prior 3D information as guidance, RADLER obtains more fine-grained details to enhance radar object detection. We extensively evaluate RADLER on the collected RadarCity dataset and demonstrate average improvements of 5.46% in mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over previous radar object detection methods. We believe this work will foster further research on semantic-guided and map-supported radar object detection. Our project page is publicly available athttps:\/\/gpp-communication.github.io\/RADLER .\n        △ Less",
        "authors":"['Yuan Luo', 'Rudolf Hoffmann', 'Yan Xia', 'Olaf Wysocki', 'Benedikt Schwab', 'Thomas H. Kolbe', 'Daniel Cremers']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Material Network: Overview, applications and current directions",
        "abstract":"Deep Material Network (DMN) has emerged as a powerful framework for multiscale material modeling, enabling efficient and accurate predictions of material behavior across different length scales. Unlike traditional machine learning approaches, the trainable parameters in DMN have direct physical interpretations, capturing the geometric characteristics of the microstructure rather than serving as purely statistical fitting parameters. Its hierarchical tree structure effectively encodes microstructural interactions and deformation mechanisms, allowing DMN to achieve a balance between accuracy and computational efficiency. This physics-informed architecture significantly reduces computational costs compared to direct numerical simulations while preserving essential microstructural physics. Furthermore, DMN can be trained solely on a linear elastic dataset while effectively extrapolating nonlinear responses during online prediction, making it a highly efficient and scalable approach for multiscale material modeling. This article provides a comprehensive review of DMN, detailing its motivation, underlying methodology, and recent advancements. We discuss key modeling aspects, including its hierarchical structure, training process, and the role of physics-based constraints in enhancing predictive accuracy. Furthermore, we highlight its applications in component-scale multiscale analysis and inverse parameter identification, demonstrating its capability to bridge microscale material behavior with macroscale engineering predictions. Finally, we discuss challenges and future directions in improving DMN's generalization capabilities and its potential extensions for broader applications in multiscale modeling.\n        △ Less",
        "authors":"['Ting-Ju Wei', 'Wen-Ning Wan', 'Chuin-Shan Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications",
        "abstract":"In many applications, especially those involving prediction, models may yield near-optimal performance yet significantly disagree on individual-level outcomes. This phenomenon, known as predictive multiplicity, has been formally defined in binary, probabilistic, and multi-target classification, and undermines the reliability of predictive systems. However, its implications remain unexplored in the context of survival analysis, which involves estimating the time until a failure or similar event while properly handling censored data. We frame predictive multiplicity as a critical concern in survival-based models and introduce formal measures -- ambiguity, discrepancy, and obscurity -- to quantify it. This is particularly relevant for downstream tasks such as maintenance scheduling, where precise individual risk estimates are essential. Understanding and reporting predictive multiplicity helps build trust in models deployed in high-stakes environments. We apply our methodology to benchmark datasets from predictive maintenance, extending the notion of multiplicity to survival models. Our findings show that ambiguity steadily increases, reaching up to 40-45% of observations; discrepancy is lower but exhibits a similar trend; and obscurity remains mild and concentrated in a few models. These results demonstrate that multiple accurate survival models may yield conflicting estimations of failure risk and degradation progression for the same equipment. This highlights the need to explicitly measure and communicate predictive multiplicity to ensure reliable decision-making in process health management.\n        △ Less",
        "authors":"['Mustafa Cavus']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis",
        "abstract":"Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack of interpretability in the decision logic of multimodal fusion and modality imbalance caused by disparities in inter-modal information density. To address these issues, we propose KAN-MCP, a novel framework that integrates the interpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the Multimodal Clean Pareto (MCPareto) framework. First, KAN leverages its univariate function decomposition to achieve transparent analysis of cross-modal interactions. This structural design allows direct inspection of feature transformations without relying on external interpretation tools, thereby ensuring both high expressiveness and interpretability. Second, the proposed MCPareto enhances robustness by addressing modality imbalance and noise interference. Specifically, we introduce the Dimensionality Reduction and Denoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises and reduces feature dimensionality. This approach provides KAN with discriminative low-dimensional inputs to reduce the modeling complexity of KAN while preserving critical sentiment-related information. Furthermore, MCPareto dynamically balances gradient contributions across modalities using the purified features output by DRD-MIB, ensuring lossless transmission of auxiliary signals and effectively alleviating modality imbalance. This synergy of interpretability and robustness not only achieves superior performance on benchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers an intuitive visualization interface through KAN's interpretable architecture.\n        △ Less",
        "authors":"['Miaosen Luo', 'Yuncheng Jiang', 'Sijie Mai']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -",
        "abstract":"Despite recent advances in Large Vision Language Models (LVLMs), these models still suffer from generating hallucinatory responses that do not align with the visual input provided. To mitigate such hallucinations, we introduce Efficient Contrastive Decoding (ECD), a simple method that leverages probabilistic hallucination detection to shift the output distribution towards contextually accurate answers at inference time. By contrasting token probabilities and hallucination scores, ECD subtracts hallucinated concepts from the original distribution, effectively suppressing hallucinations. Notably, our proposed method can be applied to any open-source LVLM and does not require additional LVLM training. We evaluate our method on several benchmark datasets and across different LVLMs. Our experiments show that ECD effectively mitigates hallucinations, outperforming state-of-the-art methods with respect to performance on LVLM benchmarks and computation time.\n        △ Less",
        "authors":"['Laura Fieback', 'Nishilkumar Balar', 'Jakob Spiegelberg', 'Hanno Gottschalk']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals",
        "abstract":"Detection of spatial areas where biodiversity is at risk is of paramount importance for the conservation and monitoring of ecosystems. Large terrestrial mammalian herbivores are keystone species as their activity not only has deep effects on soils, plants, and animals, but also shapes landscapes, as large herbivores act as allogenic ecosystem engineers. One key landscape feature that indicates intense herbivore activity and potentially impacts biodiversity is the formation of grazing trails. Grazing trails are formed by the continuous trampling activity of large herbivores that can produce complex networks of tracks of bare soil. Here, we evaluated different algorithms based on machine learning techniques to identify grazing trails. Our goal is to automatically detect potential areas with intense herbivory activity, which might be beneficial for conservation and management plans.\n  We have applied five semantic segmentation methods combined with fourteen encoders aimed at mapping grazing trails on aerial images. Our results indicate that in most cases the chosen methodology successfully mapped the trails, although there were a few instances where the actual trail structure was underestimated. The UNet architecture with the MambaOut encoder was the best architecture for mapping trails. The proposed approach could be applied to develop tools for mapping and monitoring temporal changes in these landscape structures to support habitat conservation and land management programs. This is the first time, to the best of our knowledge, that competitive image segmentation results are obtained for the detection and delineation of trails of large herbivorous mammals.\n        △ Less",
        "authors":"['Jose Francisco Diez-Pastor', 'Francisco Javier Gonzalez-Moya', 'Pedro Latorre-Carmona', 'Francisco Javier Perez-Barbería', 'Ludmila I. Kuncheva', 'Antonio Canepa-Oneto', 'Alvar Arnaiz-González', 'Cesar Garcia-Osorio']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AttentionDrop: A Novel Regularization Method for Transformer Models",
        "abstract":"Transformer-based architectures achieve state-of-the-art performance across a wide range of tasks in natural language processing, computer vision, and speech. However, their immense capacity often leads to overfitting, especially when training data is limited or noisy. We propose AttentionDrop, a unified family of stochastic regularization techniques that operate directly on the self-attention distributions. We introduces three variants: 1. Hard Attention Masking: randomly zeroes out top-k attention logits per query to encourage diverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic Gaussian convolution over attention logits to diffuse overly peaked distributions. 3. Consistency-Regularized AttentionDrop: enforces output stability under multiple independent AttentionDrop perturbations via a KL-based consistency loss.\n        △ Less",
        "authors":"['Mirza Samad Ahmed Baig', 'Syeda Anshrah Gillani', 'Abdul Akbar Khan', 'Shahid Munir Shah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Contextual Bandits Under Delayed Feedback Constraints",
        "abstract":"This paper presents a new algorithm for neural contextual bandits (CBs) that addresses the challenge of delayed reward feedback, where the reward for a chosen action is revealed after a random, unknown delay. This scenario is common in applications such as online recommendation systems and clinical trials, where reward feedback is delayed because the outcomes or results of a user's actions (such as recommendations or treatment responses) take time to manifest and be measured. The proposed algorithm, called Delayed NeuralUCB, uses an upper confidence bound (UCB)-based exploration strategy. Under the assumption of independent and identically distributed sub-exponential reward delays, we derive an upper bound on the cumulative regret over a T-length horizon. We further consider a variant of the algorithm, called Delayed NeuralTS, that uses Thompson Sampling-based exploration. Numerical experiments on real-world datasets, such as MNIST and Mushroom, along with comparisons to benchmark approaches, demonstrate that the proposed algorithms effectively manage varying delays and are well-suited for complex real-world scenarios.\n        △ Less",
        "authors":"['Mohammadali Moghimi', 'Sharu Theresa Jose', 'Shana Moothedath']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generative Deep Learning Framework for Inverse Design of Fuels",
        "abstract":"In the present work, a generative deep learning framework combining a Co-optimized Variational Autoencoder (Co-VAE) architecture with quantitative structure-property relationship (QSPR) techniques is developed to enable accelerated inverse design of fuels. The Co-VAE integrates a property prediction component coupled with the VAE latent space, enhancing molecular reconstruction and accurate estimation of Research Octane Number (RON) (chosen as the fuel property of interest). A subset of the GDB-13 database, enriched with a curated RON database, is used for model training. Hyperparameter tuning is further utilized to optimize the balance among reconstruction fidelity, chemical validity, and RON prediction. An independent regression model is then used to refine RON prediction, while a differential evolution algorithm is employed to efficiently navigate the VAE latent space and identify promising fuel molecule candidates with high RON. This methodology addresses the limitations of traditional fuel screening approaches by capturing complex structure-property relationships within a comprehensive latent representation. The generative model provides a flexible tool for systematically exploring vast chemical spaces, paving the way for discovering fuels with superior anti-knock properties. The demonstrated approach can be readily extended to incorporate additional fuel properties and synthesizability criteria to enhance applicability and reliability for de novo design of new fuels.\n        △ Less",
        "authors":"['Kiran K. Yalamanchi', 'Pinaki Pal', 'Balaji Mohan', 'Abdullah S. AlRamadan', 'Jihad A. Badra', 'Yuanjiang Pei']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimizing Compound Retrieval Systems",
        "abstract":"Modern retrieval systems do not rely on a single ranking model to construct their rankings. Instead, they generally take a cascading approach where a sequence of ranking models are applied in multiple re-ranking stages. Thereby, they balance the quality of the top-K ranking with computational costs by limiting the number of documents each model re-ranks. However, the cascading approach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of retrieval systems that apply multiple prediction models. This encapsulates cascading models but also allows other types of interactions than top-K re-ranking. In particular, we enable interactions with large language models (LLMs) which can provide relative relevance comparisons. We focus on the optimization of compound retrieval system design which uniquely involves learning where to apply the component models and how to aggregate their predictions into a final ranking. This work shows how our compound approach can combine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM relevance predictions, while optimizing a given ranking metric and efficiency target. Our experimental results show optimized compound retrieval systems provide better trade-offs between effectiveness and efficiency than cascading approaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the information retrieval field to more out-of-the-box thinking on how prediction models can interact to form rankings.\n        △ Less",
        "authors":"['Harrie Oosterhuis', 'Rolf Jagerman', 'Zhen Qin', 'Xuanhui Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Static to Dynamic Correlation Clustering",
        "abstract":"Correlation clustering is a well-studied problem, first proposed by Bansal, Blum, and Chawla [BBC04]. The input is an unweighted, undirected graph. The problem is to cluster the vertices so as to minimizing the number of edges between vertices in different clusters and missing edges between vertices inside the same cluster. This problem has a wide application in data mining and machine learning. We introduce a general framework that transforms existing static correlation clustering algorithms into fully-dynamic ones that work against an adaptive adversary.\n  We show how to apply our framework to known efficient correlation clustering algorithms, starting from the classic 3-approximate Pivot algorithm from [ACN08]. Applied to the most recent near-linear 1.437-approximation algorithm from [CCL+25], we get a 1.437-approximation fully-dynamic algorithm that works with worst-case constant update time. The original static algorithm gets its approximation factor with constant probability, and we get the same against an adaptive adversary in the sense that for any given update step not known to our algorithm, our solution is a 1.437-approximation with constant probability when we reach this update.\n  Previous dynamic algorithms had approximation factors around 3 in expectation, and they could only handle an oblivious adversary.\n        △ Less",
        "authors":"['Nairen Cao', 'Vincent Cohen-Addad', 'Euiwoong Lee', 'Shi Li', 'David Rasmussen Lolck', 'Alantha Newman', 'Mikkel Thorup', 'Lukas Vogl', 'Shuyi Yan', 'Hanwen Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the calibration of Just-in-time Defect Prediction",
        "abstract":"Just in time defect prediction (JIT DP) leverages ML to identify defect-prone code commits, enabling quality assurance (QA) teams to allocate resources more efficiently by focusing on commits that are most likely to contain defects. Although JIT DP techniques have introduced improvements in terms of predictive accuracy, they are still susceptible to misclassification errors such as false positives and negatives. This can lead to wasted resources or undetected defects, a particularly critical concern when QA resources are limited. To mitigate these challenges and preserve the practical utility of JIT DP tools, it becomes essential to estimate the reliability of the predictions, i.e., computing confidence scores. Such scores can help practitioners determine the trustworthiness of predictions and thus prioritize them efficiently. A simple approach to computing confidence scores is to extract, alongside each prediction, the corresponding prediction probabilities and use them as indicators of confidence. However, for these probabilities to reliably serve as confidence scores, the predictive model must be well-calibrated. This means that the prediction probabilities must accurately represent the true likelihood of each prediction being correct. Miscalibration, common in modern ML models, distorts probability scores such that they do not align with the actual correctness probability. In this study, we evaluate the calibration of three JIT DP techniques to determine whether and to what extent they exhibit poor calibration. Furthermore, we assess whether post-calibration methods can improve the calibration of existing JIT defect prediction models. Our results reveal that all evaluated JIT DP models exhibit some level of miscalibration, with ECE ranging from 2-35%. Furthermore, post-calibration methods do not consistently improve the calibration.\n        △ Less",
        "authors":"['Xhulja Shahini', 'Jone Bartel', 'Klaus Pohl']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild",
        "abstract":"Computer vision models have seen increased usage in sports, and reinforcement learning (RL) is famous for beating humans in strategic games such as Chess and Go. In this paper, we are interested in building upon these advances and examining the game of classic 8-ball pool. We introduce pix2pockets, a foundation for an RL-assisted pool coach. Given a single image of a pool table, we first aim to detect the table and the balls and then propose the optimal shot suggestion. For the first task, we build a dataset with 195 diverse images where we manually annotate all balls and table dots, leading to 5748 object segmentation masks. For the second task, we build a standardized RL environment that allows easy development and benchmarking of any RL algorithm. Our object detection model yields an AP50 of 91.2 while our ball location pipeline obtains an error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set a baseline for the shot suggestion task and we show that all of them fail to pocket all balls without making a foul move. We also present a simple baseline that achieves a per-shot success rate of 94.7% and clears a full game in a single turn 30% of the time.\n        △ Less",
        "authors":"['Jonas Myhre Schiøtt', 'Viktor Sebastian Petersen', 'Dimitrios P. Papadopoulos']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model",
        "abstract":"Radar-based HAR has emerged as a promising alternative to conventional monitoring approaches, such as wearable devices and camera-based systems, due to its unique privacy preservation and robustness advantages. However, existing solutions based on convolutional and recurrent neural networks, although effective, are computationally demanding during deployment. This limits their applicability in scenarios with constrained resources or those requiring multiple sensors. Advanced architectures, such as ViT and SSM architectures, offer improved modeling capabilities and have made efforts toward lightweight designs. However, their computational complexity remains relatively high. To leverage the strengths of transformer architectures while simultaneously enhancing accuracy and reducing computational complexity, this paper introduces RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM specifically tailored for radar-based HAR. Across three diverse datasets, RadMamba matches the top-performing previous model's 99.8% classification accuracy on Dataset DIAT with only 1\/400 of its parameters and equals the leading models' 92.0% accuracy on Dataset CI4R with merely 1\/10 of their parameters. In scenarios with continuous sequences of actions evaluated on Dataset UoG2020, RadMamba surpasses other models with significantly higher parameter counts by at least 3%, achieving this with only 6.7k parameters. Our code is available at: https:\/\/github.com\/lab-emi\/AIRHAR.\n        △ Less",
        "authors":"['Yizhuo Wu', 'Francesco Fioranelli', 'Chang Gao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FedEPA: Enhancing Personalization and Modality Alignment in Multimodal Federated Learning",
        "abstract":"Federated Learning (FL) enables decentralized model training across multiple parties while preserving privacy. However, most FL systems assume clients hold only unimodal data, limiting their real-world applicability, as institutions often possess multimodal data. Moreover, the lack of labeled data further constrains the performance of most FL methods. In this work, we propose FedEPA, a novel FL framework for multimodal learning. FedEPA employs a personalized local model aggregation strategy that leverages labeled data on clients to learn personalized aggregation weights, thereby alleviating the impact of data heterogeneity. We also propose an unsupervised modality alignment strategy that works effectively with limited labeled data. Specifically, we decompose multimodal features into aligned features and context features. We then employ contrastive learning to align the aligned features across modalities, ensure the independence between aligned features and context features within each modality, and promote the diversity of context features. A multimodal feature fusion strategy is introduced to obtain a joint embedding. The experimental results show that FedEPA significantly outperforms existing FL methods in multimodal classification tasks under limited labeled data conditions.\n        △ Less",
        "authors":"['Yu Zhang', 'Qingfeng Du', 'Jiaqi Lv']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Active Human Feedback Collection via Neural Contextual Dueling Bandits",
        "abstract":"Collecting human preference feedback is often expensive, leading recent works to develop principled algorithms to select them more efficiently. However, these works assume that the underlying reward function is linear, an assumption that does not hold in many real-life applications, such as online recommendation and LLM alignment. To address this limitation, we propose Neural-ADB, an algorithm based on the neural contextual dueling bandit framework that provides a principled and practical method for collecting human preference feedback when the underlying latent reward function is non-linear. We theoretically show that when preference feedback follows the Bradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by Neural-ADB decreases at a sub-linear rate as the preference dataset increases. Our experimental results on problem instances derived from synthetic preference datasets further validate the effectiveness of Neural-ADB.\n        △ Less",
        "authors":"['Arun Verma', 'Xiaoqiang Lin', 'Zhongxiang Dai', 'Daniela Rus', 'Bryan Kian Hsiang Low']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Balancing Graph Embedding Smoothness in Self-Supervised Learning via Information-Theoretic Decomposition",
        "abstract":"Self-supervised learning (SSL) in graphs has garnered significant attention, particularly in employing Graph Neural Networks (GNNs) with pretext tasks initially designed for other domains, such as contrastive learning and feature reconstruction. However, it remains uncertain whether these methods effectively reflect essential graph properties, precisely representation similarity with its neighbors. We observe that existing methods position opposite ends of a spectrum driven by the graph embedding smoothness, with each end corresponding to outperformance on specific downstream tasks. Decomposing the SSL objective into three terms via an information-theoretic framework with a neighbor representation variable reveals that this polarization stems from an imbalance among the terms, which existing methods may not effectively maintain. Further insights suggest that balancing between the extremes can lead to improved performance across a wider range of downstream tasks. A framework, BSG (Balancing Smoothness in Graph SSL), introduces novel loss functions designed to supplement the representation quality in graph-based SSL by balancing the derived three terms: neighbor loss, minimal loss, and divergence loss. We present a theoretical analysis of the effects of these loss functions, highlighting their significance from both the SSL and graph smoothness perspectives. Extensive experiments on multiple real-world datasets across node classification and link prediction consistently demonstrate that BSG achieves state-of-the-art performance, outperforming existing methods. Our implementation code is available at https:\/\/github.com\/steve30572\/BSG.\n        △ Less",
        "authors":"['Heesoo Jung', 'Hogun Park']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder",
        "abstract":"Voice conversion is a task of synthesizing an utterance with target speaker's voice while maintaining linguistic information of the source utterance. While a speaker can produce varying utterances from a single script with different intonations, conventional voice conversion models were limited to producing only one result per source input. To overcome this limitation, we propose a novel approach for voice conversion with diverse intonations using conditional variational autoencoder (CVAE). Experiments have shown that the speaker's style feature can be mapped into a latent space with Gaussian distribution. We have also been able to convert voices with more diverse intonation by making the posterior of the latent space more complex with inverse autoregressive flow (IAF). As a result, the converted voice not only has a diversity of intonations, but also has better sound quality than the model without CVAE.\n        △ Less",
        "authors":"['Soobin Suh', 'Dabi Ahn', 'Heewoong Park', 'Jonghun Park']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Control of Rayleigh-Bénard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime",
        "abstract":"Data-driven flow control has significant potential for industry, energy systems, and climate science. In this work, we study the effectiveness of Reinforcement Learning (RL) for reducing convective heat transfer in the 2D Rayleigh-Bénard Convection (RBC) system under increasing turbulence. We investigate the generalizability of control across varying initial conditions and turbulence levels and introduce a reward shaping technique to accelerate the training. RL agents trained via single-agent Proximal Policy Optimization (PPO) are compared to linear proportional derivative (PD) controllers from classical control theory. The RL agents reduced convection, measured by the Nusselt Number, by up to 33% in moderately turbulent systems and 10% in highly turbulent settings, clearly outperforming PD control in all settings. The agents showed strong generalization performance across different initial conditions and to a significant extent, generalized to higher degrees of turbulence. The reward shaping improved sample efficiency and consistently stabilized the Nusselt Number to higher turbulence levels.\n        △ Less",
        "authors":"['Thorben Markmann', 'Michiel Straat', 'Sebastian Peitz', 'Barbara Hammer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs",
        "abstract":"We study reinforcement learning in infinite-horizon average-reward settings with linear MDPs. Previous work addresses this problem by approximating the average-reward setting by discounted setting and employing a value iteration-based algorithm that uses clipping to constrain the span of the value function for improved statistical efficiency. However, the clipping procedure requires computing the minimum of the value function over the entire state space, which is prohibitive since the state space in linear MDP setting can be large or even infinite. In this paper, we introduce a value iteration method with efficient clipping operation that only requires computing the minimum of value functions over the set of states visited by the algorithm. Our algorithm enjoys the same regret bound as the previous work while being computationally efficient, with computational complexity that is independent of the size of the state space.\n        △ Less",
        "authors":"['Kihyuk Hong', 'Ambuj Tewari']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation",
        "abstract":"A domain (distribution) shift between training and test data often hinders the real-world performance of deep neural networks, necessitating unsupervised domain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged as a solution for practical scenarios where access to source data is restricted and target data is received as a continuous stream. However, the open-world nature of many real-world applications additionally introduces category shifts meaning that the source and target label spaces may differ. Online source-free universal domain adaptation (SF-UniDA) addresses this challenge. Existing methods mainly rely on self-training with pseudo-labels, yet the relationship between pseudo-labeling and adaptation outcomes has not been studied yet. To bridge this gap, we conduct a systematic analysis through controlled experiments with simulated pseudo-labeling, offering valuable insights into pseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap between the current state-of-the-art and the upper bound of adaptation achieved with perfect pseudo-labeling. Moreover, we show that a contrastive loss enables effective adaptation even with moderate pseudo-label accuracy, while a cross-entropy loss, though less robust to pseudo-label errors, achieves superior results when pseudo-labeling approaches perfection. Lastly, our findings indicate that pseudo-label accuracy is in general more crucial than quantity, suggesting that prioritizing fewer but high-confidence pseudo-labels is beneficial. Overall, our study highlights the critical role of pseudo-labeling in (online) SF-UniDA and provides actionable insights to drive future advancements in the field. Our code is available at https:\/\/github.com\/pascalschlachter\/PLAnalysis.\n        △ Less",
        "authors":"['Pascal Schlachter', 'Jonathan Fuss', 'Bin Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets",
        "abstract":"Transfer learning from pre-trained encoders has become essential in modern machine learning, enabling efficient model adaptation across diverse tasks. However, this combination of pre-training and downstream adaptation creates an expanded attack surface, exposing models to sophisticated backdoor embeddings at both the encoder and dataset levels--an area often overlooked in prior research. Additionally, the limited computational resources typically available to users of pre-trained encoders constrain the effectiveness of generic backdoor defenses compared to end-to-end training from scratch. In this work, we investigate how to mitigate potential backdoor risks in resource-constrained transfer learning scenarios. Specifically, we conduct an exhaustive analysis of existing defense strategies, revealing that many follow a reactive workflow based on assumptions that do not scale to unknown threats, novel attack types, or different training paradigms. In response, we introduce a proactive mindset focused on identifying clean elements and propose the Trusted Core (T-Core) Bootstrapping framework, which emphasizes the importance of pinpointing trustworthy data and neurons to enhance model security. Our empirical evaluations demonstrate the effectiveness and superiority of T-Core, specifically assessing 5 encoder poisoning attacks, 7 dataset poisoning attacks, and 14 baseline defenses across five benchmark datasets, addressing four scenarios of 3 potential backdoor threats.\n        △ Less",
        "authors":"['Yechao Zhang', 'Yuxuan Zhou', 'Tianyu Li', 'Minghui Li', 'Shengshan Hu', 'Wei Luo', 'Leo Yu Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Evolution of Zero Trust Architecture (ZTA) from Concept to Implementation",
        "abstract":"Zero Trust Architecture (ZTA) is one of the paradigm changes in cybersecurity, from the traditional perimeter-based model to perimeterless. This article studies the core concepts of ZTA, its beginning, a few use cases and future trends. Emphasising the always verify and least privilege access, some key tenets of ZTA have grown to be integration technologies like Identity Management, Multi-Factor Authentication (MFA) and real-time analytics. ZTA is expected to strengthen cloud environments, education, work environments (including from home) while controlling other risks like lateral movement and insider threats. Despite ZTA's benefits, it comes with challenges in the form of complexity, performance overhead and vulnerabilities in the control plane. These require phased implementation and continuous refinement to keep up with evolving organisational needs and threat landscapes. Emerging technologies, such as Artificial Intelligence (AI) and Machine Learning (ML) will further automate policy enforcement and threat detection in keeping up with dynamic cyber threats.\n        △ Less",
        "authors":"['Md Nasiruzzaman', 'Maaruf Ali', 'Iftekhar Salam', 'Mahdi H. Miraz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient identification of linear, parameter-varying, and nonlinear systems with noise models",
        "abstract":"We present a general system identification procedure capable of estimating of a broad spectrum of state-space dynamical models, including linear time-invariant (LTI), linear parameter-varying} (LPV), and nonlinear (NL) dynamics, along with rather general classes of noise models. Similar to the LTI case, we show that for this general class of model structures, including the NL case, the model dynamics can be separated into a deterministic process and a stochastic noise part, allowing to seamlessly tune the complexity of the combined model both in terms of nonlinearity and noise modeling. We parameterize the involved nonlinear functional relations by means of artificial neural-networks (ANNs), although alternative parametric nonlinear mappings can also be used. To estimate the resulting model structures, we optimize a prediction-error-based criterion using an efficient combination of a constrained quasi-Newton approach and automatic differentiation, achieving training times in the order of seconds compared to existing state-of-the-art ANN methods which may require hours for models of similar complexity. We formally establish the consistency guarantees for the proposed approach and demonstrate its superior estimation accuracy and computational efficiency on several benchmark LTI, LPV, and NL system identification problems.\n        △ Less",
        "authors":"['Alberto Bemporad', 'Roland Tóth']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hardware-Friendly Delayed-Feedback Reservoir for Multivariate Time-Series Classification",
        "abstract":"Reservoir computing (RC) is attracting attention as a machine-learning technique for edge computing. In time-series classification tasks, the number of features obtained using a reservoir depends on the length of the input series. Therefore, the features must be converted to a constant-length intermediate representation (IR), such that they can be processed by an output layer. Existing conversion methods involve computationally expensive matrix inversion that significantly increases the circuit size and requires processing power when implemented in hardware. In this article, we propose a simple but effective IR, namely, dot-product-based reservoir representation (DPRR), for RC based on the dot product of data features. Additionally, we propose a hardware-friendly delayed-feedback reservoir (DFR) consisting of a nonlinear element and delayed feedback loop with DPRR. The proposed DFR successfully classified multivariate time series data that has been considered particularly difficult to implement efficiently in hardware. In contrast to conventional DFR models that require analog circuits, the proposed model can be implemented in a fully digital manner suitable for high-level syntheses. A comparison with existing machine-learning methods via field-programmable gate array implementation using 12 multivariate time-series classification tasks confirmed the superior accuracy and small circuit size of the proposed method.\n        △ Less",
        "authors":"['Sosei Ikeda', 'Hiromitsu Awano', 'Takashi Sato']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews",
        "abstract":"Many existing digital triage systems are questionnaire-based, guiding patients to appropriate care levels based on information (e.g., symptoms, medical history, and urgency) provided by the patients answering questionnaires. Such a system often uses a deterministic model with predefined rules to determine care levels. It faces challenges with incomplete triage interviews since it can only assist patients who finish the process. In this study, we explore the use of machine learning (ML) to predict outcomes of unfinished interviews, aiming to enhance patient care and service quality. Predicting triage outcomes from incomplete data is crucial for patient safety and healthcare efficiency. Our findings show that decision-tree models, particularly LGBMClassifier and CatBoostClassifier, achieve over 80\\% accuracy in predicting outcomes from complete interviews while having a linear correlation between the prediction accuracy and interview completeness degree. For example, LGBMClassifier achieves 88,2\\% prediction accuracy for interviews with 100\\% completeness, 79,6\\% accuracy for interviews with 80\\% completeness, 58,9\\% accuracy for 60\\% completeness, and 45,7\\% accuracy for 40\\% completeness. The TabTransformer model demonstrated exceptional accuracy of over 80\\% for all degrees of completeness but required extensive training time, indicating a need for more powerful computational resources. The study highlights the linear correlation between interview completeness and predictive power of the decision-tree models.\n        △ Less",
        "authors":"['Sofia Krylova', 'Fabian Schmidt', 'Vladimir Vlassov']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring Video-Based Driver Activity Recognition under Noisy Labels",
        "abstract":"As an open research topic in the field of deep learning, learning with noisy labels has attracted much attention and grown rapidly over the past ten years. Learning with label noise is crucial for driver distraction behavior recognition, as real-world video data often contains mislabeled samples, impacting model reliability and performance. However, label noise learning is barely explored in the driver activity recognition field. In this paper, we propose the first label noise learning approach for the driver activity recognition task. Based on the cluster assumption, we initially enable the model to learn clustering-friendly low-dimensional representations from given videos and assign the resultant embeddings into clusters. We subsequently perform co-refinement within each cluster to smooth the classifier outputs. Furthermore, we propose a flexible sample selection strategy that combines two selection criteria without relying on any hyperparameters to filter clean samples from the training dataset. We also incorporate a self-adaptive parameter into the sample selection process to enforce balancing across classes. A comprehensive variety of experiments on the public Drive&Act dataset for all granularity levels demonstrates the superior performance of our method in comparison with other label-denoising methods derived from the image classification field. The source code is available at https:\/\/github.com\/ilonafan\/DAR-noisy-labels.\n        △ Less",
        "authors":"['Linjuan Fan', 'Di Wen', 'Kunyu Peng', 'Kailun Yang', 'Jiaming Zhang', 'Ruiping Liu', 'Yufan Chen', 'Junwei Zheng', 'Jiamin Wu', 'Xudong Han', 'Rainer Stiefelhagen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust and Fine-Grained Detection of AI Generated Texts",
        "abstract":"An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.\n        △ Less",
        "authors":"['Ram Mohan Rao Kadiyala', 'Siddartha Pullakhandam', 'Kanwal Mehreen', 'Drishti Sharma', 'Siddhant Gupta', 'Jebish Purbey', 'Ashay Srivastava', 'Subhasya TippaReddy', 'Arvind Reddy Bobbili', 'Suraj Telugara Chandrashekhar', 'Modabbir Adeeb', 'Srinadh Vura', 'Hamza Farooq']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning",
        "abstract":"Offline reinforcement learning (RL) learns effective policies from pre-collected datasets, offering a practical solution for applications where online interactions are risky or costly. Model-based approaches are particularly advantageous for offline RL, owing to their data efficiency and generalizability. However, due to inherent model errors, model-based methods often artificially introduce conservatism guided by heuristic uncertainty estimation, which can be unreliable. In this paper, we introduce VIPO, a novel model-based offline RL algorithm that incorporates self-supervised feedback from value estimation to enhance model training. Specifically, the model is learned by additionally minimizing the inconsistency between the value learned directly from the offline data and the one estimated from the model. We perform comprehensive evaluations from multiple perspectives to show that VIPO can learn a highly accurate model efficiently and consistently outperform existing methods. It offers a general framework that can be readily integrated into existing model-based offline RL algorithms to systematically enhance model accuracy. As a result, VIPO achieves state-of-the-art performance on almost all tasks in both D4RL and NeoRL benchmarks.\n        △ Less",
        "authors":"['Xuyang Chen', 'Guojian Wang', 'Keyu Yan', 'Lin Zhao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models",
        "abstract":"Unrestricted adversarial examples (UAEs), allow the attacker to create non-constrained adversarial examples without given clean samples, posing a severe threat to the safety of deep learning models. Recent works utilize diffusion models to generate UAEs. However, these UAEs often lack naturalness and imperceptibility due to simply optimizing in intermediate latent noises. In light of this, we propose SemDiff, a novel unrestricted adversarial attack that explores the semantic latent space of diffusion models for meaningful attributes, and devises a multi-attributes optimization approach to ensure attack success while maintaining the naturalness and imperceptibility of generated UAEs. We perform extensive experiments on four tasks on three high-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results demonstrate that SemDiff outperforms state-of-the-art methods in terms of attack success rate and imperceptibility. The generated UAEs are natural and exhibit semantically meaningful changes, in accord with the attributes' weights. In addition, SemDiff is found capable of evading different defenses, which further validates its effectiveness and threatening.\n        △ Less",
        "authors":"['Zeyu Dai', 'Shengcai Liu', 'Rui He', 'Jiahao Wu', 'Ning Lu', 'Wenqi Fan', 'Qing Li', 'Ke Tang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FedCanon: Non-Convex Composite Federated Learning with Efficient Proximal Operation on Heterogeneous Data",
        "abstract":"Composite federated learning offers a general framework for solving machine learning problems with additional regularization terms. However, many existing methods require clients to perform multiple proximal operations to handle non-smooth terms and their performance are often susceptible to data heterogeneity. To overcome these limitations, we propose a novel composite federated learning algorithm called \\textbf{FedCanon}, designed to solve the optimization problems comprising a possibly non-convex loss function and a weakly convex, potentially non-smooth regularization term. By decoupling proximal mappings from local updates, FedCanon requires only a single proximal evaluation on the server per iteration, thereby reducing the overall proximal computation cost. It also introduces control variables that incorporate global gradient information into client updates, which helps mitigate the effects of data heterogeneity. Theoretical analysis demonstrates that FedCanon achieves sublinear convergence rates under general non-convex settings and linear convergence under the Polyak-Łojasiewicz condition, without relying on bounded heterogeneity assumptions. Experiments demonstrate that FedCanon outperforms the state-of-the-art methods in terms of both accuracy and computational efficiency, particularly under heterogeneous data distributions.\n        △ Less",
        "authors":"['Yuan Zhou', 'Jiachen Zhong', 'Xinli Shi', 'Guanghui Wen', 'Xinghuo Yu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HyperSAT: Unsupervised Hypergraph Neural Networks for Weighted MaxSAT Problems",
        "abstract":"Graph neural networks (GNNs) have shown promising performance in solving both Boolean satisfiability (SAT) and Maximum Satisfiability (MaxSAT) problems due to their ability to efficiently model and capture the structural dependencies between literals and clauses. However, GNN methods for solving Weighted MaxSAT problems remain underdeveloped. The challenges arise from the non-linear dependency and sensitive objective function, which are caused by the non-uniform distribution of weights across clauses. In this paper, we present HyperSAT, a novel neural approach that employs an unsupervised hypergraph neural network model to solve Weighted MaxSAT problems. We propose a hypergraph representation for Weighted MaxSAT instances and design a cross-attention mechanism along with a shared representation constraint loss function to capture the logical interactions between positive and negative literal nodes in the hypergraph. Extensive experiments on various Weighted MaxSAT datasets demonstrate that HyperSAT achieves better performance than state-of-the-art competitors.\n        △ Less",
        "authors":"['Qiyue Chen', 'Shaolin Tan', 'Suixiang Gao', 'Jinhu Lü']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Benchmarking Mutual Information-based Loss Functions in Federated Learning",
        "abstract":"Federated Learning (FL) has attracted considerable interest due to growing privacy concerns and regulations like the General Data Protection Regulation (GDPR), which stresses the importance of privacy-preserving and fair machine learning approaches. In FL, model training takes place on decentralized data, so as to allow clients to upload a locally trained model and receive a globally aggregated model without exposing sensitive information. However, challenges related to fairness-such as biases, uneven performance among clients, and the \"free rider\" issue complicates its adoption. In this paper, we examine the use of Mutual Information (MI)-based loss functions to address these concerns. MI has proven to be a powerful method for measuring dependencies between variables and optimizing deep learning models. By leveraging MI to extract essential features and minimize biases, we aim to improve both the fairness and effectiveness of FL systems. Through extensive benchmarking, we assess the impact of MI-based losses in reducing disparities among clients while enhancing the overall performance of FL.\n        △ Less",
        "authors":"['Sarang S', 'Harsh D. Chothani', 'Qilei Li', 'Ahmed M. Abdelmoniem', 'Arnab K. Paul']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic framework for dynamic portfolio optimization",
        "abstract":"Typical deep reinforcement learning (DRL) agents for dynamic portfolio optimization learn the factors influencing portfolio return and risk by analyzing the output values of the reward function while adjusting portfolio weights within the training environment. However, it faces a major limitation where it is difficult for investors to intervene in the training based on different levels of risk aversion towards each portfolio asset. This difficulty arises from another limitation: existing DRL agents may not develop a thorough understanding of the factors responsible for the portfolio return and risk by only learning from the output of the reward function. As a result, the strategy for determining the target portfolio weights is entirely dependent on the DRL agents themselves. To address these limitations, we propose a reward factor matrix for elucidating the return and risk of each asset in the portfolio. Additionally, we propose a novel learning system named Factor-MCLS using a multi-critic framework that facilitates learning of the reward factor matrix. In this way, our DRL-based learning system can effectively learn the factors influencing portfolio return and risk. Moreover, based on the critic networks within the multi-critic framework, we develop a risk constraint term in the training objective function of the policy function. This risk constraint term allows investors to intervene in the training of the DRL agent according to their individual levels of risk aversion towards the portfolio assets.\n        △ Less",
        "authors":"['Ruoyu Sun', 'Angelos Stefanidis', 'Zhengyong Jiang', 'Jionglong Su']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transferable Deployment of Semantic Edge Inference Systems via Unsupervised Domain Adaption",
        "abstract":"This paper investigates deploying semantic edge inference systems for performing a common image clarification task. In particular, each system consists of multiple Internet of Things (IoT) devices that first locally encode the sensing data into semantic features and then transmit them to an edge server for subsequent data fusion and task inference. The inference accuracy is determined by efficient training of the feature encoder\/decoder using labeled data samples. Due to the difference in sensing data and communication channel distributions, deploying the system in a new environment may induce high costs in annotating data labels and re-training the encoder\/decoder models. To achieve cost-effective transferable system deployment, we propose an efficient Domain Adaptation method for Semantic Edge INference systems (DASEIN) that can maintain high inference accuracy in a new environment without the need for labeled samples. Specifically, DASEIN exploits the task-relevant data correlation between different deployment scenarios by leveraging the techniques of unsupervised domain adaptation and knowledge distillation. It devises an efficient two-step adaptation procedure that sequentially aligns the data distributions and adapts to the channel variations. Numerical results show that, under a substantial change in sensing data distributions, the proposed DASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in inference accuracy when the new environment has similar or 25 dB lower channel signal to noise power ratios (SNRs), respectively. This verifies the effectiveness of the proposed method in adapting both data and channel distributions in practical transfer deployment applications.\n        △ Less",
        "authors":"['Weiqiang Jiao', 'Suzhi Bi', 'Xian Li', 'Cheng Guo', 'Hao Chen', 'Zhi Quan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Problem of Best Arm Retention",
        "abstract":"This paper presents a comprehensive study on the problem of Best Arm Retention (BAR), which has recently found applications in streaming algorithms for multi-armed bandits. In the BAR problem, the goal is to retain $m$ arms with the best arm included from $n$ after some trials, in stochastic multi-armed bandit settings. We first investigate pure exploration for the BAR problem under different criteria, and then minimize the regret with specific constraints, in the context of further exploration in streaming algorithms.\n  - We begin by revisiting the lower bound for the $(\\varepsilon,δ)$-PAC algorithm for Best Arm Identification (BAI) and adapt the classical KL-divergence argument to derive optimal bounds for $(\\varepsilon,δ)$-PAC algorithms for BAR.\n  - We further study another variant of the problem, called $r$-BAR, which requires the expected gap between the best arm and the optimal arm retained is less than $r$. We prove tight sample complexity for the problem.\n  - We explore the regret minimization problem for $r$-BAR and develop algorithm beyond pure exploration. We conclude with a conjecture on the optimal regret in this setting.\n        △ Less",
        "authors":"['Houshuang Chen', 'Yuchen He', 'Chihao Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evaluating the Goal-Directedness of Large Language Models",
        "abstract":"To what extent do LLMs use their capabilities towards their given goal? We take this as a measure of their goal-directedness. We evaluate goal-directedness on tasks that require information gathering, cognitive effort, and plan execution, where we use subtasks to infer each model's relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI, and Anthropic show that goal-directedness is relatively consistent across tasks, differs from task performance, and is only moderately sensitive to motivational prompts. Notably, most models are not fully goal-directed. We hope our goal-directedness evaluations will enable better monitoring of LLM progress, and enable more deliberate design choices of agentic properties in LLMs.\n        △ Less",
        "authors":"['Tom Everitt', 'Cristina Garbacea', 'Alexis Bellot', 'Jonathan Richens', 'Henry Papadatos', 'Siméon Campos', 'Rohin Shah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using Spiking Vector Quantization",
        "abstract":"Graph Transformers (GTs), which simultaneously integrate message-passing and self-attention mechanisms, have achieved promising empirical results in some graph prediction tasks. Although these approaches show the potential of Transformers in capturing long-range graph topology information, issues concerning the quadratic complexity and high computing energy consumption severely limit the scalability of GTs on large-scale graphs. Recently, as brain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the development of graph representation learning methods with lower computational and storage overhead through the unique event-driven spiking neurons. Inspired by these characteristics, we propose a linear-time Graph Transformer using Spiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ reconstructs codebooks based on rate coding outputs from spiking neurons, and injects the codebooks into self-attention blocks to aggregate global information in linear complexity. Besides, spiking vector quantization effectively alleviates codebook collapse and the reliance on complex machinery (distance measure, auxiliary loss, etc.) present in previous vector quantization-based graph learning methods. In experiments, we compare GT-SVQ with other state-of-the-art baselines on node classification datasets ranging from small to large. Experimental results show that GT-SVQ has achieved competitive performances on most datasets while maintaining up to 130x faster inference speed compared to other GTs.\n        △ Less",
        "authors":"['Huizhe Zhang', 'Jintang Li', 'Yuchang Zhu', 'Liang Chen', 'Zibin Zheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Support is All You Need for Certified VAE Training",
        "abstract":"Variational Autoencoders (VAEs) have become increasingly popular and deployed in safety-critical applications. In such applications, we want to give certified probabilistic guarantees on performance under adversarial attacks. We propose a novel method, CIVET, for certified training of VAEs. CIVET depends on the key insight that we can bound worst-case VAE error by bounding the error on carefully chosen support sets at the latent layer. We show this point mathematically and present a novel training algorithm utilizing this insight. We show in an extensive evaluation across different datasets (in both the wireless and vision application areas), architectures, and perturbation magnitudes that our method outperforms SOTA methods achieving good standard performance with strong robustness guarantees.\n        △ Less",
        "authors":"['Changming Xu', 'Debangshu Banerjee', 'Deepak Vasisht', 'Gagandeep Singh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Emergence of Computational Structure in a Neural Network Physics Simulator",
        "abstract":"Neural networks often have identifiable computational structures - components of the network which perform an interpretable algorithm or task - but the mechanisms by which these emerge and the best methods for detecting these structures are not well understood. In this paper we investigate the emergence of computational structure in a transformer-like model trained to simulate the physics of a particle system, where the transformer's attention mechanism is used to transfer information between particles. We show that (a) structures emerge in the attention heads of the transformer which learn to detect particle collisions, (b) the emergence of these structures is associated to degenerate geometry in the loss landscape, and (c) the dynamics of this emergence follows a power law. This suggests that these components are governed by a degenerate \"effective potential\". These results have implications for the convergence time of computational structure within neural networks and suggest that the emergence of computational structure can be detected by studying the dynamics of network components.\n        △ Less",
        "authors":"['Rohan Hitchcock', 'Gary W. Delaney', 'Jonathan H. Manton', 'Richard Scalzo', 'Jingge Zhu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "abstract":"LLM inference is essential for applications like text summarization, translation, and data analysis, but the high cost of GPU instances from Cloud Service Providers (CSPs) like AWS is a major burden. This paper proposes InferSave, a cost-efficient VM selection framework for cloud based LLM inference. InferSave optimizes KV cache offloading based on Service Level Objectives (SLOs) and workload charac teristics, estimating GPU memory needs, and recommending cost-effective VM instances. Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. Experiments on AWS GPU instances show that selecting lower-cost instances without KV cache offloading improves cost efficiency by up to 73.7% for online workloads, while KV cache offloading saves up to 20.19% for offline workloads.\n        △ Less",
        "authors":"['Kihyun Kim', 'Jinwoo Kim', 'Hyunsun Chung', 'Myung-Hoon Cha', 'Hong-Yeon Kim', 'Youngjae Kim']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Manifold meta-learning for reduced-complexity neural system identification",
        "abstract":"System identification has greatly benefited from deep learning techniques, particularly for modeling complex, nonlinear dynamical systems with partially unknown physics where traditional approaches may not be feasible. However, deep learning models often require large datasets and significant computational resources at training and inference due to their high-dimensional parameterizations. To address this challenge, we propose a meta-learning framework that discovers a low-dimensional manifold within the parameter space of an over-parameterized neural network architecture. This manifold is learned from a meta-dataset of input-output sequences generated by a class of related dynamical systems, enabling efficient model training while preserving the network's expressive power for the considered system class. Unlike bilevel meta-learning approaches, our method employs an auxiliary neural network to map datasets directly onto the learned manifold, eliminating the need for costly second-order gradient computations during meta-training and reducing the number of first-order updates required in inference, which could be expensive for large models. We validate our approach on a family of Bouc-Wen oscillators, which is a well-studied nonlinear system identification benchmark. We demonstrate that we are able to learn accurate models even in small-data scenarios.\n        △ Less",
        "authors":"['Marco Forgione', 'Ankush Chakrabarty', 'Dario Piga', 'Matteo Rufolo', 'Alberto Bemporad']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs",
        "abstract":"Graph Neural Network (GNN) research is rapidly advancing due to GNNs' capacity to learn distributed representations from graph-structured data. However, centralizing large volumes of real-world graph data for GNN training is often impractical due to privacy concerns, regulatory restrictions, and commercial competition. Federated learning (FL), a distributed learning paradigm, offers a solution by preserving data privacy with collaborative model training. Despite progress in training huge vision and language models, federated learning for GNNs remains underexplored. To address this challenge, we present a novel method for federated learning on GNNs based on spectral GNNs equipped with neural ordinary differential equations (ODE) for better information capture, showing promising results across both homophilic and heterophilic graphs. Our approach effectively handles non-Independent and Identically Distributed (non-IID) data, while also achieving performance comparable to existing methods that only operate on IID data. It is designed to be privacy-preserving and bandwidth-optimized, making it suitable for real-world applications such as social network analysis, recommendation systems, and fraud detection, which often involve complex, non-IID, and heterophilic graph structures. Our results in the area of federated learning on non-IID heterophilic graphs demonstrate significant improvements, while also achieving better performance on homophilic graphs. This work highlights the potential of federated learning in diverse and challenging graph settings. Open-source code available on GitHub (https:\/\/github.com\/SpringWiz11\/Fed-GNODEFormer).\n        △ Less",
        "authors":"['Kishan Gurumurthy', 'Himanshu Pal', 'Charu Sharma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Résumé abstractif à partir d'une transcription audio",
        "abstract":"Currently, large language models are gaining popularity, their achievements are used in many areas, ranging from text translation to generating answers to queries. However, the main problem with these new machine learning algorithms is that training such models requires large computing resources that only large IT companies have. To avoid this problem, a number of methods (LoRA, quantization) have been proposed so that existing models can be effectively fine-tuned for specific tasks. In this paper, we propose an E2E (end to end) audio summarization model using these techniques. In addition, this paper examines the effectiveness of these approaches to the problem under consideration and draws conclusions about the applicability of these methods.\n        △ Less",
        "authors":"['Ilia Derkach']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Large Language Models for Drug Overdose Prediction from Longitudinal Medical Records",
        "abstract":"The ability to predict drug overdose risk from a patient's medical records is crucial for timely intervention and prevention. Traditional machine learning models have shown promise in analyzing longitudinal medical records for this task. However, recent advancements in large language models (LLMs) offer an opportunity to enhance prediction performance by leveraging their ability to process long textual data and their inherent prior knowledge across diverse tasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in predicting drug overdose events using patients' longitudinal insurance claims records. We evaluate its performance in both fine-tuned and zero-shot settings, comparing them to strong traditional machine learning methods as baselines. Our results show that LLMs not only outperform traditional models in certain settings but can also predict overdose risk in a zero-shot setting without task-specific training. These findings highlight the potential of LLMs in clinical decision support, particularly for drug overdose risk prediction.\n        △ Less",
        "authors":"['Md Sultan Al Nahian', 'Chris Delcher', 'Daniel Harris', 'Peter Akpunonu', 'Ramakanth Kavuluru']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model",
        "abstract":"Unsupervised anomaly detection in hyperspectral images (HSI), aiming to detect unknown targets from backgrounds, is challenging for earth surface monitoring. However, current studies are hindered by steep computational costs due to the high-dimensional property of HSI and dense sampling-based training paradigm, constraining their rapid deployment. Our key observation is that, during training, not all samples within the same homogeneous area are indispensable, whereas ingenious sampling can provide a powerful substitute for reducing costs. Motivated by this, we propose an Asymmetrical Consensus State Space Model (ACMamba) to significantly reduce computational costs without compromising accuracy. Specifically, we design an asymmetrical anomaly detection paradigm that utilizes region-level instances as an efficient alternative to dense pixel-level samples. In this paradigm, a low-cost Mamba-based module is introduced to discover global contextual attributes of regions that are essential for HSI reconstruction. Additionally, we develop a consensus learning strategy from the optimization perspective to simultaneously facilitate background reconstruction and anomaly compression, further alleviating the negative impact of anomaly reconstruction. Theoretical analysis and extensive experiments across eight benchmarks verify the superiority of ACMamba, demonstrating a faster speed and stronger performance over the state-of-the-art.\n        △ Less",
        "authors":"['Guanchun Wang', 'Xiangrong Zhang', 'Yifei Zhang', 'Zelin Peng', 'Tianyang Zhang', 'Xu Tang', 'Licheng Jiao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets",
        "abstract":"Medical Visual Question Answering (MVQA) systems can interpret medical images in response to natural language queries. However, linguistic variability in question phrasing often undermines the consistency of these systems. To address this challenge, we propose a Semantically Equivalent Question Augmentation (SEQA) framework, which leverages large language models (LLMs) to generate diverse yet semantically equivalent rephrasings of questions. Specifically, this approach enriches linguistic diversity while preserving semantic meaning. We further introduce an evaluation metric, Total Agreement Rate with Semantically Equivalent Input and Correct Answer (TAR-SC), which assesses a model's capability to generate consistent and correct responses to semantically equivalent linguistic variations. In addition, we also propose three other diversity metrics - average number of QA items per image (ANQI), average number of questions per image with the same answer (ANQA), and average number of open-ended questions per image with the same semantics (ANQS). Using the SEQA framework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD, and PathVQA. As a result, all three datasets achieved significant improvements by incorporating more semantically equivalent questions: ANQI increased by an average of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate three MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and fine-tuning settings on the enhanced datasets. Experimental results in MVQA datasets show that fine-tuned models achieve an average accuracy improvement of 19.35%, while our proposed TAR-SC metric shows an average improvement of 11. 61%, indicating a substantial enhancement in model consistency.\n        △ Less",
        "authors":"['Yongpei Ma', 'Pengyu Wang', 'Adam Dunn', 'Usman Naseem', 'Jinman Kim']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Discrimination-free Insurance Pricing with Privatized Sensitive Attributes",
        "abstract":"Fairness has emerged as a critical consideration in the landscape of machine learning algorithms, particularly as AI continues to transform decision-making across societal domains. To ensure that these algorithms are free from bias and do not discriminate against individuals based on sensitive attributes such as gender and race, the field of algorithmic bias has introduced various fairness concepts, along with methodologies to achieve these notions in different contexts. Despite the rapid advancement, not all sectors have embraced these fairness principles to the same extent. One specific sector that merits attention in this regard is insurance. Within the realm of insurance pricing, fairness is defined through a distinct and specialized framework. Consequently, achieving fairness according to established notions does not automatically ensure fair pricing in insurance. In particular, regulators are increasingly emphasizing transparency in pricing algorithms and imposing constraints on insurance companies on the collection and utilization of sensitive consumer attributes. These factors present additional challenges in the implementation of fairness in pricing algorithms. To address these complexities and comply with regulatory demands, we propose an efficient method for constructing fair models that are tailored to the insurance domain, using only privatized sensitive attributes. Notably, our approach ensures statistical guarantees, does not require direct access to sensitive attributes, and adapts to varying transparency requirements, addressing regulatory demands while ensuring fairness in insurance pricing.\n        △ Less",
        "authors":"['Tianhe Zhang', 'Suhan Liu', 'Peng Shi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamics and Computational Principles of Echo State Networks: A Mathematical Perspective",
        "abstract":"Reservoir computing (RC) represents a class of state-space models (SSMs) characterized by a fixed state transition mechanism (the reservoir) and a flexible readout layer that maps from the state space. It is a paradigm of computational dynamical systems that harnesses the transient dynamics of high-dimensional state spaces for efficient processing of temporal data. Rooted in concepts from recurrent neural networks, RC achieves exceptional computational power by decoupling the training of the dynamic reservoir from the linear readout layer, thereby circumventing the complexities of gradient-based optimization. This work presents a systematic exploration of RC, addressing its foundational properties such as the echo state property, fading memory, and reservoir capacity through the lens of dynamical systems theory. We formalize the interplay between input signals and reservoir states, demonstrating the conditions under which reservoirs exhibit stability and expressive power. Further, we delve into the computational trade-offs and robustness characteristics of RC architectures, extending the discussion to their applications in signal processing, time-series prediction, and control systems. The analysis is complemented by theoretical insights into optimization, training methodologies, and scalability, highlighting open challenges and potential directions for advancing the theoretical underpinnings of RC.\n        △ Less",
        "authors":"['Pradeep Singh', 'Ashutosh Kumar', 'Sutirtha Ghosh', 'Hrishit B P', 'Balasubramanian Raman']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision",
        "abstract":"We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GrabS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then design an embodied agent to learn to discover multiple objects by querying against the pretrained generative priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods.\n        △ Less",
        "authors":"['Zihui Zhang', 'Yafei Yang', 'Hongtao Wen', 'Bo Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?",
        "abstract":"Recent supervised fine-tuning (SFT) approaches have significantly improved language models' performance on mathematical reasoning tasks, even when models are trained at a small scale. However, the specific capabilities enhanced through such fine-tuning remain poorly understood. In this paper, we conduct a detailed analysis of model performance on the AIME24 dataset to understand how reasoning capabilities evolve. We discover a ladder-like structure in problem difficulty, categorize questions into four tiers (Easy, Medium, Hard, and Extremely Hard (Exh)), and identify the specific requirements for advancing between tiers. We find that progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances), while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain, with accuracy plateauing at around 65% despite logarithmic scaling. Exh-level questions present a fundamentally different challenge; they require unconventional problem-solving skills that current models uniformly struggle with. Additional findings reveal that carefully curated small-scale datasets offer limited advantage-scaling dataset size proves far more effective. Our analysis provides a clearer roadmap for advancing language model capabilities in mathematical reasoning.\n        △ Less",
        "authors":"['Yiyou Sun', 'Georgia Zhou', 'Hao Wang', 'Dacheng Li', 'Nouha Dziri', 'Dawn Song']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception",
        "abstract":"Inertial measurement units (IMUs), have been prevalently used in a wide range of mobile perception applications such as activity recognition and user authentication, where a large amount of labelled data are normally required to train a satisfactory model. However, it is difficult to label micro-activities in massive IMU data due to the hardness of understanding raw IMU data and the lack of ground truth. In this paper, we propose a novel fine-grained user perception approach, called Saga, which only needs a small amount of labelled IMU data to achieve stunning user perception accuracy. The core idea of Saga is to first pre-train a backbone feature extraction model, utilizing the rich semantic information of different levels embedded in the massive unlabelled IMU data. Meanwhile, for a specific downstream user perception application, Bayesian Optimization is employed to determine the optimal weights for pre-training tasks involving different semantic levels. We implement Saga on five typical mobile phones and evaluate Saga on three typical tasks on three IMU datasets. Results show that when only using about 100 training samples per class, Saga can achieve over 90% accuracy of the full-fledged model trained on over ten thousands training samples with no additional system overhead.\n        △ Less",
        "authors":"['Yunzhe Li', 'Facheng Hu', 'Hongzi Zhu', 'Shifan Zhang', 'Liang Zhang', 'Shan Chang', 'Minyi Guo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Unravelling Technical debt topics through Time, Programming Languages and Repository",
        "abstract":"This study explores the dynamic landscape of Technical Debt (TD) topics in software engineering by examining its evolution across time, programming languages, and repositories. Despite the extensive research on identifying and quantifying TD, there remains a significant gap in understanding the diversity of TD topics and their temporal development. To address this, we have conducted an explorative analysis of TD data extracted from GitHub issues spanning from 2015 to September 2023. We employed BERTopic for sophisticated topic modelling. This study categorises the TD topics and tracks their progression over time. Furthermore, we have incorporated sentiment analysis for each identified topic, providing a deeper insight into the perceptions and attitudes associated with these topics. This offers a more nuanced understanding of the trends and shifts in TD topics through time, programming language, and repository.\n        △ Less",
        "authors":"['Karthik Shivashankar', 'Antonio Martini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching",
        "abstract":"We introduce Adjoint Sampling, a highly scalable and efficient algorithm for learning diffusion processes that sample from unnormalized densities, or energy functions. It is the first on-policy approach that allows significantly more gradient updates than the number of energy evaluations and model samples, allowing us to scale to much larger problem settings than previously explored by similar methods. Our framework is theoretically grounded in stochastic optimal control and shares the same theoretical guarantees as Adjoint Matching, being able to train without the need for corrective measures that push samples towards the target distribution. We show how to incorporate key symmetries, as well as periodic boundary conditions, for modeling molecules in both cartesian and torsional coordinates. We demonstrate the effectiveness of our approach through extensive experiments on classical energy functions, and further scale up to neural network-based energy models where we perform amortized conformer generation across many molecular systems. To encourage further research in developing highly scalable sampling methods, we plan to open source these challenging benchmarks, where successful methods can directly impact progress in computational chemistry.\n        △ Less",
        "authors":"['Aaron Havens', 'Benjamin Kurt Miller', 'Bing Yan', 'Carles Domingo-Enrich', 'Anuroop Sriram', 'Brandon Wood', 'Daniel Levine', 'Bin Hu', 'Brandon Amos', 'Brian Karrer', 'Xiang Fu', 'Guan-Horng Liu', 'Ricky T. Q. Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Clustering and analysis of user behaviour in blockchain: A case study of Planet IX",
        "abstract":"Decentralised applications (dApps) that run on public blockchains have the benefit of trustworthiness and transparency as every activity that happens on the blockchain can be publicly traced through the transaction data. However, this introduces a potential privacy problem as this data can be tracked and analysed, which can reveal user-behaviour information. A user behaviour analysis pipeline was proposed to present how this type of information can be extracted and analysed to identify separate behavioural clusters that can describe how users behave in the game. The pipeline starts with the collection of transaction data, involving smart contracts, that is collected from a blockchain-based game called Planet IX. Both the raw transaction information and the transaction events are considered in the data collection. From this data, separate game actions can be formed and those are leveraged to present how and when the users conducted their in-game activities in the form of user flows. An extended version of these user flows also presents how the Non-Fungible Tokens (NFTs) are being leveraged in the user actions. The latter is given as input for a Graph Neural Network (GNN) model to provide graph embeddings for these flows which then can be leveraged by clustering algorithms to cluster user behaviours into separate behavioural clusters. We benchmark and compare well-known clustering algorithms as a part of the proposed method. The user behaviour clusters were analysed and visualised in a graph format. It was found that behavioural information can be extracted regarding the users that belong to these clusters. Such information can be exploited by malicious users to their advantage. To demonstrate this, a privacy threat model was also presented based on the results that correspond to multiple potentially affected areas.\n        △ Less",
        "authors":"['Dorottya Zelenyanszki', 'Zhe Hou', 'Kamanashis Biswas', 'Vallipuram Muthukkumarasamy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint Structural Node Encoding and Self-Supervised Learning",
        "abstract":"Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in representation learning, a challenge further amplified in self-supervised settings. We propose H$^3$GNNs, an end-to-end self-supervised learning framework that harmonizes both structural properties through two key innovations: (i) Joint Structural Node Encoding. We embed nodes into a unified space combining linear and non-linear feature projections with K-hop structural representations via a Weighted Graph Convolution Network(WGCN). A cross-attention mechanism enhances awareness and adaptability to heterophily and homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive Architectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a teacher-student model, the student sees the masked input graph and predicts node features inferred by the teacher that sees the full input graph in the joint encoding space. To enhance learning difficulty, we introduce two novel node-predictive-difficulty-based masking strategies. Experiments on seven benchmarks (four heterophily datasets and three homophily datasets) confirm the effectiveness and efficiency of H$^3$GNNs across diverse graph types. Our H$^3$GNNs achieves overall state-of-the-art performance on the four heterophily datasets, while retaining on-par performance to previous state-of-the-art methods on the three homophily datasets.\n        △ Less",
        "authors":"['Rui Xue', 'Tianfu Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation",
        "abstract":"Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications.\n        △ Less",
        "authors":"['Ji Ma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transformer-Driven Neural Beamforming with Imperfect CSI in Urban Macro Wireless Channels",
        "abstract":"The literature is abundant with methodologies focusing on using transformer architectures due to their prominence in wireless signal processing and their capability to capture long-range dependencies via attention mechanisms. In particular, depthwise separable convolutions enhance parameter efficiency for the process of high-dimensional data characteristics of MIMO systems. In this work, we introduce a novel unsupervised deep learning framework that integrates depthwise separable convolutions and transformers to generate beamforming weights under imperfect channel state information (CSI) for a multi-user single-input multiple-output (MU-SIMO) system in dense urban environments. The primary goal is to enhance throughput by maximizing sum-rate while ensuring reliable communication. Spectral efficiency and block error rate (BLER) are considered as performance metrics. Experiments are carried out under various conditions to compare the performance of the proposed NNBF framework against baseline methods zero-forcing beamforming (ZFBF) and minimum mean square error (MMSE) beamforming. Experimental results demonstrate the superiority of the proposed framework over the baseline techniques.\n        △ Less",
        "authors":"['Cemil Vahapoglu', \"Timothy J. O'Shea\", 'Wan Liu', 'Tamoghna Roy', 'Sennur Ulukus']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Real-time Object and Event Detection Service through Computer Vision and Edge Computing",
        "abstract":"The World Health Organization suggests that road traffic crashes cost approximately 518 billion dollars globally each year, which accounts for 3% of the gross domestic product for most countries. Most fatal road accidents in urban areas involve Vulnerable Road Users (VRUs). Smart cities environments present innovative approaches to combat accidents involving cutting-edge technologies, that include advanced sensors, extensive datasets, Machine Learning (ML) models, communication systems, and edge computing. This paper proposes a strategy and an implementation of a system for road monitoring and safety for smart cities, based on Computer Vision (CV) and edge computing. Promising results were obtained by implementing vision algorithms and tracking using surveillance cameras, that are part of a Smart City testbed, the Aveiro Tech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars, pedestrians, and bicycles, while predicting the road state, the distance between moving objects, and inferring on collision events to prevent collisions, in near real-time.\n        △ Less",
        "authors":"['Marcos Mendes', 'Gonçalo Perna', 'Pedro Rito', 'Duarte Raposo', 'Susana Sargento']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float",
        "abstract":"Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read\/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at https:\/\/github.com\/LeanModels\/DFloat11.\n        △ Less",
        "authors":"['Tianyi Zhang', 'Yang Sui', 'Shaochen Zhong', 'Vipin Chaudhary', 'Xia Hu', 'Anshumali Shrivastava']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data driven approach towards more efficient Newton-Raphson power flow calculation for distribution grids",
        "abstract":"Power flow (PF) calculations are fundamental to power system analysis to ensure stable and reliable grid operation. The Newton-Raphson (NR) method is commonly used for PF analysis due to its rapid convergence when initialized properly. However, as power grids operate closer to their capacity limits, ill-conditioned cases and convergence issues pose significant challenges. This work, therefore, addresses these challenges by proposing strategies to improve NR initialization, hence minimizing iterations and avoiding divergence. We explore three approaches: (i) an analytical method that estimates the basin of attraction using mathematical bounds on voltages, (ii) Two data-driven models leveraging supervised learning or physics-informed neural networks (PINNs) to predict optimal initial guesses, and (iii) a reinforcement learning (RL) approach that incrementally adjusts voltages to accelerate convergence. These methods are tested on benchmark systems. This research is particularly relevant for modern power systems, where high penetration of renewables and decentralized generation require robust and scalable PF solutions. In experiments, all three proposed methods demonstrate a strong ability to provide an initial guess for Newton-Raphson method to converge with fewer steps. The findings provide a pathway for more efficient real-time grid operations, which, in turn, support the transition toward smarter and more resilient electricity networks.\n        △ Less",
        "authors":"['Shengyuan Yan', 'Farzad Vazinram', 'Zeynab Kaseb', 'Lindsay Spoor', 'Jochen Stiasny', 'Betul Mamudi', 'Amirhossein Heydarian Ardakani', 'Ugochukwu Orji', 'Pedro P. Vergara', 'Yu Xiang', 'Jerry Guo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling",
        "abstract":"Motivated by collaborative reinforcement learning (RL) and optimization with time-correlated data, we study a generic federated stochastic approximation problem involving $M$ agents, where each agent is characterized by an agent-specific (potentially nonlinear) local operator. The goal is for the agents to communicate intermittently via a server to find the root of the average of the agents' local operators. The generality of our setting stems from allowing for (i) Markovian data at each agent and (ii) heterogeneity in the roots of the agents' local operators. The limited recent work that has accounted for both these features in a federated setting fails to guarantee convergence to the desired point or to show any benefit of collaboration; furthermore, they rely on projection steps in their algorithms to guarantee bounded iterates. Our work overcomes each of these limitations. We develop a novel algorithm titled \\texttt{FedHSA}, and prove that it guarantees convergence to the correct point, while enjoying an $M$-fold linear speedup in sample-complexity due to collaboration. To our knowledge, \\emph{this is the first finite-time result of its kind}, and establishing it (without relying on a projection step) entails a fairly intricate argument that accounts for the interplay between complex temporal correlations due to Markovian sampling, multiple local steps to save communication, and the drift-effects induced by heterogeneous local operators. Our results have implications for a broad class of heterogeneous federated RL problems (e.g., policy evaluation and control) with function approximation, where the agents' Markov decision processes can differ in their probability transition kernels and reward functions.\n        △ Less",
        "authors":"['Feng Zhu', 'Aritra Mitra', 'Robert W. Heath']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Possibility for Proactive Anomaly Detection",
        "abstract":"Time-series anomaly detection, which detects errors and failures in a workflow, is one of the most important topics in real-world applications. The purpose of time-series anomaly detection is to reduce potential damages or losses. However, existing anomaly detection models detect anomalies through the error between the model output and the ground truth (observed) value, which makes them impractical. In this work, we present a \\textit{proactive} approach for time-series anomaly detection based on a time-series forecasting model specialized for anomaly detection and a data-driven anomaly detection model. Our proactive approach establishes an anomaly threshold from training data with a data-driven anomaly detection model, and anomalies are subsequently detected by identifying predicted values that exceed the anomaly threshold. In addition, we extensively evaluated the model using four anomaly detection benchmarks and analyzed both predictable and unpredictable anomalies. We attached the source code as supplementary material.\n        △ Less",
        "authors":"['Jinsung Jeon', 'Jaehyeon Park', 'Sewon Park', 'Jeongwhan Choi', 'Minjung Kim', 'Noseong Park']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' \"Typo\" Correction",
        "abstract":"The large integration of microphones into devices increases the opportunities for Acoustic Side-Channel Attacks (ASCAs), as these can be used to capture keystrokes' audio signals that might reveal sensitive information. However, the current State-Of-The-Art (SOTA) models for ASCAs, including Convolutional Neural Networks (CNNs) and hybrid models, such as CoAtNet, still exhibit limited robustness under realistic noisy conditions. Solving this problem requires either: (i) an increased model's capacity to infer contextual information from longer sequences, allowing the model to learn that an initially noisily typed word is the same as a futurely collected non-noisy word, or (ii) an approach to fix misidentified information from the contexts, as one does not type random words, but the ones that best fit the conversation context. In this paper, we demonstrate that both strategies are viable and complementary solutions for making ASCAs practical. We observed that no existing solution leverages advanced transformer architectures' power for these tasks and propose that: (i) Visual Transformers (VTs) are the candidate solutions for capturing long-term contextual information and (ii) transformer-powered Large Language Models (LLMs) are the candidate solutions to fix the ``typos'' (mispredictions) the model might make. Thus, we here present the first-of-its-kind approach that integrates VTs and LLMs for ASCAs.\n  We first show that VTs achieve SOTA performance in classifying keystrokes when compared to the previous CNN benchmark. Second, we demonstrate that LLMs can mitigate the impact of real-world noise. Evaluations on the natural sentences revealed that: (i) incorporating LLMs (e.g., GPT-4o) in our ASCA pipeline boosts the performance of error-correction tasks; and (ii) the comparable performance can be attained by a lightweight, fine-tuned smaller LLM (67 times smaller than GPT-4o), using...\n        △ Less",
        "authors":"['Seyyed Ali Ayati', 'Jin Hyun Park', 'Yichen Cai', 'Marcus Botacin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust Markov stability for community detection at a scale learned based on the structure",
        "abstract":"Community detection, the unsupervised task of clustering nodes of a graph, finds applications across various fields. The common approaches for community detection involve optimizing an objective function to partition the nodes into communities at a single scale of granularity. However, the single-scale approaches often fall short of producing partitions that are robust and at a suitable scale. The existing algorithm, PyGenStability, returns multiple robust partitions for a network by optimizing the multi-scale Markov stability function. However, in cases where the suitable scale is not known or assumed by the user, there is no principled method to select a single robust partition at a suitable scale from the multiple partitions that PyGenStability produces. Our proposed method combines the Markov stability framework with a pre-trained machine learning model for scale selection to obtain one robust partition at a scale that is learned based on the graph structure. This automatic scale selection involves using a gradient boosting model pre-trained on hand-crafted and embedding-based network features from a labeled dataset of 10k benchmark networks. This model was trained to predicts the scale value that maximizes the similarity of the output partition to the planted partition of the benchmark network. Combining our scale selection algorithm with the PyGenStability algorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale community detection algorithm that returns one robust partition at a suitable scale without the need for any assumptions, input, or tweaking from the user. We compare the performance of PO against 29 algorithms and show that it outperforms 25 other algorithms by statistically meaningful margins. Our results facilitate choosing between community detection algorithms, among which PO stands out as the accurate, robust, and hyperparameter-free method.\n        △ Less",
        "authors":"['Samin Aref', 'Sanchaai Mathiyarasan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generalized probabilistic canonical correlation analysis for multi-modal data integration with full or partial observations",
        "abstract":"Background: The integration and analysis of multi-modal data are increasingly essential across various domains including bioinformatics. As the volume and complexity of such data grow, there is a pressing need for computational models that not only integrate diverse modalities but also leverage their complementary information to improve clustering accuracy and insights, especially when dealing with partial observations with missing data. Results: We propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an unsupervised method for the integration and joint dimensionality reduction of multi-modal data. GPCCA addresses key challenges in multi-modal data analysis by handling missing values within the model, enabling the integration of more than two modalities, and identifying informative features while accounting for correlations within individual modalities. The model demonstrates robustness to various missing data patterns and provides low-dimensional embeddings that facilitate downstream clustering and analysis. In a range of simulation settings, GPCCA outperforms existing methods in capturing essential patterns across modalities. Additionally, we demonstrate its applicability to multi-omics data from TCGA cancer datasets and a multi-view image dataset. Conclusion: GPCCA offers a useful framework for multi-modal data integration, effectively handling missing data and providing informative low-dimensional embeddings. Its performance across cancer genomics and multi-view image data highlights its robustness and potential for broad application. To make the method accessible to the wider research community, we have released an R package, GPCCA, which is available at https:\/\/github.com\/Kaversoniano\/GPCCA.\n        △ Less",
        "authors":"['Tianjian Yang', 'Wei Vivian Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Interpretable Deep Generative Models via Causal Representation Learning",
        "abstract":"Recent developments in generative artificial intelligence (AI) rely on machine learning techniques such as deep learning and generative modeling to achieve state-of-the-art performance across wide-ranging domains. These methods' surprising performance is due in part to their ability to learn implicit \"representations'' of complex, multi-modal data. Unfortunately, deep neural networks are notoriously black boxes that obscure these representations, making them difficult to interpret or analyze. To resolve these difficulties, one approach is to build new interpretable neural network models from the ground up. This is the goal of the emerging field of causal representation learning (CRL) that uses causality as a vector for building flexible, interpretable, and transferable generative AI. CRL can be seen as a culmination of three intrinsically statistical problems: (i) latent variable models such as factor analysis; (ii) causal graphical models with latent variables; and (iii) nonparametric statistics and deep learning. This paper reviews recent progress in CRL from a statistical perspective, focusing on connections to classical models and statistical and causal identifiablity results. This review also highlights key application areas, implementation strategies, and open statistical questions in CRL.\n        △ Less",
        "authors":"['Gemma E. Moran', 'Bryon Aragam']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Measuring Computational Universality of Fully Homomorphic Encryption",
        "abstract":"Many real-world applications, such as machine learning and graph analytics, involve combinations of linear and non-linear operations. As these applications increasingly handle sensitive data, there is a significant demand for privacy-preserving computation techniques capable of efficiently supporting both types of operations-a property we define as \"computational universality.\" Fully Homomorphic Encryption (FHE) has emerged as a powerful approach to perform computations directly on encrypted data. In this paper, we systematically evaluate and measure whether existing FHE methods achieve computational universality or primarily favor either linear or non-linear operations, especially in non-interactive settings. We evaluate FHE universality in three stages. First, we categorize existing FHE methods into ten distinct approaches and analyze their theoretical complexities, selecting the three most promising universal candidates. Next, we perform measurements on representative workloads that combine linear and non-linear operations in various sequences, assessing performance across different bit lengths and with SIMD parallelization enabled or disabled. Finally, we empirically evaluate these candidates on five real-world, privacy-sensitive applications, where each involving arithmetic (linear) and comparison-like (non-linear) operations. Our findings indicate significant overheads in current universal FHE solutions, with efficiency strongly influenced by SIMD parallelism, word-wise versus bit-wise operations, and the trade-off between approximate and exact computations. Additionally, our analysis provides practical guidance to help practitioners select the most suitable universal FHE schemes and algorithms based on specific application requirements.\n        △ Less",
        "authors":"['Jiaqi Xue', 'Xin Xin', 'Wei Zhang', 'Mengxin Zheng', 'Qianqian Song', 'Minxuan Zhou', 'Yushun Dong', 'Dongjie Wang', 'Xun Chen', 'Jiafeng Xie', 'Liqiang Wang', 'David Mohaisen', 'Hongyi Wu', 'Qian Lou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dueling Deep Reinforcement Learning for Financial Time Series",
        "abstract":"Reinforcement learning (RL) has emerged as a powerful paradigm for solving decision-making problems in dynamic environments. In this research, we explore the application of Double DQN (DDQN) and Dueling Network Architectures, to financial trading tasks using historical SP500 index data. Our focus is training agents capable of optimizing trading strategies while accounting for practical constraints such as transaction costs. The study evaluates the model performance across scenarios with and without commissions, highlighting the impact of cost-sensitive environments on reward dynamics. Despite computational limitations and the inherent complexity of financial time series data, the agent successfully learned meaningful trading policies. The findings confirm that RL agents, even when trained on limited datasets, can outperform random strategies by leveraging advanced architectures such as DDQN and Dueling Networks. However, significant challenges persist, particularly with a sub-optimal policy due to the complexity of data source.\n        △ Less",
        "authors":"['Bruno Giorgio']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards a Universal Vibration Analysis Dataset: A Framework for Transfer Learning in Predictive Maintenance and Structural Health Monitoring",
        "abstract":"ImageNet has become a reputable resource for transfer learning, allowing the development of efficient ML models with reduced training time and data requirements. However, vibration analysis in predictive maintenance, structural health monitoring, and fault diagnosis, lacks a comparable large-scale, annotated dataset to facilitate similar advancements. To address this, a dataset framework is proposed that begins with bearing vibration data as an initial step towards creating a universal dataset for vibration-based spectrogram analysis for all machinery. The initial framework includes a collection of bearing vibration signals from various publicly available datasets. To demonstrate the advantages of this framework, experiments were conducted using a deep learning architecture, showing improvements in model performance when pre-trained on bearing vibration data and fine-tuned on a smaller, domain-specific dataset. These findings highlight the potential to parallel the success of ImageNet in visual computing but for vibration analysis. For future work, this research will include a broader range of vibration signals from multiple types of machinery, emphasizing spectrogram-based representations of the data. Each sample will be labeled according to machinery type, operational status, and the presence or type of faults, ensuring its utility for supervised and unsupervised learning tasks. Additionally, a framework for data preprocessing, feature extraction, and model training specific to vibration data will be developed. This framework will standardize methodologies across the research community, allowing for collaboration and accelerating progress in predictive maintenance, structural health monitoring, and related fields. By mirroring the success of ImageNet in visual computing, this dataset has the potential to improve the development of intelligent systems in industrial applications.\n        △ Less",
        "authors":"['Mert Sehri', 'Igor Varejão', 'Zehui Hua', 'Vitor Bonella', 'Adriano Santos', 'Francisco de Assis Boldt', 'Patrick Dumond', 'Flavio Miguel Varejão']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks",
        "abstract":"Detecting Distributed Denial of Service (DDoS) attacks in Multi-Environment (M-En) networks presents significant challenges due to diverse malicious traffic patterns and the evolving nature of cyber threats. Existing AI-based detection systems struggle to adapt to new attack strategies and lack real-time attack detection capabilities with high accuracy and efficiency. This study proposes an online, continuous learning methodology for DDoS detection in M-En networks, enabling continuous model updates and real-time adaptation to emerging threats, including zero-day attacks. First, we develop a unique M-En network dataset by setting up a realistic, real-time simulation using the NS-3 tool, incorporating both victim and bot devices. DDoS attacks with varying packet sizes are simulated using the DDoSim application across IoT and traditional IP-based environments under M-En network criteria. Our approach employs a multi-level framework (MULTI-LF) featuring two machine learning models: a lightweight Model 1 (M1) trained on a selective, critical packet dataset for fast and efficient initial detection, and a more complex, highly accurate Model 2 (M2) trained on extensive data. When M1 exhibits low confidence in its predictions, the decision is escalated to M2 for verification and potential fine-tuning of M1 using insights from M2. If both models demonstrate low confidence, the system flags the incident for human intervention, facilitating model updates with human-verified categories to enhance adaptability to unseen attack patterns. We validate the MULTI-LF through real-world simulations, demonstrating superior classification accuracy of 0.999 and low prediction latency of 0.866 seconds compared to established baselines. Furthermore, we evaluate performance in terms of memory usage (3.632 MB) and CPU utilization (10.05%) in real-time scenarios.\n        △ Less",
        "authors":"['Furqan Rustam', 'Islam Obaidat', 'Anca Delia Jurcut']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Traffic Adaptive Moving-window Service Patrolling for Real-time Incident Management during High-impact Events",
        "abstract":"This paper presents the Traffic Adaptive Moving-window Patrolling Algorithm (TAMPA), designed to improve real-time incident management during major events like sports tournaments and concerts. Such events significantly stress transportation networks, requiring efficient and adaptive patrol solutions. TAMPA integrates predictive traffic modeling and real-time complaint estimation, dynamically optimizing patrol deployment. Using dynamic programming, the algorithm continuously adjusts patrol strategies within short planning windows, effectively balancing immediate response and efficient routing. Leveraging the Dvoretzky-Kiefer-Wolfowitz inequality, TAMPA detects significant shifts in complaint patterns, triggering proactive adjustments in patrol routes. Theoretical analyses ensure performance remains closely aligned with optimal solutions. Simulation results from an urban traffic network demonstrate TAMPA's superior performance, showing improvements of approximately 87.5\\% over stationary methods and 114.2\\% over random strategies. Future work includes enhancing adaptability and incorporating digital twin technology for improved predictive accuracy, particularly relevant for events like the 2026 FIFA World Cup at MetLife Stadium.\n        △ Less",
        "authors":"['Haozhe Lei', 'Ya-Ting Yang', 'Tao Li', 'Zilin Bian', 'Fan Zuo', 'Sundeep Rangan', 'Kaan Ozbay']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism",
        "abstract":"We introduce the Error Broadcast and Decorrelation (EBD) algorithm, a novel learning framework that addresses the credit assignment problem in neural networks by directly broadcasting output error to individual layers. Leveraging the stochastic orthogonality property of the optimal minimum mean square error (MMSE) estimator, EBD defines layerwise loss functions to penalize correlations between layer activations and output errors, offering a principled approach to error broadcasting without the need for weight transport. The optimization framework naturally leads to the experimentally observed three-factor learning rule and integrates with biologically plausible frameworks to enhance performance and plausibility. Numerical experiments demonstrate that EBD achieves performance comparable to or better than known error-broadcast methods on benchmark datasets. While the scalability of EBD to very large or complex datasets remains to be further explored, our findings suggest it provides a biologically plausible, efficient, and adaptable alternative for neural network training. This approach could inform future advancements in artificial and natural learning paradigms.\n        △ Less",
        "authors":"['Mete Erdogan', 'Cengiz Pehlevan', 'Alper T. Erdogan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sub-optimality of the Separation Principle for Quadratic Control from Bilinear Observations",
        "abstract":"We consider the problem of controlling a linear dynamical system from bilinear observations with minimal quadratic cost. Despite the similarity of this problem to standard linear quadratic Gaussian (LQG) control, we show that when the observation model is bilinear, neither does the Separation Principle hold, nor is the optimal controller affine in the estimated state. Moreover, the cost-to-go is non-convex in the control input. Hence, finding an analytical expression for the optimal feedback controller is difficult in general. Under certain settings, we show that the standard LQG controller locally maximizes the cost instead of minimizing it. Furthermore, the optimal controllers (derived analytically) are not unique and are nonlinear in the estimated state. We also introduce a notion of input-dependent observability and derive conditions under which the Kalman filter covariance remains bounded. We illustrate our theoretical results through numerical experiments in multiple synthetic settings.\n        △ Less",
        "authors":"['Yahya Sattar', 'Sunmook Choi', 'Yassir Jedra', 'Maryam Fazel', 'Sarah Dean']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Normalizing Flow Regression for Bayesian Inference with Offline Likelihood Evaluations",
        "abstract":"Bayesian inference with computationally expensive likelihood evaluations remains a significant challenge in many scientific domains. We propose normalizing flow regression (NFR), a novel offline inference method for approximating posterior distributions. Unlike traditional surrogate approaches that require additional sampling or inference steps, NFR directly yields a tractable posterior approximation through regression on existing log-density evaluations. We introduce training techniques specifically for flow regression, such as tailored priors and likelihood functions, to achieve robust posterior and model evidence estimation. We demonstrate NFR's effectiveness on synthetic benchmarks and real-world applications from neuroscience and biology, showing superior or comparable performance to existing methods. NFR represents a promising approach for Bayesian inference when standard methods are computationally prohibitive or existing model evaluations can be recycled.\n        △ Less",
        "authors":"['Chengkun Li', 'Bobby Huggins', 'Petrus Mikkola', 'Luigi Acerbi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
        "abstract":"There is growing interest in hypothesis generation with large language models (LLMs). However, fundamental questions remain: what makes a good hypothesis, and how can we systematically evaluate methods for hypothesis generation? To address this, we introduce HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. We evaluate four state-of-the-art LLMs combined with six existing hypothesis-generation methods. Overall, our results suggest that existing methods are capable of discovering valid and novel patterns in the data. However, the results from synthetic datasets indicate that there is still significant room for improvement, as current hypothesis generation methods do not fully uncover all relevant or meaningful patterns. Specifically, in synthetic settings, as task difficulty increases, performance significantly drops, with best models and methods only recovering 38.8% of the ground-truth hypotheses. These findings highlight challenges in hypothesis generation and demonstrate that HypoBench serves as a valuable resource for improving AI systems designed to assist scientific discovery.\n        △ Less",
        "authors":"['Haokun Liu', 'Sicong Huang', 'Jingyu Hu', 'Yangqiaoyu Zhou', 'Chenhao Tan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation",
        "abstract":"Evaluating autonomous vehicles with controllability enables scalable testing in counterfactual or structured settings, enhancing both efficiency and safety. We introduce LangTraj, a language-conditioned scene-diffusion model that simulates the joint behavior of all agents in traffic scenarios. By conditioning on natural language inputs, LangTraj provides flexible and intuitive control over interactive behaviors, generating nuanced and realistic scenarios. Unlike prior approaches that depend on domain-specific guidance functions, LangTraj incorporates language conditioning during training, facilitating more intuitive traffic simulation control. We propose a novel closed-loop training strategy for diffusion models, explicitly tailored to enhance stability and realism during closed-loop simulation. To support language-conditioned simulation, we develop Inter-Drive, a large-scale dataset with diverse and interactive labels for training language-conditioned diffusion models. Our dataset is built upon a scalable pipeline for annotating agent-agent interactions and single-agent behaviors, ensuring rich and varied supervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates strong performance in realism, language controllability, and language-conditioned safety-critical simulation, establishing a new paradigm for flexible and scalable autonomous vehicle testing.\n        △ Less",
        "authors":"['Wei-Jer Chang', 'Wei Zhan', 'Masayoshi Tomizuka', 'Manmohan Chandraker', 'Francesco Pittaluga']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Strengthening Anomaly Awareness",
        "abstract":"We present a refined version of the Anomaly Awareness framework for enhancing unsupervised anomaly detection. Our approach introduces minimal supervision into Variational Autoencoders (VAEs) through a two-stage training strategy: the model is first trained in an unsupervised manner on background data, and then fine-tuned using a small sample of labeled anomalies to encourage larger reconstruction errors for anomalous samples.\n  We validate the method across diverse domains, including the MNIST dataset with synthetic anomalies, network intrusion data from the CICIDS benchmark, collider physics data from the LHCO2020 dataset, and simulated events from the Standard Model Effective Field Theory (SMEFT). The latter provides a realistic example of subtle kinematic deviations in Higgs boson production. In all cases, the model demonstrates improved sensitivity to unseen anomalies, achieving better separation between normal and anomalous samples. These results indicate that even limited anomaly information, when incorporated through targeted fine-tuning, can substantially improve the generalization and performance of unsupervised models for anomaly detection.\n        △ Less",
        "authors":"['Adam Banda', 'Charanjit K. Khosa', 'Veronica Sanz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FACT: Foundation Model for Assessing Cancer Tissue Margins with Mass Spectrometry",
        "abstract":"Purpose: Accurately classifying tissue margins during cancer surgeries is crucial for ensuring complete tumor removal. Rapid Evaporative Ionization Mass Spectrometry (REIMS), a tool for real-time intraoperative margin assessment, generates spectra that require machine learning models to support clinical decision-making. However, the scarcity of labeled data in surgical contexts presents a significant challenge. This study is the first to develop a foundation model tailored specifically for REIMS data, addressing this limitation and advancing real-time surgical margin assessment. Methods: We propose FACT, a Foundation model for Assessing Cancer Tissue margins. FACT is an adaptation of a foundation model originally designed for text-audio association, pretrained using our proposed supervised contrastive approach based on triplet loss. An ablation study is performed to compare our proposed model against other models and pretraining methods. Results: Our proposed model significantly improves the classification performance, achieving state-of-the-art performance with an AUROC of $82.4\\% \\pm 0.8$. The results demonstrate the advantage of our proposed pretraining method and selected backbone over the self-supervised and semi-supervised baselines and alternative models. Conclusion: Our findings demonstrate that foundation models, adapted and pretrained using our novel approach, can effectively classify REIMS data even with limited labeled examples. This highlights the viability of foundation models for enhancing real-time surgical margin assessment, particularly in data-scarce clinical environments.\n        △ Less",
        "authors":"['Mohammad Farahmand', 'Amoon Jamzad', 'Fahimeh Fooladgar', 'Laura Connolly', 'Martin Kaufmann', 'Kevin Yi Mi Ren', 'John Rudan', 'Doug McKay', 'Gabor Fichtinger', 'Parvin Mousavi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FEAT: Free energy Estimators with Adaptive Transport",
        "abstract":"We present Free energy Estimators with Adaptive Transport (FEAT), a novel framework for free energy estimation -- a critical challenge across scientific domains. FEAT leverages learned transports implemented via stochastic interpolants and provides consistent, minimum-variance estimators based on escorted Jarzynski equality and controlled Crooks theorem, alongside variational upper and lower bounds on free energy differences. Unifying equilibrium and non-equilibrium methods under a single theoretical framework, FEAT establishes a principled foundation for neural free energy calculations. Experimental validation on toy examples, molecular simulations, and quantum field theory demonstrates improvements over existing learning-based methods.\n        △ Less",
        "authors":"['Jiajun He', 'Yuanqi Du', 'Francisco Vargas', 'Yuanqing Wang', 'Carla P. Gomes', 'José Miguel Hernández-Lobato', 'Eric Vanden-Eijnden']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models",
        "abstract":"Neural Networks (NNs) trained through supervised learning struggle with managing edge-case scenarios common in real-world driving due to the intractability of exhaustive datasets covering all edge-cases, making knowledge-driven approaches, akin to how humans intuitively detect unexpected driving behavior, a suitable complement to data-driven methods. This work proposes a hybrid architecture combining low-level Model Predictive Controller (MPC) with locally deployed Large Language Models (LLMs) to enhance decision-making and Human Machine Interaction (HMI). The DecisionxLLM module evaluates robotic state information against natural language instructions to ensure adherence to desired driving behavior. The MPCxLLM module then adjusts MPC parameters based on LLM-generated insights, achieving control adaptability while preserving the safety and constraint guarantees of traditional MPC systems. Further, to enable efficient on-board deployment and to eliminate dependency on cloud connectivity, we shift processing to the on-board computing platform: We propose an approach that exploits Retrieval Augmented Generation (RAG), Low Rank Adaptation (LoRA) fine-tuning, and quantization. Experimental results demonstrate that these enhancements yield significant improvements in reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%, and up to 10.5x increase in computational efficiency (tokens\/s), validating the proposed framework's practicality for real-time deployment even on down-scaled robotic platforms. This work bridges high-level decision-making with low-level control adaptability, offering a synergistic framework for knowledge-driven and adaptive Autonomous Driving Systems (ADS).\n        △ Less",
        "authors":"['Nicolas Baumann', 'Cheng Hu', 'Paviththiren Sivasothilingam', 'Haotong Qin', 'Lei Xie', 'Michele Magno', 'Luca Benini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-output Classification Framework and Frequency Layer Normalization for Compound Fault Diagnosis in Motor",
        "abstract":"This work introduces a multi-output classification (MOC) framework designed for domain adaptation in fault diagnosis, particularly under partially labeled (PL) target domain scenarios and compound fault conditions in rotating machinery. Unlike traditional multi-class classification (MCC) methods that treat each fault combination as a distinct class, the proposed approach independently estimates the severity of each fault type, improving both interpretability and diagnostic accuracy. The model incorporates multi-kernel maximum mean discrepancy (MK-MMD) and entropy minimization (EM) losses to facilitate feature transfer from the source to the target domain. In addition, frequency layer normalization (FLN) is applied to preserve structural properties in the frequency domain, which are strongly influenced by system dynamics and are often stationary with respect to changes in rpm. Evaluations across six domain adaptation cases with PL data demonstrate that MOC outperforms baseline models in macro F1 score. Moreover, MOC consistently achieves better classification performance for individual fault types, and FLN shows superior adaptability compared to other normalization techniques.\n        △ Less",
        "authors":"['Wonjun Yi', 'Yong-Hwa Park']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learned enclosure method for experimental EIT data",
        "abstract":"Electrical impedance tomography (EIT) is a non-invasive imaging method with diverse applications, including medical imaging and non-destructive testing. The inverse problem of reconstructing internal electrical conductivity from boundary measurements is nonlinear and highly ill-posed, making it difficult to solve accurately. In recent years, there has been growing interest in combining analytical methods with machine learning to solve inverse problems. In this paper, we propose a method for estimating the convex hull of inclusions from boundary measurements by combining the enclosure method proposed by Ikehata with neural networks. We demonstrate its performance using experimental data. Compared to the classical enclosure method with least squares fitting, the learned convex hull achieves superior performance on both simulated and experimental data.\n        △ Less",
        "authors":"['Sara Sippola', 'Siiri Rautio', 'Andreas Hauptmann', 'Takanori Ide', 'Samuli Siltanen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs",
        "abstract":"The rise of reinforcement learning (RL) in critical real-world applications demands a fundamental rethinking of privacy in AI systems. Traditional privacy frameworks, designed to protect isolated data points, fall short for sequential decision-making systems where sensitive information emerges from temporal patterns, behavioral strategies, and collaborative dynamics. Modern RL paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in large language models (LLMs), exacerbate these challenges by introducing complex, interactive, and context-dependent learning environments that traditional methods do not address. In this position paper, we argue for a new privacy paradigm built on four core principles: multi-scale protection, behavioral pattern protection, collaborative privacy preservation, and context-aware adaptation. These principles expose inherent tensions between privacy, utility, and interpretability that must be navigated as RL systems become more pervasive in high-stakes domains like healthcare, autonomous vehicles, and decision support systems powered by LLMs. To tackle these challenges, we call for the development of new theoretical frameworks, practical mechanisms, and rigorous evaluation methodologies that collectively enable effective privacy protection in sequential decision-making systems.\n        △ Less",
        "authors":"['Flint Xiaofeng Fan', 'Cheston Tan', 'Roger Wattenhofer', 'Yew-Soon Ong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems",
        "abstract":"In various networks and mobile applications, users are highly susceptible to attribute inference attacks, with particularly prevalent occurrences in recommender systems. Attackers exploit partially exposed user profiles in recommendation models, such as user embeddings, to infer private attributes of target users, such as gender and political views. The goal of defenders is to mitigate the effectiveness of these attacks while maintaining recommendation performance. Most existing defense methods, such as differential privacy and attribute unlearning, focus on post-training settings, which limits their capability of utilizing training data to preserve recommendation performance. Although adversarial training extends defenses to in-training settings, it often struggles with convergence due to unstable training processes. In this paper, we propose RAID, an in-training defense method against attribute inference attacks in recommender systems. In addition to the recommendation objective, we define a defensive objective to ensure that the distribution of protected attributes becomes independent of class labels, making users indistinguishable from attribute inference attacks. Specifically, this defensive objective aims to solve a constrained Wasserstein barycenter problem to identify the centroid distribution that makes the attribute indistinguishable while complying with recommendation performance constraints. To optimize our proposed objective, we use optimal transport to align users with the centroid distribution. We conduct extensive experiments on four real-world datasets to evaluate RAID. The experimental results validate the effectiveness of RAID and demonstrate its significant superiority over existing methods in multiple aspects.\n        △ Less",
        "authors":"['Xiaohua Feng', 'Yuyuan Li', 'Fengyuan Yu', 'Ke Xiong', 'Junjie Fang', 'Li Zhang', 'Tianyu Du', 'Chaochao Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reward Distance Comparisons Under Transition Sparsity",
        "abstract":"Reward comparisons are vital for evaluating differences in agent behaviors induced by a set of reward functions. Most conventional techniques utilize the input reward functions to learn optimized policies, which are then used to compare agent behaviors. However, learning these policies can be computationally expensive and can also raise safety concerns. Direct reward comparison techniques obviate policy learning but suffer from transition sparsity, where only a small subset of transitions are sampled due to data collection challenges and feasibility constraints. Existing state-of-the-art direct reward comparison methods are ill-suited for these sparse conditions since they require high transition coverage, where the majority of transitions from a given coverage distribution are sampled. When this requirement is not satisfied, a distribution mismatch between sampled and expected transitions can occur, leading to significant errors. This paper introduces the Sparsity Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need for high transition coverage by accommodating diverse sample distributions, which are common under transition sparsity. We provide theoretical justification for SRRD's robustness and conduct experiments to demonstrate its practical efficacy across multiple domains.\n        △ Less",
        "authors":"['Clement Nyanhongo', 'Bruno Miranda Henrique', 'Eugene Santos']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning",
        "abstract":"More than the adherence to specific traffic regulations, driving culture touches upon a more implicit part - an informal, conventional, collective behavioral pattern followed by drivers - that varies across countries, regions, and even cities. Such cultural divergence has become one of the biggest challenges in deploying autonomous vehicles (AVs) across diverse regions today. The current emergence of data-driven methods has shown a potential solution to enable culture-compatible driving through learning from data, but what if some underdeveloped regions cannot provide sufficient local data to inform driving culture? This issue is particularly significant for a broader global AV market. Here, we propose a cross-cultural deployment scheme for AVs, called data-light inverse reinforcement learning, designed to re-calibrate culture-specific AVs and assimilate them into other cultures. First, we report the divergence in driving cultures through a comprehensive comparative analysis of naturalistic driving datasets on highways from three countries: Germany, China, and the USA. Then, we demonstrate the effectiveness of our scheme by testing the expeditious cross-cultural deployment across these three countries, with cumulative testing mileage of over 56084 km. The performance is particularly advantageous when cross-cultural deployment is carried out without affluent local data. Results show that we can reduce the dependence on local data by a margin of 98.67% at best. This study is expected to bring a broader, fairer AV global market, particularly in those regions that lack enough local data to develop culture-compatible AVs.\n        △ Less",
        "authors":"['Hongliang Lu', 'Shuqi Shen', 'Junjie Yang', 'Chao Lu', 'Xinhu Zheng', 'Hai Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets",
        "abstract":"As machine learning models are increasingly used in educational settings, from detecting at-risk students to predicting student performance, algorithmic bias and its potential impacts on students raise critical concerns about algorithmic fairness. Although group fairness is widely explored in education, works on individual fairness in a causal context are understudied, especially on counterfactual fairness. This paper explores the notion of counterfactual fairness for educational data by conducting counterfactual fairness analysis of machine learning models on benchmark educational datasets. We demonstrate that counterfactual fairness provides meaningful insight into the causality of sensitive attributes and causal-based individual fairness in education.\n        △ Less",
        "authors":"['Woojin Kim', 'Hyeoncheol Kim']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Timing Analysis Agent: Autonomous Multi-Corner Multi-Mode (MCMM) Timing Debugging with Timing Debug Relation Graph",
        "abstract":"Timing analysis is an essential and demanding verification method for Very Large Scale Integrated (VLSI) circuit design and optimization. In addition, it also serves as the cornerstone of the final sign-off, determining whether the chip is ready to be sent to the semiconductor foundry for fabrication. Recently, as the technology advance relentlessly, smaller metal pitches and the increasing number of devices have led to greater challenges and longer turn-around-time for experienced human designers to debug timing issues from the Multi-Corner Multi-Mode (MCMM) timing reports. As a result, an efficient and intelligent methodology is highly necessary and essential for debugging timing issues and reduce the turnaround times. Recently, Large Language Models (LLMs) have shown great promise across various tasks in language understanding and interactive decision-making, incorporating reasoning and actions. In this work, we propose a timing analysis agent, that is empowered by multi-LLMs task solving, and incorporates a novel hierarchical planning and solving flow to automate the analysis of timing reports from commercial tool. In addition, we build a Timing Debug Relation Graph (TDRG) that connects the reports with the relationships of debug traces from experienced timing engineers. The timing analysis agent employs the novel Agentic Retrieval Augmented Generation (RAG) approach, that includes agent and coding to retrieve data accurately, on the developed TDRG. In our studies, the proposed timing analysis agent achieves an average 98% pass-rate on a single-report benchmark and a 90% pass-rate for multi-report benchmark from industrial designs, demonstrating its effectiveness and adaptability.\n        △ Less",
        "authors":"['Jatin Nainani', 'Chia-Tung Ho', 'Anirudh Dhurka', 'Haoxing Ren']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit",
        "abstract":"The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose an LLM-based AI agent for AMS circuit design to assist in the sizing process. By integrating LLMs with external circuit simulation tools and data analysis functions and employing prompt engineering strategies, the agent successfully optimized multiple circuits to achieve target performance metrics. We evaluated the performance of different LLMs to assess their applicability and optimization effectiveness across seven basic circuits, and selected the best-performing model Claude 3.5 Sonnet for further exploration on an operational amplifier, with complementary input stage and class AB output stage. This circuit was evaluated against nine performance metrics, and we conducted experiments under three distinct performance requirement groups. A success rate of up to 60% was achieved for reaching the target requirements. Overall, this work demonstrates the potential of LLMs to improve AMS circuit design.\n        △ Less",
        "authors":"['Chang Liu', 'Emmanuel A. Olowe', 'Danial Chitnis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Probabilistic Task Parameterization of Tool-Tissue Interaction via Sparse Landmarks Tracking in Robotic Surgery",
        "abstract":"Accurate modeling of tool-tissue interactions in robotic surgery requires precise tracking of deformable tissues and integration of surgical domain knowledge. Traditional methods rely on labor-intensive annotations or rigid assumptions, limiting flexibility. We propose a framework combining sparse keypoint tracking and probabilistic modeling that propagates expert-annotated landmarks across endoscopic frames, even with large tissue deformations. Clustered tissue keypoints enable dynamic local transformation construction via PCA, and tool poses, tracked similarly, are expressed relative to these frames. Embedding these into a Task-Parameterized Gaussian Mixture Model (TP-GMM) integrates data-driven observations with labeled clinical expertise, effectively predicting relative tool-tissue poses and enhancing visual understanding of robotic surgical motions directly from video data.\n        △ Less",
        "authors":"['Yiting Wang', 'Yunxin Fan', 'Fei Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Attention GhostUNet++: Enhanced Segmentation of Adipose Tissue and Liver in CT Images",
        "abstract":"Accurate segmentation of abdominal adipose tissue, including subcutaneous (SAT) and visceral adipose tissue (VAT), along with liver segmentation, is essential for understanding body composition and associated health risks such as type 2 diabetes and cardiovascular disease. This study proposes Attention GhostUNet++, a novel deep learning model incorporating Channel, Spatial, and Depth Attention mechanisms into the Ghost UNet++ bottleneck for automated, precise segmentation. Evaluated on the AATTCT-IDS and LiTS datasets, the model achieved Dice coefficients of 0.9430 for VAT, 0.9639 for SAT, and 0.9652 for liver segmentation, surpassing baseline models. Despite minor limitations in boundary detail segmentation, the proposed model significantly enhances feature refinement, contextual understanding, and computational efficiency, offering a robust solution for body composition analysis. The implementation of the proposed Attention GhostUNet++ model is available at:https:\/\/github.com\/MansoorHayat777\/Attention-GhostUNetPlusPlus.\n        △ Less",
        "authors":"['Mansoor Hayat', 'Supavadee Aramvith', 'Subrata Bhattacharjee', 'Nouman Ahmad']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines",
        "abstract":"Restricted kernel machines (RKMs) represent a versatile and powerful framework within the kernel machine family, leveraging conjugate feature duality to address a wide range of machine learning tasks, including classification, regression, and feature learning. However, their performance can degrade significantly in the presence of noise and outliers, which compromises robustness and predictive accuracy. In this paper, we propose a novel enhancement to the RKM framework by integrating a class-informed weighted function. This weighting mechanism dynamically adjusts the contribution of individual training points based on their proximity to class centers and class-specific characteristics, thereby mitigating the adverse effects of noisy and outlier data. By incorporating weighted conjugate feature duality and leveraging the Schur complement theorem, we introduce the class-informed restricted kernel machine (CI-RKM), a robust extension of the RKM designed to improve generalization and resilience to data imperfections. Experimental evaluations on benchmark datasets demonstrate that the proposed CI-RKM consistently outperforms existing baselines, achieving superior classification accuracy and enhanced robustness against noise and outliers. Our proposed method establishes a significant advancement in the development of kernel-based learning models, addressing a core challenge in the field.\n        △ Less",
        "authors":"['Ritik Mishra', 'Mushir Akhtar', 'M. Tanveer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MultiCore+TPU Accelerated Multi-Modal TinyML for Livestock Behaviour Recognition",
        "abstract":"The advancement of technology has revolutionised the agricultural industry, transitioning it from labour-intensive farming practices to automated, AI-powered management systems. In recent years, more intelligent livestock monitoring solutions have been proposed to enhance farming efficiency and productivity. This work presents a novel approach to animal activity recognition and movement tracking, leveraging tiny machine learning (TinyML) techniques, wireless communication framework, and microcontroller platforms to develop an efficient, cost-effective livestock sensing system. It collects and fuses accelerometer data and vision inputs to build a multi-modal network for three tasks: image classification, object detection, and behaviour recognition. The system is deployed and evaluated on commercial microcontrollers for real-time inference using embedded applications, demonstrating up to 270$\\times$ model size reduction, less than 80ms response latency, and on-par performance comparable to existing methods. The incorporation of the TinyML technique allows for seamless data transmission between devices, benefiting use cases in remote locations with poor Internet connectivity. This work delivers a robust, scalable IoT-edge livestock monitoring solution adaptable to diverse farming needs, offering flexibility for future extensions.\n        △ Less",
        "authors":"['Qianxue Zhang', 'Eiman Kanjo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Elucidating the Design Space of Multimodal Protein Language Models",
        "abstract":"Multimodal protein language models (PLMs) integrate sequence and token-based structural information, serving as a powerful foundation for protein modeling, generation, and design. However, the reliance on tokenizing 3D structures into discrete tokens causes substantial loss of fidelity about fine-grained structural details and correlations. In this paper, we systematically elucidate the design space of multimodal PLMs to overcome their limitations. We identify tokenization loss and inaccurate structure token predictions by the PLMs as major bottlenecks. To address these, our proposed design space covers improved generative modeling, structure-aware architectures and representation learning, and data exploration. Our advancements approach finer-grained supervision, demonstrating that token-based multimodal PLMs can achieve robust structural modeling. The effective design methods dramatically improve the structure generation diversity, and notably, folding abilities of our 650M model by reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B baselines and on par with the specialized folding models.\n        △ Less",
        "authors":"['Cheng-Yen Hsieh', 'Xinyou Wang', 'Daiheng Zhang', 'Dongyu Xue', 'Fei Ye', 'Shujian Huang', 'Zaixiang Zheng', 'Quanquan Gu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Clean Slate for Offline Reinforcement Learning",
        "abstract":"Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches within a single, comprehensive hyperparameter space, enabling algorithm development in a shared hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https:\/\/github.com\/EmptyJackson\/unifloral.\n        △ Less",
        "authors":"['Matthew Thomas Jackson', 'Uljad Berdica', 'Jarek Liesen', 'Shimon Whiteson', 'Jakob Nicolaus Foerster']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Early Impacts of M365 Copilot",
        "abstract":"Advances in generative AI have rapidly expanded the potential of computers to perform or assist in a wide array of tasks traditionally performed by humans. We analyze a large, real-world randomized experiment of over 6,000 workers at 56 firms to present some of the earliest evidence on how these technologies are changing the way knowledge workers do their jobs. We find substantial time savings on common core tasks across a wide range of industries and occupations: workers who make use of this technology spent half an hour less reading email each week and completed documents 12% faster. Despite the newness of the technology, nearly 40% of workers who were given access to the tool used it regularly in their work throughout the 6-month study.\n        △ Less",
        "authors":"['Eleanor Wiske Dillon', 'Sonia Jaffe', 'Sida Peng', 'Alexia Cambon']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TextArena",
        "abstract":"TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https:\/\/github.com\/LeonGuertler\/TextArena and https:\/\/www.textarena.ai\/.\n        △ Less",
        "authors":"['Leon Guertler', 'Bobby Cheng', 'Simon Yu', 'Bo Liu', 'Leshem Choshen', 'Cheston Tan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mamba-Based Ensemble learning for White Blood Cell Classification",
        "abstract":"White blood cell (WBC) classification assists in assessing immune health and diagnosing various diseases, yet manual classification is labor-intensive and prone to inconsistencies. Recent advancements in deep learning have shown promise over traditional methods; however, challenges such as data imbalance and the computational demands of modern technologies, such as Transformer-based models which do not scale well with input size, limit their practical application. This paper introduces a novel framework that leverages Mamba models integrated with ensemble learning to improve WBC classification. Mamba models, known for their linear complexity, provide a scalable alternative to Transformer-based approaches, making them suitable for deployment in resource-constrained environments. Additionally, we introduce a new WBC dataset, Chula-WBC-8, for benchmarking. Our approach not only validates the effectiveness of Mamba models in this domain but also demonstrates their potential to significantly enhance classification efficiency without compromising accuracy. The source code can be found at https:\/\/github.com\/LewisClifton\/Mamba-WBC-Classification.\n        △ Less",
        "authors":"['Lewis Clifton', 'Xin Tian', 'Duangdao Palasuwan', 'Phandee Watanaboonyongcharoen', 'Ponlapat Rojnuckarin', 'Nantheera Anantrasirichai']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Shifting Work Patterns with Generative AI",
        "abstract":"We present evidence on how generative AI changes the work patterns of knowledge workers using data from a 6-month-long, cross-industry, randomized field experiment. Half of the 6,000 workers in the study received access to a generative AI tool integrated into the applications they already used for emails, document creation, and meetings. We find that access to the AI tool during the first year of its release primarily impacted behaviors that could be changed independently and not behaviors that required coordination to change: workers who used the tool spent 3 fewer hours, or 25% less time on email each week (intent to treat estimate is 1.4 hours) and seemed to complete documents moderately faster, but did not significantly change time spent in meetings.\n        △ Less",
        "authors":"['Eleanor Wiske Dillon', 'Sonia Jaffe', 'Nicole Immorlica', 'Christopher T. Stanton']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Out-of-Distribution Detection with Extended Logit Normalization",
        "abstract":"Out-of-distribution (OOD) detection is essential for the safe deployment of machine learning models. Recent advances have explored improved classification losses and representation learning strategies to enhance OOD detection. However, these methods are often tailored to specific post-hoc detection techniques, limiting their generalizability. In this work, we identify a critical issue in Logit Normalization (LogitNorm), which inhibits its effectiveness in improving certain post-hoc OOD detection methods. To address this, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel hyperparameter-free formulation that significantly benefits a wide range of post-hoc detection methods. By incorporating feature distance-awareness to LogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and in-distribution (ID) confidence calibration than its predecessor. Extensive experiments across standard benchmarks demonstrate that our approach outperforms state-of-the-art training-time methods in OOD detection while maintaining strong ID classification accuracy.\n        △ Less",
        "authors":"['Yifan Ding', 'Xixi Liu', 'Jonas Unger', 'Gabriel Eilertsen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Predicting Wave Dynamics using Deep Learning with Multistep Integration Inspired Attention and Physics-Based Loss Decomposition",
        "abstract":"In this paper, we present a physics-based deep learning framework for data-driven prediction of wave propagation in fluid media. The proposed approach, termed Multistep Integration-Inspired Attention (MI2A), combines a denoising-based convolutional autoencoder for reduced latent representation with an attention-based recurrent neural network with long-short-term memory cells for time evolution of reduced coordinates. This proposed architecture draws inspiration from classical linear multistep methods to enhance stability and long-horizon accuracy in latent-time integration. Despite the efficiency of hybrid neural architectures in modeling wave dynamics, autoregressive predictions are often prone to accumulating phase and amplitude errors over time. To mitigate this issue within the MI2A framework, we introduce a novel loss decomposition strategy that explicitly separates the training loss function into distinct phase and amplitude components. We assess the performance of MI2A against two baseline reduced-order models trained with standard mean-squared error loss: a sequence-to-sequence recurrent neural network and a variant using Luong-style attention. To demonstrate the effectiveness of the MI2A model, we consider three benchmark wave propagation problems of increasing complexity, namely one-dimensional linear convection, the nonlinear viscous Burgers equation, and the two-dimensional Saint-Venant shallow water system. Our results demonstrate that the MI2A framework significantly improves the accuracy and stability of long-term predictions, accurately preserving wave amplitude and phase characteristics. Compared to the standard long-short term memory and attention-based models, MI2A-based deep learning exhibits superior generalization and temporal accuracy, making it a promising tool for real-time wave modeling.\n        △ Less",
        "authors":"['Indu Kant Deo', 'Rajeev K. Jaiman']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models",
        "abstract":"Masculine defaults are widely recognized as a significant type of gender bias, but they are often unseen as they are under-researched. Masculine defaults involve three key parts: (i) the cultural context, (ii) the masculine characteristics or behaviors, and (iii) the reward for, or simply acceptance of, those masculine characteristics or behaviors. In this work, we study discourse-based masculine defaults, and propose a twofold framework for (i) the large-scale discovery and analysis of gendered discourse words in spoken content via our Gendered Discourse Correlation Framework (GDCF); and (ii) the measurement of the gender bias associated with these gendered discourse words in LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus our study on podcasts, a popular and growing form of social media, analyzing 15,117 podcast episodes. We analyze correlations between gender and discourse words -- discovered via LDA and BERTopic -- to automatically form gendered discourse word lists. We then study the prevalence of these gendered discourse words in domain-specific contexts, and find that gendered discourse-based masculine defaults exist in the domains of business, technology\/politics, and video games. Next, we study the representation of these gendered discourse words from a state-of-the-art LLM embedding model from OpenAI, and find that the masculine discourse words have a more stable and robust representation than the feminine discourse words, which may result in better system performance on downstream tasks for men. Hence, men are rewarded for their discourse patterns with better system performance by one of the state-of-the-art language models -- and this embedding disparity is a representational harm and a masculine default.\n        △ Less",
        "authors":"['Maria Teleki', 'Xiangjue Dong', 'Haoran Liu', 'James Caverlee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Dual-Space Framework for General Knowledge Distillation of Large Language Models",
        "abstract":"Knowledge distillation (KD) is a promising solution to compress large language models (LLMs) by transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output distributions of the teacher model and the student model to transfer more information. However, we reveal that the current white-box KD framework exhibits two limitations: a) bridging probability distributions from different output spaces will limit the similarity between the teacher model and the student model; b) this framework cannot be applied to LLMs with different vocabularies. One of the root causes for these limitations is that the distributions from the teacher and the student for KD are output by different prediction heads, which yield distributions in different output spaces and dimensions. Therefore, in this paper, we propose a dual-space knowledge distillation (DSKD) framework that unifies the prediction heads of the teacher and the student models for KD. Specifically, we first introduce two projectors with ideal initialization to project the teacher\/student hidden states into the student\/teacher representation spaces. After this, the hidden states from different models can share the same head and unify the output spaces of the distributions. Furthermore, we develop an exact token alignment (ETA) algorithm to align the same tokens in two differently-tokenized sequences. Based on the above, our DSKD framework is a general KD framework that supports both off-policy and on-policy KD, and KD between any two LLMs regardless of their vocabularies. Extensive experiments on instruction-following, mathematical reasoning, and code generation benchmarks show that DSKD significantly outperforms existing methods based on the current white-box KD framework and surpasses other cross-tokenizer KD methods for LLMs with different vocabularies.\n        △ Less",
        "authors":"['Xue Zhang', 'Songming Zhang', 'Yunlong Liang', 'Fandong Meng', 'Yufeng Chen', 'Jinan Xu', 'Jie Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HeatSense: Intelligent Thermal Anomaly Detection for Securing NoC-Enabled MPSoCs",
        "abstract":"Multi-Processor System-on-Chips (MPSoCs) are highly vulnerable to thermal attacks that manipulate dynamic thermal management systems. To counter this, we propose an adaptive real-time monitoring mechanism that detects abnormal thermal patterns in chip tiles. Our design space exploration helped identify key thermal features for an efficient anomaly detection module to be implemented at routers of network-enabled MPSoCs. To minimize hardware overhead, we employ weighted moving average (WMA) calculations and bit-shift operations, ensuring a lightweight yet effective implementation. By defining a spectrum of abnormal behaviors, our system successfully detects and mitigates malicious temperature fluctuations, reducing severe cases from 3.00°C to 1.9°C. The anomaly detection module achieves up to 82% of accuracy in detecting thermal attacks, which is only 10-15% less than top-performing machine learning (ML) models like Random Forest. However, our approach reduces hardware usage by up to 75% for logic resources and 100% for specialized resources, making it significantly more efficient than ML-based solutions. This method provides a practical, low-cost solution for resource-constrained environments, ensuring resilience against thermal attacks while maintaining system performance.\n        △ Less",
        "authors":"['Mahdi Hasanzadeh', 'Kasem Khalil', 'Cynthia Sturton', 'Ahmad Patooghy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Embodied World Models Emerge from Navigational Task in Open-Ended Environments",
        "abstract":"Understanding how artificial systems can develop spatial awareness and reasoning has long been a challenge in AI research. Traditional models often rely on passive observation, but embodied cognition theory suggests that deeper understanding emerges from active interaction with the environment. This study investigates whether neural networks can autonomously internalize spatial concepts through interaction, focusing on planar navigation tasks. Using Gated Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we show that agents can learn to encode spatial properties like direction, distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS) to model the agent-environment interaction as a closed dynamical system, revealing stable limit cycles that correspond to optimal navigation strategies. Ridge Representation allows us to map navigation paths into a fixed-dimensional behavioral space, enabling comparison with neural states. Canonical Correlation Analysis (CCA) confirms strong alignment between these representations, suggesting that the agent's neural states actively encode spatial knowledge. Intervention experiments further show that specific neural dimensions are causally linked to navigation performance. This work provides an approach to bridging the gap between action and perception in AI, offering new insights into building adaptive, interpretable models that can generalize across complex environments. The causal validation of neural representations also opens new avenues for understanding and controlling the internal mechanisms of AI systems, pushing the boundaries of how machines learn and reason in dynamic, real-world scenarios.\n        △ Less",
        "authors":"['Li Jin', 'Liu Jia']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robustness and sex differences in skin cancer detection: logistic regression vs CNNs",
        "abstract":"Deep learning has been reported to achieve high performances in the detection of skin cancer, yet many challenges regarding the reproducibility of results and biases remain. This study is a replication (different data, same analysis) of a study on Alzheimer's disease [28] which studied robustness of logistic regression (LR) and convolutional neural networks (CNN) across patient sexes. We explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset with LR trained on handcrafted features reflecting dermatological guidelines (ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We evaluate these models in alignment with [28]: across multiple training datasets with varied sex composition to determine their robustness. Our results show that both the LR and the CNN were robust to the sex distributions, but the results also revealed that the CNN had a significantly higher accuracy (ACC) and area under the receiver operating characteristics (AUROC) for male patients than for female patients. We hope these findings to contribute to the growing field of investigating potential bias in popular medical machine learning methods. The data and relevant scripts to reproduce our results can be found in our Github.\n        △ Less",
        "authors":"['Nikolette Pedersen', 'Regitze Sydendal', 'Andreas Wulff', 'Ralf Raumanns', 'Eike Petersen', 'Veronika Cheplygina']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Measures of Variability for Risk-averse Policy Gradient",
        "abstract":"Risk-averse reinforcement learning (RARL) is critical for decision-making under uncertainty, which is especially valuable in high-stake applications. However, most existing works focus on risk measures, e.g., conditional value-at-risk (CVaR), while measures of variability remain underexplored. In this paper, we comprehensively study nine common measures of variability, namely Variance, Gini Deviation, Mean Deviation, Mean-Median Deviation, Standard Deviation, Inter-Quantile Range, CVaR Deviation, Semi_Variance, and Semi_Standard Deviation. Among them, four metrics have not been previously studied in RARL. We derive policy gradient formulas for these unstudied metrics, improve gradient estimation for Gini Deviation, analyze their gradient properties, and incorporate them with the REINFORCE and PPO frameworks to penalize the dispersion of returns.\n  Our empirical study reveals that variance-based metrics lead to unstable policy updates. In contrast, CVaR Deviation and Gini Deviation show consistent performance across different randomness and evaluation domains, achieving high returns while effectively learning risk-averse policies. Mean Deviation and Semi_Standard Deviation are also competitive across different scenarios. This work provides a comprehensive overview of variability measures in RARL, offering practical insights for risk-aware decision-making and guiding future research on risk metrics and RARL algorithms.\n        △ Less",
        "authors":"['Yudong Luo', 'Yangchen Pan', 'Jiaqi Tan', 'Pascal Poupart']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MLPs and KANs for data-driven learning in physical problems: A performance comparison",
        "abstract":"There is increasing interest in solving partial differential equations (PDEs) by casting them as machine learning problems. Recently, there has been a spike in exploring Kolmogorov-Arnold Networks (KANs) as an alternative to traditional neural networks represented by Multi-Layer Perceptrons (MLPs). While showing promise, their performance advantages in physics-based problems remain largely unexplored. Several critical questions persist: Can KANs capture complex physical dynamics and under what conditions might they outperform traditional architectures? In this work, we present a comparative study of KANs and MLPs for learning physical systems governed by PDEs. We assess their performance when applied in deep operator networks (DeepONet) and graph network-based simulators (GNS), and test them on physical problems that vary significantly in scale and complexity. Drawing inspiration from the Kolmogorov Representation Theorem, we examine the behavior of KANs and MLPs across shallow and deep network architectures. Our results reveal that although KANs do not consistently outperform MLPs when configured as deep neural networks, they demonstrate superior expressiveness in shallow network settings, significantly outpacing MLPs in accuracy over our test cases. This suggests that KANs are a promising choice, offering a balance of efficiency and accuracy in applications involving physical systems.\n        △ Less",
        "authors":"['Raghav Pant', 'Sikan Li', 'Xingjian Li', 'Hassan Iqbal', 'Krishna Kumar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Property Inheritance for Subtensors in Tensor Train Decompositions",
        "abstract":"Tensor dimensionality reduction is one of the fundamental tools for modern data science. To address the high computational overhead, fiber-wise sampled subtensors that preserve the original tensor rank are often used in designing efficient and scalable tensor dimensionality reduction. However, the theory of property inheritance for subtensors is still underdevelopment, that is, how the essential properties of the original tensor will be passed to its subtensors. This paper theoretically studies the property inheritance of the two key tensor properties, namely incoherence and condition number, under the tensor train setting. We also show how tensor train rank is preserved through fiber-wise sampling. The key parameters introduced in theorems are numerically evaluated under various settings. The results show that the properties of interest can be well preserved to the subtensors formed via fiber-wise sampling. Overall, this paper provides several handy analytic tools for developing efficient tensor analysis\n        △ Less",
        "authors":"['HanQin Cai', 'Longxiu Huang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DataDecide: How to Predict Best Pretraining Data with Small Experiments",
        "abstract":"Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.\n        △ Less",
        "authors":"['Ian Magnusson', 'Nguyen Tai', 'Ben Bogin', 'David Heineman', 'Jena D. Hwang', 'Luca Soldaini', 'Akshita Bhagia', 'Jiacheng Liu', 'Dirk Groeneveld', 'Oyvind Tafjord', 'Noah A. Smith', 'Pang Wei Koh', 'Jesse Dodge']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Trajectory Encoding Temporal Graph Networks",
        "abstract":"Temporal Graph Networks (TGNs) have demonstrated significant success in dynamic graph tasks such as link prediction and node classification. Both tasks comprise transductive settings, where the model predicts links among known nodes, and in inductive settings, where it generalises learned patterns to previously unseen nodes. Existing TGN designs face a dilemma under these dual scenarios. Anonymous TGNs, which rely solely on temporal and structural information, offer strong inductive generalisation but struggle to distinguish known nodes. In contrast, non-anonymous TGNs leverage node features to excel in transductive tasks yet fail to adapt to new nodes. To address this challenge, we propose Trajectory Encoding TGN (TETGN). Our approach introduces automatically expandable node identifiers (IDs) as learnable temporal positional features and performs message passing over these IDs to capture each node's historical context. By integrating this trajectory-aware module with a standard TGN using multi-head attention, TETGN effectively balances transductive accuracy with inductive generalisation. Experimental results on three real-world datasets show that TETGN significantly outperforms strong baselines on both link prediction and node classification tasks, demonstrating its ability to unify the advantages of anonymous and non-anonymous models for dynamic graph learning.\n        △ Less",
        "authors":"['Jiafeng Xiong', 'Rizos Sakellariou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and Neural Operators with Domain Decomposition",
        "abstract":"Numerical solvers for partial differential equations (PDEs) face challenges balancing computational cost and accuracy, especially in multiscale and dynamic systems. Neural operators can significantly speed up simulations; however, they often face challenges such as error accumulation and limited generalization in multiphysics problems. This work introduces a novel hybrid framework that integrates physics-informed DeepONet with FEM through domain decomposition. The core innovation lies in adaptively coupling FEM and DeepONet subdomains via a Schwarz alternating method. This methodology strategically allocates computationally demanding regions to a pre-trained Deep Operator Network, while the remaining computational domain is solved through FEM. To address dynamic systems, we integrate the Newmark time-stepping scheme directly into the DeepONet, significantly mitigating error accumulation in long-term simulations. Furthermore, an adaptive subdomain evolution enables the ML-resolved region to expand dynamically, capturing emerging fine-scale features without remeshing. The framework's efficacy has been validated across a range of solid mechanics problems, including static, quasi-static, and dynamic regimes, demonstrating accelerated convergence rates (up to 20% improvement compared to FE-FE approaches), while preserving solution fidelity with error < 1%. Our case studies show that our proposed hybrid solver: (1) maintains solution continuity across subdomain interfaces, (2) reduces computational costs by eliminating fine mesh requirements, (3) mitigates error accumulation in time-dependent simulations, and (4) enables automatic adaptation to evolving physical phenomena. This work bridges the gap between numerical methods and AI-driven surrogates, offering a scalable pathway for high-fidelity simulations in engineering and scientific applications.\n        △ Less",
        "authors":"['Wei Wang', 'Maryam Hakimzadeh', 'Haihui Ruan', 'Somdatta Goswami']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution",
        "abstract":"Open Large Language Models (OLLMs) are increasingly leveraged in generative AI applications, posing new challenges for detecting their outputs. We propose OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. OpenTuringBench focuses on a representative set of OLLMs, and features a number of challenging evaluation tasks, including human\/machine-manipulated texts, out-of-domain texts, and texts from previously unseen models. We also provide OTBDetector, a contrastive learning framework to detect and attribute OLLM-based machine-generated texts. Results highlight the relevance and varying degrees of difficulty of the OpenTuringBench tasks, with our detector achieving remarkable capabilities across the various tasks and outperforming most existing detectors. Resources are available on the OpenTuringBench Hugging Face repository at https:\/\/huggingface.co\/datasets\/MLNTeam-Unical\/OpenTuringBench\n        △ Less",
        "authors":"['Lucio La Cava', 'Andrea Tagarelli']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Teaching Large Language Models to Reason through Learning and Forgetting",
        "abstract":"Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it using both successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. While fine-tuning the model with these data might seem straightforward, we identify a critical issue: the model's search capability tends to degrade rapidly if fine-tuning is performed naively. We show that this degradation can be substantially mitigated by employing a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown mathematical reasoning benchmarks show that our approach not only outperforms both standard fine-tuning and inference-time search baselines but also significantly reduces inference time by 180$\\times$.\n        △ Less",
        "authors":"['Tianwei Ni', 'Allen Nie', 'Sapana Chaudhary', 'Yao Liu', 'Huzefa Rangwala', 'Rasool Fakoor']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Adaptive Dropout Approach for High-Dimensional Bayesian Optimization",
        "abstract":"Bayesian optimization (BO) is a widely used algorithm for solving expensive black-box optimization problems. However, its performance decreases significantly on high-dimensional problems due to the inherent high-dimensionality of the acquisition function. In the proposed algorithm, we adaptively dropout the variables of the acquisition function along the iterations. By gradually reducing the dimension of the acquisition function, the proposed approach has less and less difficulty to optimize the acquisition function. Numerical experiments demonstrate that AdaDropout effectively tackle high-dimensional challenges and improve solution quality where standard Bayesian optimization methods often struggle. Moreover, it achieves superior results when compared with state-of-the-art high-dimensional Bayesian optimization approaches. This work provides a simple yet efficient solution for high-dimensional expensive optimization.\n        △ Less",
        "authors":"['Jundi Huang', 'Dawei Zhan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Erzeugunsgrad, VC-Dimension and Neural Networks with rational activation function",
        "abstract":"The notion of Erzeugungsgrad was introduced by Joos Heintz in 1983 to bound the number of non-empty cells occurring after a process of quantifier elimination. We extend this notion and the combinatorial bounds of Theorem 2 in Heintz (1983) using the degree for constructible sets defined in Pardo-Sebastián (2022). We show that the Erzeugungsgrad is the key ingredient to connect affine Intersection Theory over algebraically closed fields and the VC-Theory of Computational Learning Theory for families of classifiers given by parameterized families of constructible sets. In particular, we prove that the VC-dimension and the Krull dimension are linearly related up to logarithmic factors based on Intersection Theory. Using this relation, we study the density of correct test sequences in evasive varieties. We apply these ideas to analyze parameterized families of neural networks with rational activation function.\n        △ Less",
        "authors":"['Luis Miguel Pardo', 'Daniel Sebastián']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Interpretable Hybrid-Rule Temporal Point Processes",
        "abstract":"Temporal Point Processes (TPPs) are widely used for modeling event sequences in various medical domains, such as disease onset prediction, progression analysis, and clinical decision support. Although TPPs effectively capture temporal dynamics, their lack of interpretability remains a critical challenge. Recent advancements have introduced interpretable TPPs. However, these methods fail to incorporate numerical features, thereby limiting their ability to generate precise predictions. To address this issue, we propose Hybrid-Rule Temporal Point Processes (HRTPP), a novel framework that integrates temporal logic rules with numerical features, improving both interpretability and predictive accuracy in event modeling. HRTPP comprises three key components: basic intensity for intrinsic event likelihood, rule-based intensity for structured temporal dependencies, and numerical feature intensity for dynamic probability modulation. To effectively discover valid rules, we introduce a two-phase rule mining strategy with Bayesian optimization. To evaluate our method, we establish a multi-criteria assessment framework, incorporating rule validity, model fitting, and temporal predictive accuracy. Experimental results on real-world medical datasets demonstrate that HRTPP outperforms state-of-the-art interpretable TPPs in terms of predictive performance and clinical interpretability. In case studies, the rules extracted by HRTPP explain the disease progression, offering valuable contributions to medical diagnosis.\n        △ Less",
        "authors":"['Yunyang Cao', 'Juekai Lin', 'Hongye Wang', 'Wenhao Li', 'Bo Jin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce",
        "abstract":"Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.\n        △ Less",
        "authors":"['Wei Xiong', 'Jiarui Yao', 'Yuhui Xu', 'Bo Pang', 'Lei Wang', 'Doyen Sahoo', 'Junnan Li', 'Nan Jiang', 'Tong Zhang', 'Caiming Xiong', 'Hanze Dong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Looking beyond the next token",
        "abstract":"The structure of causal language model training assumes that each token can be accurately predicted from the previous context. This contrasts with humans' natural writing and reasoning process, where goals are typically known before the exact argument or phrasings. While this mismatch has been well studied in the literature, the working assumption has been that architectural changes are needed to address this mismatch. We argue that rearranging and processing the training data sequences can allow models to more accurately imitate the true data-generating process, and does not require any other changes to the architecture or training infrastructure. We demonstrate that this technique, Trelawney, and the inference algorithms derived from it allow us to improve performance on several key benchmarks that span planning, algorithmic reasoning, and story generation tasks. Finally, our method naturally enables the generation of long-term goals at no additional cost. We investigate how using the model's goal-generation capability can further improve planning and reasoning. Additionally, we believe Trelawney could potentially open doors to new capabilities beyond the current language modeling paradigm.\n        △ Less",
        "authors":"['Abitha Thankaraj', 'Yiding Jiang', 'J. Zico Kolter', 'Yonatan Bisk']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Code Reborn AI-Driven Legacy Systems Modernization from COBOL to Java",
        "abstract":"This study investigates AI-driven modernization of legacy COBOL code into Java, addressing a critical challenge in aging software systems. Leveraging the Legacy COBOL 2024 Corpus -- 50,000 COBOL files from public and enterprise sources -- Java parses the code, AI suggests upgrades, and React visualizes gains. Achieving 93% accuracy, complexity drops 35% (from 18 to 11.7) and coupling 33% (from 8 to 5.4), surpassing manual efforts (75%) and rule-based tools (82%). The approach offers a scalable path to rejuvenate COBOL systems, vital for industries like banking and insurance.\n        △ Less",
        "authors":"['Gopichand Bandarupalli']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Mathematical Framework of Semantic Communication based on Category Theory",
        "abstract":"While semantic communication (SemCom) has recently demonstrated great potential to enhance transmission efficiency and reliability by leveraging machine learning (ML) and knowledge base (KB), there is a lack of mathematical modeling to rigorously characterize SemCom system and quantify the performance gain obtained from ML and KB. In this paper, we develop a mathematical framework for SemCom based on category theory, rigorously model the concepts of semantic entities and semantic probability space. Within this framework, semantic entropy is introduced to quantify the uncertainty of semantic entities. We theoretically prove that semantic entropy can be effectively reduced by exploiting KB, which capture semantic dependencies. Specifically, semantic entities can be combined based on semantic ambiguity, and are encoded based on contextual relationships among them. Then we refine semantic channel capacity modeling, which considers the mutual information contained in KB to better reflect SemCom efficiency. Numerical simulations validate the effectiveness of the proposed framework, showing that SemCom with KB integration outperforms traditional communication in both entropy reduction and coding efficiency.\n        △ Less",
        "authors":"['Shuheng Hua', 'Yao Sun', 'Kairong Ma', 'Muhammad Ali Imran']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Subset-Contrastive Multi-Omics Network Embedding",
        "abstract":"Motivation: Network-based analyses of omics data are widely used, and while many of these methods have been adapted to single-cell scenarios, they often remain memory- and space-intensive. As a result, they are better suited to batch data or smaller datasets. Furthermore, the application of network-based methods in multi-omics often relies on similarity-based networks, which lack structurally-discrete topologies. This limitation may reduce the effectiveness of graph-based methods that were initially designed for topologies with better defined structures. Results: We propose Subset-Contrastive multi-Omics Network Embedding (SCONE), a method that employs contrastive learning techniques on large datasets through a scalable subgraph contrastive approach. By exploiting the pairwise similarity basis of many network-based omics methods, we transformed this characteristic into a strength, developing an approach that aims to achieve scalable and effective analysis. Our method demonstrates synergistic omics integration for cell type clustering in single-cell data. Additionally, we evaluate its performance in a bulk multi-omics integration scenario, where SCONE performs comparable to the state-of-the-art despite utilising limited views of the original data. We anticipate that our findings will motivate further research into the use of subset contrastive methods for omics data.\n        △ Less",
        "authors":"['Pedro Henrique da Costa Avelar', 'Min Wu', 'Sophia Tsoka']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints",
        "abstract":"Large Language Models (LLMs) are indispensable in today's applications, but their inference procedure -- generating responses by processing text in segments and using a memory-heavy Key-Value (KV) cache -- demands significant computational resources, particularly under memory constraints. This paper formulates LLM inference optimization as a multi-stage online scheduling problem where sequential prompt arrivals and KV cache growth render conventional scheduling ineffective. We develop a fluid dynamics approximation to provide a tractable benchmark that guides algorithm design. Building on this, we propose the Waiting for Accumulated Inference Threshold (WAIT) algorithm, which uses multiple thresholds to schedule incoming prompts optimally when output lengths are known, and extend it to Nested WAIT for cases with unknown output lengths. Theoretical analysis shows that both algorithms achieve near-optimal performance against the fluid benchmark in heavy traffic conditions, balancing throughput, latency, and Time to First Token (TTFT). Experiments with the Llama-7B model on an A100 GPU using both synthetic and real-world datasets demonstrate improved throughput and reduced latency relative to established baselines like vLLM and Sarathi. This work bridges operations research and machine learning, offering a rigorous framework for the efficient deployment of LLMs under memory constraints.\n        △ Less",
        "authors":"['Ruicheng Ao', 'Gan Luo', 'David Simchi-Levi', 'Xinshang Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mildly-Interacting Fermionic Unitaries are Efficiently Learnable",
        "abstract":"Recent work has shown that one can efficiently learn fermionic Gaussian unitaries, also commonly known as nearest-neighbor matchcircuits or non-interacting fermionic unitaries. However, one could ask a similar question about unitaries that are near Gaussian: for example, unitaries prepared with a small number of non-Gaussian circuit elements. These operators find significance in quantum chemistry and many-body physics, yet no algorithm exists to learn them.\n  We give the first such result by devising an algorithm which makes queries to a $n$-mode fermionic unitary $U$ prepared by at most $O(t)$ non-Gaussian gates and returns a circuit approximating $U$ to diamond distance $\\varepsilon$ in time $\\textrm{poly}(n,2^t,1\/\\varepsilon)$. This resolves a central open question of Mele and Herasymenko under the strongest distance metric. In fact, our algorithm is much more general: we define a property of unitary Gaussianity known as unitary Gaussian dimension and show that our algorithm can learn $n$-mode unitaries of Gaussian dimension at least $2n - O(t)$ in time $\\textrm{poly}(n,2^t,1\/\\varepsilon)$. Indeed, this class subsumes unitaries prepared by at most $O(t)$ non-Gaussian gates but also includes several unitaries that require up to $2^{O(t)}$ non-Gaussian gates to construct.\n  In addition, we give a $\\textrm{poly}(n,1\/\\varepsilon)$-time algorithm to distinguish whether an $n$-mode unitary is of Gaussian dimension at least $k$ or $\\varepsilon$-far from all such unitaries in Frobenius distance, promised that one is the case. Along the way, we prove structural results about near-Gaussian fermionic unitaries that are likely to be of independent interest.\n        △ Less",
        "authors":"['Vishnu Iyer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Differentially Private Geodesic and Linear Regression",
        "abstract":"In statistical applications it has become increasingly common to encounter data structures that live on non-linear spaces such as manifolds. Classical linear regression, one of the most fundamental methodologies of statistical learning, captures the relationship between an independent variable and a response variable which both are assumed to live in Euclidean space. Thus, geodesic regression emerged as an extension where the response variable lives on a Riemannian manifold. The parameters of geodesic regression, as with linear regression, capture the relationship of sensitive data and hence one should consider the privacy protection practices of said parameters. We consider releasing Differentially Private (DP) parameters of geodesic regression via the K-Norm Gradient (KNG) mechanism for Riemannian manifolds. We derive theoretical bounds for the sensitivity of the parameters showing they are tied to their respective Jacobi fields and hence the curvature of the space. This corroborates recent findings of differential privacy for the Fréchet mean. We demonstrate the efficacy of our methodology on the sphere, $\\mbS^2\\subset\\mbR^3$ and, since it is general to Riemannian manifolds, the manifold of Euclidean space which simplifies geodesic regression to a case of linear regression. Our methodology is general to any Riemannian manifold and thus it is suitable for data in domains such as medical imaging and computer vision.\n        △ Less",
        "authors":"['Aditya Kulkarni', 'Carlos Soto']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Limits of Discrete Energy of Families of Increasing Sets",
        "abstract":"The Hausdorff dimension of a set can be detected using the Riesz energy. Here, we consider situations where a sequence of points, $\\{x_n\\}$, ``fills in'' a set $E \\subset \\mathbb{R}^d$ in an appropriate sense and investigate the degree to which the discrete analog to the Riesz energy of these sets can be used to bound the Hausdorff dimension of $E$. We also discuss applications to data science and Erdős\/Falconer type problems.\n        △ Less",
        "authors":"['Hari Nathan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning to Be A Doctor: Searching for Effective Medical Agent Architectures",
        "abstract":"Large Language Model (LLM)-based agents have demonstrated strong capabilities across a wide range of tasks, and their application in the medical domain holds particular promise due to the demand for high generalizability and reliance on interdisciplinary knowledge. However, existing medical agent systems often rely on static, manually crafted workflows that lack the flexibility to accommodate diverse diagnostic requirements and adapt to emerging clinical scenarios. Motivated by the success of automated machine learning (AutoML), this paper introduces a novel framework for the automated design of medical agent architectures. Specifically, we define a hierarchical and expressive agent search space that enables dynamic workflow adaptation through structured modifications at the node, structural, and framework levels. Our framework conceptualizes medical agents as graph-based architectures composed of diverse, functional node types and supports iterative self-improvement guided by diagnostic feedback. Experimental results on skin disease diagnosis tasks demonstrate that the proposed method effectively evolves workflow structures and significantly enhances diagnostic accuracy over time. This work represents the first fully automated framework for medical agent architecture design and offers a scalable, adaptable foundation for deploying intelligent agents in real-world clinical environments.\n        △ Less",
        "authors":"['Yangyang Zhuang', 'Wenjia Jiang', 'Jiayu Zhang', 'Ze Yang', 'Joey Tianyi Zhou', 'Chi Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient and Stable Multi-Dimensional Kolmogorov-Smirnov Distance",
        "abstract":"We revisit extending the Kolmogorov-Smirnov distance between probability distributions to the multidimensional setting and make new arguments about the proper way to approach this generalization. Our proposed formulation maximizes the difference over orthogonal dominating rectangular ranges (d-sided rectangles in R^d), and is an integral probability metric. We also prove that the distance between a distribution and a sample from the distribution converges to 0 as the sample size grows, and bound this rate. Moreover, we show that one can, up to this same approximation error, compute the distance efficiently in 4 or fewer dimensions; specifically the runtime is near-linear in the size of the sample needed for that error. With this, we derive a delta-precision two-sample hypothesis test using this distance. Finally, we show these metric and approximation properties do not hold for other popular variants.\n        △ Less",
        "authors":"['Peter Matthew Jacobs', 'Foad Namjoo', 'Jeff M. Phillips']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation",
        "abstract":"Bipartite ranking is a fundamental supervised learning problem, with the goal of learning a ranking over instances with maximal area under the ROC curve (AUC) against a single binary target label. However, one may often observe multiple binary target labels, e.g., from distinct human annotators. How can one synthesize such labels into a single coherent ranking? In this work, we formally analyze two approaches to this problem -- loss aggregation and label aggregation -- by characterizing their Bayes-optimal solutions. Based on this, we show that while both methods can yield Pareto-optimal solutions, loss aggregation can exhibit label dictatorship: one can inadvertently (and undesirably) favor one label over others. This suggests that label aggregation can be preferable to loss aggregation, which we empirically verify.\n        △ Less",
        "authors":"['Michal Lukasik', 'Lin Chen', 'Harikrishna Narasimhan', 'Aditya Krishna Menon', 'Wittawat Jitkrittum', 'Felix X. Yu', 'Sashank J. Reddi', 'Gang Fu', 'Mohammadhossein Bateni', 'Sanjiv Kumar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DeepSelective: Feature Gating and Representation Matching for Interpretable Clinical Prediction",
        "abstract":"The rapid accumulation of Electronic Health Records (EHRs) has transformed healthcare by providing valuable data that enhance clinical predictions and diagnoses. While conventional machine learning models have proven effective, they often lack robust representation learning and depend heavily on expert-crafted features. Although deep learning offers powerful solutions, it is often criticized for its lack of interpretability. To address these challenges, we propose DeepSelective, a novel end to end deep learning framework for predicting patient prognosis using EHR data, with a strong emphasis on enhancing model interpretability. DeepSelective combines data compression techniques with an innovative feature selection approach, integrating custom-designed modules that work together to improve both accuracy and interpretability. Our experiments demonstrate that DeepSelective not only enhances predictive accuracy but also significantly improves interpretability, making it a valuable tool for clinical decision-making. The source code is freely available at http:\/\/www.healthinformaticslab.org\/supp\/resources.php .\n        △ Less",
        "authors":"['Ruochi Zhang', 'Qian Yang', 'Xiaoyang Wang', 'Haoran Wu', 'Qiong Zhou', 'Yu Wang', 'Kewei Li', 'Yueying Wang', 'Yusi Fan', 'Jiale Zhang', 'Lan Huang', 'Chang Liu', 'Fengfeng Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets",
        "abstract":"Climate change is a major threat to the future of humanity, and its impacts are being intensified by excess man-made greenhouse gas emissions. One method governments can employ to control these emissions is to provide firms with emission limits and penalize any excess emissions above the limit. Excess emissions may also be offset by firms who choose to invest in carbon reducing and capturing projects. These projects generate offset credits which can be submitted to a regulating agency to offset a firm's excess emissions, or they can be traded with other firms. In this work, we characterize the finite-agent Nash equilibrium for offset credit markets. As computing Nash equilibria is an NP-hard problem, we utilize the modern reinforcement learning technique Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate not only the validity of employing reinforcement learning methods applied to climate themed financial markets, but also the significant financial savings emitting firms may achieve when abiding by the Nash equilibria through numerical experiments.\n        △ Less",
        "authors":"['Liam Welsh', 'Udit Grover', 'Sebastian Jaimungal']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reconstructing Fine-Grained Network Data using Autoencoder Architectures with Domain Knowledge Penalties",
        "abstract":"The ability to reconstruct fine-grained network session data, including individual packets, from coarse-grained feature vectors is crucial for improving network security models. However, the large-scale collection and storage of raw network traffic pose significant challenges, particularly for capturing rare cyberattack samples. These challenges hinder the ability to retain comprehensive datasets for model training and future threat detection. To address this, we propose a machine learning approach guided by formal methods to encode and reconstruct network data. Our method employs autoencoder models with domain-informed penalties to impute PCAP session headers from structured feature representations. Experimental results demonstrate that incorporating domain knowledge through constraint-based loss terms significantly improves reconstruction accuracy, particularly for categorical features with session-level encodings. By enabling efficient reconstruction of detailed network sessions, our approach facilitates data-efficient model training while preserving privacy and storage efficiency.\n        △ Less",
        "authors":"['Mark Cheung', 'Sridhar Venkatesan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Rollout-Based Algorithm and Reward Function for Efficient Resource Allocation in Business Processes",
        "abstract":"Resource allocation plays a critical role in minimizing cycle time and improving the efficiency of business processes. Recently, Deep Reinforcement Learning (DRL) has emerged as a powerful tool to optimize resource allocation policies in business processes. In the DRL framework, an agent learns a policy through interaction with the environment, guided solely by reward signals that indicate the quality of its decisions. However, existing algorithms are not suitable for dynamic environments such as business processes. Furthermore, existing DRL-based methods rely on engineered reward functions that approximate the desired objective, but a misalignment between reward and objective can lead to undesired decisions or suboptimal policies. To address these issues, we propose a rollout-based DRL algorithm and a reward function to optimize the objective directly. Our algorithm iteratively improves the policy by evaluating execution trajectories following different actions. Our reward function directly decomposes the objective function of minimizing the mean cycle time. Maximizing our reward function guarantees that the objective function is minimized without requiring extensive reward engineering. The results show that our method consistently learns the optimal policy in all six evaluated business processes, outperforming the state-of-the-art algorithm that can only learn the optimal policy in two of the evaluated processes.\n        △ Less",
        "authors":"['Jeroen Middelhuis', 'Zaharah Bukhsh', 'Ivo Adan', 'Remco Dijkman']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cryo-em images are intrinsically low dimensional",
        "abstract":"Simulation-based inference provides a powerful framework for cryo-electron microscopy, employing neural networks in methods like CryoSBI to infer biomolecular conformations via learned latent representations. This latent space represents a rich opportunity, encoding valuable information about the physical system and the inference process. Harnessing this potential hinges on understanding the underlying geometric structure of these representations. We investigate this structure by applying manifold learning techniques to CryoSBI representations of hemagglutinin (simulated and experimental). We reveal that these high-dimensional data inherently populate low-dimensional, smooth manifolds, with simulated data effectively covering the experimental counterpart. By characterizing the manifold's geometry using Diffusion Maps and identifying its principal axes of variation via coordinate interpretation methods, we establish a direct link between the latent structure and key physical parameters. Discovering this intrinsic low-dimensionality and interpretable geometric organization not only validates the CryoSBI approach but enables us to learn more from the data structure and provides opportunities for improving future inference strategies by exploiting this revealed manifold geometry.\n        △ Less",
        "authors":"['Luke Evans', 'Octavian-Vlad Murad', 'Lars Dingeldein', 'Pilar Cossio', 'Roberto Covino', 'Marina Meila']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks",
        "abstract":"Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art algorithm for achieving sample-efficient multi-goal reinforcement learning (RL) in robotic manipulation tasks with binary rewards. HER facilitates learning from failed attempts by replaying trajectories with redefined goals. However, it relies on a heuristic-based replay method that lacks a principled framework. To address this limitation, we introduce a novel replay strategy, \"Next-Future\", which focuses on rewarding single-step transitions. This approach significantly enhances sample efficiency and accuracy in learning multi-goal Markov decision processes (MDPs), particularly under stringent accuracy requirements -- a critical aspect for performing complex and precise robotic-arm tasks. We demonstrate the efficacy of our method by highlighting how single-step learning enables improved value approximation within the multi-goal RL framework. The performance of the proposed replay strategy is evaluated across eight challenging robotic manipulation tasks, using ten random seeds for training. Our results indicate substantial improvements in sample efficiency for seven out of eight tasks and higher success rates in six tasks. Furthermore, real-world experiments validate the practical feasibility of the learned policies, demonstrating the potential of \"Next-Future\" in solving complex robotic-arm tasks.\n        △ Less",
        "authors":"['Fikrican Özgür', 'René Zurbrügg', 'Suryansh Kumar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Respiratory Inhaler Sound Event Classification Using Self-Supervised Learning",
        "abstract":"Asthma is a chronic respiratory condition that affects millions of people worldwide. While this condition can be managed by administering controller medications through handheld inhalers, clinical studies have shown low adherence to the correct inhaler usage technique. Consequently, many patients may not receive the full benefit of their medication. Automated classification of inhaler sounds has recently been studied to assess medication adherence. However, the existing classification models were typically trained using data from specific inhaler types, and their ability to generalize to sounds from different inhalers remains unexplored. In this study, we adapted the wav2vec 2.0 self-supervised learning model for inhaler sound classification by pre-training and fine-tuning this model on inhaler sounds. The proposed model shows a balanced accuracy of 98% on a dataset collected using a dry powder inhaler and smartwatch device. The results also demonstrate that re-finetuning this model on minimal data from a target inhaler is a promising approach to adapting a generic inhaler sound classification model to a different inhaler device and audio capture hardware. This is the first study in the field to demonstrate the potential of smartwatches as assistive technologies for the personalized monitoring of inhaler adherence using machine learning models.\n        △ Less",
        "authors":"['Davoud Shariat Panah', 'Alessandro N Franciosi', 'Cormac McCarthy', 'Andrew Hines']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Automated Safety Requirements Derivation Using Agent-based RAG",
        "abstract":"We study the automated derivation of safety requirements in a self-driving vehicle use case, leveraging LLMs in combination with agent-based retrieval-augmented generation. Conventional approaches that utilise pre-trained LLMs to assist in safety analyses typically lack domain-specific knowledge. Existing RAG approaches address this issue, yet their performance deteriorates when handling complex queries and it becomes increasingly harder to retrieve the most relevant information. This is particularly relevant for safety-relevant applications. In this paper, we propose the use of agent-based RAG to derive safety requirements and show that the retrieved information is more relevant to the queries. We implement an agent-based approach on a document pool of automotive standards and the Apollo case study, as a representative example of an automated driving perception system. Our solution is tested on a data set of safety requirement questions and answers, extracted from the Apollo data. Evaluating a set of selected RAG metrics, we present and discuss advantages of a agent-based approach compared to default RAG methods.\n        △ Less",
        "authors":"['Balahari Vignesh Balu', 'Florian Geissler', 'Francesco Carella', 'Joao-Vitor Zacchi', 'Josef Jiru', 'Nuria Mata', 'Reinhard Stolle']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Forward-Forward Algorithm: Characterizing Training Behavior",
        "abstract":"The Forward-Forward algorithm is an alternative learning method which consists of two forward passes rather than a forward and backward pass employed by backpropagation. Forward-Forward networks employ layer local loss functions which are optimized based on the layer activation for each forward pass rather than a single global objective function. This work explores the dynamics of model and layer accuracy changes in Forward-Forward networks as training progresses in pursuit of a mechanistic understanding of their internal behavior. Treatments to various system characteristics are applied to investigate changes in layer and overall model accuracy as training progresses, how accuracy is impacted by layer depth, and how strongly individual layer accuracy is correlated with overall model accuracy. The empirical results presented suggest that layers deeper within Forward-Forward networks experience a delay in accuracy improvement relative to shallower layers and that shallower layer accuracy is strongly correlated with overall model accuracy.\n        △ Less",
        "authors":"['Reece Adamson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax Computation in Transformers",
        "abstract":"While Transformers are dominated by Floating-Point (FP) Matrix-Multiplications, their aggressive acceleration through dedicated hardware or many-core programmable systems has shifted the performance bottleneck to non-linear functions like Softmax. Accelerating Softmax is challenging due to its non-pointwise, non-linear nature, with exponentiation as the most demanding step. To address this, we design a custom arithmetic block for Bfloat16 exponentiation leveraging a novel approximation algorithm based on Schraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of the RISC-V cores of a compute cluster, through custom Instruction Set Architecture (ISA) extensions, with a negligible area overhead of 1\\%. By optimizing the software kernels to leverage the extension, we execute Softmax with 162.7$\\times$ less latency and 74.3$\\times$ less energy compared to the baseline cluster, achieving an 8.2$\\times$ performance improvement and 4.1$\\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2 configuration. Moreover, the proposed approach enables a multi-cluster system to efficiently execute end-to-end inference of pre-trained Transformer models, such as GPT-2, GPT-3 and ViT, achieving up to 5.8$\\times$ and 3.6$\\times$ reduction in latency and energy consumption, respectively, without requiring re-training and with negligible accuracy loss.\n        △ Less",
        "authors":"['Run Wang', 'Gamze Islamoglu', 'Andrea Belano', 'Viviane Potocnik', 'Francesco Conti', 'Angelo Garofalo', 'Luca Benini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diversity-Driven Learning: Tackling Spurious Correlations and Data Heterogeneity in Federated Models",
        "abstract":"Federated Learning (FL) enables decentralized training of machine learning models on distributed data while preserving privacy. However, in real-world FL settings, client data is often non-identically distributed and imbalanced, resulting in statistical data heterogeneity which impacts the generalization capabilities of the server's model across clients, slows convergence and reduces performance. In this paper, we address this challenge by first proposing a characterization of statistical data heterogeneity by means of 6 metrics of global and client attribute imbalance, class imbalance, and spurious correlations. Next, we create and share 7 computer vision datasets for binary and multiclass image classification tasks in Federated Learning that cover a broad range of statistical data heterogeneity and hence simulate real-world situations. Finally, we propose FedDiverse, a novel client selection algorithm in FL which is designed to manage and leverage data heterogeneity across clients by promoting collaboration between clients with complementary data distributions. Experiments on the seven proposed FL datasets demonstrate FedDiverse's effectiveness in enhancing the performance and robustness of a variety of FL methods while having low communication and computational overhead.\n        △ Less",
        "authors":"['Gergely D. Németh', 'Eros Fanì', 'Yeat Jeng Ng', 'Barbara Caputo', 'Miguel Ángel Lozano', 'Nuria Oliver', 'Novi Quadrianto']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SDFs from Unoriented Point Clouds using Neural Variational Heat Distances",
        "abstract":"We propose a novel variational approach for computing neural Signed Distance Fields (SDF) from unoriented point clouds. To this end, we replace the commonly used eikonal equation with the heat method, carrying over to the neural domain what has long been standard practice for computing distances on discrete surfaces. This yields two convex optimization problems for whose solution we employ neural networks: We first compute a neural approximation of the gradients of the unsigned distance field through a small time step of heat flow with weighted point cloud densities as initial data. Then we use it to compute a neural approximation of the SDF. We prove that the underlying variational problems are well-posed. Through numerical experiments, we demonstrate that our method provides state-of-the-art surface reconstruction and consistent SDF gradients. Furthermore, we show in a proof-of-concept that it is accurate enough for solving a PDE on the zero-level set.\n        △ Less",
        "authors":"['Samuel Weidemaier', 'Florine Hartwig', 'Josua Sassen', 'Sergio Conti', 'Mirela Ben-Chen', 'Martin Rumpf']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Distributed Retrieval-Augmented Generation for Enhancing Language Model Performance",
        "abstract":"Small language models (SLMs) support efficient deployments on resource-constrained edge devices, but their limited capacity compromises inference performance. Retrieval-augmented generation (RAG) is a promising solution to enhance model performance by integrating external databases, without requiring intensive on-device model retraining. However, large-scale public databases and user-specific private contextual documents are typically located on the cloud and the device separately, while existing RAG implementations are primarily centralized. To bridge this gap, we propose DRAGON, a distributed RAG framework to enhance on-device SLMs through both general and personal knowledge without the risk of leaking document privacy. Specifically, DRAGON decomposes multi-document RAG into multiple parallel token generation processes performed independently and locally on the cloud and the device, and employs a newly designed Speculative Aggregation, a dual-side speculative algorithm to avoid frequent output synchronization between the cloud and device. A new scheduling algorithm is further introduced to identify the optimal aggregation side based on real-time network conditions. Evaluations on real-world hardware testbed demonstrate a significant performance improvement of DRAGON-up to 1.9x greater gains over standalone SLM compared to the centralized RAG, substantial reduction in per-token latency, and negligible Time to First Token (TTFT) overhead.\n        △ Less",
        "authors":"['Shangyu Liu', 'Zhenzhe Zheng', 'Xiaoyao Huang', 'Fan Wu', 'Guihai Chen', 'Jie Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning",
        "abstract":"Vision-language models (VLMs), such as CLIP, have gained significant popularity as foundation models, with numerous fine-tuning methods developed to enhance performance on downstream tasks. However, due to their inherent vulnerability and the common practice of selecting from a limited set of open-source models, VLMs suffer from a higher risk of adversarial attacks than traditional vision models. Existing defense techniques typically rely on adversarial fine-tuning during training, which requires labeled data and lacks of flexibility for downstream tasks. To address these limitations, we propose robust test-time prompt tuning (R-TPT), which mitigates the impact of adversarial attacks during the inference stage. We first reformulate the classic marginal entropy objective by eliminating the term that introduces conflicts under adversarial conditions, retaining only the pointwise entropy minimization. Furthermore, we introduce a plug-and-play reliability-based weighted ensembling strategy, which aggregates useful information from reliable augmented views to strengthen the defense. R-TPT enhances defense against adversarial attacks without requiring labeled training data while offering high flexibility for inference tasks. Extensive experiments on widely used benchmarks with various attacks demonstrate the effectiveness of R-TPT. The code is available in https:\/\/github.com\/TomSheng21\/R-TPT.\n        △ Less",
        "authors":"['Lijun Sheng', 'Jian Liang', 'Zilei Wang', 'Ran He']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space",
        "abstract":"The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code will be made publicly available after acceptance.\n        △ Less",
        "authors":"['Taewook Kang', 'Bum-Jae You', 'Juyoun Park', 'Yisoo Lee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
        "abstract":"Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems.\n        △ Less",
        "authors":"['William Hackett', 'Lewis Birch', 'Stefan Trawicki', 'Neeraj Suri', 'Peter Garraghan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention",
        "abstract":"Predicting future trajectories of surrounding vehicles heavily relies on what contextual information is given to a motion prediction model. The context itself can be static (lanes, regulatory elements, etc) or dynamic (traffic participants). This paper presents a lane graph-based motion prediction model that first predicts graph-based goal proposals and later fuses them with cross attention over multiple contextual elements. We follow the famous encoder-interactor-decoder architecture where the encoder encodes scene context using lightweight Gated Recurrent Units, the interactor applies cross-context attention over encoded scene features and graph goal proposals, and the decoder regresses multimodal trajectories via Laplacian Mixture Density Network from the aggregated encodings. Using cross-attention over graph-based goal proposals gives robust trajectory estimates since the model learns to attend to future goal-relevant scene elements for the intended agent. We evaluate our work on nuScenes motion prediction dataset, achieving state-of-the-art results.\n        △ Less",
        "authors":"['Mahir Gulzar', 'Yar Muhammad', 'Naveed Muhammad']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Divergence of Empirical Neural Tangent Kernel in Classification Problems",
        "abstract":"This paper demonstrates that in classification problems, fully connected neural networks (FCNs) and residual neural networks (ResNets) cannot be approximated by kernel logistic regression based on the Neural Tangent Kernel (NTK) under overtraining (i.e., when training time approaches infinity). Specifically, when using the cross-entropy loss, regardless of how large the network width is (as long as it is finite), the empirical NTK diverges from the NTK on the training samples as training time increases. To establish this result, we first demonstrate the strictly positive definiteness of the NTKs for multi-layer FCNs and ResNets. Then, we prove that during training, % with the cross-entropy loss, the neural network parameters diverge if the smallest eigenvalue of the empirical NTK matrix (Gram matrix) with respect to training samples is bounded below by a positive constant. This behavior contrasts sharply with the lazy training regime commonly observed in regression problems. Consequently, using a proof by contradiction, we show that the empirical NTK does not uniformly converge to the NTK across all times on the training samples as the network width increases. We validate our theoretical results through experiments on both synthetic data and the MNIST classification task. This finding implies that NTK theory is not applicable in this context, with significant theoretical implications for understanding neural networks in classification problems.\n        △ Less",
        "authors":"['Zixiong Yu', 'Songtao Tian', 'Guhan Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"K-means Enhanced Density Gradient Analysis for Urban and Transport Metrics Using Multi-Modal Satellite Imagery",
        "abstract":"This paper presents a novel computational approach for evaluating urban metrics through density gradient analysis using multi-modal satellite imagery, with applications including public transport and other urban systems. By combining optical and Synthetic Aperture Radar (SAR) data, we develop a method to segment urban areas, identify urban centers, and quantify density gradients. Our approach calculates two key metrics: the density gradient coefficient ($α$) and the minimum effective distance (LD) at which density reaches a target threshold. We further employ machine learning techniques, specifically K-means clustering, to objectively identify uniform and high-variability regions within density gradient plots. We demonstrate that these metrics provide an effective screening tool for public transport analyses by revealing the underlying urban structure. Through comparative analysis of two representative cities with contrasting urban morphologies (monocentric vs polycentric), we establish relationships between density gradient characteristics and public transport network topologies. Cities with clear density peaks in their gradient plots indicate distinct urban centers requiring different transport strategies than those with more uniform density distributions. This methodology offers urban planners a cost-effective, globally applicable approach to preliminary public transport assessment using freely available satellite data. The complete implementation, with additional examples and documentation, is available in an open-source repository under the MIT license at https:\/\/github.com\/nexri\/Satellite-Imagery-Urban-Analysis.\n        △ Less",
        "authors":"['P. Tomkiewicz', 'J. Jaworski', 'P. Zielonka', 'A. Wilinski']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay",
        "abstract":"This study introduces a novel method for revealing human covert attention patterns using gameplay data alone, utilizing offline attention techniques from reinforcement learning (RL). We propose the contextualized, task-relevant (CTR) attention network, which generates attention maps from both human and RL agent gameplay in Atari environments. These maps are sparse yet retain the necessary information for the current player's decision making. We compare the CTR-derived attention maps with a temporally integrated overt attention (TIOA) model based on eye-tracking data, serving as a point of comparison and discussion. Visual inspection reveals distinct attention patterns: human CTR maps focus on the player and rather nearby opponents, occasionally shifting between stronger focus and broader views - sometimes even attending to empty space ahead. In contrast, agent maps maintain a consistent broad focus on most objects, including distant ones and the player. Quantitative analysis further demonstrates that human CTR maps align more closely with TIOA than agent maps do. Our findings indicate that the CTR attention network can effectively reveal human covert attention patterns from gameplay alone, without the need for additional data like brain activity recordings. This work contributes to understanding human-agent attention differences and enables the development of RL agents augmented with human covert attention.\n        △ Less",
        "authors":"['Henrik Krauss', 'Takehisa Yairi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fine-Tuning Large Language Models on Quantum Optimization Problems for Circuit Generation",
        "abstract":"Large language models (LLM) have achieved remarkable outcomes in addressing complex problems, including math, coding, and analyzing large amounts of scientific reports. Yet few works have explored the potential of LLM in quantum computing. The most challenging problem is how to leverage LLMs to automatically generate quantum circuits at a large scale. In this paper, we address such a challenge by fine-tuning LLMs and injecting the domain-specific knowledge of quantum computing. In particular, we investigate the mechanisms to generate training data sets and construct the end-to-end pipeline to fine-tune pre-trained LLMs that produce parameterized quantum circuits for optimization problems. We have prepared 14,000 quantum circuits covering a substantial part of the quantum optimization landscape: 12 optimization problem instances and their optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can construct syntactically correct parametrized quantum circuits in the most recent OpenQASM 3.0. We have evaluated the quality of the parameters by comparing them to the optimized expectation values and distributions. Our evaluation shows that the fine-tuned LLM outperforms state-of-the-art models and that the parameters are better than random. The LLM-generated parametrized circuits and initial parameters can be used as a starting point for further optimization, \\emph{e.g.,} templates in quantum machine learning and the benchmark for compilers and hardware.\n        △ Less",
        "authors":"['Linus Jern', 'Valter Uotila', 'Cong Yu', 'Bo Zhao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification",
        "abstract":"Antibiotic resistance presents a growing global health crisis, demanding new therapeutic strategies that target novel bacterial mechanisms. Recent advances in protein structure prediction and machine learning-driven molecule generation offer a promising opportunity to accelerate drug discovery. However, practical guidance on selecting and integrating these models into real-world pipelines remains limited. In this study, we develop an end-to-end, artificial intelligence-guided antibiotic discovery pipeline that spans target identification to compound realization. We leverage structure-based clustering across predicted proteomes of multiple pathogens to identify conserved, essential, and non-human-homologous targets. We then systematically evaluate six leading 3D-structure-aware generative models$\\unicode{x2014}$spanning diffusion, autoregressive, graph neural network, and language model architectures$\\unicode{x2014}$on their usability, chemical validity, and biological relevance. Rigorous post-processing filters and commercial analogue searches reduce over 100 000 generated compounds to a focused, synthesizable set. Our results highlight DeepBlock and TamGen as top performers across diverse criteria, while also revealing critical trade-offs between model complexity, usability, and output quality. This work provides a comparative benchmark and blueprint for deploying artificial intelligence in early-stage antibiotic development.\n        △ Less",
        "authors":"['Maximilian G. Schuh', 'Joshua Hesse', 'Stephan A. Sieber']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InfoClus: Informative Clustering of High-dimensional Data Embeddings",
        "abstract":"Developing an understanding of high-dimensional data can be facilitated by visualizing that data using dimensionality reduction. However, the low-dimensional embeddings are often difficult to interpret. To facilitate the exploration and interpretation of low-dimensional embeddings, we introduce a new concept named partitioning with explanations. The idea is to partition the data shown through the embedding into groups, each of which is given a sparse explanation using the original high-dimensional attributes. We introduce an objective function that quantifies how much we can learn through observing the explanations of the data partitioning, using information theory, and also how complex the explanations are. Through parameterization of the complexity, we can tune the solutions towards the desired granularity. We propose InfoClus, which optimizes the partitioning and explanations jointly, through greedy search constrained over a hierarchical clustering. We conduct a qualitative and quantitative analysis of InfoClus on three data sets. We contrast the results on the Cytometry data with published manual analysis results, and compare with two other recent methods for explaining embeddings (RVX and VERA). These comparisons highlight that InfoClus has distinct advantages over existing procedures and methods. We find that InfoClus can automatically create good starting points for the analysis of dimensionality-reduction-based scatter plots.\n        △ Less",
        "authors":"['Fuyin Lai', 'Edith Heiter', 'Guillaume Bied', 'Jefrey Lijffijt']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TD-Suite: All Batteries Included Framework for Technical Debt Classification",
        "abstract":"Recognizing that technical debt is a persistent and significant challenge requiring sophisticated management tools, TD-Suite offers a comprehensive software framework specifically engineered to automate the complex task of its classification within software projects. It leverages the advanced natural language understanding of state-of-the-art transformer models to analyze textual artifacts, such as developer discussions in issue reports, where subtle indicators of debt often lie hidden.\n  TD-Suite provides a seamless end-to-end pipeline, managing everything from initial data ingestion and rigorous preprocessing to model training, thorough evaluation, and final inference. This allows it to support both straightforward binary classification (debt or no debt) and more valuable, identifying specific categories like code, design, or documentation debt, thus enabling more targeted management strategies.\n  To ensure the generated models are robust and perform reliably on real-world, often imbalanced, datasets, TD-Suite incorporates critical training methodologies: k-fold cross-validation assesses generalization capability, early stopping mechanisms prevent overfitting to the training data, and class weighting strategies effectively address skewed data distributions. Beyond core functionality, and acknowledging the growing importance of sustainability, the framework integrates tracking and reporting of carbon emissions associated with the computationally intensive model training process.\n  It also features a user-friendly Gradio web interface in a Docker container setup, simplifying model interaction, evaluation, and inference.\n        △ Less",
        "authors":"['Karthik Shivashankar', 'Antonio Martini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"QAMA: Quantum annealing multi-head attention operator with classical deep learning framework",
        "abstract":"As large language models scale up, the conventional attention mechanism faces critical challenges of exponential growth in memory consumption and energy costs. Quantum annealing computing, with its inherent advantages in computational efficiency and low energy consumption, offers an innovative direction for constructing novel deep learning architectures. This study proposes the first Quantum Annealing-based Multi-head Attention (QAMA) mechanism, achieving seamless compatibility with classical attention architectures through quadratic unconstrained binary optimization (QUBO) modeling of forward propagation and energy-based backpropagation. The method innovatively leverages the quantum bit interaction characteristics of Ising models to optimize the conventional $O(n^2)$ spatiotemporal complexity into linear resource consumption. Integrated with the optical computing advantages of coherent Ising machines (CIM), the system maintains millisecond-level real-time responsiveness while significantly reducing energy consumption. Our key contributions include: Theoretical proofs establish QAMA mathematical equivalence to classical attention mechanisms; Dual optimization of multi-head specificity and long-range information capture via QUBO constraints; Explicit gradient proofs for the Ising energy equation are utilized to implement gradient conduction as the only path in the computational graph as a layer; Proposed soft selection mechanism overcoming traditional binary attention limitations to approximate continuous weights. Experiments on QBoson CPQC quantum computer show QAMA achieves comparable accuracy to classical operators while reducing inference time to millisecond level and improving solution quality. This work pioneers architectural-level integration of quantum computing and deep learning, applicable to any attention-based model, driving paradigm innovation in AI foundational computing.\n        △ Less",
        "authors":"['Peng Du', 'Shuolei Wang', 'Shicheng Li', 'Jinjing Shi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DPS: Design Pattern Summarisation Using Code Features",
        "abstract":"Automatic summarisation has been used efficiently in recent years to condense texts, conversations, audio, code, and various other artefacts. A range of methods, from simple template-based summaries to complex machine learning techniques -- and more recently, large language models -- have been employed to generate these summaries. Summarising software design patterns is important because it helps developers quickly understand and reuse complex design concepts, thereby improving software maintainability and development efficiency. However, the generation of summaries for software design patterns has not yet been explored. Our approach utilises code features and JavaParser to parse the code and create a JSON representation. Using an NLG library on this JSON representation, we convert it into natural language text that acts as a summary of the code, capturing the contextual information of the design pattern. Our empirical results indicate that the summaries generated by our approach capture the context in which patterns are applied in the codebase. Statistical evaluations demonstrate that our summaries closely align with human-written summaries, as evident from high values in the ROUGE-L, BLEU-4, NIST, and FrugalScore metrics. A follow-up survey further shows that DPS summaries were rated as capturing context better than human-generated summaries.\n        △ Less",
        "authors":"['Najam Nazar', 'Sameer Sikka', 'Christoph Treude']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scalability and Maintainability Challenges and Solutions in Machine Learning: Systematic Literature Review",
        "abstract":"This systematic literature review examines the critical challenges and solutions related to scalability and maintainability in Machine Learning (ML) systems. As ML applications become increasingly complex and widespread across industries, the need to balance system scalability with long-term maintainability has emerged as a significant concern. This review synthesizes current research and practices addressing these dual challenges across the entire ML life-cycle, from data engineering to model deployment in production. We analyzed 124 papers to identify and categorize 41 maintainability challenges and 13 scalability challenges, along with their corresponding solutions. Our findings reveal intricate inter dependencies between scalability and maintainability, where improvements in one often impact the other.\n  The review is structured around six primary research questions, examining maintainability and scalability challenges in data engineering, model engineering, and ML system development. We explore how these challenges manifest differently across various stages of the ML life-cycle.\n  This comprehensive overview offers valuable insights for both researchers and practitioners in the field of ML systems. It aims to guide future research directions, inform best practices, and contribute to the development of more robust, efficient, and sustainable ML applications across various domains.\n        △ Less",
        "authors":"['Karthik Shivashankar', 'Ghadi S. Al Hajj', 'Antonio Martini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using Time Structure to Estimate Causal Effects",
        "abstract":"There exist several approaches for estimating causal effects in time series when latent confounding is present. Many of these approaches rely on additional auxiliary observed variables or time series such as instruments, negative controls or time series that satisfy the front- or backdoor criterion in certain graphs. In this paper, we present a novel approach for estimating direct (and via Wright's path rule total) causal effects in a time series setup which does not rely on additional auxiliary observed variables or time series. This approach assumes that the underlying time series is a Structural Vector Autoregressive (SVAR) process and estimates direct causal effects by solving certain linear equation systems made up of different covariances and model parameters. We state sufficient graphical criteria in terms of the so-called full time graph under which these linear equations systems are uniquely solvable and under which their solutions contain the to-be-identified direct causal effects as components. We also state sufficient lag-based criteria under which the previously mentioned graphical conditions are satisfied and, thus, under which direct causal effects are identifiable. Several numerical experiments underline the correctness and applicability of our results.\n        △ Less",
        "authors":"['Tom Hochsprung', 'Jakob Runge', 'Andreas Gerhardus']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamical errors in machine learning forecasts",
        "abstract":"In machine learning forecasting, standard error metrics such as mean absolute error (MAE) and mean squared error (MSE) quantify discrepancies between predictions and target values. However, these metrics do not directly evaluate the physical and\/or dynamical consistency of forecasts, an increasingly critical concern in scientific and engineering applications.\n  Indeed, a fundamental yet often overlooked question is whether machine learning forecasts preserve the dynamical behavior of the underlying system. Addressing this issue is essential for assessing the fidelity of machine learning models and identifying potential failure modes, particularly in applications where maintaining correct dynamical behavior is crucial.\n  In this work, we investigate the relationship between standard forecasting error metrics, such as MAE and MSE, and the dynamical properties of the underlying system. To achieve this goal, we use two recently developed dynamical indices: the instantaneous dimension ($d$), and the inverse persistence ($θ$). Our results indicate that larger forecast errors -- e.g., higher MSE -- tend to occur in states with higher $d$ (higher complexity) and higher $θ$ (lower persistence). To further assess dynamical consistency, we propose error metrics based on the dynamical indices that measure the discrepancy of the forecasted $d$ and $θ$ versus their correct values. Leveraging these dynamical indices-based metrics, we analyze direct and recursive forecasting strategies for three canonical datasets -- Lorenz, Kuramoto-Sivashinsky equation, and Kolmogorov flow -- as well as a real-world weather forecasting task. Our findings reveal substantial distortions in dynamical properties in ML forecasts, especially for long forecast lead times or long recursive simulations, providing complementary information on ML forecast fidelity that can be used to improve ML models.\n        △ Less",
        "authors":"['Zhou Fang', 'Gianmarco Mengaldo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Morphing-based Compression for Data-centric ML Pipelines",
        "abstract":"Data-centric ML pipelines extend traditional machine learning (ML) pipelines -- of feature transformations and ML model training -- by outer loops for data cleaning, augmentation, and feature engineering to create high-quality input data. Existing lossless matrix compression applies lightweight compression schemes to numeric matrices and performs linear algebra operations such as matrix-vector multiplications directly on the compressed representation but struggles to efficiently rediscover structural data redundancy. Compressed operations are effective at fitting data in available memory, reducing I\/O across the storage-memory-cache hierarchy, and improving instruction parallelism. The applied data cleaning, augmentation, and feature transformations provide a rich source of information about data characteristics such as distinct items, column sparsity, and column correlations. In this paper, we introduce BWARE -- an extension of AWARE for workload-aware lossless matrix compression -- that pushes compression through feature transformations and engineering to leverage information about structural transformations. Besides compressed feature transformations, we introduce a novel technique for lightweight morphing of a compressed representation into workload-optimized compressed representations without decompression. BWARE shows substantial end-to-end runtime improvements, reducing the execution time for training data-centric ML pipelines from days to hours.\n        △ Less",
        "authors":"['Sebastian Baunsgaard', 'Matthias Boehm']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models",
        "abstract":"Unsupervised reinforcement learning (RL) aims at pre-training agents that can solve a wide range of downstream tasks in complex environments. Despite recent advancements, existing approaches suffer from several limitations: they may require running an RL process on each downstream task to achieve a satisfactory performance, they may need access to datasets with good coverage or well-curated task-specific samples, or they may pre-train policies with unsupervised losses that are poorly correlated with the downstream tasks of interest. In this paper, we introduce a novel algorithm regularizing unsupervised RL towards imitating trajectories from unlabeled behavior datasets. The key technical novelty of our method, called Forward-Backward Representations with Conditional-Policy Regularization, is to train forward-backward representations to embed the unlabeled trajectories to the same latent space used to represent states, rewards, and policies, and use a latent-conditional discriminator to encourage policies to ``cover'' the states in the unlabeled behavior dataset. As a result, we can learn policies that are well aligned with the behaviors in the dataset, while retaining zero-shot generalization capabilities for reward-based and imitation tasks. We demonstrate the effectiveness of this new approach in a challenging humanoid control problem: leveraging observation-only motion capture datasets, we train Meta Motivo, the first humanoid behavioral foundation model that can be prompted to solve a variety of whole-body tasks, including motion tracking, goal reaching, and reward optimization. The resulting model is capable of expressing human-like behaviors and it achieves competitive performance with task-specific methods while outperforming state-of-the-art unsupervised RL and model-based baselines.\n        △ Less",
        "authors":"['Andrea Tirinzoni', 'Ahmed Touati', 'Jesse Farebrother', 'Mateusz Guzek', 'Anssi Kanervisto', 'Yingchen Xu', 'Alessandro Lazaric', 'Matteo Pirotta']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"QualiTagger: Automating software quality detection in issue trackers",
        "abstract":"A systems quality is a major concern for development teams when it evolve. Understanding the effects of a loss of quality in the codebase is crucial to avoid side effects like the appearance of technical debt. Although the identification of these qualities in software requirements described in natural language has been investigated, most of the results are often not applicable in practice, and rely on having been validated on small datasets and limited amount of projects. For many years, machine learning (ML) techniques have been proved as a valid technique to identify and tag terms described in natural language. In order to advance previous works, in this research we use cutting edge models like Transformers, together with a vast dataset mined and curated from GitHub, to identify what text is usually associated with different quality properties. We also study the distribution of such qualities in issue trackers from openly accessible software repositories, and we evaluate our approach both with students from a software engineering course and with its application to recognize security labels in industry.\n        △ Less",
        "authors":"['Karthik Shivashankar', 'Rafael Capilla', 'Maren Maritsdatter Kruke', 'Mili Orucevic', 'Antonio Martini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging LLMs and attention-mechanism for automatic annotation of historical maps",
        "abstract":"Historical maps are essential resources that provide insights into the geographical landscapes of the past. They serve as valuable tools for researchers across disciplines such as history, geography, and urban studies, facilitating the reconstruction of historical environments and the analysis of spatial transformations over time. However, when constrained to analogue or scanned formats, their interpretation is limited to humans and therefore not scalable. Recent advancements in machine learning, particularly in computer vision and large language models (LLMs), have opened new avenues for automating the recognition and classification of features and objects in historical maps. In this paper, we propose a novel distillation method that leverages LLMs and attention mechanisms for the automatic annotation of historical maps. LLMs are employed to generate coarse classification labels for low-resolution historical image patches, while attention mechanisms are utilized to refine these labels to higher resolutions. Experimental results demonstrate that the refined labels achieve a high recall of more than 90%. Additionally, the intersection over union (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with precision scores of 87.1% and 79.5%, respectively, indicate that most labels are well-aligned with ground-truth annotations. Notably, these results were achieved without the use of fine-grained manual labels during training, underscoring the potential of our approach for efficient and scalable historical map analysis.\n        △ Less",
        "authors":"['Yunshuang Yuan', 'Monika Sester']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Defending Against Frequency-Based Attacks with Diffusion Models",
        "abstract":"Adversarial training is a common strategy for enhancing model robustness against adversarial attacks. However, it is typically tailored to the specific attack types it is trained on, limiting its ability to generalize to unseen threat models. Adversarial purification offers an alternative by leveraging a generative model to remove perturbations before classification. Since the purifier is trained independently of both the classifier and the threat models, it is better equipped to handle previously unseen attack scenarios. Diffusion models have proven highly effective for noise purification, not only in countering pixel-wise adversarial perturbations but also in addressing non-adversarial data shifts. In this study, we broaden the focus beyond pixel-wise robustness to explore the extent to which purification can mitigate both spectral and spatial adversarial attacks. Our findings highlight its effectiveness in handling diverse distortion patterns across low- to high-frequency regions.\n        △ Less",
        "authors":"['Fatemeh Amerehi', 'Patrick Healy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A PyTorch-Compatible Spike Encoding Framework for Energy-Efficient Neuromorphic Applications",
        "abstract":"Spiking Neural Networks (SNNs) offer promising energy efficiency advantages, particularly when processing sparse spike trains. However, their incompatibility with traditional datasets, which consist of batches of input vectors rather than spike trains, necessitates the development of efficient encoding methods. This paper introduces a novel, open-source PyTorch-compatible Python framework for spike encoding, designed for neuromorphic applications in machine learning and reinforcement learning. The framework supports a range of encoding algorithms, including Leaky Integrate-and-Fire (LIF), Step Forward (SF), Pulse Width Modulation (PWM), and Ben's Spiker Algorithm (BSA), as well as specialized encoding strategies covering population coding and reinforcement learning scenarios. Furthermore, we investigate the performance trade-offs of each method on embedded hardware using C\/C++ implementations, considering energy consumption, computation time, spike sparsity, and reconstruction accuracy. Our findings indicate that SF typically achieves the lowest reconstruction error and offers the highest energy efficiency and fastest encoding speed, achieving the second-best spike sparsity. At the same time, other methods demonstrate particular strengths depending on the signal characteristics. This framework and the accompanying empirical analysis provide valuable resources for selecting optimal encoding strategies for energy-efficient SNN applications.\n        △ Less",
        "authors":"['Alexandru Vasilache', 'Jona Scholz', 'Vincent Schilling', 'Sven Nitzsche', 'Florian Kaelber', 'Johannes Korsch', 'Juergen Becker']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Meta-learning For Few-Shot Time Series Crop Type Classification: A Benchmark On The EuroCropsML Dataset",
        "abstract":"Spatial imbalances in crop type data pose significant challenges for accurate classification in remote sensing applications. Algorithms aiming at transferring knowledge from data-rich to data-scarce tasks have thus surged in popularity. However, despite their effectiveness in previous evaluations, their performance in challenging real-world applications is unclear and needs to be evaluated. This study benchmarks transfer learning and several meta-learning algorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML), Almost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the real-world EuroCropsML time series dataset, which combines farmer-reported crop data with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal. Our findings indicate that MAML-based meta-learning algorithms achieve slightly higher accuracy compared to simpler transfer learning methods when applied to crop type classification tasks in Estonia after pre-training on data from Latvia. However, this improvement comes at the cost of increased computational demands and training time. Moreover, we find that the transfer of knowledge between geographically disparate regions, such as Estonia and Portugal, poses significant challenges to all investigated algorithms. These insights underscore the trade-offs between accuracy and computational resource requirements in selecting machine learning methods for real-world crop type classification tasks and highlight the difficulties of transferring knowledge between different regions of the Earth. To facilitate future research in this domain, we present the first comprehensive benchmark for evaluating transfer and meta-learning methods for crop type classification under real-world conditions. The corresponding code is publicly available at https:\/\/github.com\/dida-do\/eurocrops-meta-learning.\n        △ Less",
        "authors":"['Joana Reuss', 'Jan Macdonald', 'Simon Becker', 'Konrad Schultka', 'Lorenz Richter', 'Marco Körner']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Vertical Public-Private Split for Improved Synthetic Data Generation",
        "abstract":"Differentially Private Synthetic Data Generation (DP-SDG) is a key enabler of private and secure tabular-data sharing, producing artificial data that carries through the underlying statistical properties of the input data. This typically involves adding carefully calibrated statistical noise to guarantee individual privacy, at the cost of synthetic data quality. Recent literature has explored scenarios where a small amount of public data is used to help enhance the quality of synthetic data. These methods study a horizontal public-private partitioning which assumes access to a small number of public rows that can be used for model initialization, providing a small utility gain. However, realistic datasets often naturally consist of public and private attributes, making a vertical public-private partitioning relevant for practical synthetic data deployments. We propose a novel framework that adapts horizontal public-assisted methods into the vertical setting. We compare this framework against our alternative approach that uses conditional generation, highlighting initial limitations of public-data assisted methods and proposing future research directions to address these challenges.\n        △ Less",
        "authors":"['Samuel Maddock', 'Shripad Gade', 'Graham Cormode', 'Will Bullock']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ProtFlow: Fast Protein Sequence Design via Flow Matching on Compressed Protein Language Model Embeddings",
        "abstract":"The design of protein sequences with desired functionalities is a fundamental task in protein engineering. Deep generative methods, such as autoregressive models and diffusion models, have greatly accelerated the discovery of novel protein sequences. However, these methods mainly focus on local or shallow residual semantics and suffer from low inference efficiency, large modeling space and high training cost. To address these challenges, we introduce ProtFlow, a fast flow matching-based protein sequence design framework that operates on embeddings derived from semantically meaningful latent space of protein language models. By compressing and smoothing the latent space, ProtFlow enhances performance while training on limited computational resources. Leveraging reflow techniques, ProtFlow enables high-quality single-step sequence generation. Additionally, we develop a joint design pipeline for the design scene of multichain proteins. We evaluate ProtFlow across diverse protein design tasks, including general peptides and long-chain proteins, antimicrobial peptides, and antibodies. Experimental results demonstrate that ProtFlow outperforms task-specific methods in these applications, underscoring its potential and broad applicability in computational protein sequence design and analysis.\n        △ Less",
        "authors":"['Zitai Kong', 'Yiheng Zhu', 'Yinlong Xu', 'Hanjing Zhou', 'Mingzhe Yin', 'Jialu Wu', 'Hongxia Xu', 'Chang-Yu Hsieh', 'Tingjun Hou', 'Jian Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Early Detection of Cognitive Impairment in Elderly using a Passive FPVS-EEG BCI and Machine Learning -- Extended Version",
        "abstract":"Early dementia diagnosis requires biomarkers sensitive to both structural and functional brain changes. While structural neuroimaging biomarkers have progressed significantly, objective functional biomarkers of early cognitive decline remain a critical unmet need. Current cognitive assessments often rely on behavioral responses, making them susceptible to factors like effort, practice effects, and educational background, thereby hindering early and accurate detection. This work introduces a novel approach, leveraging a lightweight convolutional neural network (CNN) to infer cognitive impairment levels directly from electroencephalography (EEG) data. Critically, this method employs a passive fast periodic visual stimulation (FPVS) paradigm, eliminating the need for explicit behavioral responses or task comprehension from the participant. This passive approach provides an objective measure of working memory function, independent of confounding factors inherent in active cognitive tasks, and offers a promising new avenue for early and unbiased detection of cognitive decline.\n        △ Less",
        "authors":"['Tomasz M. Rutkowski', 'Stanisław Narębski', 'Mihoko Otake-Matsuura', 'Tomasz Komendziński']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning-Based User Association for MmWave Vehicular Networks With Kernelized Contextual Bandits",
        "abstract":"Vehicles require timely channel conditions to determine the base station (BS) to communicate with, but it is costly to estimate the fast-fading mmWave channels frequently. Without additional channel estimations, the proposed Distributed Kernelized Upper Confidence Bound (DK-UCB) algorithm estimates the current instantaneous transmission rates utilizing past contexts, such as the vehicle's location and velocity, along with past instantaneous transmission rates. To capture the nonlinear mapping from a context to the instantaneous transmission rate, DK-UCB maps a context into the reproducing kernel Hilbert space (RKHS) where a linear mapping becomes observable. To improve estimation accuracy, we propose a novel kernel function in RKHS which incorporates the propagation characteristics of the mmWave signals. Moreover, DK-UCB encourages a vehicle to share necessary information when it has conducted significant explorations, which speeds up the learning process while maintaining affordable communication costs.\n        △ Less",
        "authors":"['Xiaoyang He', 'Xiaoxia Huang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Recognition of Geometrical Shapes by Dictionary Learning",
        "abstract":"Dictionary learning is a versatile method to produce an overcomplete set of vectors, called atoms, to represent a given input with only a few atoms. In the literature, it has been used primarily for tasks that explore its powerful representation capabilities, such as for image reconstruction. In this work, we present a first approach to make dictionary learning work for shape recognition, considering specifically geometrical shapes. As we demonstrate, the choice of the underlying optimization method has a significant impact on recognition quality. Experimental results confirm that dictionary learning may be an interesting method for shape recognition tasks.\n        △ Less",
        "authors":"['Alexander Köhler', 'Michael Breuß']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers",
        "abstract":"Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).\n        △ Less",
        "authors":"['Hongkang Li', 'Yihua Zhang', 'Shuai Zhang', 'Meng Wang', 'Sijia Liu', 'Pin-Yu Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BEACON: A Benchmark for Efficient and Accurate Counting of Subgraphs",
        "abstract":"Subgraph counting the task of determining the number of instances of a query pattern within a large graph lies at the heart of many critical applications, from analyzing financial networks and transportation systems to understanding biological interactions. Despite decades of work yielding efficient algorithmic (AL) solutions and, more recently, machine learning (ML) approaches, a clear comparative understanding is elusive. This gap stems from the absence of a unified evaluation framework, standardized datasets, and accessible ground truths, all of which hinder systematic analysis and fair benchmarking. To overcome these barriers, we introduce BEACON: a comprehensive benchmark designed to rigorously evaluate both AL and ML-based subgraph counting methods. BEACON provides a standardized dataset with verified ground truths, an integrated evaluation environment, and a public leaderboard, enabling reproducible and transparent comparisons across diverse approaches. Our extensive experiments reveal that while AL methods excel in efficiently counting subgraphs on very large graphs, they struggle with complex patterns (e.g., those exceeding six nodes). In contrast, ML methods are capable of handling larger patterns but demand massive graph data inputs and often yield suboptimal accuracy on small, dense graphs. These insights not only highlight the unique strengths and limitations of each approach but also pave the way for future advancements in subgraph counting techniques. Overall, BEACON represents a significant step towards unifying and accelerating research in subgraph counting, encouraging innovative solutions and fostering a deeper understanding of the trade-offs between algorithmic and machine learning paradigms.\n        △ Less",
        "authors":"['Mohammad Matin Najafi', 'Xianju Zhu', 'Chrysanthi Kosyfaki', 'Laks V. S. Lakshmanan', 'Reynold Cheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can LLMs Leverage Observational Data? Towards Data-Driven Causal Discovery with LLMs",
        "abstract":"Causal discovery traditionally relies on statistical methods applied to observational data, often requiring large datasets and assumptions about underlying causal structures. Recent advancements in Large Language Models (LLMs) have introduced new possibilities for causal discovery by providing domain expert knowledge. However, it remains unclear whether LLMs can effectively process observational data for causal discovery. In this work, we explore the potential of LLMs for data-driven causal discovery by integrating observational data for LLM-based reasoning. Specifically, we examine whether LLMs can effectively utilize observational data through two prompting strategies: pairwise prompting and breadth first search (BFS)-based prompting. In both approaches, we incorporate the observational data directly into the prompt to assess LLMs' ability to infer causal relationships from such data. Experiments on benchmark datasets show that incorporating observational data enhances causal discovery, boosting F1 scores by up to 0.11 point using both pairwise and BFS LLM-based prompting, while outperforming traditional statistical causal discovery baseline by up to 0.52 points. Our findings highlight the potential and limitations of LLMs for data-driven causal discovery, demonstrating their ability to move beyond textual metadata and effectively interpret and utilize observational data for more informed causal reasoning. Our studies lays the groundwork for future advancements toward fully LLM-driven causal discovery.\n        △ Less",
        "authors":"['Yuni Susanti', 'Michael Färber']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-scale DeepOnet (Mscale-DeepOnet) for Mitigating Spectral Bias in Learning High Frequency Operators of Oscillatory Functions",
        "abstract":"In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce the spectral bias of the DeepOnet in learning high-frequency mapping between highly oscillatory functions, with an application to the nonlinear mapping between the coefficient of the Helmholtz equation and its solution. The Mscale-DeepOnet introduces the multiscale neural network in the branch and trunk networks of the original DeepOnet, the resulting Mscale-DeepOnet is shown to be able to capture various high-frequency components of the mapping itself and its image. Numerical results demonstrate the substantial improvement of the Mscale-DeepOnet for the problem of wave scattering in the high-frequency regime over the normal DeepOnet with a similar number of network parameters.\n        △ Less",
        "authors":"['Bo Wang', 'Lizuo Liu', 'Wei Cai']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transfer Learning for Temporal Link Prediction",
        "abstract":"Link prediction on graphs has applications spanning from recommender systems to drug discovery. Temporal link prediction (TLP) refers to predicting future links in a temporally evolving graph and adds additional complexity related to the dynamic nature of graphs. State-of-the-art TLP models incorporate memory modules alongside graph neural networks to learn both the temporal mechanisms of incoming nodes and the evolving graph topology. However, memory modules only store information about nodes seen at train time, and hence such models cannot be directly transferred to entirely new graphs at test time and deployment. In this work, we study a new transfer learning task for temporal link prediction, and develop transfer-effective methods for memory-laden models. Specifically, motivated by work showing the informativeness of structural signals for the TLP task, we augment a structural mapping module to the existing TLP model architectures, which learns a mapping from graph structural (topological) features to memory embeddings. Our work paves the way for a memory-free foundation model for TLP.\n        △ Less",
        "authors":"['Ayan Chatterjee', 'Barbara Ikica', 'Babak Ravandi', 'John Palowitch']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fast-Powerformer: A Memory-Efficient Transformer for Accurate Mid-Term Wind Power Forecasting",
        "abstract":"Wind power forecasting (WPF), as a significant research topic within renewable energy, plays a crucial role in enhancing the security, stability, and economic operation of power grids. However, due to the high stochasticity of meteorological factors (e.g., wind speed) and significant fluctuations in wind power output, mid-term wind power forecasting faces a dual challenge of maintaining high accuracy and computational efficiency. To address these issues, this paper proposes an efficient and lightweight mid-term wind power forecasting model, termed Fast-Powerformer. The proposed model is built upon the Reformer architecture, incorporating structural enhancements such as a lightweight Long Short-Term Memory (LSTM) embedding module, an input transposition mechanism, and a Frequency Enhanced Channel Attention Mechanism (FECAM). These improvements enable the model to strengthen temporal feature extraction, optimize dependency modeling across variables, significantly reduce computational complexity, and enhance sensitivity to periodic patterns and dominant frequency components. Experimental results conducted on multiple real-world wind farm datasets demonstrate that the proposed Fast-Powerformer achieves superior prediction accuracy and operational efficiency compared to mainstream forecasting approaches. Furthermore, the model exhibits fast inference speed and low memory consumption, highlighting its considerable practical value for real-world deployment scenarios.\n        △ Less",
        "authors":"['Mingyi Zhu', 'Zhaoxin Li', 'Qiao Lin', 'Li Ding']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards A Universal Graph Structural Encoder",
        "abstract":"Recent advancements in large-scale pre-training have shown the potential to learn generalizable representations for downstream tasks. In the graph domain, however, capturing and transferring structural information across different graph domains remains challenging, primarily due to the inherent differences in topological patterns across various contexts. Additionally, most existing models struggle to capture the complexity of rich graph structures, leading to inadequate exploration of the embedding space. To address these challenges, we propose GFSE, a universal graph structural encoder designed to capture transferable structural patterns across diverse domains such as molecular graphs, social networks, and citation networks. GFSE is the first cross-domain graph structural encoder pre-trained with multiple self-supervised learning objectives. Built on a Graph Transformer, GFSE incorporates attention mechanisms informed by graph inductive bias, enabling it to encode intricate multi-level and fine-grained topological features. The pre-trained GFSE produces generic and theoretically expressive positional and structural encoding for graphs, which can be seamlessly integrated with various downstream graph feature encoders, including graph neural networks for vectorized features and Large Language Models for text-attributed graphs. Comprehensive experiments on synthetic and real-world datasets demonstrate GFSE's capability to significantly enhance the model's performance while requiring substantially less task-specific fine-tuning. Notably, GFSE achieves state-of-the-art performance in 81.6% evaluated cases, spanning diverse graph models and datasets, highlighting its potential as a powerful and versatile encoder for graph-structured data.\n        △ Less",
        "authors":"['Jialin Chen', 'Haolan Zuo', 'Haoyu Peter Wang', 'Siqi Miao', 'Pan Li', 'Rex Ying']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs",
        "abstract":"Task arithmetic is a straightforward yet highly effective strategy for model merging, enabling the resultant model to exhibit multi-task capabilities. Recent research indicates that models demonstrating linearity enhance the performance of task arithmetic. In contrast to existing methods that rely on the global linearization of the model, we argue that this linearity already exists within the model's submodules. In particular, we present a statistical analysis and show that submodules (e.g., layers, self-attentions, and MLPs) exhibit significantly higher linearity than the overall model. Based on these findings, we propose an innovative model merging strategy that independently merges these submodules. Especially, we derive a closed-form solution for optimal merging weights grounded in the linear properties of these submodules. Experimental results demonstrate that our method consistently outperforms the standard task arithmetic approach and other established baselines across different model scales and various tasks. This result highlights the benefits of leveraging the linearity of submodules and provides a new perspective for exploring solutions for effective and practical multi-task model merging.\n        △ Less",
        "authors":"['Rui Dai', 'Sile Hu', 'Xu Shen', 'Yonggang Zhang', 'Xinmei Tian', 'Jieping Ye']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bridging Distribution Gaps in Time Series Foundation Model Pretraining with Prototype-Guided Normalization",
        "abstract":"Foundation models have achieved remarkable success across diverse machine-learning domains through large-scale pretraining on large, diverse datasets. However, pretraining on such datasets introduces significant challenges due to substantial mismatches in data distributions, a problem particularly pronounced with time series data. In this paper, we tackle this issue by proposing a domain-aware adaptive normalization strategy within the Transformer architecture. Specifically, we replace the traditional LayerNorm with a prototype-guided dynamic normalization mechanism (ProtoNorm), where learned prototypes encapsulate distinct data distributions, and sample-to-prototype affinity determines the appropriate normalization layer. This mechanism effectively captures the heterogeneity of time series characteristics, aligning pretrained representations with downstream tasks. Through comprehensive empirical evaluation, we demonstrate that our method significantly outperforms conventional pretraining techniques across both classification and forecasting tasks, while effectively mitigating the adverse effects of distribution shifts during pretraining. Incorporating ProtoNorm is as simple as replacing a single line of code. Extensive experiments on diverse real-world time series benchmarks validate the robustness and generalizability of our approach, advancing the development of more versatile time series foundation models.\n        △ Less",
        "authors":"['Peiliang Gong', 'Emadeldeen Eldele', 'Min Wu', 'Zhenghua Chen', 'Xiaoli Li', 'Daoqiang Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation of Credibility Perceptions of Visual Content",
        "abstract":"In today's visually dominated social media landscape, predicting the perceived credibility of visual content and understanding what drives human judgment are crucial for countering misinformation. However, these tasks are challenging due to the diversity and richness of visual features. We introduce a Large Language Model (LLM)-informed feature discovery framework that leverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and explain its reasoning. We extract and quantify interpretable features using targeted prompts and integrate them into machine learning models to improve credibility predictions. We tested this approach on 4,191 visual social media posts across eight topics in science, health, and politics, using credibility ratings from 5,355 crowdsourced workers. Our method outperformed zero-shot GPT-based predictions by 13 percent in R2, and revealed key features like information concreteness and image format. We discuss the implications for misinformation mitigation, visual credibility, and the role of LLMs in social science.\n        △ Less",
        "authors":"['Yilang Peng', 'Sijia Qian', 'Yingdan Lu', 'Cuihua Shen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration",
        "abstract":"In this paper, we study the theoretical properties of the projected Bellman equation (PBE) and two algorithms to solve this equation: linear Q-learning and approximate value iteration (AVI). We consider two sufficient conditions for the existence of a solution to PBE : strictly negatively row dominating diagonal (SNRDD) assumption and a condition motivated by the convergence of AVI. The SNRDD assumption also ensures the convergence of linear Q-learning, and its relationship with the convergence of AVI is examined. Lastly, several interesting observations on the solution of PBE are provided when using $ε$-greedy policy.\n        △ Less",
        "authors":"['Han-Dong Lim', 'Donghwan Lee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ICAFS: Inter-Client-Aware Feature Selection for Vertical Federated Learning",
        "abstract":"Vertical federated learning (VFL) enables a paradigm for vertically partitioned data across clients to collaboratively train machine learning models. Feature selection (FS) plays a crucial role in Vertical Federated Learning (VFL) due to the unique nature that data are distributed across multiple clients. In VFL, different clients possess distinct subsets of features for overlapping data samples, making the process of identifying and selecting the most relevant features a complex yet essential task. Previous FS efforts have primarily revolved around intra-client feature selection, overlooking vital feature interaction across clients, leading to subpar model outcomes. We introduce ICAFS, a novel multi-stage ensemble approach for effective FS in VFL by considering inter-client interactions. By employing conditional feature synthesis alongside multiple learnable feature selectors, ICAFS facilitates ensemble FS over these selectors using synthetic embeddings. This method bypasses the limitations of private gradient sharing and allows for model training using real data with refined embeddings. Experiments on multiple real-world datasets demonstrate that ICAFS surpasses current state-of-the-art methods in prediction accuracy.\n        △ Less",
        "authors":"['Ruochen Jin', 'Boning Tong', 'Shu Yang', 'Bojian Hou', 'Li Shen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?",
        "abstract":"With the rise of powerful foundation models, a pre-training-fine-tuning paradigm becomes increasingly popular these days: A foundation model is pre-trained using a huge amount of data from various sources, and then the downstream users only need to fine-tune and adapt it to specific downstream tasks. However, due to the high computation complexity of adversarial training, it is not feasible to fine-tune the foundation model to improve its robustness on the downstream task. Observing the above challenge, we want to improve the downstream robustness without updating\/accessing the weights in the foundation model. Inspired from existing literature in robustness inheritance (Kim et al., 2020), through theoretical investigation, we identify a close relationship between robust contrastive learning with the adversarial robustness of supervised learning. To further validate and utilize this theoretical insight, we design a simple-yet-effective robust auto-encoder as a data pre-processing method before feeding the data into the foundation model. The proposed approach has zero access to the foundation model when training the robust auto-encoder. Extensive experiments demonstrate the effectiveness of the proposed method in improving the robustness of downstream tasks, verifying the connection between the feature robustness (implied by small adversarial contrastive loss) and the robustness of the downstream task.\n        △ Less",
        "authors":"['Meiqi Liu', 'Zhuoqun Huang', 'Yue Xing']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations",
        "abstract":"Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a promising tool for generating semantic explanations of the decision-making processes in deep neural networks, having applications in both model improvement and understanding. It is vital that the explanation is accurate, or faithful, to the model, yet we identify several limitations of prior faithfulness metrics that inhibit an accurate evaluation; most notably, prior metrics involve only the set of concepts present, ignoring how they may be spatially distributed. We address these limitations with Surrogate Faithfulness (SF), an evaluation method that introduces a spatially-aware surrogate and two novel faithfulness metrics. Using SF, we produce Optimally Faithful (OF) explanations, where concepts are found that maximize faithfulness. Our experiments show that (1) adding spatial-awareness to prior U-CBEMs increases faithfulness in all cases; (2) OF produces significantly more faithful explanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's learned concepts generalize well to out-of-domain data and are more robust to adversarial examples, where prior U-CBEMs struggle.\n        △ Less",
        "authors":"['Shubham Kumar', 'Dwip Dalal', 'Narendra Ahuja']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Progressive Rock Music Classification",
        "abstract":"This study investigates the classification of progressive rock music, a genre characterized by complex compositions and diverse instrumentation, distinct from other musical styles. Addressing this Music Information Retrieval (MIR) task, we extracted comprehensive audio features, including spectrograms, Mel-Frequency Cepstral Coefficients (MFCCs), chromagrams, and beat positions from song snippets using the Librosa library. A winner-take-all voting strategy was employed to aggregate snippet-level predictions into final song classifications. We conducted a comparative analysis of various machine learning techniques. Ensemble methods, encompassing Bagging (Random Forest, ExtraTrees, Bagging Classifier) and Boosting (XGBoost, Gradient Boosting), were explored, utilizing Principal Component Analysis (PCA) for dimensionality reduction to manage computational constraints with high-dimensional feature sets. Additionally, deep learning approaches were investigated, including the development of custom 1D Convolutional Neural Network (1D CNN) architectures (named \"Zuck\" and \"Satya\") featuring specific layer configurations, normalization, and activation functions. Furthermore, we fine-tuned a state-of-the-art Audio Spectrogram Transformer (AST) model, leveraging its attention-based mechanisms for audio classification. Performance evaluation on validation and test sets revealed varying effectiveness across models, with ensemble methods like Extra Trees achieving test accuracies up to 76.38%. This research provides insights into the application and relative performance of diverse machine learning paradigms for the nuanced task of progressive rock genre classification.\n        △ Less",
        "authors":"['Arpan Nagar', 'Joseph Bensabat', 'Jokent Gaza', 'Moinak Dey']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FHBench: Towards Efficient and Personalized Federated Learning for Multimodal Healthcare",
        "abstract":"Federated Learning (FL) has emerged as an effective solution for multi-institutional collaborations without sharing patient data, offering a range of methods tailored for diverse applications. However, real-world medical datasets are often multimodal, and computational resources are limited, posing significant challenges for existing FL approaches. Recognizing these limitations, we developed the Federated Healthcare Benchmark(FHBench), a benchmark specifically designed from datasets derived from real-world healthcare applications. FHBench encompasses critical diagnostic tasks across domains such as the nervous, cardiovascular, and respiratory systems and general pathology, providing comprehensive support for multimodal healthcare evaluations and filling a significant gap in existing benchmarks. Building on FHBench, we introduced Efficient Personalized Federated Learning with Adaptive LoRA(EPFL), a personalized FL framework that demonstrates superior efficiency and effectiveness across various healthcare modalities. Our results highlight the robustness of FHBench as a benchmarking tool and the potential of EPFL as an innovative approach to advancing healthcare-focused FL, addressing key limitations of existing methods.\n        △ Less",
        "authors":"['Penghao Wang', 'Qian Chen', 'Teng Zhang', 'Yingwei Zhang', 'Wang Lu', 'Yiqiang Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Tabular foundation model to detect empathy from visual cues",
        "abstract":"Detecting empathy from video interactions is an emerging area of research. Video datasets, however, are often released as extracted features (i.e., tabular data) rather than raw footage due to privacy and ethical concerns. Prior research on such tabular datasets established tree-based classical machine learning approaches as the best-performing models. Motivated by the recent success of textual foundation models (i.e., large language models), we explore the use of tabular foundation models in empathy detection from tabular visual features. We experiment with two recent tabular foundation models $-$ TabPFN v2 and TabICL $-$ through in-context learning and fine-tuning setups. Our experiments on a public human-robot interaction benchmark demonstrate a significant boost in cross-subject empathy detection accuracy over several strong baselines (accuracy: $0.590 \\rightarrow 0.730$; AUC: $0.564 \\rightarrow 0.669$). In addition to performance improvement, we contribute novel insights and an evaluation setup to ensure generalisation on unseen subjects in this public benchmark. As the practice of releasing video features as tabular datasets is likely to persist due to privacy constraints, our findings will be widely applicable to future empathy detection video datasets as well.\n        △ Less",
        "authors":"['Md Rakibul Hasan', 'Shafin Rahman', 'Md Zakir Hossain', 'Aneesh Krishna', 'Tom Gedeon']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Power-scaled Bayesian Inference with Score-based Generative mModels",
        "abstract":"We propose a score-based generative algorithm for sampling from power-scaled priors and likelihoods within the Bayesian inference framework. Our algorithm enables flexible control over prior-likelihood influence without requiring retraining for different power-scaling configurations. Specifically, we focus on synthesizing seismic velocity models conditioned on imaged seismic. Our method enables sensitivity analysis by sampling from intermediate power posteriors, allowing us to assess the relative influence of the prior and likelihood on samples of the posterior distribution. Through a comprehensive set of experiments, we evaluate the effects of varying the power parameter in different settings: applying it solely to the prior, to the likelihood of a Bayesian formulation, and to both simultaneously. The results show that increasing the power of the likelihood up to a certain threshold improves the fidelity of posterior samples to the conditioning data (e.g., seismic images), while decreasing the prior power promotes greater structural diversity among samples. Moreover, we find that moderate scaling of the likelihood leads to a reduced shot data residual, confirming its utility in posterior refinement.\n        △ Less",
        "authors":"['Huseyin Tuna Erdinc', 'Yunlin Zeng', 'Abhinav Prakash Gahlot', 'Felix J. Herrmann']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Wasserstein Distributionally Robust Regret Optimization",
        "abstract":"Distributionally Robust Optimization (DRO) is a popular framework for decision-making under uncertainty, but its adversarial nature can lead to overly conservative solutions. To address this, we study ex-ante Distributionally Robust Regret Optimization (DRRO), focusing on Wasserstein-based ambiguity sets which are popular due to their links to regularization and machine learning. We provide a systematic analysis of Wasserstein DRRO, paralleling known results for Wasserstein DRO. Under smoothness and regularity conditions, we show that Wasserstein DRRO coincides with Empirical Risk Minimization (ERM) up to first-order terms, and exactly so in convex quadratic settings. We revisit the Wasserstein DRRO newsvendor problem, where the loss is the maximum of two linear functions of demand and decision. Extending [25], we show that the regret can be computed by maximizing two one-dimensional concave functions. For more general loss functions involving the maximum of multiple linear terms in multivariate random variables and decision vectors, we prove that computing the regret and thus also the DRRO policy is NP-hard. We then propose a convex relaxation for these more general Wasserstein DRRO problems and demonstrate its strong empirical performance. Finally, we provide an upper bound on the optimality gap of our relaxation and show it improves over recent alternatives.\n        △ Less",
        "authors":"['Lukas-Benedikt Fiechtner', 'Jose Blanchet']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SonicSieve: Bringing Directional Speech Extraction to Smartphones Using Acoustic Microstructures",
        "abstract":"Imagine placing your smartphone on a table in a noisy restaurant and clearly capturing the voices of friends seated around you, or recording a lecturer's voice with clarity in a reverberant auditorium. We introduce SonicSieve, the first intelligent directional speech extraction system for smartphones using a bio-inspired acoustic microstructure. Our passive design embeds directional cues onto incoming speech without any additional electronics. It attaches to the in-line mic of low-cost wired earphones which can be attached to smartphones. We present an end-to-end neural network that processes the raw audio mixtures in real-time on mobile devices. Our results show that SonicSieve achieves a signal quality improvement of 5.0 dB when focusing on a 30° angular region. Additionally, the performance of our system based on only two microphones exceeds that of conventional 5-microphone arrays.\n        △ Less",
        "authors":"['Kuang Yuan', 'Yifeng Wang', 'Xiyuxing Zhang', 'Chengyi Shen', 'Swarun Kumar', 'Justin Chan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Visual Language Models show widespread visual deficits on neuropsychological tests",
        "abstract":"Visual Language Models (VLMs) show remarkable performance in visual reasoning tasks, successfully tackling college-level challenges that require high-level understanding of images. However, some recent reports of VLMs struggling to reason about elemental visual concepts like orientation, position, continuity, and occlusion suggest a potential gulf between human and VLM vision. Here we use the toolkit of neuropsychology to systematically assess the capabilities of three state-of-the-art VLMs across visual domains. Using 51 tests drawn from six clinical and experimental batteries, we characterise the visual abilities of leading VLMs relative to normative performance in healthy adults. While the models excel in straightforward object recognition tasks, we find widespread deficits in low- and mid-level visual abilities that would be considered clinically significant in humans. These selective deficits, profiled through validated test batteries, suggest that an artificial system can achieve complex object recognition without developing foundational visual concepts that in humans require no explicit training.\n        △ Less",
        "authors":"['Gene Tangtartharakul', 'Katherine R. Storrs']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Network Emulation of the Classical Limit in Quantum Systems via Learned Observable Mappings",
        "abstract":"The classical limit of quantum mechanics, formally investigated through frameworks like strict deformation quantization, remains a profound area of inquiry in the philosophy of physics. This paper explores a computational approach employing a neural network to emulate the emergence of classical behavior from the quantum harmonic oscillator as Planck's constant $\\hbar$ approaches zero. We develop and train a neural network architecture to learn the mapping from initial expectation values and $\\hbar$ to the time evolution of the expectation value of position. By analyzing the network's predictions across different regimes of hbar, we aim to provide computational insights into the nature of the quantum-classical transition. This work demonstrates the potential of machine learning as a complementary tool for exploring foundational questions in quantum mechanics and its classical limit.\n        △ Less",
        "authors":"['Kamran Majid']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AtlasD: Automatic Local Symmetry Discovery",
        "abstract":"Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they fail to consider the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we formalize the notion of local symmetry as atlas equivariance. Our proposed pipeline, automatic local symmetry discovery (AtlasD), recovers the local symmetries of a function by training local predictor networks and then learning a Lie group basis to which the predictors are equivariant. We demonstrate AtlasD is capable of discovering local symmetry groups with multiple connected components in top-quark tagging and partial differential equation experiments. The discovered local symmetry is shown to be a useful inductive bias that improves the performance of downstream tasks in climate segmentation and vision tasks.\n        △ Less",
        "authors":"['Manu Bhat', 'Jonghyun Park', 'Jianke Yang', 'Nima Dehmamy', 'Robin Walters', 'Rose Yu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Collaborative Bayesian Optimization via Wasserstein Barycenters",
        "abstract":"Motivated by the growing need for black-box optimization and data privacy, we introduce a collaborative Bayesian optimization (BO) framework that addresses both of these challenges. In this framework agents work collaboratively to optimize a function they only have oracle access to. In order to mitigate against communication and privacy constraints, agents are not allowed to share their data but can share their Gaussian process (GP) surrogate models. To enable collaboration under these constraints, we construct a central model to approximate the objective function by leveraging the concept of Wasserstein barycenters of GPs. This central model integrates the shared models without accessing the underlying data. A key aspect of our approach is a collaborative acquisition function that balances exploration and exploitation, allowing for the optimization of decision variables collaboratively in each iteration. We prove that our proposed algorithm is asymptotically consistent and that its implementation via Monte Carlo methods is numerically accurate. Through numerical experiments, we demonstrate that our approach outperforms other baseline collaborative frameworks and is competitive with centralized approaches that do not consider data privacy.\n        △ Less",
        "authors":"['Donglin Zhan', 'Haoting Zhang', 'Rhonda Righter', 'Zeyu Zheng', 'James Anderson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients",
        "abstract":"As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low\/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.\n        △ Less",
        "authors":"['Ming Li', 'Yanhong Li', 'Ziyue Li', 'Tianyi Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Auto-Test: Learning Semantic-Domain Constraints for Unsupervised Error Detection in Tables",
        "abstract":"Data cleaning is a long-standing challenge in data management. While powerful logic and statistical algorithms have been developed to detect and repair data errors in tables, existing algorithms predominantly rely on domain-experts to first manually specify data-quality constraints specific to a given table, before data cleaning algorithms can be applied.\n  In this work, we propose a new class of data-quality constraints that we call Semantic-Domain Constraints, which can be reliably inferred and automatically applied to any tables, without requiring domain-experts to manually specify on a per-table basis. We develop a principled framework to systematically learn such constraints from table corpora using large-scale statistical tests, which can further be distilled into a core set of constraints using our optimization framework, with provable quality guarantees. Extensive evaluations show that this new class of constraints can be used to both (1) directly detect errors on real tables in the wild, and (2) augment existing expert-driven data-cleaning techniques as a new class of complementary constraints.\n  Our extensively labeled benchmark dataset with 2400 real data columns, as well as our code are available at https:\/\/github.com\/qixuchen\/AutoTest to facilitate future research.\n        △ Less",
        "authors":"['Qixu Chen', 'Yeye He', 'Raymond Chi-Wing Wong', 'Weiwei Cui', 'Song Ge', 'Haidong Zhang', 'Dongmei Zhang', 'Surajit Chaudhuri']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models",
        "abstract":"Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. We compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions. These findings highlight the importance of transparent decision processes in safety-critical domains and offer a promising direction for developing more interpretable autonomous driving systems.\n        △ Less",
        "authors":"['Amirhosein Chahe', 'Lifeng Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"auto-fpt: Automating Free Probability Theory Calculations for Machine Learning Theory",
        "abstract":"A large part of modern machine learning theory often involves computing the high-dimensional expected trace of a rational expression of large rectangular random matrices. To symbolically compute such quantities using free probability theory, we introduce auto-fpt, a lightweight Python and SymPy-based tool that can automatically produce a reduced system of fixed-point equations which can be solved for the quantities of interest, and effectively constitutes a theory. We overview the algorithmic ideas underlying auto-fpt and its applications to various interesting problems, such as the high-dimensional error of linearized feed-forward neural networks, recovering well-known results. We hope that auto-fpt streamlines the majority of calculations involved in high-dimensional analysis, while helping the machine learning community reproduce known and uncover new phenomena.\n        △ Less",
        "authors":"['Arjun Subramonian', 'Elvis Dohmatob']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Epistemic Uncertainty-aware Recommendation Systems via Bayesian Deep Ensemble Learning",
        "abstract":"Recommending items to users has long been a fundamental task, and studies have tried to improve it ever since. Most well-known models commonly employ representation learning to map users and items into a unified embedding space for matching assessment. These approaches have primary limitations, especially when dealing with explicit feedback and sparse data contexts. Two primary limitations are their proneness to overfitting and failure to incorporate epistemic uncertainty in predictions. To address these problems, we propose a novel Bayesian Deep Ensemble Collaborative Filtering method named BDECF. To improve model generalization and quality, we utilize Bayesian Neural Networks, which incorporate uncertainty within their weight parameters. In addition, we introduce a new interpretable non-linear matching approach for the user and item embeddings, leveraging the advantages of the attention mechanism. Furthermore, we endorse the implementation of an ensemble-based supermodel to generate more robust and reliable predictions, resulting in a more complete model. Empirical evaluation through extensive experiments and ablation studies across a range of publicly accessible real-world datasets with differing sparsity characteristics confirms our proposed method's effectiveness and the importance of its components.\n        △ Less",
        "authors":"['Radin Cheraghi', 'Amir Mohammad Mahfoozi', 'Sepehr Zolfaghari', 'Mohammadshayan Shabani', 'Maryam Ramezani', 'Hamid R. Rabiee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Time-varying EEG spectral power predicts evoked and spontaneous fMRI motor brain activity",
        "abstract":"Simultaneous EEG-fMRI recordings are increasingly used to investigate brain activity by leveraging the complementary high spatial and high temporal resolution of fMRI and EEG signals respectively. It remains unclear, however, to what degree these two imaging modalities capture shared information about neural activity. Here, we investigate whether it is possible to predict both task-evoked and spontaneous fMRI signals of motor brain networks from EEG time-varying spectral power using interpretable models trained for individual subjects with Sparse Group Lasso regularization. Critically, we test the trained models on data acquired from each subject on a different day and obtain statistical validation by comparison with appropriate null models as well as the conventional EEG sensorimotor rhythm. We find significant prediction results in most subjects, although less frequently for resting-state compared to task-based conditions. Furthermore, we interpret the model learned parameters to understand representations of EEG-fMRI coupling in terms of predictive EEG channels, frequencies, and haemodynamic delays. In conclusion, our work provides evidence of the ability to predict fMRI motor brain activity from EEG recordings alone across different days, in both task-evoked and spontaneous conditions, with statistical significance in individual subjects. These results present great potential for translation to EEG neurofeedback applications.\n        △ Less",
        "authors":"['Neil Mehta', 'Ines Goncalves', 'Alberto Montagna', 'Mathis Fleury', 'Gustavo Caetano', 'Ines Esteves', 'Athanasios Vourvopoulos', 'Pulkit Grover', 'Patricia Figueiredo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Real-time Seafloor Segmentation and Mapping",
        "abstract":"Posidonia oceanica meadows are a species of seagrass highly dependent on rocks for their survival and conservation. In recent years, there has been a concerning global decline in this species, emphasizing the critical need for efficient monitoring and assessment tools. While deep learning-based semantic segmentation and visual automated monitoring systems have shown promise in a variety of applications, their performance in underwater environments remains challenging due to complex water conditions and limited datasets. This paper introduces a framework that combines machine learning and computer vision techniques to enable an autonomous underwater vehicle (AUV) to inspect the boundaries of Posidonia oceanica meadows autonomously. The framework incorporates an image segmentation module using an existing Mask R-CNN model and a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a new class dedicated to rocks is introduced to enhance the existing model, aiming to contribute to a comprehensive monitoring approach and provide a deeper understanding of the intricate interactions between the meadow and its surrounding environment. The image segmentation model is validated using real underwater images, while the overall inspection framework is evaluated in a realistic simulation environment, replicating actual monitoring scenarios with real underwater images. The results demonstrate that the proposed framework enables the AUV to autonomously accomplish the main tasks of underwater inspection and segmentation of rocks. Consequently, this work holds significant potential for the conservation and protection of marine environments, providing valuable insights into the status of Posidonia oceanica meadows and supporting targeted preservation efforts\n        △ Less",
        "authors":"['Michele Grimaldi', 'Nouf Alkaabi', 'Francesco Ruscio', 'Sebastian Realpe Rua', 'Rafael Garcia', 'Nuno Gracias']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hearing Anywhere in Any Environment",
        "abstract":"In mixed reality applications, a realistic acoustic experience in spatial environments is as crucial as the visual experience for achieving true immersion. Despite recent advances in neural approaches for Room Impulse Response (RIR) estimation, most existing methods are limited to the single environment on which they are trained, lacking the ability to generalize to new rooms with different geometries and surface materials. We aim to develop a unified model capable of reconstructing the spatial acoustic experience of any environment with minimum additional measurements. To this end, we present xRIR, a framework for cross-room RIR prediction. The core of our generalizable approach lies in combining a geometric feature extractor, which captures spatial context from panorama depth images, with a RIR encoder that extracts detailed acoustic features from only a few reference RIR samples. To evaluate our method, we introduce ACOUSTICROOMS, a new dataset featuring high-fidelity simulation of over 300,000 RIRs from 260 rooms. Experiments show that our method strongly outperforms a series of baselines. Furthermore, we successfully perform sim-to-real transfer by evaluating our model on four real-world environments, demonstrating the generalizability of our approach and the realism of our dataset.\n        △ Less",
        "authors":"['Xiulong Liu', 'Anurag Kumar', 'Paul Calamia', 'Sebastia V. Amengual', 'Calvin Murdock', 'Ishwarya Ananthabhotla', 'Philip Robinson', 'Eli Shlizerman', 'Vamsi Krishna Ithapu', 'Ruohan Gao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates",
        "abstract":"The rapid growth of intelligent connected vehicles (ICVs) and integrated vehicle-road-cloud systems has increased the demand for accurate, real-time HD map updates. However, ensuring map reliability remains challenging due to inconsistencies in crowdsourced data, which suffer from motion blur, lighting variations, adverse weather, and lane marking degradation. This paper introduces CleanMAP, a Multimodal Large Language Model (MLLM)-based distillation framework designed to filter and refine crowdsourced data for high-confidence HD map updates. CleanMAP leverages an MLLM-driven lane visibility scoring model that systematically quantifies key visual parameters, assigning confidence scores (0-10) based on their impact on lane detection. A novel dynamic piecewise confidence-scoring function adapts scores based on lane visibility, ensuring strong alignment with human evaluations while effectively filtering unreliable data. To further optimize map accuracy, a confidence-driven local map fusion strategy ranks and selects the top-k highest-scoring local maps within an optimal confidence range (best score minus 10%), striking a balance between data quality and quantity. Experimental evaluations on a real-world autonomous vehicle dataset validate CleanMAP's effectiveness, demonstrating that fusing the top three local maps achieves the lowest mean map update error of 0.28m, outperforming the baseline (0.37m) and meeting stringent accuracy thresholds (<= 0.32m). Further validation with real-vehicle data confirms 84.88% alignment with human evaluators, reinforcing the model's robustness and reliability. This work establishes CleanMAP as a scalable and deployable solution for crowdsourced HD map updates, ensuring more precise and reliable autonomous navigation. The code will be available at https:\/\/Ankit-Zefan.github.io\/CleanMap\/\n        △ Less",
        "authors":"['Ankit Kumar Shaw', 'Kun Jiang', 'Tuopu Wen', 'Chandan Kumar Sah', 'Yining Shi', 'Mengmeng Yang', 'Diange Yang', 'Xiaoli Lian']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization",
        "abstract":"As model sizes grow, finding efficient and cost-effective hyperparameter optimization (HPO) methods becomes increasingly crucial for deep learning pipelines. While multi-fidelity HPO (MF-HPO) trades off computational resources required for DL training with lower fidelity estimations, existing fidelity sources often fail under lower compute and memory constraints. We propose a novel fidelity source: the number of layers that are trained or frozen during training. For deep networks, this approach offers significant compute and memory savings while preserving rank correlations between hyperparameters at low fidelities compared to full model training. We demonstrate this in our empirical evaluation across ResNets and Transformers and additionally analyze the utility of frozen layers as a fidelity in using GPU resources as a fidelity in HPO, and for a combined MF-HPO with other fidelity sources. This contribution opens new applications for MF-HPO with hardware resources as a fidelity and creates opportunities for improved algorithms navigating joint fidelity spaces.\n        △ Less",
        "authors":"['Timur Carstensen', 'Neeratyoy Mallik', 'Frank Hutter', 'Martin Rapp']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cross-Problem Parameter Transfer in Quantum Approximate Optimization Algorithm: A Machine Learning Approach",
        "abstract":"Quantum Approximate Optimization Algorithm (QAOA) is one of the most promising candidates to achieve the quantum advantage in solving combinatorial optimization problems. The process of finding a good set of variational parameters in the QAOA circuit has proven to be challenging due to multiple factors, such as barren plateaus. As a result, there is growing interest in exploiting parameter transferability, where parameter sets optimized for one problem instance are transferred to another that could be more complex either to estimate the solution or to serve as a warm start for further optimization. But can we transfer parameters from one class of problems to another? Leveraging parameter sets learned from a well-studied class of problems could help navigate the less studied one, reducing optimization overhead and mitigating performance pitfalls. In this paper, we study whether pretrained QAOA parameters of MaxCut can be used as is or to warm start the Maximum Independent Set (MIS) circuits. Specifically, we design machine learning models to find good donor candidates optimized on MaxCut and apply their parameters to MIS acceptors. Our experimental results show that such parameter transfer can significantly reduce the number of optimization iterations required while achieving comparable approximation ratios.\n        △ Less",
        "authors":"['Kien X. Nguyen', 'Bao Bach', 'Ilya Safro']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving",
        "abstract":"Deploying large language models (LLMs) presents critical challenges due to the inherent trade-offs associated with key performance metrics, such as latency, accuracy, and throughput. Typically, gains in one metric is accompanied with degradation in others. Early-Exit LLMs (EE-LLMs) efficiently navigate this trade-off space by skipping some of the later model layers when it confidently finds an output token early, thus reducing latency without impacting accuracy. However, as the early exits taken depend on the task and are unknown apriori to request processing, EE-LLMs conservatively load the entire model, limiting resource savings and throughput. Also, current frameworks statically select a model for a user task, limiting our ability to adapt to changing nature of the input queries.\n  We propose HELIOS to address these challenges. First, HELIOS shortlists a set of candidate LLMs, evaluates them using a subset of prompts, gathering telemetry data in real-time. Second, HELIOS uses the early exit data from these evaluations to greedily load the selected model only up to a limited number of layers. This approach yields memory savings which enables us to process more requests at the same time, thereby improving throughput. Third, HELIOS monitors and periodically reassesses the performance of the candidate LLMs and if needed, switches to another model that can service incoming queries more efficiently (such as using fewer layers without lowering accuracy). Our evaluations show that HELIOS achieves 1.48$\\times$ throughput, 1.10$\\times$ energy-efficiency, 1.39$\\times$ lower response time, and 3.7$\\times$ improvements in inference batch sizes compared to the baseline, when optimizing for the respective service level objectives.\n        △ Less",
        "authors":"['Avinash Kumar', 'Shashank Nag', 'Jason Clemons', 'Lizy John', 'Poulami Das']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Deep Operator Networks (DeepONet) for Acoustic Full Waveform Inversion (FWI)",
        "abstract":"Full Waveform Inversion (FWI) is an important geophysical technique considered in subsurface property prediction. It solves the inverse problem of predicting high-resolution Earth interior models from seismic data. Traditional FWI methods are computationally demanding. Inverse problems in geophysics often face challenges of non-uniqueness due to limited data, as data are often collected only on the surface. In this study, we introduce a novel methodology that leverages Deep Operator Networks (DeepONet) to attempt to improve both the efficiency and accuracy of FWI. The proposed DeepONet methodology inverts seismic waveforms for the subsurface velocity field. This approach is able to capture some key features of the subsurface velocity field. We have shown that the architecture can be applied to noisy seismic data with an accuracy that is better than some other machine learning methods. We also test our proposed method with out-of-distribution prediction for different velocity models. The proposed DeepONet shows comparable and better accuracy in some velocity models than some other machine learning methods. To improve the FWI workflow, we propose using the DeepONet output as a starting model for conventional FWI and that it may improve FWI performance. While we have only shown that DeepONet facilitates faster convergence than starting with a homogeneous velocity field, it may have some benefits compared to other approaches to constructing starting models. This integration of DeepONet into FWI may accelerate the inversion process and may also enhance its robustness and reliability.\n        △ Less",
        "authors":"['Kamaljyoti Nath', 'Khemraj Shukla', 'Victor C. Tsai', 'Umair bin Waheed', 'Christian Huber', 'Omer Alpak', 'Chuen-Song Chen', 'Ligang Lu', 'Amik St-Cyr']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Distinct hydrologic response patterns and trends worldwide revealed by physics-embedded learning",
        "abstract":"To track rapid changes within our water sector, Global Water Models (GWMs) need to realistically represent hydrologic systems' response patterns - such as baseflow fraction - but are hindered by their limited ability to learn from data. Here we introduce a high-resolution physics-embedded big-data-trained model as a breakthrough in reliably capturing characteristic hydrologic response patterns ('signatures') and their shifts. By realistically representing the long-term water balance, the model revealed widespread shifts - up to ~20% over 20 years - in fundamental green-blue-water partitioning and baseflow ratios worldwide. Shifts in these response patterns, previously considered static, contributed to increasing flood risks in northern mid-latitudes, heightening water supply stresses in southern subtropical regions, and declining freshwater inputs to many European estuaries, all with ecological implications. With more accurate simulations at monthly and daily scales than current operational systems, this next-generation model resolves large, nonlinear seasonal runoff responses to rainfall ('elasticity') and streamflow flashiness in semi-arid and arid regions. These metrics highlight regions with management challenges due to large water supply variability and high climate sensitivity, but also provide tools to forecast seasonal water availability. This capability newly enables global-scale models to deliver reliable and locally relevant insights for water management.\n        △ Less",
        "authors":"['Haoyu Ji', 'Yalan Song', 'Tadd Bindas', 'Chaopeng Shen', 'Yuan Yang', 'Ming Pan', 'Jiangtao Liu', 'Farshid Rahmani', 'Ather Abbas', 'Hylke Beck', 'Yoshihide Wada', 'Kathryn Lawson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PDSP-Bench: A Benchmarking System for Parallel and Distributed Stream Processing",
        "abstract":"The paper introduces PDSP-Bench, a novel benchmarking system designed for a systematic understanding of performance of parallel stream processing in a distributed environment. Such an understanding is essential for determining how Stream Processing Systems (SPS) use operator parallelism and the available resources to process massive workloads of modern applications. Existing benchmarking systems focus on analyzing SPS using queries with sequential operator pipelines within a homogeneous centralized environment. Quite differently, PDSP-Bench emphasizes the aspects of parallel stream processing in a distributed heterogeneous environment and simultaneously allows the integration of machine learning models for SPS workloads. In our results, we benchmark a well-known SPS, Apache Flink, using parallel query structures derived from real-world applications and synthetic queries to show the capabilities of PDSP-Bench towards parallel stream processing. Moreover, we compare different learned cost models using generated SPS workloads on PDSP-Bench by showcasing their evaluations on model and training efficiency. We present key observations from our experiments using PDSP-Bench that highlight interesting trends given different query workloads, such as non-linearity and paradoxical effects of parallelism on the performance.\n        △ Less",
        "authors":"['Pratyush Agnihotri', 'Boris Koldehofe', 'Roman Heinrich', 'Carsten Binnig', 'Manisha Luthra']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Jailbreak Tax: How Useful are Your Jailbreak Outputs?",
        "abstract":"Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs. In this paper, we ask whether the model outputs produced by existing jailbreaks are actually useful. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions? Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math). Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the jailbreak tax. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy. Overall, our work proposes the jailbreak tax as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks. We make the benchmark available at https:\/\/github.com\/ethz-spylab\/jailbreak-tax\n        △ Less",
        "authors":"['Kristina Nikolić', 'Luze Sun', 'Jie Zhang', 'Florian Tramèr']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Keyword Extraction, and Aspect Classification in Sinhala, English, and Code-Mixed Content",
        "abstract":"Brand reputation in the banking sector is maintained through insightful analysis of customer opinion on code-mixed and multilingual content. Conventional NLP models misclassify or ignore code-mixed text, when mix with low resource languages such as Sinhala-English and fail to capture domain-specific knowledge. This study introduces a hybrid NLP method to improve keyword extraction, content filtering, and aspect-based classification of banking content. Keyword extraction in English is performed with a hybrid approach comprising a fine-tuned SpaCy NER model, FinBERT-based KeyBERT embeddings, YAKE, and EmbedRank, which results in a combined accuracy of 91.2%. Code-mixed and Sinhala keywords are extracted using a fine-tuned XLM-RoBERTa model integrated with a domain-specific Sinhala financial vocabulary, and it results in an accuracy of 87.4%. To ensure data quality, irrelevant comment filtering was performed using several models, with the BERT-base-uncased model achieving 85.2% for English and XLM-RoBERTa 88.1% for Sinhala, which was better than GPT-4o, SVM, and keyword-based filtering. Aspect classification followed the same pattern, with the BERT-base-uncased model achieving 87.4% for English and XLM-RoBERTa 85.9% for Sinhala, both exceeding GPT-4 and keyword-based approaches. These findings confirm that fine-tuned transformer models outperform traditional methods in multilingual financial text analysis. The present framework offers an accurate and scalable solution for brand reputation monitoring in code-mixed and low-resource banking environments.\n        △ Less",
        "authors":"['F. A. Rizvi', 'T. Navojith', 'A. M. N. H. Adhikari', 'W. P. U. Senevirathna', 'Dharshana Kasthurirathna', 'Lakmini Abeywardhana']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning",
        "abstract":"In this paper, we present a multi-agent reinforcement learning (MARL) framework for optimizing tissue repair processes using engineered biological agents. Our approach integrates: (1) stochastic reaction-diffusion systems modeling molecular signaling, (2) neural-like electrochemical communication with Hebbian plasticity, and (3) a biologically informed reward function combining chemical gradient tracking, neural synchronization, and robust penalties. A curriculum learning scheme guides the agent through progressively complex repair scenarios. In silico experiments demonstrate emergent repair strategies, including dynamic secretion control and spatial coordination.\n        △ Less",
        "authors":"['Muhammad Al-Zafar Khan', 'Jamal Al-Karaki']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Perturbed State Space Feature Encoders for Optical Flow with Event Cameras",
        "abstract":"With their motion-responsive nature, event-based cameras offer significant advantages over traditional cameras for optical flow estimation. While deep learning has improved upon traditional methods, current neural networks adopted for event-based optical flow still face temporal and spatial reasoning limitations. We propose Perturbed State Space Feature Encoders (P-SSE) for multi-frame optical flow with event cameras to address these challenges. P-SSE adaptively processes spatiotemporal features with a large receptive field akin to Transformer-based methods, while maintaining the linear computational complexity characteristic of SSMs. However, the key innovation that enables the state-of-the-art performance of our model lies in our perturbation technique applied to the state dynamics matrix governing the SSM system. This approach significantly improves the stability and performance of our model. We integrate P-SSE into a framework that leverages bi-directional flows and recurrent connections, expanding the temporal context of flow prediction. Evaluations on DSEC-Flow and MVSEC datasets showcase P-SSE's superiority, with 8.48% and 11.86% improvements in EPE performance, respectively.\n        △ Less",
        "authors":"['Gokul Raju Govinda Raju', 'Nikola Zubić', 'Marco Cannici', 'Davide Scaramuzza']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Emotion Alignment: Discovering the Gap Between Social Media and Real-World Sentiments in Persian Tweets and Images",
        "abstract":"In contemporary society, widespread social media usage is evident in people's daily lives. Nevertheless, disparities in emotional expressions between the real world and online platforms can manifest. We comprehensively analyzed Persian community on X to explore this phenomenon. An innovative pipeline was designed to measure the similarity between emotions in the real world compared to social media. Accordingly, recent tweets and images of participants were gathered and analyzed using Transformers-based text and image sentiment analysis modules. Each participant's friends also provided insights into the their real-world emotions. A distance criterion was used to compare real-world feelings with virtual experiences. Our study encompassed N=105 participants, 393 friends who contributed their perspectives, over 8,300 collected tweets, and 2,000 media images. Results indicated a 28.67% similarity between images and real-world emotions, while tweets exhibited a 75.88% alignment with real-world feelings. Additionally, the statistical significance confirmed that the observed disparities in sentiment proportions.\n        △ Less",
        "authors":"['Sina Elahimanesh', 'Mohammadali Mohammadkhani', 'Shohreh Kasaei']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transfer Learning Assisted XgBoost For Adaptable Cyberattack Detection In Battery Packs",
        "abstract":"Optimal charging of electric vehicle (EVs) depends heavily on reliable sensor measurements from the battery pack to the cloud-controller of the smart charging station. However, an adversary could corrupt the voltage sensor data during transmission, potentially causing local to wide-scale disruptions. Therefore, it is essential to detect sensor cyberattacks in real-time to ensure secure EV charging, and the developed algorithms must be readily adaptable to variations, including pack configurations. To tackle these challenges, we propose adaptable fine-tuning of an XgBoost-based cell-level model using limited pack-level data to use for voltage prediction and residual generation. We used battery cell and pack data from high-fidelity charging experiments in PyBaMM and `liionpack' package to train and test the detection algorithm. The algorithm's performance has been evaluated for two large-format battery packs under sensor swapping and replay attacks. The simulation results also highlight the adaptability and efficacy of our proposed detection algorithm.\n        △ Less",
        "authors":"['Sanchita Ghosh', 'Tanushree Roy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MatterTune: An Integrated, User-Friendly Platform for Fine-Tuning Atomistic Foundation Models to Accelerate Materials Simulation and Discovery",
        "abstract":"Geometric machine learning models such as graph neural networks have achieved remarkable success in recent years in chemical and materials science research for applications such as high-throughput virtual screening and atomistic simulations. The success of these models can be attributed to their ability to effectively learn latent representations of atomic structures directly from the training data. Conversely, this also results in high data requirements for these models, hindering their application to problems which are data sparse which are common in this domain. To address this limitation, there is a growing development in the area of pre-trained machine learning models which have learned general, fundamental, geometric relationships in atomistic data, and which can then be fine-tuned to much smaller application-specific datasets. In particular, models which are pre-trained on diverse, large-scale atomistic datasets have shown impressive generalizability and flexibility to downstream applications, and are increasingly referred to as atomistic foundation models. To leverage the untapped potential of these foundation models, we introduce MatterTune, a modular and extensible framework that provides advanced fine-tuning capabilities and seamless integration of atomistic foundation models into downstream materials informatics and simulation workflows, thereby lowering the barriers to adoption and facilitating diverse applications in materials science. In its current state, MatterTune supports a number of state-of-the-art foundation models such as ORB, MatterSim, JMP, and EquformerV2, and hosts a wide range of features including a modular and flexible design, distributed and customizable fine-tuning, broad support for downstream informatics tasks, and more.\n        △ Less",
        "authors":"['Lingyu Kong', 'Nima Shoghi', 'Guoxiang Hu', 'Pan Li', 'Victor Fung']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Contractivity of Stochastic Interpolation Flow",
        "abstract":"We investigate stochastic interpolation, a recently introduced framework for high dimensional sampling which bears many similarities to diffusion modeling. Stochastic interpolation generates a data sample by first randomly initializing a particle drawn from a simple base distribution, then simulating deterministic or stochastic dynamics such that in finite time the particle's distribution converges to the target. We show that for a Gaussian base distribution and a strongly log-concave target distribution, the stochastic interpolation flow map is Lipschitz with a sharp constant which matches that of Caffarelli's theorem for optimal transport maps. We are further able to construct Lipschitz transport maps between non-Gaussian distributions, generalizing some recent constructions in the literature on transport methods for establishing functional inequalities. We discuss the practical implications of our theorem for the sampling and estimation problems required by stochastic interpolation.\n        △ Less",
        "authors":"['Max Daniels']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Weight-of-Thought Reasoning: Exploring Neural Network Weights for Enhanced LLM Reasoning",
        "abstract":"Large language models (LLMs) have demonstrated remarkable reasoning capabilities when prompted with strategies such as Chain-of-Thought (CoT). However, these approaches focus on token-level output without considering internal weight dynamics. We introduce Weight-of-Thought (WoT) reasoning, a novel approach that examines neural network weights before inference to identify reasoning pathways. Unlike existing methods, WoT explores the weight space through graph-based message passing, multi-step reasoning processes, and attention mechanisms. Our implementation creates an interconnected graph of reasoning nodes. Experiments on diverse reasoning tasks (syllogistic, mathematical, algebraic, combinatorial, and geometric) demonstrate that WoT achieves superior performance compared to traditional methods, particularly for complex problems. This approach leads to both improved performance and greater interpretability of the reasoning process, offering a promising direction for enhancing LLM reasoning capabilities.\n        △ Less",
        "authors":"['Saif Punjwani', 'Larry Heck']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Better Estimation of the KL Divergence Between Language Models",
        "abstract":"Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to the use of sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance, and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is also unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially in practice. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient.\n        △ Less",
        "authors":"['Afra Amini', 'Tim Vieira', 'Ryan Cotterell']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SPreV",
        "abstract":"SPREV, short for hyperSphere Reduced to two-dimensional Regular Polygon for Visualisation, is a novel dimensionality reduction technique developed to address the challenges of reducing dimensions and visualizing labeled datasets that exhibit a unique combination of three characteristics: small class size, high dimensionality, and low sample size. SPREV is designed not only to uncover but also to visually represent hidden patterns within such datasets. Its distinctive integration of geometric principles, adapted for discrete computational environments, makes it an indispensable tool in the modern data science toolkit, enabling users to identify trends, extract insights, and navigate complex data efficiently and effectively.\n        △ Less",
        "authors":"['Srivathsan Amruth']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling",
        "abstract":"Generative models often map noise to data by matching flows or scores, but these approaches become cumbersome for incorporating partial observations or additional priors. Inspired by recent advances in Wasserstein gradient flows, we propose Energy Matching, a framework that unifies flow-based approaches with the flexibility of energy-based models (EBMs). Far from the data manifold, samples move along curl-free, optimal transport paths from noise to data. As they approach the data manifold, an entropic energy term guides the system into a Boltzmann equilibrium distribution, explicitly capturing the underlying likelihood structure of the data. We parameterize this dynamic with a single time-independent scalar field, which serves as both a powerful generator and a flexible prior for effective regularization of inverse problems. Our method substantially outperforms existing EBMs on CIFAR-10 generation (FID 3.97 compared to 8.61), while retaining the simulation-free training of transport-based approaches away from the data manifold. Additionally, we exploit the flexibility of our method and introduce an interaction energy for diverse mode exploration. Our approach focuses on learning a static scalar potential energy -- without time conditioning, auxiliary generators, or additional networks -- marking a significant departure from recent EBM methods. We believe this simplified framework significantly advances EBM capabilities and paves the way for their broader adoption in generative modeling across diverse domains.\n        △ Less",
        "authors":"['Michal Balcerak', 'Tamaz Amiranashvili', 'Suprosanna Shit', 'Antonio Terpin', 'Sebastian Kaltenbach', 'Petros Koumoutsakos', 'Bjoern Menze']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond Worst-Case Online Classification: VC-Based Regret Bounds for Relaxed Benchmarks",
        "abstract":"We revisit online binary classification by shifting the focus from competing with the best-in-class binary loss to competing against relaxed benchmarks that capture smoothed notions of optimality. Instead of measuring regret relative to the exact minimal binary error -- a standard approach that leads to worst-case bounds tied to the Littlestone dimension -- we consider comparing with predictors that are robust to small input perturbations, perform well under Gaussian smoothing, or maintain a prescribed output margin. Previous examples of this were primarily limited to the hinge loss. Our algorithms achieve regret guarantees that depend only on the VC dimension and the complexity of the instance space (e.g., metric entropy), and notably, they incur only an $O(\\log(1\/γ))$ dependence on the generalized margin $γ$. This stands in contrast to most existing regret bounds, which typically exhibit a polynomial dependence on $1\/γ$. We complement this with matching lower bounds. Our analysis connects recent ideas from adversarial robustness and smoothed online learning.\n        △ Less",
        "authors":"['Omar Montasser', 'Abhishek Shetty', 'Nikita Zhivotovskiy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FLOWR: Flow Matching for Structure-Aware De Novo, Interaction- and Fragment-Based Ligand Generation",
        "abstract":"We introduce FLOWR, a novel structure-based framework for the generation and optimization of three-dimensional ligands. FLOWR integrates continuous and categorical flow matching with equivariant optimal transport, enhanced by an efficient protein pocket conditioning. Alongside FLOWR, we present SPINDR, a thoroughly curated dataset comprising ligand-pocket co-crystal complexes specifically designed to address existing data quality issues. Empirical evaluations demonstrate that FLOWR surpasses current state-of-the-art diffusion- and flow-based methods in terms of PoseBusters-validity, pose accuracy, and interaction recovery, while offering a significant inference speedup, achieving up to 70-fold faster performance. In addition, we introduce FLOWR.multi, a highly accurate multi-purpose model allowing for the targeted sampling of novel ligands that adhere to predefined interaction profiles and chemical substructures for fragment-based design without the need of re-training or any re-sampling strategies\n        △ Less",
        "authors":"['Julian Cremer', 'Ross Irwin', 'Alessandro Tibot', 'Jon Paul Janet', 'Simon Olsson', 'Djork-Arné Clevert']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Self-Controlled Dynamic Expansion Model for Continual Learning",
        "abstract":"Continual Learning (CL) epitomizes an advanced training paradigm wherein prior data samples remain inaccessible during the acquisition of new tasks. Numerous investigations have delved into leveraging a pre-trained Vision Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless, these approaches typically utilize a singular, static backbone, which inadequately adapts to novel tasks, particularly when engaging with diverse data domains, due to a substantial number of inactive parameters. This paper addresses this limitation by introducing an innovative Self-Controlled Dynamic Expansion Model (SCDEM), which orchestrates multiple distinct trainable pre-trained ViT backbones to furnish diverse and semantically enriched representations. Specifically, by employing the multi-backbone architecture as a shared module, the proposed SCDEM dynamically generates a new expert with minimal parameters to accommodate a new task. A novel Collaborative Optimization Mechanism (COM) is introduced to synergistically optimize multiple backbones by harnessing prediction signals from historical experts, thereby facilitating new task learning without erasing previously acquired knowledge. Additionally, a novel Feature Distribution Consistency (FDC) approach is proposed to align semantic similarity between previously and currently learned representations through an optimal transport distance-based mechanism, effectively mitigating negative knowledge transfer effects. Furthermore, to alleviate over-regularization challenges, this paper presents a novel Dynamic Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the penalization intensity on each trainable representation layer. An extensive series of experiments have been conducted to evaluate the proposed methodology's efficacy, with empirical results corroborating that the approach attains state-of-the-art performance.\n        △ Less",
        "authors":"['Runqing Wu', 'Kaihui Huang', 'Hanyi Zhang', 'Fei Ye']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Molecular Learning Dynamics",
        "abstract":"We apply the physics-learning duality to molecular systems by complementing the physical description of interacting particles with a dual learning description, where each particle is modeled as an agent minimizing a loss function. In the traditional physics framework, the equations of motion are derived from the Lagrangian function, while in the learning framework, the same equations emerge from learning dynamics driven by the agent loss function. The loss function depends on scalar quantities that describe invariant properties of all other agents or particles. To demonstrate this approach, we first infer the loss functions of oxygen and hydrogen directly from a dataset generated by the CP2K physics-based simulation of water molecules. We then employ the loss functions to develop a learning-based simulation of water molecules, which achieves comparable accuracy while being significantly more computationally efficient than standard physics-based simulations.\n        △ Less",
        "authors":"['Yaroslav Gusev', 'Vitaly Vanchurin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Process Reward Model Training via Active Learning",
        "abstract":"Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.\n        △ Less",
        "authors":"['Keyu Duan', 'Zichen Liu', 'Xin Mao', 'Tianyu Pang', 'Changyu Chen', 'Qiguang Chen', 'Michael Qizhe Shieh', 'Longxu Dou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"VAE-based Feature Disentanglement for Data Augmentation and Compression in Generalized GNSS Interference Classification",
        "abstract":"Distributed learning and Edge AI necessitate efficient data processing, low-latency communication, decentralized model training, and stringent data privacy to facilitate real-time intelligence on edge devices while reducing dependency on centralized infrastructure and ensuring high model performance. In the context of global navigation satellite system (GNSS) applications, the primary objective is to accurately monitor and classify interferences that degrade system performance in distributed environments, thereby enhancing situational awareness. To achieve this, machine learning (ML) models can be deployed on low-resource devices, ensuring minimal communication latency and preserving data privacy. The key challenge is to compress ML models while maintaining high classification accuracy. In this paper, we propose variational autoencoders (VAEs) for disentanglement to extract essential latent features that enable accurate classification of interferences. We demonstrate that the disentanglement approach can be leveraged for both data compression and data augmentation by interpolating the lower-dimensional latent representations of signal power. To validate our approach, we evaluate three VAE variants - vanilla, factorized, and conditional generative - on four distinct datasets, including two collected in controlled indoor environments and two real-world highway datasets. Additionally, we conduct extensive hyperparameter searches to optimize performance. Our proposed VAE achieves a data compression rate ranging from 512 to 8,192 and achieves an accuracy up to 99.92%.\n        △ Less",
        "authors":"['Lucas Heublein', 'Simon Kocher', 'Tobias Feigl', 'Alexander Rügamer', 'Christopher Mutschler', 'Felix Ott']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond the Generative Learning Trilemma: Generative Model Assessment in Data Scarcity Domains",
        "abstract":"Data scarcity remains a critical bottleneck impeding technological advancements across various domains, including but not limited to medicine and precision agriculture. To address this challenge, we explore the potential of Deep Generative Models (DGMs) in producing synthetic data that satisfies the Generative Learning Trilemma: fidelity, diversity, and sampling efficiency. However, recognizing that these criteria alone are insufficient for practical applications, we extend the trilemma to include utility, robustness, and privacy, factors crucial for ensuring the applicability of DGMs in real-world scenarios. Evaluating these metrics becomes particularly challenging in data-scarce environments, as DGMs traditionally rely on large datasets to perform optimally. This limitation is especially pronounced in domains like medicine and precision agriculture, where ensuring acceptable model performance under data constraints is vital. To address these challenges, we assess the Generative Learning Trilemma in data-scarcity settings using state-of-the-art evaluation metrics, comparing three prominent DGMs: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models (DMs). Furthermore, we propose a comprehensive framework to assess utility, robustness, and privacy in synthetic data generated by DGMs. Our findings demonstrate varying strengths among DGMs, with each model exhibiting unique advantages based on the application context. This study broadens the scope of the Generative Learning Trilemma, aligning it with real-world demands and providing actionable guidance for selecting DGMs tailored to specific applications.\n        △ Less",
        "authors":"['Marco Salmè', 'Lorenzo Tronchin', 'Rosa Sicilia', 'Paolo Soda', 'Valerio Guarrasi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Inferring the Hubble Constant Using Simulated Strongly Lensed Supernovae and Neural Network Ensembles",
        "abstract":"Strongly lensed supernovae are a promising new probe to obtain independent measurements of the Hubble constant (${H_0}$). In this work, we employ simulated gravitationally lensed Type Ia supernovae (glSNe Ia) to train our machine learning (ML) pipeline to constrain $H_0$. We simulate image time-series of glSNIa, as observed with the upcoming Nancy Grace Roman Space Telescope, that we employ for training an ensemble of five convolutional neural networks (CNNs). The outputs of this ensemble network are combined with a simulation-based inference (SBI) framework to quantify the uncertainties on the network predictions and infer full posteriors for the $H_0$ estimates. We illustrate that the combination of multiple glSN systems enhances constraint precision, providing a $4.4\\%$ estimate of $H_0$ based on 100 simulated systems, which is in agreement with the ground truth. This research highlights the potential of leveraging the capabilities of ML with glSNe systems to obtain a pipeline capable of fast and automated $H_0$ measurements.\n        △ Less",
        "authors":"['Gonçalo Gonçalves', 'Nikki Arendse', 'Doogesh Kodi Ramanah', 'Radosław Wojtak']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LEMUR Neural Network Dataset: Towards Seamless AutoML",
        "abstract":"Neural networks are fundamental in artificial intelligence, driving progress in computer vision and natural language processing. High-quality datasets are crucial for their development, and there is growing interest in datasets composed of neural networks themselves to support benchmarking, automated machine learning (AutoML), and model analysis. We introduce LEMUR, an open source dataset of neural network models with well-structured code for diverse architectures across tasks such as object detection, image classification, segmentation, and natural language processing. LEMUR is primarily designed to enable fine-tuning of large language models (LLMs) for AutoML tasks, providing a rich source of structured model representations and associated performance data. Leveraging Python and PyTorch, LEMUR enables seamless extension to new datasets and models while maintaining consistency. It integrates an Optuna-powered framework for evaluation, hyperparameter optimization, statistical analysis, and graphical insights. LEMUR provides an extension that enables models to run efficiently on edge devices, facilitating deployment in resource-constrained environments. Providing tools for model evaluation, preprocessing, and database management, LEMUR supports researchers and practitioners in developing, testing, and analyzing neural networks. Additionally, it offers an API that delivers comprehensive information about neural network models and their complete performance statistics with a single request, which can be used in experiments with code-generating large language models. The LEMUR will be released as an open source project under the MIT license upon acceptance of the paper.\n        △ Less",
        "authors":"['Arash Torabi Goodarzi', 'Roman Kochnev', 'Waleed Khalid', 'Furui Qin', 'Tolgay Atinc Uzun', 'Yashkumar Sanjaybhai Dhameliya', 'Yash Kanubhai Kathiriya', 'Zofia Antonina Bentyn', 'Dmitry Ignatov', 'Radu Timofte']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers",
        "abstract":"Empirical Risk Minimization (ERM) models often rely on spurious correlations between features and labels during the learning process, leading to shortcut learning behavior that undermines robustness generalization performance. Current research mainly targets identifying or mitigating a single shortcut; however, in real-world scenarios, cues within the data are diverse and unknown. In empirical studies, we reveal that the models rely to varying extents on different shortcuts. Compared to weak shortcuts, models depend more heavily on strong shortcuts, resulting in their poor generalization ability. To address these challenges, we propose MiMu, a novel method integrated with Transformer-based ERMs designed to Mitigate Multiple shortcut learning behavior, which incorporates self-calibration strategy and self-improvement strategy. In the source model, we preliminarily propose the self-calibration strategy to prevent the model from relying on shortcuts and make overconfident predictions. Then, we further design self-improvement strategy in target model to reduce the reliance on multiple shortcuts. The random mask strategy involves randomly masking partial attention positions to diversify the focus of target model other than concentrating on a fixed region. Meanwhile, the adaptive attention alignment module facilitates the alignment of attention weights to the calibrated source model, without the need for post-hoc attention maps or supervision. Finally, extensive experiments conducted on Natural Language Processing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu in improving robustness generalization abilities.\n        △ Less",
        "authors":"['Lili Zhao', 'Qi Liu', 'Wei Chen', 'Liyi Chen', 'Ruijun Sun', 'Min Hou', 'Yang Wang', 'Shijin Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LCDC: Bridging Science and Machine Learning for Light Curve Analysis",
        "abstract":"The characterization and analysis of light curves are vital for understanding the physical and rotational properties of artificial space objects such as satellites, rocket stages, and space debris. This paper introduces the Light Curve Dataset Creator (LCDC), a Python-based toolkit designed to facilitate the preprocessing, analysis, and machine learning applications of light curve data. LCDC enables seamless integration with publicly available datasets, such as the newly introduced Mini Mega Tortora (MMT) database. Moreover, it offers data filtering, transformation, as well as feature extraction tooling. To demonstrate the toolkit's capabilities, we created the first standardized dataset for rocket body classification, RoBo6, which was used to train and evaluate several benchmark machine learning models, addressing the lack of reproducibility and comparability in recent studies. Furthermore, the toolkit enables advanced scientific analyses, such as surface characterization of the Atlas 2AS Centaur and the rotational dynamics of the Delta 4 rocket body, by streamlining data preprocessing, feature extraction, and visualization. These use cases highlight LCDC's potential to advance space debris characterization and promote sustainable space exploration. Additionally, they highlight the toolkit's ability to enable AI-focused research within the space debris community.\n        △ Less",
        "authors":"['Daniel Kyselica', 'Tomáš Hrobár', 'Jiří Šilha', 'Roman Ďurikovič', 'Marek Šuppa']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Integrating Textual Embeddings from Contrastive Learning with Generative Recommender for Enhanced Personalization",
        "abstract":"Recent advances in recommender systems have highlighted the complementary strengths of generative modeling and pretrained language models. We propose a hybrid framework that augments the Hierarchical Sequential Transduction Unit (HSTU) generative recommender with BLaIR -- a contrastive text embedding model. This integration enriches item representations with semantic signals from textual metadata while preserving HSTU's powerful sequence modeling capabilities.\n  We evaluate our method on two domains from the Amazon Reviews 2023 dataset, comparing it against the original HSTU and a variant that incorporates embeddings from OpenAI's state-of-the-art text-embedding-3-large model. While the OpenAI embedding model is likely trained on a substantially larger corpus with significantly more parameters, our lightweight BLaIR-enhanced approach -- pretrained on domain-specific data -- consistently achieves better performance, highlighting the effectiveness of contrastive text embeddings in compute-efficient settings.\n        △ Less",
        "authors":"['Yijun Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Efficient Quantum Classifier Based on Hamiltonian Representations",
        "abstract":"Quantum machine learning (QML) is a discipline that seeks to transfer the advantages of quantum computing to data-driven tasks. However, many studies rely on toy datasets or heavy feature reduction, raising concerns about their scalability. Progress is further hindered by hardware limitations and the significant costs of encoding dense vector representations on quantum devices. To address these challenges, we propose an efficient approach called Hamiltonian classifier that circumvents the costs associated with data encoding by mapping inputs to a finite set of Pauli strings and computing predictions as their expectation values. In addition, we introduce two classifier variants with different scaling in terms of parameters and sample complexity. We evaluate our approach on text and image classification tasks, against well-established classical and quantum models. The Hamiltonian classifier delivers performance comparable to or better than these methods. Notably, our method achieves logarithmic complexity in both qubits and quantum gates, making it well-suited for large-scale, real-world applications. We make our implementation available on GitHub.\n        △ Less",
        "authors":"['Federico Tiblias', 'Anna Schroeder', 'Yue Zhang', 'Mariami Gachechiladze', 'Iryna Gurevych']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AB-Cache: Training-Free Acceleration of Diffusion Models via Adams-Bashforth Cached Feature Reuse",
        "abstract":"Diffusion models have demonstrated remarkable success in generative tasks, yet their iterative denoising process results in slow inference, limiting their practicality. While existing acceleration methods exploit the well-known U-shaped similarity pattern between adjacent steps through caching mechanisms, they lack theoretical foundation and rely on simplistic computation reuse, often leading to performance degradation. In this work, we provide a theoretical understanding by analyzing the denoising process through the second-order Adams-Bashforth method, revealing a linear relationship between the outputs of consecutive steps. This analysis explains why the outputs of adjacent steps exhibit a U-shaped pattern. Furthermore, extending Adams-Bashforth method to higher order, we propose a novel caching-based acceleration approach for diffusion models, instead of directly reusing cached results, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step size. Extensive validation across diverse image and video diffusion models (including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates our method's effectiveness in achieving nearly $3\\times$ speedup while maintaining original performance levels, offering a practical real-time solution without compromising generation quality.\n        △ Less",
        "authors":"['Zichao Yu', 'Zhen Zou', 'Guojiang Shao', 'Chengwei Zhang', 'Shengze Xu', 'Jie Huang', 'Feng Zhao', 'Xiaodong Cun', 'Wenyi Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP",
        "abstract":"Federated learning (FL) enables collaborative model training across organizations without sharing raw data, addressing crucial privacy concerns in healthcare natural language processing (NLP). However, training large language models (LLMs) in federated settings faces significant challenges, including communication overhead and data heterogeneity. We propose Layer-Skipping Federated Learning, where only selected layers of a pre-trained LLM are fine-tuned across clients while others remain frozen. Applied to LLaMA 3.2-1B, our approach reduces communication costs by approximately 70% while maintaining performance within 2% of centralized training. We evaluate our method on clinical NER and classification tasks using i2b2 and MIMIC-III datasets. Our experiments demonstrate that Layer-Skipping FL outperforms competitive baselines, handles non-IID clinical data distributions effectively, and shows robustness when combined with differential privacy. This approach represents a practical solution for privacy-preserving collaborative learning in healthcare NLP.\n        △ Less",
        "authors":"['Lihong Zhang', 'Yue Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Integrating Emotion Distribution Networks and Textual Message Analysis for X User Emotional State Classification",
        "abstract":"As the popularity and reach of social networks continue to surge, a vast reservoir of opinions and sentiments across various subjects inundates these platforms. Among these, X social network (formerly Twitter) stands as a juggernaut, boasting approximately 420 million active users. Extracting users' emotional and mental states from their expressed opinions on social media has become a common pursuit. While past methodologies predominantly focused on the textual content of messages to analyze user sentiment, the interactive nature of these platforms suggests a deeper complexity. This study employs hybrid methodologies, integrating textual analysis, profile examination, follower analysis, and emotion dissemination patterns. Initially, user interactions are leveraged to refine emotion classification within messages, encompassing exchanges where users respond to each other. Introducing the concept of a communication tree, a model is extracted to map these interactions. Subsequently, users' bios and interests from this tree are juxtaposed with message text to enrich analysis. Finally, influential figures are identified among users' followers in the communication tree, categorized into different topics to gauge interests. The study highlights that traditional sentiment analysis methodologies, focusing solely on textual content, are inadequate in discerning sentiment towards significant events, notably the presidential election. Comparative analysis with conventional methods reveals a substantial improvement in accuracy with the incorporation of emotion distribution patterns and user profiles. The proposed approach yields a 12% increase in accuracy with emotion distribution patterns and a 15% increase when considering user profiles, underscoring its efficacy in capturing nuanced sentiment dynamics.\n        △ Less",
        "authors":"['Pardis Moradbeiki', 'Mohammad Ali Zare Chahooki']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Toward Super Agent System with Hybrid AI Routers",
        "abstract":"AI Agents powered by Large Language Models are transforming the world through enormous applications. A super agent has the potential to fulfill diverse user needs, such as summarization, coding, and research, by accurately understanding user intent and leveraging the appropriate tools to solve tasks. However, to make such an agent viable for real-world deployment and accessible at scale, significant optimizations are required to ensure high efficiency and low cost. This paper presents a design of the Super Agent System. Upon receiving a user prompt, the system first detects the intent of the user, then routes the request to specialized task agents with the necessary tools or automatically generates agentic workflows. In practice, most applications directly serve as AI assistants on edge devices such as phones and robots. As different language models vary in capability and cloud-based models often entail high computational costs, latency, and privacy concerns, we then explore the hybrid mode where the router dynamically selects between local and cloud models based on task complexity. Finally, we introduce the blueprint of an on-device super agent enhanced with cloud. With advances in multi-modality models and edge hardware, we envision that most computations can be handled locally, with cloud collaboration only as needed. Such architecture paves the way for super agents to be seamlessly integrated into everyday life in the near future.\n        △ Less",
        "authors":"['Yuhang Yao', 'Haixin Wang', 'Yibo Chen', 'Jiawen Wang', 'Min Chang Jordan Ren', 'Bosheng Ding', 'Salman Avestimehr', 'Chaoyang He']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "abstract":"Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.\n        △ Less",
        "authors":"['Yijun Liang', 'Ming Li', 'Chenrui Fan', 'Ziyue Li', 'Dang Nguyen', 'Kwesi Cobbina', 'Shweta Bhardwaj', 'Jiuhai Chen', 'Fuxiao Liu', 'Tianyi Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems",
        "abstract":"Generative retrieval methods utilize generative sequential modeling techniques, such as transformers, to generate candidate items for recommender systems. These methods have demonstrated promising results in academic benchmarks, surpassing traditional retrieval models like two-tower architectures. However, current generative retrieval methods lack the scalability required for industrial recommender systems, and they are insufficiently flexible to satisfy the multiple metric requirements of modern systems.\n  This paper introduces PinRec, a novel generative retrieval model developed for applications at Pinterest. PinRec utilizes outcome-conditioned generation, enabling modelers to specify how to balance various outcome metrics, such as the number of saves and clicks, to effectively align with business goals and user exploration. Additionally, PinRec incorporates multi-token generation to enhance output diversity while optimizing generation. Our experiments demonstrate that PinRec can successfully balance performance, diversity, and efficiency, delivering a significant positive impact to users using generative models. This paper marks a significant milestone in generative retrieval, as it presents, to our knowledge, the first rigorous study on implementing generative retrieval at the scale of Pinterest.\n        △ Less",
        "authors":"['Anirudhan Badrinath', 'Prabhat Agarwal', 'Laksh Bhasin', 'Jaewon Yang', 'Jiajing Xu', 'Charles Rosenberg']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Human-Oriented Image Retrieval System (HORSE): A Neuro-Symbolic Approach to Optimizing Retrieval of Previewed Images",
        "abstract":"Image retrieval remains a challenging task due to the complex interaction between human visual perception, memory, and computational processes. Current image search engines often struggle to efficiently retrieve images based on natural language descriptions, as they rely on time-consuming preprocessing, tagging, and machine learning pipelines. This paper introduces the Human-Oriented Retrieval Search Engine for Images (HORSE), a novel approach that leverages neuro-symbolic indexing to improve image retrieval by focusing on human-oriented indexing. By integrating cognitive science insights with advanced computational techniques, HORSE enhances the retrieval process, making it more aligned with how humans perceive, store, and recall visual information. The neuro-symbolic framework combines the strengths of neural networks and symbolic reasoning, mitigating their individual limitations. The proposed system optimizes image retrieval, offering a more intuitive and efficient solution for users. We discuss the design and implementation of HORSE, highlight its potential applications in fields such as design error detection and knowledge management, and suggest future directions for research to further refine the system's metrics and capabilities.\n        △ Less",
        "authors":"['Abraham Itzhak Weinberg']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Auto-Distillation and Generative Self-Supervised Learning in Residual Graph Transformers for Enhanced Recommender Systems",
        "abstract":"This paper introduces a cutting-edge method for enhancing recommender systems through the integration of generative self-supervised learning (SSL) with a Residual Graph Transformer. Our approach emphasizes the importance of superior data enhancement through the use of pertinent pretext tasks, automated through rationale-aware SSL to distill clear ways of how users and items interact. The Residual Graph Transformer incorporates a topology-aware transformer for global context and employs residual connections to improve graph representation learning. Additionally, an auto-distillation process refines self-supervised signals to uncover consistent collaborative rationales. Experimental evaluations on multiple datasets demonstrate that our approach consistently outperforms baseline methods.\n        △ Less",
        "authors":"['Eya Mhedhbi', 'Youssef Mourchid', 'Alice Othmani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ArxivBench: Can LLMs Assist Researchers in Conducting Research?",
        "abstract":"Large language models (LLMs) have demonstrated remarkable effectiveness in completing various tasks such as reasoning, translation, and question answering. However the issue of factual incorrect content in LLM-generated responses remains a persistent challenge. In this study, we evaluate both proprietary and open-source LLMs on their ability to respond with relevant research papers and accurate links to articles hosted on the arXiv platform, based on high level prompts. To facilitate this evaluation, we introduce arXivBench, a benchmark specifically designed to assess LLM performance across eight major subject categories on arXiv and five subfields within computer science, one of the most popular categories among them. Our findings reveal a concerning accuracy of LLM-generated responses depending on the subject, with some subjects experiencing significantly lower accuracy than others. Notably, Claude-3.5-Sonnet exhibits a substantial advantage in generating both relevant and accurate responses. And interestingly, most LLMs achieve a much higher accuracy in the Artificial Intelligence sub-field than other sub-fields. This benchmark provides a standardized tool for evaluating the reliability of LLM-generated scientific responses, promoting more dependable use of LLMs in academic and research environments. Our code is open-sourced at https:\/\/github.com\/arxivBenchLLM\/arXivBench and our dataset is available on huggingface at https:\/\/huggingface.co\/datasets\/arXivBenchLLM\/arXivBench.\n        △ Less",
        "authors":"['Ning Li', 'Jingran Zhang', 'Justin Cui']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA",
        "abstract":"We explore the potential of integrating learnable and interpretable modules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based representations--within a pre-trained GPT-2 model to enhance multi-task learning accuracy. Motivated by the recent surge in using KAN and graph attention (GAT) architectures in chain-of-thought (CoT) models and debates over their benefits compared to simpler architectures like MLPs, we begin by enhancing a standard self-attention transformer using Low-Rank Adaptation (LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This approach yields significant improvements. To further boost interpretability and richer representations, we develop two variants that attempt to improve the standard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However, systematic evaluations reveal that neither variant outperforms the optimized LoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set, 99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On sonnet generation, we get a CHRF score of 42.097. These findings highlight that efficient parameter adaptation via LoRA remains the most effective strategy for our tasks: sentiment analysis, paraphrase detection, and sonnet generation.\n        △ Less",
        "authors":"['Gabriel Bo', 'Marc Bernardino', 'Justin Gu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning",
        "abstract":"In this paper, we present Roamify, an Artificial Intelligence powered travel assistant that aims to ease the process of travel planning. We have tested and used multiple Large Language Models like Llama and T5 to generate personalised itineraries per user preferences. Results from user surveys highlight the preference for AI powered mediums over existing methods to help in travel planning across all user age groups. These results firmly validate the potential need of such a travel assistant. We highlight the two primary design considerations for travel assistance: D1) incorporating a web-scraping method to gather up-to-date news articles about destinations from various blog sources, which significantly improves our itinerary suggestions, and D2) utilising user preferences to create customised travel experiences along with a recommendation system which changes the itinerary according to the user needs. Our findings suggest that Roamify has the potential to improve and simplify how users across multiple age groups plan their travel experiences.\n        △ Less",
        "authors":"['Vikranth Udandarao', 'Noel Abraham Tiju', 'Muthuraj Vairamuthu', 'Harsh Mistry', 'Dhruv Kumar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation",
        "abstract":"Recent Open-Vocabulary Semantic Segmentation (OVSS) models extend the CLIP model to segmentation while maintaining the use of multiple templates (e.g., a photo of <class>, a sketch of a <class>, etc.) for constructing class-wise averaged text embeddings, acting as a classifier. In this paper, we challenge this status quo and investigate the impact of templates for OVSS. Empirically, we observe that for each class, there exist single-template classifiers significantly outperforming the conventional averaged classifier. We refer to them as class-experts. Given access to unlabeled images and without any training involved, we estimate these experts by leveraging the class-wise prediction entropy of single-template classifiers, selecting as class-wise experts those which yield the lowest entropy. All experts, each specializing in a specific class, collaborate in a newly proposed fusion method to generate more accurate OVSS predictions. Our plug-and-play method, coined FLOSS, is orthogonal and complementary to existing OVSS methods, offering a ''free lunch'' to systematically improve OVSS without labels and additional training. Extensive experiments demonstrate that FLOSS consistently boosts state-of-the-art methods on various OVSS benchmarks. Moreover, the selected expert templates can generalize well from one dataset to others sharing the same semantic categories, yet exhibiting distribution shifts. Additionally, we obtain satisfactory improvements under a low-data regime, where only a few unlabeled images are available. Our code is available at https:\/\/github.com\/yasserben\/FLOSS .\n        △ Less",
        "authors":"['Yasser Benigmim', 'Mohammad Fahes', 'Tuan-Hung Vu', 'Andrei Bursuc', 'Raoul de Charette']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers",
        "abstract":"In this paper we tackle a fundamental question: \"Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?\" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https:\/\/end2end-diffusion.github.io.\n        △ Less",
        "authors":"['Xingjian Leng', 'Jaskirat Singh', 'Yunzhong Hou', 'Zhenchang Xing', 'Saining Xie', 'Liang Zheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Weight Ensembling Improves Reasoning in Language Models",
        "abstract":"We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades-off between bias and variance.\n        △ Less",
        "authors":"['Xingyu Dang', 'Christina Baek', 'Kaiyue Wen', 'Zico Kolter', 'Aditi Raghunathan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Privacy-Preserving Distributed Link Predictions Among Peers in Online Classrooms Using Federated Learning",
        "abstract":"Social interactions among classroom peers, represented as social learning networks (SLNs), play a crucial role in enhancing learning outcomes. While SLN analysis has recently garnered attention, most existing approaches rely on centralized training, where data is aggregated and processed on a local\/cloud server with direct access to raw data. However, in real-world educational settings, such direct access across multiple classrooms is often restricted due to privacy concerns. Furthermore, training models on isolated classroom data prevents the identification of common interaction patterns that exist across multiple classrooms, thereby limiting model performance. To address these challenges, we propose one of the first frameworks that integrates Federated Learning (FL), a distributed and collaborative machine learning (ML) paradigm, with SLNs derived from students' interactions in multiple classrooms' online forums to predict future link formations (i.e., interactions) among students. By leveraging FL, our approach enables collaborative model training across multiple classrooms while preserving data privacy, as it eliminates the need for raw data centralization. Recognizing that each classroom may exhibit unique student interaction dynamics, we further employ model personalization techniques to adapt the FL model to individual classroom characteristics. Our results demonstrate the effectiveness of our approach in capturing both shared and classroom-specific representations of student interactions in SLNs. Additionally, we utilize explainable AI (XAI) techniques to interpret model predictions, identifying key factors that influence link formation across different classrooms. These insights unveil the drivers of social learning interactions within a privacy-preserving, collaborative, and distributed ML framework -- an aspect that has not been explored before.\n        △ Less",
        "authors":"['Anurata Prabha Hridi', 'Muntasir Hoq', 'Zhikai Gao', 'Collin Lynch', 'Rajeev Sahay', 'Seyyedali Hosseinalipour', 'Bita Akram']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Anchors no more: Using peculiar velocities to constrain $H_0$ and the primordial Universe without calibrators",
        "abstract":"We develop a novel approach to constrain the Hubble parameter $H_0$ and the primordial power spectrum amplitude $A_\\mathrm{s}$ using supernovae type Ia (SNIa) data. By considering SNIa as tracers of the peculiar velocity field, we can model their distance and their covariance as a function of cosmological parameters without the need of calibrators like Cepheids; this yields a new independent probe of the large-scale structure based on SNIa data without distance anchors. Crucially, we implement a differentiable pipeline in JAX, including efficient emulators and affine sampling, reducing inference time from years to hours on a single GPU. We first validate our method on mock datasets, demonstrating that we can constrain $H_0$ and $\\log 10^{10}A_\\mathrm{s}$ within $\\sim10\\%$ using $\\sim10^3$ SNIa. We then test our pipeline with SNIa from an $N$-body simulation, obtaining $7\\%$-level unbiased constraints on $H_0$ with a moderate noise level. We finally apply our method to Pantheon+ data, constraining $H_0$ at the $10\\%$ level without Cepheids when fixing $A_\\mathrm{s}$ to its $\\it{Planck}$ value. On the other hand, we obtain $15\\%$-level constraints on $\\log 10^{10}A_\\mathrm{s}$ in agreement with $\\it{Planck}$ when including Cepheids in the analysis. In light of upcoming observations of low redshift SNIa from the Zwicky Transient Facility and the Vera Rubin Legacy Survey of Space and Time, surveys for which our method will develop its full potential, we make our code publicly available.\n        △ Less",
        "authors":"['Davide Piras', 'Francesco Sorrenti', 'Ruth Durrer', 'Martin Kunz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Integrating Vision and Location with Transformers: A Multimodal Deep Learning Framework for Medical Wound Analysis",
        "abstract":"Effective recognition of acute and difficult-to-heal wounds is a necessary step in wound diagnosis. An efficient classification model can help wound specialists classify wound types with less financial and time costs and also help in deciding on the optimal treatment method. Traditional machine learning models suffer from feature selection and are usually cumbersome models for accurate recognition. Recently, deep learning (DL) has emerged as a powerful tool in wound diagnosis. Although DL seems promising for wound type recognition, there is still a large scope for improving the efficiency and accuracy of the model. In this study, a DL-based multimodal classifier was developed using wound images and their corresponding locations to classify them into multiple classes, including diabetic, pressure, surgical, and venous ulcers. A body map was also created to provide location data, which can help wound specialists label wound locations more effectively. The model uses a Vision Transformer to extract hierarchical features from input images, a Discrete Wavelet Transform (DWT) layer to capture low and high frequency components, and a Transformer to extract spatial features. The number of neurons and weight vector optimization were performed using three swarm-based optimization techniques (Monster Gorilla Toner (MGTO), Improved Gray Wolf Optimization (IGWO), and Fox Optimization Algorithm). The evaluation results show that weight vector optimization using optimization algorithms can increase diagnostic accuracy and make it a very effective approach for wound detection. In the classification using the original body map, the proposed model was able to achieve an accuracy of 0.8123 using image data and an accuracy of 0.8007 using a combination of image data and wound location. Also, the accuracy of the model in combination with the optimization models varied from 0.7801 to 0.8342.\n        △ Less",
        "authors":"['Ramin Mousa', 'Hadis Taherinia', 'Khabiba Abdiyeva', 'Amir Ali Bengari', 'Mohammadmahdi Vahediahmar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
        "abstract":"Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.\n        △ Less",
        "authors":"['Junxiong Wang', 'Wen-Ding Li', 'Daniele Paliotta', 'Daniel Ritter', 'Alexander M. Rush', 'Tri Dao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users",
        "abstract":"To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond performing a single, clearly defined task. For instance, real-world human instructions can be ambiguous, require different levels of AI assistance, and may evolve over time, reflecting changes in the user's mental state. To address this gap, we introduce RealWebAssist, a novel benchmark designed to evaluate sequential instruction-following in realistic scenarios involving long-horizon interactions with the web, visual GUI grounding, and understanding ambiguous real-world user instructions. RealWebAssist includes a dataset of sequential instructions collected from real-world human users. Each user instructs a web-based assistant to perform a series of tasks on multiple websites. A successful agent must reason about the true intent behind each instruction, keep track of the mental state of the user, understand user-specific routines, and ground the intended tasks to actions on the correct GUI elements. Our experimental results show that state-of-the-art models struggle to understand and ground user instructions, posing critical challenges in following real-world user instructions for long-horizon web assistance.\n        △ Less",
        "authors":"['Suyu Ye', 'Haojun Shi', 'Darren Shih', 'Hyokun Yun', 'Tanya Roosta', 'Tianmin Shu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multimodal Long Video Modeling Based on Temporal Dynamic Context",
        "abstract":"Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent methods are designed for long video understanding, they often lose crucial information during token compression and struggle with additional modality like audio. In this work, we propose a dynamic long video encoding method utilizing the temporal relationship between frames, named Temporal Dynamic Context (TDC). Firstly, we segment the video into semantically consistent scenes based on inter-frame similarities, then encode each frame into tokens using visual-audio encoders. Secondly, we propose a novel temporal context compressor to reduce the number of tokens within each segment. Specifically, we employ a query-based Transformer to aggregate video, audio, and instruction text tokens into a limited set of temporal context tokens. Finally, we feed the static frame tokens and the temporal context tokens into the LLM for video understanding. Furthermore, to handle extremely long videos, we propose a training-free chain-of-thought strategy that progressively extracts answers from multiple video segments. These intermediate answers serve as part of the reasoning process and contribute to the final answer. We conduct extensive experiments on general video understanding and audio-video understanding benchmarks, where our method demonstrates strong performance. The code and models are available at https:\/\/github.com\/Hoar012\/TDC-Video.\n        △ Less",
        "authors":"['Haoran Hao', 'Jiaming Han', 'Yiyuan Zhang', 'Xiangyu Yue']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning with Positive and Imperfect Unlabeled Data",
        "abstract":"We study the problem of learning binary classifiers from positive and unlabeled data when the unlabeled data distribution is shifted, which we call Positive and Imperfect Unlabeled (PIU) Learning. In the absence of covariate shifts, i.e., with perfect unlabeled data, Denis (1998) reduced this problem to learning under Massart noise; however, that reduction fails under even slight shifts.\n  Our main results on PIU learning are the characterizations of the sample complexity of PIU learning and a computationally and sample-efficient algorithm achieving a misclassification error $\\varepsilon$. We further show that our results lead to new algorithms for several related problems.\n  1. Learning from smooth distributions: We give algorithms that learn interesting concept classes from only positive samples under smooth feature distributions, bypassing known existing impossibility results and contributing to recent advances in smoothened learning (Haghtalab et al, J.ACM'24) (Chandrasekaran et al., COLT'24).\n  2. Learning with a list of unlabeled distributions: We design new algorithms that apply to a broad class of concept classes under the assumption that we are given a list of unlabeled distributions, one of which--unknown to the learner--is $O(1)$-close to the true feature distribution.\n  3. Estimation in the presence of unknown truncation: We give the first polynomial sample and time algorithm for estimating the parameters of an exponential family distribution from samples truncated to an unknown set approximable by polynomials in $L_1$-norm. This improves the algorithm by Lee et al. (FOCS'24) that requires approximation in $L_2$-norm.\n  4. Detecting truncation: We present new algorithms for detecting whether given samples have been truncated (or not) for a broad class of non-product distributions, including non-product distributions, improving the algorithm by De et al. (STOC'24).\n        △ Less",
        "authors":"['Jane H. Lee', 'Anay Mehrotra', 'Manolis Zampetakis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Foundation models for electronic health records: representation dynamics and transferability",
        "abstract":"Foundation models (FMs) trained on electronic health records (EHRs) have shown strong performance on a range of clinical prediction tasks. However, adapting these models to local health systems remains challenging due to limited data availability and resource constraints. In this study, we investigated what these models learn and evaluated the transferability of an FM trained on MIMIC-IV to an institutional EHR dataset at the University of Chicago Medical Center. We assessed their ability to identify outlier patients and examined representation-space patient trajectories in relation to future clinical outcomes. We also evaluated the performance of supervised fine-tuned classifiers on both source and target datasets. Our findings offer insights into the adaptability of FMs across different healthcare systems, highlight considerations for their effective implementation, and provide an empirical analysis of the underlying factors that contribute to their predictive performance.\n        △ Less",
        "authors":"['Michael C. Burkhart', 'Bashar Ramadan', 'Zewei Liao', 'Kaveri Chhikara', 'Juan C. Rojas', 'William F. Parker', 'Brett K. Beaulieu-Jones']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
        "abstract":"Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.\n        △ Less",
        "authors":"['Parshin Shojaee', 'Ngoc-Hieu Nguyen', 'Kazem Meidani', 'Amir Barati Farimani', 'Khoa D Doan', 'Chandan K Reddy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance Software Maintainability",
        "abstract":"This study explores Graph Neural Networks (GNNs) as a transformative tool for code refactoring, using abstract syntax trees (ASTs) to boost software maintainability. It analyzes a dataset of 2 million snippets from CodeSearchNet and a custom 75000-file GitHub Python corpus, comparing GNNs against rule-based SonarQube and decision trees. Metrics include cyclomatic complexity (target below 10), coupling (target below 5), and refactoring precision. GNNs achieve 92% accuracy, reducing complexity by 35% and coupling by 33%, outperforming SonarQube (78%, 16%) and decision trees (85%, 25%). Preprocessing fixed 60% of syntax errors. Bar graphs, tables, and AST visuals clarify results. This offers a scalable AI-driven path to cleaner codebases, which is crucial for software engineering.\n        △ Less",
        "authors":"['Gopichand Bandarupalli']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks",
        "abstract":"Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited bandwidth. Satellite federated learning (FL) offers a solution by enabling model fine-tuning directly on-board satellites and aggregating model updates without data downloading. Nevertheless, for large foundation models, the computational capacity of satellites is insufficient to support effective on-board fine-tuning in traditional satellite FL frameworks. To address these challenges, we propose a satellite-ground collaborative federated fine-tuning framework. The key of the framework lies in how to reasonably decompose and allocate model components to alleviate insufficient on-board computation capabilities. During fine-tuning, satellites exchange intermediate results with ground stations or other satellites for forward propagation and back propagation, which brings communication challenges due to the special communication topology of space transmission networks, such as intermittent satellite-ground communication, short duration of satellite-ground communication windows, and unstable inter-orbit inter-satellite links (ISLs). To reduce transmission delays, we further introduce tailored communication strategies that integrate both communication and computing resources. Specifically, we propose a parallel intra-orbit communication strategy, a topology-aware satellite-ground communication strategy, and a latency-minimalization inter-orbit communication strategy to reduce space communication costs. Simulation results demonstrate significant reductions in training time with improvements of approximately 33%.\n        △ Less",
        "authors":"['Yan zhu', 'Jingyang zhu', 'Ting Wang', 'Yuanming Shi', 'Chunxiao Jiang', 'Khaled Ben Letaief']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?",
        "abstract":"Objective: This study investigates the potential of Large Language Models (LLMs) as an alternative to human expert elicitation for extracting structured causal knowledge and facilitating causal modeling in biometric and healthcare applications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian networks (BNs), were benchmarked against traditional statistical methods (e.g., Bayesian Information Criterion) using healthcare datasets. Validation techniques included structural equation modeling (SEM) to verifying relationships, and measures such as entropy, predictive accuracy, and robustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than expert-elicited and statistically generated BNs, suggesting higher confidence and precision in predictions. However, limitations such as contextual constraints, hallucinated dependencies, and potential biases inherited from training data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for probabilistic causal modeling, promising to improve transparency and reduce uncertainty in the decision-making using such models.\n        △ Less",
        "authors":"['Olha Shaposhnyk', 'Daria Zahorska', 'Svetlana Yanushkevich']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Inferring genotype-phenotype maps using attention models",
        "abstract":"Predicting phenotype from genotype is a central challenge in genetics. Traditional approaches in quantitative genetics typically analyze this problem using methods based on linear regression. These methods generally assume that the genetic architecture of complex traits can be parameterized in terms of an additive model, where the effects of loci are independent, plus (in some cases) pairwise epistatic interactions between loci. However, these models struggle to analyze more complex patterns of epistasis or subtle gene-environment interactions. Recent advances in machine learning, particularly attention-based models, offer a promising alternative. Initially developed for natural language processing, attention-based models excel at capturing context-dependent interactions and have shown exceptional performance in predicting protein structure and function. Here, we apply attention-based models to quantitative genetics. We analyze the performance of this attention-based approach in predicting phenotype from genotype using simulated data across a range of models with increasing epistatic complexity, and using experimental data from a recent quantitative trait locus mapping study in budding yeast. We find that our model demonstrates superior out-of-sample predictions in epistatic regimes compared to standard methods. We also explore a more general multi-environment attention-based model to jointly analyze genotype-phenotype maps across multiple environments and show that such architectures can be used for \"transfer learning\" - predicting phenotypes in novel environments with limited training data.\n        △ Less",
        "authors":"['Krishna Rijal', 'Caroline M. Holmes', 'Samantha Petti', 'Gautam Reddy', 'Michael M. Desai', 'Pankaj Mehta']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PG-DPIR: An efficient plug-and-play method for high-count Poisson-Gaussian inverse problems",
        "abstract":"Poisson-Gaussian noise describes the noise of various imaging systems thus the need of efficient algorithms for Poisson-Gaussian image restoration. Deep learning methods offer state-of-the-art performance but often require sensor-specific training when used in a supervised setting. A promising alternative is given by plug-and-play (PnP) methods, which consist in learning only a regularization through a denoiser, allowing to restore images from several sources with the same network. This paper introduces PG-DPIR, an efficient PnP method for high-count Poisson-Gaussian inverse problems, adapted from DPIR. While DPIR is designed for white Gaussian noise, a naive adaptation to Poisson-Gaussian noise leads to prohibitively slow algorithms due to the absence of a closed-form proximal operator. To address this, we adapt DPIR for the specificities of Poisson-Gaussian noise and propose in particular an efficient initialization of the gradient descent required for the proximal step that accelerates convergence by several orders of magnitude. Experiments are conducted on satellite image restoration and super-resolution problems. High-resolution realistic Pleiades images are simulated for the experiments, which demonstrate that PG-DPIR achieves state-of-the-art performance with improved efficiency, which seems promising for on-ground satellite processing chains.\n        △ Less",
        "authors":"['Maud Biquard', 'Marie Chabert', 'Florence Genin', 'Christophe Latry', 'Thomas Oberlin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ctrl-Z: Controlling AI Agents via Resampling",
        "abstract":"Control evaluations measure whether monitoring and security protocols for AI systems prevent intentionally subversive AI models from causing harm. Our work presents the first control evaluation performed in an agent environment. We construct BashBench, a dataset of 257 challenging multi-step system administration tasks, and evaluate whether various safety measures can prevent an adversarially constructed AI agent from covertly downloading and executing malicious code in this environment. This multi-step setting introduces new attack and defense dynamics, which we investigate in order to design novel control protocols that prevent safety failures without hindering the ability of non-malicious agents to perform useful work. We introduce a class of control protocols called resample protocols that dynamically take additional samples of certain actions. We find these protocols significantly improve on existing techniques by selectively blocking the AI agent from executing suspicious code and incriminating the agent by generating additional examples of dangerous behavior. We measure the tradeoff between attack prevention and usefulness; our best protocol combines resampling with analysis of previous steps, reducing the success rate of attacks from 58% to 7% at a 5% cost to the performance of a non-malicious agent.\n        △ Less",
        "authors":"['Aryan Bhatt', 'Cody Rushing', 'Adam Kaufman', 'Tyler Tracy', 'Vasil Georgiev', 'David Matolcsi', 'Akbir Khan', 'Buck Shlegeris']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DUE: A Deep Learning Framework and Library for Modeling Unknown Equations",
        "abstract":"Equations, particularly differential equations, are fundamental for understanding natural phenomena and predicting complex dynamics across various scientific and engineering disciplines. However, the governing equations for many complex systems remain unknown due to intricate underlying mechanisms. Recent advancements in machine learning and data science offer a new paradigm for modeling unknown equations from measurement or simulation data. This paradigm shift, known as data-driven discovery or modeling, stands at the forefront of AI for science, with significant progress made in recent years. In this paper, we introduce a systematic framework for data-driven modeling of unknown equations using deep learning. This versatile framework is capable of learning unknown ODEs, PDEs, DAEs, IDEs, SDEs, reduced or partially observed systems, and non-autonomous differential equations. Based on this framework, we have developed Deep Unknown Equations (DUE), an open-source software package designed to facilitate the data-driven modeling of unknown equations using modern deep learning techniques. DUE serves as an educational tool for classroom instruction, enabling students and newcomers to gain hands-on experience with differential equations, data-driven modeling, and contemporary deep learning approaches such as FNN, ResNet, generalized ResNet, operator semigroup networks (OSG-Net), and Transformers. Additionally, DUE is a versatile and accessible toolkit for researchers across various scientific and engineering fields. It is applicable not only for learning unknown equations from data but also for surrogate modeling of known, yet complex, equations that are costly to solve using traditional numerical methods. We provide detailed descriptions of DUE and demonstrate its capabilities through diverse examples, which serve as templates that can be easily adapted for other applications.\n        △ Less",
        "authors":"['Junfeng Chen', 'Kailiang Wu', 'Dongbin Xiu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning",
        "abstract":"Optimizing Register Transfer Level (RTL) code is crucial for improving the power, performance, and area (PPA) of digital circuits in the early stages of synthesis. Manual rewriting, guided by synthesis feedback, can yield high-quality results but is time-consuming and error-prone. Most existing compiler-based approaches have difficulty handling complex design constraints. Large Language Model (LLM)-based methods have emerged as a promising alternative to address these challenges. However, LLM-based approaches often face difficulties in ensuring alignment between the generated code and the provided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL optimization framework that seamlessly integrates LLM-based code rewriting with symbolic reasoning techniques. Our method incorporates a retrieval-augmented generation (RAG) system of optimization rules and Abstract Syntax Tree (AST)-based templates, enabling LLM-based rewriting that maintains syntactic correctness while minimizing undesired circuit behaviors. A symbolic module is proposed for analyzing and optimizing finite state machine (FSM) logic, allowing fine-grained state merging and partial specification handling beyond the scope of pattern-based compilers. Furthermore, a fast verification pipeline, combining formal equivalence checks with test-driven validation, further reduces the complexity of verification. Experiments on the RTL-Rewriter benchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves power, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%, respectively, compared to the state-of-the-art methods.\n        △ Less",
        "authors":"['Yiting Wang', 'Wanghao Ye', 'Ping Guo', 'Yexiao He', 'Ziyao Wang', 'Yexiao He', 'Bowei Tian', 'Shwai He', 'Guoheng Sun', 'Zheyu Shen', 'Sihan Chen', 'Ankur Srivastava', 'Qingfu Zhang', 'Gang Qu', 'Ang Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Domain-Adversarial Neural Network and Explainable AI for Reducing Tissue-of-Origin Signal in Pan-cancer Mortality Classification",
        "abstract":"Tissue-of-origin signals dominate pan-cancer gene expression, often obscuring molecular features linked to patient survival. This hampers the discovery of generalizable biomarkers, as models tend to overfit tissue-specific patterns rather than capture survival-relevant signals. To address this, we propose a Domain-Adversarial Neural Network (DANN) trained on TCGA RNA-seq data to learn representations less biased by tissue and more focused on survival. Identifying tissue-independent genetic profiles is key to revealing core cancer programs. We assess the DANN using: (1) Standard SHAP, based on the original input space and DANN's mortality classifier; (2) A layer-aware strategy applied to hidden activations, including an unsupervised manifold from raw activations and a supervised manifold from mortality-specific SHAP values. Standard SHAP remains confounded by tissue signals due to biases inherent in its computation. The raw activation manifold was dominated by high-magnitude activations, which masked subtle tissue and mortality-related signals. In contrast, the layer-aware SHAP manifold offers improved low-dimensional representations of both tissue and mortality signals, independent of activation strength, enabling subpopulation stratification and pan-cancer identification of survival-associated genes.\n        △ Less",
        "authors":"['Cristian Padron-Manrique', 'Juan José Oropeza Valdez', 'Osbaldo Resendis-Antonio']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families",
        "abstract":"Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings--extracted via an LLM-assisted annotation pipeline--serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use.\n        △ Less",
        "authors":"['Shahriar Noroozizadeh', 'Sayantan Kumar', 'Jeremy C. Weiss']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Siamese Network with Dual Attention for EEG-Driven Social Learning: Bridging the Human-Robot Gap in Long-Tail Autonomous Driving",
        "abstract":"Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments.\n        △ Less",
        "authors":"['Xiaoshan Zhou', 'Carol C. Menassa', 'Vineet R. Kamat']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost imaging",
        "abstract":"We present a new self-supervised deep-learning-based Ghost Imaging (GI) reconstruction method, which provides unparalleled reconstruction performance for noisy acquisitions among unsupervised methods. We present the supporting mathematical framework and results from theoretical and real data use cases. Self-supervision removes the need for clean reference data while offering strong noise reduction. This provides the necessary tools for addressing signal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge low-light GI scenarios. Notable examples include micro- and nano-scale x-ray emission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples. Their applications include in-vivo and in-operando case studies for biological samples and batteries.\n        △ Less",
        "authors":"['Mathieu Manni', 'Dmitry Karpov', 'K. Joost Batenburg', 'Sharon Shwartz', 'Nicola Viganò']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"$α$-Flow: A Unified Framework for Continuous-State Discrete Flow Matching Models",
        "abstract":"Recent efforts have extended the flow-matching framework to discrete generative modeling. One strand of models directly works with the continuous probabilities instead of discrete tokens, which we colloquially refer to as Continuous-State Discrete Flow Matching (CS-DFM). Existing CS-DFM models differ significantly in their representations and geometric assumptions. This work presents a unified framework for CS-DFM models, under which the existing variants can be understood as operating on different $α$-representations of probabilities. Building upon the theory of information geometry, we introduce $α$-Flow, a family of CS-DFM models that adheres to the canonical $α$-geometry of the statistical manifold, and demonstrate its optimality in minimizing the generalized kinetic energy. Theoretically, we show that the flow matching loss for $α$-flow establishes a unified variational bound for the discrete negative log-likelihood. We comprehensively evaluate different instantiations of $α$-flow on various discrete generation domains to demonstrate their effectiveness in discrete generative modeling, including intermediate values whose geometries have never been explored before. $α$-flow significantly outperforms its discrete-state counterpart in image and protein sequence generation and better captures the entropy in language modeling.\n        △ Less",
        "authors":"['Chaoran Cheng', 'Jiahan Li', 'Jiajun Fan', 'Ge Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials",
        "abstract":"Characterization of atomic-scale materials traditionally requires human experts with months to years of specialized training. Even for trained human operators, accurate and reliable characterization remains challenging when examining newly discovered materials such as two-dimensional (2D) structures. This bottleneck drives demand for fully autonomous experimentation systems capable of comprehending research objectives without requiring large training datasets. In this work, we present ATOMIC (Autonomous Technology for Optical Microscopy & Intelligent Characterization), an end-to-end framework that integrates foundation models to enable fully autonomous, zero-shot characterization of 2D materials. Our system integrates the vision foundation model (i.e., Segment Anything Model), large language models (i.e., ChatGPT), unsupervised clustering, and topological analysis to automate microscope control, sample scanning, image segmentation, and intelligent analysis through prompt engineering, eliminating the need for additional training. When analyzing typical MoS2 samples, our approach achieves 99.7% segmentation accuracy for single layer identification, which is equivalent to that of human experts. In addition, the integrated model is able to detect grain boundary slits that are challenging to identify with human eyes. Furthermore, the system retains robust accuracy despite variable conditions including defocus, color temperature fluctuations, and exposure variations. It is applicable to a broad spectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless of whether they were fabricated via chemical vapor deposition or mechanical exfoliation. This work represents the implementation of foundation models to achieve autonomous analysis, establishing a scalable and data-efficient characterization paradigm that fundamentally transforms the approach to nanoscale materials research.\n        △ Less",
        "authors":"['Jingyun Yang', 'Ruoyan Avery Yin', 'Chi Jiang', 'Yuepeng Hu', 'Xiaokai Zhu', 'Xingjian Hu', 'Sutharsika Kumar', 'Xiao Wang', 'Xiaohua Zhai', 'Keran Rong', 'Yunyue Zhu', 'Tianyi Zhang', 'Zongyou Yin', 'Jing Kong', 'Neil Zhenqiang Gong', 'Zhichu Ren', 'Haozhe Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LMFormer: Lane based Motion Prediction Transformer",
        "abstract":"Motion prediction plays an important role in autonomous driving. This study presents LMFormer, a lane-aware transformer network for trajectory prediction tasks. In contrast to previous studies, our work provides a simple mechanism to dynamically prioritize the lanes and shows that such a mechanism introduces explainability into the learning behavior of the network. Additionally, LMFormer uses the lane connection information at intersections, lane merges, and lane splits, in order to learn long-range dependency in lane structure. Moreover, we also address the issue of refining the predicted trajectories and propose an efficient method for iterative refinement through stacked transformer layers. For benchmarking, we evaluate LMFormer on the nuScenes dataset and demonstrate that it achieves SOTA performance across multiple metrics. Furthermore, the Deep Scenario dataset is used to not only illustrate cross-dataset network performance but also the unification capabilities of LMFormer to train on multiple datasets and achieve better performance.\n        △ Less",
        "authors":"['Harsh Yadav', 'Maximilian Schaefer', 'Kun Zhao', 'Tobias Meisen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sidecar: A Structure-Preserving Framework for Solving Partial Differential Equations with Neural Networks",
        "abstract":"Solving partial differential equations (PDEs) with neural networks (NNs) has shown great potential in various scientific and engineering fields. However, most existing NN solvers mainly focus on satisfying the given PDEs, without explicitly considering intrinsic physical properties such as mass conservation or energy dissipation. This limitation can result in unstable or nonphysical solutions, particularly in long-term simulations. To address this issue, we propose Sidecar, a novel framework that enhances the accuracy and physical consistency of existing NN solvers by incorporating structure-preserving knowledge. Inspired by the Time-Dependent Spectral Renormalization (TDSR) approach, our Sidecar framework introduces a small copilot network, which is trained to guide the existing NN solver in preserving physical structure. This framework is designed to be highly flexible, enabling the incorporation of structure-preserving principles from diverse PDEs into a wide range of NN solvers. Our experimental results on benchmark PDEs demonstrate the improvement of the existing neural network solvers in terms of accuracy and consistency with structure-preserving properties.\n        △ Less",
        "authors":"['Gaohang Chen', 'Zhonghua Qiao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Vision based driving agent for race car simulation environments",
        "abstract":"In recent years, autonomous driving has become a popular field of study. As control at tire grip limit is essential during emergency situations, algorithms developed for racecars are useful for road cars too. This paper examines the use of Deep Reinforcement Learning (DRL) to solve the problem of grip limit driving in a simulated environment. Proximal Policy Optimization (PPO) method is used to train an agent to control the steering wheel and pedals of the vehicle, using only visual inputs to achieve professional human lap times. The paper outlines the formulation of the task of time optimal driving on a race track as a deep reinforcement learning problem, and explains the chosen observations, actions, and reward functions. The results demonstrate human-like learning and driving behavior that utilize maximum tire grip potential.\n        △ Less",
        "authors":"['Gergely Bári', 'László Palkovics']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TinyverseGP: Towards a Modular Cross-domain Benchmarking Framework for Genetic Programming",
        "abstract":"Over the years, genetic programming (GP) has evolved, with many proposed variations, especially in how they represent a solution. Being essentially a program synthesis algorithm, it is capable of tackling multiple problem domains. Current benchmarking initiatives are fragmented, as the different representations are not compared with each other and their performance is not measured across the different domains. In this work, we propose a unified framework, dubbed TinyverseGP (inspired by tinyGP), which provides support to multiple representations and problem domains, including symbolic regression, logic synthesis and policy search.\n        △ Less",
        "authors":"['Roman Kalkreuth', 'Fabricio Olivetti de França', 'Julian Dierkes', 'Marie Anastacio', 'Anja Jankovic', 'Zdenek Vasicek', 'Holger Hoos']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for Dynamic Data Acquisition in Digital Twins",
        "abstract":"This paper introduces a sensor steering methodology based on deep reinforcement learning to enhance the predictive accuracy and decision support capabilities of digital twins by optimising the data acquisition process. Traditional sensor placement techniques are often constrained by one-off optimisation strategies, which limit their applicability for online applications requiring continuous informative data assimilation. The proposed approach addresses this limitation by offering an adaptive framework for sensor placement within the digital twin paradigm. The sensor placement problem is formulated as a Markov decision process, enabling the training and deployment of an agent capable of dynamically repositioning sensors in response to the evolving conditions of the physical structure as represented by the digital twin. This ensures that the digital twin maintains a highly representative and reliable connection to its physical counterpart. The proposed framework is validated through a series of comprehensive case studies involving a cantilever plate structure subjected to diverse conditions, including healthy and damaged conditions. The results demonstrate the capability of the deep reinforcement learning agent to adaptively reposition sensors improving the quality of data acquisition and hence enhancing the overall accuracy of digital twins.\n        △ Less",
        "authors":"['Collins O. Ogbodo', 'Timothy J. Rogers', 'Mattia Dal Borgo', 'David J. Wagg']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GNN-ACLP: Graph Neural Networks based Analog Circuit Link Prediction",
        "abstract":"Circuit link prediction identifying missing component connections from incomplete netlists is crucial in automating analog circuit design. However, existing methods face three main challenges: 1) Insufficient use of topological patterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to the complexity of annotations hinders model generalization; 3) Limited adaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural Networks (GNNs) based framework featuring three innovations to tackle these challenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes for Link Prediction) framework and achieve port-level accuracy in circuit link prediction. Second, we propose Netlist Babel Fish, a netlist format conversion tool leveraging retrieval-augmented generation (RAG) with large language model (LLM) to enhance the compatibility of netlist formats. Finally, we construct SpiceNetlist, a comprehensive dataset that contains 775 annotated circuits across 10 different classes of components. The experimental results demonstrate an improvement of 15.05% on the SpiceNetlist dataset and 12.01% on the Image2Net dataset over the existing approach.\n        △ Less",
        "authors":"['Guanyuan Pan', 'Tiansheng Zhou', 'Bingtao Ma', 'Yaqi Wang', 'Jianxiang Zhao', 'Shuai Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Model Zoo of Vision Transformers",
        "abstract":"The availability of large, structured populations of neural networks - called 'model zoos' - has led to the development of a multitude of downstream tasks ranging from model analysis, to representation learning on model weights or generative modeling of neural network parameters. However, existing model zoos are limited in size and architecture and neglect the transformer, which is among the currently most successful neural network architectures. We address this gap by introducing the first model zoo of vision transformers (ViT). To better represent recent training approaches, we develop a new blueprint for model zoo generation that encompasses both pre-training and fine-tuning steps, and publish 250 unique models. They are carefully generated with a large span of generating factors, and their diversity is validated using a thorough choice of weight-space and behavioral metrics. To further motivate the utility of our proposed dataset, we suggest multiple possible applications grounded in both extensive exploratory experiments and a number of examples from the existing literature. By extending previous lines of similar work, our model zoo allows researchers to push their model population-based methods from the small model regime to state-of-the-art architectures. We make our model zoo available at github.com\/ModelZoos\/ViTModelZoo.\n        △ Less",
        "authors":"['Damian Falk', 'Léo Meynent', 'Florence Pfammatter', 'Konstantin Schürholt', 'Damian Borth']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ROSFD: Robust Online Streaming Fraud Detection with Resilience to Concept Drift in Data Streams",
        "abstract":"Continuous generation of streaming data from diverse sources, such as online transactions and digital interactions, necessitates timely fraud detection. Traditional batch processing methods often struggle to capture the rapidly evolving patterns of fraudulent activities. This paper highlights the critical importance of processing streaming data for effective fraud detection. To address the inherent challenges of latency, scalability, and concept drift in streaming environments, we propose a robust online streaming fraud detection (ROSFD) framework. Our proposed framework comprises two key stages: (i) Stage One: Offline Model Initialization. In this initial stage, a model is built in offline settings using incremental learning principles to overcome the \"cold-start\" problem. (ii) Stage Two: Real-time Model Adaptation. In this dynamic stage, drift detection algorithms (viz.,, DDM, EDDM, and ADWIN) are employed to identify concept drift in the incoming data stream and incrementally train the model accordingly. This \"train-only-when-required\" strategy drastically reduces the number of retrains needed without significantly impacting the area under the receiver operating characteristic curve (AUC). Overall, ROSFD utilizing ADWIN as the drift detection method demonstrated the best performance among the employed methods. In terms of model efficacy, Adaptive Random Forest consistently outperformed other models, achieving the highest AUC in four out of five datasets.\n        △ Less",
        "authors":"['Vivek Yelleti']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From Prompting to Alignment: A Generative Framework for Query Recommendation",
        "abstract":"In modern search systems, search engines often suggest relevant queries to users through various panels or components, helping refine their information needs. Traditionally, these recommendations heavily rely on historical search logs to build models, which suffer from cold-start or long-tail issues. Furthermore, tasks such as query suggestion, completion or clarification are studied separately by specific design, which lacks generalizability and hinders adaptation to novel applications. Despite recent attempts to explore the use of LLMs for query recommendation, these methods mainly rely on the inherent knowledge of LLMs or external sources like few-shot examples, retrieved documents, or knowledge bases, neglecting the importance of the calibration and alignment with user feedback, thus limiting their practical utility. To address these challenges, we first propose a general Generative Query Recommendation (GQR) framework that aligns LLM-based query generation with user preference. Specifically, we unify diverse query recommendation tasks by a universal prompt framework, leveraging the instruct-following capability of LLMs for effective generation. Secondly, we align LLMs with user feedback via presenting a CTR-alignment framework, which involves training a query-wise CTR predictor as a process reward model and employing list-wise preference alignment to maximize the click probability of the generated query list. Furthermore, recognizing the inconsistency between LLM knowledge and proactive search intents arising from the separation of user-initiated queries from models, we align LLMs with user initiative via retrieving co-occurrence queries as side information when historical logs are available.\n        △ Less",
        "authors":"['Erxue Min', 'Hsiu-Yuan Huang', 'Min Yang', 'Xihong Yang', 'Xin Jia', 'Yunfang Wu', 'Hengyi Cai', 'Shuaiqiang Wang', 'Dawei Yin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Generative Model Training via Embedded Representation Warmup",
        "abstract":"Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40$\\times$ acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https:\/\/github.com\/LINs-lab\/ERW.\n        △ Less",
        "authors":"['Deyuan Liu', 'Peng Sun', 'Xufeng Li', 'Tao Lin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
        "abstract":"Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https:\/\/github.com\/OPTML-Group\/MU-Coreset.\n        △ Less",
        "authors":"['Soumyadeep Pal', 'Changsheng Wang', 'James Diffenderfer', 'Bhavya Kailkhura', 'Sijia Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Challenges in interpretability of additive models",
        "abstract":"We review generalized additive models as a type of ``transparent'' model that has recently seen renewed interest in the deep learning community as neural additive models. We highlight multiple types of nonidentifiability in this model class and discuss challenges in interpretability, arguing for restraint when claiming ``interpretability'' or ``suitability for safety-critical applications'' of such models.\n        △ Less",
        "authors":"['Xinyu Zhang', 'Julien Martinelli', 'ST John']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning",
        "abstract":"Large-scale reinforcement learning (RL) methods have proven highly effective in enhancing the reasoning abilities of large language models (LLMs), particularly for tasks with verifiable solutions such as mathematics and coding. However, applying this idea to machine translation (MT), where outputs are flexibly formatted and difficult to automatically evaluate with explicit rules, remains underexplored. In this work, we introduce MT-R1-Zero, the first open-source adaptation of the R1-Zero RL framework for MT without supervised fine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to guide LLMs towards improved translation quality via emergent reasoning. On the WMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive performance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points. Meanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across all metrics, placing it on par with advanced proprietary models such as GPT-4o and Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves state-of-the-art scores on semantic metrics. Moreover, our work exhibits strong generalization capabilities on out-of-distribution MT tasks, robustly supporting multilingual and low-resource settings. Extensive analysis of model behavior across different initializations and reward metrics offers pioneering insight into the critical role of reward design, LLM adaptability, training dynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT. Our code is available at https:\/\/github.com\/fzp0424\/MT-R1-Zero.\n        △ Less",
        "authors":"['Zhaopeng Feng', 'Shaosheng Cao', 'Jiahan Ren', 'Jiayuan Su', 'Ruizhe Chen', 'Yan Zhang', 'Zhe Xu', 'Yao Hu', 'Jian Wu', 'Zuozhu Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Continual learning for rotating machinery fault diagnosis with cross-domain environmental and operational variations",
        "abstract":"Although numerous machine learning models exist to detect issues like rolling bearing strain and deformation, typically caused by improper mounting, overloading, or poor lubrication, these models often struggle to isolate faults from the noise of real-world operational and environmental variability. Conditions such as variable loads, high temperatures, stress, and rotational speeds can mask early signs of failure, making reliable detection challenging. To address these limitations, this work proposes a continual deep learning approach capable of learning across domains that share underlying structure over time. This approach goes beyond traditional accuracy metrics by addressing four second-order challenges: catastrophic forgetting (where new learning overwrites past knowledge), lack of plasticity (where models fail to adapt to new data), forward transfer (using past knowledge to improve future learning), and backward transfer (refining past knowledge with insights from new domains). The method comprises a feature generator and domain-specific classifiers, allowing capacity to grow as new domains emerge with minimal interference, while an experience replay mechanism selectively revisits prior domains to mitigate forgetting. Moreover, nonlinear dependencies across domains are exploited by prioritizing replay from those with the highest prior errors, refining models based on most informative past experiences. Experiments show high average domain accuracy (up to 88.96%), with forgetting measures as low as .0027 across non-stationary class-incremental environments.\n        △ Less",
        "authors":"['Diogo Risca', 'Afonso Lourenço', 'Goreti Marreiros']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BoTTA: Benchmarking on-device Test Time Adaptation",
        "abstract":"The performance of deep learning models depends heavily on test samples at runtime, and shifts from the training data distribution can significantly reduce accuracy. Test-time adaptation (TTA) addresses this by adapting models during inference without requiring labeled test data or access to the original training set. While research has explored TTA from various perspectives like algorithmic complexity, data and class distribution shifts, model architectures, and offline versus continuous learning, constraints specific to mobile and edge devices remain underexplored. We propose BoTTA, a benchmark designed to evaluate TTA methods under practical constraints on mobile and edge devices. Our evaluation targets four key challenges caused by limited resources and usage conditions: (i) limited test samples, (ii) limited exposure to categories, (iii) diverse distribution shifts, and (iv) overlapping shifts within a sample. We assess state-of-the-art TTA methods under these scenarios using benchmark datasets and report system-level metrics on a real testbed. Furthermore, unlike prior work, we align with on-device requirements by advocating periodic adaptation instead of continuous inference-time adaptation. Experiments reveal key insights: many recent TTA algorithms struggle with small datasets, fail to generalize to unseen categories, and depend on the diversity and complexity of distribution shifts. BoTTA also reports device-specific resource use. For example, while SHOT improves accuracy by $2.25\\times$ with $512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi versus the base model. BoTTA offers actionable guidance for TTA in real-world, resource-constrained deployments.\n        △ Less",
        "authors":"['Michal Danilowski', 'Soumyajit Chatterjee', 'Abhirup Ghosh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Survey of Personalization: From RAG to Agent",
        "abstract":"Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more advanced agent-based architectures within personalized settings to enhance user satisfaction. Building on this foundation, this survey systematically examines personalization across the three core stages of RAG: pre-retrieval, retrieval, and generation. Beyond RAG, we further extend its capabilities into the realm of Personalized LLM-based Agents, which enhance traditional RAG systems with agentic functionalities, including user understanding, personalized planning and execution, and dynamic generation. For both personalization in RAG and agent-based personalization, we provide formal definitions, conduct a comprehensive review of recent literature, and summarize key datasets and evaluation metrics. Additionally, we discuss fundamental challenges, limitations, and promising research directions in this evolving field. Relevant papers and resources are continuously updated at https:\/\/github.com\/Applied-Machine-Learning-Lab\/Awesome-Personalized-RAG-Agent.\n        △ Less",
        "authors":"['Xiaopeng Li', 'Pengyue Jia', 'Derong Xu', 'Yi Wen', 'Yingyi Zhang', 'Wenlin Zhang', 'Wanyu Wang', 'Yichao Wang', 'Zhaocheng Du', 'Xiangyang Li', 'Yong Liu', 'Huifeng Guo', 'Ruiming Tang', 'Xiangyu Zhao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions",
        "abstract":"We propose GeoUni, the first unified geometry expert model capable of generating problem solutions and diagrams within a single framework in a way that enables the creation of unique and individualized geometry problems. Traditionally, solving geometry problems and generating diagrams have been treated as separate tasks in machine learning, with no models successfully integrating both to support problem creation. However, we believe that mastery in geometry requires frictionless integration of all of these skills, from solving problems to visualizing geometric relationships, and finally, crafting tailored problems. Our extensive experiments demonstrate that GeoUni, with only 1.5B parameters, achieves performance comparable to larger models such as DeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also excels in generating precise geometric diagrams, surpassing both text-to-image models and unified models, including the GPT-4o image generation. Most importantly, GeoUni is the only model capable of successfully generating textual problems with matching diagrams based on specific knowledge points, thus offering a wider range of capabilities that extend beyond current models.\n        △ Less",
        "authors":"['Jo-Ku Cheng', 'Zeren Zhang', 'Ran Chen', 'Jingyang Deng', 'Ziran Qin', 'Jinwen Ma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning",
        "abstract":"Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize misalignment by introducing two specific mechanisms: selection bias, where some semantic variables are missing, and perturbation bias, where semantic variables are distorted -- both affecting latent variables shared across modalities. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings through extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of misalignment on multimodal representation learning.\n        △ Less",
        "authors":"['Yichao Cai', 'Yuhang Liu', 'Erdun Gao', 'Tianjiao Jiang', 'Zhen Zhang', 'Anton van den Hengel', 'Javen Qinfeng Shi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Impact of Model Zoo Size and Composition on Weight Space Learning",
        "abstract":"Re-using trained neural network models is a common strategy to reduce training cost and transfer knowledge. Weight space learning - using the weights of trained models as data modality - is a promising new field to re-use populations of pre-trained models for future tasks. Approaches in this field have demonstrated high performance both on model analysis and weight generation tasks. However, until now their learning setup requires homogeneous model zoos where all models share the same exact architecture, limiting their capability to generalize beyond the population of models they saw during training. In this work, we remove this constraint and propose a modification to a common weight space learning method to accommodate training on heterogeneous populations of models. We further investigate the resulting impact of model diversity on generating unseen neural network model weights for zero-shot knowledge transfer. Our extensive experimental evaluation shows that including models with varying underlying image datasets has a high impact on performance and generalization, for both in- and out-of-distribution settings. Code is available on github.com\/HSG-AIML\/MultiZoo-SANE.\n        △ Less",
        "authors":"['Damian Falk', 'Konstantin Schürholt', 'Damian Borth']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conditional Distribution Compression via the Kernel Conditional Mean Embedding",
        "abstract":"Existing distribution compression methods, like Kernel Herding (KH), were originally developed for unlabelled data. However, no existing approach directly compresses the conditional distribution of labelled data. To address this gap, we first introduce the Average Maximum Conditional Mean Discrepancy (AMCMD), a natural metric for comparing conditional distributions. We then derive a consistent estimator for the AMCMD and establish its rate of convergence. Next, we make a key observation: in the context of distribution compression, the cost of constructing a compressed set targeting the AMCMD can be reduced from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(n)$. Building on this, we extend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a linear-time greedy algorithm that constructs a compressed set targeting the AMCMD. To better understand the advantages of directly compressing the conditional distribution rather than doing so via the joint distribution, we introduce Joint Kernel Herding (JKH), a straightforward adaptation of KH designed to compress the joint distribution of labelled data. While herding methods provide a simple and interpretable selection process, they rely on a greedy heuristic. To explore alternative optimisation strategies, we propose Joint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing Points (ACKIP), which jointly optimise the compressed set while maintaining linear complexity. Experiments show that directly preserving conditional distributions with ACKIP outperforms both joint distribution compression (via JKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP consistently outperforms JKH.\n        △ Less",
        "authors":"['Dominic Broadbent', 'Nick Whiteley', 'Robert Allison', 'Tom Lovett']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Uncertainty Propagation in the Fast Fourier Transform",
        "abstract":"We address the problem of uncertainty propagation in the discrete Fourier transform by modeling the fast Fourier transform as a factor graph. Building on this representation, we propose an efficient framework for approximate Bayesian inference using belief propagation (BP) and expectation propagation, extending its applicability beyond Gaussian assumptions. By leveraging an appropriate BP message representation and a suitable schedule, our method achieves stable convergence with accurate mean and variance estimates. Numerical experiments in representative scenarios from communications demonstrate the practical potential of the proposed framework for uncertainty-aware inference in probabilistic systems operating across both time and frequency domain.\n        △ Less",
        "authors":"['Luca Schmid', 'Charlotte Muth', 'Laurent Schmalen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"STaRFormer: Semi-Supervised Task-Informed Representation Learning via Dynamic Attention-Based Regional Masking for Sequential Data",
        "abstract":"Accurate predictions using sequential spatiotemporal data are crucial for various applications. Utilizing real-world data, we aim to learn the intent of a smart device user within confined areas of a vehicle's surroundings. However, in real-world scenarios, environmental factors and sensor limitations result in non-stationary and irregularly sampled data, posing significant challenges. To address these issues, we developed a Transformer-based approach, STaRFormer, which serves as a universal framework for sequential modeling. STaRFormer employs a novel, dynamic attention-based regional masking scheme combined with semi-supervised contrastive learning to enhance task-specific latent representations. Comprehensive experiments on 15 datasets varying in types (including non-stationary and irregularly sampled), domains, sequence lengths, training samples, and applications, demonstrate the efficacy and practicality of STaRFormer. We achieve notable improvements over state-of-the-art approaches. Code and data will be made available.\n        △ Less",
        "authors":"['Maxmilian Forstenhäusler', 'Daniel Külzer', 'Christos Anagnostopoulos', 'Shameem Puthiya Parambath', 'Natascha Weber']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Quantifying Commonsense Reasoning with Mechanistic Insights",
        "abstract":"Commonsense reasoning deals with the implicit knowledge that is well understood by humans and typically acquired via interactions with the world. In recent times, commonsense reasoning and understanding of various LLMs have been evaluated using text-based tasks. In this work, we argue that a proxy of this understanding can be maintained as a graphical structure that can further help to perform a rigorous evaluation of commonsense reasoning abilities about various real-world activities. We create an annotation scheme for capturing this implicit knowledge in the form of a graphical structure for 37 daily human activities. We find that the created resource can be used to frame an enormous number of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of commonsense reasoning in LLMs. Moreover, recently, the remarkable performance of LLMs has raised questions about whether these models are truly capable of reasoning in the wild and, in general, how reasoning occurs inside these models. In this resource paper, we bridge this gap by proposing design mechanisms that facilitate research in a similar direction. Our findings suggest that the reasoning components are localized in LLMs that play a prominent role in decision-making when prompted with a commonsense query.\n        △ Less",
        "authors":"['Abhinav Joshi', 'Areeb Ahmad', 'Divyaksh Shukla', 'Ashutosh Modi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Scalable Bayesian Optimization via Gradient-Informed Bayesian Neural Networks",
        "abstract":"Bayesian optimization (BO) is a widely used method for data-driven optimization that generally relies on zeroth-order data of objective function to construct probabilistic surrogate models. These surrogates guide the exploration-exploitation process toward finding global optimum. While Gaussian processes (GPs) are commonly employed as surrogates of the unknown objective function, recent studies have highlighted the potential of Bayesian neural networks (BNNs) as scalable and flexible alternatives. Moreover, incorporating gradient observations into GPs, when available, has been shown to improve BO performance. However, the use of gradients within BNN surrogates remains unexplored. By leveraging automatic differentiation, gradient information can be seamlessly integrated into BNN training, resulting in more informative surrogates for BO. We propose a gradient-informed loss function for BNN training, effectively augmenting function observations with local gradient information. The effectiveness of this approach is demonstrated on well-known benchmarks in terms of improved BNN predictions and faster BO convergence as the number of decision variables increases.\n        △ Less",
        "authors":"['Georgios Makrygiorgos', 'Joshua Hang Sai Ip', 'Ali Mesbah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Undermining Federated Learning Accuracy in EdgeIoT via Variational Graph Auto-Encoders",
        "abstract":"EdgeIoT represents an approach that brings together mobile edge computing with Internet of Things (IoT) devices, allowing for data processing close to the data source. Sending source data to a server is bandwidth-intensive and may compromise privacy. Instead, federated learning allows each device to upload a shared machine-learning model update with locally processed data. However, this technique, which depends on aggregating model updates from various IoT devices, is vulnerable to attacks from malicious entities that may inject harmful data into the learning process. This paper introduces a new attack method targeting federated learning in EdgeIoT, known as data-independent model manipulation attack. This attack does not rely on training data from the IoT devices but instead uses an adversarial variational graph auto-encoder (AV-GAE) to create malicious model updates by analyzing benign model updates intercepted during communication. AV-GAE identifies and exploits structural relationships between benign models and their training data features. By manipulating these structural correlations, the attack maximizes the training loss of the federated learning system, compromising its overall effectiveness.\n        △ Less",
        "authors":"['Kai Li', 'Shuyan Hu', 'Bochun Wu', 'Sai Zou', 'Wei Ni', 'Falko Dressler']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CHARM: Calibrating Reward Models With Chatbot Arena Scores",
        "abstract":"Reward models (RMs) play a crucial role in Reinforcement Learning from Human Feedback by serving as proxies for human preferences in aligning large language models. In this paper, we identify a model preference bias in RMs, where they systematically assign disproportionately high scores to responses from certain policy models. This bias distorts ranking evaluations and leads to unfair judgments. To address this issue, we propose a calibration method named CHatbot Arena calibrated Reward Modeling (CHARM) that leverages Elo scores from the Chatbot Arena leaderboard to mitigate RM overvaluation. We also introduce a Mismatch Degree metric to measure this preference bias. Our approach is computationally efficient, requiring only a small preference dataset for continued training of the RM. We conduct extensive experiments on reward model benchmarks and human preference alignment. Results demonstrate that our calibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the Chat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with human preferences by producing scores more closely aligned with Elo rankings. By mitigating model preference bias, our method provides a generalizable and efficient solution for building fairer and more reliable reward models.\n        △ Less",
        "authors":"['Xiao Zhu', 'Chenmien Tan', 'Pinzhen Chen', 'Rico Sennrich', 'Yanlin Zhang', 'Hanxu Hu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Investigating the Role of Bilateral Symmetry for Inpainting Brain MRI",
        "abstract":"Inpainting has recently emerged as a valuable and interesting technology to employ in the analysis of medical imaging data, in particular brain MRI. A wide variety of methodologies for inpainting MRI have been proposed and demonstrated on tasks including anomaly detection. In this work we investigate the statistical relationship between inpainted brain structures and the amount of subject-specific conditioning information, i.e. the other areas of the image that are masked. In particular, we analyse the distribution of inpainting results when masking additional regions of the image, specifically the contra-lateral structure. This allows us to elucidate where in the brain the model is drawing information from, and in particular, what is the importance of hemispherical symmetry? Our experiments interrogate a diffusion inpainting model through analysing the inpainting of subcortical brain structures based on intensity and estimated area change. We demonstrate that some structures show a strong influence of symmetry in the conditioning of the inpainting process.\n        △ Less",
        "authors":"['Sergey Kuznetsov', 'Sanduni Pinnawala', 'Peter A. Wijeratne', 'Ivor J. A. Simpson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using Reinforcement Learning to Integrate Subjective Wellbeing into Climate Adaptation Decision Making",
        "abstract":"Subjective wellbeing is a fundamental aspect of human life, influencing life expectancy and economic productivity, among others. Mobility plays a critical role in maintaining wellbeing, yet the increasing frequency and intensity of both nuisance and high-impact floods due to climate change are expected to significantly disrupt access to activities and destinations, thereby affecting overall wellbeing. Addressing climate adaptation presents a complex challenge for policymakers, who must select and implement policies from a broad set of options with varying effects while managing resource constraints and uncertain climate projections. In this work, we propose a multi-modular framework that uses reinforcement learning as a decision-support tool for climate adaptation in Copenhagen, Denmark. Our framework integrates four interconnected components: long-term rainfall projections, flood modeling, transport accessibility, and wellbeing modeling. This approach enables decision-makers to identify spatial and temporal policy interventions that help sustain or enhance subjective wellbeing over time. By modeling climate adaptation as an open-ended system, our framework provides a structured framework for exploring and evaluating adaptation policy pathways. In doing so, it supports policymakers to make informed decisions that maximize wellbeing in the long run.\n        △ Less",
        "authors":"['Arthur Vandervoort', 'Miguel Costa', 'Morten W. Petersen', 'Martin Drews', 'Sonja Haustein', 'Karyn Morrissey', 'Francisco C. Pereira']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sequence models for by-trial decoding of cognitive strategies from neural data",
        "abstract":"Understanding the sequence of cognitive operations that underlie decision-making is a fundamental challenge in cognitive neuroscience. Traditional approaches often rely on group-level statistics, which obscure trial-by-trial variations in cognitive strategies. In this study, we introduce a novel machine learning method that combines Hidden Multivariate Pattern analysis with a Structured State Space Sequence model to decode cognitive strategies from electroencephalography data at the trial level. We apply this method to a decision-making task, where participants were instructed to prioritize either speed or accuracy in their responses. Our results reveal an additional cognitive operation, labeled Confirmation, which seems to occur predominantly in the accuracy condition but also frequently in the speed condition. The modeled probability that this operation occurs is associated with higher probability of responding correctly as well as changes of mind, as indexed by electromyography data. By successfully modeling cognitive operations at the trial level, we provide empirical evidence for dynamic variability in decision strategies, challenging the assumption of homogeneous cognitive processes within experimental conditions. Our approach shows the potential of sequence modeling in cognitive neuroscience to capture trial-level variability that is obscured by aggregate analyses. The introduced method offers a new way to detect and understand cognitive strategies in a data-driven manner, with implications for both theoretical research and practical applications in many fields.\n        △ Less",
        "authors":"['Rick den Otter', 'Gabriel Weindel', 'Sjoerd Stuit', 'Leendert van Maanen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics",
        "abstract":"Whereas in general computer vision, transformer-based architectures have quickly become the gold standard, microelectronics defect detection still heavily relies on convolutional neural networks (CNNs). We hypothesize that this is due to the fact that a) transformers have an increased need for data and b) labelled image generation procedures for microelectronics are costly, and labelled data is therefore sparse. Whereas in other domains, pre-training on large natural image datasets can mitigate this problem, in microelectronics transfer learning is hindered due to the dissimilarity of domain data and natural images. Therefore, we evaluate self pre-training, where models are pre-trained on the target dataset, rather than another dataset. We propose a vision transformer (ViT) pre-training framework for defect detection in microelectronics based on masked autoencoders (MAE). In MAE, a large share of image patches is masked and reconstructed by the model during pre-training. We perform pre-training and defect detection using a dataset of less than 10.000 scanning acoustic microscopy (SAM) images labelled using transient thermal analysis (TTA). Our experimental results show that our approach leads to substantial performance gains compared to a) supervised ViT, b) ViT pre-trained on natural image datasets, and c) state-of-the-art CNN-based defect detection models used in the literature. Additionally, interpretability analysis reveals that our self pre-trained models, in comparison to ViT baselines, correctly focus on defect-relevant features such as cracks in the solder material. This demonstrates that our approach yields fault-specific feature representations, making our self pre-trained models viable for real-world defect detection in microelectronics.\n        △ Less",
        "authors":"['Nikolai Röhrich', 'Alwin Hoffmann', 'Richard Nordsieck', 'Emilio Zarbali', 'Alireza Javanmardi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Air Quality Prediction with A Meteorology-Guided Modality-Decoupled Spatio-Temporal Network",
        "abstract":"Air quality prediction plays a crucial role in public health and environmental protection. Accurate air quality prediction is a complex multivariate spatiotemporal problem, that involves interactions across temporal patterns, pollutant correlations, spatial station dependencies, and particularly meteorological influences that govern pollutant dispersion and chemical transformations. Existing works underestimate the critical role of atmospheric conditions in air quality prediction and neglect comprehensive meteorological data utilization, thereby impairing the modeling of dynamic interdependencies between air quality and meteorological data. To overcome this, we propose MDSTNet, an encoder-decoder framework that explicitly models air quality observations and atmospheric conditions as distinct modalities, integrating multi-pressure-level meteorological data and weather forecasts to capture atmosphere-pollution dependencies for prediction. Meantime, we construct ChinaAirNet, the first nationwide dataset combining air quality records with multi-pressure-level meteorological observations. Experimental results on ChinaAirNet demonstrate MDSTNet's superiority, substantially reducing 48-hour prediction errors by 17.54\\% compared to the state-of-the-art model. The source code and dataset will be available on github.\n        △ Less",
        "authors":"['Hang Yin', 'Yan-Ming Zhang', 'Jian Xu', 'Jian-Long Chang', 'Yin Li', 'Cheng-Lin Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration",
        "abstract":"In recent years, deep neural networks (DNNs) have demonstrated state-of-the-art performance across various domains. However, despite their success, they often face calibration issues, particularly in safety-critical applications such as autonomous driving and healthcare, where unreliable predictions can have serious consequences. Recent research has started to improve model calibration from the view of the classifier. However, the exploration of designing the classifier to solve the model calibration problem is insufficient. Let alone most of the existing methods ignore the calibration errors arising from underconfidence. In this work, we propose a novel method by balancing learnable and ETF classifiers to solve the overconfidence or underconfidence problem for model Calibration named BalCAL. By introducing a confidence-tunable module and a dynamic adjustment method, we ensure better alignment between model confidence and its true accuracy. Extensive experimental validation shows that ours significantly improves model calibration performance while maintaining high predictive accuracy, outperforming existing techniques. This provides a novel solution to the calibration challenges commonly encountered in deep learning.\n        △ Less",
        "authors":"['Jiani Ni', 'He Zhao', 'Jintong Gao', 'Dandan Guo', 'Hongyuan Zha']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving Controller Generalization with Dimensionless Markov Decision Processes",
        "abstract":"Controllers trained with Reinforcement Learning tend to be very specialized and thus generalize poorly when their testing environment differs from their training one. We propose a Model-Based approach to increase generalization where both world model and policy are trained in a dimensionless state-action space. To do so, we introduce the Dimensionless Markov Decision Process ($Π$-MDP): an extension of Contextual-MDPs in which state and action spaces are non-dimensionalized with the Buckingham-$Π$ theorem. This procedure induces policies that are equivariant with respect to changes in the context of the underlying dynamics. We provide a generic framework for this approach and apply it to a model-based policy search algorithm using Gaussian Process models. We demonstrate the applicability of our method on simulated actuated pendulum and cartpole systems, where policies trained on a single environment are robust to shifts in the distribution of the context.\n        △ Less",
        "authors":"['Valentin Charvet', 'Sebastian Stein', 'Roderick Murray-Smith']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Session-based Recommender Systems: User Interest as a Stochastic Process in the Latent Space",
        "abstract":"This paper jointly addresses the problem of data uncertainty, popularity bias, and exposure bias in session-based recommender systems. We study the symptoms of this bias both in item embeddings and in recommendations. We propose treating user interest as a stochastic process in the latent space and providing a model-agnostic implementation of this mathematical concept. The proposed stochastic component consists of elements: debiasing item embeddings with regularization for embedding uniformity, modeling dense user interest from session prefixes, and introducing fake targets in the data to simulate extended exposure. We conducted computational experiments on two popular benchmark datasets, Diginetica and YooChoose 1\/64, as well as several modifications of the YooChoose dataset with different ratios of popular items. The results show that the proposed approach allows us to mitigate the challenges mentioned.\n        △ Less",
        "authors":"['Klaudia Balcer', 'Piotr Lipinski']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation of Reward Functions",
        "abstract":"Preference-based reinforcement learning (PbRL) is a suitable approach for style adaptation of pre-trained robotic behavior: adapting the robot's policy to follow human user preferences while still being able to perform the original task. However, collecting preferences for the adaptation process in robotics is often challenging and time-consuming. In this work we explore the adaptation of pre-trained robots in the low-preference-data regime. We show that, in this regime, recent adaptation approaches suffer from catastrophic reward forgetting (CRF), where the updated reward model overfits to the new preferences, leading the agent to become unable to perform the original task. To mitigate CRF, we propose to enhance the original reward model with a small number of parameters (low-rank matrices) responsible for modeling the preference adaptation. Our evaluation shows that our method can efficiently and effectively adjust robotic behavior to human preferences across simulation benchmark tasks and multiple real-world robotic tasks.\n        △ Less",
        "authors":"['Daniel Marta', 'Simon Holk', 'Miguel Vasco', 'Jens Lundell', 'Timon Homberger', 'Finn Busch', 'Olov Andersson', 'Danica Kragic', 'Iolanda Leite']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
        "abstract":"Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations. However, the lack of safety measures specifically designed for multi-modal inputs creates an alignment gap, leaving MLLMs vulnerable to vision-domain attacks such as typographic manipulation. Current methods utilize a carefully designed safety dataset to enhance model defense capability, while the specific knowledge or patterns acquired from the high-quality dataset remain unclear. Through comparison experiments, we find that the alignment gap primarily arises from data distribution biases, while image content, response quality, or the contrastive behavior of the dataset makes little contribution to boosting multi-modal safety. To further investigate this and identify the key factors in improving MLLM safety, we propose finetuning MLLMs on a small set of benign instruct-following data with responses replaced by simple, clear rejection sentences. Experiments show that, without the need for labor-intensive collection of high-quality malicious data, model safety can still be significantly improved, as long as a specific fraction of rejection data exists in the finetuning set, indicating the security alignment is not lost but rather obscured during multi-modal pretraining or instruction finetuning. Simply correcting the underlying data bias could narrow the safety gap in the vision domain.\n        △ Less",
        "authors":"['Yanbo Wang', 'Jiyang Guan', 'Jian Liang', 'Ran He']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Physical Scales Matter: The Role of Receptive Fields and Advection in Satellite-Based Thunderstorm Nowcasting with Convolutional Neural Networks",
        "abstract":"The focus of nowcasting development is transitioning from physically motivated advection methods to purely data-driven Machine Learning (ML) approaches. Nevertheless, recent work indicates that incorporating advection into the ML value chain has improved skill for radar-based precipitation nowcasts. However, the generality of this approach and the underlying causes remain unexplored. This study investigates the generality by probing the approach on satellite-based thunderstorm nowcasts for the first time. Resorting to a scale argument, we then put forth an explanation when and why skill improvements can be expected. In essence, advection guarantees that thunderstorm patterns relevant for nowcasting are contained in the receptive field at long lead times. To test our hypotheses, we train ResU-Nets solving segmentation tasks with lightning observations as ground truth. The input of the Baseline Neural Network (BNN) are short time series of multispectral satellite imagery and lightning observations, whereas the Advection-Informed Neural Network (AINN) additionally receives the Lagrangian persistence nowcast of all input channels at the desired lead time. Overall, we find only a minor skill improvement of the AINN over the BNN when considering fully averaged scores. However, assessing skill conditioned on lead time and wind speed, we demonstrate that our scale argument correctly predicts the onset of skill improvement of the AINN over the BNN after 2h lead time. We confirm that generally advection becomes gradually more important with longer lead times and higher wind speeds. Our work accentuates the importance of considering and incorporating the underlying physical scales when designing ML based forecasting models.\n        △ Less",
        "authors":"['Christoph Metzl', 'Kianusch Vahid Yousefnia', 'Richard Müller', 'Virginia Poli', 'Miria Celano', 'Tobias Bölle']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AimTS: Augmented Series and Image Contrastive Learning for Time Series Classification",
        "abstract":"Time series classification (TSC) is an important task in time series analysis. Existing TSC methods mainly train on each single domain separately, suffering from a degradation in accuracy when the samples for training are insufficient in certain domains. The pre-training and fine-tuning paradigm provides a promising direction for solving this problem. However, time series from different domains are substantially divergent, which challenges the effective pre-training on multi-source data and the generalization ability of pre-trained models. To handle this issue, we introduce Augmented Series and Image Contrastive Learning for Time Series Classification (AimTS), a pre-training framework that learns generalizable representations from multi-source time series data. We propose a two-level prototype-based contrastive learning method to effectively utilize various augmentations in multi-source pre-training, which learns representations for TSC that can be generalized to different domains. In addition, considering augmentations within the single time series modality are insufficient to fully address classification problems with distribution shift, we introduce the image modality to supplement structural information and establish a series-image contrastive learning to improve the generalization of the learned representations for TSC tasks. Extensive experiments show that after multi-source pre-training, AimTS achieves good generalization performance, enabling efficient learning and even few-shot learning on various downstream TSC datasets.\n        △ Less",
        "authors":"['Yuxuan Chen', 'Shanshan Huang', 'Yunyao Cheng', 'Peng Chen', 'Zhongwen Rao', 'Yang Shu', 'Bin Yang', 'Lujia Pan', 'Chenjuan Guo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"IsoSEL: Isometric Structural Entropy Learning for Deep Graph Clustering in Hyperbolic Space",
        "abstract":"Graph clustering is a longstanding topic in machine learning. In recent years, deep learning methods have achieved encouraging results, but they still require predefined cluster numbers K, and typically struggle with imbalanced graphs, especially in identifying minority clusters. The limitations motivate us to study a challenging yet practical problem: deep graph clustering without K considering the imbalance in reality. We approach this problem from a fresh perspective of information theory (i.e., structural information). In the literature, structural information has rarely been touched in deep clustering, and the classic definition falls short in its discrete formulation, neglecting node attributes and exhibiting prohibitive complexity. In this paper, we first establish a new Differentiable Structural Information, generalizing the discrete formalism to continuous realm, so that the optimal partitioning tree, revealing the cluster structure, can be created by the gradient backpropagation. Theoretically, we demonstrate its capability in clustering without requiring K and identifying the minority clusters in imbalanced graphs, while reducing the time complexity to O(N) w.r.t. the number of nodes. Subsequently, we present a novel IsoSEL framework for deep graph clustering, where we design a hyperbolic neural network to learn the partitioning tree in the Lorentz model of hyperbolic space, and further conduct Lorentz Tree Contrastive Learning with isometric augmentation. As a result, the partitioning tree incorporates node attributes via mutual information maximization, while the cluster assignment is refined by the proposed tree contrastive learning. Extensive experiments on five benchmark datasets show the IsoSEL outperforms 14 recent baselines by an average of +1.3% in NMI.\n        △ Less",
        "authors":"['Li Sun', 'Zhenhao Huang', 'Yujie Wang', 'Hongbo Lv', 'Chunyang Liu', 'Hao Peng', 'Philip S. Yu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data",
        "abstract":"The emergence of medical generalist foundation models has revolutionized conventional task-specific model development paradigms, aiming to better handle multiple tasks through joint training on large-scale medical datasets. However, recent advances prioritize simple data scaling or architectural component enhancement, while neglecting to re-examine multi-task learning from a data-centric perspective. Critically, simply aggregating existing data resources leads to decentralized image-task alignment, which fails to cultivate comprehensive image understanding or align with clinical needs for multi-dimensional image interpretation. In this paper, we introduce the image-centric multi-annotation X-ray dataset (IMAX), the first attempt to enhance the multi-task learning capabilities of medical multi-modal large language models (MLLMs) from the data construction level. To be specific, IMAX is featured from the following attributes: 1) High-quality data curation. A comprehensive collection of more than 354K entries applicable to seven different medical tasks. 2) Image-centric dense annotation. Each X-ray image is associated with an average of 4.10 tasks and 7.46 training entries, ensuring multi-task representation richness per image. Compared to the general decentralized multi-annotation X-ray dataset (DMAX), IMAX consistently demonstrates significant multi-task average performance gains ranging from 3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs. Moreover, we investigate differences in statistical patterns exhibited by IMAX and DMAX training processes, exploring potential correlations between optimization dynamics and multi-task performance. Finally, leveraging the core concept of IMAX data construction, we propose an optimized DMAX-based training strategy to alleviate the dilemma of obtaining high-quality IMAX data in practical scenarios.\n        △ Less",
        "authors":"['Xun Zhu', 'Fanbin Mo', 'Zheng Zhang', 'Jiaxi Wang', 'Yiming Shi', 'Ming Wu', 'Chuang Zhang', 'Miao Li', 'Ji Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Unbiased Federated Graph Learning: Label and Topology Perspectives",
        "abstract":"Federated Graph Learning (FGL) enables privacy-preserving, distributed training of graph neural networks without sharing raw data. Among its approaches, subgraph-FL has become the dominant paradigm, with most work focused on improving overall node classification accuracy. However, these methods often overlook fairness due to the complexity of node features, labels, and graph structures. In particular, they perform poorly on nodes with disadvantaged properties, such as being in the minority class within subgraphs or having heterophilous connections (neighbors with dissimilar labels or misleading features). This reveals a critical issue: high accuracy can mask degraded performance on structurally or semantically marginalized nodes. To address this, we advocate for two fairness goals: (1) improving representation of minority class nodes for class-wise fairness and (2) mitigating topological bias from heterophilous connections for topology-aware fairness. We propose FairFGL, a novel framework that enhances fairness through fine-grained graph mining and collaborative learning. On the client side, the History-Preserving Module prevents overfitting to dominant local classes, while the Majority Alignment Module refines representations of heterophilous majority-class nodes. The Gradient Modification Module transfers minority-class knowledge from structurally favorable clients to improve fairness. On the server side, FairFGL uploads only the most influenced subset of parameters to reduce communication costs and better reflect local distributions. A cluster-based aggregation strategy reconciles conflicting updates and curbs global majority dominance . Extensive evaluations on eight benchmarks show FairFGL significantly improves minority-group performance , achieving up to a 22.62 percent Macro-F1 gain while enhancing convergence over state-of-the-art baselines.\n        △ Less",
        "authors":"['Zhengyu Wu', 'Boyang Pang', 'Xunkai Li', 'Yinlin Zhu', 'Daohan Su', 'Bowen Fan', 'Rong-Hua Li', 'Guoren Wang', 'Chenghu Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Semantic Depth Matters: Explaining Errors of Deep Vision Networks through Perceived Class Similarities",
        "abstract":"Understanding deep neural network (DNN) behavior requires more than evaluating classification accuracy alone; analyzing errors and their predictability is equally crucial. Current evaluation methodologies lack transparency, particularly in explaining the underlying causes of network misclassifications. To address this, we introduce a novel framework that investigates the relationship between the semantic hierarchy depth perceived by a network and its real-data misclassification patterns. Central to our framework is the Similarity Depth (SD) metric, which quantifies the semantic hierarchy depth perceived by a network along with a method of evaluation of how closely the network's errors align with its internally perceived similarity structure. We also propose a graph-based visualization of model semantic relationships and misperceptions. A key advantage of our approach is that leveraging class templates -- representations derived from classifier layer weights -- is applicable to already trained networks without requiring additional data or experiments. Our approach reveals that deep vision networks encode specific semantic hierarchies and that high semantic depth improves the compliance between perceived class similarities and actual errors.\n        △ Less",
        "authors":"['Katarzyna Filus', 'Michał Romaszewski', 'Mateusz Żarski']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Weaker Variance Assumptions for Stochastic Optimization",
        "abstract":"We revisit a classical assumption for analyzing stochastic gradient algorithms where the squared norm of the stochastic subgradient (or the variance for smooth problems) is allowed to grow as fast as the squared norm of the optimization variable. We contextualize this assumption in view of its inception in the 1960s, its seemingly independent appearance in the recent literature, its relationship to weakest-known variance assumptions for analyzing stochastic gradient algorithms, and its relevance in deterministic problems for non-Lipschitz nonsmooth convex optimization. We build on and extend a connection recently made between this assumption and the Halpern iteration. For convex nonsmooth, and potentially stochastic, optimization, we analyze horizon-free, anytime algorithms with last-iterate rates. For problems beyond simple constrained optimization, such as convex problems with functional constraints or regularized convex-concave min-max problems, we obtain rates for optimality measures that do not require boundedness of the feasible set.\n        △ Less",
        "authors":"['Ahmet Alacaoglu', 'Yura Malitsky', 'Stephen J. Wright']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FedRecon: Missing Modality Reconstruction in Distributed Heterogeneous Environments",
        "abstract":"Multimodal data are often incomplete and exhibit Non-Independent and Identically Distributed (Non-IID) characteristics in real-world scenarios. These inherent limitations lead to both modality heterogeneity through partial modality absence and data heterogeneity from distribution divergence, creating fundamental challenges for effective federated learning (FL). To address these coupled challenges, we propose FedRecon, the first method targeting simultaneous missing modality reconstruction and Non-IID adaptation in multimodal FL. Our approach first employs a lightweight Multimodal Variational Autoencoder (MVAE) to reconstruct missing modalities while preserving cross-modal consistency. Distinct from conventional imputation methods, we achieve sample-level alignment through a novel distribution mapping mechanism that guarantees both data consistency and completeness. Additionally, we introduce a strategy employing global generator freezing to prevent catastrophic forgetting, which in turn mitigates Non-IID fluctuations. Extensive evaluations on multimodal datasets demonstrate FedRecon's superior performance in modality reconstruction under Non-IID conditions, surpassing state-of-the-art methods.\n        △ Less",
        "authors":"['Junming Liu', 'Guosun Zeng', 'Ding Wang', 'Yanting Gao', 'Yufei Jin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TianQuan-Climate: A Subseasonal-to-Seasonal Global Weather Model via Incorporate Climatology State",
        "abstract":"Subseasonal forecasting serves as an important support for Sustainable Development Goals (SDGs), such as climate challenges, agricultural yield and sustainable energy production. However, subseasonal forecasting is a complex task in meteorology due to dissipating initial conditions and delayed external forces. Although AI models are increasingly pushing the boundaries of this forecasting limit, they face two major challenges: error accumulation and Smoothness. To address these two challenges, we propose Climate Furnace Subseasonal-to-Seasonal (TianQuan-Climate), a novel machine learning model designed to provide global daily mean forecasts up to 45 days, covering five upper-air atmospheric variables at 13 pressure levels and two surface variables. Our proposed TianQuan-Climate has two advantages: 1) it utilizes a multi-model prediction strategy to reduce system error impacts in long-term subseasonal forecasts; 2) it incorporates a Content Fusion Module for climatological integration and extends ViT with uncertainty blocks (UD-ViT) to improve generalization by learning from uncertainty. We demonstrate the effectiveness of TianQuan-Climate on benchmarks for weather forecasting and climate projections within the 15 to 45-day range, where TianQuan-Climate outperforms existing numerical and AI methods.\n        △ Less",
        "authors":"['Guowen Li', 'Xintong Liu', 'Shilei Cao', 'Haoyuan Liang', 'Mengxuan Chen', 'Lixian Zhang', 'Jinxiao Zhang', 'Jiuke Wang', 'Meng Jin', 'Juepeng Zheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference",
        "abstract":"Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.\n        △ Less",
        "authors":"['Yuxuan Tian', 'Zihan Wang', 'Yebo Peng', 'Aomufei Yuan', 'Zhiming Wang', 'Bairen Yi', 'Xin Liu', 'Yong Cui', 'Tong Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-objective Bayesian Optimization With Mixed-categorical Design Variables for Expensive-to-evaluate Aeronautical Applications",
        "abstract":"This work aims at developing new methodologies to optimize computational costly complex systems (e.g., aeronautical engineering systems). The proposed surrogate-based method (often called Bayesian optimization) uses adaptive sampling to promote a trade-off between exploration and exploitation. Our in-house implementation, called SEGOMOE, handles a high number of design variables (continuous, discrete or categorical) and nonlinearities by combining mixtures of experts for the objective and\/or the constraints. Additionally, the method handles multi-objective optimization settings, as it allows the construction of accurate Pareto fronts with a minimal number of function evaluations. Different infill criteria have been implemented to handle multiple objectives with or without constraints. The effectiveness of the proposed method was tested on practical aeronautical applications within the context of the European Project AGILE 4.0 and demonstrated favorable results. A first example concerns a retrofitting problem where a comparison between two optimizers have been made. A second example introduces hierarchical variables to deal with architecture system in order to design an aircraft family. The third example increases drastically the number of categorical variables as it combines aircraft design, supply chain and manufacturing process. In this article, we show, on three different realistic problems, various aspects of our optimization codes thanks to the diversity of the treated aircraft problems.\n        △ Less",
        "authors":"['Nathalie Bartoli', 'Thierry Lefebvre', 'Rémi Lafage', 'Paul Saves', 'Youssef Diouane', 'Joseph Morlier', 'Jasper Bussemaker', 'Giuseppa Donelli', 'Joao Marcos Gomes de Mello', 'Massimo Mandorino', 'Pierluigi Della Vecchia']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Moderate Actor-Critic Methods: Controlling Overestimation Bias via Expectile Loss",
        "abstract":"Overestimation is a fundamental characteristic of model-free reinforcement learning (MF-RL), arising from the principles of temporal difference learning and the approximation of the Q-function. To address this challenge, we propose a novel moderate target in the Q-function update, formulated as a convex optimization of an overestimated Q-function and its lower bound. Our primary contribution lies in the efficient estimation of this lower bound through the lower expectile of the Q-value distribution conditioned on a state. Notably, our moderate target integrates seamlessly into state-of-the-art (SOTA) MF-RL algorithms, including Deep Deterministic Policy Gradient (DDPG) and Soft Actor Critic (SAC). Experimental results validate the effectiveness of our moderate target in mitigating overestimation bias in DDPG, SAC, and distributional RL algorithms.\n        △ Less",
        "authors":"['Ukjo Hwang', 'Songnam Hong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data",
        "abstract":"Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are resource-intensive but necessary for human preference transferring. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function for LLM alignment. Using similarity as a reward circumvents training reward models, and collecting a single reference answer potentially costs less time than constructing binary preference pairs when multiple candidates are available. Specifically, we develop \\textit{RefAlign}, a versatile REINFORCE-style alignment algorithm, which is free of reference and reward models. Instead, RefAlign utilizes BERTScore between sampled generations and high-quality reference answers as the surrogate reward. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, {RefAlign} demonstrates comparable performance to previous alignment methods while offering high efficiency.\n        △ Less",
        "authors":"['Shuai Zhao', 'Linchao Zhu', 'Yi Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Truncated Matrix Completion - An Empirical Study",
        "abstract":"Low-rank Matrix Completion (LRMC) describes the problem where we wish to recover missing entries of partially observed low-rank matrix. Most existing matrix completion work deals with sampling procedures that are independent of the underlying data values. While this assumption allows the derivation of nice theoretical guarantees, it seldom holds in real-world applications. In this paper, we consider various settings where the sampling mask is dependent on the underlying data values, motivated by applications in sensing, sequential decision-making, and recommender systems. Through a series of experiments, we study and compare the performance of various LRMC algorithms that were originally successful for data-independent sampling patterns.\n        △ Less",
        "authors":"['Rishhabh Naik', 'Nisarg Trivedi', 'Davoud Ataee Tarzanagh', 'Laura Balzano']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures",
        "abstract":"Irregular embedding lookups are a critical bottleneck in recommender models, sparse large language models, and graph learning models. In this paper, we first demonstrate that, by offloading these lookups to specialized access units, Decoupled Access-Execute (DAE) processors achieve 2.6$\\times$ higher performance and 6.4$\\times$ higher performance\/watt than GPUs on end-to-end models. Then, we propose the Ember compiler for automatically generating optimized DAE code from PyTorch and TensorFlow. Conversely from other DAE compilers, Ember features multiple intermediate representations specifically designed for different optimization levels. In this way, Ember can implement all optimizations to match the performance of hand-written code, unlocking the full potential of DAE architectures at scale.\n        △ Less",
        "authors":"['Marco Siracusa', 'Olivia Hsu', 'Victor Soria-Pardos', 'Joshua Randall', 'Arnaud Grasset', 'Eric Biscondi', 'Doug Joseph', 'Randy Allen', 'Fredrik Kjolstad', 'Miquel Moretó Planas', 'Adrià Armejach']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"NeRF-Based Transparent Object Grasping Enhanced by Shape Priors",
        "abstract":"Transparent object grasping remains a persistent challenge in robotics, largely due to the difficulty of acquiring precise 3D information. Conventional optical 3D sensors struggle to capture transparent objects, and machine learning methods are often hindered by their reliance on high-quality datasets. Leveraging NeRF's capability for continuous spatial opacity modeling, our proposed architecture integrates a NeRF-based approach for reconstructing the 3D information of transparent objects. Despite this, certain portions of the reconstructed 3D information may remain incomplete. To address these deficiencies, we introduce a shape-prior-driven completion mechanism, further refined by a geometric pose estimation method we have developed. This allows us to obtain a complete and reliable 3D information of transparent objects. Utilizing this refined data, we perform scene-level grasp prediction and deploy the results in real-world robotic systems. Experimental validation demonstrates the efficacy of our architecture, showcasing its capability to reliably capture 3D information of various transparent objects in cluttered scenes, and correspondingly, achieve high-quality, stables, and executable grasp predictions.\n        △ Less",
        "authors":"['Yi Han', 'Zixin Lin', 'Dongjie Li', 'Lvping Chen', 'Yongliang Shi', 'Gan Ma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence",
        "abstract":"Millimeter-wave radar provides a privacy-preserving solution for human motion analysis, yet its sparse point clouds pose significant challenges for semantic understanding. We present Radar-LLM, the first framework that leverages large language models (LLMs) for human motion understanding using millimeter-wave radar as the sensing modality. Our approach introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture that incorporates deformable body templates and masked trajectory modeling to encode spatiotemporal point clouds into compact semantic tokens, and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To address data scarcity, we introduce a physics-aware synthesis pipeline that generates realistic radar-text pairs from motion-text datasets. Extensive experiments demonstrate that Radar-LLM achieves state-of-the-art performance across both synthetic and real-world benchmarks, enabling accurate translation of millimeter-wave signals to natural language descriptions. This breakthrough facilitates comprehensive motion understanding in privacy-sensitive applications like healthcare and smart homes. We will release the full implementation to support further research on https:\/\/inowlzy.github.io\/RadarLLM\/.\n        △ Less",
        "authors":"['Zengyuan Lai', 'Jiarui Yang', 'Songpengcheng Xia', 'Lizhou Lin', 'Lan Sun', 'Renwen Wang', 'Jianran Liu', 'Qi Wu', 'Ling Pei']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accelerating Differentially Private Federated Learning via Adaptive Extrapolation",
        "abstract":"The federated learning (FL) framework enables multiple clients to collaboratively train machine learning models without sharing their raw data, but it remains vulnerable to privacy attacks. One promising approach is to incorporate differential privacy (DP)-a formal notion of privacy-into the FL framework. DP-FedAvg is one of the most popular algorithms for DP-FL, but it is known to suffer from the slow convergence in the presence of heterogeneity among clients' data. Most of the existing methods to accelerate DP-FL require 1) additional hyperparameters or 2) additional computational cost for clients, which is not desirable since 1) hyperparameter tuning is computationally expensive and data-dependent choice of hyperparameters raises the risk of privacy leakage, and 2) clients are often resource-constrained. To address this issue, we propose DP-FedEXP, which adaptively selects the global step size based on the diversity of the local updates without requiring any additional hyperparameters or client computational cost. We show that DP-FedEXP provably accelerates the convergence of DP-FedAvg and it empirically outperforms existing methods tailored for DP-FL.\n        △ Less",
        "authors":"['Shokichi Takakura', 'Seng Pei Liew', 'Satoshi Hasegawa']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GlyTwin: Digital Twin for Glucose Control in Type 1 Diabetes Through Optimal Behavioral Modifications Using Patient-Centric Counterfactuals",
        "abstract":"Frequent and long-term exposure to hyperglycemia (i.e., high blood glucose) increases the risk of chronic complications such as neuropathy, nephropathy, and cardiovascular disease. Current technologies like continuous subcutaneous insulin infusion (CSII) and continuous glucose monitoring (CGM) primarily model specific aspects of glycemic control-like hypoglycemia prediction or insulin delivery. Similarly, most digital twin approaches in diabetes management simulate only physiological processes. These systems lack the ability to offer alternative treatment scenarios that support proactive behavioral interventions. To address this, we propose GlyTwin, a novel digital twin framework that uses counterfactual explanations to simulate optimal treatments for glucose regulation. Our approach helps patients and caregivers modify behaviors like carbohydrate intake and insulin dosing to avoid abnormal glucose events. GlyTwin generates behavioral treatment suggestions that proactively prevent hyperglycemia by recommending small adjustments to daily choices, reducing both frequency and duration of these events. Additionally, it incorporates stakeholder preferences into the intervention design, making recommendations patient-centric and tailored. We evaluate GlyTwin on AZT1D, a newly constructed dataset with longitudinal data from 21 type 1 diabetes (T1D) patients on automated insulin delivery systems over 26 days. Results show GlyTwin outperforms state-of-the-art counterfactual methods, generating 76.6% valid and 86% effective interventions. These findings demonstrate the promise of counterfactual-driven digital twins in delivering personalized healthcare.\n        △ Less",
        "authors":"['Asiful Arefeen', 'Saman Khamesian', 'Maria Adela Grando', 'Bithika Thompson', 'Hassan Ghasemzadeh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
        "abstract":"Speech synthesis technology has brought great convenience, while the widespread usage of realistic deepfake audio has triggered hazards. Malicious adversaries may unauthorizedly collect victims' speeches and clone a similar voice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the existing defense methods cannot effectively prevent deepfake exploitation and are vulnerable to robust training techniques. Therefore, a more effective and robust data protection method is urgently needed. In response, we propose a defensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users' audio before uploading by embedding imperceptible perturbations on original speeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a robust and universal proactive protection technique, \\textbf{S}peech \\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a surrogate model to generate universally applicable perturbation for generative synthetic models. Moreover, we optimize the human perception of embedded perturbation in terms of time and frequency domains. To evaluate our method comprehensively, we conduct extensive experiments across advanced models and datasets, both subjectively and objectively. Our experimental results demonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection effectiveness and transferability and is highly robust against advanced adaptive adversaries. Moreover, SafeSpeech has real-time capability in real-world tests. The source code is available at \\href{https:\/\/github.com\/wxzyd123\/SafeSpeech}{https:\/\/github.com\/wxzyd123\/SafeSpeech}.\n        △ Less",
        "authors":"['Zhisheng Zhang', 'Derui Wang', 'Qianyi Yang', 'Pengyang Huang', 'Junhan Pu', 'Yuxin Cao', 'Kai Ye', 'Jie Hao', 'Yixian Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Score Matching Diffusion Based Feedback Control and Planning of Nonlinear Systems",
        "abstract":"We propose a novel control-theoretic framework that leverages principles from generative modeling -- specifically, Denoising Diffusion Probabilistic Models (DDPMs) -- to stabilize control-affine systems with nonholonomic constraints. Unlike traditional stochastic approaches, which rely on noise-driven dynamics in both forward and reverse processes, our method crucially eliminates the need for noise in the reverse phase, making it particularly relevant for control applications. We introduce two formulations: one where noise perturbs all state dimensions during the forward phase while the control system enforces time reversal deterministically, and another where noise is restricted to the control channels, embedding system constraints directly into the forward process.\n  For controllable nonlinear drift-free systems, we prove that deterministic feedback laws can exactly reverse the forward process, ensuring that the system's probability density evolves correctly without requiring artificial diffusion in the reverse phase. Furthermore, for linear time-invariant systems, we establish a time-reversal result under the second formulation. By eliminating noise in the backward process, our approach provides a more practical alternative to machine learning-based denoising methods, which are unsuitable for control applications due to the presence of stochasticity. We validate our results through numerical simulations on benchmark systems, including a unicycle model in a domain with obstacles, a driftless five-dimensional system, and a four-dimensional linear system, demonstrating the potential for applying diffusion-inspired techniques in linear, nonlinear, and settings with state space constraints.\n        △ Less",
        "authors":"['Karthik Elamvazhuthi', 'Darshan Gadginmath', 'Fabio Pasqualetti']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand",
        "abstract":"In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering quantities, inventory levels, covariates, and censored sales levels, to estimate the optimal pricing and inventory control policy that maximizes long-term profit. While the underlying dynamic without censoring can be modeled by Markov decision process (MDP), the primary obstacle arises from the observed process where demand censoring is present, resulting in missing profit information, the failure of the Markov property, and a non-stationary optimal policy. To overcome these challenges, we first approximate the optimal policy by solving a high-order MDP characterized by the number of consecutive censoring instances, which ultimately boils down to solving a specialized Bellman equation tailored for this problem. Inspired by offline reinforcement learning and survival analysis, we propose two novel data-driven algorithms to solving these Bellman equations and, thus, estimate the optimal policy. Furthermore, we establish finite sample regret bounds to validate the effectiveness of these algorithms. Finally, we conduct numerical experiments to demonstrate the efficacy of our algorithms in estimating the optimal policy. To the best of our knowledge, this is the first data-driven approach to learning optimal pricing and inventory control policies in a sequential decision-making environment characterized by censored and dependent demand. The implementations of the proposed algorithms are available at https:\/\/github.com\/gundemkorel\/Inventory_Pricing_Control\n        △ Less",
        "authors":"['Korel Gundem', 'Zhengling Qi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FATE: A Prompt-Tuning-Based Semi-Supervised Learning Framework for Extremely Limited Labeled Data",
        "abstract":"Semi-supervised learning (SSL) has achieved significant progress by leveraging both labeled data and unlabeled data. Existing SSL methods overlook a common real-world scenario when labeled data is extremely scarce, potentially as limited as a single labeled sample in the dataset. General SSL approaches struggle to train effectively from scratch under such constraints, while methods utilizing pre-trained models often fail to find an optimal balance between leveraging limited labeled data and abundant unlabeled data. To address this challenge, we propose Firstly Adapt, Then catEgorize (FATE), a novel SSL framework tailored for scenarios with extremely limited labeled data. At its core, the two-stage prompt tuning paradigm FATE exploits unlabeled data to compensate for scarce supervision signals, then transfers to downstream tasks. Concretely, FATE first adapts a pre-trained model to the feature distribution of downstream data using volumes of unlabeled samples in an unsupervised manner. It then applies an SSL method specifically designed for pre-trained models to complete the final classification task. FATE is designed to be compatible with both vision and vision-language pre-trained models. Extensive experiments demonstrate that FATE effectively mitigates challenges arising from the scarcity of labeled samples in SSL, achieving an average performance improvement of 33.74% across seven benchmarks compared to state-of-the-art SSL methods. Code is available at https:\/\/anonymous.4open.science\/r\/Semi-supervised-learning-BA72.\n        △ Less",
        "authors":"['Hezhao Liu', 'Yang Lu', 'Mengke Li', 'Yiqun Zhang', 'Shreyank N Gowda', 'Chen Gong', 'Hanzi Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Multi-Task Modeling through Automated Fusion of Trained Models",
        "abstract":"Although multi-task learning is widely applied in intelligent services, traditional multi-task modeling methods often require customized designs based on specific task combinations, resulting in a cumbersome modeling process. Inspired by the rapid development and excellent performance of single-task models, this paper proposes an efficient multi-task modeling method that can automatically fuse trained single-task models with different structures and tasks to form a multi-task model. As a general framework, this method allows modelers to simply prepare trained models for the required tasks, simplifying the modeling process while fully utilizing the knowledge contained in the trained models. This eliminates the need for excessive focus on task relationships and model structure design. To achieve this goal, we consider the structural differences among various trained models and employ model decomposition techniques to hierarchically decompose them into multiple operable model components. Furthermore, we have designed an Adaptive Knowledge Fusion (AKF) module based on Transformer, which adaptively integrates intra-task and inter-task knowledge based on model components. Through the proposed method, we achieve efficient and automated construction of multi-task models, and its effectiveness is verified through extensive experiments on three datasets.\n        △ Less",
        "authors":"['Jingxuan Zhou', 'Weidong Bao', 'Ji Wang', 'Zhengyi Zhong', 'Dayu Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Virtual domain extension for imposing boundary conditions in flow simulation using pre-trained local neural operator",
        "abstract":"This paper builds up a virtual domain extension (VDE) framework for imposing boundary conditions (BCs) in flow simulation using pre-trained local neural operator (LNO). It creates extended virtual domains to the input function to compensate for the corrosion nature of computational domains during LNO inference, thus turns the implementation of BC into the determination of field values on the extended domain. Several strategies to calculate the field values are proposed and validated in solving numerical examples, including padding operation, direct imposition, pressure symmetry, and optimization by backpropagation, and compared with boundary imposition in traditional solvers. It is found that the large time interval of LNO induces a relatively wide near-boundary domain to be processed, thus imposing BC on only a few nodes near the boundary following the immersed boundary conception in traditional solvers can hardly achieve high accuracy. With appropriate values assigned on the extended virtual domains, VDE can accurately impose BCs and lead to reasonable flow field predictions. This work provides a guidance for imposing BCs reliably in LNO prediction, which could facilitate the reuse of pre-trained LNO in more applications.\n        △ Less",
        "authors":"['Ximeng Ye', 'Hongyu Li', 'Zhen-Guo Yan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BO-SA-PINNs: Self-adaptive physics-informed neural networks based on Bayesian optimization for automatically designing PDE solvers",
        "abstract":"Physics-informed neural networks (PINNs) is becoming a popular alternative method for solving partial differential equations (PDEs). However, they require dedicated manual modifications to the hyperparameters of the network, the sampling methods and loss function weights for different PDEs, which reduces the efficiency of the solvers. In this paper, we pro- pose a general multi-stage framework, i.e. BO-SA-PINNs to alleviate this issue. In the first stage, Bayesian optimization (BO) is used to select hyperparameters for the training process, and based on the results of the pre-training, the network architecture, learning rate, sampling points distribution and loss function weights suitable for the PDEs are automatically determined. The proposed hyperparameters search space based on experimental results can enhance the efficiency of BO in identifying optimal hyperparameters. After selecting the appropriate hyperparameters, we incorporate a global self-adaptive (SA) mechanism the second stage. Using the pre-trained model and loss information in the second-stage training, the exponential moving average (EMA) method is employed to optimize the loss function weights, and residual-based adaptive refinement with distribution (RAR-D) is used to optimize the sampling points distribution. In the third stage, L-BFGS is used for stable training. In addition, we introduce a new activation function that enables BO-SA-PINNs to achieve higher accuracy. In numerical experiments, we conduct comparative and ablation experiments to verify the performance of the model on Helmholtz, Maxwell, Burgers and high-dimensional Poisson equations. The comparative experiment results show that our model can achieve higher accuracy and fewer iterations in test cases, and the ablation experiments demonstrate the positive impact of every improvement.\n        △ Less",
        "authors":"['Rui Zhang', 'Liang Li', 'Stéphane Lanteri', 'Hao Kang', 'Jiaqi Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CUT: Pruning Pre-Trained Multi-Task Models into Compact Models for Edge Devices",
        "abstract":"Multi-task learning has garnered widespread attention in the industry due to its efficient data utilization and strong generalization capabilities, making it particularly suitable for providing high-quality intelligent services to users. Edge devices, as the primary platforms directly serving users, play a crucial role in delivering multi-task services. However, current multi-task models are often large, and user task demands are increasingly diverse. Deploying such models directly on edge devices not only increases the burden on these devices but also leads to task redundancy. To address this issue, this paper innovatively proposes a pre-trained multi-task model pruning method specifically designed for edge computing. The goal is to utilize existing pre-trained multi-task models to construct a compact multi-task model that meets the needs of edge devices. The specific implementation steps are as follows: First, decompose the tasks within the pre-trained multi-task model and select tasks based on actual user needs. Next, while retaining the knowledge of the original pre-trained model, evaluate parameter importance and use a parameter fusion method to effectively integrate shared parameters among tasks. Finally, obtain a compact multi-task model suitable for edge devices. To validate the effectiveness of the proposed method, we conducted experiments on three public image datasets. The experimental results fully demonstrate the superiority and efficiency of this method, providing a new solution for multi-task learning on edge devices.\n        △ Less",
        "authors":"['Jingxuan Zhou', 'Weidong Bao', 'Ji Wang', 'Zhengyi Zhong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-task Federated Learning with Encoder-Decoder Structure: Enabling Collaborative Learning Across Different Tasks",
        "abstract":"Federated learning has been extensively studied and applied due to its ability to ensure data security in distributed environments while building better models. However, clients participating in federated learning still face limitations, as clients with different structures or tasks cannot participate in learning together. In view of this, constructing a federated learning framework that allows collaboration between clients with different model structures and performing different tasks, enabling them to share valuable knowledge to enhance model efficiency, holds significant practical implications for the widespread application of federated learning. To achieve this goal, we propose a multi-task federated learning with encoder-decoder structure (M-Fed). Specifically, given the widespread adoption of the encoder-decoder architecture in current models, we leverage this structure to share intra-task knowledge through traditional federated learning methods and extract general knowledge from the encoder to achieve cross-task knowledge sharing. The training process is similar to traditional federated learning, and we incorporate local decoder and global decoder information into the loss function. The local decoder iteratively updates and gradually approaches the global decoder until sufficient cross-task knowledge sharing is achieved. Our method is lightweight and modular, demonstrating innovation compared to previous research. It enables clients performing different tasks to share general knowledge while maintaining the efficiency of traditional federated learning systems. We conducted experiments on two widely used benchmark datasets to verify the feasibility of M-Fed and compared it with traditional methods. The experimental results demonstrate the effectiveness of M-Fed in multi-task federated learning.\n        △ Less",
        "authors":"['Jingxuan Zhou', 'Weidong Bao', 'Ji Wang', 'Dayu Zhang', 'Xiongtao Zhang', 'Yaohong Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Advancing RFI-Detection in Radio Astronomy with Liquid State Machines",
        "abstract":"Radio Frequency Interference (RFI) from anthropogenic radio sources poses significant challenges to current and future radio telescopes. Contemporary approaches to detecting RFI treat the task as a semantic segmentation problem on radio telescope spectrograms. Typically, complex heuristic algorithms handle this task of `flagging' in combination with manual labeling (in the most difficult cases). While recent machine-learning approaches have demonstrated high accuracy, they often fail to meet the stringent operational requirements of modern radio observatories. Owing to their inherently time-varying nature, spiking neural networks (SNNs) are a promising alternative method to RFI-detection by utilizing the time-varying nature of the spectrographic source data. In this work, we apply Liquid State Machines (LSMs), a class of spiking neural networks, to RFI-detection. We employ second-order Leaky Integrate-and-Fire (LiF) neurons, marking the first use of this architecture and neuron type for RFI-detection. We test three encoding methods and three increasingly complex readout layers, including a transformer decoder head, providing a hybrid of SNN and ANN techniques. Our methods extend LSMs beyond conventional classification tasks to fine-grained spatio-temporal segmentation. We train LSMs on simulated data derived from the Hyrogen Epoch of Reionization Array (HERA), a known benchmark for RFI-detection. Our model achieves a per-pixel accuracy of 98% and an F1-score of 0.743, demonstrating competitive performance on this highly challenging task. This work expands the sophistication of SNN techniques and architectures applied to RFI-detection, and highlights the effectiveness of LSMs in handling fine-grained, complex, spatio-temporal signal-processing tasks.\n        △ Less",
        "authors":"['Nicholas J Pritchard', 'Andreas Wicenec', 'Mohammed Bennamoun', 'Richard Dodson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Tale of Two Learning Algorithms: Multiple Stream Random Walk and Asynchronous Gossip",
        "abstract":"Although gossip and random walk-based learning algorithms are widely known for decentralized learning, there has been limited theoretical and experimental analysis to understand their relative performance for different graph topologies and data heterogeneity. We first design and analyze a random walk-based learning algorithm with multiple streams (walks), which we name asynchronous \"Multi-Walk (MW)\". We provide a convergence analysis for MW w.r.t iteration (computation), wall-clock time, and communication. We also present a convergence analysis for \"Asynchronous Gossip\", noting the lack of a comprehensive analysis of its convergence, along with the computation and communication overhead, in the literature. Our results show that MW has better convergence in terms of iterations as compared to Asynchronous Gossip in graphs with large diameters (e.g., cycles), while its relative performance, as compared to Asynchronous Gossip, depends on the number of walks and the data heterogeneity in graphs with small diameters (e.g., complete graphs). In wall-clock time analysis, we observe a linear speed-up with the number of walks and nodes in MW and Asynchronous Gossip, respectively. Finally, we show that MW outperforms Asynchronous Gossip in communication overhead, except in small-diameter topologies with extreme data heterogeneity. These results highlight the effectiveness of each algorithm in different graph topologies and data heterogeneity. Our codes are available for reproducibility.\n        △ Less",
        "authors":"['Peyman Gholami', 'Hulya Seferoglu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reasoning without Regret",
        "abstract":"Chain-of-thought reasoning enables large language models to solve multi-step tasks by framing problem solving as sequential decision problems. Outcome-based rewards, which provide feedback only on final answers, show impressive success, but face challenges with credit assignment and slow convergence. In contrast, procedure-based rewards offer efficient step-level feedback, but typically require costly human supervision. We introduce \\emph{Backwards Adaptive Reward Shaping} (BARS), a no-regret framework that converts sparse outcomes-based rewards into effective procedure-based signals. BARS uses sparse rewards generated from terminal-state priors and cover trees to scale rewards while preventing exploitation. With Bellman contraction and $(Δ, ε)$-gap rewards, our backward Euler solver achieves $ε$-accuracy in $O\\left((R_{\\max}\/Δ)\\log(1\/ε)\\right)$ iterations with $O(\\log T)$ dynamic regret over $T$ rounds. Our analysis, based on generic chaining, continuous scaling limits, and non-linear Feynman-Kac bounds, connects recent outcome-based methods' empirical successes with the benefits of intermediate supervision. Combined, this provides the first rigorous no-regret algorithm for outcome reward shaping, providing a theoretical foundation for the empirical success of DeepSeek's R1.\n        △ Less",
        "authors":"['Tarun Chitra']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Investigation of Large Language Models and Their Vulnerabilities in Spam Detection",
        "abstract":"Spam messages continue to present significant challenges to digital users, cluttering inboxes and posing security risks. Traditional spam detection methods, including rules-based, collaborative, and machine learning approaches, struggle to keep up with the rapidly evolving tactics employed by spammers. This project studies new spam detection systems that leverage Large Language Models (LLMs) fine-tuned with spam datasets. More importantly, we want to understand how LLM-based spam detection systems perform under adversarial attacks that purposefully modify spam emails and data poisoning attacks that exploit the differences between the training data and the massages in detection, to which traditional machine learning models are shown to be vulnerable. This experimentation employs two LLM models of GPT2 and BERT and three spam datasets of Enron, LingSpam, and SMSspamCollection for extensive training and testing tasks. The results show that, while they can function as effective spam filters, the LLM models are susceptible to the adversarial and data poisoning attacks. This research provides very useful insights for future applications of LLM models for information security.\n        △ Less",
        "authors":"['Qiyao Tang', 'Xiangyang Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Understanding and Optimizing Multi-Stage AI Inference Pipelines",
        "abstract":"The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads.\n        △ Less",
        "authors":"['Abhimanyu Rajeshkumar Bambhaniya', 'Hanjiang Wu', 'Suvinay Subramanian', 'Sudarshan Srinivasan', 'Souvik Kundu', 'Amir Yazdanbakhsh', 'Midhilesh Elavazhagan', 'Madhu Kumar', 'Tushar Krishna']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the representation of stack operators by mathematical morphology",
        "abstract":"This paper introduces the class of grey-scale image stack operators as those that (a) map binary-images into binary-images and (b) commute in average with cross-sectioning. We show that stack operators are 1-Lipchitz extensions of set operators which can be represented by applying a characteristic set operator to the cross-sections of the image and summing. In particular, they are a generalisation of stack filters, for which the characteristic set operators are increasing. Our main result is that stack operators inherit lattice properties of the characteristic set operators. We focus on the case of translation-invariant and locally defined stack operators and show the main result by deducing the characteristic function, kernel, and basis representation of stack operators. The results of this paper have implications on the design of image operators, since imply that to solve some grey-scale image processing problems it is enough to design an operator for performing the desired transformation on binary images, and then considering its extension given by a stack operator. We leave many topics for future research regarding the machine learning of stack operators and the characterisation of the image processing problems that can be solved by them.\n        △ Less",
        "authors":"['Diego Marcondes']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems",
        "abstract":"Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.\n        △ Less",
        "authors":"['Zaid Khan', 'Elias Stengel-Eskin', 'Archiki Prasad', 'Jaemin Cho', 'Mohit Bansal']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamical symmetries in the fluctuation-driven regime: an application of Noether's theorem to noisy dynamical systems",
        "abstract":"Noether's theorem provides a powerful link between continuous symmetries and conserved quantities for systems governed by some variational principle. Perhaps unfortunately, most dynamical systems of interest in neuroscience and artificial intelligence cannot be described by any such principle. On the other hand, nonequilibrium physics provides a variational principle that describes how fairly generic noisy dynamical systems are most likely to transition between two states; in this work, we exploit this principle to apply Noether's theorem, and hence learn about how the continuous symmetries of dynamical systems constrain their most likely trajectories. We identify analogues of the conservation of energy, momentum, and angular momentum, and briefly discuss examples of each in the context of models of decision-making, recurrent neural networks, and diffusion generative models.\n        △ Less",
        "authors":"['John J. Vastola']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Classifier Evaluation: A Fairer Benchmarking Strategy Based on Ability and Robustness",
        "abstract":"Benchmarking is a fundamental practice in machine learning (ML) for comparing the performance of classification algorithms. However, traditional evaluation methods often overlook a critical aspect: the joint consideration of dataset complexity and an algorithm's ability to generalize. Without this dual perspective, assessments may favor models that perform well on easy instances while failing to capture their true robustness. To address this limitation, this study introduces a novel evaluation methodology that combines Item Response Theory (IRT) with the Glicko-2 rating system, originally developed to measure player strength in competitive games. IRT assesses classifier ability based on performance over difficult instances, while Glicko-2 updates performance metrics - such as rating, deviation, and volatility - via simulated tournaments between classifiers. This combined approach provides a fairer and more nuanced measure of algorithm capability. A case study using the OpenML-CC18 benchmark showed that only 15% of the datasets are truly challenging and that a reduced subset with 50% of the original datasets offers comparable evaluation power. Among the algorithms tested, Random Forest achieved the highest ability score. The results highlight the importance of improving benchmark design by focusing on dataset quality and adopting evaluation strategies that reflect both difficulty and classifier proficiency.\n        △ Less",
        "authors":"['Lucas Cardoso', 'Vitor Santos', 'José Ribeiro', 'Regiane Kawasaki', 'Ricardo Prudêncio', 'Ronnie Alves']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Integrating Large Language Models for Automated Structural Analysis",
        "abstract":"Automated analysis for engineering structures offers considerable potential for boosting efficiency by minimizing repetitive tasks. Although AI-driven methods are increasingly common, no systematic framework yet leverages Large Language Models (LLMs) for automatic structural analysis. To address this gap, we propose a novel framework that integrates LLMs with structural analysis software. LLMs serve as the core engine: they parse structural descriptions from text and translate them into executable Python scripts. Moreover, the framework integrates the generative capabilities of LLMs with code-based finite element (FE) tools like OpenSeesPy. It employs domain-specific prompt design and in-context learning strategies to enhance the LLM's problem-solving capabilities and generative stability, enabling fully automated structural analysis from descriptive text to model outputs. In our experiments, we introduce a well-curated small-scale benchmark dataset of 20 structural analysis word problems (SAWPs) with ground-truth solutions and evaluate the performance of different LLMs within our framework in solving these SAWPs. The role of system instructions, crafted by structural engineers, is also investigated to understand their impact on LLM-driven structural analysis. Additionally, the generative stability of our framework is examined. Through multiple validation experiments on the benchmark, our results demonstrate that the proposed framework can substantially increase the level of automation in solving SAWPs compared to traditional methods. Quantitatively, the framework, built on GPT-4o, achieved 100% accuracy, surpassing GPT-4 (85%), Gemini 1.5 Pro (80%), and Llama-3.3 (30%) on the test examples. Furthermore, integrating domain-specific instructions enhanced performance by 30% on problems with asymmetrical structural configurations.\n        △ Less",
        "authors":"['Haoran Liang', 'Mohammad Talebi Kalaleh', 'Qipei Mei']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Stochastic generative methods for stable and accurate closure modeling of chaotic dynamical systems",
        "abstract":"Traditional deterministic subgrid-scale (SGS) models are often dissipative and unstable, especially in regions of chaotic and turbulent flow. Ongoing work in climate science and ocean modeling motivates the use of stochastic SGS models for chaotic dynamics. Further, developing stochastic generative models of underlying dynamics is a rapidly expanding field. In this work, we aim to incorporate stochastic integration toward closure modeling for chaotic dynamical systems. Further, we want to explore the potential stabilizing effect that stochastic models could have on linearized chaotic systems. We propose parametric and generative approaches for closure modeling using stochastic differential equations (SDEs). We derive and implement a quadratic diffusion model based on the fluctuations, demonstrating increased accuracy from bridging theoretical models with generative approaches. Results are demonstrated on the Lorenz-63 dynamical system.\n        △ Less",
        "authors":"['Emily Williams', 'David Darmofal']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Automatic Detection of Intro and Credits in Video using CLIP and Multihead Attention",
        "abstract":"Detecting transitions between intro\/credits and main content in videos is a crucial task for content segmentation, indexing, and recommendation systems. Manual annotation of such transitions is labor-intensive and error-prone, while heuristic-based methods often fail to generalize across diverse video styles. In this work, we introduce a deep learning-based approach that formulates the problem as a sequence-to-sequence classification task, where each second of a video is labeled as either \"intro\" or \"film.\" Our method extracts frames at a fixed rate of 1 FPS, encodes them using CLIP (Contrastive Language-Image Pretraining), and processes the resulting feature representations with a multihead attention model incorporating learned positional encoding. The system achieves an F1-score of 91.0%, Precision of 89.0%, and Recall of 97.0% on the test set, and is optimized for real-time inference, achieving 11.5 FPS on CPU and 107 FPS on high-end GPUs. This approach has practical applications in automated content indexing, highlight detection, and video summarization. Future work will explore multimodal learning, incorporating audio features and subtitles to further enhance detection accuracy.\n        △ Less",
        "authors":"['Vasilii Korolkov', 'Andrey Yanchenko']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025",
        "abstract":"Peer review at AI conferences is stressed by rapidly rising submission volumes, leading to deteriorating review quality and increased author dissatisfaction. To address these issues, we developed Review Feedback Agent, a system leveraging multiple large language models (LLMs) to improve review clarity and actionability by providing automated feedback on vague comments, content misunderstandings, and unprofessional remarks to reviewers. Implemented at ICLR 2025 as a large randomized control study, our system provided optional feedback to more than 20,000 randomly selected reviews. To ensure high-quality feedback for reviewers at this scale, we also developed a suite of automated reliability tests powered by LLMs that acted as guardrails to ensure feedback quality, with feedback only being sent to reviewers if it passed all the tests. The results show that 27% of reviewers who received feedback updated their reviews, and over 12,000 feedback suggestions from the agent were incorporated by those reviewers. This suggests that many reviewers found the AI-generated feedback sufficiently helpful to merit updating their reviews. Incorporating AI feedback led to significantly longer reviews (an average increase of 80 words among those who updated after receiving feedback) and more informative reviews, as evaluated by blinded researchers. Moreover, reviewers who were selected to receive AI feedback were also more engaged during paper rebuttals, as seen in longer author-reviewer discussions. This work demonstrates that carefully designed LLM-generated review feedback can enhance peer review quality by making reviews more specific and actionable while increasing engagement between reviewers and authors. The Review Feedback Agent is publicly available at https:\/\/github.com\/zou-group\/review_feedback_agent.\n        △ Less",
        "authors":"['Nitya Thakkar', 'Mert Yuksekgonul', 'Jake Silberg', 'Animesh Garg', 'Nanyun Peng', 'Fei Sha', 'Rose Yu', 'Carl Vondrick', 'James Zou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE) of 2D Black Box Classifier Functions",
        "abstract":"Accurately estimating decision boundaries in black box systems is critical when ensuring safety, quality, and feasibility in real-world applications. However, existing methods iteratively refine boundary estimates by sampling in regions of uncertainty, without providing guarantees on the closeness to the decision boundary and also result in unnecessary exploration that is especially disadvantageous when evaluations are costly. This paper presents the Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE), a sample efficient and function-agnostic algorithm that leverages the intermediate value theorem to estimate the location of the decision boundary of a black box binary classifier within a user-specified epsilon-neighborhood. Evaluations are conducted on three nonlinear test functions and a case study of an electric grid stability problem with uncertain renewable power injection. The EDGE algorithm demonstrates superior sample efficiency and better boundary approximation than adaptive sampling techniques and grid-based searches.\n        △ Less",
        "authors":"['Mithun Goutham', 'Riccardo DalferroNucci', 'Stephanie Stockar', 'Meghna Menon', 'Sneha Nayak', 'Harshad Zade', 'Chetan Patel', 'Mario Santillo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training",
        "abstract":"Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https:\/\/github.com\/ZhentingWang\/DUMP.\n        △ Less",
        "authors":"['Zhenting Wang', 'Guofeng Cui', 'Kun Wan', 'Wentian Zhao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Preconditioned Gradient Descent for Over-Parameterized Nonconvex Matrix Factorization",
        "abstract":"In practical instances of nonconvex matrix factorization, the rank of the true solution $r^{\\star}$ is often unknown, so the rank $r$ of the model can be overspecified as $r>r^{\\star}$. This over-parameterized regime of matrix factorization significantly slows down the convergence of local search algorithms, from a linear rate with $r=r^{\\star}$ to a sublinear rate when $r>r^{\\star}$. We propose an inexpensive preconditioner for the matrix sensing variant of nonconvex matrix factorization that restores the convergence rate of gradient descent back to linear, even in the over-parameterized case, while also making it agnostic to possible ill-conditioning in the ground truth. Classical gradient descent in a neighborhood of the solution slows down due to the need for the model matrix factor to become singular. Our key result is that this singularity can be corrected by $\\ell_{2}$ regularization with a specific range of values for the damping parameter. In fact, a good damping parameter can be inexpensively estimated from the current iterate. The resulting algorithm, which we call preconditioned gradient descent or PrecGD, is stable under noise, and converges linearly to an information theoretically optimal error bound. Our numerical experiments find that PrecGD works equally well in restoring the linear convergence of other variants of nonconvex matrix factorization in the over-parameterized regime.\n        △ Less",
        "authors":"['Gavin Zhang', 'Salar Fattahi', 'Richard Y. Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InfoMAE: Pair-Efficient Cross-Modal Alignment for Multimodal Time-Series Sensing Signals",
        "abstract":"Standard multimodal self-supervised learning (SSL) algorithms regard cross-modal synchronization as implicit supervisory labels during pretraining, thus posing high requirements on the scale and quality of multimodal samples. These constraints significantly limit the performance of sensing intelligence in IoT applications, as the heterogeneity and the non-interpretability of time-series signals result in abundant unimodal data but scarce high-quality multimodal pairs. This paper proposes InfoMAE, a cross-modal alignment framework that tackles the challenge of multimodal pair efficiency under the SSL setting by facilitating efficient cross-modal alignment of pretrained unimodal representations. InfoMAE achieves \\textit{efficient cross-modal alignment} with \\textit{limited data pairs} through a novel information theory-inspired formulation that simultaneously addresses distribution-level and instance-level alignment. Extensive experiments on two real-world IoT applications are performed to evaluate InfoMAE's pairing efficiency to bridge pretrained unimodal models into a cohesive joint multimodal model. InfoMAE enhances downstream multimodal tasks by over 60% with significantly improved multimodal pairing efficiency. It also improves unimodal task accuracy by an average of 22%.\n        △ Less",
        "authors":"['Tomoyoshi Kimura', 'Xinlin Li', 'Osama Hanna', 'Yatong Chen', 'Yizhuo Chen', 'Denizhan Kara', 'Tianshi Wang', 'Jinyang Li', 'Xiaomin Ouyang', 'Shengzhong Liu', 'Mani Srivastava', 'Suhas Diggavi', 'Tarek Abdelzaher']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transformer-Based Representation Learning for Robust Gene Expression Modeling and Cancer Prognosis",
        "abstract":"Transformer-based models have achieved remarkable success in natural language and vision tasks, but their application to gene expression analysis remains limited due to data sparsity, high dimensionality, and missing values. We present GexBERT, a transformer-based autoencoder framework for robust representation learning of gene expression data. GexBERT learns context-aware gene embeddings by pretraining on large-scale transcriptomic profiles with a masking and restoration objective that captures co-expression relationships among thousands of genes. We evaluate GexBERT across three critical tasks in cancer research: pan-cancer classification, cancer-specific survival prediction, and missing value imputation. GexBERT achieves state-of-the-art classification accuracy from limited gene subsets, improves survival prediction by restoring expression of prognostic anchor genes, and outperforms conventional imputation methods under high missingness. Furthermore, its attention-based interpretability reveals biologically meaningful gene patterns across cancer types. These findings demonstrate the utility of GexBERT as a scalable and effective tool for gene expression modeling, with translational potential in settings where gene coverage is limited or incomplete.\n        △ Less",
        "authors":"['Shuai Jiang', 'Saeed Hassanpour']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?",
        "abstract":"Existing evaluation of large language model (LLM) agents on scientific discovery lacks objective baselines and metrics to assess the viability of their proposed methods. To address this issue, we introduce MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. Our benchmark highlights open research problems that demand novel methodologies, in contrast to recent benchmarks such as OpenAI's MLE-Bench (Chan et al., 2024) and METR's RE-Bench (Wijk et al., 2024), which focus on well-established research tasks that are largely solvable through sufficient engineering effort. Unlike prior work, e.g., AI Scientist (Lu et al., 2024b), which evaluates the end-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the key steps of proposing and implementing novel research methods and evaluates them with newly proposed rigorous protocol and objective metrics. Our curated suite of 7 competition tasks reveals significant challenges for LLM agents. Even the best-performing tested agent (gemini-exp-1206 under MLAB (Huang et al., 2024a)) closes only 9.3% of the gap between baseline and top human participant scores. Furthermore, our analysis reveals a misalignment between the LLM-judged innovation and their actual performance on cutting-edge ML research problems. MLRC-Bench is a dynamic benchmark, which is designed to continually grow with new ML competitions to encourage rigorous and objective evaluations of AI's research capabilities.\n        △ Less",
        "authors":"['Yunxiang Zhang', 'Muhammad Khalifa', 'Shitanshu Bhushan', 'Grant D Murphy', 'Lajanugen Logeswaran', 'Jaekyeom Kim', 'Moontae Lee', 'Honglak Lee', 'Lu Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow",
        "abstract":"Recent prompt-based image editing models have demonstrated impressive prompt-following capability at structural editing tasks. However, existing models still fail to perform local edits, follow detailed editing prompts, or maintain global image quality beyond a single editing step. To address these challenges, we introduce SPICE, a training-free workflow that accepts arbitrary resolutions and aspect ratios, accurately follows user requirements, and improves image quality consistently during more than 100 editing steps. By synergizing the strengths of a base diffusion model and a Canny edge ControlNet model, SPICE robustly handles free-form editing instructions from the user. SPICE outperforms state-of-the-art baselines on a challenging realistic image-editing dataset consisting of semantic editing (object addition, removal, replacement, and background change), stylistic editing (texture changes), and structural editing (action change) tasks. Not only does SPICE achieve the highest quantitative performance according to standard evaluation metrics, but it is also consistently preferred by users over existing image-editing methods. We release the workflow implementation for popular diffusion model Web UIs to support further research and artistic exploration.\n        △ Less",
        "authors":"['Kenan Tang', 'Yanhong Li', 'Yao Qin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Computer-Aided Layout Generation for Building Design: A Review",
        "abstract":"Generating realistic building layouts for automatic building design has been studied in both the computer vision and architecture domains. Traditional approaches from the architecture domain, which are based on optimization techniques or heuristic design guidelines, can synthesize desirable layouts, but usually require post-processing and involve human interaction in the design pipeline, making them costly and timeconsuming. The advent of deep generative models has significantly improved the fidelity and diversity of the generated architecture layouts, reducing the workload by designers and making the process much more efficient. In this paper, we conduct a comprehensive review of three major research topics of architecture layout design and generation: floorplan layout generation, scene layout synthesis, and generation of some other formats of building layouts. For each topic, we present an overview of the leading paradigms, categorized either by research domains (architecture or machine learning) or by user input conditions or constraints. We then introduce the commonly-adopted benchmark datasets that are used to verify the effectiveness of the methods, as well as the corresponding evaluation metrics. Finally, we identify the well-solved problems and limitations of existing approaches, then propose new perspectives as promising directions for future research in this important research area. A project associated with this survey to maintain the resources is available at awesome-building-layout-generation.\n        △ Less",
        "authors":"['Jiachen Liu', 'Yuan Xue', 'Haomiao Ni', 'Rui Yu', 'Zihan Zhou', 'Sharon X. Huang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety",
        "abstract":"The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https:\/\/github.com\/1akaman\/EmoAgent\n        △ Less",
        "authors":"['Jiahao Qiu', 'Yinghui He', 'Xinzhe Juan', 'Yiming Wang', 'Yuhan Liu', 'Zixin Yao', 'Yue Wu', 'Xun Jiang', 'Ling Yang', 'Mengdi Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GAMA: High-Performance GEMM Acceleration on AMD Versal ML-Optimized AI Engines",
        "abstract":"General matrix-matrix multiplication (GEMM) is a fundamental operation in machine learning (ML) applications. We present the first comprehensive performance acceleration of GEMM workloads on AMD's second-generation AIE-ML (AIE2) architecture, which is specifically optimized for ML applications. Compared to AI-Engine (AIE1), AIE offers increased compute throughput and larger on-chip memory capacity. We propose a novel design that maximizes AIE memory utilization, incorporates custom buffer placement within the AIE2 and staggered kernel placement across the AIE2 array, significantly reducing performance bottlenecks such as memory stalls and routing congestion, resulting in improved performance and efficiency compared to the default compiler provided by AMD. We evaluate the performance benefits of our design at three levels: single AIE, pack of AIEs and the complete AIE array. GAMA achieves state-of-the-art performance, delivering up to 165 TOPS (85% of peak) for int8 precision and 83 TBFLOPS (86% of peak) for bfloat16 precision GEMM workloads. Our solution achieves 8.7%, 9%, 39% and 53.6% higher peak throughput efficiency compared to the state-of-the-art AIE1 frameworks AMA, MAXEVA, ARIES and CHARM, respectively.\n        △ Less",
        "authors":"['Kaustubh Mhatre', 'Endri Taka', 'Aman Arora']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Domain-Adaptive Continued Pre-Training of Small Language Models",
        "abstract":"Continued pre-training of small language models offers a promising path for domain adaptation with limited computational resources. I've investigated this approach within educational domains, evaluating it as a resource-efficient alternative to training models from scratch. Using a 125M parameter model, I demonstrate significant performance improvements through incremental training on 400 million tokens, followed by further training to reach 1 billion tokens. My approach includes comprehensive data preprocessing, memory-optimized training configurations, and benchmark-based evaluation. Results show notable gains in knowledge-intensive tasks (MMLU +8.1%) and contextual understanding (HellaSwag +7.6%), while revealing educational domain specialization trade-offs. I analyze token efficiency, catastrophic forgetting mitigation strategies, and scaling patterns. My findings suggest that thoughtful preprocessing and training methodologies enable meaningful improvements in language model capabilities even with constrained computational resources, opening pathways for domain-specific adaptation of smaller language models.\n        △ Less",
        "authors":"['Salman Faroz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can LLMs Revolutionize the Design of Explainable and Efficient TinyML Models?",
        "abstract":"This paper introduces a novel framework for designing efficient neural network architectures specifically tailored to tiny machine learning (TinyML) platforms. By leveraging large language models (LLMs) for neural architecture search (NAS), a vision transformer (ViT)-based knowledge distillation (KD) strategy, and an explainability module, the approach strikes an optimal balance between accuracy, computational efficiency, and memory usage. The LLM-guided search explores a hierarchical search space, refining candidate architectures through Pareto optimization based on accuracy, multiply-accumulate operations (MACs), and memory metrics. The best-performing architectures are further fine-tuned using logits-based KD with a pre-trained ViT-B\/16 model, which enhances generalization without increasing model size. Evaluated on the CIFAR-100 dataset and deployed on an STM32H7 microcontroller (MCU), the three proposed models, LMaNet-Elite, LMaNet-Core, and QwNet-Core, achieve accuracy scores of 74.50%, 74.20% and 73.00%, respectively. All three models surpass current state-of-the-art (SOTA) models, such as MCUNet-in3\/in4 (69.62% \/ 72.86%) and XiNet (72.27%), while maintaining a low computational cost of less than 100 million MACs and adhering to the stringent 320 KB static random-access memory (SRAM) constraint. These results demonstrate the efficiency and performance of the proposed framework for TinyML platforms, underscoring the potential of combining LLM-driven search, Pareto optimization, KD, and explainability to develop accurate, efficient, and interpretable models. This approach opens new possibilities in NAS, enabling the design of efficient architectures specifically suited for TinyML.\n        △ Less",
        "authors":"['Christophe El Zeinaty', 'Wassim Hamidouche', 'Glenn Herrou', 'Daniel Menard', 'Merouane Debbah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SPOT: Spatio-Temporal Pattern Mining and Optimization for Load Consolidation in Freight Transportation Networks",
        "abstract":"Freight consolidation has significant potential to reduce transportation costs and mitigate congestion and pollution. An effective load consolidation plan relies on carefully chosen consolidation points to ensure alignment with existing transportation management processes, such as driver scheduling, personnel planning, and terminal operations. This complexity represents a significant challenge when searching for optimal consolidation strategies. Traditional optimization-based methods provide exact solutions, but their computational complexity makes them impractical for large-scale instances and they fail to leverage historical data. Machine learning-based approaches address these issues but often ignore operational constraints, leading to infeasible consolidation plans.\n  This work proposes SPOT, an end-to-end approach that integrates the benefits of machine learning (ML) and optimization for load consolidation. The ML component plays a key role in the planning phase by identifying the consolidation points through spatio-temporal clustering and constrained frequent itemset mining, while the optimization selects the most cost-effective feasible consolidation routes for a given operational day. Extensive experiments conducted on industrial load data demonstrate that SPOT significantly reduces travel distance and transportation costs (by about 50% on large terminals) compared to the existing industry-standard load planning strategy and a neighborhood-based heuristic. Moreover, the ML component provides valuable tactical-level insights by identifying frequently recurring consolidation opportunities that guide proactive planning. In addition, SPOT is computationally efficient and can be easily scaled to accommodate large transportation networks.\n        △ Less",
        "authors":"['Sikai Cheng', 'Amira Hijazi', 'Jeren Konak', 'Alan Erera', 'Pascal Van Hentenryck']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adapting to the Unknown: Robust Meta-Learning for Zero-Shot Financial Time Series Forecasting",
        "abstract":"Financial time series forecasting in the zero-shot setting is essential for risk management and investment decision-making, particularly during abrupt market regime shifts or in emerging markets with limited historical data. While Model-Agnostic Meta-Learning (MAML)-based approaches have shown promise in this domain, existing meta task construction strategies often lead to suboptimal performance, especially when dealing with highly turbulent financial time series. To address this challenge, we propose a novel task construction method that leverages learned embeddings for more effective meta-learning in the zero-shot setting. Specifically, we construct two complementary types of meta-tasks based on the learned embeddings: intra-cluster tasks and inter-cluster tasks. To capture diverse fine-grained patterns, we apply stochastic projection matrices to the learned embeddings and use clustering algorithm to form the tasks. Additionally, to improve generalization capabilities, we employ hard task mining strategies and leverage inter-cluster tasks to identify invariant patterns across different time series. Extensive experiments on the real world financial dataset demonstrate that our method significantly outperforms existing approaches, showing better generalization ability in the zero-shot scenario.\n        △ Less",
        "authors":"['Anxian Liu', 'Junying Ma', 'Guang Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ordinary Least Squares as an Attention Mechanism",
        "abstract":"I show that ordinary least squares (OLS) predictions can be rewritten as the output of a restricted attention module, akin to those forming the backbone of large language models. This connection offers an alternative perspective on attention beyond the conventional information retrieval framework, making it more accessible to researchers and analysts with a background in traditional statistics. It falls into place when OLS is framed as a similarity-based method in a transformed regressor space, distinct from the standard view based on partial correlations. In fact, the OLS solution can be recast as the outcome of an alternative problem: minimizing squared prediction errors by optimizing the embedding space in which training and test vectors are compared via inner products. Rather than estimating coefficients directly, we equivalently learn optimal encoding and decoding operations for predictors. From this vantage point, OLS maps naturally onto the query-key-value structure of attention mechanisms. Building on this foundation, I discuss key elements of Transformer-style attention and draw connections to classic ideas from time series econometrics.\n        △ Less",
        "authors":"['Philippe Goulet Coulombe']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GitBugs: Bug Reports for Duplicate Detection, Retrieval Augmented Generation, Triage, and More",
        "abstract":"Bug reports provide critical insights into software quality, yet existing datasets often suffer from limited scope, outdated content, or insufficient metadata for machine learning. To address these limitations, we present GitBugs-a comprehen- sive and up-to-date dataset comprising over 150,000 bug reports from nine actively maintained open-source projects, including Firefox, Cassandra, and VS Code. GitBugs aggregates data from Github, Bugzilla and Jira issue trackers, offering standardized categorical fields for classification tasks and predefined train\/test splits for duplicate bug detection. In addition, it includes ex- ploratory analysis notebooks and detailed project-level statistics, such as duplicate rates and resolution times. GitBugs supports various software engineering research tasks, including duplicate detection, retrieval augmented generation, resolution prediction, automated triaging, and temporal analysis. The openly licensed dataset provides a valuable cross-project resource for bench- marking and advancing automated bug report analysis. Access the data and code at https:\/\/github.com\/av9ash\/gitbugs\/.\n        △ Less",
        "authors":"['Avinash Patil']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions",
        "abstract":"In this paper, we study the problem of robust subspace recovery (RSR) in the presence of both strong adversarial corruptions and Gaussian noise. Specifically, given a limited number of noisy samples -- some of which are tampered by an adaptive and strong adversary -- we aim to recover a low-dimensional subspace that approximately contains a significant fraction of the uncorrupted samples, up to an error that scales with the Gaussian noise. Existing approaches to this problem often suffer from high computational costs or rely on restrictive distributional assumptions, limiting their applicability in truly adversarial settings. To address these challenges, we revisit the classical random sample consensus (RANSAC) algorithm, which offers strong robustness to adversarial outliers, but sacrifices efficiency and robustness against Gaussian noise and model misspecification in the process. We propose a two-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure modes of standard RANSAC. Our method is provably robust to both Gaussian and adversarial corruptions, achieves near-optimal sample complexity without requiring prior knowledge of the subspace dimension, and is more efficient than existing RANSAC-type methods.\n        △ Less",
        "authors":"['Guixian Chen', 'Jianhao Ma', 'Salar Fattahi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evaluating Machine Learning-Driven Intrusion Detection Systems in IoT: Performance and Energy Consumption",
        "abstract":"In the evolving landscape of the Internet of Things (IoT), Machine Learning (ML)-based Intrusion Detection Systems (IDS) represent a significant advancement, especially when integrated with Software-Defined Networking (SDN). These systems play a critical role in enhancing security infrastructure within resource-constrained IoT systems. Despite their growing adoption, limited research has explored the impact of ML-based IDS on key performance metrics, such as CPU load, CPU usage, and energy consumption, particularly under real-time cyber threats. This study bridges that gap through an empirical evaluation of cutting-edge ML-based IDSs deployed at the edge of IoT networks under both benign and attack scenarios. Additionally, we investigate how SDN's centralized control and dynamic resource management influence IDS performance. Our experimental framework compares traditional ML-based IDS with deep learning (DL)-based counterparts, both with and without SDN integration. Results reveal that edge-deployed ML-based IDSs significantly impact system performance during cyber threats, with marked increases in resource consumption. SDN integration further influences these outcomes, emphasizing the need for optimized architectural design. Statistical analysis using ANOVA confirms the significance of our findings. This research provides critical insights into the performance and trade-offs of deploying ML-based IDSs in edge-based IoT systems.\n        △ Less",
        "authors":"['Saeid Jamshidi', 'Kawser Wazed Nafi', 'Amin Nikanjam', 'Foutse Khomh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization",
        "abstract":"Layer-wise post-training quantization has emerged as a widely used technique for compressing large language models (LLMs) without retraining. However, recent progress in this line of research is saturating, underscoring the need to revisit its core limitation and explore further improvements. This study identifies a critical bottleneck in existing layer-wise PTQ methods: the accumulation of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this, we propose Quantization Error Propagation (QEP), a lightweight and general framework that enhances layer-wise PTQ by explicitly propagating the quantization error which enable compensating for accumulated quantization errors. Additionally, we introduce a tunable propagation mechanism that allows for control over both propagation strength and computational overhead, making the framework adaptable to various architectures and resource constraints. Empirical evaluation on LLaMA2 models (7B, 13B, 70B) demonstrate that incorporating QEP into standard layer-wise PTQ pipelines outperforms standard PTQ methods. Notably, QEP yields substantial performance improvements under extreme low-bit quantization settings.\n        △ Less",
        "authors":"['Yamato Arai', 'Yuma Ichikawa']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mitigating Many-Shot Jailbreaking",
        "abstract":"Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the long context windows of modern LLMs to circumvent model safety training by including in the prompt many examples of a ``fake'' assistant responding inappropriately before the final request. With enough examples, the model's in-context learning abilities override its safety training, and it responds as if it were the ``fake'' assistant. In this work, we probe the effectiveness of different fine tuning and input sanitization approaches on mitigating MSJ attacks, alone and in combination. We find incremental mitigation effectiveness for each, and we show that the combined techniques significantly reduce the effectiveness of MSJ attacks, while retaining model performance in benign in-context learning and conversational tasks. We suggest that our approach could meaningfully ameliorate this vulnerability if incorporated into model safety post-training.\n        △ Less",
        "authors":"['Christopher M. Ackerman', 'Nina Panickssery']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation",
        "abstract":"Single domain generalization (SDG) has recently attracted growing attention in medical image segmentation. One promising strategy for SDG is to leverage consistent semantic shape priors across different imaging protocols, scanner vendors, and clinical sites. However, existing dictionary learning methods that encode shape priors often suffer from limited representational power with a small set of offline computed shape elements, or overfitting when the dictionary size grows. Moreover, they are not readily compatible with large foundation models such as the Segment Anything Model (SAM). In this paper, we propose a novel Mixture-of-Shape-Experts (MoSE) framework that seamlessly integrates the idea of mixture-of-experts (MoE) training into dictionary learning to efficiently capture diverse and robust shape priors. Our method conceptualizes each dictionary atom as a shape expert, which specializes in encoding distinct semantic shape information. A gating network dynamically fuses these shape experts into a robust shape map, with sparse activation guided by SAM encoding to prevent overfitting. We further provide this shape map as a prompt to SAM, utilizing the powerful generalization capability of SAM through bidirectional integration. All modules, including the shape dictionary, are trained in an end-to-end manner. Extensive experiments on multiple public datasets demonstrate its effectiveness.\n        △ Less",
        "authors":"['Jia Wei', 'Xiaoqi Zhao', 'Jonghye Woo', 'Jinsong Ouyang', 'Georges El Fakhri', 'Qingyu Chen', 'Xiaofeng Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
        "abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet principled explanations for their underlying mechanisms and several phenomena, such as scaling laws, hallucinations, and related behaviors, remain elusive. In this work, we revisit the classical relationship between compression and prediction, grounded in Kolmogorov complexity and Shannon information theory, to provide deeper insights into LLM behaviors. By leveraging the Kolmogorov Structure Function and interpreting LLM compression as a two-part coding process, we offer a detailed view of how LLMs acquire and store information across increasing model and data scales -- from pervasive syntactic patterns to progressively rarer knowledge elements. Motivated by this theoretical perspective and natural assumptions inspired by Heap's and Zipf's laws, we introduce a simplified yet representative hierarchical data-generation framework called the Syntax-Knowledge model. Under the Bayesian setting, we show that prediction and compression within this model naturally lead to diverse learning and scaling behaviors of LLMs. In particular, our theoretical analysis offers intuitive and principled explanations for both data and model scaling laws, the dynamics of knowledge acquisition during training and fine-tuning, factual knowledge hallucinations in LLMs. The experimental results validate our theoretical predictions.\n        △ Less",
        "authors":"['Zhixuan Pan', 'Shaowen Wang', 'Jian Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ControlNET: A Firewall for RAG-based LLM System",
        "abstract":"Retrieval-Augmented Generation (RAG) has significantly enhanced the factual accuracy and domain adaptability of Large Language Models (LLMs). This advancement has enabled their widespread deployment across sensitive domains such as healthcare, finance, and enterprise applications. RAG mitigates hallucinations by integrating external knowledge, yet introduces privacy risk and security risk, notably data breaching risk and data poisoning risk. While recent studies have explored prompt injection and poisoning attacks, there remains a significant gap in comprehensive research on controlling inbound and outbound query flows to mitigate these threats. In this paper, we propose an AI firewall, ControlNET, designed to safeguard RAG-based LLM systems from these vulnerabilities. ControlNET controls query flows by leveraging activation shift phenomena to detect adversarial queries and mitigate their impact through semantic divergence. We conduct comprehensive experiments on four different benchmark datasets including Msmarco, HotpotQA, FinQA, and MedicalSys using state-of-the-art open source LLMs (Llama3, Vicuna, and Mistral). Our results demonstrate that ControlNET achieves over 0.909 AUROC in detecting and mitigating security threats while preserving system harmlessness. Overall, ControlNET offers an effective, robust, harmless defense mechanism, marking a significant advancement toward the secure deployment of RAG-based LLM systems.\n        △ Less",
        "authors":"['Hongwei Yao', 'Haoran Shi', 'Yidou Chen', 'Yixin Jiang', 'Cong Wang', 'Zhan Qin', 'Kui Ren', 'Chun Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reduction of Supervision for Biomedical Knowledge Discovery",
        "abstract":"Knowledge discovery is hindered by the increasing volume of publications and the scarcity of extensive annotated data. To tackle the challenge of information overload, it is essential to employ automated methods for knowledge extraction and processing. Finding the right balance between the level of supervision and the effectiveness of models poses a significant challenge. While supervised techniques generally result in better performance, they have the major drawback of demanding labeled data. This requirement is labor-intensive and time-consuming and hinders scalability when exploring new domains. In this context, our study addresses the challenge of identifying semantic relationships between biomedical entities (e.g., diseases, proteins) in unstructured text while minimizing dependency on supervision. We introduce a suite of unsupervised algorithms based on dependency trees and attention mechanisms and employ a range of pointwise binary classification methods. Transitioning from weakly supervised to fully unsupervised settings, we assess the methods' ability to learn from data with noisy labels. The evaluation on biomedical benchmark datasets explores the effectiveness of the methods. Our approach tackles a central issue in knowledge discovery: balancing performance with minimal supervision. By gradually decreasing supervision, we assess the robustness of pointwise binary classification techniques in handling noisy labels, revealing their capability to shift from weakly supervised to entirely unsupervised scenarios. Comprehensive benchmarking offers insights into the effectiveness of these techniques, suggesting an encouraging direction toward adaptable knowledge discovery systems, representing progress in creating data-efficient methodologies for extracting useful insights when annotated data is limited.\n        △ Less",
        "authors":"['Christos Theodoropoulos', 'Andrei Catalin Coman', 'James Henderson', 'Marie-Francine Moens']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline",
        "abstract":"When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple prompt \"Translate the following sentence from [src lang] into [tgt lang]:\". However, in many real scenarios, the source tokens arrive in a streaming manner and simultaneous machine translation (SiMT) is required, then the efficiency and performance of decoder-only LLMs are significantly limited by their auto-regressive nature. To enable LLMs to achieve high-quality SiMT as efficiently as offline translation, we propose a novel paradigm that includes constructing supervised fine-tuning (SFT) data for SiMT, along with new training and inference strategies. To replicate the token input\/output stream in SiMT, the source and target tokens are rearranged into an interleaved sequence, separated by special tokens according to varying latency requirements. This enables powerful LLMs to learn read and write operations adaptively, based on varying latency prompts, while still maintaining efficient auto-regressive decoding. Experimental results show that, even with limited SFT data, our approach achieves state-of-the-art performance across various SiMT benchmarks, and preserves the original abilities of offline translation. Moreover, our approach generalizes well to document-level SiMT setting without requiring specific fine-tuning, even beyond the offline translation model.\n        △ Less",
        "authors":"['Biao Fu', 'Minpeng Liao', 'Kai Fan', 'Chengxi Li', 'Liang Zhang', 'Yidong Chen', 'Xiaodong Shi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conditional Independence Test Based on Transport Maps",
        "abstract":"Testing conditional independence between two random vectors given a third is a fundamental and challenging problem in statistics, particularly in multivariate nonparametric settings due to the complexity of conditional structures. We propose a novel framework for testing conditional independence using transport maps. At the population level, we show that two well-defined transport maps can transform the conditional independence test into an unconditional independence test, this substantially simplifies the problem. These transport maps are estimated from data using conditional continuous normalizing flow models. Within this framework, we derive a test statistic and prove its consistency under both the null and alternative hypotheses. A permutation-based procedure is employed to evaluate the significance of the test. We validate the proposed method through extensive simulations and real-data analysis. Our numerical studies demonstrate the practical effectiveness of the proposed method for conditional independence testing.\n        △ Less",
        "authors":"['Chenxuan He', 'Yuan Gao', 'Liping Zhu', 'Jian Huang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Causal integration of chemical structures improves representations of microscopy images for morphological profiling",
        "abstract":"Recent advances in self-supervised deep learning have improved our ability to quantify cellular morphological changes in high-throughput microscopy screens, a process known as morphological profiling. However, most current methods only learn from images, despite many screens being inherently multimodal, as they involve both a chemical or genetic perturbation as well as an image-based readout. We hypothesized that incorporating chemical compound structure during self-supervised pre-training could improve learned representations of images in high-throughput microscopy screens. We introduce a representation learning framework, MICON (Molecular-Image Contrastive Learning), that models chemical compounds as treatments that induce counterfactual transformations of cell phenotypes. MICON significantly outperforms classical hand-crafted features such as CellProfiler and existing deep-learning-based representation learning methods in challenging evaluation settings where models must identify reproducible effects of drugs across independent replicates and data-generating centers. We demonstrate that incorporating chemical compound information into the learning process provides consistent improvements in our evaluation setting and that modeling compounds specifically as treatments in a causal framework outperforms approaches that directly align images and compounds in a single representation space. Our findings point to a new direction for representation learning in morphological profiling, suggesting that methods should explicitly account for the multimodal nature of microscopy screening data.\n        △ Less",
        "authors":"['Yemin Yu', 'Neil Tenenholtz', 'Lester Mackey', 'Ying Wei', 'David Alvarez-Melis', 'Ava P. Amini', 'Alex X. Lu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Capturing Longitudinal Changes in Brain Morphology Using Temporally Parameterized Neural Displacement Fields",
        "abstract":"Longitudinal image registration enables studying temporal changes in brain morphology which is useful in applications where monitoring the growth or atrophy of specific structures is important. However this task is challenging due to; noise\/artifacts in the data and quantifying small anatomical changes between sequential scans. We propose a novel longitudinal registration method that models structural changes using temporally parameterized neural displacement fields. Specifically, we implement an implicit neural representation (INR) using a multi-layer perceptron that serves as a continuous coordinate-based approximation of the deformation field at any time point. In effect, for any N scans of a particular subject, our model takes as input a 3D spatial coordinate location x, y, z and a corresponding temporal representation t and learns to describe the continuous morphology of structures for both observed and unobserved points in time. Furthermore, we leverage the analytic derivatives of the INR to derive a new regularization function that enforces monotonic rate of change in the trajectory of the voxels, which is shown to provide more biologically plausible patterns. We demonstrate the effectiveness of our method on 4D brain MR registration.\n        △ Less",
        "authors":"['Aisha L. Shuaibu', 'Kieran A. Gibb', 'Peter A. Wijeratne', 'Ivor J. A. Simpson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimal sparse phase retrieval via a quasi-Bayesian approach",
        "abstract":"This paper addresses the problem of sparse phase retrieval, a fundamental inverse problem in applied mathematics, physics, and engineering, where a signal need to be reconstructed using only the magnitude of its transformation while phase information remains inaccessible. Leveraging the inherent sparsity of many real-world signals, we introduce a novel sparse quasi-Bayesian approach and provide the first theoretical guarantees for such an approach. Specifically, we employ a scaled Student distribution as a continuous shrinkage prior to enforce sparsity and analyze the method using the PAC-Bayesian inequality framework. Our results establish that the proposed Bayesian estimator achieves minimax-optimal convergence rates under sub-exponential noise, matching those of state-of-the-art frequentist methods. To ensure computational feasibility, we develop an efficient Langevin Monte Carlo sampling algorithm. Through numerical experiments, we demonstrate that our method performs comparably to existing frequentist techniques, highlighting its potential as a principled alternative for sparse phase retrieval in noisy settings.\n        △ Less",
        "authors":"['The Tien Mai']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PCM-SAR: Physics-Driven Contrastive Mutual Learning for SAR Classification",
        "abstract":"Existing SAR image classification methods based on Contrastive Learning often rely on sample generation strategies designed for optical images, failing to capture the distinct semantic and physical characteristics of SAR data. To address this, we propose Physics-Driven Contrastive Mutual Learning for SAR Classification (PCM-SAR), which incorporates domain-specific physical insights to improve sample generation and feature extraction. PCM-SAR utilizes the gray-level co-occurrence matrix (GLCM) to simulate realistic noise patterns and applies semantic detection for unsupervised local sampling, ensuring generated samples accurately reflect SAR imaging properties. Additionally, a multi-level feature fusion mechanism based on mutual learning enables collaborative refinement of feature representations. Notably, PCM-SAR significantly enhances smaller models by refining SAR feature representations, compensating for their limited capacity. Experimental results show that PCM-SAR consistently outperforms SOTA methods across diverse datasets and SAR classification tasks.\n        △ Less",
        "authors":"['Pengfei Wang', 'Hao Zheng', 'Zhigang Hu', 'Aikun Xu', 'Meiguang Zheng', 'Liu Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Decoding the mechanisms of the Hattrick football manager game using Bayesian network structure learning for optimal decision-making",
        "abstract":"Hattrick is a free web-based probabilistic football manager game with over 200,000 users competing for titles at national and international levels. Launched in Sweden in 1997 as part of an MSc project, the game's slow-paced design has fostered a loyal community, with many users remaining active for decades. Hattrick's game-engine mechanics are partially hidden, and users have attempted to decode them with incremental success over the years. Rule-based, statistical and machine learning models have been developed to aid this effort and are widely used by the community. However, these models or tools have not been formally described or evaluated in the scientific literature. This study is the first to explore Hattrick using structure learning techniques and Bayesian networks, integrating both data and domain knowledge to develop models capable of explaining and simulating the game engine. We present a comprehensive analysis assessing the effectiveness of structure learning algorithms in relation to knowledge-based structures, and show that while structure learning may achieve a higher overall network fit, it does not result in more accurate predictions for selected variables of interest, when compared to knowledge-based networks that produce a lower overall network fit. Additionally, we introduce and publicly share a fully specified Bayesian network model that matches the performance of top models used by the Hattrick community. We further demonstrate how analysis extends beyond prediction by providing a visual representation of conditional dependencies, and using the best performing Bayesian network model for in-game decision-making. To support future research, we make all data, graphical structures, and models publicly available online.\n        △ Less",
        "authors":"['Anthony C. Constantinou', 'Nicholas Higgins', 'Neville K. Kitson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Prototype Graph Learning",
        "abstract":"In recent years, Federated Graph Learning (FGL) has gained significant attention for its distributed training capabilities in graph-based machine intelligence applications, mitigating data silos while offering a new perspective for privacy-preserve large-scale graph learning. However, multi-level FGL heterogeneity presents various client-server collaboration challenges: (1) Model-level: The variation in clients for expected performance and scalability necessitates the deployment of heterogeneous models. Unfortunately, most FGL methods rigidly demand identical client models due to the direct model weight aggregation on the server. (2) Data-level: The intricate nature of graphs, marked by the entanglement of node profiles and topology, poses an optimization dilemma. This implies that models obtained by federated training struggle to achieve superior performance. (3) Communication-level: Some FGL methods attempt to increase message sharing among clients or between clients and the server to improve training, which inevitably leads to high communication costs. In this paper, we propose FedPG as a general prototype-guided optimization method for the above multi-level FGL heterogeneity. Specifically, on the client side, we integrate multi-level topology-aware prototypes to capture local graph semantics. Subsequently, on the server side, leveraging the uploaded prototypes, we employ topology-guided contrastive learning and personalized technology to tailor global prototypes for each client, broadcasting them to improve local training. Experiments demonstrate that FedPG outperforms SOTA baselines by an average of 3.57\\% in accuracy while reducing communication costs by 168x.\n        △ Less",
        "authors":"['Zhengyu Wu', 'Xunkai Li', 'Yinlin Zhu', 'Rong-Hua Li', 'Guoren Wang', 'Chenghu Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Cluster-Based Synthetic Minority Oversampling Technique for Traffic Mode Choice Prediction with Imbalanced Dataset",
        "abstract":"Urban datasets such as citizen transportation modes often contain disproportionately distributed classes, posing significant challenges to the classification of under-represented samples using data-driven models. In the literature, various resampling methods have been developed to create synthetic data for minority classes (oversampling) or remove samples from majority classes (undersampling) to alleviate class imbalance. However, oversampling approaches tend to overgeneralize minor classes that are closely clustered and neglect sparse regions which may contain crucial information. Conversely, undersampling methods potentially remove useful information on certain subgroups. Hence, a resampling approach that takes the inherent distribution of data into consideration is required to ensure appropriate synthetic data creation. This study proposes an adaptive cluster-based synthetic minority oversampling technique. Density-based spatial clustering is applied on minority classes to identify subgroups based on their input features. The classes in each of these subgroups are then oversampled according to the ratio of data points of their local cluster to the largest majority class. When used in conjunction with machine learning models such as random forest and extreme gradient boosting, this oversampling method results in significantly higher F1 scores for the minority classes compared to other resampling techniques. These improved models provide accurate classification of transportation modes.\n        △ Less",
        "authors":"['Guang An Ooi', 'Shehab Ahmed']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GenEDA: Unleashing Generative Reasoning on Netlist via Multimodal Encoder-Decoder Aligned Foundation Model",
        "abstract":"The success of foundation AI has motivated the research of circuit foundation models, which are customized to assist the integrated circuit (IC) design process. However, existing pre-trained circuit models are typically limited to standalone encoders for predictive tasks or decoders for generative tasks. These two model types are developed independently, operate on different circuit modalities, and reside in separate latent spaces, which restricts their ability to complement each other for more advanced applications. In this work, we present GenEDA, the first framework that aligns circuit encoders with decoders within a shared latent space. GenEDA bridges the gap between graph-based circuit representations and text-based large language models (LLMs), enabling communication between their respective latent spaces. To achieve the alignment, we propose two paradigms that support both open-source trainable LLMs and commercial frozen LLMs. Built on this aligned architecture, GenEDA enables three unprecedented generative reasoning tasks over netlists, where the model reversely generates the high-level functionality from low-level netlists in different granularities. These tasks extend traditional gate-type prediction to direct generation of full-circuit functionality. Experiments demonstrate that GenEDA significantly boosts advanced LLMs' (e.g., GPT-4o and DeepSeek-V3) performance in all tasks.\n        △ Less",
        "authors":"['Wenji Fang', 'Jing Wang', 'Yao Lu', 'Shang Liu', 'Zhiyao Xie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An overview of condensation phenomenon in deep learning",
        "abstract":"In this paper, we provide an overview of a common phenomenon, condensation, observed during the nonlinear training of neural networks: During the nonlinear training of neural networks, neurons in the same layer tend to condense into groups with similar outputs. Empirical observations suggest that the number of condensed clusters of neurons in the same layer typically increases monotonically as training progresses. Neural networks with small weight initializations or Dropout optimization can facilitate this condensation process. We also examine the underlying mechanisms of condensation from the perspectives of training dynamics and the structure of the loss landscape. The condensation phenomenon offers valuable insights into the generalization abilities of neural networks and correlates to stronger reasoning abilities in transformer-based language models.\n        △ Less",
        "authors":"['Zhi-Qin John Xu', 'Yaoyu Zhang', 'Zhangchen Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation",
        "abstract":"Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core observation is that the canonical randomized split of a test set in conventional evaluation leaves the test set dominated by samples with high similarity to the training set. The performance of models is severely degraded on samples with lower similarity to the training set but the drawback is highly overlooked in current evaluation. As a result, the performance can hardly be trusted when the model meets low-similarity samples in real practice. To address this problem, we propose a framework of similarity aware evaluation in which a novel split methodology is proposed to adapt to any desired distribution. This is achieved by a formulation of optimization problems which are approximately and efficiently solved by gradient descent. We perform extensive experiments across five representative methods in four datasets for two typical target evaluations and compare them with various counterpart methods. Results demonstrate that the proposed split methodology can significantly better fit desired distributions and guide the development of models. Code is released at https:\/\/github.com\/Amshoreline\/SAE\/tree\/main.\n        △ Less",
        "authors":"['Chenbin Zhang', 'Zhiqiang Hu', 'Chuchu Jiang', 'Wen Chen', 'Jie Xu', 'Shaoting Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A highly maneuverable flying squirrel drone with controllable foldable wings",
        "abstract":"Typical drones with multi rotors are generally less maneuverable due to unidirectional thrust, which may be unfavorable to agile flight in very narrow and confined spaces. This paper suggests a new bio-inspired drone that is empowered with high maneuverability in a lightweight and easy-to-carry way. The proposed flying squirrel inspired drone has controllable foldable wings to cover a wider range of flight attitudes and provide more maneuverable flight capability with stable tracking performance. The wings of a drone are fabricated with silicone membranes and sophisticatedly controlled by reinforcement learning based on human-demonstrated data. Specially, such learning based wing control serves to capture even the complex aerodynamics that are often impossible to model mathematically. It is shown through experiment that the proposed flying squirrel drone intentionally induces aerodynamic drag and hence provides the desired additional repulsive force even under saturated mechanical thrust. This work is very meaningful in demonstrating the potential of biomimicry and machine learning for realizing an animal-like agile drone.\n        △ Less",
        "authors":"['Jun-Gill Kang', 'Dohyeon Lee', 'Soohee Han']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comorbidity-Informed Transfer Learning for Neuro-developmental Disorder Diagnosis",
        "abstract":"Neuro-developmental disorders are manifested as dysfunctions in cognition, communication, behaviour and adaptability, and deep learning-based computer-aided diagnosis (CAD) can alleviate the increasingly strained healthcare resources on neuroimaging. However, neuroimaging such as fMRI contains complex spatio-temporal features, which makes the corresponding representations susceptible to a variety of distractions, thus leading to less effective in CAD. For the first time, we present a Comorbidity-Informed Transfer Learning(CITL) framework for diagnosing neuro-developmental disorders using fMRI. In CITL, a new reinforced representation generation network is proposed, which first combines transfer learning with pseudo-labelling to remove interfering patterns from the temporal domain of fMRI and generates new representations using encoder-decoder architecture. The new representations are then trained in an architecturally simple classification network to obtain CAD model. In particular, the framework fully considers the comorbidity mechanisms of neuro-developmental disorders and effectively integrates them with semi-supervised learning and transfer learning, providing new perspectives on interdisciplinary. Experimental results demonstrate that CITL achieves competitive accuracies of 76.32% and 73.15% for detecting autism spectrum disorder and attention deficit hyperactivity disorder, respectively, which outperforms existing related transfer learning work for 7.2% and 0.5% respectively.\n        △ Less",
        "authors":"['Xin Wen', 'Shijie Guo', 'Wenbo Ning', 'Rui Cao', 'Jie Xiang', 'Xiaobo Liu', 'Jintai Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Measuring Leakage in Concept-Based Methods: An Information Theoretic Approach",
        "abstract":"Concept Bottleneck Models (CBMs) aim to enhance interpretability by structuring predictions around human-understandable concepts. However, unintended information leakage, where predictive signals bypass the concept bottleneck, compromises their transparency. This paper introduces an information-theoretic measure to quantify leakage in CBMs, capturing the extent to which concept embeddings encode additional, unintended information beyond the specified concepts. We validate the measure through controlled synthetic experiments, demonstrating its effectiveness in detecting leakage trends across various configurations. Our findings highlight that feature and concept dimensionality significantly influence leakage, and that classifier choice impacts measurement stability, with XGBoost emerging as the most reliable estimator. Additionally, preliminary investigations indicate that the measure exhibits the anticipated behavior when applied to soft joint CBMs, suggesting its reliability in leakage quantification beyond fully synthetic settings. While this study rigorously evaluates the measure in controlled synthetic experiments, future work can extend its application to real-world datasets.\n        △ Less",
        "authors":"['Mikael Makonnen', 'Moritz Vandenhirtz', 'Sonia Laguna', 'Julia E Vogt']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"aweSOM: a CPU\/GPU-accelerated Self-organizing Map and Statistically Combined Ensemble Framework for Machine-learning Clustering Analysis",
        "abstract":"We introduce aweSOM, an open-source Python package for machine learning (ML) clustering and classification, using a Self-organizing Maps (SOM) algorithm that incorporates CPU\/GPU acceleration to accommodate large ($N > 10^6$, where $N$ is the number of data points), multidimensional datasets. aweSOM consists of two main modules, one that handles the initialization and training of the SOM, and another that stacks the results of multiple SOM realizations to obtain more statistically robust clusters. Existing Python-based SOM implementations (e.g., POPSOM, Yuan (2018); MiniSom, Vettigli (2018); sklearn-som) primarily serve as proof-of-concept demonstrations, optimized for smaller datasets, but lacking scalability for large, multidimensional data. aweSOM provides a solution for this gap in capability, with good performance scaling up to $\\sim 10^8$ individual points, and capable of utilizing multiple features per point. We compare the code performance against the legacy implementations it is based on, and find a 10-100x speed up, as well as significantly improved memory efficiency, due to several built-in optimizations.\n        △ Less",
        "authors":"['Trung Ha', 'Joonas Nättilä', 'Jordy Davelaar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InfoBound: A Provable Information-Bounds Inspired Framework for Both OoD Generalization and OoD Detection",
        "abstract":"In real-world scenarios, distribution shifts give rise to the importance of two problems: out-of-distribution (OoD) generalization, which focuses on models' generalization ability against covariate shifts (i.e., the changes of environments), and OoD detection, which aims to be aware of semantic shifts (i.e., test-time unseen classes). Real-world testing environments often involve a combination of both covariate and semantic shifts. While numerous methods have been proposed to address these critical issues, only a few works tackled them simultaneously. Moreover, prior works often improve one problem but sacrifice the other. To overcome these limitations, we delve into boosting OoD detection and OoD generalization from the perspective of information theory, which can be easily applied to existing models and different tasks. Building upon the theoretical bounds for mutual information and conditional entropy, we provide a unified approach, composed of Mutual Information Minimization (MI-Min) and Conditional Entropy Maximizing (CE-Max). Extensive experiments and comprehensive evaluations on multi-label image classification and object detection have demonstrated the superiority of our method. It successfully mitigates trade-offs between the two challenges compared to competitive baselines.\n        △ Less",
        "authors":"['Lin Zhu', 'Yifeng Yang', 'Zichao Nie', 'Yuan Gao', 'Jiarui Li', 'Qinying Gu', 'Xinbing Wang', 'Chenghu Zhou', 'Nanyang Ye']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Constants of motion network revisited",
        "abstract":"Discovering constants of motion is meaningful in helping understand the dynamical systems, but inevitably needs proficient mathematical skills and keen analytical capabilities. With the prevalence of deep learning, methods employing neural networks, such as Constant Of Motion nETwork (COMET), are promising in handling this scientific problem. Although the COMET method can produce better predictions on dynamics by exploiting the discovered constants of motion, there is still plenty of room to sharpen it. In this paper, we propose a novel neural network architecture, built using the singular-value-decomposition (SVD) technique, and a two-phase training algorithm to improve the performance of COMET. Extensive experiments show that our approach not only retains the advantages of COMET, such as applying to non-Hamiltonian systems and indicating the number of constants of motion, but also can be more lightweight and noise-robust than COMET.\n        △ Less",
        "authors":"['Wenqi Fang', 'Chao Chen', 'Yongkui Yang', 'Zheng Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ensemble-Enhanced Graph Autoencoder with GAT and Transformer-Based Encoders for Robust Fault Diagnosis",
        "abstract":"Fault classification in industrial machinery is vital for enhancing reliability and reducing downtime, yet it remains challenging due to the variability of vibration patterns across diverse operating conditions. This study introduces a novel graph-based framework for fault classification, converting time-series vibration data from machinery operating at varying horsepower levels into a graph representation. We utilize Shannon's entropy to determine the optimal window size for data segmentation, ensuring each segment captures significant temporal patterns, and employ Dynamic Time Warping (DTW) to define graph edges based on segment similarity. A Graph Auto Encoder (GAE) with a deep graph transformer encoder, decoder, and ensemble classifier is developed to learn latent graph representations and classify faults across various categories. The GAE's performance is evaluated on the Case Western Reserve University (CWRU) dataset, with cross-dataset generalization assessed on the HUST dataset. Results show that GAE achieves a mean F1-score of 0.99 on the CWRU dataset, significantly outperforming baseline models-CNN, LSTM, RNN, GRU, and Bi-LSTM (F1-scores: 0.94-0.97, p < 0.05, Wilcoxon signed-rank test for Bi-LSTM: p < 0.05) -- particularly in challenging classes (e.g., Class 8: 0.99 vs. 0.71 for Bi-LSTM). Visualization of dataset characteristics reveals that datasets with amplified vibration patterns and diverse fault dynamics enhance generalization. This framework provides a robust solution for fault diagnosis under varying conditions, offering insights into dataset impacts on model performance.\n        △ Less",
        "authors":"['Moirangthem Tiken Singh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Spatially Directional Dual-Attention GAT for Spatial Fluoride Health Risk Modeling",
        "abstract":"Environmental exposure to fluoride is a major public health concern, particularly in regions with naturally elevated fluoride concentrations. Accurate modeling of fluoride-related health risks, such as dental fluorosis, requires spatially aware learning frameworks capable of capturing both geographic and semantic heterogeneity. In this work, we propose Spatially Directional Dual-Attention Graph Attention Network (SDD-GAT), a novel spatial graph neural network designed for fine-grained health risk prediction. SDD-GAT introduces a dual-graph architecture that disentangles geographic proximity and attribute similarity, and incorporates a directional attention mechanism that explicitly encodes spatial orientation and distance into the message passing process. To further enhance spatial coherence, we introduce a spatial smoothness regularization term that enforces consistency in predictions across neighboring locations. We evaluate SDD-GAT on a large-scale dataset covering over 50,000 fluoride monitoring samples and fluorosis records across Guizhou Province, China. Results show that SDD-GAT significantly outperforms traditional models and state-of-the-art GNNs in both regression and classification tasks, while also exhibiting improved spatial autocorrelation as measured by Moran's I. Our framework provides a generalizable foundation for spatial health risk modeling and geospatial learning under complex environmental settings.\n        △ Less",
        "authors":"['Da Yuan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems",
        "abstract":"In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between consumer electronic devices and DoS attackers is critical. This paper focuses on the adversarial strategy between consumer electronic devices and DoS attackers in IoT-enabled RSE Systems. We first propose a remote joint estimation model for distributed measurements to effectively reduce consumer electronic device workload and minimize data leakage risks. The Kalman filter is deployed on the remote estimator, and the DoS attacks with open-loop as well as closed-loop are considered. We further introduce advanced reinforcement learning techniques, including centralized and distributed Minimax-DQN, to address high-dimensional decision-making challenges in both open-loop and closed-loop scenarios. Especially, the Q-network instead of the Q-table is used in the proposed approaches, which effectively solves the challenge of Q-learning. Moreover, the proposed distributed Minimax-DQN reduces the action space to expedite the search for Nash Equilibrium (NE). The experimental results validate that the proposed model can expeditiously restore the RSE error covariance to a stable state in the presence of DoS attacks, exhibiting notable attack robustness. The proposed centralized and distributed Minimax-DQN effectively resolves the NE in both open and closed-loop case, showcasing remarkable performance in terms of convergence. It reveals that substantial advantages in both efficiency and stability are achieved compared with the state-of-the-art methods.\n        △ Less",
        "authors":"['Gengcan Chen', 'Donghong Cai', 'Zahid Khan', 'Jawad Ahmad', 'Wadii Boulila']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bregman Linearized Augmented Lagrangian Method for Nonconvex Constrained Stochastic Zeroth-order Optimization",
        "abstract":"In this paper, we study nonconvex constrained stochastic zeroth-order optimization problems, for which we have access to exact information of constraints and noisy function values of the objective. We propose a Bregman linearized augmented Lagrangian method that utilizes stochastic zeroth-order gradient estimators combined with a variance reduction technique. We analyze its oracle complexity, in terms of the total number of stochastic function value evaluations required to achieve an \\(ε\\)-KKT point in \\(\\ell_p\\)-norm metrics with \\(p \\ge 2\\), where \\(p\\) is a parameter associated with the selected Bregman distance. In particular, starting from a near-feasible initial point and using Rademacher smoothing, the oracle complexity is in order \\(O(p d^{2\/p} ε^{-3})\\) for \\(p \\in [2, 2 \\ln d]\\), and \\(O(\\ln d \\cdot ε^{-3})\\) for \\(p > 2 \\ln d\\), where \\(d\\) denotes the problem dimension. Those results show that the complexity of the proposed method can achieve a dimensional dependency lower than \\(O(d)\\) without requiring additional assumptions, provided that a Bregman distance is chosen properly. This offers a significant improvement in the high-dimensional setting over existing work, and matches the lowest complexity order with respect to the tolerance \\(ε\\) reported in the literature. Numerical experiments on constrained Lasso and black-box adversarial attack problems highlight the promising performances of the proposed method.\n        △ Less",
        "authors":"['Qiankun Shi', 'Xiao Wang', 'Hao Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Tin-Tin: Towards Tiny Learning on Tiny Devices with Integer-based Neural Network Training",
        "abstract":"Recent advancements in machine learning (ML) have enabled its deployment on resource-constrained edge devices, fostering innovative applications such as intelligent environmental sensing. However, these devices, particularly microcontrollers (MCUs), face substantial challenges due to limited memory, computing capabilities, and the absence of dedicated floating-point units (FPUs). These constraints hinder the deployment of complex ML models, especially those requiring lifelong learning capabilities. To address these challenges, we propose Tin-Tin, an integer-based on-device training framework designed specifically for low-power MCUs. Tin-Tin introduces novel integer rescaling techniques to efficiently manage dynamic ranges and facilitate efficient weight updates using integer data types. Unlike existing methods optimized for devices with FPUs, GPUs, or FPGAs, Tin-Tin addresses the unique demands of tiny MCUs, prioritizing energy efficiency and optimized memory utilization. We validate the effectiveness of Tin-Tin through end-to-end application examples on real-world tiny devices, demonstrating its potential to support energy-efficient and sustainable ML applications on edge platforms.\n        △ Less",
        "authors":"['Yi Hu', 'Jinhang Zuo', 'Eddie Zhang', 'Bob Iannucci', 'Carlee Joe-Wong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes",
        "abstract":"This paper proposes a reinforcement learning (RL) framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints. To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure.\n  The reward structure penalizes reserve shortfall, capital inefficiency, and solvency floor violations, with design elements informed by Solvency II and Own Risk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two industry datasets--Workers' Compensation, and Other Liability--demonstrate that the RL-CVaR agent achieves superior performance relative to classical reserving methods across multiple criteria, including tail-risk control (CVaR$_{0.95}$), capital efficiency, and regulatory violation rate. The framework also accommodates fixed-shock stress testing and regime-stratified analysis, providing a principled and extensible approach to reserving under uncertainty.\n        △ Less",
        "authors":"['Stella C. Dong', 'James R. Finlay']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Vision Transformers Exhibit Human-Like Biases: Evidence of Orientation and Color Selectivity, Categorical Perception, and Phase Transitions",
        "abstract":"This study explored whether Vision Transformers (ViTs) developed orientation and color biases similar to those observed in the human brain. Using synthetic datasets with controlled variations in noise levels, angles, lengths, widths, and colors, we analyzed the behavior of ViTs fine-tuned with LoRA. Our findings revealed four key insights: First, ViTs exhibited an oblique effect showing the lowest angle prediction errors at 180 deg (horizontal) across all conditions. Second, angle prediction errors varied by color. Errors were highest for bluish hues and lowest for yellowish ones. Additionally, clustering analysis of angle prediction errors showed that ViTs grouped colors in a way that aligned with human perceptual categories. In addition to orientation and color biases, we observed phase transition phenomena. While two phase transitions occurred consistently across all conditions, the training loss curves exhibited delayed transitions when color was incorporated as an additional data attribute. Finally, we observed that attention heads in certain layers inherently develop specialized capabilities, functioning as task-agnostic feature extractors regardless of the downstream task. These observations suggest that biases and properties arise primarily from pre-training on the original dataset which shapes the model's foundational representations and the inherent architectural constraints of the vision transformer, rather than being solely determined by downstream data statistics.\n        △ Less",
        "authors":"['Nooshin Bahador']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Expressivity of Quadratic Neural ODEs",
        "abstract":"This work focuses on deriving quantitative approximation error bounds for neural ordinary differential equations having at most quadratic nonlinearities in the dynamics. The simple dynamics of this model form demonstrates how expressivity can be derived primarily from iteratively composing many basic elementary operations, versus from the complexity of those elementary operations themselves. Like the analog differential analyzer and universal polynomial DAEs, the expressivity is derived instead primarily from the \"depth\" of the model. These results contribute to our understanding of what depth specifically imparts to the capabilities of deep learning architectures.\n        △ Less",
        "authors":"['Joshua Hanson', 'Maxim Raginsky']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Machine Learning-Based Cyberattack Detection and Identification for Automatic Generation Control Systems Considering Nonlinearities",
        "abstract":"Automatic generation control (AGC) systems play a crucial role in maintaining system frequency across power grids. However, AGC systems' reliance on communicated measurements exposes them to false data injection attacks (FDIAs), which can compromise the overall system stability. This paper proposes a machine learning (ML)-based detection framework that identifies FDIAs and determines the compromised measurements. The approach utilizes an ML model trained offline to accurately detect attacks and classify the manipulated signals based on a comprehensive set of statistical and time-series features extracted from AGC measurements before and after disturbances. For the proposed approach, we compare the performance of several powerful ML algorithms. Our results demonstrate the efficacy of the proposed method in detecting FDIAs while maintaining a low false alarm rate, with an F1-score of up to 99.98%, outperforming existing approaches.\n        △ Less",
        "authors":"['Nour M. Shabar', 'Ahmad Mohammad Saber', 'Deepa Kundur']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis",
        "abstract":"Timely and accurate diagnosis of neurodegenerative disorders, such as Alzheimer's disease, is central to disease management. Existing deep learning models require large-scale annotated datasets and often function as \"black boxes\". Additionally, datasets in clinical practice are frequently small or unlabeled, restricting the full potential of deep learning methods. Here, we introduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning -- a new machine learning framework that facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans through a reference-based reasoning process. Specifically, REMEMBER first trains a contrastively aligned vision-text model using expert-annotated reference data and extends pseudo-text modalities that encode abnormality types, diagnosis labels, and composite clinical descriptions. Then, at inference time, REMEMBER retrieves similar, human-validated cases from a curated dataset and integrates their contextual information through a dedicated evidence encoding module and attention-based inference head. Such an evidence-guided design enables REMEMBER to imitate real-world clinical decision-making process by grounding predictions in retrieved imaging and textual context. Specifically, REMEMBER outputs diagnostic predictions alongside an interpretable report, including reference images and explanations aligned with clinical workflows. Experimental results demonstrate that REMEMBER achieves robust zero- and few-shot performance and offers a powerful and explainable framework to neuroimaging-based diagnosis in the real world, especially under limited data.\n        △ Less",
        "authors":"['Duy-Cat Can', 'Quang-Huy Tang', 'Huong Ha', 'Binh T. Nguyen', 'Oliver Y. Chén']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Explorer: Robust Collection of Interactable GUI Elements",
        "abstract":"Automation of existing Graphical User Interfaces (GUIs) is important but hard to achieve. Upstream of making the GUI user-accessible or somehow scriptable, even the data-collection to understand the original interface poses significant challenges. For example, large quantities of general UI data seem helpful for training general machine learning (ML) models, but accessibility for each person can hinge on the ML's precision on a specific app. We therefore take the perspective that a given user needs confidence, that the relevant UI elements are being detected correctly throughout one app or digital environment. We mostly assume that the target application is known in advance, so that data collection and ML-training can be personalized for the test-time target domain. The proposed Explorer system focuses on detecting on-screen buttons and text-entry fields, i.e. interactables, where the training process has access to a live version of the application. The live application can run on almost any popular platform except iOS phones, and the collection is especially streamlined for Android phones or for desktop Chrome browsers. Explorer also enables the recording of interactive user sessions, and subsequent mapping of how such sessions overlap and sometimes loop back to similar states. We show how having such a map enables a kind of path planning through the GUI, letting a user issue audio commands to get to their destination. Critically, we are releasing our code for Explorer openly at https:\/\/github.com\/varnelis\/Explorer.\n        △ Less",
        "authors":"['Iason Chaimalas', 'Arnas Vyšniauskas', 'Gabriel Brostow']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph-Based Prediction Models for Data Debiasing",
        "abstract":"Bias in data collection, arising from both under-reporting and over-reporting, poses significant challenges in critical applications such as healthcare and public safety. In this work, we introduce Graph-based Over- and Under-reporting Debiasing (GROUD), a novel graph-based optimization framework that debiases reported data by jointly estimating the true incident counts and the associated reporting bias probabilities. By modeling the bias as a smooth signal over a graph constructed from geophysical or feature-based similarities, our convex formulation not only ensures a unique solution but also comes with theoretical recovery guarantees under certain assumptions. We validate GROUD on both challenging simulated experiments and real-world datasets -- including Atlanta emergency calls and COVID-19 vaccine adverse event reports -- demonstrating its robustness and superior performance in accurately recovering debiased counts. This approach paves the way for more reliable downstream decision-making in systems affected by reporting irregularities.\n        △ Less",
        "authors":"['Dongze Wu', 'Hanyang Jiang', 'Yao Xie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Inferring Outcome Means of Exponential Family Distributions Estimated by Deep Neural Networks",
        "abstract":"While deep neural networks (DNNs) are widely used for prediction, inference on DNN-estimated subject-specific means for categorical or exponential family outcomes remains underexplored. We address this by proposing a DNN estimator under generalized nonparametric regression models (GNRMs) and developing a rigorous inference framework. Unlike existing approaches that assume independence between prediction errors and inputs to establish the error bound, a condition often violated in GNRMs, we allow for dependence and our theoretical analysis demonstrates the feasibility of drawing inference under GNRMs. To implement inference, we consider an Ensemble Subsampling Method (ESM) that leverages U-statistics and the Hoeffding decomposition to construct reliable confidence intervals for DNN estimates. We show that, under GNRM settings, ESM enables model-free variance estimation and accounts for heterogeneity among individuals in the population. Through simulations under nonparametric logistic, Poisson, and binomial regression models, we demonstrate the effectiveness and efficiency of our method. We further apply the method to the electronic Intensive Care Unit (eICU) dataset, a large-scale collection of anonymized health records from ICU patients, to predict ICU readmission risk and offer patient-centric insights for clinical decision-making.\n        △ Less",
        "authors":"['Xuran Meng', 'Yi Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Context-Aware Adaptive Sampling for Intelligent Data Acquisition Systems Using DQN",
        "abstract":"Multi-sensor systems are widely used in the Internet of Things, environmental monitoring, and intelligent manufacturing. However, traditional fixed-frequency sampling strategies often lead to severe data redundancy, high energy consumption, and limited adaptability, failing to meet the dynamic sensing needs of complex environments. To address these issues, this paper proposes a DQN-based multi-sensor adaptive sampling optimization method. By leveraging a reinforcement learning framework to learn the optimal sampling strategy, the method balances data quality, energy consumption, and redundancy. We first model the multi-sensor sampling task as a Markov Decision Process (MDP), then employ a Deep Q-Network to optimize the sampling policy. Experiments on the Intel Lab Data dataset confirm that, compared with fixed-frequency sampling, threshold-triggered sampling, and other reinforcement learning approaches, DQN significantly improves data quality while lowering average energy consumption and redundancy rates. Moreover, in heterogeneous multi-sensor environments, DQN-based adaptive sampling shows enhanced robustness, maintaining superior data collection performance even in the presence of interference factors. These findings demonstrate that DQN-based adaptive sampling can enhance overall data acquisition efficiency in multi-sensor systems, providing a new solution for efficient and intelligent sensing.\n        △ Less",
        "authors":"['Weiqiang Huang', 'Juecen Zhan', 'Yumeng Sun', 'Xu Han', 'Tai An', 'Nan Jiang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Minority Reports: Balancing Cost and Quality in Ground Truth Data Annotation",
        "abstract":"High-quality data annotation is an essential but laborious and costly aspect of developing machine learning-based software. We explore the inherent tradeoff between annotation accuracy and cost by detecting and removing minority reports -- instances where annotators provide incorrect responses -- that indicate unnecessary redundancy in task assignments. We propose an approach to prune potentially redundant annotation task assignments before they are executed by estimating the likelihood of an annotator disagreeing with the majority vote for a given task. Our approach is informed by an empirical analysis over computer vision datasets annotated by a professional data annotation platform, which reveals that the likelihood of a minority report event is dependent primarily on image ambiguity, worker variability, and worker fatigue. Simulations over these datasets show that we can reduce the number of annotations required by over 60% with a small compromise in label quality, saving approximately 6.6 days-equivalent of labor. Our approach provides annotation service platforms with a method to balance cost and dataset quality. Machine learning practitioners can tailor annotation accuracy levels according to specific application needs, thereby optimizing budget allocation while maintaining the data quality necessary for critical settings like autonomous driving technology.\n        △ Less",
        "authors":"['Hsuan Wei Liao', 'Christopher Klugmann', 'Daniel Kondermann', 'Rafid Mahmood']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Building Machine Learning Limited Area Models: Kilometer-Scale Weather Forecasting in Realistic Settings",
        "abstract":"Machine learning is revolutionizing global weather forecasting, with models that efficiently produce highly accurate forecasts. Apart from global forecasting there is also a large value in high-resolution regional weather forecasts, focusing on accurate simulations of the atmosphere for a limited area. Initial attempts have been made to use machine learning for such limited area scenarios, but these experiments do not consider realistic forecasting settings and do not investigate the many design choices involved. We present a framework for building kilometer-scale machine learning limited area models with boundary conditions imposed through a flexible boundary forcing method. This enables boundary conditions defined either from reanalysis or operational forecast data. Our approach employs specialized graph constructions with rectangular and triangular meshes, along with multi-step rollout training strategies to improve temporal consistency. We perform systematic evaluation of different design choices, including the boundary width, graph construction and boundary forcing integration. Models are evaluated across both a Danish and a Swiss domain, two regions that exhibit different orographical characteristics. Verification is performed against both gridded analysis data and in-situ observations, including a case study for the storm Ciara in February 2020. Both models achieve skillful predictions across a wide range of variables, with our Swiss model outperforming the numerical weather prediction baseline for key surface variables. With their substantially lower computational cost, our findings demonstrate great potential for machine learning limited area models in the future of regional weather forecasting.\n        △ Less",
        "authors":"['Simon Adamov', 'Joel Oskarsson', 'Leif Denby', 'Tomas Landelius', 'Kasper Hintz', 'Simon Christiansen', 'Irene Schicker', 'Carlos Osuna', 'Fredrik Lindsten', 'Oliver Fuhrer', 'Sebastian Schemm']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Optimal Differentially Private Regret Bounds in Linear MDPs",
        "abstract":"We study regret minimization under privacy constraints in episodic inhomogeneous linear Markov Decision Processes (MDPs), motivated by the growing use of reinforcement learning (RL) in personalized decision-making systems that rely on sensitive user data. In this setting, both transition probabilities and reward functions are assumed to be linear in a feature mapping $φ(s, a)$, and we aim to ensure privacy through joint differential privacy (JDP), a relaxation of differential privacy suited to online learning. Prior work has established suboptimal regret bounds by privatizing the LSVI-UCB algorithm, which achieves $\\widetilde{O}(\\sqrt{d^3 H^4 K})$ regret in the non-private setting. Building on recent advances that improve this to minimax optimal regret $\\widetilde{O}(HD\\sqrt{K})$ via LSVI-UCB++ with Bernstein-style bonuses, we design a new differentially private algorithm by privatizing LSVI-UCB++ and adapting techniques for variance-aware analysis from offline RL. Our algorithm achieves a regret bound of $\\widetilde{O}(d \\sqrt{H^3 K} + H^{4.5} d^{7\/6} K^{1\/2} \/ ε)$, improving over previous private methods. Empirical results show that our algorithm retains near-optimal utility compared to non-private baselines, indicating that privacy can be achieved with minimal performance degradation in this setting.\n        △ Less",
        "authors":"['Sharan Sahu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Implementation of Reinforcement Learning over Homomorphic Encryption",
        "abstract":"We investigate encrypted control policy synthesis over the cloud. While encrypted control implementations have been studied previously, we focus on the less explored paradigm of privacy-preserving control synthesis, which can involve heavier computations ideal for cloud outsourcing. We classify control policy synthesis into model-based, simulator-driven, and data-driven approaches and examine their implementation over fully homomorphic encryption (FHE) for privacy enhancements. A key challenge arises from comparison operations (min or max) in standard reinforcement learning algorithms, which are difficult to execute over encrypted data. This observation motivates our focus on Relative-Entropy-regularized reinforcement learning (RL) problems, which simplifies encrypted evaluation of synthesis algorithms due to their comparison-free structures. We demonstrate how linearly solvable value iteration, path integral control, and Z-learning can be readily implemented over FHE. We conduct a case study of our approach through numerical simulations of encrypted Z-learning in a grid world environment using the CKKS encryption scheme, showing convergence with acceptable approximation error. Our work suggests the potential for secure and efficient cloud-based reinforcement learning.\n        △ Less",
        "authors":"['Jihoon Suh', 'Takashi Tanaka']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Regretful Decisions under Label Noise",
        "abstract":"Machine learning models are routinely used to support decisions that affect individuals -- be it to screen a patient for a serious illness or to gauge their response to treatment. In these tasks, we are limited to learning models from datasets with noisy labels. In this paper, we study the instance-level impact of learning under label noise. We introduce a notion of regret for this regime which measures the number of unforeseen mistakes due to noisy labels. We show that standard approaches to learning under label noise can return models that perform well at a population level while subjecting individuals to a lottery of mistakes. We present a versatile approach to estimate the likelihood of mistakes at the individual level from a noisy dataset by training models over plausible realizations of datasets without label noise. This is supported by a comprehensive empirical study of label noise in clinical prediction tasks. Our results reveal how failure to anticipate mistakes can compromise model reliability and adoption, and demonstrate how we can address these challenges by anticipating and avoiding regretful decisions.\n        △ Less",
        "authors":"['Sujay Nagaraj', 'Yang Liu', 'Flavio P. Calmon', 'Berk Ustun']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Text To 3D Object Generation For Scalable Room Assembly",
        "abstract":"Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, and customizable 3D indoor scenes. By integrating and adapting text-to-image and multi-view diffusion models with Neural Radiance Field-based meshing, this system generates highfidelity 3D object assets from text prompts and incorporates them into pre-defined floor plans using a rendering tool. By introducing novel loss functions and training strategies into existing methods, the system supports on-demand scene generation, aiming to alleviate the scarcity of current available data, generally manually crafted by artists. This system advances the role of synthetic data in addressing machine learning training limitations, enabling more robust and generalizable models for real-world applications.\n        △ Less",
        "authors":"['Sonia Laguna', 'Alberto Garcia-Garcia', 'Marie-Julie Rakotosaona', 'Stylianos Moschoglou', 'Leonhard Helminger', 'Sergio Orts-Escolano']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MedIL: Implicit Latent Spaces for Generating Heterogeneous Medical Images at Arbitrary Resolutions",
        "abstract":"In this work, we introduce MedIL, a first-of-its-kind autoencoder built for encoding medical images with heterogeneous sizes and resolutions for image generation. Medical images are often large and heterogeneous, where fine details are of vital clinical importance. Image properties change drastically when considering acquisition equipment, patient demographics, and pathology, making realistic medical image generation challenging. Recent work in latent diffusion models (LDMs) has shown success in generating images resampled to a fixed-size. However, this is a narrow subset of the resolutions native to image acquisition, and resampling discards fine anatomical details. MedIL utilizes implicit neural representations to treat images as continuous signals, where encoding and decoding can be performed at arbitrary resolutions without prior resampling. We quantitatively and qualitatively show how MedIL compresses and preserves clinically-relevant features over large multi-site, multi-resolution datasets of both T1w brain MRIs and lung CTs. We further demonstrate how MedIL can influence the quality of images generated with a diffusion model, and discuss how MedIL can enhance generative models to resemble raw clinical acquisitions.\n        △ Less",
        "authors":"['Tyler Spears', 'Shen Zhu', 'Yinzhu Jin', 'Aman Shrivastava', 'P. Thomas Fletcher']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conformal Calibration: Ensuring the Reliability of Black-Box AI in Wireless Systems",
        "abstract":"AI is poised to revolutionize telecommunication networks by boosting efficiency, automation, and decision-making. However, the black-box nature of most AI models introduces substantial risk, possibly deterring adoption by network operators. These risks are not addressed by the current prevailing deployment strategy, which typically follows a best-effort train-and-deploy paradigm. This paper reviews conformal calibration, a general framework that moves beyond the state of the art by adopting computationally lightweight, advanced statistical tools that offer formal reliability guarantees without requiring further training or fine-tuning. Conformal calibration encompasses pre-deployment calibration via uncertainty quantification or hyperparameter selection; online monitoring to detect and mitigate failures in real time; and counterfactual post-deployment performance analysis to address \"what if\" diagnostic questions after deployment. By weaving conformal calibration into the AI model lifecycle, network operators can establish confidence in black-box AI models as a dependable enabling technology for wireless systems.\n        △ Less",
        "authors":"['Osvaldo Simeone', 'Sangwoo Park', 'Matteo Zecchin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Contrastive Demonstration Selection with Semantic Diversity for Robust In-Context Machine Translation",
        "abstract":"In-Context Learning (ICL) empowers large language models to perform tasks by conditioning on a few input-output examples. However, the performance of ICL is highly sensitive to the selection of these demonstrations. While existing methods focus on similarity or contrastive selection, they often overlook the importance of diversity among the chosen examples. In this paper, we propose DiverseConE (Diversity-Enhanced Contrastive Example Selection), a novel approach for demonstration selection in in-context learning for machine translation. Our method builds upon contrastive selection by incorporating a diversity enhancement step based on embedding space dissimilarity. We conduct extensive experiments on the Llama2-7b model across four language pairs (English-Chinese, Chinese-English, Russian-German, German-Russian) in 1-shot and 3-shot settings, using COMET20 and COMET22 for evaluation. Our results demonstrate that DiverseConE consistently outperforms strong baseline methods, including random selection, BM25, TopK, and a state-of-the-art contrastive selection method. Further analysis, including diversity metrics and human evaluation, validates the effectiveness of our approach and highlights the benefits of considering demonstration diversity for improved translation quality.\n        △ Less",
        "authors":"['Owen Patterson', 'Chee Ng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Application of Contrastive Learning on ECG Data: Evaluating Performance in Japanese and Classification with Around 100 Labels",
        "abstract":"The electrocardiogram (ECG) is a fundamental tool in cardiovascular diagnostics due to its powerful and non-invasive nature. One of the most critical usages is to determine whether more detailed examinations are necessary, with users ranging across various levels of expertise. Given this diversity in expertise, it is essential to assist users to avoid critical errors. Recent studies in machine learning have addressed this challenge by extracting valuable information from ECG data. Utilizing language models, these studies have implemented multimodal models aimed at classifying ECGs according to labeled terms. However, the number of classes was reduced, and it remains uncertain whether the technique is effective for languages other than English. To move towards practical application, we utilized ECG data from regular patients visiting hospitals in Japan, maintaining a large number of Japanese labels obtained from actual ECG readings. Using a contrastive learning framework, we found that even with 98 labels for classification, our Japanese-based language model achieves accuracy comparable to previous research. This study extends the applicability of multimodal machine learning frameworks to broader clinical studies and non-English languages.\n        △ Less",
        "authors":"['Junichiro Takahashi', 'JingChuan Guan', 'Masataka Sato', 'Kaito Baba', 'Kazuto Haruguchi', 'Daichi Nagashima', 'Satoshi Kodera', 'Norihiko Takeda']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond Glucose-Only Assessment: Advancing Nocturnal Hypoglycemia Prediction in Children with Type 1 Diabetes",
        "abstract":"The dead-in-bed syndrome describes the sudden and unexplained death of young individuals with Type 1 Diabetes (T1D) without prior long-term complications. One leading hypothesis attributes this phenomenon to nocturnal hypoglycemia (NH), a dangerous drop in blood glucose during sleep. This study aims to improve NH prediction in children with T1D by leveraging physiological data and machine learning (ML) techniques. We analyze an in-house dataset collected from 16 children with T1D, integrating physiological metrics from wearable sensors. We explore predictive performance through feature engineering, model selection, architectures, and oversampling. To address data limitations, we apply transfer learning from a publicly available adult dataset. Our results achieve an AUROC of 0.75 +- 0.21 on the in-house dataset, further improving to 0.78 +- 0.05 with transfer learning. This research moves beyond glucose-only predictions by incorporating physiological parameters, showcasing the potential of ML to enhance NH detection and improve clinical decision-making for pediatric diabetes management.\n        △ Less",
        "authors":"['Marco Voegeli', 'Sonia Laguna', 'Heike Leutheuser', 'Marc Pfister', 'Marie-Anne Burckhardt', 'Julia E Vogt']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sparse Hybrid Linear-Morphological Networks",
        "abstract":"We investigate hybrid linear-morphological networks. Recent studies highlight the inherent affinity of morphological layers to pruning, but also their difficulty in training. We propose a hybrid network structure, wherein morphological layers are inserted between the linear layers of the network, in place of activation functions. We experiment with the following morphological layers: 1) maxout pooling layers (as a special case of a morphological layer), 2) fully connected dense morphological layers, and 3) a novel, sparsely initialized variant of (2). We conduct experiments on the Magna-Tag-A-Tune (music auto-tagging) and CIFAR-10 (image classification) datasets, replacing the linear classification heads of state-of-the-art convolutional network architectures with our proposed network structure for the various morphological layers. We demonstrate that these networks induce sparsity to their linear layers, making them more prunable under L1 unstructured pruning. We also show that on MTAT our proposed sparsely initialized layer achieves slightly better performance than ReLU, maxout, and densely initialized max-plus layers, and exhibits faster initial convergence.\n        △ Less",
        "authors":"['Konstantinos Fotopoulos', 'Christos Garoufis', 'Petros Maragos']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"No-Regret Generative Modeling via Parabolic Monge-Ampère PDE",
        "abstract":"We introduce a novel generative modeling framework based on a discretized parabolic Monge-Ampère PDE, which emerges as a continuous limit of the Sinkhorn algorithm commonly used in optimal transport. Our method performs iterative refinement in the space of Brenier maps using a mirror gradient descent step. We establish theoretical guarantees for generative modeling through the lens of no-regret analysis, demonstrating that the iterates converge to the optimal Brenier map under a variety of step-size schedules. As a technical contribution, we derive a new Evolution Variational Inequality tailored to the parabolic Monge-Ampère PDE, connecting geometry, transportation cost, and regret. Our framework accommodates non-log-concave target distributions, constructs an optimal sampling process via the Brenier map, and integrates favorable learning techniques from generative adversarial networks and score-based diffusion models. As direct applications, we illustrate how our theory paves new pathways for generative modeling and variational inference.\n        △ Less",
        "authors":"['Nabarun Deb', 'Tengyuan Liang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mixture of Group Experts for Learning Invariant Representations",
        "abstract":"Sparsely activated Mixture-of-Experts (MoE) models effectively increase the number of parameters while maintaining consistent computational costs per token. However, vanilla MoE models often suffer from limited diversity and specialization among experts, constraining their performance and scalability, especially as the number of experts increases. In this paper, we present a novel perspective on vanilla MoE with top-$k$ routing inspired by sparse representation. This allows us to bridge established theoretical insights from sparse representation into MoE models. Building on this foundation, we propose a group sparse regularization approach for the input of top-$k$ routing, termed Mixture of Group Experts (MoGE). MoGE indirectly regularizes experts by imposing structural constraints on the routing inputs, while preserving the original MoE architecture. Furthermore, we organize the routing input into a 2D topographic map, spatially grouping neighboring elements. This structure enables MoGE to capture representations invariant to minor transformations, thereby significantly enhancing expert diversity and specialization. Comprehensive evaluations across various Transformer models for image classification and language modeling tasks demonstrate that MoGE substantially outperforms its MoE counterpart, with minimal additional memory and computation overhead. Our approach provides a simple yet effective solution to scale the number of experts and reduce redundancy among them. The source code is included in the supplementary material and will be publicly released.\n        △ Less",
        "authors":"['Lei Kang', 'Jia Li', 'Mi Tian', 'Hua Huang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Machine Learning-Based AP Selection in User-Centric Cell-free Multiple-Antenna Networks",
        "abstract":"User-centric cell-free (UCCF) massive multiple-input multiple-output (MIMO) systems are considered a viable solution to realize the advantages offered by cell-free (CF) networks, including reduced interference and consistent quality of service while maintaining manageable complexity. In this paper, we propose novel learning-based access point (AP) selection schemes tailored for UCCF massive MIMO systems. The learning model exploits the dataset generated from two distinct AP selection schemes, based on large-scale fading (LSF) coefficients and the sum-rate coefficients, respectively. The proposed learning-based AP selection schemes could be implemented centralized or distributed, with the aim of performing AP selection efficiently. We evaluate our model's performance against CF and two heuristic clustering schemes for UCCF networks. The results demonstrate that the learning-based approach achieves a comparable sum-rate performance to that of competing techniques for UCCF networks, while significantly reducing computational complexity.\n        △ Less",
        "authors":"['S. Salehi', 'S. Mashdour', 'O. Tamyigit', 'S. Seyedmasoumian', 'M. Moradikia', 'R. C. de Lamare', 'A. Schmeink']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"NetTAG: A Multimodal RTL-and-Layout-Aligned Netlist Foundation Model via Text-Attributed Graph",
        "abstract":"Circuit representation learning has shown promise in advancing Electronic Design Automation (EDA) by capturing structural and functional circuit properties for various tasks. Existing pre-trained solutions rely on graph learning with complex functional supervision, such as truth table simulation. However, they only handle simple and-inverter graphs (AIGs), struggling to fully encode other complex gate functionalities. While large language models (LLMs) excel at functional understanding, they lack the structural awareness for flattened netlists. To advance netlist representation learning, we present NetTAG, a netlist foundation model that fuses gate semantics with graph structure, handling diverse gate types and supporting a variety of functional and physical tasks. Moving beyond existing graph-only methods, NetTAG formulates netlists as text-attributed graphs, with gates annotated by symbolic logic expressions and physical characteristics as text attributes. Its multimodal architecture combines an LLM-based text encoder for gate semantics and a graph transformer for global structure. Pre-trained with gate and graph self-supervised objectives and aligned with RTL and layout stages, NetTAG captures comprehensive circuit intrinsics. Experimental results show that NetTAG consistently outperforms each task-specific method on four largely different functional and physical tasks and surpasses state-of-the-art AIG encoders, demonstrating its versatility.\n        △ Less",
        "authors":"['Wenji Fang', 'Wenkai Li', 'Shang Liu', 'Yao Lu', 'Hongce Zhang', 'Zhiyao Xie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MiMIC: Multi-Modal Indian Earnings Calls Dataset to Predict Stock Prices",
        "abstract":"Predicting stock market prices following corporate earnings calls remains a significant challenge for investors and researchers alike, requiring innovative approaches that can process diverse information sources. This study investigates the impact of corporate earnings calls on stock prices by introducing a multi-modal predictive model. We leverage textual data from earnings call transcripts, along with images and tables from accompanying presentations, to forecast stock price movements on the trading day immediately following these calls. To facilitate this research, we developed the MiMIC (Multi-Modal Indian Earnings Calls) dataset, encompassing companies representing the Nifty 50, Nifty MidCap 50, and Nifty Small 50 indices. The dataset includes earnings call transcripts, presentations, fundamentals, technical indicators, and subsequent stock prices. We present a multimodal analytical framework that integrates quantitative variables with predictive signals derived from textual and visual modalities, thereby enabling a holistic approach to feature representation and analysis. This multi-modal approach demonstrates the potential for integrating diverse information sources to enhance financial forecasting accuracy. To promote further research in computational economics, we have made the MiMIC dataset publicly available under the CC-NC-SA-4.0 licence. Our work contributes to the growing body of literature on market reactions to corporate communications and highlights the efficacy of multi-modal machine learning techniques in financial analysis.\n        △ Less",
        "authors":"['Sohom Ghosh', 'Arnab Maji', 'Sudip Kumar Naskar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adiabatic Encoding of Pre-trained MPS Classifiers into Quantum Circuits",
        "abstract":"Although Quantum Neural Networks (QNNs) offer powerful methods for classification tasks, the training of QNNs faces two major training obstacles: barren plateaus and local minima. A promising solution is to first train a tensor-network (TN) model classically and then embed it into a QNN.\\ However, embedding TN-classifiers into quantum circuits generally requires postselection whose success probability may decay exponentially with the system size. We propose an \\emph{adiabatic encoding} framework that encodes pre-trained MPS-classifiers into quantum MPS (qMPS) circuits with postselection, and gradually removes the postselection while retaining performance. We prove that training qMPS-classifiers from scratch on a certain artificial dataset is exponentially hard due to barren plateaus, but our adiabatic encoding circumvents this issue. Additional numerical experiments on binary MNIST also confirm its robustness.\n        △ Less",
        "authors":"['Keisuke Murota']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes Understanding",
        "abstract":"Understanding and reasoning over academic handwritten notes remains a challenge in document AI, particularly for mathematical equations, diagrams, and scientific notations. Existing visual question answering (VQA) benchmarks focus on printed or structured handwritten text, limiting generalization to real-world note-taking. To address this, we introduce NoTeS-Bank, an evaluation benchmark for Neural Transcription and Search in note-based question answering. NoTeS-Bank comprises complex notes across multiple domains, requiring models to process unstructured and multimodal content. The benchmark defines two tasks: (1) Evidence-Based VQA, where models retrieve localized answers with bounding-box evidence, and (2) Open-Domain VQA, where models classify the domain before retrieving relevant documents and answers. Unlike classical Document VQA datasets relying on optical character recognition (OCR) and structured data, NoTeS-BANK demands vision-language fusion, retrieval, and multimodal reasoning. We benchmark state-of-the-art Vision-Language Models (VLMs) and retrieval frameworks, exposing structured transcription and reasoning limitations. NoTeS-Bank provides a rigorous evaluation with NDCG@5, MRR, Recall@K, IoU, and ANLS, establishing a new standard for visual document understanding and reasoning.\n        △ Less",
        "authors":"['Aniket Pal', 'Sanket Biswas', 'Alloy Das', 'Ayush Lodh', 'Priyanka Banerjee', 'Soumitri Chattopadhyay', 'Dimosthenis Karatzas', 'Josep Llados', 'C. V. Jawahar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Type-Constrained Code Generation with Language Models",
        "abstract":"Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue, it has only been applied to handle either domain-specific languages or syntactic language features. This leaves typing errors, which are beyond the domain of syntax and generally hard to adequately constrain. To address this challenge, we introduce a type-constrained decoding approach that leverages type systems to guide code generation. We develop novel prefix automata for this purpose and introduce a sound approach to enforce well-typedness based on type inference and a search over inhabitable types. We formalize our approach on a simply-typed language and extend it to TypeScript to demonstrate practicality. Our evaluation on HumanEval shows that our approach reduces compilation errors by more than half and increases functional correctness in code synthesis, translation, and repair tasks across LLMs of various sizes and model families, including SOTA open-weight models with more than 30B parameters.\n        △ Less",
        "authors":"['Niels Mündler', 'Jingxuan He', 'Hao Wang', 'Koushik Sen', 'Dawn Song', 'Martin Vechev']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models",
        "abstract":"Improving the efficiency of inference in Large Language Models (LLMs) is a critical area of research. Post-training Quantization (PTQ) is a popular technique, but it often faces challenges at low-bit levels, particularly in downstream tasks. Quantization-aware Training (QAT) can alleviate this problem, but it requires significantly more computational resources. To tackle this, we introduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT), which merges the advantages of QAT while training only less than 1% of the total parameters. Specifically, we introduce a group-specific quantization magnitude to adjust the overall scale of each quantization group. Within each quantization group, we use LoRA matrices to update the weight size and direction in the quantization space. We validated the effectiveness of our method on the LLaMA and LLaMA2 model families. The results show significant improvements over our baseline method across different quantization granularities. For instance, for LLaMA-7B, our approach outperforms the previous state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model. Additionally, our quantization results on pre-trained models also surpass previous QAT methods, demonstrating the superior performance and efficiency of our approach.\n        △ Less",
        "authors":"['Wenjin Ke', 'Zhe Li', 'Dong Li', 'Lu Tian', 'Emad Barsoum']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CMCRD: Cross-Modal Contrastive Representation Distillation for Emotion Recognition",
        "abstract":"Emotion recognition is an important component of affective computing, and also human-machine interaction. Unimodal emotion recognition is convenient, but the accuracy may not be high enough; on the contrary, multi-modal emotion recognition may be more accurate, but it also increases the complexity and cost of the data collection system. This paper considers cross-modal emotion recognition, i.e., using both electroencephalography (EEG) and eye movement in training, but only EEG or eye movement in test. We propose cross-modal contrastive representation distillation (CMCRD), which uses a pre-trained eye movement classification model to assist the training of an EEG classification model, improving feature extraction from EEG signals, or vice versa. During test, only EEG signals (or eye movement signals) are acquired, eliminating the need for multi-modal data. CMCRD not only improves the emotion recognition accuracy, but also makes the system more simplified and practical. Experiments using three different neural network architectures on three multi-modal emotion recognition datasets demonstrated the effectiveness of CMCRD. Compared with the EEG-only model, it improved the average classification accuracy by about 6.2%.\n        △ Less",
        "authors":"['Siyuan Kan', 'Huanyu Wu', 'Zhenyao Cui', 'Fan Huang', 'Xiaolong Xu', 'Dongrui Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Spiking Neural Network for Intra-cortical Brain Signal Decoding",
        "abstract":"Decoding brain signals accurately and efficiently is crucial for intra-cortical brain-computer interfaces. Traditional decoding approaches based on neural activity vector features suffer from low accuracy, whereas deep learning based approaches have high computational cost. To improve both the decoding accuracy and efficiency, this paper proposes a spiking neural network (SNN) for effective and energy-efficient intra-cortical brain signal decoding. We also propose a feature fusion approach, which integrates the manually extracted neural activity vector features with those extracted by a deep neural network, to further improve the decoding accuracy. Experiments in decoding motor-related intra-cortical brain signals of two rhesus macaques demonstrated that our SNN model achieved higher accuracy than traditional artificial neural networks; more importantly, it was tens or hundreds of times more efficient. The SNN model is very suitable for high precision and low power applications like intra-cortical brain-computer interfaces.\n        △ Less",
        "authors":"['Song Yang', 'Haotian Fu', 'Herui Zhang', 'Peng Zhang', 'Wei Li', 'Dongrui Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accurate Diagnosis of Respiratory Viruses Using an Explainable Machine Learning with Mid-Infrared Biomolecular Fingerprinting of Nasopharyngeal Secretions",
        "abstract":"Accurate identification of respiratory viruses (RVs) is critical for outbreak control and public health. This study presents a diagnostic system that combines Attenuated Total Reflectance Fourier Transform Infrared Spectroscopy (ATR-FTIR) from nasopharyngeal secretions with an explainable Rotary Position Embedding-Sparse Attention Transformer (RoPE-SAT) model to accurately identify multiple RVs within 10 minutes. Spectral data (4000-00 cm-1) were collected, and the bio-fingerprint region (1800-900 cm-1) was employed for analysis. Standard normal variate (SNV) normalization and second-order derivation were applied to reduce scattering and baseline drift. Gradient-weighted class activation mapping (Grad-CAM) was employed to generate saliency maps, highlighting spectral regions most relevant to classification and enhancing the interpretability of model outputs. Two independent cohorts from Beijing Youan Hospital, processed with different viral transport media (VTMs) and drying methods, were evaluated, with one including influenza B, SARS-CoV-2, and healthy controls, and the other including mycoplasma, SARS-CoV-2, and healthy controls. The model achieved sensitivity and specificity above 94.40% across both cohorts. By correlating model-selected infrared regions with known biomolecular signatures, we verified that the system effectively recognizes virus-specific spectral fingerprints, including lipids, Amide I, Amide II, Amide III, nucleic acids, and carbohydrates, and leverages their weighted contributions for accurate classification.\n        △ Less",
        "authors":"['Wenwen Zhang', 'Zhouzhuo Tang', 'Yingmei Feng', 'Xia Yu', 'Qi Jie Wang', 'Zhiping Lin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training",
        "abstract":"Fairness has been a significant challenge in graph neural networks (GNNs) since degree biases often result in un-equal prediction performance among nodes with varying degrees. Existing GNN models focus on prediction accuracy, frequently overlooking fairness across different degree groups. To addressthis issue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric Contrastive Ensemble (FairACE), which inte-grates asymmetric contrastive learning with adversarial training to improve degree fairness. FairACE captures one-hop local neighborhood information and two-hop monophily similarity to create fairer node representations and employs a degree fairness regulator to balance performance between high-degree and low-degree nodes. During model training, a novel group-balanced fairness loss is proposed to minimize classification disparities across degree groups. In addition, we also propose a novel fairness metric, the Accuracy Distribution Gap (ADG), which can quantitatively assess and ensure equitable performance across different degree-based node groups. Experimental results on both synthetic and real-world datasets demonstrate that FairACE significantly improves degree fairness metrics while maintaining competitive accuracy in comparison to the state-of-the-art GNN models.\n        △ Less",
        "authors":"['Jiaxin Liu', 'Xiaoqian Jiang', 'Xiang Li', 'Bohan Zhang', 'Jing Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rethinking Remaining Useful Life Prediction with Scarce Time Series Data: Regression under Indirect Supervision",
        "abstract":"Supervised time series prediction relies on directly measured target variables, but real-world use cases such as predicting remaining useful life (RUL) involve indirect supervision, where the target variable is labeled as a function of another dependent variable. Trending temporal regression techniques rely on sequential time series inputs to capture temporal patterns, requiring interpolation when dealing with sparsely and irregularly sampled covariates along the timeline. However, interpolation can introduce significant biases, particularly with highly scarce data. In this paper, we address the RUL prediction problem with data scarcity as time series regression under indirect supervision. We introduce a unified framework called parameterized static regression, which takes single data points as inputs for regression of target values, inherently handling data scarcity without requiring interpolation. The time dependency under indirect supervision is captured via a parametrical rectification (PR) process, approximating a parametric function during inference with historical posteriori estimates, following the same underlying distribution used for labeling during training. Additionally, we propose a novel batch training technique for tasks in indirect supervision to prevent overfitting and enhance efficiency. We evaluate our model on public benchmarks for RUL prediction with simulated data scarcity. Our method demonstrates competitive performance in prediction accuracy when dealing with highly scarce time series data.\n        △ Less",
        "authors":"['Jiaxiang Cheng', 'Yipeng Pang', 'Guoqiang Hu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Query-based Knowledge Transfer for Heterogeneous Learning Environments",
        "abstract":"Decentralized collaborative learning under data heterogeneity and privacy constraints has rapidly advanced. However, existing solutions like federated learning, ensembles, and transfer learning, often fail to adequately serve the unique needs of clients, especially when local data representation is limited. To address this issue, we propose a novel framework called Query-based Knowledge Transfer (QKT) that enables tailored knowledge acquisition to fulfill specific client needs without direct data exchange. QKT employs a data-free masking strategy to facilitate communication-efficient query-focused knowledge transfer while refining task-specific parameters to mitigate knowledge interference and forgetting. Our experiments, conducted on both standard and clinical benchmarks, show that QKT significantly outperforms existing collaborative learning methods by an average of 20.91\\% points in single-class query settings and an average of 14.32\\% points in multi-class query scenarios. Further analysis and ablation studies reveal that QKT effectively balances the learning of new and existing knowledge, showing strong potential for its application in decentralized learning.\n        △ Less",
        "authors":"['Norah Alballa', 'Wenxuan Zhang', 'Ziquan Liu', 'Ahmed M. Abdelmoniem', 'Mohamed Elhoseiny', 'Marco Canini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning",
        "abstract":"The primary goal of my Ph.D. study is to develop provably efficient and practical algorithms for data-driven online sequential decision-making under uncertainty. My work focuses on reinforcement learning (RL), multi-armed bandits, and their applications, including recommendation systems, computer networks, video analytics, and large language models (LLMs). Online learning methods, such as bandits and RL, have demonstrated remarkable success - ranging from outperforming human players in complex games like Atari and Go to advancing robotics, recommendation systems, and fine-tuning LLMs. Despite these successes, many established algorithms rely on idealized models that can fail under model misspecifications or adversarial perturbations, particularly in settings where accurate prior knowledge of the underlying model class is unavailable or where malicious users operate within dynamic systems. These challenges are pervasive in real-world applications, where robust and adaptive solutions are critical. Furthermore, while worst-case guarantees provide theoretical reliability, they often fail to capture instance-dependent performance, which can lead to more efficient and practical solutions. Another key challenge lies in generalizing to new, unseen environments, a crucial requirement for deploying these methods in dynamic and unpredictable settings. To address these limitations, my research aims to develop more efficient, robust, instance-adaptive, and generalizable online learning algorithms for both reinforcement learning and bandits. Towards this end, I focus on developing more efficient, robust, instance-adaptive, and generalizable for both general reinforcement learning (RL) and bandits.\n        △ Less",
        "authors":"['Zhiyong Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RSLAQ -- A Robust SLA-driven 6G O-RAN QoS xApp using deep reinforcement learning",
        "abstract":"The evolution of 6G envisions a wide range of applications and services characterized by highly differentiated and stringent Quality of Service (QoS) requirements. Open Radio Access Network (O-RAN) technology has emerged as a transformative approach that enables intelligent software-defined management of the RAN. A cornerstone of O-RAN is the RAN Intelligent Controller (RIC), which facilitates the deployment of intelligent applications (xApps and rApps) near the radio unit. In this context, QoS management through O-RAN has been explored using network slice and machine learning (ML) techniques. Although prior studies have demonstrated the ability to optimize RAN resource allocation and prioritize slices effectively, they have not considered the critical integration of Service Level Agreements (SLAs) into the ML learning process. This omission can lead to suboptimal resource utilization and, in many cases, service outages when target Key Performance Indicators (KPIs) are not met. This work introduces RSLAQ, an innovative xApp designed to ensure robust QoS management for RAN slicing while incorporating SLAs directly into its operational framework. RSLAQ translates operator policies into actionable configurations, guiding resource distribution and scheduling for RAN slices. Using deep reinforcement learning (DRL), RSLAQ dynamically monitors RAN performance metrics and computes optimal actions, embedding SLA constraints to mitigate conflicts and prevent outages. Extensive system-level simulations validate the efficacy of the proposed solution, demonstrating its ability to optimize resource allocation, improve SLA adherence, and maintain operational reliability (>95%) in challenging scenarios.\n        △ Less",
        "authors":"['Noe M. Yungaicela-Naula', 'Vishal Sharma', 'Sandra Scott-Hayward']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction",
        "abstract":"Long sequence prediction is a key challenge in time series forecasting. While Mamba-based models have shown strong performance due to their sequence selection capabilities, they still struggle with insufficient focus on critical time steps and incomplete noise suppression, caused by limited selective abilities. To address this, we introduce Repetitive Contrastive Learning (RCL), a token-level contrastive pretraining framework aimed at enhancing Mamba's selective capabilities. RCL pretrains a single Mamba block to strengthen its selective abilities and then transfers these pretrained parameters to initialize Mamba blocks in various backbone models, improving their temporal prediction performance. RCL uses sequence augmentation with Gaussian noise and applies inter-sequence and intra-sequence contrastive learning to help the Mamba module prioritize information-rich time steps while ignoring noisy ones. Extensive experiments show that RCL consistently boosts the performance of backbone models, surpassing existing methods and achieving state-of-the-art results. Additionally, we propose two metrics to quantify Mamba's selective capabilities, providing theoretical, qualitative, and quantitative evidence for the improvements brought by RCL.\n        △ Less",
        "authors":"['Wenbo Yan', 'Hanzhong Cao', 'Ying Tan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification",
        "abstract":"In open data sets of functional magnetic resonance imaging (fMRI), the heterogeneity of the data is typically attributed to a combination of factors, including differences in scanning procedures, the presence of confounding effects, and population diversities between multiple sites. These factors contribute to the diminished effectiveness of representation learning, which in turn affects the overall efficacy of subsequent classification procedures. To address these limitations, we propose a novel multi-site adversarial learning network (MSalNET) for fMRI-based mental disorder detection. Firstly, a representation learning module is introduced with a node information assembly (NIA) mechanism to better extract features from functional connectivity (FC). This mechanism aggregates edge information from both horizontal and vertical directions, effectively assembling node information. Secondly, to generalize the feature across sites, we proposed a site-level feature extraction module that can learn from individual FC data, which circumvents additional prior information. Lastly, an adversarial learning network is proposed as a means of balancing the trade-off between individual classification and site regression tasks, with the introduction of a novel loss function. The proposed method was evaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data Exchange (ABIDE) and ADHD-200. The results indicate that the proposed method achieves a better performance than other related algorithms with the accuracy of 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore, the result of the site regression indicates that the proposed method reduces site variability from a data-driven perspective. The most discriminative brain regions revealed by NIA are consistent with statistical findings, uncovering the \"black box\" of deep learning to a certain extent.\n        △ Less",
        "authors":"['Xin Wen', 'Shijie Guo', 'Wenbo Ning', 'Rui Cao', 'Yan Niu', 'Bin Wan', 'Peng Wei', 'Xiaobo Liu', 'Jie Xiang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deconfounded Reasoning for Multimodal Fake News Detection via Causal Intervention",
        "abstract":"The rapid growth of social media has led to the widespread dissemination of fake news across multiple content forms, including text, images, audio, and video. Traditional unimodal detection methods fall short in addressing complex cross-modal manipulations; as a result, multimodal fake news detection has emerged as a more effective solution. However, existing multimodal approaches, especially in the context of fake news detection on social media, often overlook the confounders hidden within complex cross-modal interactions, leading models to rely on spurious statistical correlations rather than genuine causal mechanisms. In this paper, we propose the Causal Intervention-based Multimodal Deconfounded Detection (CIMDD) framework, which systematically models three types of confounders via a unified Structural Causal Model (SCM): (1) Lexical Semantic Confounder (LSC); (2) Latent Visual Confounder (LVC); (3) Dynamic Cross-Modal Coupling Confounder (DCCC). To mitigate the influence of these confounders, we specifically design three causal modules based on backdoor adjustment, frontdoor adjustment, and cross-modal joint intervention to block spurious correlations from different perspectives and achieve causal disentanglement of representations for deconfounded reasoning. Experimental results on the FakeSV and FVC datasets demonstrate that CIMDD significantly improves detection accuracy, outperforming state-of-the-art methods by 4.27% and 4.80%, respectively. Furthermore, extensive experimental results indicate that CIMDD exhibits strong generalization and robustness across diverse multimodal scenarios.\n        △ Less",
        "authors":"['Moyang Liu', 'Kaiying Yan', 'Yukun Liu', 'Ruibo Fu', 'Zhengqi Wen', 'Xuefei Liu', 'Chenxing Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dose-finding design based on level set estimation in phase I cancer clinical trials",
        "abstract":"The primary objective of phase I cancer clinical trials is to evaluate the safety of a new experimental treatment and to find the maximum tolerated dose (MTD). We show that the MTD estimation problem can be regarded as a level set estimation (LSE) problem whose objective is to determine the regions where an unknown function value is above or below a given threshold. Then, we propose a novel dose-finding design in the framework of LSE. The proposed design determines the next dose on the basis of an acquisition function incorporating uncertainty in the posterior distribution of the dose-toxicity curve as well as overdose control. Simulation experiments show that the proposed LSE design achieves a higher accuracy in estimating the MTD and involves a lower risk of overdosing allocation compared to existing designs, thereby indicating that it provides an effective methodology for phase I cancer clinical trial design.\n        △ Less",
        "authors":"['Keiichiro Seno', 'Kota Matsui', 'Shogo Iwazaki', 'Yu Inatsu', 'Shion Takeno', 'Shigeyuki Matsui']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring Modality Disruption in Multimodal Fake News Detection",
        "abstract":"The rapid growth of social media has led to the widespread dissemination of fake news across multiple content forms, including text, images, audio, and video. Compared to unimodal fake news detection, multimodal fake news detection benefits from the increased availability of information across multiple modalities. However, in the context of social media, certain modalities in multimodal fake news detection tasks may contain disruptive or over-expressive information. These elements often include exaggerated or embellished content. We define this phenomenon as modality disruption and explore its impact on detection models through experiments. To address the issue of modality disruption in a targeted manner, we propose a multimodal fake news detection framework, FND-MoE. Additionally, we design a two-pass feature selection mechanism to further mitigate the impact of modality disruption. Extensive experiments on the FakeSV and FVC-2018 datasets demonstrate that FND-MoE significantly outperforms state-of-the-art methods, with accuracy improvements of 3.45% and 3.71% on the respective datasets compared to baseline models.\n        △ Less",
        "authors":"['Moyang Liu', 'Kaiying Yan', 'Yukun Liu', 'Ruibo Fu', 'Zhengqi Wen', 'Xuefei Liu', 'Chenxing Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MatWheel: Addressing Data Scarcity in Materials Science Through Synthetic Data",
        "abstract":"Data scarcity and the high cost of annotation have long been persistent challenges in the field of materials science. Inspired by its potential in other fields like computer vision, we propose the MatWheel framework, which train the material property prediction model using the synthetic data generated by the conditional generative model. We explore two scenarios: fully-supervised and semi-supervised learning. Using CGCNN for property prediction and Con-CDVAE as the conditional generative model, experiments on two data-scarce material property datasets from Matminer database are conducted. Results show that synthetic data has potential in extreme data-scarce scenarios, achieving performance close to or exceeding that of real samples in all two tasks. We also find that pseudo-labels have little impact on generated data quality. Future work will integrate advanced models and optimize generation conditions to boost the effectiveness of the materials data flywheel.\n        △ Less",
        "authors":"['Wentao Li', 'Yizhe Chen', 'Jiangjie Qiu', 'Xiaonan Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Kernel-Based Enhanced Oversampling Method for Imbalanced Classification",
        "abstract":"This paper introduces a novel oversampling technique designed to improve classification performance on imbalanced datasets. The proposed method enhances the traditional SMOTE algorithm by incorporating convex combination and kernel-based weighting to generate synthetic samples that better represent the minority class. Through experiments on multiple real-world datasets, we demonstrate that the new technique outperforms existing methods in terms of F1-score, G-mean, and AUC, providing a robust solution for handling imbalanced datasets in classification tasks.\n        △ Less",
        "authors":"['Wenjie Li', 'Sibo Zhu', 'Zhijian Li', 'Hanlin Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Self-Supervised Autoencoder Network for Robust Heart Rate Extraction from Noisy Photoplethysmogram: Applying Blind Source Separation to Biosignal Analysis",
        "abstract":"Biosignals can be viewed as mixtures measuring particular physiological events, and blind source separation (BSS) aims to extract underlying source signals from mixtures. This paper proposes a self-supervised multi-encoder autoencoder (MEAE) to separate heartbeat-related source signals from photoplethysmogram (PPG), enhancing heart rate (HR) detection in noisy PPG data. The MEAE is trained on PPG signals from a large open polysomnography database without any pre-processing or data selection. The trained network is then applied to a noisy PPG dataset collected during the daily activities of nine subjects. The extracted heartbeat-related source signal significantly improves HR detection as compared to the original PPG. The absence of pre-processing and the self-supervised nature of the proposed method, combined with its strong performance, highlight the potential of BSS in biosignal analysis.\n        △ Less",
        "authors":"['Matthew B. Webster', 'Dongheon Lee', 'Joonnyong Lee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deploying Large AI Models on Resource-Limited Devices with Split Federated Learning",
        "abstract":"Large Artificial Intelligence Models (LAMs) powered by massive datasets, extensive parameter scales, and extensive computational resources, leading to significant transformations across various industries. Yet, their practical deployment on resource-limited mobile edge devices is hindered by critical challenges such as data privacy, constrained resources, and high overhead costs. Addressing this gap, this paper proposes a novel framework, named Quantized Split Federated Fine-Tuning Large AI Model (SFLAM). By partitioning the training load between edge devices and servers using a split learning paradigm, SFLAM can facilitate the operation of large models on devices and significantly lowers the memory requirements on edge devices. Additionally, SFLAM incorporates quantization management, power control, and bandwidth allocation strategies to enhance training efficiency while concurrently reducing energy consumption and communication latency. A theoretical analysis exploring the latency-energy trade-off is presented, and the framework's efficacy is validated via comprehensive simulations. The findings indicate that SFLAM achieves superior performance in terms of learning efficiency and scalability compared to conventional methods, thereby providing a valuable approach for enabling advanced AI services in resource-constrained scenarios.\n        △ Less",
        "authors":"['Xianke Qiang', 'Hongda Liu', 'Xinran Zhang', 'Zheng Chang', 'Ying-Chang Liang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Shrinkage Initialization for Smooth Learning of Neural Networks",
        "abstract":"The successes of intelligent systems have quite relied on the artificial learning of information, which lead to the broad applications of neural learning solutions. As a common sense, the training of neural networks can be largely improved by specifically defined initialization, neuron layers as well as the activation functions. Though there are sequential layer based initialization available, the generalized solution to initial stages is still desired. In this work, an improved approach to initialization of neural learning is presented, which adopts the shrinkage approach to initialize the transformation of each layer of networks. It can be universally adapted for the structures of any networks with random layers, while stable performance can be attained. Furthermore, the smooth learning of networks is adopted in this work, due to the diverse influence on neural learning. Experimental results on several artificial data sets demonstrate that, the proposed method is able to present robust results with the shrinkage initialization, and competent for smooth learning of neural networks.\n        △ Less",
        "authors":"['Miao Cheng', 'Feiyan Zhou', 'Hongwei Zou', 'Limin Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE",
        "abstract":"In modern air traffic management, generating synthetic flight trajectories has emerged as a promising solution for addressing data scarcity, protecting sensitive information, and supporting large-scale analyses. In this paper, we propose a novel method for trajectory synthesis by adapting the Time-Based Vector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages time-frequency domain processing, vector quantization, and transformer-based priors to capture both global and local dynamics in flight data. By discretizing the latent space and integrating transformer priors, the model learns long-range spatiotemporal dependencies and preserves coherence across entire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite of quality, statistical, and distributional metrics, as well as a flyability assessment conducted in an open-source air traffic simulator. Results indicate that TimeVQVAE outperforms a temporal convolutional VAE baseline, generating synthetic trajectories that mirror real flight data in terms of spatial accuracy, temporal consistency, and statistical properties. Furthermore, the simulator-based assessment shows that most generated trajectories maintain operational feasibility, although occasional outliers underscore the potential need for additional domain-specific constraints. Overall, our findings underscore the importance of multi-scale representation learning for capturing complex flight behaviors and demonstrate the promise of TimeVQVAE in producing representative synthetic trajectories for downstream tasks such as model training, airspace design, and air traffic forecasting.\n        △ Less",
        "authors":"['Abdulmajid Murad', 'Massimiliano Ruocco']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"High dimensional online calibration in polynomial time",
        "abstract":"In online (sequential) calibration, a forecaster predicts probability distributions over a finite outcome space $[d]$ over a sequence of $T$ days, with the goal of being calibrated. While asymptotically calibrated strategies are known to exist, they suffer from the curse of dimensionality: the best known algorithms require $\\exp(d)$ days to achieve non-trivial calibration.\n  In this work, we present the first asymptotically calibrated strategy that guarantees non-trivial calibration after a polynomial number of rounds. Specifically, for any desired accuracy $ε> 0$, our forecaster becomes $ε$-calibrated after $T = d^{O(1\/ε^2)}$ days. We complement this result with a lower bound, proving that at least $T = d^{Ω(\\log(1\/ε))}$ rounds are necessary to achieve $ε$-calibration. Our results resolve the open questions posed by [Abernethy-Mannor'11, Hazan-Kakade'12].\n  Our algorithm is inspired by recent breakthroughs in swap regret minimization [Peng-Rubinstein'24, Dagan et al.'24]. Despite its strong theoretical guarantees, the approach is remarkably simple and intuitive: it randomly selects among a set of sub-forecasters, each of which predicts the empirical outcome frequency over recent time windows.\n        △ Less",
        "authors":"['Binghui Peng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"crowd-hpo: Realistic Hyperparameter Optimization and Benchmarking for Learning from Crowds with Noisy Labels",
        "abstract":"Crowdworking is a cost-efficient solution to acquire class labels. Since these labels are subject to noise, various approaches to learning from crowds have been proposed. Typically, these approaches are evaluated with default hyperparameters, resulting in suboptimal performance, or with hyperparameters tuned using a validation set with ground truth class labels, representing an often unrealistic scenario. Moreover, both experimental setups can produce different rankings of approaches, complicating comparisons between studies. Therefore, we introduce crowd-hpo as a realistic benchmark and experimentation protocol including hyperparameter optimization under noisy crowd-labeled data. At its core, crowd-hpo investigates model selection criteria to identify well-performing hyperparameter configurations only with access to noisy crowd-labeled validation data. Extensive experimental evaluations with neural networks show that these criteria are effective for optimizing hyperparameters in learning from crowds approaches. Accordingly, incorporating such criteria into experimentation protocols is essential for enabling more realistic and fair benchmarking.\n        △ Less",
        "authors":"['Marek Herde', 'Lukas Lührs', 'Denis Huseljic', 'Bernhard Sick']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Case for Kolmogorov-Arnold Networks in Prefetching: Towards Low-Latency, Generalizable ML-Based Prefetchers",
        "abstract":"The memory wall problem arises due to the disparity between fast processors and slower memory, causing significant delays in data access, even more so on edge devices. Data prefetching is a key strategy to address this, with traditional methods evolving to incorporate Machine Learning (ML) for improved accuracy. Modern prefetchers must balance high accuracy with low latency to further practicality. We explore the applicability of utilizing Kolmogorov-Arnold Networks (KAN) with learnable activation functions,a prefetcher we implemented called KANBoost, to further this aim. KANs are a novel, state-of-the-art model that work on breaking down continuous, bounded multi-variate functions into functions of their constituent variables, and use these constitutent functions as activations on each individual neuron. KANBoost predicts the next memory access by modeling deltas between consecutive addresses, offering a balance of accuracy and efficiency to mitigate the memory wall problem with minimal overhead, instead of relying on address-correlation prefetching. Initial results indicate that KAN-based prefetching reduces inference latency (18X lower than state-of-the-art ML prefetchers) while achieving moderate IPC improvements (2.5\\% over no-prefetching). While KANs still face challenges in capturing long-term dependencies, we propose that future research should explore hybrid models that combine KAN efficiency with stronger sequence modeling techniques, paving the way for practical ML-based prefetching in edge devices and beyond.\n        △ Less",
        "authors":"['Dhruv Kulkarni', 'Bharat Bhammar', 'Henil Thaker', 'Pranav Dhobi', 'R. P. Gohil', 'Sai Manoj Pudukotai Dinkarrao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation",
        "abstract":"We offer a novel approach, MGS (Markov Greedy Sums), to improve the accuracy of low-bitwidth floating-point dot products in neural network computations. In conventional 32-bit floating-point summation, adding values with different exponents may lead to loss of precision in the mantissa of the smaller term, which is right-shifted to align with the larger term's exponent. Such shifting (a.k.a. 'swamping') is a significant source of numerical errors in accumulation when implementing low-bitwidth dot products (e.g., 8-bit floating point) as the mantissa has a small number of bits. We avoid most swamping errors by arranging the terms in dot product summation based on their exponents and summing the mantissas without overflowing the low-bitwidth accumulator. We design, analyze, and implement the algorithm to minimize 8-bit floating point error at inference time for several neural networks. In contrast to traditional sequential summation, our method has significantly lowered numerical errors, achieving classification accuracy on par with high-precision floating-point baselines for multiple image classification tasks. Our dMAC hardware units can reduce power consumption by up to 34.1\\% relative to conventional MAC units.\n        △ Less",
        "authors":"['Vikas Natesh', 'H. T. Kung', 'David Kong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Incremental Non-Linear Manifold Approximation Method",
        "abstract":"Analyzing high-dimensional data presents challenges due to the \"curse of dimensionality'', making computations intensive. Dimension reduction techniques, categorized as linear or non-linear, simplify such data. Non-linear methods are particularly essential for efficiently visualizing and processing complex data structures in interactive and graphical applications. This research develops an incremental non-linear dimension reduction method using the Geometric Multi-Resolution Analysis (GMRA) framework for streaming data. The proposed method enables real-time data analysis and visualization by incrementally updating the cluster map, PCA basis vectors, and wavelet coefficients. Numerical experiments show that the incremental GMRA accurately represents non-linear manifolds even with small initial samples and aligns closely with batch GMRA, demonstrating efficient updates and maintaining the multiscale structure. The findings highlight the potential of Incremental GMRA for real-time visualization and interactive graphics applications that require adaptive high-dimensional data representations.\n        △ Less",
        "authors":"['Praveen T. W. Hettige', 'Benjamin W. Ong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products in Neural Network Computations",
        "abstract":"We present PQS, which uses three techniques together - Prune, Quantize, and Sort - to achieve low-bitwidth accumulation of dot products in neural network computations. In conventional quantized (e.g., 8-bit) dot products, partial results are accumulated into wide (e.g., 32-bit) accumulators to avoid overflows when accumulating intermediate partial sums. However, such wide accumulators increase memory bandwidth usage and reduce energy efficiency. We show that iterative N:M pruning in floating point followed by quantization to 8 (or fewer) bits, and accumulation of partial products in a sorted order (\"small to large\") allows for accurate, compressed models with short dot product lengths that do not require wide accumulators. We design, analyze, and implement the PQS algorithm to eliminate accumulation overflows at inference time for several neural networks. Our method offers a 2.5x reduction in accumulator bitwidth while achieving model accuracy on par with floating-point baselines for multiple image classification tasks.\n        △ Less",
        "authors":"['Vikas Natesh', 'H. T. Kung']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Practical Approach to using Supervised Machine Learning Models to Classify Aviation Safety Occurrences",
        "abstract":"This paper describes a practical approach of using supervised machine learning (ML) models to assist safety investigators to classify aviation occurrences into either incident or serious incident categories. Our implementation currently deployed as a ML web application is trained on a labelled dataset derived from publicly available aviation investigation reports. A selection of five supervised learning models (Support Vector Machine, Logistic Regression, Random Forest Classifier, XGBoost and K-Nearest Neighbors) were evaluated. This paper showed the best performing ML algorithm was the Random Forest Classifier with accuracy = 0.77, F1 Score = 0.78 and MCC = 0.51 (average of 100 sample runs). The study had also explored the effect of applying Synthetic Minority Over-sampling Technique (SMOTE) to the imbalanced dataset, and the overall observation ranged from no significant effect to substantial degradation in performance for some of the models after the SMOTE adjustment.\n        △ Less",
        "authors":"['Bryan Y. Siow']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multimodal 3D Genome Pre-training",
        "abstract":"Deep learning techniques have driven significant progress in various analytical tasks within 3D genomics in computational biology. However, a holistic understanding of 3D genomics knowledge remains underexplored. Here, we propose MIX-HIC, the first multimodal foundation model of 3D genome that integrates both 3D genome structure and epigenomic tracks, which obtains unified and comprehensive semantics. For accurate heterogeneous semantic fusion, we design the cross-modal interaction and mapping blocks for robust unified representation, yielding the accurate aggregation of 3D genome knowledge. Besides, we introduce the first large-scale dataset comprising over 1 million pairwise samples of Hi-C contact maps and epigenomic tracks for high-quality pre-training, enabling the exploration of functional implications in 3D genomics. Extensive experiments show that MIX-HIC can significantly surpass existing state-of-the-art methods in diverse downstream tasks. This work provides a valuable resource for advancing 3D genomics research.\n        △ Less",
        "authors":"['Minghao Yang', 'Pengteng Li', 'Yan Liang', 'Qianyi Cai', 'Zhihang Zheng', 'Shichen Zhang', 'Pengfei Zhang', 'Zhi-An Huang', 'Hui Xiong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization",
        "abstract":"Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose \\textbf{Dynamic Mask coupled with Concept-Aware Loss}, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask} mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. The code will be released publicly.\n        △ Less",
        "authors":"['Gen Li', 'Yang Xiao', 'Jie Ji', 'Kaiyuan Deng', 'Bo Hui', 'Linke Guo', 'Xiaolong Ma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InterQ: A DQN Framework for Optimal Intermittent Control",
        "abstract":"In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the system's state but transmits it to the controller intermittently to balance the communication cost and control performance. The controller, in turn, determines the control input based on the intermittently received information. Given the partially nested information structure, we show that the optimal control policy follows a certainty-equivalence form. Subsequently, we analyze the qualitative behavior of the scheduling policy. To develop the optimal scheduling policy, we propose InterQ, a deep reinforcement learning algorithm which uses a deep neural network to approximate the Q-function. Through extensive numerical evaluations, we analyze the scheduling landscape and further compare our approach against two baseline strategies: (a) a multi-period periodic scheduling policy, and (b) an event-triggered policy. The results demonstrate that our proposed method outperforms both baselines. The open source implementation can be found at https:\/\/github.com\/AC-sh\/InterQ.\n        △ Less",
        "authors":"['Shubham Aggarwal', 'Dipankar Maity', 'Tamer Başar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Hierarchical Decomposition of Kullback-Leibler Divergence: Disentangling Marginal Mismatches from Statistical Dependencies",
        "abstract":"The Kullback-Leibler (KL) divergence is a foundational measure for comparing probability distributions. Yet in multivariate settings, its single value often obscures the underlying reasons for divergence, conflating mismatches in individual variable distributions (marginals) with effects arising from statistical dependencies. We derive an algebraically exact, additive, and hierarchical decomposition of the KL divergence between a joint distribution P(X1,...,Xn) and a standard product reference distribution Q(X1,...,Xn) = product_i q(Xi), where variables are assumed independent and identically distributed according to a common reference q. The total divergence precisely splits into two primary components: (1) the summed divergence of each marginal distribution Pi(Xi) from the common reference q(Xi), quantifying marginal deviations; and (2) the total correlation (or multi-information), capturing the total statistical dependency among variables. Leveraging Mobius inversion on the subset lattice, we further decompose this total correlation term into a hierarchy of signed contributions from distinct pairwise, triplet, and higher-order statistical interactions, expressed using standard Shannon information quantities. This decomposition provides an algebraically complete and interpretable breakdown of KL divergence using established information measures, requiring no approximations or model assumptions. Numerical validation using hypergeometric sampling confirms the decomposition's exactness to machine precision across diverse system configurations. This framework enables a precise diagnosis of divergence origins--distinguishing marginal effects from interaction effects--with potential applications across machine learning, econometrics, and complex systems analysis.\n        △ Less",
        "authors":"['William Cook']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards On-Device Learning and Reconfigurable Hardware Implementation for Encoded Single-Photon Signal Processing",
        "abstract":"Deep neural networks (DNNs) enhance the accuracy and efficiency of reconstructing key parameters from time-resolved photon arrival signals recorded by single-photon detectors. However, the performance of conventional backpropagation-based DNNs is highly dependent on various parameters of the optical setup and biological samples under examination, necessitating frequent network retraining, either through transfer learning or from scratch. Newly collected data must also be stored and transferred to a high-performance GPU server for retraining, introducing latency and storage overhead. To address these challenges, we propose an online training algorithm based on a One-Sided Jacobi rotation-based Online Sequential Extreme Learning Machine (OSOS-ELM). We fully exploit parallelism in executing OSOS-ELM on a heterogeneous FPGA with integrated ARM cores. Extensive evaluations of OSOS-ELM and OSELM demonstrate that both achieve comparable accuracy across different network dimensions (i.e., input, hidden, and output layers), while OSOS-ELM proves to be more hardware-efficient. By leveraging the parallelism of OSOS-ELM, we implement a holistic computing prototype on a Xilinx ZCU104 FPGA, which integrates a multi-core CPU and programmable logic fabric. We validate our approach through three case studies involving single-photon signal analysis: sensing through fog using commercial single-photon LiDAR, fluorescence lifetime estimation in FLIM, and blood flow index reconstruction in DCS, all utilizing one-dimensional data encoded from photonic signals. From a hardware perspective, we optimize the OSOS-ELM workload by employing multi-tasked processing on ARM CPU cores and pipelined execution on the FPGA's logic fabric. We also implement our OSOS-ELM on the NVIDIA Jetson Xavier NX GPU to comprehensively investigate its computing performance on another type of heterogeneous computing platform.\n        △ Less",
        "authors":"['Zhenya Zang', 'Xingda Li', 'David Day Uei Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Associating transportation planning-related measures with Mild Cognitive Impairment",
        "abstract":"Understanding the relationship between mild cognitive impairment and driving behavior is essential to improve road safety, especially among older adults. In this study, we computed certain variables that reflect daily driving habits, such as trips to specific locations (e.g., home, work, medical, social, and errands) of older drivers in Nebraska using geohashing. The computed variables were then analyzed using a two-fold approach involving data visualization and machine learning models (C5.0, Random Forest, Support Vector Machines) to investigate the efficiency of the computed variables in predicting whether a driver is cognitively impaired or unimpaired. The C5.0 model demonstrated robust and stable performance with a median recall of 74\\%, indicating that our methodology was able to identify cognitive impairment in drivers 74\\% of the time correctly. This highlights our model's effectiveness in minimizing false negatives which is an important consideration given the cost of missing impaired drivers could be potentially high. Our findings highlight the potential of life space variables in understanding and predicting cognitive decline, offering avenues for early intervention and tailored support for affected individuals.\n        △ Less",
        "authors":"['Souradeep Chattopadhyay', 'Guillermo Basulto-Elias', 'Jun Ha Chang', 'Matthew Rizzo', 'Shauna Hallmark', 'Anuj Sharma', 'Soumik Sarkar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detecting Instruction Fine-tuning Attack on Language Models with Influence Function",
        "abstract":"Instruction fine-tuning attacks pose a significant threat to large language models (LLMs) by subtly embedding poisoned data in fine-tuning datasets, which can trigger harmful or unintended responses across a range of tasks. This undermines model alignment and poses security risks in real-world deployment. In this work, we present a simple and effective approach to detect and mitigate such attacks using influence functions, a classical statistical tool adapted for machine learning interpretation. Traditionally, the high computational costs of influence functions have limited their application to large models and datasets. The recent Eigenvalue-Corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation method enables efficient influence score computation, making it feasible for large-scale analysis.\n  We are the first to apply influence functions for detecting language model instruction fine-tuning attacks on large-scale datasets, as both the instruction fine-tuning attack on language models and the influence calculation approximation technique are relatively new. Our large-scale empirical evaluation of influence functions on 50,000 fine-tuning examples and 32 tasks reveals a strong association between influence scores and sentiment. Building on this, we introduce a novel sentiment transformation combined with influence functions to detect and remove critical poisons -- poisoned data points that skew model predictions. Removing these poisons (only 1% of total data) recovers model performance to near-clean levels, demonstrating the effectiveness and efficiency of our approach. Artifact is available at https:\/\/github.com\/lijiawei20161002\/Poison-Detection.\n  WARNING: This paper contains offensive data examples.\n        △ Less",
        "authors":"['Jiawei Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Champion-level Vision-based Reinforcement Learning Agent for Competitive Racing in Gran Turismo 7",
        "abstract":"Deep reinforcement learning has achieved superhuman racing performance in high-fidelity simulators like Gran Turismo 7 (GT7). It typically utilizes global features that require instrumentation external to a car, such as precise localization of agents and opponents, limiting real-world applicability. To address this limitation, we introduce a vision-based autonomous racing agent that relies solely on ego-centric camera views and onboard sensor data, eliminating the need for precise localization during inference. This agent employs an asymmetric actor-critic framework: the actor uses a recurrent neural network with the sensor data local to the car to retain track layouts and opponent positions, while the critic accesses the global features during training. Evaluated in GT7, our agent consistently outperforms GT7's built-drivers. To our knowledge, this work presents the first vision-based autonomous racing agent to demonstrate champion-level performance in competitive racing scenarios.\n        △ Less",
        "authors":"['Hojoon Lee', 'Takuma Seno', 'Jun Jet Tai', 'Kaushik Subramanian', 'Kenta Kawamoto', 'Peter Stone', 'Peter R. Wurman']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning in Structured Stackelberg Games",
        "abstract":"We study structured Stackelberg games, in which both players (the leader and the follower) observe information about the state of the world at time of play. Importantly, this information may contain information about the follower, which the leader may use when deciding her strategy. Under this setting, we show that no-regret learning is possible if and only if the set of mappings from contexts to follower types that the leader uses to learn is not ``too complex''. Specifically, we find that standard learning theoretic measures of complexity do not characterize learnability in our setting and we give a new dimension which does, which we term the Stackelberg-Littlestone dimension. In the distributional setting, we give analogous results by showing that standard complexity measures do not characterize the sample complexity of learning, but a new dimension called the Stackelberg-Natarajan dimension does. We then show that an appropriate empirical risk minimization procedure achieves the corresponding sample complexity.\n        △ Less",
        "authors":"['Maria-Florina Balcan', 'Kiriaki Fragkia', 'Keegan Harris']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ReCA: A Parametric ReLU Composite Activation Function",
        "abstract":"Activation functions have been shown to affect the performance of deep neural networks significantly. While the Rectified Linear Unit (ReLU) remains the dominant choice in practice, the optimal activation function for deep neural networks remains an open research question. In this paper, we propose a novel parametric activation function, ReCA, based on ReLU, which has been shown to outperform all baselines on state-of-the-art datasets using different complex neural network architectures.\n        △ Less",
        "authors":"['John Chidiac', 'Danielle Azar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RouterKT: Mixture-of-Experts for Knowledge Tracing",
        "abstract":"Knowledge Tracing (KT) is a fundamental task in Intelligent Tutoring Systems (ITS), which aims to model the dynamic knowledge states of students based on their interaction histories. However, existing KT models often rely on a global forgetting decay mechanism for capturing learning patterns, assuming that students' performance is predominantly influenced by their most recent interactions. Such approaches fail to account for the diverse and complex learning patterns arising from individual differences and varying learning stages. To address this limitation, we propose RouterKT, a novel Mixture-of-Experts (MoE) architecture designed to capture heterogeneous learning patterns by enabling experts to specialize in different patterns without any handcrafted learning pattern bias such as forgetting decay. Specifically, RouterKT introduces a \\textbf{person-wise routing mechanism} to effectively model individual-specific learning behaviors and employs \\textbf{multi-heads as experts} to enhance the modeling of complex and diverse patterns. Comprehensive experiments on ten benchmark datasets demonstrate that RouterKT exhibits significant flexibility and improves the performance of various KT backbone models, with a maximum average AUC improvement of 3.29\\% across different backbones and datasets, outperforming other state-of-the-art models. Moreover, RouterKT demonstrates consistently superior inference efficiency compared to existing approaches based on handcrafted learning pattern bias, highlighting its usability for real-world educational applications. The source code is available at https:\/\/github.com\/derek-liao\/RouterKT.git.\n        △ Less",
        "authors":"['Han Liao', 'Shuaishuai Zu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AGENT: An Aerial Vehicle Generation and Design Tool Using Large Language Models",
        "abstract":"Computer-aided design (CAD) is a promising application area for emerging artificial intelligence methods. Traditional workflows for cyberphysical systems create detailed digital models which can be evaluated by physics simulators in order to narrow the search space before creating physical prototypes. A major bottleneck of this approach is that the simulators are often computationally expensive and slow. Recent advancements in AI methods offer the possibility to accelerate these pipelines. We use the recently released AircraftVerse dataset, which is especially suited for developing and evaluating large language models for designs. AircraftVerse contains a diverse set of UAV designs represented via textual design trees together with detailed physics simulation results. Following the recent success of large language models (LLMs), we propose AGENT (Aircraft GENeraTor). AGENT is a comprehensive design tool built on the CodeT5+ LLM which learns powerful representations of aircraft textual designs directly from JSON files. We develop a curriculum of training tasks which imbues a single model with a suite of useful features. AGENT is able to generate designs conditioned on properties of flight dynamics (hover time, maximum speed, etc.). Additionally, AGENT can issue evaluations of designs allowing it to act as a surrogate model of the physics simulation that underlies the AircraftVerse dataset. We present a series of experiments which demonstrate our system's abilities. We are able to achieve strong performance using the smallest member of the CodeT5+ family (220M parameters). This allows for a flexible and powerful system which can be executed on a single GPU enabling a clear path toward future deployment.\n        △ Less",
        "authors":"['Colin Samplawski', 'Adam D. Cobb', 'Susmit Jha']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving municipal responsiveness through AI-powered image analysis in E-Government",
        "abstract":"Integration of Machine Learning (ML) techniques into public administration marks a new and transformative era for e-government systems. While traditionally e-government studies were focusing on text-based interactions, this one explores the innovative application of ML for image analysis, an approach that enables governments to address citizen petitions more efficiently. By using image classification and object detection algorithms, the model proposed in this article supports public institutions in identifying and fast responding to evidence submitted by citizens in picture format, such as infrastructure issues, environmental concerns or other urban issues that citizens might face. The research also highlights the Jevons Paradox as a critical factor, wherein increased efficiency from the citizen side (especially using mobile platforms and apps) may generate higher demand which should lead to scalable and robust solutions. Using as a case study a Romanian municipality who provided datasets of citizen-submitted images, the author analysed and proved that ML can improve accuracy and responsiveness of public institutions. The findings suggest that adopting ML for e-petition systems can not only enhance citizen participation but also speeding up administrative processes, paving the way for more transparent and effective governance. This study contributes to the discourse on e-government 3.0 by showing the potential of Artificial Intelligence (AI) to transform public service delivery, ensuring sustainable (and scalable) solutions for the growing demands of modern urban governance.\n        △ Less",
        "authors":"['Catalin Vrabie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion",
        "abstract":"Knowledge graph embedding (KGE) models are extensively studied for knowledge graph completion, yet their evaluation remains constrained by unrealistic benchmarks. Commonly used datasets are either faulty or too small to reflect real-world data. Few studies examine the role of mediator nodes, which are essential for modeling n-ary relationships, or investigate model performance variation across domains. Standard evaluation metrics rely on the closed-world assumption, which penalizes models for correctly predicting missing triples, contradicting the fundamental goals of link prediction. These metrics often compress accuracy assessment into a single value, obscuring models' specific strengths and weaknesses. The prevailing evaluation protocol operates under the unrealistic assumption that an entity's properties, for which values are to be predicted, are known in advance. While alternative protocols such as property prediction, entity-pair ranking and triple classification address some of these limitations, they remain underutilized. This paper conducts a comprehensive evaluation of four representative KGE models on large-scale datasets FB-CVT-REV and FB+CVT-REV. Our analysis reveals critical insights, including substantial performance variations between small and large datasets, both in relative rankings and absolute metrics, systematic overestimation of model capabilities when n-ary relations are binarized, and fundamental limitations in current evaluation protocols and metrics.\n        △ Less",
        "authors":"['Nasim Shirvani-Mahdavi', 'Farahnaz Akrami', 'Chengkai Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bidirectional Linear Recurrent Models for Sequence-Level Multisource Fusion",
        "abstract":"Sequence modeling is a critical yet challenging task with wide-ranging applications, especially in time series forecasting for domains like weather prediction, temperature monitoring, and energy load forecasting. Transformers, with their attention mechanism, have emerged as state-of-the-art due to their efficient parallel training, but they suffer from quadratic time complexity, limiting their scalability for long sequences. In contrast, recurrent neural networks (RNNs) offer linear time complexity, spurring renewed interest in linear RNNs for more computationally efficient sequence modeling. In this work, we introduce BLUR (Bidirectional Linear Unit for Recurrent network), which uses forward and backward linear recurrent units (LRUs) to capture both past and future dependencies with high computational efficiency. BLUR maintains the linear time complexity of traditional RNNs, while enabling fast parallel training through LRUs. Furthermore, it offers provably stable training and strong approximation capabilities, making it highly effective for modeling long-term dependencies. Extensive experiments on sequential image and time series datasets reveal that BLUR not only surpasses transformers and traditional RNNs in accuracy but also significantly reduces computational costs, making it particularly suitable for real-world forecasting tasks. Our code is available here.\n        △ Less",
        "authors":"['Qisai Liu', 'Zhanhong Jiang', 'Joshua R. Waite', 'Chao Liu', 'Aditya Balu', 'Soumik Sarkar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Forecasting Cryptocurrency Prices using Contextual ES-adRNN with Exogenous Variables",
        "abstract":"In this paper, we introduce a new approach to multivariate forecasting cryptocurrency prices using a hybrid contextual model combining exponential smoothing (ES) and recurrent neural network (RNN). The model consists of two tracks: the context track and the main track. The context track provides additional information to the main track, extracted from representative series. This information as well as information extracted from exogenous variables is dynamically adjusted to the individual series forecasted by the main track. The RNN stacked architecture with hierarchical dilations, incorporating recently developed attentive dilated recurrent cells, allows the model to capture short and long-term dependencies across time series and dynamically weight input information. The model generates both point daily forecasts and predictive intervals for one-day, one-week and four-week horizons. We apply our model to forecast prices of 15 cryptocurrencies based on 17 input variables and compare its performance with that of comparative models, including both statistical and ML ones.\n        △ Less",
        "authors":"['Slawek Smyl', 'Grzegorz Dudek', 'Paweł Pełka']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Investigating the Treacherous Turn in Deep Reinforcement Learning",
        "abstract":"The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by the human supervisor, but when deployed to perform its task, it performs an alternate behavior without the supervisor there to prevent it. Initial experiments applying DRL to an implementation of the A Link to the Past example do not produce the treacherous turn effect naturally, despite various modifications to the environment intended to produce it. However, in this work, we find the treacherous behavior to be reproducible in a DRL agent when using other trojan injection strategies. This approach deviates from the prototypical treacherous turn behavior since the behavior is explicitly trained into the agent, rather than occurring as an emergent consequence of environmental complexity or poor objective specification. Nonetheless, these experiments provide new insights into the challenges of producing agents capable of true treacherous turn behavior.\n        △ Less",
        "authors":"['Chace Ashcraft', 'Kiran Karra', 'Josh Carney', 'Nathan Drenkow']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
        "abstract":"Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https:\/\/agent-reward-bench.github.io\n        △ Less",
        "authors":"['Xing Han Lù', 'Amirhossein Kazemnejad', 'Nicholas Meade', 'Arkil Patel', 'Dongchan Shin', 'Alejandra Zambrano', 'Karolina Stańczak', 'Peter Shaw', 'Christopher J. Pal', 'Siva Reddy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Combining Forecasts using Meta-Learning: A Comparative Study for Complex Seasonality",
        "abstract":"In this paper, we investigate meta-learning for combining forecasts generated by models of different types. While typical approaches for combining forecasts involve simple averaging, machine learning techniques enable more sophisticated methods of combining through meta-learning, leading to improved forecasting accuracy. We use linear regression, $k$-nearest neighbors, multilayer perceptron, random forest, and long short-term memory as meta-learners. We define global and local meta-learning variants for time series with complex seasonality and compare meta-learners on multiple forecasting problems, demonstrating their superior performance compared to simple averaging.\n        △ Less",
        "authors":"['Grzegorz Dudek']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose Deep Image Fusion",
        "abstract":"In image fusion tasks, due to the lack of real fused images as priors, most deep learning-based fusion methods obtain global weight features from original images in large-scale data pairs to generate images that approximate real fused images. However, unlike previous studies, this paper utilizes Granular Ball adaptation to extract features in the brightness space as priors for deep networks, enabling the fusion network to converge quickly and complete the fusion task. This leads to few-shot training for a general image fusion network, and based on this, we propose the GBFF fusion method. According to the information expression division of pixel pairs in the original fused image, we classify pixel pairs with significant performance as the positive domain and non-significant pixel pairs as the boundary domain. We perform split inference in the brightness space using Granular Ball adaptation to compute weights for pixels that express information to varying degrees, generating approximate supervision images that provide priors for the neural network in the structural brightness space. Additionally, the extracted global saliency features also adaptively provide priors for setting the loss function weights of each image in the network, guiding the network to converge quickly at both global and pixel levels alongside the supervised images, thereby enhancing the expressiveness of the fused images. Each modality only used 10 pairs of images as the training set, completing the fusion task with a limited number of iterations. Experiments validate the effectiveness of the algorithm and theory, and qualitative and quantitative comparisons with SOTA methods show that this approach is highly competitive in terms of fusion time and image expressiveness.\n        △ Less",
        "authors":"['Minjie Deng', 'Yan Wei', 'Hao Zhai', 'An Wu', 'Yuncan Ouyang', 'Qianyao Peng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Long Context In-Context Compression by Getting to the Gist of Gisting",
        "abstract":"Long context processing is critical for the adoption of LLMs, but existing methods often introduce architectural complexity that hinders their practical adoption. Gisting, an in-context compression method with no architectural modification to the decoder transformer, is a promising approach due to its simplicity and compatibility with existing frameworks. While effective for short instructions, we demonstrate that gisting struggles with longer contexts, with significant performance drops even at minimal compression rates. Surprisingly, a simple average pooling baseline consistently outperforms gisting. We analyze the limitations of gisting, including information flow interruptions, capacity limitations and the inability to restrict its attention to subsets of the context. Motivated by theoretical insights into the performance gap between gisting and average pooling, and supported by extensive experimentation, we propose GistPool, a new in-context compression method. GistPool preserves the simplicity of gisting, while significantly boosting its performance on long context compression tasks.\n        △ Less",
        "authors":"['Aleksandar Petrov', 'Mark Sandler', 'Andrey Zhmoginov', 'Nolan Miller', 'Max Vladymyrov']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Adaptive Vector Index Partitioning Scheme for Low-Latency RAG Pipeline",
        "abstract":"Retrieval Augmented Generation (RAG) systems enhance response quality by integrating Large Language Models (LLMs) with vector databases, enabling external knowledge retrieval to support language model reasoning. While RAG enables efficient question answering with smaller LLMs, existing optimizations for vector search and LLM serving have largely been developed in isolation. As a result, their integration often leads to suboptimal end-to-end performance. ... This paper introduces VectorLiteRAG, an optimized vector index partitioning mechanism designed for RAG systems that enhances the responsiveness of the system by jointly optimizing vector search and LLM serving across CPU and GPU system. A key challenge is to determine which indices and how much of the vector index should reside on the GPU and adjusting LLM batch sizes to balance the pipeline for lower Time-To-First-Token (TTFT) and meeting user-defined Service-Level Objectives (SLOs). To address this, we leverage the insight that cluster access in vector databases exhibits access skew, where a subset of clusters are queried significantly more frequently than others. VectorLiteRAG exploits this property through an optimized memory distribution strategy, dynamically allocating the minimum number of vector indices corresponding to frequently accessed clusters onto the GPU HBM to ensure a balanced pipeline with the LLM for high responsiveness. This adaptive partitioning scheme is guided by a statistical model that informs memory allocation and workload distribution. Our evaluation demonstrates that VectorLiteRAG improves vector search responsiveness by 2x, significantly reduces end-to-end TTFT in RAG systems by intelligently balancing memory resources between vector search and LLM execution.\n        △ Less",
        "authors":"['Junkyum Kim', 'Divya Mahajan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Are We Merely Justifying Results ex Post Facto? Quantifying Explanatory Inversion in Post-Hoc Model Explanations",
        "abstract":"Post-hoc explanation methods provide interpretation by attributing predictions to input features. Natural explanations are expected to interpret how the inputs lead to the predictions. Thus, a fundamental question arises: Do these explanations unintentionally reverse the natural relationship between inputs and outputs? Specifically, are the explanations rationalizing predictions from the output rather than reflecting the true decision process? To investigate such explanatory inversion, we propose Inversion Quantification (IQ), a framework that quantifies the degree to which explanations rely on outputs and deviate from faithful input-output relationships. Using the framework, we demonstrate on synthetic datasets that widely used methods such as LIME and SHAP are prone to such inversion, particularly in the presence of spurious correlations, across tabular, image, and text domains. Finally, we propose Reproduce-by-Poking (RBP), a simple and model-agnostic enhancement to post-hoc explanation methods that integrates forward perturbation checks. We further show that under the IQ framework, RBP theoretically guarantees the mitigation of explanatory inversion. Empirically, for example, on the synthesized data, RBP can reduce the inversion by 1.8% on average across iconic post-hoc explanation approaches and domains.\n        △ Less",
        "authors":"['Zhen Tan', 'Song Wang', 'Yifan Li', 'Yu Kong', 'Jundong Li', 'Tianlong Chen', 'Huan Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving the evaluation of samplers on multi-modal targets",
        "abstract":"Addressing multi-modality constitutes one of the major challenges of sampling. In this reflection paper, we advocate for a more systematic evaluation of samplers towards two sources of difficulty that are mode separation and dimension. For this, we propose a synthetic experimental setting that we illustrate on a selection of samplers, focusing on the challenging criterion of recovery of the mode relative importance. These evaluations are crucial to diagnose the potential of samplers to handle multi-modality and therefore to drive progress in the field.\n        △ Less",
        "authors":"['Louis Grenioux', 'Maxence Noble', 'Marylou Gabrié']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HyperCore: The Core Framework for Building Hyperbolic Foundation Models with Comprehensive Modules",
        "abstract":"Hyperbolic neural networks have emerged as a powerful tool for modeling hierarchical data across diverse modalities. Recent studies show that token distributions in foundation models exhibit scale-free properties, suggesting that hyperbolic space is a more suitable ambient space than Euclidean space for many pre-training and downstream tasks. However, existing tools lack essential components for building hyperbolic foundation models, making it difficult to leverage recent advancements. We introduce HyperCore, a comprehensive open-source framework that provides core modules for constructing hyperbolic foundation models across multiple modalities. HyperCore's modules can be effortlessly combined to develop novel hyperbolic foundation models, eliminating the need to extensively modify Euclidean modules from scratch and possible redundant research efforts. To demonstrate its versatility, we build and test the first fully hyperbolic vision transformers (LViT) with a fine-tuning pipeline, the first fully hyperbolic multimodal CLIP model (L-CLIP), and a hybrid Graph RAG with a hyperbolic graph encoder. Our experiments demonstrate that LViT outperforms its Euclidean counterpart. Additionally, we benchmark and reproduce experiments across hyperbolic GNNs, CNNs, Transformers, and vision Transformers to highlight HyperCore's advantages.\n        △ Less",
        "authors":"['Neil He', 'Menglin Yang', 'Rex Ying']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hybrid AI-Physical Modeling for Penetration Bias Correction in X-band InSAR DEMs: A Greenland Case Study",
        "abstract":"Digital elevation models derived from Interferometric Synthetic Aperture Radar (InSAR) data over glacial and snow-covered regions often exhibit systematic elevation errors, commonly termed \"penetration bias.\" We leverage existing physics-based models and propose an integrated correction framework that combines parametric physical modeling with machine learning. We evaluate the approach across three distinct training scenarios - each defined by a different set of acquisition parameters - to assess overall performance and the model's ability to generalize. Our experiments on Greenland's ice sheet using TanDEM-X data show that the proposed hybrid model corrections significantly reduce the mean and standard deviation of DEM errors compared to a purely physical modeling baseline. The hybrid framework also achieves significantly improved generalization than a pure ML approach when trained on data with limited diversity in acquisition parameters.\n        △ Less",
        "authors":"['Islam Mansour', 'Georg Fischer', 'Ronny Haensch', 'Irena Hajnsek']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
        "abstract":"Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce Laplacian Pyramid Warping, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions.\n        △ Less",
        "authors":"['Pascal Chang', 'Sergio Sancho', 'Jingwei Tang', 'Markus Gross', 'Vinicius C. Azevedo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Toward Spiking Neural Network Local Learning Modules Resistant to Adversarial Attacks",
        "abstract":"Recent research has shown the vulnerability of Spiking Neural Networks (SNNs) under adversarial examples that are nearly indistinguishable from clean data in the context of frame-based and event-based information. The majority of these studies are constrained in generating adversarial examples using Backpropagation Through Time (BPTT), a gradient-based method which lacks biological plausibility. In contrast, local learning methods, which relax many of BPTT's constraints, remain under-explored in the context of adversarial attacks. To address this problem, we examine adversarial robustness in SNNs through the framework of four types of training algorithms. We provide an in-depth analysis of the ineffectiveness of gradient-based adversarial attacks to generate adversarial instances in this scenario. To overcome these limitations, we introduce a hybrid adversarial attack paradigm that leverages the transferability of adversarial instances. The proposed hybrid approach demonstrates superior performance, outperforming existing adversarial attack methods. Furthermore, the generalizability of the method is assessed under multi-step adversarial attacks, adversarial attacks in black-box FGSM scenarios, and within the non-spiking domain.\n        △ Less",
        "authors":"['Jiaqi Lin', 'Abhronil Sengupta']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Position: Beyond Euclidean -- Foundation Models Should Embrace Non-Euclidean Geometries",
        "abstract":"In the era of foundation models and Large Language Models (LLMs), Euclidean space has been the de facto geometric setting for machine learning architectures. However, recent literature has demonstrated that this choice comes with fundamental limitations. At a large scale, real-world data often exhibit inherently non-Euclidean structures, such as multi-way relationships, hierarchies, symmetries, and non-isotropic scaling, in a variety of domains, such as languages, vision, and the natural sciences. It is challenging to effectively capture these structures within the constraints of Euclidean spaces. This position paper argues that moving beyond Euclidean geometry is not merely an optional enhancement but a necessity to maintain the scaling law for the next-generation of foundation models. By adopting these geometries, foundation models could more efficiently leverage the aforementioned structures. Task-aware adaptability that dynamically reconfigures embeddings to match the geometry of downstream applications could further enhance efficiency and expressivity. Our position is supported by a series of theoretical and empirical investigations of prevalent foundation models.Finally, we outline a roadmap for integrating non-Euclidean geometries into foundation models, including strategies for building geometric foundation models via fine-tuning, training from scratch, and hybrid approaches.\n        △ Less",
        "authors":"['Neil He', 'Jiahong Liu', 'Buze Zhang', 'Ngoc Bui', 'Ali Maatouk', 'Menglin Yang', 'Irwin King', 'Melanie Weber', 'Rex Ying']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Knowledge Graph-extended Retrieval Augmented Generation for Question Answering",
        "abstract":"Large Language Models (LLMs) and Knowledge Graphs (KGs) offer a promising approach to robust and explainable Question Answering (QA). While LLMs excel at natural language understanding, they suffer from knowledge gaps and hallucinations. KGs provide structured knowledge but lack natural language interaction. Ideally, an AI system should be both robust to missing facts as well as easy to communicate with. This paper proposes such a system that integrates LLMs and KGs without requiring training, ensuring adaptability across different KGs with minimal human effort. The resulting approach can be classified as a specific form of a Retrieval Augmented Generation (RAG) with a KG, thus, it is dubbed Knowledge Graph-extended Retrieval Augmented Generation (KG-RAG). It includes a question decomposition module to enhance multi-hop information retrieval and answer explainability. Using In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting, it generates explicit reasoning chains processed separately to improve truthfulness. Experiments on the MetaQA benchmark show increased accuracy for multi-hop questions, though with a slight trade-off in single-hop performance compared to LLM with KG baselines. These findings demonstrate KG-RAG's potential to improve transparency in QA by bridging unstructured language understanding with structured knowledge retrieval.\n        △ Less",
        "authors":"['Jasper Linders', 'Jakub M. Tomczak']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The SERENADE project: Sensor-Based Explainable Detection of Cognitive Decline",
        "abstract":"Mild Cognitive Impairment (MCI) affects 12-18% of individuals over 60. MCI patients exhibit cognitive dysfunctions without significant daily functional loss. While MCI may progress to dementia, predicting this transition remains a clinical challenge due to limited and unreliable indicators. Behavioral changes, like in the execution of Activities of Daily Living (ADLs), can signal such progression. Sensorized smart homes and wearable devices offer an innovative solution for continuous, non-intrusive monitoring ADLs for MCI patients. However, current machine learning models for detecting behavioral changes lack transparency, hindering clinicians' trust. This paper introduces the SERENADE project, a European Union-funded initiative that aims to detect and explain behavioral changes associated with cognitive decline using explainable AI methods. SERENADE aims at collecting one year of data from 30 MCI patients living alone, leveraging AI to support clinical decision-making and offering a new approach to early dementia detection.\n        △ Less",
        "authors":"['Gabriele Civitarese', 'Michele Fiori', 'Andrea Arighi', 'Daniela Galimberti', 'Graziana Florio', 'Claudio Bettini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DataMap: A Portable Application for Visualizing High-Dimensional Data",
        "abstract":"Motivation: The visualization and analysis of high-dimensional data are essential in biomedical research. There is a need for secure, scalable, and reproducible tools to facilitate data exploration and interpretation. Results: We introduce DataMap, a browser-based application for visualization of high-dimensional data using heatmaps, principal component analysis (PCA), and t-distributed stochastic neighbor embedding (t-SNE). DataMap runs in the web browser, ensuring data privacy while eliminating the need for installation or a server. The application has an intuitive user interface for data transformation, annotation, and generation of reproducible R code. Availability and Implementation: Freely available as a GitHub page https:\/\/gexijin.github.io\/datamap\/. The source code can be found at https:\/\/github.com\/gexijin\/datamap, and can also be installed as an R package. Contact: Xijin.Ge@sdstate.ed\n        △ Less",
        "authors":"['Xijin Ge']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions",
        "abstract":"Machine learning and Bayesian optimization (BO) algorithms can significantly accelerate the optimization of chemical reactions. Transfer learning can bolster the effectiveness of BO algorithms in low-data regimes by leveraging pre-existing chemical information or data outside the direct optimization task (i.e., source data). Large language models (LLMs) have demonstrated that chemical information present in foundation training data can give them utility for processing chemical data. Furthermore, they can be augmented with and help synthesize potentially multiple modalities of source chemical data germane to the optimization task. In this work, we examine how chemical information from LLMs can be elicited and used for transfer learning to accelerate the BO of reaction conditions to maximize yield. Specifically, we show that a survey-like prompting scheme and preference learning can be used to infer a utility function which models prior chemical information embedded in LLMs over a chemical parameter space; we find that the utility function shows modest correlation to true experimental measurements (yield) over the parameter space despite operating in a zero-shot setting. Furthermore, we show that the utility function can be leveraged to focus BO efforts in promising regions of the parameter space, improving the yield of the initial BO query and enhancing optimization in 4 of the 6 datasets studied. Overall, we view this work as a step towards bridging the gap between the chemistry knowledge embedded in LLMs and the capabilities of principled BO methods to accelerate reaction optimization.\n        △ Less",
        "authors":"['Roshan Patel', 'Saeed Moayedpour', 'Louis De Lescure', 'Lorenzo Kogler-Anele', 'Alan Cherney', 'Sven Jager', 'Yasser Jangjou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Personalizing Federated Learning for Hierarchical Edge Networks with Non-IID Data",
        "abstract":"Accommodating edge networks between IoT devices and the cloud server in Hierarchical Federated Learning (HFL) enhances communication efficiency without compromising data privacy. However, devices connected to the same edge often share geographic or contextual similarities, leading to varying edge-level data heterogeneity with different subsets of labels per edge, on top of device-level heterogeneity. This hierarchical non-Independent and Identically Distributed (non-IID) nature, which implies that each edge has its own optimization goal, has been overlooked in HFL research. Therefore, existing edge-accommodated HFL demonstrates inconsistent performance across edges in various hierarchical non-IID scenarios. To ensure robust performance with diverse edge-level non-IID data, we propose a Personalized Hierarchical Edge-enabled Federated Learning (PHE-FL), which personalizes each edge model to perform well on the unique class distributions specific to each edge. We evaluated PHE-FL across 4 scenarios with varying levels of edge-level non-IIDness, with extreme IoT device level non-IIDness. To accurately assess the effectiveness of our personalization approach, we deployed test sets on each edge server instead of the cloud server, and used both balanced and imbalanced test sets. Extensive experiments show that PHE-FL achieves up to 83 percent higher accuracy compared to existing federated learning approaches that incorporate edge networks, given the same number of training rounds. Moreover, PHE-FL exhibits improved stability, as evidenced by reduced accuracy fluctuations relative to the state-of-the-art FedAvg with two-level (edge and cloud) aggregation.\n        △ Less",
        "authors":"['Seunghyun Lee', 'Omid Tavallaie', 'Shuaijun Chen', 'Kanchana Thilakarathna', 'Suranga Seneviratne', 'Adel Nadjaran Toosi', 'Albert Y. Zomaya']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"In almost all shallow analytic neural network optimization landscapes, efficient minimizers have strongly convex neighborhoods",
        "abstract":"Whether or not a local minimum of a cost function has a strongly convex neighborhood greatly influences the asymptotic convergence rate of optimizers. In this article, we rigorously analyze the prevalence of this property for the mean squared error induced by shallow, 1-hidden layer neural networks with analytic activation functions when applied to regression problems. The parameter space is divided into two domains: the 'efficient domain' (all parameters for which the respective realization function cannot be generated by a network having a smaller number of neurons) and the 'redundant domain' (the remaining parameters). In almost all regression problems on the efficient domain the optimization landscape only features local minima that are strongly convex. Formally, we will show that for certain randomly picked regression problems the optimization landscape is almost surely a Morse function on the efficient domain. The redundant domain has significantly smaller dimension than the efficient domain and on this domain, potential local minima are never isolated.\n        △ Less",
        "authors":"['Felix Benning', 'Steffen Dereich']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On Transfer-based Universal Attacks in Pure Black-box Setting",
        "abstract":"Despite their impressive performance, deep visual models are susceptible to transferable black-box adversarial attacks. Principally, these attacks craft perturbations in a target model-agnostic manner. However, surprisingly, we find that existing methods in this domain inadvertently take help from various priors that violate the black-box assumption such as the availability of the dataset used to train the target model, and the knowledge of the number of classes in the target model. Consequently, the literature fails to articulate the true potency of transferable black-box attacks. We provide an empirical study of these biases and propose a framework that aids in a prior-free transparent study of this paradigm. Using our framework, we analyze the role of prior knowledge of the target model data and number of classes in attack performance. We also provide several interesting insights based on our analysis, and demonstrate that priors cause overestimation in transferability scores. Finally, we extend our framework to query-based attacks. This extension inspires a novel image-blending technique to prepare data for effective surrogate model training.\n        △ Less",
        "authors":"['Mohammad A. A. K. Jalwana', 'Naveed Akhtar', 'Ajmal Mian', 'Nazanin Rahnavard', 'Mubarak Shah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diachronic and synchronic variation in the performance of adaptive machine learning systems: The ethical challenges",
        "abstract":"Objectives: Machine learning (ML) has the potential to facilitate \"continual learning\" in medicine, in which an ML system continues to evolve in response to exposure to new data over time, even after being deployed in a clinical setting. In this paper, we provide a tutorial on the range of ethical issues raised by the use of such \"adaptive\" ML systems in medicine that have, thus far, been neglected in the literature.\n  Target audience: The target audiences for this tutorial are the developers of machine learning AI systems, healthcare regulators, the broader medical informatics community, and practicing clinicians.\n  Scope: Discussions of adaptive ML systems to date have overlooked the distinction between two sorts of variance that such systems may exhibit -- diachronic evolution (change over time) and synchronic variation (difference between cotemporaneous instantiations of the algorithm at different sites) -- and under-estimated the significance of the latter. We highlight the challenges that diachronic evolution and synchronic variation present for the quality of patient care, informed consent, and equity, and discuss the complex ethical trade-offs involved in the design of such systems.\n        △ Less",
        "authors":"['Joshua Hatherley', 'Robert Sparrow']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Examining GPT's Capability to Generate and Map Course Concepts and Their Relationship",
        "abstract":"Extracting key concepts and their relationships from course information and materials facilitates the provision of visualizations and recommendations for learners who need to select the right courses to take from a large number of courses. However, identifying and extracting themes manually is labor-intensive and time-consuming. Previous machine learning-based methods to extract relevant concepts from courses heavily rely on detailed course materials, which necessitates labor-intensive preparation of course materials. This paper investigates the potential of LLMs such as GPT in automatically generating course concepts and their relations. Specifically, we design a suite of prompts and provide GPT with the course information with different levels of detail, thereby generating high-quality course concepts and identifying their relations. Furthermore, we comprehensively evaluate the quality of the generated concepts and relationships through extensive experiments. Our results demonstrate the viability of LLMs as a tool for supporting educational content selection and delivery.\n        △ Less",
        "authors":"['Tianyuan Yang', 'Ren Baofeng', 'Chenghao Gu', 'Tianjia He', 'Boxuan Ma', 'Shinichi Konomi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ML For Hardware Design Interpretability: Challenges and Opportunities",
        "abstract":"The increasing size and complexity of machine learning (ML) models have driven the growing need for custom hardware accelerators capable of efficiently supporting ML workloads. However, the design of such accelerators remains a time-consuming process, heavily relying on engineers to manually ensure design interpretability through clear documentation and effective communication. Recent advances in large language models (LLMs) offer a promising opportunity to automate these design interpretability tasks, particularly the generation of natural language descriptions for register-transfer level (RTL) code, what we refer to as \"RTL-to-NL tasks.\" In this paper, we examine how design interpretability, particularly in RTL-to-NL tasks, influences the efficiency of the hardware design process. We review existing work adapting LLMs for these tasks, highlight key challenges that remain unaddressed, including those related to data, computation, and model development, and identify opportunities to address them. By doing so, we aim to guide future research in leveraging ML to automate RTL-to-NL tasks and improve hardware design interpretability, thereby accelerating the hardware design process and meeting the increasing demand for custom hardware accelerators in machine learning and beyond.\n        △ Less",
        "authors":"['Raymond Baartmans', 'Andrew Ensinger', 'Victor Agostinelli', 'Lizhong Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mimic In-Context Learning for Multimodal Tasks",
        "abstract":"Recently, In-context Learning (ICL) has become a significant inference paradigm in Large Multimodal Models (LMMs), utilizing a few in-context demonstrations (ICDs) to prompt LMMs for new tasks. However, the synergistic effects in multimodal data increase the sensitivity of ICL performance to the configurations of ICDs, stimulating the need for a more stable and general mapping function. Mathematically, in Transformer-based models, ICDs act as ``shift vectors'' added to the hidden states of query tokens. Inspired by this, we introduce Mimic In-Context Learning (MimIC) to learn stable and generalizable shift effects from ICDs. Specifically, compared with some previous shift vector-based methods, MimIC more strictly approximates the shift effects by integrating lightweight learnable modules into LMMs with four key enhancements: 1) inserting shift vectors after attention layers, 2) assigning a shift vector to each attention head, 3) making shift magnitude query-dependent, and 4) employing a layer-wise alignment loss. Extensive experiments on two LMMs (Idefics-9b and Idefics2-8b-base) across three multimodal tasks (VQAv2, OK-VQA, Captioning) demonstrate that MimIC outperforms existing shift vector-based methods. The code is available at https:\/\/github.com\/Kamichanw\/MimIC.\n        △ Less",
        "authors":"['Yuchu Jiang', 'Jiale Fu', 'Chenduo Hao', 'Xinting Hu', 'Yingzhe Peng', 'Xin Geng', 'Xu Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-University: An LLM-based platform for instructional alignment to scientific classrooms",
        "abstract":"We introduce AI University (AI-U), a flexible framework for AI-driven course content delivery that adapts to instructors' teaching styles. At its core, AI-U fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to generate instructor-aligned responses from lecture videos, notes, and textbooks. Using a graduate-level finite-element-method (FEM) course as a case study, we present a scalable pipeline to systematically construct training data, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and optimize its responses through RAG-based synthesis. Our evaluation - combining cosine similarity, LLM-based assessment, and expert review - demonstrates strong alignment with course materials. We also have developed a prototype web application, available at https:\/\/my-ai-university.com, that enhances traceability by linking AI-generated responses to specific sections of the relevant course material and time-stamped instances of the open-access video lectures. Our expert model is found to have greater cosine similarity with a reference on 86% of test cases. An LLM judge also found our expert model to outperform the base Llama 3.2 model approximately four times out of five. AI-U offers a scalable approach to AI-assisted education, paving the way for broader adoption in higher education. Here, our framework has been presented in the setting of a class on FEM - a subject that is central to training PhD and Master students in engineering science. However, this setting is a particular instance of a broader context: fine-tuning LLMs to research content in science.\n        △ Less",
        "authors":"['Mostafa Faghih Shojaei', 'Rahul Gulati', 'Benjamin A. Jasperson', 'Shangshang Wang', 'Simone Cimolato', 'Dangli Cao', 'Willie Neiswanger', 'Krishna Garikipati']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Combinatorial Interpretability of Neural Computation",
        "abstract":"We introduce combinatorial interpretability, a methodology for understanding neural computation by analyzing the combinatorial structures in the sign-based categorization of a network's weights and biases. We demonstrate its power through feature channel coding, a theory that explains how neural networks compute Boolean expressions and potentially underlies other categories of neural network computation. According to this theory, features are computed via feature channels: unique cross-neuron encodings shared among the inputs the feature operates on. Because different feature channels share neurons, the neurons are polysemantic and the channels interfere with one another, making the computation appear inscrutable.\n  We show how to decipher these computations by analyzing a network's feature channel coding, offering complete mechanistic interpretations of several small neural networks that were trained with gradient descent. Crucially, this is achieved via static combinatorial analysis of the weight matrices, without examining activations or training new autoencoding networks. Feature channel coding reframes the superposition hypothesis, shifting the focus from neuron activation directionality in high-dimensional space to the combinatorial structure of codes. It also allows us for the first time to exactly quantify and explain the relationship between a network's parameter size and its computational capacity (i.e. the set of features it can compute with low error), a relationship that is implicitly at the core of many modern scaling laws.\n  Though our initial studies of feature channel coding are restricted to Boolean functions, we believe they provide a rich, controlled, and informative research space, and that the path we propose for combinatorial interpretation of neural computation can provide a basis for understanding both artificial and biological neural circuits.\n        △ Less",
        "authors":"['Micah Adler', 'Dan Alistarh', 'Nir Shavit']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Shrinkage Estimation For Personalized Deep Kernel Regression In Modeling Brain Trajectories",
        "abstract":"Longitudinal biomedical studies monitor individuals over time to capture dynamics in brain development, disease progression, and treatment effects. However, estimating trajectories of brain biomarkers is challenging due to biological variability, inconsistencies in measurement protocols (e.g., differences in MRI scanners), scarcity, and irregularity in longitudinal measurements. Herein, we introduce a novel personalized deep kernel regression framework for forecasting brain biomarkers, with application to regional volumetric measurements. Our approach integrates two key components: a population model that captures brain trajectories from a large and diverse cohort, and a subject-specific model that captures individual trajectories. To optimally combine these, we propose Adaptive Shrinkage Estimation, which effectively balances population and subject-specific models. We assess our model's performance through predictive accuracy metrics, uncertainty quantification, and validation against external clinical studies. Benchmarking against state-of-the-art statistical and machine learning models -- including linear mixed effects models, generalized additive models, and deep learning methods -- demonstrates the superior predictive performance of our approach. Additionally, we apply our method to predict trajectories of composite neuroimaging biomarkers, which highlights the versatility of our approach in modeling the progression of longitudinal neuroimaging biomarkers. Furthermore, validation on three external neuroimaging studies confirms the robustness of our method across different clinical contexts. We make the code available at https:\/\/github.com\/vatass\/AdaptiveShrinkageDKGP.\n        △ Less",
        "authors":"['Vasiliki Tassopoulou', 'Haochang Shou', 'Christos Davatzikos']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning",
        "abstract":"Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.\n        △ Less",
        "authors":"['Haozhe Wang', 'Chao Qu', 'Zuming Huang', 'Wei Chu', 'Fangzhen Lin', 'Wenhu Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Double Machine Learning for Causal Inference under Shared-State Interference",
        "abstract":"Researchers and practitioners often wish to measure treatment effects in settings where units interact via markets and recommendation systems. In these settings, units are affected by certain shared states, like prices, algorithmic recommendations or social signals. We formalize this structure, calling it shared-state interference, and argue that our formulation captures many relevant applied settings. Our key modeling assumption is that individuals' potential outcomes are independent conditional on the shared state. We then prove an extension of a double machine learning (DML) theorem providing conditions for achieving efficient inference under shared-state interference. We also instantiate our general theorem in several models of interest where it is possible to efficiently estimate the average direct effect (ADE) or global average treatment effect (GATE).\n        △ Less",
        "authors":"['Chris Hays', 'Manish Raghavan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Datum-wise Transformer for Synthetic Tabular Data Detection in the Wild",
        "abstract":"The growing power of generative models raises major concerns about the authenticity of published content. To address this problem, several synthetic content detection methods have been proposed for uniformly structured media such as image or text. However, little work has been done on the detection of synthetic tabular data, despite its importance in industry and government. This form of data is complex to handle due to the diversity of its structures: the number and types of the columns may vary wildly from one table to another. We tackle the tough problem of detecting synthetic tabular data ''in the wild'', i.e. when the model is deployed on table structures it has never seen before. We introduce a novel datum-wise transformer architecture and show that it outperforms existing models. Furthermore, we investigate the application of domain adaptation techniques to enhance the effectiveness of our model, thereby providing a more robust data-forgery detection solution.\n        △ Less",
        "authors":"['G. Charbel N. Kindji', 'Elisa Fromont', 'Lina Maria Rojas-Barahona', 'Tanguy Urvoy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PatchTrAD: A Patch-Based Transformer focusing on Patch-Wise Reconstruction Error for Time Series Anomaly Detection",
        "abstract":"Time series anomaly detection (TSAD) focuses on identifying whether observations in streaming data deviate significantly from normal patterns. With the prevalence of connected devices, anomaly detection on time series has become paramount, as it enables real-time monitoring and early detection of irregular behaviors across various application domains. In this work, we introduce PatchTrAD, a Patch-based Transformer model for time series anomaly detection. Our approach leverages a Transformer encoder along with the use of patches under a reconstructionbased framework for anomaly detection. Empirical evaluations on multiple benchmark datasets show that PatchTrAD is on par, in terms of detection performance, with state-of-the-art deep learning models for anomaly detection while being time efficient during inference.\n        △ Less",
        "authors":"['Samy-Melwan Vilhes', 'Gilles Gasso', 'Mokhtar Z Alaya']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer",
        "abstract":"Colorectal cancer (CRC) ranks as the second leading cause of cancer-related deaths and the third most prevalent malignant tumour worldwide. Early detection of CRC remains problematic due to its non-specific and often embarrassing symptoms, which patients frequently overlook or hesitate to report to clinicians. Crucially, the stage at which CRC is diagnosed significantly impacts survivability, with a survival rate of 80-95\\% for Stage I and a stark decline to 10\\% for Stage IV. Unfortunately, in the UK, only 14.4\\% of cases are diagnosed at the earliest stage (Stage I).\n  In this study, we propose ColonScopeX, a machine learning framework utilizing explainable AI (XAI) methodologies to enhance the early detection of CRC and pre-cancerous lesions. Our approach employs a multimodal model that integrates signals from blood sample measurements, processed using the Savitzky-Golay algorithm for fingerprint smoothing, alongside comprehensive patient metadata, including medication history, comorbidities, age, weight, and BMI. By leveraging XAI techniques, we aim to render the model's decision-making process transparent and interpretable, thereby fostering greater trust and understanding in its predictions. The proposed framework could be utilised as a triage tool or a screening tool of the general population.\n  This research highlights the potential of combining diverse patient data sources and explainable machine learning to tackle critical challenges in medical diagnostics.\n        △ Less",
        "authors":"['Natalia Sikora', 'Robert L. Manschke', 'Alethea M. Tang', 'Peter Dunstan', 'Dean A. Harris', 'Su Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning",
        "abstract":"How to adapt a pre-trained model continuously for sequential tasks with different prediction class labels and domains and finally learn a generalizable model across diverse tasks is a long-lasting challenge. Continual learning (CL) has emerged as a promising approach to leverage pre-trained models (e.g., Transformers) for sequential tasks. While many existing CL methods incrementally store additional learned structures, such as Low-Rank Adaptation (LoRA) adapters or prompts and sometimes even preserve features from previous samples to maintain performance. This leads to unsustainable parameter growth and escalating storage costs as the number of tasks increases. Moreover, current approaches often lack task similarity awareness, which further hinders the models ability to effectively adapt to new tasks without interfering with previously acquired knowledge. To address these challenges, we propose FM-LoRA, a novel and efficient low-rank adaptation method that integrates both a dynamic rank selector (DRS) and dynamic meta-prompting (DMP). This framework allocates model capacity more effectively across tasks by leveraging a shared low-rank subspace critical for preserving knowledge, thereby avoiding continual parameter expansion. Extensive experiments on various CL benchmarks, including ImageNet-R, CIFAR100, and CUB200 for class-incremental learning (CIL), and DomainNet for domain-incremental learning (DIL), with Transformers backbone demonstrate that FM-LoRA effectively mitigates catastrophic forgetting while delivering robust performance across a diverse range of tasks and domains.\n        △ Less",
        "authors":"['Xiaobing Yu', 'Jin Yang', 'Xiao Wu', 'Peijie Qiu', 'Xiaofeng Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Probabilistic QoS Metric Forecasting in Delay-Tolerant Networks Using Conditional Diffusion Models on Latent Dynamics",
        "abstract":"Active QoS metric prediction, commonly employed in the maintenance and operation of DTN, could enhance network performance regarding latency, throughput, energy consumption, and dependability. Naturally formulated as a multivariate time series forecasting problem, it attracts substantial research efforts. Traditional mean regression methods for time series forecasting cannot capture the data complexity adequately, resulting in deteriorated performance in operational tasks in DTNs such as routing. This paper formulates the prediction of QoS metrics in DTN as a probabilistic forecasting problem on multivariate time series, where one could quantify the uncertainty of forecasts by characterizing the distribution of these samples. The proposed approach hires diffusion models and incorporates the latent temporal dynamics of non-stationary and multi-mode data into them. Extensive experiments demonstrate the efficacy of the proposed approach by showing that it outperforms the popular probabilistic time series forecasting methods.\n        △ Less",
        "authors":"['Enming Zhang', 'Zheng Liu', 'Yu Xiang', 'Yanwen Qu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From Text to Time? Rethinking the Effectiveness of the Large Language Model for Time Series Forecasting",
        "abstract":"Using pre-trained large language models (LLMs) as the backbone for time series prediction has recently gained significant research interest. However, the effectiveness of LLM backbones in this domain remains a topic of debate. Based on thorough empirical analyses, we observe that training and testing LLM-based models on small datasets often leads to the Encoder and Decoder becoming overly adapted to the dataset, thereby obscuring the true predictive capabilities of the LLM backbone. To investigate the genuine potential of LLMs in time series prediction, we introduce three pre-training models with identical architectures but different pre-training strategies. Thereby, large-scale pre-training allows us to create unbiased Encoder and Decoder components tailored to the LLM backbone. Through controlled experiments, we evaluate the zero-shot and few-shot prediction performance of the LLM, offering insights into its capabilities. Extensive experiments reveal that although the LLM backbone demonstrates some promise, its forecasting performance is limited. Our source code is publicly available in the anonymous repository: https:\/\/anonymous.4open.science\/r\/LLM4TS-0B5C.\n        △ Less",
        "authors":"['Xinyu Zhang', 'Shanshan Feng', 'Xutao Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring utilization of generative AI for research and education in data-driven materials science",
        "abstract":"Generative AI has recently had a profound impact on various fields, including daily life, research, and education. To explore its efficient utilization in data-driven materials science, we organized a hackathon -- AIMHack2024 -- in July 2024. In this hackathon, researchers from fields such as materials science, information science, bioinformatics, and condensed matter physics worked together to explore how generative AI can facilitate research and education. Based on the results of the hackathon, this paper presents topics related to (1) conducting AI-assisted software trials, (2) building AI tutors for software, and (3) developing GUI applications for software. While generative AI continues to evolve rapidly, this paper provides an early record of its application in data-driven materials science and highlights strategies for integrating AI into research and education.\n        △ Less",
        "authors":"['Takahiro Misawa', 'Ai Koizumi', 'Ryo Tamura', 'Kazuyoshi Yoshimi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Graph-Enhanced DeepONet Approach for Real-Time Estimating Hydrogen-Enriched Natural Gas Flow under Variable Operations",
        "abstract":"Blending green hydrogen into natural gas presents a promising approach for renewable energy integration and fuel decarbonization. Accurate estimation of hydrogen fraction in hydrogen-enriched natural gas (HENG) pipeline networks is crucial for operational safety and efficiency, yet it remains challenging due to complex dynamics. While existing data-driven approaches adopt end-to-end architectures for HENG flow state estimation, their limited adaptability to varying operational conditions hinders practical applications. To this end, this study proposes a graph-enhanced DeepONet framework for the real-time estimation of HENG flow, especially hydrogen fractions. First, a dual-network architecture, called branch network and trunk network, is employed to characterize operational conditions and sparse sensor measurements to estimate the HENG state at targeted locations and time points. Second, a graph-enhance branch network is proposed to incorporate pipeline topology, improving the estimation accuracy in large-scale pipeline networks. Experimental results demonstrate that the proposed method achieves superior estimation accuracy for HCNG flow under varying operational conditions compared to conventional approaches.\n        △ Less",
        "authors":"['Sicheng Liu', 'Hongchang Huang', 'Bo Yang', 'Mingxuan Cai', 'Xu Yang', 'Xinping Guan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"When Federated Learning Meets Quantum Computing: Survey and Research Opportunities",
        "abstract":"Quantum Federated Learning (QFL) is an emerging field that harnesses advances in Quantum Computing (QC) to improve the scalability and efficiency of decentralized Federated Learning (FL) models. This paper provides a systematic and comprehensive survey of the emerging problems and solutions when FL meets QC, from research protocol to a novel taxonomy, particularly focusing on both quantum and federated limitations, such as their architectures, Noisy Intermediate Scale Quantum (NISQ) devices, and privacy preservation, so on. This work explores key developments and integration strategies, along with the impact of quantum computing on FL, keeping a sharp focus on hybrid quantum-classical approaches. The paper offers an in-depth understanding of how the strengths of QC, such as gradient hiding, state entanglement, quantum key distribution, quantum security, and quantum-enhanced differential privacy, have been integrated into FL to ensure the privacy of participants in an enhanced, fast, and secure framework. Finally, this study proposes potential future directions to address the identified research gaps and challenges, aiming to inspire faster and more secure QFL models for practical use.\n        △ Less",
        "authors":"['Aakar Mathur', 'Ashish Gupta', 'Sajal K. Das']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models",
        "abstract":"The rapid advancement of multi-modal large reasoning models (MLRMs) -- enhanced versions of multimodal language models (MLLMs) equipped with reasoning capabilities -- has revolutionized diverse applications. However, their safety implications remain underexplored. While prior work has exposed critical vulnerabilities in unimodal reasoning models, MLRMs introduce distinct risks from cross-modal reasoning pathways. This work presents the first systematic safety analysis of MLRMs through large-scale empirical studies comparing MLRMs with their base MLLMs. Our experiments reveal three critical findings: (1) The Reasoning Tax: Acquiring reasoning capabilities catastrophically degrades inherited safety alignment. MLRMs exhibit 37.44% higher jailbreaking success rates than base MLLMs under adversarial attacks. (2) Safety Blind Spots: While safety degradation is pervasive, certain scenarios (e.g., Illegal Activity) suffer 25 times higher attack rates -- far exceeding the average 3.4 times increase, revealing scenario-specific vulnerabilities with alarming cross-model and datasets consistency. (3) Emergent Self-Correction: Despite tight reasoning-answer safety coupling, MLRMs demonstrate nascent self-correction -- 16.9% of jailbroken reasoning steps are overridden by safe answers, hinting at intrinsic safeguards. These findings underscore the urgency of scenario-aware safety auditing and mechanisms to amplify MLRMs' self-correction potential. To catalyze research, we open-source OpenSafeMLRM, the first toolkit for MLRM safety evaluation, providing unified interface for mainstream models, datasets, and jailbreaking methods. Our work calls for immediate efforts to harden reasoning-augmented AI, ensuring its transformative potential aligns with ethical safeguards.\n        △ Less",
        "authors":"['Junfeng Fang', 'Yukai Wang', 'Ruipeng Wang', 'Zijun Yao', 'Kun Wang', 'An Zhang', 'Xiang Wang', 'Tat-Seng Chua']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mechanistic Anomaly Detection for \"Quirky\" Language Models",
        "abstract":"As LLMs grow in capability, the task of supervising LLMs becomes more challenging. Supervision failures can occur if LLMs are sensitive to factors that supervisors are unaware of. We investigate Mechanistic Anomaly Detection (MAD) as a technique to augment supervision of capable models; we use internal model features to identify anomalous training signals so they can be investigated or discarded. We train detectors to flag points from the test environment that differ substantially from the training environment, and experiment with a large variety of detector features and scoring rules to detect anomalies in a set of ``quirky'' language models. We find that detectors can achieve high discrimination on some tasks, but no detector is effective across all models and tasks. MAD techniques may be effective in low-stakes applications, but advances in both detection and evaluation are likely needed if they are to be used in high stakes settings.\n        △ Less",
        "authors":"['David O. Johnston', 'Arkajyoti Chakraborty', 'Nora Belrose']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Analogical Learning for Cross-Scenario Generalization: Framework and Application to Intelligent Localization",
        "abstract":"Existing learning models often exhibit poor generalization when deployed across diverse scenarios. It is mainly due to that the underlying reference frame of the data varies with the deployment environment and settings. However, despite the data of each scenario has its distinct reference frame, its generation generally follows the same underlying physical rule. Based on these findings, this article proposes a brand-new universal deep learning framework named analogical learning (AL), which provides a highly efficient way to implicitly retrieve the reference frame information associated with a scenario and then to make accurate prediction by relative analogy across scenarios. Specifically, an elegant bipartite neural network architecture called Mateformer is designed, the first part of which calculates the relativity within multiple feature spaces between the input data and a small amount of embedded data from the current scenario, while the second part uses these relativity to guide the nonlinear analogy. We apply AL to the typical multi-scenario learning problem of intelligent wireless localization in cellular networks. Extensive experiments show that AL achieves state-of-the-art accuracy, stable transferability and robust adaptation to new scenarios without any tuning, and outperforming conventional methods with a precision improvement of nearly two orders of magnitude. All data and code are available at https:\/\/github.com\/ziruichen-research\/ALLoc.\n        △ Less",
        "authors":"['Zirui Chen', 'Zhaoyang Zhang', 'Ziqing Xing', 'Ridong Li', 'Zhaohui Yang', 'Richeng Jin', 'Chongwen Huang', 'Yuzhi Yang', 'Mérouane Debbah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration",
        "abstract":"Complex chemical space and limited knowledge scope with biases holds immense challenge for human scientists, yet in automated materials discovery. Existing intelligent methods relies more on numerical computation, leading to inefficient exploration and results with hard-interpretability. To bridge this gap, we introduce a principles-guided material discovery system powered by language inferential multi-agent system (MAS), namely PriM. Our framework integrates automated hypothesis generation with experimental validation in a roundtable system of MAS, enabling systematic exploration while maintaining scientific rigor. Based on our framework, the case study of nano helix demonstrates higher materials exploration rate and property value while providing transparent reasoning pathways. This approach develops an automated-and-transparent paradigm for material discovery, with broad implications for rational design of functional materials. Code is publicly available at our \\href{https:\/\/github.com\/amair-lab\/PriM}{GitHub}.\n        △ Less",
        "authors":"['Zheyuan Lai', 'Yingming Pu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models",
        "abstract":"Although multimodal large language models (MLLMs) exhibit remarkable reasoning capabilities on complex multimodal understanding tasks, they still suffer from the notorious hallucination issue: generating outputs misaligned with obvious visual or factual evidence. Currently, training-based solutions, like direct preference optimization (DPO), leverage paired preference data to suppress hallucinations. However, they risk sacrificing general reasoning capabilities due to the likelihood displacement. Meanwhile, training-free solutions, like contrastive decoding, achieve this goal by subtracting the estimated hallucination pattern from a distorted input. Yet, these handcrafted perturbations (e.g., add noise to images) may poorly capture authentic hallucination patterns. To avoid these weaknesses of existing methods, and realize robust hallucination mitigation (i.e., maintaining general reasoning performance), we propose a novel framework: Decoupling Contrastive Decoding (DCD). Specifically, DCD decouples the learning of positive and negative samples in preference datasets, and trains separate positive and negative image projections within the MLLM. The negative projection implicitly models real hallucination patterns, which enables vision-aware negative images in the contrastive decoding inference stage. Our DCD alleviates likelihood displacement by avoiding pairwise optimization and generalizes robustly without handcrafted degradation. Extensive ablations across hallucination benchmarks and general reasoning tasks demonstrate the effectiveness of DCD, i.e., it matches DPO's hallucination suppression while preserving general capabilities and outperforms the handcrafted contrastive decoding methods.\n        △ Less",
        "authors":"['Wei Chen', 'Xin Yan', 'Bin Wen', 'Fan Yang', 'Tingting Gao', 'Di Zhang', 'Long Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generative AI in Live Operations: Evidence of Productivity Gains in Cybersecurity and Endpoint Management",
        "abstract":"We measure the association between generative AI (GAI) tool adoption and four metrics spanning security operations, information protection, and endpoint management: 1) number of security alerts per incident, 2) probability of security incident reopenings, 3) time to classify a data loss prevention alert, and 4) time to resolve device policy conflicts. We find that GAI is associated with robust and statistically and practically significant improvements in the four metrics. Although unobserved confounders inhibit causal identification, these results are among the first to use observational data from live operations to investigate the relationship between GAI adoption and security operations, data loss prevention, and device policy management.\n        △ Less",
        "authors":"['James Bono', 'Justin Grana', 'Kleanthis Karakolios', 'Pruthvi Hanumanthapura Ramakrishna', 'Ankit Srivastava']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Estimating Item Difficulty Using Large Language Models and Tree-Based Machine Learning Algorithms",
        "abstract":"Estimating item difficulty through field-testing is often resource-intensive and time-consuming. As such, there is strong motivation to develop methods that can predict item difficulty at scale using only the item content. Large Language Models (LLMs) represent a new frontier for this goal. The present research examines the feasibility of using an LLM to predict item difficulty for K-5 mathematics and reading assessment items (N = 5170). Two estimation approaches were implemented: (a) a direct estimation method that prompted the LLM to assign a single difficulty rating to each item, and (b) a feature-based strategy where the LLM extracted multiple cognitive and linguistic features, which were then used in ensemble tree-based models (random forests and gradient boosting) to predict difficulty. Overall, direct LLM estimates showed moderate to strong correlations with true item difficulties. However, their accuracy varied by grade level, often performing worse for early grades. In contrast, the feature-based method yielded stronger predictive accuracy, with correlations as high as r = 0.87 and lower error estimates compared to both direct LLM predictions and baseline regressors. These findings highlight the promise of LLMs in streamlining item development and reducing reliance on extensive field testing and underscore the importance of structured feature extraction. We provide a seven-step workflow for testing professionals who would want to implement a similar item difficulty estimation approach with their item pool.\n        △ Less",
        "authors":"['Pooya Razavi', 'Sonya J. Powers']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A temporal scale transformer framework for precise remaining useful life prediction in fuel cells",
        "abstract":"In exploring Predictive Health Management (PHM) strategies for Proton Exchange Membrane Fuel Cells (PEMFC), the Transformer model, widely used in data-driven approaches, excels in many fields but struggles with time series analysis due to its self-attention mechanism, which yields a complexity of the input sequence squared and low computational efficiency. It also faces challenges in capturing both global long-term dependencies and local details effectively. To tackle this, we propose the Temporal Scale Transformer (TSTransformer), an enhanced version of the inverted Transformer (iTransformer). Unlike traditional Transformers that treat each timestep as an input token, TSTransformer maps sequences of varying lengths into tokens at different stages for inter-sequence modeling, using attention to capture multivariate correlations and feed-forward networks (FFN) to encode sequence representations. By integrating a one-dimensional convolutional layer into the multivariate attention for multi-level scaling of K and V matrices, it improves local feature extraction, captures temporal scale characteristics, and reduces token count and computational costs. Experiments comparing TSTransformer with models like Long Short-Term Memory, iTransformer, and Transformer demonstrate its potential as a powerful tool for advancing PHM in renewable energy, effectively addressing the limitations of pure Transformer models in data-driven time series tasks.\n        △ Less",
        "authors":"['Zezhi Tang', 'Xiaoyu Chen', 'Xin Jin', 'Benyuan Zhang', 'Wenyu Liang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InfoGain Wavelets: Furthering the Design of Diffusion Wavelets for Graph-Structured Data",
        "abstract":"Diffusion wavelets extract information from graph signals at different scales of resolution by utilizing graph diffusion operators raised to various powers, known as diffusion scales. Traditionally, the diffusion scales are chosen to be dyadic integers, $\\mathbf{2^j}$. Here, we propose a novel, unsupervised method for selecting the diffusion scales based on ideas from information theory. We then show that our method can be incorporated into wavelet-based GNNs via graph classification experiments.\n        △ Less",
        "authors":"['David R. Johnson', 'Smita Krishnaswamy', 'Michael Perlmutter']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention",
        "abstract":"Transformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with input sequence length, presents significant challenges for processing very long sequences or operating under resource constraints. This paper introduces the Learnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that replaces the standard dot-product self-attention with a learnable multi-scale Haar wavelet transform module. Leveraging the intrinsic multi-resolution properties of wavelets, the LMWT efficiently captures both local details and global context. Crucially, the parameters of the wavelet transform, including scale-specific coefficients, are learned end-to-end during training, allowing the model to adapt its decomposition strategy to the data and task. We present the detailed mathematical formulation of the learnable Haar wavelet module and its integration into the transformer framework, supplemented by an architectural diagram. We conduct a comprehensive experimental evaluation on a standard machine translation benchmark (WMT16 En-De), comparing the LMWT against a baseline self-attention transformer using metrics like BLEU score, perplexity, and token accuracy. Furthermore, we analyze the computational complexity, highlighting the linear scaling of our approach, discuss its novelty in the context of related work, and explore the interpretability offered by visualizing the learned Haar coefficients. Our results indicate that the LMWT achieves competitive performance while offering substantial computational advantages, positioning it as a promising and novel alternative for efficient sequence modeling.\n        △ Less",
        "authors":"['Andrew Kiruluta', 'Priscilla Burity', 'Samantha Williams']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring Gradient-Guided Masked Language Model to Detect Textual Adversarial Attacks",
        "abstract":"Textual adversarial examples pose serious threats to the reliability of natural language processing systems. Recent studies suggest that adversarial examples tend to deviate from the underlying manifold of normal texts, whereas pre-trained masked language models can approximate the manifold of normal data. These findings inspire the exploration of masked language models for detecting textual adversarial attacks. We first introduce Masked Language Model-based Detection (MLMD), leveraging the mask and unmask operations of the masked language modeling (MLM) objective to induce the difference in manifold changes between normal and adversarial texts. Although MLMD achieves competitive detection performance, its exhaustive one-by-one masking strategy introduces significant computational overhead. Our posterior analysis reveals that a significant number of non-keywords in the input are not important for detection but consume resources. Building on this, we introduce Gradient-guided MLMD (GradMLMD), which leverages gradient information to identify and skip non-keywords during detection, significantly reducing resource consumption without compromising detection performance.\n        △ Less",
        "authors":"['Xiaomei Zhang', 'Zhaoxi Zhang', 'Yanjun Zhang', 'Xufei Zheng', 'Leo Yu Zhang', 'Shengshan Hu', 'Shirui Pan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SLOs-Serve: Optimized Serving of Multi-SLO LLMs",
        "abstract":"This paper introduces SLOs-Serve, a system designed for serving multi-stage large language model (LLM) requests with application- and stage-specific service level objectives (SLOs). The key idea behind SLOs-Serve is to customize the allocation of tokens to meet these SLO requirements. SLOs-Serve uses a multi-SLO dynamic programming-based algorithm to continuously optimize token allocations under SLO constraints by exploring the full design space of chunked prefill and (optional) speculative decoding. Leveraging this resource planning algorithm, SLOs-Serve effectively supports multi-SLOs and multi-replica serving with dynamic request routing while being resilient to bursty arrivals. Our evaluation across 6 LLM application scenarios (including summarization, coding, chatbot, tool calling, and reasoning) demonstrates that SLOs-Serve improves per-GPU serving capacity by 2.2x on average compared to prior state-of-the-art systems.\n        △ Less",
        "authors":"['Siyuan Chen', 'Zhipeng Jia', 'Samira Khan', 'Arvind Krishnamurthy', 'Phillip B. Gibbons']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Embedding Hidden Adversarial Capabilities in Pre-Trained Diffusion Models",
        "abstract":"We introduce a new attack paradigm that embeds hidden adversarial capabilities directly into diffusion models via fine-tuning, without altering their observable behavior or requiring modifications during inference. Unlike prior approaches that target specific images or adjust the generation process to produce adversarial outputs, our method integrates adversarial functionality into the model itself. The resulting tampered model generates high-quality images indistinguishable from those of the original, yet these images cause misclassification in downstream classifiers at a high rate. The misclassification can be targeted to specific output classes. Users can employ this compromised model unaware of its embedded adversarial nature, as it functions identically to a standard diffusion model. We demonstrate the effectiveness and stealthiness of our approach, uncovering a covert attack vector that raises new security concerns. These findings expose a risk arising from the use of externally-supplied models and highlight the urgent need for robust model verification and defense mechanisms against hidden threats in generative models. The code is available at https:\/\/github.com\/LucasBeerens\/CRAFTed-Diffusion .\n        △ Less",
        "authors":"['Lucas Beerens', 'Desmond J. Higham']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How Relevance Emerges: Interpreting LoRA Fine-Tuning in Reranking LLMs",
        "abstract":"We conduct a behavioral exploration of LoRA fine-tuned LLMs for Passage Reranking to understand how relevance signals are learned and deployed by Large Language Models. By fine-tuning Mistral-7B, LLaMA3.1-8B, and Pythia-6.9B on MS MARCO under diverse LoRA configurations, we investigate how relevance modeling evolves across checkpoints, the impact of LoRA rank (1, 2, 8, 32), and the relative importance of updated MHA vs. MLP components. Our ablations reveal which layers and projections within LoRA transformations are most critical for reranking accuracy. These findings offer fresh explanations into LoRA's adaptation mechanisms, setting the stage for deeper mechanistic studies in Information Retrieval. All models used in this study have been shared.\n        △ Less",
        "authors":"['Atharva Nijasure', 'Tanya Chowdhury', 'James Allan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics",
        "abstract":"With the shift from traditional to digital media, the online landscape now hosts not only reliable news articles but also a significant amount of unreliable content. Digital media has faster reachability by significantly influencing public opinion and advancing political agendas. While newspaper readers may be familiar with their preferred outlets political leanings or credibility, determining unreliable news articles is much more challenging. The credibility of many online sources is often opaque, with AI generated content being easily disseminated at minimal cost. Unreliable news articles, particularly those that followed the Russian invasion of Ukraine in 2022, closely mimic the topics and writing styles of credible sources, making them difficult to distinguish. To address this, we introduce SemCAFE, a system designed to detect news reliability by incorporating entity relatedness into its assessment. SemCAFE employs standard Natural Language Processing techniques, such as boilerplate removal and tokenization, alongside entity level semantic analysis using the YAGO knowledge base. By creating a semantic fingerprint for each news article, SemCAFE could assess the credibility of 46,020 reliable and 3,407 unreliable articles on the 2022 Russian invasion of Ukraine. Our approach improved the macro F1 score by 12% over state of the art methods. The sample data and code are available on GitHub\n        △ Less",
        "authors":"['Gautam Kishore Shahi', 'Oshani Seneviratne', 'Marc Spaniol']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Counterfactual Inference under Thompson Sampling",
        "abstract":"Recommender systems exemplify sequential decision-making under uncertainty, strategically deciding what content to serve to users, to optimise a range of potential objectives. To balance the explore-exploit trade-off successfully, Thompson sampling provides a natural and widespread paradigm to probabilistically select which action to take. Questions of causal and counterfactual inference, which underpin use-cases like offline evaluation, are not straightforward to answer in these contexts. Specifically, whilst most existing estimators rely on action propensities, these are not readily available under Thompson sampling procedures.\n  We derive exact and efficiently computable expressions for action propensities under a variety of parameter and outcome distributions, enabling the use of off-policy estimators in Thompson sampling scenarios. This opens up a range of practical use-cases where counterfactual inference is crucial, including unbiased offline evaluation of recommender systems, as well as general applications of causal inference in online advertising, personalisation, and beyond.\n        △ Less",
        "authors":"['Olivier Jeunen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning",
        "abstract":"In offline reinforcement learning (RL), learning from fixed datasets presents a promising solution for domains where real-time interaction with the environment is expensive or risky. However, designing dense reward signals for offline dataset requires significant human effort and domain expertise. Reinforcement learning with human feedback (RLHF) has emerged as an alternative, but it remains costly due to the human-in-the-loop process, prompting interest in automated reward generation models. To address this, we propose Reward Generation via Large Vision-Language Models (RG-VLM), which leverages the reasoning capabilities of LVLMs to generate rewards from offline data without human involvement. RG-VLM improves generalization in long-horizon tasks and can be seamlessly integrated with the sparse reward signals to enhance task performance, demonstrating its potential as an auxiliary reward signal.\n        △ Less",
        "authors":"['Younghwan Lee', 'Tung M. Luu', 'Donghoon Lee', 'Chang D. Yoo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"High-order expansion of Neural Ordinary Differential Equations flows",
        "abstract":"Artificial neural networks, widely recognised for their role in machine learning, are now transforming the study of ordinary differential equations (ODEs), bridging data-driven modelling with classical dynamical systems and enabling the development of infinitely deep neural models. However, the practical applicability of these models remains constrained by the opacity of their learned dynamics, which operate as black-box systems with limited explainability, thereby hindering trust in their deployment. Existing approaches for the analysis of these dynamical systems are predominantly restricted to first-order gradient information due to computational constraints, thereby limiting the depth of achievable insight. Here, we introduce Event Transition Tensors, a framework based on high-order differentials that provides a rigorous mathematical description of neural ODE dynamics on event manifolds. We demonstrate its versatility across diverse applications: characterising uncertainties in a data-driven prey-predator control model, analysing neural optimal feedback dynamics, and mapping landing trajectories in a three-body neural Hamiltonian system. In all cases, our method enhances the interpretability and rigour of neural ODEs by expressing their behaviour through explicit mathematical structures. Our findings contribute to a deeper theoretical foundation for event-triggered neural differential equations and provide a mathematical construct for explaining complex system dynamics.\n        △ Less",
        "authors":"['Dario Izzo', 'Sebastien Origer', 'Giacomo Acciarini', 'Francesco Biscani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards scientific machine learning for granular material simulations -- challenges and opportunities",
        "abstract":"Micro-scale mechanisms, such as inter-particle and particle-fluid interactions, govern the behaviour of granular systems. While particle-scale simulations provide detailed insights into these interactions, their computational cost is often prohibitive. Attended by researchers from both the granular materials (GM) and machine learning (ML) communities, a recent Lorentz Center Workshop on \"Machine Learning for Discrete Granular Media\" brought the ML community up to date with GM challenges.\n  This position paper emerged from the workshop discussions. We define granular materials and identify seven key challenges that characterise their distinctive behaviour across various scales and regimes, ranging from gas-like to fluid-like and solid-like. Addressing these challenges is essential for developing robust and efficient digital twins for granular systems in various industrial applications. To showcase the potential of ML to the GM community, we present classical and emerging machine\/deep learning techniques that have been, or could be, applied to granular materials. We reviewed sequence-based learning models for path-dependent constitutive behaviour, followed by encoder-decoder type models for representing high-dimensional data. We then explore graph neural networks and recent advances in neural operator learning. Lastly, we discuss model-order reduction and probabilistic learning techniques for high-dimensional parameterised systems, which are crucial for quantifying uncertainties arising from physics-based and data-driven models.\n  We present a workflow aimed at unifying data structures and modelling pipelines and guiding readers through the selection, training, and deployment of ML surrogates for granular material simulations. Finally, we illustrate the workflow's practical use with two representative examples, focusing on granular materials in solid-like and fluid-like regimes.\n        △ Less",
        "authors":"['Marc Fransen', 'Andreas Fürst', 'Deepak Tunuguntla', 'Daniel N. Wilke', 'Benedikt Alkin', 'Daniel Barreto', 'Johannes Brandstetter', 'Miguel Angel Cabrera', 'Xinyan Fan', 'Mengwu Guo', 'Bram Kieskamp', 'Krishna Kumar', 'John Morrissey', 'Jonathan Nuttall', 'Jin Ooi', 'Luisa Orozco', 'Stefanos-Aldo Papanicolopulos', 'Tongming Qu', 'Dingena Schott', 'Takayuki Shuku', 'WaiChing Sun', 'Thomas Weinhart', 'Dongwei Ye', 'Hongyang Cheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"WebMap -- Large Language Model-assisted Semantic Link Induction in the Web",
        "abstract":"Carrying out research tasks is only inadequately supported, if not hindered, by current web search engines. This paper therefore proposes functional extensions of WebMap, a semantically induced overlay linking structure on the web to inherently facilitate research activities. These add-ons support the dynamic determination and regrouping of document clusters, the creation of a semantic signpost in the web, and the interactive tracing of topics back to their origins.\n        △ Less",
        "authors":"['Shiraj Pokharel', 'Georg P. Roßrucker', 'Mario M. Kubek']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Survey of Multimodal Retrieval-Augmented Generation",
        "abstract":"Multimodal Retrieval-Augmented Generation (MRAG) enhances large language models (LLMs) by integrating multimodal data (text, images, videos) into retrieval and generation processes, overcoming the limitations of text-only Retrieval-Augmented Generation (RAG). While RAG improves response accuracy by incorporating external textual knowledge, MRAG extends this framework to include multimodal retrieval and generation, leveraging contextual information from diverse data types. This approach reduces hallucinations and enhances question-answering systems by grounding responses in factual, multimodal knowledge. Recent studies show MRAG outperforms traditional RAG, especially in scenarios requiring both visual and textual understanding. This survey reviews MRAG's essential components, datasets, evaluation methods, and limitations, providing insights into its construction and improvement. It also identifies challenges and future research directions, highlighting MRAG's potential to revolutionize multimodal information retrieval and generation. By offering a comprehensive perspective, this work encourages further exploration into this promising paradigm.\n        △ Less",
        "authors":"['Lang Mei', 'Siyu Mo', 'Zhihan Yang', 'Chong Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamic Topic Analysis in Academic Journals using Convex Non-negative Matrix Factorization Method",
        "abstract":"With the rapid advancement of large language models, academic topic identification and topic evolution analysis are crucial for enhancing AI's understanding capabilities. Dynamic topic analysis provides a powerful approach to capturing and understanding the temporal evolution of topics in large-scale datasets. This paper presents a two-stage dynamic topic analysis framework that incorporates convex optimization to improve topic consistency, sparsity, and interpretability. In Stage 1, a two-layer non-negative matrix factorization (NMF) model is employed to extract annual topics and identify key terms. In Stage 2, a convex optimization algorithm refines the dynamic topic structure using the convex NMF (cNMF) model, further enhancing topic integration and stability. Applying the proposed method to IEEE journal abstracts from 2004 to 2022 effectively identifies and quantifies emerging research topics, such as COVID-19 and digital twins. By optimizing sparsity differences in the clustering feature space between traditional and emerging research topics, the framework provides deeper insights into topic evolution and ranking analysis. Moreover, the NMF-cNMF model demonstrates superior stability in topic consistency. At sparsity levels of 0.4, 0.6, and 0.9, the proposed approach improves topic ranking stability by 24.51%, 56.60%, and 36.93%, respectively. The source code (to be open after publication) is available at https:\/\/github.com\/meetyangyang\/CDNMF.\n        △ Less",
        "authors":"['Yang Yang', 'Tong Zhang', 'Jian Wu', 'Lijie Su']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DEEP: Edge-based Dataflow Processing with Hybrid Docker Hub and Regional Registries",
        "abstract":"Reducing energy consumption is essential to lessen greenhouse gas emissions, conserve natural resources, and help mitigate the impacts of climate change. In this direction, edge computing, a complementary technology to cloud computing, extends computational capabilities closer to the data producers, enabling energy-efficient and latency-sensitive service delivery for end users. To properly manage data and microservice storage, expanding the Docker Hub registry to the edge using an AWS S3-compatible MinIO-based object storage service can reduce completion time and energy consumption. To address this, we introduce Docker rEgistry-based Edge dataflow Processing (DEEP) to optimize the energy consumption of microservice-based application deployments by focusing on deployments from Docker Hub and MinIO-based regional registries and their processing on edge devices. After applying nash equilibrium and benchmarking the execution of two compute-intensive machine learning (ML) applications of video and text processing, we compare energy consumption across three deployment scenarios: exclusively from Docker Hub, exclusively from the regional registry, and a hybrid method utilizing both. Experimental results show that deploying 83% of text processing microservices from the regional registry improves the energy consumption by 0.34% (18J) compared to microservice deployments exclusively from Docker Hub.\n        △ Less",
        "authors":"['Narges Mehran', 'Zahra Najafabadi Samani', 'Reza Farahani', 'Josef Hammer', 'Dragi Kimovski']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Recommendation System in Advertising and Streaming Media: Unsupervised Data Enhancement Sequence Suggestions",
        "abstract":"Sequential recommendation is an extensively explored approach to capturing users' evolving preferences based on past interactions, aimed at predicting their next likely choice. Despite significant advancements in this domain, including methods based on RNNs and self-attention, challenges like limited supervised signals and noisy data caused by unintentional clicks persist. To address these challenges, some studies have incorporated unsupervised learning by leveraging local item contexts within individual sequences. However, these methods often overlook the intricate associations between items across multiple sequences and are susceptible to noise in item co-occurrence patterns. In this context, we introduce a novel framework, Global Unsupervised Data-Augmentation (UDA4SR), which adopts a graph contrastive learning perspective to generate more robust item embeddings for sequential recommendation. Our approach begins by integrating Generative Adversarial Networks (GANs) for data augmentation, which serves as the first step to enhance the diversity and richness of the training data. Then, we build a Global Item Relationship Graph (GIG) based on all user interaction sequences. Subsequently, we employ graph contrastive learning on the refined graph to enhance item embeddings by capturing complex global associations. To model users' dynamic and diverse interests more effectively, we enhance the CapsNet module with a novel target-attention mechanism. Extensive experiments show that UDA4SR significantly outperforms state-of-the-art approaches.\n        △ Less",
        "authors":"['Kowei Shih', 'Yi Han', 'Li Tan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-Driven Sentiment Analytics: Unlocking Business Value in the E-Commerce Landscape_v1",
        "abstract":"The rapid growth of e-commerce has led to an overwhelming volume of customer feedback, from product reviews to service interactions. Extracting meaningful insights from this data is crucial for businesses aiming to improve customer satisfaction and optimize decision-making. This paper presents an AI-driven sentiment analysis system designed specifically for e-commerce applications, balancing accuracy with interpretability. Our approach integrates traditional machine learning techniques with modern deep learning models, allowing for a more nuanced understanding of customer sentiment while ensuring transparency in decision-making. Experimental results show that our system outperforms standard sentiment analysis methods, achieving an accuracy of 89.7% on diverse, large-scale datasets. Beyond technical performance, real-world implementation across multiple e-commerce platforms demonstrates tangible improvements in customer engagement and operational efficiency. This study highlights both the potential and the challenges of applying AI to sentiment analysis in a commercial setting, offering insights into practical deployment strategies and areas for future refinement.\n        △ Less",
        "authors":"['Qianye Wu', 'Chengxuan Xia', 'Sixuan Tian']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dimension reduction for derivative-informed operator learning: An analysis of approximation errors",
        "abstract":"We study the derivative-informed learning of nonlinear operators between infinite-dimensional separable Hilbert spaces by neural networks. Such operators can arise from the solution of partial differential equations (PDEs), and are used in many simulation-based outer-loop tasks in science and engineering, such as PDE-constrained optimization, Bayesian inverse problems, and optimal experimental design. In these settings, the neural network approximations can be used as surrogate models to accelerate the solution of the outer-loop tasks. However, since outer-loop tasks in infinite dimensions often require knowledge of the underlying geometry, the approximation accuracy of the operator's derivatives can also significantly impact the performance of the surrogate model. Motivated by this, we analyze the approximation errors of neural operators in Sobolev norms over infinite-dimensional Gaussian input measures. We focus on the reduced basis neural operator (RBNO), which uses linear encoders and decoders defined on dominant input\/output subspaces spanned by reduced sets of orthonormal bases. To this end, we study two methods for generating the bases; principal component analysis (PCA) and derivative-informed subspaces (DIS), which use the dominant eigenvectors of the covariance of the data or the derivatives as the reduced bases, respectively. We then derive bounds for errors arising from both the dimension reduction and the latent neural network approximation, including the sampling errors associated with the empirical estimation of the PCA\/DIS. Our analysis is validated on numerical experiments with elliptic PDEs, where our results show that bases informed by the map (i.e., DIS or output PCA) yield accurate reconstructions and generalization errors for both the operator and its derivatives, while input PCA may underperform unless ranks and training sample sizes are sufficiently large.\n        △ Less",
        "authors":"['Dingcheng Luo', \"Thomas O'Leary-Roseberry\", 'Peng Chen', 'Omar Ghattas']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Steering CLIP's vision transformer with sparse autoencoders",
        "abstract":"While vision models are highly capable, their internal mechanisms remain poorly understood -- a challenge which sparse autoencoders (SAEs) have helped address in language, but which remains underexplored in vision. We address this gap by training SAEs on CLIP's vision transformer and uncover key differences between vision and language processing, including distinct sparsity patterns for SAEs trained across layers and token types. We then provide the first systematic analysis on the steerability of CLIP's vision transformer by introducing metrics to quantify how precisely SAE features can be steered to affect the model's output. We find that 10-15\\% of neurons and features are steerable, with SAEs providing thousands more steerable features than the base model. Through targeted suppression of SAE features, we then demonstrate improved performance on three vision disentanglement tasks (CelebA, Waterbirds, and typographic attacks), finding optimal disentanglement in middle model layers, and achieving state-of-the-art performance on defense against typographic attacks.\n        △ Less",
        "authors":"['Sonia Joseph', 'Praneet Suresh', 'Ethan Goldfarb', 'Lorenz Hufe', 'Yossi Gandelsman', 'Robert Graham', 'Danilo Bzdok', 'Wojciech Samek', 'Blake Aaron Richards']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DocAgent: A Multi-Agent System for Automated Code Documentation Generation",
        "abstract":"High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.\n        △ Less",
        "authors":"['Dayu Yang', 'Antoine Simoulin', 'Xin Qian', 'Xiaoyi Liu', 'Yuwei Cao', 'Zhaopu Teng', 'Grey Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Surrogate-based optimization of system architectures subject to hidden constraints",
        "abstract":"The exploration of novel architectures requires physics-based simulation due to a lack of prior experience to start from, which introduces two specific challenges for optimization algorithms: evaluations become more expensive (in time) and evaluations might fail. The former challenge is addressed by Surrogate-Based Optimization (SBO) algorithms, in particular Bayesian Optimization (BO) using Gaussian Process (GP) models. An overview is provided of how BO can deal with challenges specific to architecture optimization, such as design variable hierarchy and multiple objectives: specific measures include ensemble infills and a hierarchical sampling algorithm. Evaluations might fail due to non-convergence of underlying solvers or infeasible geometry in certain areas of the design space. Such failed evaluations, also known as hidden constraints, pose a particular challenge to SBO\/BO, as the surrogate model cannot be trained on empty results. This work investigates various strategies for satisfying hidden constraints in BO algorithms. Three high-level strategies are identified: rejection of failed points from the training set, replacing failed points based on viable (non-failed) points, and predicting the failure region. Through investigations on a set of test problems including a jet engine architecture optimization problem, it is shown that best performance is achieved with a mixed-discrete GP to predict the Probability of Viability (PoV), and by ensuring selected infill points satisfy some minimum PoV threshold. This strategy is demonstrated by solving a jet engine architecture problem that features at 50% failure rate and could not previously be solved by a BO algorithm. The developed BO algorithm and used test problems are available in the open-source Python library SBArchOpt.\n        △ Less",
        "authors":"['Jasper Bussemaker', 'Paul Saves', 'Nathalie Bartoli', 'Thierry Lefebvre', 'Björn Nagel']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generating Fine Details of Entity Interactions",
        "abstract":"Images not only depict objects but also encapsulate rich interactions between them. However, generating faithful and high-fidelity images involving multiple entities interacting with each other, is a long-standing challenge. While pre-trained text-to-image models are trained on large-scale datasets to follow diverse text instructions, they struggle to generate accurate interactions, likely due to the scarcity of training data for uncommon object interactions. This paper introduces InterActing, an interaction-focused dataset with 1000 fine-grained prompts covering three key scenarios: (1) functional and action-based interactions, (2) compositional spatial relationships, and (3) multi-subject interactions. To address interaction generation challenges, we propose a decomposition-augmented refinement procedure. Our approach, DetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose interactions into finer-grained concepts, uses a VLM to critique generated images, and applies targeted interventions within the diffusion process in refinement. Automatic and human evaluations show significantly improved image quality, demonstrating the potential of enhanced inference strategies. Our dataset and code are available at https:\/\/concepts-ai.com\/p\/detailscribe\/ to facilitate future exploration of interaction-rich image generation.\n        △ Less",
        "authors":"['Xinyi Gu', 'Jiayuan Mao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning",
        "abstract":"Deep learning-based electrocardiogram (ECG) classification has shown impressive performance but clinical adoption has been slowed by the lack of transparent and faithful explanations. Post hoc methods such as saliency maps may fail to reflect a model's true decision process. Prototype-based reasoning offers a more transparent alternative by grounding decisions in similarity to learned representations of real ECG segments, enabling faithful, case-based explanations. We introduce ProtoECGNet, a prototype-based deep learning model for interpretable, multi-label ECG classification. ProtoECGNet employs a structured, multi-branch architecture that reflects clinical interpretation workflows: it integrates a 1D CNN with global prototypes for rhythm classification, a 2D CNN with time-localized prototypes for morphology-based reasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each branch is trained with a prototype loss designed for multi-label learning, combining clustering, separation, diversity, and a novel contrastive loss that encourages appropriate separation between prototypes of unrelated classes while allowing clustering for frequently co-occurring diagnoses. We evaluate ProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating competitive performance relative to state-of-the-art black-box models while providing structured, case-based explanations. To assess prototype quality, we conduct a structured clinician review of the final model's projected prototypes, finding that they are rated as representative and clear. ProtoECGNet shows that prototype learning can be effectively scaled to complex, multi-label time-series classification, offering a practical path toward transparent and trustworthy deep learning models for clinical decision support.\n        △ Less",
        "authors":"['Sahil Sethi', 'David Chen', 'Thomas Statchen', 'Michael C. Burkhart', 'Nipun Bhandari', 'Bashar Ramadan', 'Brett Beaulieu-Jones']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond Black-Box Predictions: Identifying Marginal Feature Effects in Tabular Transformer Networks",
        "abstract":"In recent years, deep neural networks have showcased their predictive power across a variety of tasks. Beyond natural language processing, the transformer architecture has proven efficient in addressing tabular data problems and challenges the previously dominant gradient-based decision trees in these areas. However, this predictive power comes at the cost of intelligibility: Marginal feature effects are almost completely lost in the black-box nature of deep tabular transformer networks. Alternative architectures that use the additivity constraints of classical statistical regression models can maintain intelligible marginal feature effects, but often fall short in predictive power compared to their more complex counterparts. To bridge the gap between intelligibility and performance, we propose an adaptation of tabular transformer networks designed to identify marginal feature effects. We provide theoretical justifications that marginal feature effects can be accurately identified, and our ablation study demonstrates that the proposed model efficiently detects these effects, even amidst complex feature interactions. To demonstrate the model's predictive capabilities, we compare it to several interpretable as well as black-box models and find that it can match black-box performances while maintaining intelligibility. The source code is available at https:\/\/github.com\/OpenTabular\/NAMpy.\n        △ Less",
        "authors":"['Anton Thielmann', 'Arik Reuter', 'Benjamin Saefken']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing",
        "abstract":"Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values.\n        △ Less",
        "authors":"['Vinal Asodia', 'Zhenhua Feng', 'Saber Fallah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow",
        "abstract":"Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE agents, have made tremendous progress (>60% on SWE-Bench Verified) on real-world coding challenges including GitHub issue resolution. SWE agents use a combination of reasoning, environment interaction and self-reflection to resolve issues thereby generating \"trajectories\". Analysis of SWE agent trajectories is difficult, not only as they exceed LLM sequence length (sometimes, greater than 128k) but also because it involves a relatively prolonged interaction between an LLM and the environment managed by the agent. In case of an agent error, it can be hard to decipher, locate and understand its scope. Similarly, it can be hard to track improvements or regression over multiple runs or experiments. While a lot of research has gone into making these SWE agents reach state-of-the-art, much less focus has been put into creating tools to help analyze and visualize agent output. We propose a novel tool called SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow, with a vision to assist SWE-agent researchers to visualize and inspect their experiments. SeaView's novel mechanisms help compare experimental runs with varying hyper-parameters or LLMs, and quickly get an understanding of LLM or environment related problems. Based on our user study, experienced researchers spend between 10 and 30 minutes to gather the information provided by SeaView, while researchers with little experience can spend between 30 minutes to 1 hour to diagnose their experiment.\n        △ Less",
        "authors":"['Timothy Bula', 'Saurabh Pujar', 'Luca Buratti', 'Mihaela Bornea', 'Avirup Sil']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design",
        "abstract":"Multidisciplinary design optimization methods aim at adapting numerical optimization techniques to the design of engineering systems involving multiple disciplines. In this context, a large number of mixed continuous, integer and categorical variables might arise during the optimization process and practical applications involve a large number of design variables. Recently, there has been a growing interest in mixed variables constrained Bayesian optimization but most existing approaches severely increase the number of the hyperparameters related to the surrogate model. In this paper, we address this issue by constructing surrogate models using less hyperparameters. The reduction process is based on the partial least squares method. An adaptive procedure for choosing the number of hyperparameters is proposed. The performance of the proposed approach is confirmed on analytical tests as well as two real applications related to aircraft design. A significant improvement is obtained compared to genetic algorithms.\n        △ Less",
        "authors":"['Paul Saves', 'Nathalie Bartoli', 'Youssef Diouane', 'Thierry Lefebvre', 'Joseph Morlier', 'Christophe David', 'Eric Nguyen Van', 'Sébastien Defoort']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning",
        "abstract":"Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https:\/\/github.com\/xufangzhi\/Genius.\n        △ Less",
        "authors":"['Fangzhi Xu', 'Hang Yan', 'Chang Ma', 'Haiteng Zhao', 'Qiushi Sun', 'Kanzhi Cheng', 'Junxian He', 'Jun Liu', 'Zhiyong Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Regularized infill criteria for multi-objective Bayesian optimization with application to aircraft design",
        "abstract":"Bayesian optimization is an advanced tool to perform ecient global optimization It consists on enriching iteratively surrogate Kriging models of the objective and the constraints both supposed to be computationally expensive of the targeted optimization problem Nowadays efficient extensions of Bayesian optimization to solve expensive multiobjective problems are of high interest The proposed method in this paper extends the super efficient global optimization with mixture of experts SEGOMOE to solve constrained multiobjective problems To cope with the illposedness of the multiobjective inll criteria different enrichment procedures using regularization techniques are proposed The merit of the proposed approaches are shown on known multiobjective benchmark problems with and without constraints The proposed methods are then used to solve a biobjective application related to conceptual aircraft design with ve unknown design variables and three nonlinear inequality constraints The preliminary results show a reduction of the total cost in terms of function evaluations by a factor of 20 compared to the evolutionary algorithm NSGA-II.\n        △ Less",
        "authors":"['Robin Grapin', 'Youssef Diouane', 'Joseph Morlier', 'Nathalie Bartoli', 'Thierry Lefebvre', 'Paul Saves', 'Jasper Bussemaker']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Channel Estimation by Infinite Width Convolutional Networks",
        "abstract":"In wireless communications, estimation of channels in OFDM systems spans frequency and time, which relies on sparse collections of pilot data, posing an ill-posed inverse problem. Moreover, deep learning estimators require large amounts of training data, computational resources, and true channels to produce accurate channel estimates, which are not realistic. To address this, a convolutional neural tangent kernel (CNTK) is derived from an infinitely wide convolutional network whose training dynamics can be expressed by a closed-form equation. This CNTK is used to impute the target matrix and estimate the missing channel response using only the known values available at pilot locations. This is a promising solution for channel estimation that does not require a large training set. Numerical results on realistic channel datasets demonstrate that our strategy accurately estimates the channels without a large dataset and significantly outperforms deep learning methods in terms of speed, accuracy, and computational resources.\n        △ Less",
        "authors":"['Mohammed Mallik', 'Guillaume Villemaud']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Application of machine learning models to predict the relationship between air pollution, ecosystem degradation, and health disparities and lung cancer in Vietnam",
        "abstract":"Lung cancer is one of the major causes of death worldwide, and Vietnam is not an exception. This disease is the second most common type of cancer globally and the second most common cause of death in Vietnam, just after liver cancer, with 23,797 fatal cases and 26,262 new cases, or 14.4% of the disease in 2020. Recently, with rising disease rates in Vietnam causing a huge public health burden, lung cancer continues to hold the top position in attention and care. Especially together with climate change, under a variety of types of pollution, deforestation, and modern lifestyles, lung cancer risks are on red alert, particularly in Vietnam. To understand more about the severe disease sources in Vietnam from a diversity of key factors, including environmental features and the current health state, with a particular emphasis on Vietnam's distinct socioeconomic and ecological context, we utilize large datasets such as patient health records and environmental indicators containing necessary information, such as deforestation rate, green cover rate, air pollution, and lung cancer risks, that is collected from well-known governmental sharing websites. Then, we process and connect them and apply analytical methods (heatmap, information gain, p-value, spearman correlation) to determine causal correlations influencing lung cancer risks. Moreover, we deploy machine learning (ML) models (Decision Tree, Random Forest, Support Vector Machine, K-mean clustering) to discover cancer risk patterns. Our experimental results, leveraged by the aforementioned ML models to identify the disease patterns, are promising, particularly, the models as Random Forest, SVM, and PCA are working well on the datasets and give high accuracy (99%), however, the K means clustering has very low accuracy (10%) and does not fit the datasets.\n        △ Less",
        "authors":"['Ngoc Hong Tran', 'Lan Kim Vien', 'Ngoc-Thao Thi Le']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transformer Learns Optimal Variable Selection in Group-Sparse Classification",
        "abstract":"Transformers have demonstrated remarkable success across various applications. However, the success of transformers have not been understood in theory. In this work, we give a case study of how transformers can be trained to learn a classic statistical model with \"group sparsity\", where the input variables form multiple groups, and the label only depends on the variables from one of the groups. We theoretically demonstrate that, a one-layer transformer trained by gradient descent can correctly leverage the attention mechanism to select variables, disregarding irrelevant ones and focusing on those beneficial for classification. We also demonstrate that a well-pretrained one-layer transformer can be adapted to new downstream tasks to achieve good prediction accuracy with a limited number of samples. Our study sheds light on how transformers effectively learn structured data.\n        △ Less",
        "authors":"['Chenyang Zhang', 'Xuran Meng', 'Yuan Cao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines",
        "abstract":"One of the key safety considerations of battery manufacturing is thermal runaway, the uncontrolled increase in temperature which can lead to fires, explosions, and emissions of toxic gasses. As such, development of automated systems capable of detecting such events is of considerable importance in both academic and industrial contexts. In this work, we investigate the use of deep learning for detecting thermal runaway in the battery production line of VDL Nedcar, a Dutch automobile manufacturer. Specifically, we collect data from the production line to represent both baseline (non thermal runaway) and thermal runaway conditions. Thermal runaway was simulated through the use of external heat and smoke sources. The data consisted of both optical and thermal images which were then preprocessed and fused before serving as input to our models. In this regard, we evaluated three deep-learning models widely used in computer vision including shallow convolutional neural networks, residual neural networks, and vision transformers on two performance metrics. Furthermore, we evaluated these models using explainability methods to gain insight into their ability to capture the relevant feature information from their inputs. The obtained results indicate that the use of deep learning is a viable approach to thermal runaway detection in battery production lines.\n        △ Less",
        "authors":"['Athanasios Athanasopoulos', 'Matúš Mihalák', 'Marcin Pietrasik']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Gradient Descent Robustly Learns the Intrinsic Dimension of Data in Training Convolutional Neural Networks",
        "abstract":"Modern neural networks are usually highly over-parameterized. Behind the wide usage of over-parameterized networks is the belief that, if the data are simple, then the trained network will be automatically equivalent to a simple predictor. Following this intuition, many existing works have studied different notions of \"ranks\" of neural networks and their relation to the rank of data. In this work, we study the rank of convolutional neural networks (CNNs) trained by gradient descent, with a specific focus on the robustness of the rank to image background noises. Specifically, we point out that, when adding background noises to images, the rank of the CNN trained with gradient descent is affected far less compared with the rank of the data. We support our claim with a theoretical case study, where we consider a particular data model to characterize low-rank clean images with added background noises. We prove that CNNs trained by gradient descent can learn the intrinsic dimension of clean images, despite the presence of relatively large background noises. We also conduct experiments on synthetic and real datasets to further validate our claim.\n        △ Less",
        "authors":"['Chenyang Zhang', 'Peifeng Gao', 'Difan Zou', 'Yuan Cao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Task-conditioned Ensemble of Expert Models for Continuous Learning",
        "abstract":"One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed model with new data could be one remedy. However, the question arises as to how we should update the model with new training data so that it retains its accuracy on the old data while adapting to the new data. In this work, we propose a task-conditioned ensemble of models to maintain the performance of the existing model. The method involves an ensemble of expert models based on task membership information. The in-domain models-based on the local outlier concept (different from the expert models) provide task membership information dynamically at run-time to each probe sample. To evaluate the proposed method, we experiment with three setups: the first represents distribution shift between tasks (LivDet-Iris-2017), the second represents distribution shift both between and within tasks (LivDet-Iris-2020), and the third represents disjoint distribution between tasks (Split MNIST). The experiments highlight the benefits of the proposed method. The source code is available at https:\/\/github.com\/iPRoBe-lab\/Continuous_Learning_FE_DM.\n        △ Less",
        "authors":"['Renu Sharma', 'Debasmita Pal', 'Arun Ross']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MooseAgent: A LLM Based Multi-agent Framework for Automating Moose Simulation",
        "abstract":"The Finite Element Method (FEM) is widely used in engineering and scientific computing, but its pre-processing, solver configuration, and post-processing stages are often time-consuming and require specialized knowledge. This paper proposes an automated solution framework, MooseAgent, for the multi-physics simulation framework MOOSE, which combines large-scale pre-trained language models (LLMs) with a multi-agent system. The framework uses LLMs to understand user-described simulation requirements in natural language and employs task decomposition and multi-round iterative verification strategies to automatically generate MOOSE input files. To improve accuracy and reduce model hallucinations, the system builds and utilizes a vector database containing annotated MOOSE input cards and function documentation. We conducted experimental evaluations on several typical cases, including heat transfer, mechanics, phase field, and multi-physics coupling. The results show that MooseAgent can automate the MOOSE simulation process to a certain extent, especially demonstrating a high success rate when dealing with relatively simple single-physics problems. The main contribution of this research is the proposal of a multi-agent automated framework for MOOSE, which validates its potential in simplifying finite element simulation processes and lowering the user barrier, providing new ideas for the development of intelligent finite element simulation software. The code for the MooseAgent framework proposed in this paper has been open-sourced and is available at https:\/\/github.com\/taozhan18\/MooseAgent\n        △ Less",
        "authors":"['Tao Zhang', 'Zhenhai Liu', 'Yong Xin', 'Yongjun Jiao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English",
        "abstract":"The dissemination of online hate speech can have serious negative consequences for individuals, online communities, and entire societies. This and the large volume of hateful online content prompted both practitioners', i.e., in content moderation or law enforcement, and researchers' interest in machine learning models to automatically classify instances of hate speech. Whereas most scientific works address hate speech classification as a binary task, practice often requires a differentiation into sub-types, e.g., according to target, severity, or legality, which may overlap for individual content. Hence, researchers created datasets and machine learning models that approach hate speech classification in textual data as a multi-label problem. This work presents the first systematic and comprehensive survey of scientific literature on this emerging research landscape in English (N=46). We contribute with a concise overview of 28 datasets suited for training multi-label classification models that reveals significant heterogeneity regarding label-set, size, meta-concept, annotation process, and inter-annotator agreement. Our analysis of 24 publications proposing suitable classification models further establishes inconsistency in evaluation and a preference for architectures based on Bidirectional Encoder Representation from Transformers (BERT) and Recurrent Neural Networks (RNNs). We identify imbalanced training data, reliance on crowdsourcing platforms, small and sparse datasets, and missing methodological alignment as critical open issues and formulate ten recommendations for research.\n        △ Less",
        "authors":"['Julian Bäumler', 'Louis Blöcher', 'Lars-Joel Frey', 'Xian Chen', 'Markus Bayer', 'Christian Reuter']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Fidelity Calibration for Informative Sim-to-Real Adaptation",
        "abstract":"Deep reinforcement learning can seamlessly transfer agile locomotion and navigation skills from the simulator to real world. However, bridging the sim-to-real gap with domain randomization or adversarial methods often demands expert physics knowledge to ensure policy robustness. Even so, cutting-edge simulators may fall short of capturing every real-world detail, and the reconstructed environment may introduce errors due to various perception uncertainties. To address these challenges, we propose Neural Fidelity Calibration (NFC), a novel framework that employs conditional score-based diffusion models to calibrate simulator physical coefficients and residual fidelity domains online during robot execution. Specifically, the residual fidelity reflects the simulation model shift relative to the real-world dynamics and captures the uncertainty of the perceived environment, enabling us to sample realistic environments under the inferred distribution for policy fine-tuning. Our framework is informative and adaptive in three key ways: (a) we fine-tune the pretrained policy only under anomalous scenarios, (b) we build sequential NFC online with the pretrained NFC's proposal prior, reducing the diffusion model's training burden, and (c) when NFC uncertainty is high and may degrade policy improvement, we leverage optimistic exploration to enable hallucinated policy optimization. Our framework achieves superior simulator calibration precision compared to state-of-the-art methods across diverse robots with high-dimensional parametric spaces. We study the critical contribution of residual fidelity to policy improvement in simulation and real-world experiments. Notably, our approach demonstrates robust robot navigation under challenging real-world conditions, such as a broken wheel axle on snowy surfaces.\n        △ Less",
        "authors":"['Youwei Yu', 'Lantao Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On Background Bias of Post-Hoc Concept Embeddings in Computer Vision DNNs",
        "abstract":"The thriving research field of concept-based explainable artificial intelligence (C-XAI) investigates how human-interpretable semantic concepts embed in the latent spaces of deep neural networks (DNNs). Post-hoc approaches therein use a set of examples to specify a concept, and determine its embeddings in DNN latent space using data driven techniques. This proved useful to uncover biases between different target (foreground or concept) classes. However, given that the background is mostly uncontrolled during training, an important question has been left unattended so far: Are\/to what extent are state-of-the-art, data-driven post-hoc C-XAI approaches themselves prone to biases with respect to their backgrounds? E.g., wild animals mostly occur against vegetation backgrounds, and they seldom appear on roads. Even simple and robust C-XAI methods might abuse this shortcut for enhanced performance. A dangerous performance degradation of the concept-corner cases of animals on the road could thus remain undiscovered. This work validates and thoroughly confirms that established Net2Vec-based concept segmentation techniques frequently capture background biases, including alarming ones, such as underperformance on road scenes. For the analysis, we compare 3 established techniques from the domain of background randomization on >50 concepts from 2 datasets, and 7 diverse DNN architectures. Our results indicate that even low-cost setups can provide both valuable insight and improved background robustness.\n        △ Less",
        "authors":"['Gesina Schwalbe', 'Georgii Mikriukov', 'Edgar Heinert', 'Stavros Gerolymatos', 'Mert Keser', 'Alois Knoll', 'Matthias Rottmann', 'Annika Mütze']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ready, Bid, Go! On-Demand Delivery Using Fleets of Drones with Unknown, Heterogeneous Energy Storage Constraints",
        "abstract":"Unmanned Aerial Vehicles (UAVs) are expected to transform logistics, reducing delivery time, costs, and emissions. This study addresses an on-demand delivery , in which fleets of UAVs are deployed to fulfil orders that arrive stochastically. Unlike previous work, it considers UAVs with heterogeneous, unknown energy storage capacities and assumes no knowledge of the energy consumption models. We propose a decentralised deployment strategy that combines auction-based task allocation with online learning. Each UAV independently decides whether to bid for orders based on its energy storage charge level, the parcel mass, and delivery distance. Over time, it refines its policy to bid only for orders within its capability. Simulations using realistic UAV energy models reveal that, counter-intuitively, assigning orders to the least confident bidders reduces delivery times and increases the number of successfully fulfilled orders. This strategy is shown to outperform threshold-based methods which require UAVs to exceed specific charge levels at deployment. We propose a variant of the strategy which uses learned policies for forecasting. This enables UAVs with insufficient charge levels to commit to fulfilling orders at specific future times, helping to prioritise early orders. Our work provides new insights into long-term deployment of UAV swarms, highlighting the advantages of decentralised energy-aware decision-making coupled with online learning in real-world dynamic environments.\n        △ Less",
        "authors":"['Mohamed S. Talamali', 'Genki Miyauchi', 'Thomas Watteyne', 'Micael S. Couceiro', 'Roderich Gross']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Boosting multi-demographic federated learning for chest x-ray analysis using general-purpose self-supervised representations",
        "abstract":"Reliable artificial intelligence (AI) models for medical image analysis often depend on large and diverse labeled datasets. Federated learning (FL) offers a decentralized and privacy-preserving approach to training but struggles in highly non-independent and identically distributed (non-IID) settings, where institutions with more representative data may experience degraded performance. Moreover, existing large-scale FL studies have been limited to adult datasets, neglecting the unique challenges posed by pediatric data, which introduces additional non-IID variability. To address these limitations, we analyzed n=398,523 adult chest radiographs from diverse institutions across multiple countries and n=9,125 pediatric images, leveraging transfer learning from general-purpose self-supervised image representations to classify pneumonia and cases with no abnormality. Using state-of-the-art vision transformers, we found that FL improved performance only for smaller adult datasets (P<0.001) but degraded performance for larger datasets (P<0.064) and pediatric cases (P=0.242). However, equipping FL with self-supervised weights significantly enhanced outcomes across pediatric cases (P=0.031) and most adult datasets (P<0.008), except the largest dataset (P=0.052). These findings underscore the potential of easily deployable general-purpose self-supervised image representations to address non-IID challenges in clinical FL applications and highlight their promise for enhancing patient outcomes and advancing pediatric healthcare, where data scarcity and variability remain persistent obstacles.\n        △ Less",
        "authors":"['Mahshad Lotfinia', 'Arash Tayebiarasteh', 'Samaneh Samiei', 'Mehdi Joodaki', 'Soroosh Tayebi Arasteh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AstroLLaVA: towards the unification of astronomical data and natural language",
        "abstract":"We present AstroLLaVA, a vision language model for astronomy that enables interaction with astronomical imagery through natural dialogue. By fine-tuning the LLaVA model on a diverse dataset of $\\sim$30k images with captions and question-answer pairs sourced from NASA's `Astronomy Picture of the Day', the European Southern Observatory, and the NASA\/ESA Hubble Space Telescope, we create a model capable of answering open-ended questions about astronomical concepts depicted visually. Our two-stage fine-tuning process adapts the model to both image captioning and visual question answering in the astronomy domain. We demonstrate AstroLLaVA's performance on an astronomical visual question answering benchmark and release the model weights, code, and training set to encourage further open source work in this space. Finally, we suggest a roadmap towards general astronomical data alignment with pre-trained language models, and provide an open space for collaboration towards this end for interested researchers.\n        △ Less",
        "authors":"['Sharaf Zaman', 'Michael J. Smith', 'Pranav Khetarpal', 'Rishabh Chakrabarty', 'Michele Ginolfi', 'Marc Huertas-Company', 'Maja Jabłońska', 'Sandor Kruk', 'Matthieu Le Lain', 'Sergio José Rodríguez Méndez', 'Dimitrios Tanoglidis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Boosting-inspired online learning with transfer for railway maintenance",
        "abstract":"The integration of advanced sensor technologies with deep learning algorithms has revolutionized fault diagnosis in railway systems, particularly at the wheel-track interface. Although numerous models have been proposed to detect irregularities such as wheel out-of-roundness, they often fall short in real-world applications due to the dynamic and nonstationary nature of railway operations. This paper introduces BOLT-RM (Boosting-inspired Online Learning with Transfer for Railway Maintenance), a model designed to address these challenges using continual learning for predictive maintenance. By allowing the model to continuously learn and adapt as new data become available, BOLT-RM overcomes the issue of catastrophic forgetting that often plagues traditional models. It retains past knowledge while improving predictive accuracy with each new learning episode, using a boosting-like knowledge sharing mechanism to adapt to evolving operational conditions such as changes in speed, load, and track irregularities. The methodology is validated through comprehensive multi-domain simulations of train-track dynamic interactions, which capture realistic railway operating conditions. The proposed BOLT-RM model demonstrates significant improvements in identifying wheel anomalies, establishing a reliable sequence for maintenance interventions.\n        △ Less",
        "authors":"['Diogo Risca', 'Afonso Lourenço', 'Goreti Marreiros']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Uncovering the Structure of Explanation Quality with Spectral Analysis",
        "abstract":"As machine learning models are increasingly considered for high-stakes domains, effective explanation methods are crucial to ensure that their prediction strategies are transparent to the user. Over the years, numerous metrics have been proposed to assess quality of explanations. However, their practical applicability remains unclear, in particular due to a limited understanding of which specific aspects each metric rewards. In this paper we propose a new framework based on spectral analysis of explanation outcomes to systematically capture the multifaceted properties of different explanation techniques. Our analysis uncovers two distinct factors of explanation quality-stability and target sensitivity-that can be directly observed through spectral decomposition. Experiments on both MNIST and ImageNet show that popular evaluation techniques (e.g., pixel-flipping, entropy) partially capture the trade-offs between these factors. Overall, our framework provides a foundational basis for understanding explanation quality, guiding the development of more reliable techniques for evaluating explanations.\n        △ Less",
        "authors":"['Johannes Maeß', 'Grégoire Montavon', 'Shinichi Nakajima', 'Klaus-Robert Müller', 'Thomas Schnake']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Slicing the Gaussian Mixture Wasserstein Distance",
        "abstract":"Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (MW) distance adapts the Wasserstein metric to GMMs and has been applied in various domains, including domain adaptation, dataset comparison, and reinforcement learning. However, its high computational cost -- arising from repeated Wasserstein distance computations involving matrix square root estimations and an expensive linear program -- limits its scalability to high-dimensional and large-scale problems. To address this, we propose multiple novel slicing-based approximations to the MW distance that significantly reduce computational complexity while preserving key optimal transport properties. From a theoretical viewpoint, we establish several weak and strong equivalences between the introduced metrics, and show the relations to the original MW distance and the well-established sliced Wasserstein distance. Furthermore, we validate the effectiveness of our approach through numerical experiments, demonstrating computational efficiency and applications in clustering, perceptual image comparison, and GMM minimization\n        △ Less",
        "authors":"['Moritz Piening', 'Robert Beinert']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Explainability and Continual Learning meet Federated Learning at the Network Edge",
        "abstract":"As edge devices become more capable and pervasive in wireless networks, there is growing interest in leveraging their collective compute power for distributed learning. However, optimizing learning at the network edge entails unique challenges, particularly when moving beyond conventional settings and objectives. While Federated Learning (FL) has emerged as a key paradigm for distributed model training, critical challenges persist. First, existing approaches often overlook the trade-off between predictive accuracy and interpretability. Second, they struggle to integrate inherently explainable models such as decision trees because their non-differentiable structure makes them not amenable to backpropagation-based training algorithms. Lastly, they lack meaningful mechanisms for continual Machine Learning (ML) model adaptation through Continual Learning (CL) in resource-limited environments. In this paper, we pave the way for a set of novel optimization problems that emerge in distributed learning at the network edge with wirelessly interconnected edge devices, and we identify key challenges and future directions. Specifically, we discuss how Multi-objective optimization (MOO) can be used to address the trade-off between predictive accuracy and explainability when using complex predictive models. Next, we discuss the implications of integrating inherently explainable tree-based models into distributed learning settings. Finally, we investigate how CL strategies can be effectively combined with FL to support adaptive, lifelong learning when limited-size buffers are used to store past data for retraining. Our approach offers a cohesive set of tools for designing privacy-preserving, adaptive, and trustworthy ML solutions tailored to the demands of edge computing and intelligent services.\n        △ Less",
        "authors":"['Thomas Tsouparopoulos', 'Iordanis Koutsopoulos']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LGRPool: Hierarchical Graph Pooling Via Local-Global Regularisation",
        "abstract":"Hierarchical graph pooling(HGP) are designed to consider the fact that conventional graph neural networks(GNN) are inherently flat and are also not multiscale. However, most HGP methods suffer not only from lack of considering global topology of the graph and focusing on the feature learning aspect, but also they do not align local and global features since graphs should inherently be analyzed in a multiscale way. LGRPool is proposed in the present paper as a HGP in the framework of expectation maximization in machine learning that aligns local and global aspects of message passing with each other using a regularizer to force the global topological information to be inline with the local message passing at different scales through the representations at different layers of HGP. Experimental results on some graph classification benchmarks show that it slightly outperforms some baselines.\n        △ Less",
        "authors":"['Farshad Noravesh', 'Reza Haffari', 'Layki Soon', 'Arghya Pal']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Early Experience with Confidential Computing Architecture for On-Device Model Protection",
        "abstract":"Deploying machine learning (ML) models on user devices can improve privacy (by keeping data local) and reduce inference latency. Trusted Execution Environments (TEEs) are a practical solution for protecting proprietary models, yet existing TEE solutions have architectural constraints that hinder on-device model deployment. Arm Confidential Computing Architecture (CCA), a new Arm extension, addresses several of these limitations and shows promise as a secure platform for on-device ML. In this paper, we evaluate the performance-privacy trade-offs of deploying models within CCA, highlighting its potential to enable confidential and efficient ML applications. Our evaluations show that CCA can achieve an overhead of, at most, 22% in running models of different sizes and applications, including image classification, voice recognition, and chat assistants. This performance overhead comes with privacy benefits; for example, our framework can successfully protect the model against membership inference attack by an 8.3% reduction in the adversary's success rate. To support further research and early adoption, we make our code and methodology publicly available.\n        △ Less",
        "authors":"['Sina Abdollahi', 'Mohammad Maheri', 'Sandra Siby', 'Marios Kogias', 'Hamed Haddadi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Statistically guided deep learning",
        "abstract":"We present a theoretically well-founded deep learning algorithm for nonparametric regression. It uses over-parametrized deep neural networks with logistic activation function, which are fitted to the given data via gradient descent. We propose a special topology of these networks, a special random initialization of the weights, and a data-dependent choice of the learning rate and the number of gradient descent steps. We prove a theoretical bound on the expected $L_2$ error of this estimate, and illustrate its finite sample size performance by applying it to simulated data. Our results show that a theoretical analysis of deep learning which takes into account simultaneously optimization, generalization and approximation can result in a new deep learning estimate which has an improved finite sample performance.\n        △ Less",
        "authors":"['Michael Kohler', 'Adam Krzyzak']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Physics-informed data-driven control without persistence of excitation",
        "abstract":"We show that data that is not sufficiently informative to allow for system re-identification can still provide meaningful information when combined with external or physical knowledge of the system, such as bounded system matrix norms. We then illustrate how this information can be leveraged for safety and energy minimization problems and to enhance predictions in unmodelled dynamics. This preliminary work outlines key ideas toward using limited data for effective control by integrating physical knowledge of the system and exploiting interpolation conditions.\n        △ Less",
        "authors":"['Martina Vanelli', 'Julien M. Hendrickx']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Toward Realistic Adversarial Attacks in IDS: A Novel Feasibility Metric for Transferability",
        "abstract":"Transferability-based adversarial attacks exploit the ability of adversarial examples, crafted to deceive a specific source Intrusion Detection System (IDS) model, to also mislead a target IDS model without requiring access to the training data or any internal model parameters. These attacks exploit common vulnerabilities in machine learning models to bypass security measures and compromise systems. Although the transferability concept has been widely studied, its practical feasibility remains limited due to assumptions of high similarity between source and target models. This paper analyzes the core factors that contribute to transferability, including feature alignment, model architectural similarity, and overlap in the data distributions that each IDS examines. We propose a novel metric, the Transferability Feasibility Score (TFS), to assess the feasibility and reliability of such attacks based on these factors. Through experimental evidence, we demonstrate that TFS and actual attack success rates are highly correlated, addressing the gap between theoretical understanding and real-world impact. Our findings provide needed guidance for designing more realistic transferable adversarial attacks, developing robust defenses, and ultimately improving the security of machine learning-based IDS in critical systems.\n        △ Less",
        "authors":"['Sabrine Ennaji', 'Elhadj Benkhelifa', 'Luigi Vincenzo Mancini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TickIt: Leveraging Large Language Models for Automated Ticket Escalation",
        "abstract":"In large-scale cloud service systems, support tickets serve as a critical mechanism for resolving customer issues and maintaining service quality. However, traditional manual ticket escalation processes encounter significant challenges, including inefficiency, inaccuracy, and difficulty in handling the high volume and complexity of tickets. While previous research has proposed various machine learning models for ticket classification, these approaches often overlook the practical demands of real-world escalations, such as dynamic ticket updates, topic-specific routing, and the analysis of ticket relationships. To bridge this gap, this paper introduces TickIt, an innovative online ticket escalation framework powered by Large Language Models. TickIt enables topic-aware, dynamic, and relationship-driven ticket escalations by continuously updating ticket states, assigning tickets to the most appropriate support teams, exploring ticket correlations, and leveraging category-guided supervised fine-tuning to continuously improve its performance. By deploying TickIt in ByteDance's cloud service platform Volcano Engine, we validate its efficacy and practicality, marking a significant advancement in the field of automated ticket escalation for large-scale cloud service systems.\n        △ Less",
        "authors":"['Fengrui Liu', 'Xiao He', 'Tieying Zhang', 'Jianjun Chen', 'Yi Li', 'Lihua Yi', 'Haipeng Zhang', 'Gang Wu', 'Rui Shi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Artifact detection and localization in single-channel mobile EEG for sleep research using deep learning and attention mechanisms",
        "abstract":"Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volume of data that novel mobile recording systems generate. We propose a convolutional neural network (CNN) model incorporating a convolutional block attention module (CNN-CBAM) to detect and identify the location of artifacts in the sleep EEG with attention maps. We benchmarked this model against six other machine learning and signal processing approaches. We trained\/tuned all models on 72 manually annotated EEG recordings obtained during home-based monitoring from 18 healthy participants with a mean (SD) age of 68.05 y ($\\pm$5.02). We tested them on 26 separate recordings from 6 healthy participants with a mean (SD) age of 68.33 y ($\\pm$4.08), with contained artifacts in 4\\% of epochs. CNN-CBAM achieved the highest area under the receiver operating characteristic curve (0.88), sensitivity (0.81), and specificity (0.86) when compared to the other approaches. The attention maps from CNN-CBAM localized artifacts within the epoch with a sensitivity of 0.71 and specificity of 0.67. This work demonstrates the feasibility of automating the detection and localization of artifacts in wearable sleep EEG.\n        △ Less",
        "authors":"['Khrystyna Semkiv', 'Jia Zhang', 'Maria Laura Ferster', 'Walter Karlen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generalization Bounds in Hybrid Quantum-Classical Machine Learning Models",
        "abstract":"Hybrid classical-quantum models aim to harness the strengths of both quantum computing and classical machine learning, but their practical potential remains poorly understood. In this work, we develop a unified mathematical framework for analyzing generalization in hybrid models, offering insight into how these systems learn from data. We establish a novel generalization bound of the form $O\\big( \\sqrt{\\frac{T\\log{T}}{N}} + \\fracα{\\sqrt{N}}\\big)$ for $N$ training data points, $T$ trainable quantum gates, and bounded fully-connected layers $||F|| \\leq α$. This bound decomposes cleanly into quantum and classical contributions, extending prior work on both components and clarifying their interaction. We apply our results to the quantum-classical convolutional neural network (QCCNN), an architecture that integrates quantum convolutional layers with classical processing. Alongside the bound, we highlight conceptual limitations of applying classical statistical learning theory in the hybrid setting and suggest promising directions for future theoretical work.\n        △ Less",
        "authors":"['Tongyan Wu', 'Amine Bentellis', 'Alona Sakhnenko', 'Jeanette Miriam Lorenz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Charting the Parrot's Song: A Maximum Mean Discrepancy Approach to Measuring AI Novelty, Originality, and Distinctiveness",
        "abstract":"Current intellectual property frameworks struggle to evaluate the novelty of AI-generated content, relying on subjective assessments ill-suited for comparing effectively infinite AI outputs against prior art. This paper introduces a robust, quantitative methodology grounded in Maximum Mean Discrepancy (MMD) to measure distributional differences between generative processes. By comparing entire output distributions rather than conducting pairwise similarity checks, our approach directly contrasts creative processes--overcoming the computational challenges inherent in evaluating AI outputs against unbounded prior art corpora. Through experiments combining kernel mean embeddings with domain-specific machine learning representations (LeNet-5 for MNIST digits, CLIP for art), we demonstrate exceptional sensitivity: our method distinguishes MNIST digit classes with 95% confidence using just 5-6 samples and differentiates AI-generated art from human art in the AI-ArtBench dataset (n=400 per category; p<0.0001) using as few as 7-10 samples per distribution despite human evaluators' limited discrimination ability (58% accuracy). These findings challenge the \"stochastic parrot\" hypothesis by providing empirical evidence that AI systems produce outputs from semantically distinct distributions rather than merely replicating training data. Our approach bridges technical capabilities with legal doctrine, offering a pathway to modernize originality assessments while preserving intellectual property law's core objectives. This research provides courts and policymakers with a computationally efficient, legally relevant tool to quantify AI novelty--a critical advancement as AI blurs traditional authorship and inventorship boundaries.\n        △ Less",
        "authors":"['Anirban Mukherjee', 'Hannah Hanwen Chang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Systematic Evaluation of Knowledge Graph Embeddings for Gene-Disease Association Prediction",
        "abstract":"Discovery gene-disease links is important in biology and medicine areas, enabling disease identification and drug repurposing. Machine learning approaches accelerate this process by leveraging biological knowledge represented in ontologies and the structure of knowledge graphs. Still, many existing works overlook ontologies explicitly representing diseases, missing causal and semantic relationships between them. The gene-disease association problem naturally frames itself as a link prediction task, where embedding algorithms directly predict associations by exploring the structure and properties of the knowledge graph. Some works frame it as a node-pair classification task, combining embedding algorithms with traditional machine learning algorithms. This strategy aligns with the logic of a machine learning pipeline. However, the use of negative examples and the lack of validated gene-disease associations to train embedding models may constrain its effectiveness. This work introduces a novel framework for comparing the performance of link prediction versus node-pair classification tasks, analyses the performance of state of the art gene-disease association approaches, and compares the different order-based formalizations of gene-disease association prediction. It also evaluates the impact of the semantic richness through a disease-specific ontology and additional links between ontologies. The framework involves five steps: data splitting, knowledge graph integration, embedding, modeling and prediction, and method evaluation. Results show that enriching the semantic representation of diseases slightly improves performance, while additional links generate a greater impact. Link prediction methods better explore the semantic richness encoded in knowledge graphs. Although node-pair classification methods identify all true positives, link prediction methods outperform overall.\n        △ Less",
        "authors":"['Catarina Canastra', 'Cátia Pesquita']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diffusion Models for Robotic Manipulation: A Survey",
        "abstract":"Diffusion generative models have demonstrated remarkable success in visual domains such as image and video generation. They have also recently emerged as a promising approach in robotics, especially in robot manipulations. Diffusion models leverage a probabilistic framework, and they stand out with their ability to model multi-modal distributions and their robustness to high-dimensional input and output spaces. This survey provides a comprehensive review of state-of-the-art diffusion models in robotic manipulation, including grasp learning, trajectory planning, and data augmentation. Diffusion models for scene and image augmentation lie at the intersection of robotics and computer vision for vision-based tasks to enhance generalizability and data scarcity. This paper also presents the two main frameworks of diffusion models and their integration with imitation learning and reinforcement learning. In addition, it discusses the common architectures and benchmarks and points out the challenges and advantages of current state-of-the-art diffusion-based methods.\n        △ Less",
        "authors":"['Rosa Wolf', 'Yitian Shi', 'Sheng Liu', 'Rania Rayyes']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Customizing Spider Silk: Generative Models with Mechanical Property Conditioning for Protein Engineering",
        "abstract":"The remarkable mechanical properties of spider silk, including its tensile strength and extensibility, are primarily governed by the repetitive regions of the proteins that constitute the fiber, the major ampullate spidroins (MaSps). However, establishing correlations between mechanical characteristics and repeat sequences is challenging due to the intricate sequence-structure-function relationships of MaSps and the limited availability of annotated datasets. In this study, we present a novel computational framework for designing MaSp repeat sequences with customizable mechanical properties. To achieve this, we developed a lightweight GPT-based generative model by distilling the pre-trained ProtGPT2 protein language model. The distilled model was subjected to multilevel fine-tuning using curated subsets of the Spider Silkome dataset. Specifically, we adapt the model for MaSp repeat generation using 6,000 MaSp repeat sequences and further refine it with 572 repeats associated with experimentally determined fiber-level mechanical properties. Our model generates biologically plausible MaSp repeat regions tailored to specific mechanical properties while also predicting those properties for given sequences. Validation includes sequence-level analysis, assessing physicochemical attributes and expected distribution of key motifs as well as secondary structure compositions. A correlation study using BLAST on the Spider Silkome dataset and a test set of MaSp repeats with known mechanical properties further confirmed the predictive accuracy of the model. This framework advances the rational design of spider silk-inspired biomaterials, offering a versatile tool for engineering protein sequences with tailored mechanical attributes.\n        △ Less",
        "authors":"['Neeru Dubey', 'Elin Karlsson', 'Miguel Angel Redondo', 'Johan Reimegård', 'Anna Rising', 'Hedvig Kjellström']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Standardization of Weighted Ranking Correlation Coefficients",
        "abstract":"A relevant problem in statistics is defining the correlation of two rankings of a list of items. Kendall's tau and Spearman's rho are two well established correlation coefficients, characterized by a symmetric form that ensures zero expected value between two pairs of rankings randomly chosen with uniform probability. However, in recent years, several weighted versions of the original Spearman and Kendall coefficients have emerged that take into account the greater importance of top ranks compared to low ranks, which is common in many contexts. The weighting schemes break the symmetry, causing a non-zero expected value between two random rankings. This issue is very relevant, as it undermines the concept of uncorrelation between rankings. In this paper, we address this problem by proposing a standardization function $g(x)$ that maps a correlation ranking coefficient $Γ$ in a standard form $g(Γ)$ that has zero expected value, while maintaining the relevant statistical properties of $Γ$.\n        △ Less",
        "authors":"['Pierangelo Lombardo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness",
        "abstract":"Fairness in artificial intelligence (AI) prediction models is increasingly emphasized to support responsible adoption in high-stakes domains such as health care and criminal justice. Guidelines and implementation frameworks highlight the importance of both predictive accuracy and equitable outcomes. However, current fairness toolkits often evaluate classification performance disparities in isolation, with limited attention to other critical aspects such as calibration. To address these gaps, we present seeBias, an R package for comprehensive evaluation of model fairness and predictive performance. seeBias offers an integrated evaluation across classification, calibration, and other performance domains, providing a more complete view of model behavior. It includes customizable visualizations to support transparent reporting and responsible AI implementation. Using public datasets from criminal justice and healthcare, we demonstrate how seeBias supports fairness evaluations, and uncovers disparities that conventional fairness metrics may overlook. The R package is available on GitHub, and a Python version is under development.\n        △ Less",
        "authors":"['Yilin Ning', 'Yian Ma', 'Mingxuan Liu', 'Xin Li', 'Nan Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability",
        "abstract":"Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as each others' observations. We propose the use of learned beliefs on the underlying state of the system to overcome these challenges and enable reinforcement learning with fully decentralized training and execution. Our approach leverages state information to pre-train a probabilistic belief model in a self-supervised fashion. The resulting belief states, which capture both inferred state information as well as uncertainty over this information, are then used in a state-based reinforcement learning algorithm to create an end-to-end model for cooperative multi-agent reinforcement learning under partial observability. By separating the belief and reinforcement learning tasks, we are able to significantly simplify the policy and value function learning tasks and improve both the convergence speed and the final performance. We evaluate our proposed method on diverse partially observable multi-agent tasks designed to exhibit different variants of partial observability.\n        △ Less",
        "authors":"['Paul J. Pritz', 'Kin K. Leung']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Constrained Machine Learning Through Hyperspherical Representation",
        "abstract":"The problem of ensuring constraints satisfaction on the output of machine learning models is critical for many applications, especially in safety-critical domains. Modern approaches rely on penalty-based methods at training time, which do not guarantee to avoid constraints violations; or constraint-specific model architectures (e.g., for monotonocity); or on output projection, which requires to solve an optimization problem that might be computationally demanding. We present the Hypersherical Constrained Representation, a novel method to enforce constraints in the output space for convex and bounded feasibility regions (generalizable to star domains). Our method operates on a different representation system, where Euclidean coordinates are converted into hyperspherical coordinates relative to the constrained region, which can only inherently represent feasible points. Experiments on a synthetic and a real-world dataset show that our method has predictive performance comparable to the other approaches, can guarantee 100% constraint satisfaction, and has a minimal computational cost at inference time.\n        △ Less",
        "authors":"['Gaetano Signorelli', 'Michele Lombardi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BOISHOMMO: Holistic Approach for Bangla Hate Speech",
        "abstract":"One of the most alarming issues in digital society is hate speech (HS) on social media. The severity is so high that researchers across the globe are captivated by this domain. A notable amount of work has been conducted to address the identification and alarm system. However, a noticeable gap exists, especially for low-resource languages. Comprehensive datasets are the main problem among the constrained resource languages, such as Bangla. Interestingly, hate speech or any particular speech has no single dimensionality. Similarly, the hate component can simultaneously have multiple abusive attributes, which seems to be missed in the existing datasets. Thus, a multi-label Bangla hate speech dataset named BOISHOMMO has been compiled and evaluated in this work. That includes categories of HS across race, gender, religion, politics, and more. With over two thousand annotated examples, BOISHOMMO provides a nuanced understanding of hate speech in Bangla and highlights the complexities of processing non-Latin scripts. Apart from evaluating with multiple algorithmic approaches, it also highlights the complexities of processing Bangla text and assesses model performance. This unique multi-label approach enriches future hate speech detection and analysis studies for low-resource languages by providing a more nuanced, diverse dataset.\n        △ Less",
        "authors":"['Md Abdullah Al Kafi', 'Sumit Kumar Banshal', 'Md Sadman Shakib', 'Showrov Azam', 'Tamanna Alam Tabashom']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph Reduction with Unsupervised Learning in Column Generation: A Routing Application",
        "abstract":"Column Generation (CG) is a popular method dedicated to enhancing computational efficiency in large scale Combinatorial Optimization (CO) problems. It reduces the number of decision variables in a problem by solving a pricing problem. For many CO problems, the pricing problem is an Elementary Shortest Path Problem with Resource Constraints (ESPPRC). Large ESPPRC instances are difficult to solve to near-optimality. Consequently, we use a Graph neural Network (GNN) to reduces the size of the ESPPRC such that it becomes computationally tractable with standard solving techniques. Our GNN is trained by Unsupervised Learning and outputs a distribution for the arcs to be retained in the reduced PP. The reduced PP is solved by a local search that finds columns with large reduced costs and speeds up convergence. We apply our method on a set of Capacitated Vehicle Routing Problems with Time Windows and show significant improvements in convergence compared to simple reduction techniques from the literature. For a fixed computational budget, we improve the objective values by over 9\\% for larger instances. We also analyze the performance of our CG algorithm and test the generalization of our method to different classes of instances than the training data.\n        △ Less",
        "authors":"['Abdo Abouelrous', 'Laurens Bliea', 'Adriana F. Gabor', 'Yaoxin Wu', 'Yingqian Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MixDiT: Accelerating Image Diffusion Transformer Inference with Mixed-Precision MX Quantization",
        "abstract":"Diffusion Transformer (DiT) has driven significant progress in image generation tasks. However, DiT inferencing is notoriously compute-intensive and incurs long latency even on datacenter-scale GPUs, primarily due to its iterative nature and heavy reliance on GEMM operations inherent to its encoder-based structure. To address the challenge, prior work has explored quantization, but achieving low-precision quantization for DiT inferencing with both high accuracy and substantial speedup remains an open problem. To this end, this paper proposes MixDiT, an algorithm-hardware co-designed acceleration solution that exploits mixed Microscaling (MX) formats to quantize DiT activation values. MixDiT quantizes the DiT activation tensors by selectively applying higher precision to magnitude-based outliers, which produce mixed-precision GEMM operations. To achieve tangible speedup from the mixed-precision arithmetic, we design a MixDiT accelerator that enables precision-flexible multiplications and efficient MX precision conversions. Our experimental results show that MixDiT delivers a speedup of 2.10-5.32 times over RTX 3090, with no loss in FID.\n        △ Less",
        "authors":"['Daeun Kim', 'Jinwoo Hwang', 'Changhun Oh', 'Jongse Park']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented Generation",
        "abstract":"Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for grounding large language models in external knowledge sources, improving the precision of agents responses. However, high-dimensional language model embeddings, often in the range of hundreds to thousands of dimensions, can present scalability challenges in terms of storage and latency, especially when processing massive financial text corpora. This paper investigates the use of Principal Component Analysis (PCA) to reduce embedding dimensionality, thereby mitigating computational bottlenecks without incurring large accuracy losses. We experiment with a real-world dataset and compare different similarity and distance metrics under both full-dimensional and PCA-compressed embeddings. Our results show that reducing vectors from 3,072 to 110 dimensions provides a sizeable (up to $60\\times$) speedup in retrieval operations and a $\\sim 28.6\\times$ reduction in index size, with only moderate declines in correlation metrics relative to human-annotated similarity scores. These findings demonstrate that PCA-based compression offers a viable balance between retrieval fidelity and resource efficiency, essential for real-time systems such as Zanista AI's \\textit{Newswitch} platform. Ultimately, our study underscores the practicality of leveraging classical dimensionality reduction techniques to scale RAG architectures for knowledge-intensive applications in finance and trading, where speed, memory efficiency, and accuracy must jointly be optimized.\n        △ Less",
        "authors":"['Arman Khaledian', 'Amirreza Ghadiridehkordi', 'Nariman Khaledian']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Empirical Investigation of Reconstruction-Based Models for Seizure Prediction from ECG Signals",
        "abstract":"Epileptic seizures are sudden neurological disorders characterized by abnormal, excessive neuronal activity in the brain, which is often associated with changes in cardiovascular activity. These disruptions can pose significant physical and psychological challenges for patients. Therefore, accurate seizure prediction can help mitigate these risks by enabling timely interventions, ultimately improving patients' quality of life. Traditionally, EEG signals have been the primary standard for seizure prediction due to their precision in capturing brain activity. However, their high cost, susceptibility to noise, and logistical constraints limit their practicality, restricting their use to clinical settings. In order to overcome these limitations, this study focuses on leveraging ECG signals as an alternative for seizure prediction. In this paper, we present a novel method for predicting seizures based on detecting anomalies in ECG signals during their reconstruction. By extracting time-frequency features and leveraging various advanced deep learning architectures, the proposed method identifies deviations in heart rate dynamics associated with seizure onset. The proposed approach was evaluated using the Siena database and could achieve specificity of 99.16\\%, accuracy of 76.05\\%, and false positive rate (FPR) of 0.01\/h, with an average prediction time of 45 minutes before seizure onset. These results highlight the potential of ECG-based seizure prediction as a patient-friendly alternative to traditional EEG-based methods.\n        △ Less",
        "authors":"['Mohammad Reza Chopannavaz', 'Foad Ghaderi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and Flash",
        "abstract":"Large language models (LLMs) are increasingly being deployed on mobile devices, but the limited DRAM capacity constrains the deployable model size. This paper introduces ActiveFlow, the first LLM inference framework that can achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the scaling up of deployable model sizes. The framework is based on the novel concept of active weight DRAM-flash swapping and incorporates three novel techniques: (1) Cross-layer active weights preloading. It uses the activations from the current layer to predict the active weights of several subsequent layers, enabling computation and data loading to overlap, as well as facilitating large I\/O transfers. (2) Sparsity-aware self-distillation. It adjusts the active weights to align with the dense-model output distribution, compensating for approximations introduced by contextual sparsity. (3) Active weight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation among the hot weight cache, preloaded active weights, and computation-involved weights based on available memory. Results show ActiveFlow achieves the performance-cost Pareto frontier compared to existing efficiency optimization methods.\n        △ Less",
        "authors":"['Fucheng Jia', 'Zewen Wu', 'Shiqi Jiang', 'Huiqiang Jiang', 'Qianxi Zhang', 'Yuqing Yang', 'Yunxin Liu', 'Ju Ren', 'Deyu Zhang', 'Ting Cao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Proofs as Explanations: Short Certificates for Reliable Predictions",
        "abstract":"We consider a model for explainable AI in which an explanation for a prediction $h(x)=y$ consists of a subset $S'$ of the training data (if it exists) such that all classifiers $h' \\in H$ that make at most $b$ mistakes on $S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has label $y$ under the assumption that (1) the target function $h^\\star$ belongs to $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example, if $b=0$ and $H$ is the family of linear classifiers in $\\mathbb{R}^d$, and if $x$ lies inside the convex hull of the positive data points in $S$ (and hence every consistent linear classifier labels $x$ as positive), then Carathéodory's theorem states that $x$ lies inside the convex hull of $d+1$ of those points. So, a set $S'$ of size $d+1$ could be released as an explanation for a positive prediction, and would serve as a short proof of correctness of the prediction under the assumption of realizability.\n  In this work, we consider this problem more generally, for general hypothesis classes $H$ and general values $b\\geq 0$. We define the notion of the robust hollow star number of $H$ (which generalizes the standard hollow star number), and show that it precisely characterizes the worst-case size of the smallest certificate achievable, and analyze its size for natural classes. We also consider worst-case distributional bounds on certificate size, as well as distribution-dependent bounds that we show tightly control the sample size needed to get a certificate for any given test example. In particular, we define a notion of the certificate coefficient $\\varepsilon_x$ of an example $x$ with respect to a data distribution $D$ and target function $h^\\star$, and prove matching upper and lower bounds on sample size as a function of $\\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.\n        △ Less",
        "authors":"['Avrim Blum', 'Steve Hanneke', 'Chirag Pabbaraju', 'Donya Saless']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image Representations",
        "abstract":"Visual understanding is inherently contextual -- what we focus on in an image depends on the task at hand. For instance, given an image of a person holding a bouquet of flowers, we may focus on either the person such as their clothing, or the type of flowers, depending on the context of interest. Yet, most existing image encoding paradigms represent an image as a fixed, generic feature vector, overlooking the potential needs of prioritizing varying visual information for different downstream use cases. In this work, we introduce FocalLens, a conditional visual encoding method that produces different representations for the same image based on the context of interest, expressed flexibly through natural language. We leverage vision instruction tuning data and contrastively finetune a pretrained vision encoder to take natural language instructions as additional inputs for producing conditional image representations. Extensive experiments validate that conditional image representation from FocalLens better pronounce the visual features of interest compared to generic features produced by standard vision encoders like CLIP. In addition, we show FocalLens further leads to performance improvements on a range of downstream tasks including image-image retrieval, image classification, and image-text retrieval, with an average gain of 5 and 10 points on the challenging SugarCrepe and MMVP-VLM benchmarks, respectively.\n        △ Less",
        "authors":"['Cheng-Yu Hsieh', 'Pavan Kumar Anasosalu Vasu', 'Fartash Faghri', 'Raviteja Vemulapalli', 'Chun-Liang Li', 'Ranjay Krishna', 'Oncel Tuzel', 'Hadi Pouransari']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DRIP: DRop unImportant data Points -- Enhancing Machine Learning Efficiency with Grad-CAM-Based Real-Time Data Prioritization for On-Device Training",
        "abstract":"Selecting data points for model training is critical in machine learning. Effective selection methods can reduce the labeling effort, optimize on-device training for embedded systems with limited data storage, and enhance the model performance. This paper introduces a novel algorithm that uses Grad-CAM to make online decisions about retaining or discarding data points. Optimized for embedded devices, the algorithm computes a unique DRIP Score to quantify the importance of each data point. This enables dynamic decision-making on whether a data point should be stored for potential retraining or discarded without compromising model performance. Experimental evaluations on four benchmark datasets demonstrate that our approach can match or even surpass the accuracy of models trained on the entire dataset, all while achieving storage savings of up to 39\\%. To our knowledge, this is the first algorithm that makes online decisions about data point retention without requiring access to the entire dataset.\n        △ Less",
        "authors":"['Marcus Rüb', 'Daniel Konegen', 'Axel Sikora', 'Daniel Mueller-Gritschneder']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset",
        "abstract":"Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neural Architecture Search (NAS) method that directly focuses on identifying architectures that minimize energy consumption while maintaining acceptable accuracy. Unlike previous methods that primarily target vision and language tasks, the approach proposed here specifically addresses tabular datasets. Remarkably, the optimal architecture suggested by this method can reduce energy consumption by up to 92% compared to architectures recommended by conventional NAS.\n        △ Less",
        "authors":"['Hoang-Loc La', 'Phuong Hoai Ha']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Adaptive Clustering Scheme for Client Selections in Communication-Efficient Federated Learning",
        "abstract":"Federated learning is a novel decentralized learning architecture. During the training process, the client and server must continuously upload and receive model parameters, which consumes a lot of network transmission resources. Some methods use clustering to find more representative customers, select only a part of them for training, and at the same time ensure the accuracy of training. However, in federated learning, it is not trivial to know what the number of clusters can bring the best training result. Therefore, we propose to dynamically adjust the number of clusters to find the most ideal grouping results. It may reduce the number of users participating in the training to achieve the effect of reducing communication costs without affecting the model performance. We verify its experimental results on the non-IID handwritten digit recognition dataset and reduce the cost of communication and transmission by almost 50% compared with traditional federated learning without affecting the accuracy of the model.\n        △ Less",
        "authors":"['Yan-Ann Chen', 'Guan-Lin Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates",
        "abstract":"Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry -- especially for loose-fitting clothing -- remains an open challenge. We present a novel method for high-fidelity 3D garment reconstruction from single images that bridges 2D and 3D representations. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn rich garment shape priors in a 2D UV space. A key innovation is our mapping model that establishes correspondences between 2D image pixels, UV pattern coordinates, and 3D geometry, enabling joint optimization of both 3D garment meshes and the corresponding 2D patterns by aligning learned priors with image observations. Despite training exclusively on synthetically simulated cloth data, our method generalizes effectively to real-world images, outperforming existing approaches on both tight- and loose-fitting garments. The reconstructed garments maintain physical plausibility while capturing fine geometric details, enabling downstream applications including garment retargeting and texture manipulation.\n        △ Less",
        "authors":"['Ren Li', 'Cong Cao', 'Corentin Dumery', 'Yingxuan You', 'Hao Li', 'Pascal Fua']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Entropic bounds for conditionally Gaussian vectors and applications to neural networks",
        "abstract":"Using entropic inequalities from information theory, we provide new bounds on the total variation and 2-Wasserstein distances between a conditionally Gaussian law and a Gaussian law with invertible covariance matrix. We apply our results to quantify the speed of convergence to Gaussian of a randomly initialized fully connected neural network and its derivatives - evaluated in a finite number of inputs - when the initialization is Gaussian and the sizes of the inner layers diverge to infinity. Our results require mild assumptions on the activation function, and allow one to recover optimal rates of convergence in a variety of distances, thus improving and extending the findings of Basteri and Trevisan (2023), Favaro et al. (2023), Trevisan (2024) and Apollonio et al. (2024). One of our main tools are the quantitative cumulant estimates established in Hanin (2024). As an illustration, we apply our results to bound the total variation distance between the Bayesian posterior law of the neural network and its derivatives, and the posterior law of the corresponding Gaussian limit: this yields quantitative versions of a posterior CLT by Hron et al. (2022), and extends several estimates by Trevisan (2024) to the total variation metric.\n        △ Less",
        "authors":"['Lucia Celli', 'Giovanni Peccati']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"High-dimensional Clustering and Signal Recovery under Block Signals",
        "abstract":"This paper studies computationally efficient methods and their minimax optimality for high-dimensional clustering and signal recovery under block signal structures. We propose two sets of methods, cross-block feature aggregation PCA (CFA-PCA) and moving average PCA (MA-PCA), designed for sparse and dense block signals, respectively. Both methods adaptively utilize block signal structures, applicable to non-Gaussian data with heterogeneous variances and non-diagonal covariance matrices. Specifically, the CFA method utilizes a block-wise U-statistic to aggregate and select block signals non-parametrically from data with unknown cluster labels. We show that the proposed methods are consistent for both clustering and signal recovery under mild conditions and weaker signal strengths than the existing methods without considering block structures of signals. Furthermore, we derive both statistical and computational minimax lower bounds (SMLB and CMLB) for high-dimensional clustering and signal recovery under block signals, where the CMLBs are restricted to algorithms with polynomial computation complexity. The minimax boundaries partition signals into regions of impossibility and possibility. No algorithm (or no polynomial time algorithm) can achieve consistent clustering or signal recovery if the signals fall into the statistical (or computational) region of impossibility. We show that the proposed CFA-PCA and MA-PCA methods can achieve the CMLBs for the sparse and dense block signal regimes, respectively, indicating the proposed methods are computationally minimax optimal. A tuning parameter selection method is proposed based on post-clustering signal recovery results. Simulation studies are conducted to evaluate the proposed methods. A case study on global temperature change demonstrates their utility in practice.\n        △ Less",
        "authors":"['Wu Su', 'Yumou Qiu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models",
        "abstract":"Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generality of EHR foundation models and the integration of models trained with different vocabularies. To deal with this problem, we propose MedRep for EHR foundation models based on the observational medical outcome partnership (OMOP) common data model (CDM), providing the integrated medical concept representations and the basic data augmentation strategy for patient trajectories. For concept representation learning, we enrich the information of each concept with a minimal definition through large language model (LLM) prompts and enhance the text-based representations through graph ontology of OMOP vocabulary. Trajectory augmentation randomly replaces selected concepts with other similar concepts that have closely related representations to let the model practice with the concepts out-of-vocabulary. Finally, we demonstrate that EHR foundation models trained with MedRep better maintain the prediction performance in external datasets. Our code implementation is publicly available at https:\/\/github.com\/kicarussays\/MedRep.\n        △ Less",
        "authors":"['Junmo Kim', 'Namkyeong Lee', 'Jiwon Kim', 'Kwangsoo Kim']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards generalizable single-cell perturbation modeling via the Conditional Monge Gap",
        "abstract":"Learning the response of single-cells to various treatments offers great potential to enable targeted therapies. In this context, neural optimal transport (OT) has emerged as a principled methodological framework because it inherently accommodates the challenges of unpaired data induced by cell destruction during data acquisition. However, most existing OT approaches are incapable of conditioning on different treatment contexts (e.g., time, drug treatment, drug dosage, or cell type) and we still lack methods that unanimously show promising generalization performance to unseen treatments. Here, we propose the Conditional Monge Gap which learns OT maps conditionally on arbitrary covariates. We demonstrate its value in predicting single-cell perturbation responses conditional to one or multiple drugs, a drug dosage, or combinations thereof. We find that our conditional models achieve results comparable and sometimes even superior to the condition-specific state-of-the-art on scRNA-seq as well as multiplexed protein imaging data. Notably, by aggregating data across conditions we perform cross-task learning which unlocks remarkable generalization abilities to unseen drugs or drug dosages, widely outperforming other conditional models in capturing heterogeneity (i.e., higher moments) in the perturbed population. Finally, by scaling to hundreds of conditions and testing on unseen drugs, we narrow the gap between structure-based and effect-based drug representations, suggesting a promising path to the successful prediction of perturbation effects for unseen treatments.\n        △ Less",
        "authors":"['Alice Driessen', 'Benedek Harsanyi', 'Marianna Rapsomaniki', 'Jannis Born']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Academic Network Representation via Prediction-Sampling Incorporated Tensor Factorization",
        "abstract":"Accurate representation to an academic network is of great significance to academic relationship mining like predicting scientific impact. A Latent Factorization of Tensors (LFT) model is one of the most effective models for learning the representation of a target network. However, an academic network is often High-Dimensional and Incomplete (HDI) because the relationships among numerous network entities are impossible to be fully explored, making it difficult for an LFT model to learn accurate representation of the academic network. To address this issue, this paper proposes a Prediction-sampling-based Latent Factorization of Tensors (PLFT) model with two ideas: 1) constructing a cascade LFT architecture to enhance model representation learning ability via learning academic network hierarchical features, and 2) introducing a nonlinear activation-incorporated predicting-sampling strategy to more accurately learn the network representation via generating new academic network data layer by layer. Experimental results from the three real-world academic network datasets show that the PLFT model outperforms existing models when predicting the unexplored relationships among network entities.\n        △ Less",
        "authors":"['Chunyang Zhang', 'Xin Liao', 'Hao Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SortBench: Benchmarking LLMs based on their ability to sort lists",
        "abstract":"Sorting is a tedious but simple task for human intelligence and can be solved fairly easily algorithmically. However, for Large Language Models (LLMs) this task is surprisingly hard, as some properties of sorting are among known weaknesses of LLMs: being faithful to the input data, logical comparisons between values, and strictly differentiating between syntax (used for sorting) and semantics (typically learned by embeddings). Within this paper, we describe the new SortBench benchmark for LLMs that comes with different difficulties and that can be easily scaled in terms of difficulty. We apply this benchmark to seven state-of-the-art LLMs, including current test-time reasoning models. Our results show that while the o3-mini model is very capable at sorting in general, even this can be fooled if strings are defined to mix syntactical and semantical aspects, e.g., by asking to sort numbers written-out as word. Furthermore, all models have problems with the faithfulness to the input of long lists, i.e., they drop items and add new ones. Our results also show that test-time reasoning has a tendency to overthink problems which leads to performance degradation. Finally, models without test-time reasoning like GPT-4o are not much worse than reasoning models.\n        △ Less",
        "authors":"['Steffen Herbold']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DeQompile: quantum circuit decompilation using genetic programming for explainable quantum architecture search",
        "abstract":"Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate this. The circuit ansatz in these solutions is often designed through reinforcement learning-based quantum architecture search when the domain knowledge of the problem and hardware are not effective. However, the interpretability of these synthesized circuits remains a significant bottleneck, limiting their scalability and applicability across diverse problem domains.\n  This work addresses the challenge of explainability in quantum architecture search (QAS) by introducing a novel genetic programming-based decompiler framework for reverse-engineering high-level quantum algorithms from low-level circuit representations. The proposed approach, implemented in the open-source tool DeQompile, employs program synthesis techniques, including symbolic regression and abstract syntax tree manipulation, to distill interpretable Qiskit algorithms from quantum assembly language. Validation of benchmark algorithms demonstrates the efficacy of our tool. By integrating the decompiler with online learning frameworks, this research potentiates explainable QAS by fostering the development of generalizable and provable quantum algorithms.\n        △ Less",
        "authors":"['Shubing Xie', 'Aritra Sarkar', 'Sebastian Feld']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ELSA: A Style Aligned Dataset for Emotionally Intelligent Language Generation",
        "abstract":"Advancements in emotion aware language processing increasingly shape vital NLP applications ranging from conversational AI and affective computing to computational psychology and creative content generation. Existing emotion datasets either lack emotional granularity or fail to capture necessary stylistic diversity, limiting the advancement of effective emotion conditioned text generation systems. Seeking to bridge this crucial gap between granularity and style diversity, this paper introduces a novel systematically constructed dataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine grained emotion taxonomies adapted from existing sources such as dair ai emotion dataset and GoEmotions taxonomy. This dataset comprises multiple emotionally nuanced variations of original sentences regenerated across distinct contextual styles such as conversational, formal, poetic, and narrative, using advanced Large Language Models LLMs. Rigorous computational evaluation using metrics such as perplexity, embedding variance, readability, lexical diversity, and semantic coherence measures validates the datasets emotional authenticity, linguistic fluency, and textual diversity. Comprehensive metric analyses affirm its potential to support deeper explorations into emotion conditioned style adaptive text generation. By enabling precision tuned emotionally nuanced language modeling, our dataset creates fertile ground for research on fine grained emotional control, prompt driven explanation, interpretability, and style adaptive expressive language generation with LLMs.\n        △ Less",
        "authors":"['Vishal Gandhi', 'Sagar Gandhi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enabling Automatic Differentiation with Mollified Graph Neural Operators",
        "abstract":"Physics-informed neural operators offer a powerful framework for learning solution operators of partial differential equations (PDEs) by combining data and physics losses. However, these physics losses rely on derivatives. Computing these derivatives remains challenging, with spectral and finite difference methods introducing approximation errors due to finite resolution. Here, we propose the mollified graph neural operator (mGNO), the first method to leverage automatic differentiation and compute \\emph{exact} gradients on arbitrary geometries. This enhancement enables efficient training on irregular grids and varying geometries while allowing seamless evaluation of physics losses at randomly sampled points for improved generalization. For a PDE example on regular grids, mGNO paired with autograd reduced the L2 relative data error by 20x compared to finite differences, although training was slower. It can also solve PDEs on unstructured point clouds seamlessly, using physics losses only, at resolutions vastly lower than those needed for finite differences to be accurate enough. On these unstructured point clouds, mGNO leads to errors that are consistently 2 orders of magnitude lower than machine learning baselines (Meta-PDE) for comparable runtimes, and also delivers speedups from 1 to 3 orders of magnitude compared to the numerical solver for similar accuracy. mGNOs can also be used to solve inverse design and shape optimization problems on complex geometries.\n        △ Less",
        "authors":"['Ryan Y. Lin', 'Julius Berner', 'Valentin Duruisseaux', 'David Pitt', 'Daniel Leibovici', 'Jean Kossaifi', 'Kamyar Azizzadenesheli', 'Anima Anandkumar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"To See or Not to See -- Fingerprinting Devices in Adversarial Environments Amid Advanced Machine Learning",
        "abstract":"The increasing use of the Internet of Things raises security concerns. To address this, device fingerprinting is often employed to authenticate devices, detect adversaries, and identify eavesdroppers in an environment. This requires the ability to discern between legitimate and malicious devices which is achieved by analyzing the unique physical and\/or operational characteristics of IoT devices. In the era of the latest progress in machine learning, particularly generative models, it is crucial to methodically examine the current studies in device fingerprinting. This involves explaining their approaches and underscoring their limitations when faced with adversaries armed with these ML tools. To systematically analyze existing methods, we propose a generic, yet simplified, model for device fingerprinting. Additionally, we thoroughly investigate existing methods to authenticate devices and detect eavesdropping, using our proposed model. We further study trends and similarities between works in authentication and eavesdropping detection and present the existing threats and attacks in these domains. Finally, we discuss future directions in fingerprinting based on these trends to develop more secure IoT fingerprinting schemes.\n        △ Less",
        "authors":"['Justin Feng', 'Nader Sehatbakhsh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accelerating Multi-Objective Collaborative Optimization of Doped Thermoelectric Materials via Artificial Intelligence",
        "abstract":"The thermoelectric performance of materials exhibits complex nonlinear dependencies on both elemental types and their proportions, rendering traditional trial-and-error approaches inefficient and time-consuming for material discovery. In this work, we present a deep learning model capable of accurately predicting thermoelectric properties of doped materials directly from their chemical formulas, achieving state-of-the-art performance. To enhance interpretability, we further incorporate sensitivity analysis techniques to elucidate how physical descriptors affect the thermoelectric figure of merit (zT). Moreover, we establish a coupled framework that integrates a surrogate model with a multi-objective genetic algorithm to efficiently explore the vast compositional space for high-performance candidates. Experimental validation confirms the discovery of a novel thermoelectric material with superior $zT$ values in the medium-temperature regime.\n        △ Less",
        "authors":"['Yuxuan Zeng', 'Wenhao Xie', 'Wei Cao', 'Tan Peng', 'Yue Hou', 'Ziyu Wang', 'Jing Shi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Understanding the Impact of Data Domain Extraction on Synthetic Data Privacy",
        "abstract":"Privacy attacks, particularly membership inference attacks (MIAs), are widely used to assess the privacy of generative models for tabular synthetic data, including those with Differential Privacy (DP) guarantees. These attacks often exploit outliers, which are especially vulnerable due to their position at the boundaries of the data domain (e.g., at the minimum and maximum values). However, the role of data domain extraction in generative models and its impact on privacy attacks have been overlooked. In this paper, we examine three strategies for defining the data domain: assuming it is externally provided (ideally from public data), extracting it directly from the input data, and extracting it with DP mechanisms. While common in popular implementations and libraries, we show that the second approach breaks end-to-end DP guarantees and leaves models vulnerable. While using a provided domain (if representative) is preferable, extracting it with DP can also defend against popular MIAs, even at high privacy budgets.\n        △ Less",
        "authors":"['Georgi Ganev', 'Meenatchi Sundaram Muthu Selva Annamalai', 'Sofiane Mahiou', 'Emiliano De Cristofaro']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Network-assisted Interval Reachability for Systems with Control Barrier Function-Based Safe Controllers",
        "abstract":"Control Barrier Functions (CBFs) have been widely utilized in the design of optimization-based controllers and filters for dynamical systems to ensure forward invariance of a given set of safe states. While CBF-based controllers offer safety guarantees, they can compromise the performance of the system, leading to undesirable behaviors such as unbounded trajectories and emergence of locally stable spurious equilibria. Computing reachable sets for systems with CBF-based controllers is an effective approach for runtime performance and stability verification, and can potentially serve as a tool for trajectory re-planning. In this paper, we propose a computationally efficient interval reachability method for performance verification of systems with optimization-based controllers by: (i) approximating the optimization-based controller by a pre-trained neural network to avoid solving optimization problems repeatedly, and (ii) using mixed monotone theory to construct an embedding system that leverages state-of-the-art neural network verification algorithms for bounding the output of the neural network. Results in terms of closeness of solutions of trajectories of the system with the optimization-based controller and the neural network are derived. Using a single trajectory of the embedding system along with our closeness of solutions result, we obtain an over-approximation of the reachable set of the system with optimization-based controllers. Numerical results are presented to corroborate the technical findings.\n        △ Less",
        "authors":"['Damola Ajeyemi', 'Saber Jafarpour', \"Emiliano Dall'Anese\"]",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner",
        "abstract":"State-based sequence models like RWKV-7 offer a compelling alternative to Transformer architectures, achieving linear complexity while demonstrating greater expressive power in short-context scenarios and enabling state tracking beyond the \\(\\text{TC}^0\\) complexity class. However, RWKV-7 lacks mechanisms for token-parameter interactions and native scalability, limiting its adaptability and growth without retraining. In this paper, we propose \\textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention mechanisms with a fully state-driven approach, integrating token-parameter interactions through a \\textbf{Self-State Encoder} (SSE) mechanism. The SSE repurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as transformation weights to encode token-parameter interactions in a linear, state-driven manner without introducing new trainable matrices or softmax operations, while preserving the autoregressive property of token processing. Meta-State supports progressive model scaling by expanding the WKV state and parameter tokens, reusing existing parameters without retraining. Our approach bridges the gap between state-based modeling, token-parameter interactions, and scalable architectures, offering a flexible framework for efficient and adaptable sequence modeling with linear complexity and constant memory usage.\n        △ Less",
        "authors":"['Liu Xiao', 'Li Zhiyuan', 'Lin Yueyu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Spectral Normalization for Lipschitz-Constrained Policies on Learning Humanoid Locomotion",
        "abstract":"Reinforcement learning (RL) has shown great potential in training agile and adaptable controllers for legged robots, enabling them to learn complex locomotion behaviors directly from experience. However, policies trained in simulation often fail to transfer to real-world robots due to unrealistic assumptions such as infinite actuator bandwidth and the absence of torque limits. These conditions allow policies to rely on abrupt, high-frequency torque changes, which are infeasible for real actuators with finite bandwidth.\n  Traditional methods address this issue by penalizing aggressive motions through regularization rewards, such as joint velocities, accelerations, and energy consumption, but they require extensive hyperparameter tuning. Alternatively, Lipschitz-Constrained Policies (LCP) enforce finite bandwidth action control by penalizing policy gradients, but their reliance on gradient calculations introduces significant GPU memory overhead. To overcome this limitation, this work proposes Spectral Normalization (SN) as an efficient replacement for enforcing Lipschitz continuity. By constraining the spectral norm of network weights, SN effectively limits high-frequency policy fluctuations while significantly reducing GPU memory usage. Experimental evaluations in both simulation and real-world humanoid robot show that SN achieves performance comparable to gradient penalty methods while enabling more efficient parallel training.\n        △ Less",
        "authors":"['Jaeyong Shin', 'Woohyun Cha', 'Donghyeon Kim', 'Junhyeok Cha', 'Jaeheung Park']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bringing Structure to Naturalness: On the Naturalness of ASTs",
        "abstract":"Source code comes in different shapes and forms. Previous research has already shown code to be more predictable than natural language as well as highlighted its statistical predictability at the token level: source code can be natural. More recently, the structure of code -- control flow, syntax graphs, abstract syntax trees etc. -- has been successfully used to improve the state-of-the-art on numerous tasks: code suggestion, code summarisation, method naming etc. This body of work implicitly assumes that structured representations of code are similarly statistically predictable, i.e. that a structured view of code is also natural. We consider that this view should be made explicit and propose directly studying the Structured Naturalness Hypothesis. Beyond just naming existing research that assumes this hypothesis and formulating it, we also provide evidence in the case of trees: TreeLSTM models over ASTs for some languages, such as Ruby, are competitive with $n$-gram models while handling the syntax token issue highlighted by previous research 'for free'. For other languages, such as Java or Python, we find tree models to perform worse, suggesting that downstream task improvement is uncorrelated to the language modelling task. Further, we show how such naturalness signals can be employed for near state-of-the-art results on just-in-time defect prediction while forgoing manual feature engineering work.\n        △ Less",
        "authors":"['Profir-Petru Pârţachi', 'Mahito Sugiyama']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DaemonSec: Examining the Role of Machine Learning for Daemon Security in Linux Environments",
        "abstract":"DaemonSec is an early-stage startup exploring machine learning (ML)-based security for Linux daemons, a critical yet often overlooked attack surface. While daemon security remains underexplored, conventional defenses struggle against adaptive threats and zero-day exploits. To assess the perspectives of IT professionals on ML-driven daemon protection, a systematic interview study based on semi-structured interviews was conducted with 22 professionals from industry and academia. The study evaluates adoption, feasibility, and trust in ML-based security solutions. While participants recognized the potential of ML for real-time anomaly detection, findings reveal skepticism toward full automation, limited security awareness among non-security roles, and concerns about patching delays creating attack windows. This paper presents the methods, key findings, and implications for advancing ML-driven daemon security in industry.\n        △ Less",
        "authors":"['Sheikh Muhammad Farjad']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"All Optical Echo State Network Reservoir Computing",
        "abstract":"We propose an innovative design for an all-optical Echo State Network (ESN), an advanced type of reservoir computer known for its universal computational capabilities. Our design enables fully optical implementation of arbitrary ESNs, featuring complete flexibility in optical matrix multiplication and nonlinear activation. Leveraging the nonlinear characteristics of stimulated Brillouin scattering (SBS), the architecture efficiently realizes measurement-free operations crucial for reservoir computing. The approach significantly reduces computational overhead and energy consumption compared to traditional software-based methods. Comprehensive simulations validate the system's memory capacity, nonlinear processing strength, and polynomial algebra capabilities, showcasing performance comparable to software ESNs across key benchmark tasks. Our design establishes a feasible, scalable, and universally applicable framework for optical reservoir computing, suitable for diverse machine learning applications.\n        △ Less",
        "authors":"['Ishwar S Kaushik', 'Peter J Ehlers', 'Daniel Soh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DrivAer Transformer: A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset",
        "abstract":"At the current stage, deep learning-based methods have demonstrated excellent capabilities in evaluating aerodynamic performance, significantly reducing the time and cost required for traditional computational fluid dynamics (CFD) simulations. However, when faced with the task of processing extremely complex three-dimensional (3D) vehicle models, the lack of large-scale datasets and training resources, coupled with the inherent diversity and complexity of the geometry of different vehicle models, means that the prediction accuracy and versatility of these networks are still not up to the level required for current production. In view of the remarkable success of Transformer models in the field of natural language processing and their strong potential in the field of image processing, this study innovatively proposes a point cloud learning framework called DrivAer Transformer (DAT). The DAT structure uses the DrivAerNet++ dataset, which contains high-fidelity CFD data of industrial-standard 3D vehicle shapes. enabling accurate estimation of air drag directly from 3D meshes, thus avoiding the limitations of traditional methods such as 2D image rendering or signed distance fields (SDF). DAT enables fast and accurate drag prediction, driving the evolution of the aerodynamic evaluation process and laying the critical foundation for introducing a data-driven approach to automotive design. The framework is expected to accelerate the vehicle design process and improve development efficiency.\n        △ Less",
        "authors":"['Jiaqi He', 'Xiangwen Luo', 'Yiping Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Local Distance-Preserving Node Embeddings and Their Performance on Random Graphs",
        "abstract":"Learning node representations is a fundamental problem in graph machine learning. While existing embedding methods effectively preserve local similarity measures, they often fail to capture global functions like graph distances. Inspired by Bourgain's seminal work on Hilbert space embeddings of metric spaces (1985), we study the performance of local distance-preserving node embeddings. Known as landmark-based algorithms, these embeddings approximate pairwise distances by computing shortest paths from a small subset of reference nodes (i.e., landmarks). Our main theoretical contribution shows that random graphs, such as Erdős-Rényi random graphs, require lower dimensions in landmark-based embeddings compared to worst-case graphs. Empirically, we demonstrate that the GNN-based approximations for the distances to landmarks generalize well to larger networks, offering a scalable alternative for graph representation learning.\n        △ Less",
        "authors":"['My Le', 'Luana Ruiz', 'Souvik Dhara']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Distributional Learning with Non-crossing Quantile Network",
        "abstract":"In this paper, we introduce a non-crossing quantile (NQ) network for conditional distribution learning. By leveraging non-negative activation functions, the NQ network ensures that the learned distributions remain monotonic, effectively addressing the issue of quantile crossing. Furthermore, the NQ network-based deep distributional learning framework is highly adaptable, applicable to a wide range of applications, from classical non-parametric quantile regression to more advanced tasks such as causal effect estimation and distributional reinforcement learning (RL). We also develop a comprehensive theoretical foundation for the deep NQ estimator and its application to distributional RL, providing an in-depth analysis that demonstrates its effectiveness across these domains. Our experimental results further highlight the robustness and versatility of the NQ network.\n        △ Less",
        "authors":"['Guohao Shen', 'Runpeng Dai', 'Guojun Wu', 'Shikai Luo', 'Chengchun Shi', 'Hongtu Zhu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimizing Power Grid Topologies with Reinforcement Learning: A Survey of Methods and Challenges",
        "abstract":"Power grid operation is becoming increasingly complex due to the rising integration of renewable energy sources and the need for more adaptive control strategies. Reinforcement Learning (RL) has emerged as a promising approach to power network control (PNC), offering the potential to enhance decision-making in dynamic and uncertain environments. The Learning To Run a Power Network (L2RPN) competitions have played a key role in accelerating research by providing standardized benchmarks and problem formulations, leading to rapid advancements in RL-based methods. This survey provides a comprehensive and structured overview of RL applications for power grid topology optimization, categorizing existing techniques, highlighting key design choices, and identifying gaps in current research. Additionally, we present a comparative numerical study evaluating the impact of commonly applied RL-based methods, offering insights into their practical effectiveness. By consolidating existing research and outlining open challenges, this survey aims to provide a foundation for future advancements in RL-driven power grid optimization.\n        △ Less",
        "authors":"['Erica van der Sar', 'Alessandro Zocca', 'Sandjai Bhulai']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DRAFT-ing Architectural Design Decisions using LLMs",
        "abstract":"Architectural Knowledge Management (AKM) is crucial for software development but remains challenging due to the lack of standardization and high manual effort. Architecture Decision Records (ADRs) provide a structured approach to capture Architecture Design Decisions (ADDs), but their adoption is limited due to the manual effort involved and insufficient tool support. Our previous work has shown that Large Language Models (LLMs) can assist in generating ADDs. However, simply prompting the LLM does not produce quality ADDs. Moreover, using third-party LLMs raises privacy concerns, while self-hosting them poses resource challenges.\n  To this end, we experimented with different approaches like few-shot, retrieval-augmented generation (RAG) and fine-tuning to enhance LLM's ability to generate ADDs. Our results show that both techniques improve effectiveness. Building on this, we propose Domain Specific Retreival Augumented Few Shot Fine Tuninng, DRAFT, which combines the strengths of all these three approaches for more effective ADD generation. DRAFT operates in two phases: an offline phase that fine-tunes an LLM on generating ADDs augmented with retrieved examples and an online phase that generates ADDs by leveraging retrieved ADRs and the fine-tuned model.\n  We evaluated DRAFT against existing approaches on a dataset of 4,911 ADRs and various LLMs and analyzed them using automated metrics and human evaluations. Results show DRAFT outperforms all other approaches in effectiveness while maintaining efficiency. Our findings indicate that DRAFT can aid architects in drafting ADDs while addressing privacy and resource constraints.\n        △ Less",
        "authors":"['Rudra Dhar', 'Adyansh Kakran', 'Amey Karan', 'Karthik Vaidhyanathan', 'Vasudeva Varma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Encoding and Decoding at Scale",
        "abstract":"Recent work has demonstrated that large-scale, multi-animal models are powerful tools for characterizing the relationship between neural activity and behavior. Current large-scale approaches, however, focus exclusively on either predicting neural activity from behavior (encoding) or predicting behavior from neural activity (decoding), limiting their ability to capture the bidirectional relationship between neural activity and behavior. To bridge this gap, we introduce a multimodal, multi-task model that enables simultaneous Neural Encoding and Decoding at Scale (NEDS). Central to our approach is a novel multi-task-masking strategy, which alternates between neural, behavioral, within-modality, and cross-modality masking. We pretrain our method on the International Brain Laboratory (IBL) repeated site dataset, which includes recordings from 83 animals performing the same visual decision-making task. In comparison to other large-scale models, we demonstrate that NEDS achieves state-of-the-art performance for both encoding and decoding when pretrained on multi-animal data and then fine-tuned on new animals. Surprisingly, NEDS's learned embeddings exhibit emergent properties: even without explicit training, they are highly predictive of the brain regions in each recording. Altogether, our approach is a step towards a foundation model of the brain that enables seamless translation between neural activity and behavior.\n        △ Less",
        "authors":"['Yizi Zhang', 'Yanchen Wang', 'Mehdi Azabou', 'Alexandre Andre', 'Zixuan Wang', 'Hanrui Lyu', 'The International Brain Laboratory', 'Eva Dyer', 'Liam Paninski', 'Cole Hurwitz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Influential Bandits: Pulling an Arm May Change the Environment",
        "abstract":"While classical formulations of multi-armed bandit problems assume that each arm's reward is independent and stationary, real-world applications often involve non-stationary environments and interdependencies between arms. In particular, selecting one arm may influence the future rewards of other arms, a scenario not adequately captured by existing models such as rotting bandits or restless bandits. To address this limitation, we propose the influential bandit problem, which models inter-arm interactions through an unknown, symmetric, positive semi-definite interaction matrix that governs the dynamics of arm losses. We formally define this problem and establish two regret lower bounds, including a superlinear $Ω(T^2 \/ \\log^2 T)$ bound for the standard UCB algorithm and an algorithm-independent $Ω(T)$ bound, which highlight the inherent difficulty of the setting. We then introduce a new algorithm based on a lower confidence bound (LCB) estimator tailored to the structure of the loss dynamics. Under mild assumptions, our algorithm achieves a regret of $O(KT \\log T)$, which is nearly optimal in terms of its dependence on the time horizon. The algorithm is simple to implement and computationally efficient. Empirical evaluations on both synthetic and real-world datasets demonstrate the presence of inter-arm influence and confirm the superior performance of our method compared to conventional bandit algorithms.\n        △ Less",
        "authors":"['Ryoma Sato', 'Shinji Ito']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The More is not the Merrier: Investigating the Effect of Client Size on Federated Learning",
        "abstract":"Federated Learning (FL) has been introduced as a way to keep data local to clients while training a shared machine learning model, as clients train on their local data and send trained models to a central aggregator. It is expected that FL will have a huge implication on Mobile Edge Computing, the Internet of Things, and Cross-Silo FL. In this paper, we focus on the widely used FedAvg algorithm to explore the effect of the number of clients in FL. We find a significant deterioration of learning accuracy for FedAvg as the number of clients increases. To address this issue for a general application, we propose a method called Knowledgeable Client Insertion (KCI) that introduces a very small number of knowledgeable clients to the MEC setting. These knowledgeable clients are expected to have accumulated a large set of data samples to help with training. With the help of KCI, the learning accuracy of FL increases much faster even with a normal FedAvg aggregation technique. We expect this approach to be able to provide great privacy protection for clients against security attacks such as model inversion attacks. Our code is available at https:\/\/github.com\/Eleanor-W\/KCI_for_FL.\n        △ Less",
        "authors":"['Eleanor Wallach', 'Sage Siler', 'Jing Deng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs",
        "abstract":"Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.\n        △ Less",
        "authors":"['Aashiq Muhamed', 'Jacopo Bonato', 'Mona Diab', 'Virginia Smith']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative Analysis of Different Methods for Classifying Polychromatic Sketches",
        "abstract":"Image classification is a significant challenge in computer vision, particularly in domains humans are not accustomed to. As machine learning and artificial intelligence become more prominent, it is crucial these algorithms develop a sense of sight that is on par with or exceeds human ability. For this reason, we have collected, cleaned, and parsed a large dataset of hand-drawn doodles and compared multiple machine learning solutions to classify these images into 170 distinct categories. The best model we found achieved a Top-1 accuracy of 47.5%, significantly surpassing human performance on the dataset, which stands at 41%.\n        △ Less",
        "authors":"['Fahd Baba', 'Devon Mack']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detecting Credit Card Fraud via Heterogeneous Graph Neural Networks with Graph Attention",
        "abstract":"This study proposes a credit card fraud detection method based on Heterogeneous Graph Neural Network (HGNN) to address fraud in complex transaction networks. Unlike traditional machine learning methods that rely solely on numerical features of transaction records, this approach constructs heterogeneous transaction graphs. These graphs incorporate multiple node types, including users, merchants, and transactions. By leveraging graph neural networks, the model captures higher-order transaction relationships. A Graph Attention Mechanism is employed to dynamically assign weights to different transaction relationships. Additionally, a Temporal Decay Mechanism is integrated to enhance the model's sensitivity to time-related fraud patterns. To address the scarcity of fraudulent transaction samples, this study applies SMOTE oversampling and Cost-sensitive Learning. These techniques strengthen the model's ability to identify fraudulent transactions. Experimental results demonstrate that the proposed method outperforms existing GNN models, including GCN, GAT, and GraphSAGE, on the IEEE-CIS Fraud Detection dataset. The model achieves notable improvements in both accuracy and OC-ROC. Future research may explore the integration of dynamic graph neural networks and reinforcement learning. Such advancements could enhance the real-time adaptability of fraud detection systems and provide more intelligent solutions for financial risk control.\n        △ Less",
        "authors":"['Qiuwu Sha', 'Tengda Tang', 'Xinyu Du', 'Jie Liu', 'Yixian Wang', 'Yuan Sheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Particle Hit Clustering and Identification Using Point Set Transformers in Liquid Argon Time Projection Chambers",
        "abstract":"Liquid argon time projection chambers are often used in neutrino physics and dark-matter searches because of their high spatial resolution. The images generated by these detectors are extremely sparse, as the energy values detected by most of the detector are equal to 0, meaning that despite their high resolution, most of the detector is unused in a particular interaction. Instead of representing all of the empty detections, the interaction is usually stored as a sparse matrix, a list of detection locations paired with their energy values. Traditional machine learning methods that have been applied to particle reconstruction such as convolutional neural networks (CNNs), however, cannot operate over data stored in this way and therefore must have the matrix fully instantiated as a dense matrix. Operating on dense matrices requires a lot of memory and computation time, in contrast to directly operating on the sparse matrix. We propose a machine learning model using a point set neural network that operates over a sparse matrix, greatly improving both processing speed and accuracy over methods that instantiate the dense matrix, as well as over other methods that operate over sparse matrices. Compared to competing state-of-the-art methods, our method improves classification performance by 14%, segmentation performance by more than 22%, while taking 80% less time and using 66% less memory. Compared to state-of-the-art CNN methods, our method improves classification performance by more than 86%, segmentation performance by more than 71%, while reducing runtime by 91% and reducing memory usage by 61%.\n        △ Less",
        "authors":"['Edgar E. Robles', 'Alejando Yankelevich', 'Wenjie Wu', 'Jianming Bian', 'Pierre Baldi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression",
        "abstract":"Motivated by robust and quantile regression problems, we investigate the stochastic gradient descent (SGD) algorithm for minimizing an objective function $f$ that is locally strongly convex with a sub--quadratic tail. This setting covers many widely used online statistical methods. We introduce a novel piecewise Lyapunov function that enables us to handle functions $f$ with only first-order differentiability, which includes a wide range of popular loss functions such as Huber loss. Leveraging our proposed Lyapunov function, we derive finite-time moment bounds under general diminishing stepsizes, as well as constant stepsizes. We further establish the weak convergence, central limit theorem and bias characterization under constant stepsize, providing the first geometrical convergence result for sub--quadratic SGD. Our results have wide applications, especially in online statistical methods. In particular, we discuss two applications of our results. 1) Online robust regression: We consider a corrupted linear model with sub--exponential covariates and heavy--tailed noise. Our analysis provides convergence rates comparable to those for corrupted models with Gaussian covariates and noise. 2) Online quantile regression: Importantly, our results relax the common assumption in prior work that the conditional density is continuous and provide a more fine-grained analysis for the moment bounds.\n        △ Less",
        "authors":"['Yixuan Zhang', 'Dongyan Huo', 'Yudong Chen', 'Qiaomin Xie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient measurement of neutral-atom qubits with matched filters",
        "abstract":"Quantum computers require high-fidelity measurement of many qubits to achieve a quantum advantage. Traditional approaches suffer from readout crosstalk for a neutral-atom quantum processor with a tightly spaced array. Although classical machine learning algorithms based on convolutional neural networks can improve fidelity, they are computationally expensive, making it difficult to scale them to large qubit counts. We present two simpler and scalable machine learning algorithms that realize matched filters for the readout problem. One is a local model that focuses on a single qubit, and the other uses information from neighboring qubits in the array to prevent crosstalk among the qubits. We demonstrate error reductions of up to 32% and 43% for the site and array models, respectively, compared to a conventional Gaussian threshold approach. Additionally, our array model uses two orders of magnitude fewer trainable parameters and four orders of magnitude fewer multiplications and nonlinear function evaluations than a recent convolutional neural network approach, with only a minor (3.5%) increase in error across different readout times. Another strength of our approach is its physical interpretability: the learned filter can be visualized to provide insights into experimental imperfections. We also show that a convolutional neural network model for improved can be pruned to have 70x and 4000x fewer parameters, respectively, while maintaining similar errors. Our work shows that simple machine learning approaches can achieve high-fidelity qubit measurements while remaining scalable to systems with larger qubit counts.\n        △ Less",
        "authors":"['Robert M. Kent', 'Linipun Phuttitarn', 'Chaithanya Naik Mude', 'Swamit Tannu', 'Mark Saffman', 'Gregory Lafyatis', 'Daniel J. Gauthier']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction",
        "abstract":"The predictions of click through rate (CTR) and conversion rate (CVR) play a crucial role in the success of ad-recommendation systems. A Deep Hierarchical Ensemble Network (DHEN) has been proposed to integrate multiple feature crossing modules and has achieved great success in CTR prediction. However, its performance for CVR prediction is unclear in the conversion ads setting, where an ad bids for the probability of a user's off-site actions on a third party website or app, including purchase, add to cart, sign up, etc. A few challenges in DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve the best trade-off between efficiency and efficacy? 3) What hyper-parameters to choose in each feature-crossing module? Orthogonal to the model architecture, the input personalization features also significantly impact model performance with a high degree of freedom. In this paper, we attack this problem and present our contributions biased to the applied data science side, including:\n  First, we propose a multitask learning framework with DHEN as the single backbone model architecture to predict all CVR tasks, with a detailed study on how to make DHEN work effectively in practice; Second, we build both on-site real-time user behavior sequences and off-site conversion event sequences for CVR prediction purposes, and conduct ablation study on its importance; Last but not least, we propose a self-supervised auxiliary loss to predict future actions in the input sequence, to help resolve the label sparseness issue in CVR prediction.\n  Our method achieves state-of-the-art performance compared to previous single feature crossing modules with pre-trained user personalization features.\n        △ Less",
        "authors":"['Jinfeng Zhuang', 'Yinrui Li', 'Runze Su', 'Ke Xu', 'Zhixuan Shao', 'Kungang Li', 'Ling Leng', 'Han Sun', 'Meng Qi', 'Yixiong Meng', 'Yang Tang', 'Zhifang Liu', 'Qifei Shen', 'Aayush Mudgal']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rethinking the Foundations for Continual Reinforcement Learning",
        "abstract":"Algorithms and approaches for continual reinforcement learning have gained increasing attention. Much of this early progress rests on the foundations and standard practices of traditional reinforcement learning, without questioning if they are well-suited to the challenges of continual learning agents. We suggest that many core foundations of traditional RL are, in fact, antithetical to the goals of continual reinforcement learning. We enumerate four such foundations: the Markov decision process formalism, a focus on optimal policies, the expected sum of rewards as the primary evaluation metric, and episodic benchmark environments that embrace the other three foundations. Shedding such sacredly held and taught concepts is not easy. They are self-reinforcing in that each foundation depends upon and holds up the others, making it hard to rethink each in isolation. We propose an alternative set of all four foundations that are better suited to the continual learning setting. We hope to spur on others in rethinking the traditional foundations, proposing and critiquing alternatives, and developing new algorithms and approaches enabled by better-suited foundations.\n        △ Less",
        "authors":"['Michael Bowling', 'Esraa Elelimy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"External-Wrench Estimation for Aerial Robots Exploiting a Learned Model",
        "abstract":"This paper presents an external wrench estimator that uses a hybrid dynamics model consisting of a first-principles model and a neural network. This framework addresses one of the limitations of the state-of-the-art model-based wrench observers: the wrench estimation of these observers comprises the external wrench (e.g. collision, physical interaction, wind); in addition to residual wrench (e.g. model parameters uncertainty or unmodeled dynamics). This is a problem if these wrench estimations are to be used as wrench feedback to a force controller, for example. In the proposed framework, a neural network is combined with a first-principles model to estimate the residual dynamics arising from unmodeled dynamics and parameters uncertainties, then, the hybrid trained model is used to estimate the external wrench, leading to a wrench estimation that has smaller contributions from the residual dynamics, and affected more by the external wrench. This method is validated with numerical simulations of an aerial robot in different flying scenarios and different types of residual dynamics, and the statistical analysis of the results shows that the wrench estimation error has improved significantly compared to a model-based wrench observer using only a first-principles model.\n        △ Less",
        "authors":"['Ayham Alharbat', 'Gabriele Ruscelli', 'Roberto Diversi', 'Abeje Mersha']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Bounded Exploration and Intermediate Actions for Data Debiasing",
        "abstract":"The performance of algorithmic decision rules is largely dependent on the quality of training datasets available to them. Biases in these datasets can raise economic and ethical concerns due to the resulting algorithms' disparate treatment of different groups. In this paper, we propose algorithms for sequentially debiasing the training dataset through adaptive and bounded exploration in a classification problem with costly and censored feedback. Our proposed algorithms balance between the ultimate goal of mitigating the impacts of data biases -- which will in turn lead to more accurate and fairer decisions, and the exploration risks incurred to achieve this goal. Specifically, we propose adaptive bounds to limit the region of exploration, and leverage intermediate actions which provide noisy label information at a lower cost. We analytically show that such exploration can help debias data in certain distributions, investigate how {algorithmic fairness interventions} can work in conjunction with our proposed algorithms, and validate the performance of these algorithms through numerical experiments on synthetic and real-world data.\n        △ Less",
        "authors":"['Yifan Yang', 'Yang Liu', 'Parinaz Naghizadeh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond Feature Importance: Feature Interactions in Predicting Post-Stroke Rigidity with Graph Explainable AI",
        "abstract":"This study addresses the challenge of predicting post-stroke rigidity by emphasizing feature interactions through graph-based explainable AI. Post-stroke rigidity, characterized by increased muscle tone and stiffness, significantly affects survivors' mobility and quality of life. Despite its prevalence, early prediction remains limited, delaying intervention. We analyze 519K stroke hospitalization records from the Healthcare Cost and Utilization Project dataset, where 43% of patients exhibited rigidity. We compare traditional approaches such as Logistic Regression, XGBoost, and Transformer with graph-based models like Graphormer and Graph Attention Network. These graph models inherently capture feature interactions and incorporate intrinsic or post-hoc explainability. Our results show that graph-based methods outperform others (AUROC 0.75), identifying key predictors such as NIH Stroke Scale and APR-DRG mortality risk scores. They also uncover interactions missed by conventional models. This research provides a novel application of graph-based XAI in stroke prognosis, with potential to guide early identification and personalized rehabilitation strategies.\n        △ Less",
        "authors":"['Jiawei Xu', 'Yonggeon Lee', 'Anthony Elkommos Youssef', 'Eunjin Yun', 'Tinglin Huang', 'Tianjian Guo', 'Hamidreza Saber', 'Rex Ying', 'Ying Ding']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LoRAX: LoRA eXpandable Networks for Continual Synthetic Image Attribution",
        "abstract":"As generative AI image technologies become more widespread and advanced, there is a growing need for strong attribution models. These models are crucial for verifying the authenticity of images and identifying the architecture of their originating generative models-key to maintaining media integrity. However, attribution models struggle to generalize to unseen models, and traditional fine-tuning methods for updating these models have shown to be impractical in real-world settings. To address these challenges, we propose LoRA eXpandable Networks (LoRAX), a parameter-efficient class incremental algorithm that adapts to novel generative image models without the need for full retraining. Our approach trains an extremely parameter-efficient feature extractor per continual learning task via Low Rank Adaptation. Each task-specific feature extractor learns distinct features while only requiring a small fraction of the parameters present in the underlying feature extractor's backbone model. Our extensive experimentation shows LoRAX outperforms or remains competitive with state-of-the-art class incremental learning algorithms on the Continual Deepfake Detection benchmark across all training scenarios and memory settings, while requiring less than 3% of the number of trainable parameters per feature extractor compared to the full-rank implementation. LoRAX code is available at: https:\/\/github.com\/mit-ll\/lorax_cil.\n        △ Less",
        "authors":"['Danielle Sullivan-Pao', 'Nicole Tian', 'Pooya Khorrami']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Orchestrating Agents and Data for Enterprise: A Blueprint Architecture for Compound AI",
        "abstract":"Large language models (LLMs) have gained significant interest in industry due to their impressive capabilities across a wide range of tasks. However, the widespread adoption of LLMs presents several challenges, such as integration into existing applications and infrastructure, utilization of company proprietary data, models, and APIs, and meeting cost, quality, responsiveness, and other requirements. To address these challenges, there is a notable shift from monolithic models to compound AI systems, with the premise of more powerful, versatile, and reliable applications. However, progress thus far has been piecemeal, with proposals for agentic workflows, programming models, and extended LLM capabilities, without a clear vision of an overall architecture. In this paper, we propose a 'blueprint architecture' for compound AI systems for orchestrating agents and data for enterprise applications. In our proposed architecture the key orchestration concept is 'streams' to coordinate the flow of data and instructions among agents. Existing proprietary models and APIs in the enterprise are mapped to 'agents', defined in an 'agent registry' that serves agent metadata and learned representations for search and planning. Agents can utilize proprietary data through a 'data registry' that similarly registers enterprise data of various modalities. Tying it all together, data and task 'planners' break down, map, and optimize tasks and queries for given quality of service (QoS) requirements such as cost, accuracy, and latency. We illustrate an implementation of the architecture for a use-case in the HR domain and discuss opportunities and challenges for 'agentic AI' in the enterprise.\n        △ Less",
        "authors":"['Eser Kandogan', 'Nikita Bhutani', 'Dan Zhang', 'Rafael Li Chen', 'Sairam Gurajada', 'Estevam Hruschka']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Variational quantum and neural quantum states algorithms for the linear complementarity problem",
        "abstract":"Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of concept, their practical utility in solving real-world problems -- and whether quantum-inspired classical algorithms can match their performance -- remains an open question. We present a novel application of the variational quantum linear solver (VQLS) and its classical neural quantum states-based counterpart, the variational neural linear solver (VNLS), as key components within a minimum map Newton solver for a complementarity-based rigid body contact model. We demonstrate using the VNLS that our solver accurately simulates the dynamics of rigid spherical bodies during collision events. These results suggest that quantum and quantum-inspired linear algebra algorithms can serve as viable alternatives to standard linear algebra solvers for modeling certain physical systems.\n        △ Less",
        "authors":"['Saibal De', 'Oliver Knitter', 'Rohan Kodati', 'Paramsothy Jayakumar', 'James Stokes', 'Shravan Veerapaneni']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Empowering Vector Architectures for ML: The CAMP Architecture for Matrix Multiplication",
        "abstract":"This study presents the Cartesian Accumulative Matrix Pipeline (CAMP) architecture, a novel approach designed to enhance matrix multiplication in Vector Architectures (VAs) and Single Instruction Multiple Data (SIMD) units. CAMP improves the processing efficiency of Quantized Neural Networks (QNNs). Matrix multiplication is a cornerstone of machine learning applications, and its quantized versions are increasingly popular for more efficient operations. Unfortunately, existing VAs and SIMD-support units struggle to efficiently handle these quantized formats. In this work, we propose CAMP, a simple yet effective architecture that leverages a hybrid multiplier. The CAMP architecture significantly advances the performance of vector architectures in handling quantized data, enabling more efficient execution of matrix multiplication across various platforms, specifically targeting the ARMv8 Scalable Vector Extension (SVE) and edge RISC-V SIMD-based architectures. In addition to increasing throughput, CAMP's architectural design also contributes to energy efficiency, making it an effective solution for low-power applications. Evaluations on a range of Large Language Models (LLMs) and Convolutional Neural Networks (CNNs) demonstrate that matrix multiplication operations using the proposed micro-architecture achieve up to 17$\\times$ and 23$\\times$ performance improvements compared to their respective baselines, the ARM A64FX core and a RISC-V-based edge System-on-Chip (SoC). Furthermore, synthesis and place-and-route (PnR) of the CAMP micro-architecture using Synopsys tools -- targeting ARM TSMC 7nm for A64FX and GlobalFoundries 22nm for the RISC-V SoC -- add only 1\\% and 4\\% area overhead, respectively, compared to the baseline designs.\n        △ Less",
        "authors":"['Mohammadreza Esmali Nojehdeh', 'Hossein Mokhtarnia', 'Julian Pavon Rivera', 'Narcis Rodas Quiroga', 'Roger Figueras Bagué', 'Enrico Reggiani', 'Miquel Moreto', 'Osman Unsal', 'Adrian Cristal', 'Eduard Ayguade']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A physics informed neural network approach to simulating ice dynamics governed by the shallow ice approximation",
        "abstract":"In this article we develop a Physics Informed Neural Network (PINN) approach to simulate ice sheet dynamics governed by the Shallow Ice Approximation. This problem takes the form of a time-dependent parabolic obstacle problem. Prior work has used this approach to address the stationary obstacle problem and here we extend it to the time dependent problem. Through comprehensive 1D and 2D simulations, we validate the model's effectiveness in capturing complex free-boundary conditions. By merging traditional mathematical modeling with cutting-edge deep learning methods, this approach provides a scalable and robust solution for predicting temporal variations in ice thickness. To illustrate this approach in a real world setting, we simulate the dynamics of the Devon Ice Cap, incorporating aerogeophysical data from 2000 and 2018.\n        △ Less",
        "authors":"['Kapil Chawla', 'William Holmes']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Between Linear and Sinusoidal: Rethinking the Time Encoder in Dynamic Graph Learning",
        "abstract":"Dynamic graph learning is essential for applications involving temporal networks and requires effective modeling of temporal relationships. Seminal attention-based models like TGAT and DyGFormer rely on sinusoidal time encoders to capture temporal relationships between edge events. In this paper, we study a simpler alternative: the linear time encoder, which avoids temporal information loss caused by sinusoidal functions and reduces the need for high dimensional time encoders. We show that the self-attention mechanism can effectively learn to compute time spans from linear time encodings and extract relevant temporal patterns. Through extensive experiments on six dynamic graph datasets, we demonstrate that the linear time encoder improves the performance of TGAT and DyGFormer in most cases. Moreover, the linear time encoder can lead to significant savings in model parameters with minimal performance loss. For example, compared to a 100-dimensional sinusoidal time encoder, TGAT with a 2-dimensional linear time encoder saves 43% of parameters and achieves higher average precision on five datasets. These results can be readily used to positively impact the design choices of a wide variety of dynamic graph learning architectures. The experimental code is available at: https:\/\/github.com\/hsinghuan\/dg-linear-time.git.\n        △ Less",
        "authors":"['Hsing-Huan Chung', 'Shravan Chaudhari', 'Xing Han', 'Yoav Wald', 'Suchi Saria', 'Joydeep Ghosh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Benchmarking Suite for Synthetic Aperture Radar Imagery Anomaly Detection (SARIAD) Algorithms",
        "abstract":"Anomaly detection is a key research challenge in computer vision and machine learning with applications in many fields from quality control to radar imaging. In radar imaging, specifically synthetic aperture radar (SAR), anomaly detection can be used for the classification, detection, and segmentation of objects of interest. However, there is no method for developing and benchmarking these methods on SAR imagery. To address this issue, we introduce SAR imagery anomaly detection (SARIAD). In conjunction with Anomalib, a deep-learning library for anomaly detection, SARIAD provides a comprehensive suite of algorithms and datasets for assessing and developing anomaly detection approaches on SAR imagery. SARIAD specifically integrates multiple SAR datasets along with tools to effectively apply various anomaly detection algorithms to SAR imagery. Several anomaly detection metrics and visualizations are available. Overall, SARIAD acts as a central package for benchmarking SAR models and datasets to allow for reproducible research in the field of anomaly detection in SAR imagery. This package is publicly available: https:\/\/github.com\/Advanced-Vision-and-Learning-Lab\/SARIAD.\n        △ Less",
        "authors":"['Lucian Chauvina', 'Somil Guptac', 'Angelina Ibarrac', 'Joshua Peeples']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RL-based Control of UAS Subject to Significant Disturbance",
        "abstract":"This paper proposes a Reinforcement Learning (RL)-based control framework for position and attitude control of an Unmanned Aerial System (UAS) subjected to significant disturbance that can be associated with an uncertain trigger signal. The proposed method learns the relationship between the trigger signal and disturbance force, enabling the system to anticipate and counteract the impending disturbances before they occur. We train and evaluate three policies: a baseline policy trained without exposure to the disturbance, a reactive policy trained with the disturbance but without the trigger signal, and a predictive policy that incorporates the trigger signal as an observation and is exposed to the disturbance during training. Our simulation results show that the predictive policy outperforms the other policies by minimizing position deviations through a proactive correction maneuver. This work highlights the potential of integrating predictive cues into RL frameworks to improve UAS performance.\n        △ Less",
        "authors":"['Kousheek Chakraborty', 'Thijs Hof', 'Ayham Alharbat', 'Abeje Mersha']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scaling Laws of Graph Neural Networks for Atomistic Materials Modeling",
        "abstract":"Atomistic materials modeling is a critical task with wide-ranging applications, from drug discovery to materials science, where accurate predictions of the target material property can lead to significant advancements in scientific discovery. Graph Neural Networks (GNNs) represent the state-of-the-art approach for modeling atomistic material data thanks to their capacity to capture complex relational structures. While machine learning performance has historically improved with larger models and datasets, GNNs for atomistic materials modeling remain relatively small compared to large language models (LLMs), which leverage billions of parameters and terabyte-scale datasets to achieve remarkable performance in their respective domains. To address this gap, we explore the scaling limits of GNNs for atomistic materials modeling by developing a foundational model with billions of parameters, trained on extensive datasets in terabyte-scale. Our approach incorporates techniques from LLM libraries to efficiently manage large-scale data and models, enabling both effective training and deployment of these large-scale GNN models. This work addresses three fundamental questions in scaling GNNs: the potential for scaling GNN model architectures, the effect of dataset size on model accuracy, and the applicability of LLM-inspired techniques to GNN architectures. Specifically, the outcomes of this study include (1) insights into the scaling laws for GNNs, highlighting the relationship between model size, dataset volume, and accuracy, (2) a foundational GNN model optimized for atomistic materials modeling, and (3) a GNN codebase enhanced with advanced LLM-based training techniques. Our findings lay the groundwork for large-scale GNNs with billions of parameters and terabyte-scale datasets, establishing a scalable pathway for future advancements in atomistic materials modeling.\n        △ Less",
        "authors":"['Chaojian Li', 'Zhifan Ye', 'Massimiliano Lupo Pasini', 'Jong Youl Choi', 'Cheng Wan', 'Yingyan Celine Lin', 'Prasanna Balaprakash']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-view autoencoders for Fake News Detection",
        "abstract":"Given the volume and speed at which fake news spreads across social media, automatic fake news detection has become a highly important task. However, this task presents several challenges, including extracting textual features that contain relevant information about fake news. Research about fake news detection shows that no single feature extraction technique consistently outperforms the others across all scenarios. Nevertheless, different feature extraction techniques can provide complementary information about the textual data and enable a more comprehensive representation of the content. This paper proposes using multi-view autoencoders to generate a joint feature representation for fake news detection by integrating several feature extraction techniques commonly used in the literature. Experiments on fake news datasets show a significant improvement in classification performance compared to individual views (feature representations). We also observed that selecting a subset of the views instead of composing a latent space with all the views can be advantageous in terms of accuracy and computational effort. For further details, including source codes, figures, and datasets, please refer to the project's repository: https:\/\/github.com\/ingrydpereira\/multiview-fake-news.\n        △ Less",
        "authors":"['Ingryd V. S. T. Pereira', 'George D. C. Cavalcanti', 'Rafael M. O. Cruz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Differentially Private Selection using Smooth Sensitivity",
        "abstract":"Differentially private selection mechanisms offer strong privacy guarantees for queries aiming to identify the top-scoring element r from a finite set R, based on a dataset-dependent utility function. While selection queries are fundamental in data science, few mechanisms effectively ensure their privacy. Furthermore, most approaches rely on global sensitivity to achieve differential privacy (DP), which can introduce excessive noise and impair downstream inferences. To address this limitation, we propose the Smooth Noisy Max (SNM) mechanism, which leverages smooth sensitivity to yield provably tighter (upper bounds on) expected errors compared to global sensitivity-based methods. Empirical results demonstrate that SNM is more accurate than state-of-the-art differentially private selection methods in three applications: percentile selection, greedy decision trees, and random forests.\n        △ Less",
        "authors":"['Iago Chaves', 'Victor Farias', 'Amanda Perez', 'Diego Mesquita', 'Javam Machado']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Programs as Singularities",
        "abstract":"We develop a correspondence between the structure of Turing machines and the structure of singularities of real analytic functions, based on connecting the Ehrhard-Regnier derivative from linear logic with the role of geometry in Watanabe's singular learning theory. The correspondence works by embedding ordinary (discrete) Turing machine codes into a family of noisy codes which form a smooth parameter space. On this parameter space we consider a potential function which has Turing machines as critical points. By relating the Taylor series expansion of this potential at such a critical point to combinatorics of error syndromes, we relate the local geometry to internal structure of the Turing machine.\n  The potential in question is the negative log-likelihood for a statistical model, so that the structure of the Turing machine and its associated singularity is further related to Bayesian inference. Two algorithms that produce the same predictive function can nonetheless correspond to singularities with different geometries, which implies that the Bayesian posterior can discriminate between distinct algorithmic implementations, contrary to a purely functional view of inference. In the context of singular learning theory our results point to a more nuanced understanding of Occam's razor and the meaning of simplicity in inductive inference.\n        △ Less",
        "authors":"['Daniel Murfet', 'Will Troiani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Reinforcement Learning for Day-to-day Dynamic Tolling in Tradable Credit Schemes",
        "abstract":"Tradable credit schemes (TCS) are an increasingly studied alternative to congestion pricing, given their revenue neutrality and ability to address issues of equity through the initial credit allocation. Modeling TCS to aid future design and implementation is associated with challenges involving user and market behaviors, demand-supply dynamics, and control mechanisms. In this paper, we focus on the latter and address the day-to-day dynamic tolling problem under TCS, which is formulated as a discrete-time Markov Decision Process and solved using reinforcement learning (RL) algorithms. Our results indicate that RL algorithms achieve travel times and social welfare comparable to the Bayesian optimization benchmark, with generalization across varying capacities and demand levels. We further assess the robustness of RL under different hyperparameters and apply regularization techniques to mitigate action oscillation, which generates practical tolling strategies that are transferable under day-to-day demand and supply variability. Finally, we discuss potential challenges such as scaling to large networks, and show how transfer learning can be leveraged to improve computational efficiency and facilitate the practical deployment of RL-based TCS solutions.\n        △ Less",
        "authors":"['Xiaoyi Wu', 'Ravi Seshadri', 'Filipe Rodrigues', 'Carlos Lima Azevedo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
        "abstract":"AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https:\/\/github.com\/SakanaAI\/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.\n        △ Less",
        "authors":"['Yutaro Yamada', 'Robert Tjarko Lange', 'Cong Lu', 'Shengran Hu', 'Chris Lu', 'Jakob Foerster', 'Jeff Clune', 'David Ha']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Vector Quantized-Elites: Unsupervised and Problem-Agnostic Quality-Diversity Optimization",
        "abstract":"Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowledge of the task to define the behavioral space grid, limiting their flexibility and applicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites), a novel Quality-Diversity algorithm that autonomously constructs a structured behavioral space grid using unsupervised learning, eliminating the need for prior task-specific knowledge. At the core of VQ-Elites is the integration of Vector Quantized Variational Autoencoders, which enables the dynamic learning of behavioral descriptors and the generation of a structured, rather than unstructured, behavioral space grid - a significant advancement over existing unsupervised Quality-Diversity approaches. This design establishes VQ-Elites as a flexible, robust, and task-agnostic optimization framework. To further enhance the performance of unsupervised Quality-Diversity algorithms, we introduce two key components: behavioral space bounding and cooperation mechanisms, which significantly improve convergence and performance. We validate VQ-Elites on robotic arm pose-reaching and mobile robot space-covering tasks. The results demonstrate its ability to efficiently generate diverse, high-quality solutions, emphasizing its adaptability, scalability, robustness to hyperparameters, and potential to extend Quality-Diversity optimization to complex, previously inaccessible domains.\n        △ Less",
        "authors":"['Constantinos Tsakonas', 'Konstantinos Chatzilygeroudis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Task Learning with Multi-Annotation Triplet Loss for Improved Object Detection",
        "abstract":"Triplet loss traditionally relies only on class labels and does not use all available information in multi-task scenarios where multiple types of annotations are available. This paper introduces a Multi-Annotation Triplet Loss (MATL) framework that extends triplet loss by incorporating additional annotations, such as bounding box information, alongside class labels in the loss formulation. By using these complementary annotations, MATL improves multi-task learning for tasks requiring both classification and localization. Experiments on an aerial wildlife imagery dataset demonstrate that MATL outperforms conventional triplet loss in both classification and localization. These findings highlight the benefit of using all available annotations for triplet loss in multi-task learning frameworks.\n        △ Less",
        "authors":"['Meilun Zhou', 'Aditya Dutt', 'Alina Zare']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Compositional Flows for 3D Molecule and Synthesis Pathway Co-design",
        "abstract":"Many generative applications, such as synthesis-based 3D molecular design, involve constructing compositional objects with continuous features. Here, we introduce Compositional Generative Flows (CGFlow), a novel framework that extends flow matching to generate objects in compositional steps while modeling continuous states. Our key insight is that modeling compositional state transitions can be formulated as a straightforward extension of the flow matching interpolation process. We further build upon the theoretical foundations of generative flow networks (GFlowNets), enabling reward-guided sampling of compositional structures. We apply CGFlow to synthesizable drug design by jointly designing the molecule's synthetic pathway with its 3D binding pose. Our approach achieves state-of-the-art binding affinity on all 15 targets from the LIT-PCBA benchmark, and 5.8$\\times$ improvement in sampling efficiency compared to 2D synthesis-based baseline. To our best knowledge, our method is also the first to achieve state of-art-performance in both Vina Dock (-9.38) and AiZynth success rate (62.2\\%) on the CrossDocked benchmark.\n        △ Less",
        "authors":"['Tony Shen', 'Seonghwan Seo', 'Ross Irwin', 'Kieran Didi', 'Simon Olsson', 'Woo Youn Kim', 'Martin Ester']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Self-Bootstrapping for Versatile Test-Time Adaptation",
        "abstract":"In this paper, we seek to develop a versatile test-time adaptation (TTA) objective for a variety of tasks - classification and regression across image-, object-, and pixel-level predictions. We achieve this through a self-bootstrapping scheme that optimizes prediction consistency between the test image (as target) and its deteriorated view. The key challenge lies in devising effective augmentations\/deteriorations that: i) preserve the image's geometric information, e.g., object sizes and locations, which is crucial for TTA on object\/pixel-level tasks, and ii) provide sufficient learning signals for TTA. To this end, we analyze how common distribution shifts affect the image's information power across spatial frequencies in the Fourier domain, and reveal that low-frequency components carry high power and masking these components supplies more learning signals, while masking high-frequency components can not. In light of this, we randomly mask the low-frequency amplitude of an image in its Fourier domain for augmentation. Meanwhile, we also augment the image with noise injection to compensate for missing learning signals at high frequencies, by enhancing the information power there. Experiments show that, either independently or as a plug-and-play module, our method achieves superior results across classification, segmentation, and 3D monocular detection tasks with both transformer and CNN models.\n        △ Less",
        "authors":"['Shuaicheng Niu', 'Guohao Chen', 'Peilin Zhao', 'Tianyi Wang', 'Pengcheng Wu', 'Zhiqi Shen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning",
        "abstract":"In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus on balancing these two aspects at the network level, lacking sufficient differentiation and fine-grained control of individual neurons. To overcome this limitation, we propose Neuron-level Balance between Stability and Plasticity (NBSP) method, by taking inspiration from the observation that specific neurons are strongly relevant to task-relevant skills. Specifically, NBSP first (1) defines and identifies RL skill neurons that are crucial for knowledge retention through a goal-oriented method, and then (2) introduces a framework by employing gradient masking and experience replay techniques targeting these neurons to preserve the encoded existing skills while enabling adaptation to new tasks. Numerous experimental results on the Meta-World and Atari benchmarks demonstrate that NBSP significantly outperforms existing approaches in balancing stability and plasticity.\n        △ Less",
        "authors":"['Jiahua Lan', 'Sen Zhang', 'Haixia Pan', 'Ruijun Liu', 'Li Shen', 'Dacheng Tao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fusing Global and Local: Transformer-CNN Synergy for Next-Gen Current Estimation",
        "abstract":"This paper presents a hybrid model combining Transformer and CNN for predicting the current waveform in signal lines. Unlike traditional approaches such as current source models, driver linear representations, waveform functional fitting, or equivalent load capacitance methods, our model does not rely on fixed simplified models of standard-cell drivers or RC loads. Instead, it replaces the complex Newton iteration process used in traditional SPICE simulations, leveraging the powerful sequence modeling capabilities of the Transformer framework to directly predict current responses without iterative solving steps. The hybrid architecture effectively integrates the global feature-capturing ability of Transformers with the local feature extraction advantages of CNNs, significantly improving the accuracy of current waveform predictions.\n  Experimental results demonstrate that, compared to traditional SPICE simulations, the proposed algorithm achieves an error of only 0.0098. These results highlight the algorithm's superior capabilities in predicting signal line current waveforms, timing analysis, and power evaluation, making it suitable for a wide range of technology nodes, from 40nm to 3nm.\n        △ Less",
        "authors":"['Junlang Huang', 'Hao Chen', 'Li Luo', 'Yong Cai', 'Lexin Zhang', 'Tianhao Ma', 'Yitian Zhang', 'Zhong Guan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Simple Machine Learning Baselines for GNSS RFI Detection",
        "abstract":"Machine learning research in GNSS radio frequency interference (RFI) detection often lacks a clear empirical justification for the choice of deep learning architectures over simpler machine learning approaches. In this work, we argue for a change in research direction-from developing ever more complex deep learning models to carefully assessing their real-world effectiveness in comparison to interpretable and lightweight machine learning baselines. Our findings reveal that state-of-the-art deep learning models frequently fail to outperform simple, well-engineered machine learning methods in the context of GNSS RFI detection. Leveraging a unique large-scale dataset collected by the Swiss Air Force and Swiss Air-Rescue (Rega), and preprocessed by Swiss Air Navigation Services Ltd. (Skyguide), we demonstrate that a simple baseline model achieves 91\\% accuracy in detecting GNSS RFI, outperforming more complex deep learning counterparts. These results highlight the effectiveness of pragmatic solutions and offer valuable insights to guide future research in this critical application domain.\n        △ Less",
        "authors":"['Viktor Ivanov', 'Richard C. Wilson', 'Maurizio Scaramuzza']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative analysis of Realistic EMF Exposure Estimation from Low Density Sensor Network by Finite & Infinite Neural Networks",
        "abstract":"Understanding the spatial and temporal patterns of environmental exposure to radio-frequency electromagnetic fields (RF-EMF) is essential for conducting risk assessments. These assessments aim to explore potential connections between RF-EMF exposure and its effects on human health, as well as on wildlife and plant life. Existing research has used different machine learning tools for EMF exposure estimation; however, a comparative analysis of these techniques is required to better understand their performance for real-world datasets. In this work, we present both finite and infinite-width convolutional network-based methods to estimate and assess EMF exposure levels from 70 real-world sensors in Lille, France. A comparative analysis has been conducted to analyze the performance of the methods' execution time and estimation accuracy. To improve estimation accuracy for higher-resolution grids, we utilized a preconditioned gradient descent method for kernel estimation. Root Mean Square Error (RMSE) is used as the evaluation criterion for comparing the performance of these deep learning models.\n        △ Less",
        "authors":"['Mohammed Mallik', 'Laurent Clavier', 'Davy P. Gaillot']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"mixEEG: Enhancing EEG Federated Learning for Cross-subject EEG Classification with Tailored mixup",
        "abstract":"The cross-subject electroencephalography (EEG) classification exhibits great challenges due to the diversity of cognitive processes and physiological structures between different subjects. Modern EEG models are based on neural networks, demanding a large amount of data to achieve high performance and generalizability. However, privacy concerns associated with EEG pose significant limitations to data sharing between different hospitals and institutions, resulting in the lack of large dataset for most EEG tasks. Federated learning (FL) enables multiple decentralized clients to collaboratively train a global model without direct communication of raw data, thus preserving privacy. For the first time, we investigate the cross-subject EEG classification in the FL setting. In this paper, we propose a simple yet effective framework termed mixEEG. Specifically, we tailor the vanilla mixup considering the unique properties of the EEG modality. mixEEG shares the unlabeled averaged data of the unseen subject rather than simply sharing raw data under the domain adaptation setting, thus better preserving privacy and offering an averaged label as pseudo-label. Extensive experiments are conducted on an epilepsy detection and an emotion recognition dataset. The experimental result demonstrates that our mixEEG enhances the transferability of global model for cross-subject EEG classification consistently across different datasets and model architectures. Code is published at: https:\/\/github.com\/XuanhaoLiu\/mixEEG.\n        △ Less",
        "authors":"['Xuan-Hao Liu', 'Bao-Liang Lu', 'Wei-Long Zheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EquiNO: A Physics-Informed Neural Operator for Multiscale Simulations",
        "abstract":"Multiscale problems are ubiquitous in physics. Numerical simulations of such problems by solving partial differential equations (PDEs) at high resolution are computationally too expensive for many-query scenarios, e.g., uncertainty quantification, remeshing applications, topology optimization, and so forth. This limitation has motivated the application of data-driven surrogate models, where the microscale computations are $\\textit{substituted}$ with a surrogate, usually acting as a black-box mapping between macroscale quantities. These models offer significant speedups but struggle with incorporating microscale physical constraints, such as the balance of linear momentum and constitutive models. In this contribution, we propose Equilibrium Neural Operator (EquiNO) as a $\\textit{complementary}$ physics-informed PDE surrogate for predicting microscale physics and compare it with variational physics-informed neural and operator networks. Our framework, applicable to the so-called multiscale FE$^{\\,2}\\,$ computations, introduces the FE-OL approach by integrating the finite element (FE) method with operator learning (OL). We apply the proposed FE-OL approach to quasi-static problems of solid mechanics. The results demonstrate that FE-OL can yield accurate solutions even when confronted with a restricted dataset during model development. Our results show that EquiNO achieves speedup factors exceeding 8000-fold compared to traditional methods and offers an optimal balance between data-driven and physics-based strategies.\n        △ Less",
        "authors":"['Hamidreza Eivazi', 'Jendrik-Alexander Tröger', 'Stefan Wittek', 'Stefan Hartmann', 'Andreas Rausch']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-user Wireless Image Semantic Transmission over MIMO Multiple Access Channels",
        "abstract":"This paper focuses on a typical uplink transmission scenario over multiple-input multiple-output multiple access channel (MIMO-MAC) and thus propose a multi-user learnable CSI fusion semantic communication (MU-LCFSC) framework. It incorporates CSI as the side information into both the semantic encoders and decoders to generate a proper feature mask map in order to produce a more robust attention weight distribution. Especially for the decoding end, a cooperative successive interference cancellation procedure is conducted along with a cooperative mask ratio generator, which flexibly controls the mask elements of feature mask maps. Numerical results verify the superiority of proposed MU-LCFSC compared to DeepJSCC-NOMA over 3 dB in terms of PSNR.\n        △ Less",
        "authors":"['Bingyan Xie', 'Yongpeng Wu', 'Feng Shu', 'Jiangzhou Wang', 'Wenjun Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Double Directional Wireless Channel Generation: A Statistics-Informed Generative Approach",
        "abstract":"Channel models that represent various operating conditions a communication system might experience are important for design and standardization of any communication system. While statistical channel models have long dominated this space, machine learning (ML) is becoming a popular alternative approach. However, existing approaches have mostly focused on predictive solutions to match instantaneous channel realizations. Other solutions have focused on pathloss modeling, while double-directional (DD) channel representation is needed for a complete description. Motivated by this, we (a) develop a generative solution that uses a hybrid Transformer (hTransformer) model with a low-rank projected attention calculation mechanism and a bi-directional long short-term memory (BiLSTM) layer to generate complete DD channel information and (b) design a domain-knowledge-informed training method to match the generated and true channel realizations' statistics. Our extensive simulation results validate that the generated samples' statistics closely align with the true statistics while mostly outperforming the performance of existing predictive approaches.\n        △ Less",
        "authors":"['Md-Ferdous Pervej', 'Patel Pratik', 'Koushik Manjunatha', 'Prasad Shamain', 'Andreas F. Molisch']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments",
        "abstract":"Small and mid-sized generative language models have gained increasing attention. Their size and availability make them amenable to being analyzed at a behavioral as well as a representational level, allowing investigations of how these levels interact. We evaluate 32 publicly available language models for their representational and behavioral alignment with human similarity judgments on a word triplet task. This provides a novel evaluation setting to probe semantic associations in language beyond common pairwise comparisons. We find that (1) even the representations of small language models can achieve human-level alignment, (2) instruction-tuned model variants can exhibit substantially increased agreement, (3) the pattern of alignment across layers is highly model dependent, and (4) alignment based on models' behavioral responses is highly dependent on model size, matching their representational alignment only for the largest evaluated models.\n        △ Less",
        "authors":"['Lorenz Linhardt', 'Tom Neuhäuser', 'Lenka Tětková', 'Oliver Eberle']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing",
        "abstract":"Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or \"re-mixing\" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's \"successful neighbors\" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples\/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to \"Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt\/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.\n        △ Less",
        "authors":"['Zhongyang Li', 'Ziyue Li', 'Tianyi Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory",
        "abstract":"Despite their impressive performance on complex tasks, current language models (LMs) typically operate in a vacuum: Each input query is processed separately, without retaining insights from previous attempts. Here, we present Dynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM with a persistent, evolving memory. Rather than repeatedly re-discovering or re-committing the same solutions and mistakes, DC enables models to store and reuse accumulated strategies, code snippets, and general problem-solving insights at inference time. This test-time learning enhances performance substantially across a range of tasks without needing explicit ground-truth labels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than doubled on AIME math exams once it began retaining algebraic insights across questions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to 99% after the model discovered and reused a Python-based solution. In tasks prone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o and Claude to reach near-perfect accuracy by recalling previously validated code, whereas their baselines stagnated around 50%. Beyond arithmetic challenges, DC yields notable accuracy gains on knowledge-demanding tasks. Claude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro problems. Crucially, DC's memory is self-curated, focusing on concise, transferable snippets rather than entire transcript. Unlike finetuning or static retrieval methods, DC adapts LMs' problem-solving skills on the fly, without modifying their underlying parameters. Overall, our findings present DC as a promising approach for augmenting LMs with persistent memory, bridging the divide between isolated inference events and the cumulative, experience-driven learning characteristic of human cognition.\n        △ Less",
        "authors":"['Mirac Suzgun', 'Mert Yuksekgonul', 'Federico Bianchi', 'Dan Jurafsky', 'James Zou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Trading Graph Neural Network",
        "abstract":"This paper proposes a new algorithm -- Trading Graph Neural Network (TGNN) that can structurally estimate the impact of asset features, dealer features and relationship features on asset prices in trading networks. It combines the strength of the traditional simulated method of moments (SMM) and recent machine learning techniques -- Graph Neural Network (GNN). It outperforms existing reduced-form methods with network centrality measures in prediction accuracy. The method can be used on networks with any structure, allowing for heterogeneity among both traders and assets.\n        △ Less",
        "authors":"['Xian Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Semantically Encoding Activity Labels for Context-Aware Human Activity Recognition",
        "abstract":"Prior work has primarily formulated CA-HAR as a multi-label classification problem, where model inputs are time-series sensor data and target labels are binary encodings representing whether a given activity or context occurs. These CA-HAR methods either predicted each label independently or manually imposed relationships using graphs. However, both strategies often neglect an essential aspect: activity labels have rich semantic relationships. For instance, walking, jogging, and running activities share similar movement patterns but differ in pace and intensity, indicating that they are semantically related. Consequently, prior CA-HAR methods often struggled to accurately capture these inherent and nuanced relationships, particularly on datasets with noisy labels typically used for CA-HAR or situations where the ideal sensor type is unavailable (e.g., recognizing speech without audio sensors). To address this limitation, we propose SEAL, which leverage LMs to encode CA-HAR activity labels to capture semantic relationships. LMs generate vector embeddings that preserve rich semantic information from natural language. Our SEAL approach encodes input-time series sensor data from smart devices and their associated activity and context labels (text) as vector embeddings. During training, SEAL aligns the sensor data representations with their corresponding activity\/context label embeddings in a shared embedding space. At inference time, SEAL performs a similarity search, returning the CA-HAR label with the embedding representation closest to the input data. Although LMs have been widely explored in other domains, surprisingly, their potential in CA-HAR has been underexplored, making our approach a novel contribution to the field. Our research opens up new possibilities for integrating more advanced LMs into CA-HAR tasks.\n        △ Less",
        "authors":"['Wen Ge', 'Guanyi Mou', 'Emmanuel O. Agu', 'Kyumin Lee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining",
        "abstract":"Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding. Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood. Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models. In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets. We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales. Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data. We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization. Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks. Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior.\n        △ Less",
        "authors":"['Rosie Zhao', 'Alexandru Meterez', 'Sham Kakade', 'Cengiz Pehlevan', 'Samy Jelassi', 'Eran Malach']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hodge Laplacians and Hodge Diffusion Maps",
        "abstract":"We introduce Hodge Diffusion Maps, a novel manifold learning algorithm designed to analyze and extract topological information from high-dimensional data-sets. This method approximates the exterior derivative acting on differential forms, thereby providing an approximation of the Hodge Laplacian operator. Hodge Diffusion Maps extend existing non-linear dimensionality reduction techniques, including vector diffusion maps, as well as the theories behind diffusion maps and Laplacian Eigenmaps. Our approach captures higher-order topological features of the data-set by projecting it into lower-dimensional Euclidean spaces using the Hodge Laplacian. We develop a theoretical framework to estimate the approximation error of the exterior derivative, based on sample points distributed over a real manifold. Numerical experiments support and validate the proposed methodology.\n        △ Less",
        "authors":"['Alvaro Almeida Gomez', 'Jorge Duque Franco']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Porting an LLM based Application from ChatGPT to an On-Premise Environment",
        "abstract":"Given the data-intensive nature of Machine Learning (ML) systems in general, and Large Language Models (LLM) in particular, using them in cloud based environments can become a challenge due to legislation related to privacy and security of data. Taking such aspects into consideration implies porting the LLMs to an on-premise environment, where privacy and security can be controlled. In this paper, we study this porting process of a real-life application using ChatGPT, which runs in a public cloud, to an on-premise environment. The application being ported is AIPA, a system that leverages Large Language Models (LLMs) and sophisticated data analytics to enhance the assessment of procurement call bids. The main considerations in the porting process include transparency of open source models and cost of hardware, which are central design choices of the on-premise environment. In addition to presenting the porting process, we evaluate downsides and benefits associated with porting.\n        △ Less",
        "authors":"['Teemu Paloniemi', 'Manu Setälä', 'Tommi Mikkonen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Efficacy of Semantics-Preserving Transformations in Self-Supervised Learning for Medical Ultrasound",
        "abstract":"Data augmentation is a central component of joint embedding self-supervised learning (SSL). Approaches that work for natural images may not always be effective in medical imaging tasks. This study systematically investigated the impact of data augmentation and preprocessing strategies in SSL for lung ultrasound. Three data augmentation pipelines were assessed: (1) a baseline pipeline commonly used across imaging domains, (2) a novel semantic-preserving pipeline designed for ultrasound, and (3) a distilled set of the most effective transformations from both pipelines. Pretrained models were evaluated on multiple classification tasks: B-line detection, pleural effusion detection, and COVID-19 classification. Experiments revealed that semantics-preserving data augmentation resulted in the greatest performance for COVID-19 classification - a diagnostic task requiring global image context. Cropping-based methods yielded the greatest performance on the B-line and pleural effusion object classification tasks, which require strong local pattern recognition. Lastly, semantics-preserving ultrasound image preprocessing resulted in increased downstream performance for multiple tasks. Guidance regarding data augmentation and preprocessing strategies was synthesized for practitioners working with SSL in ultrasound.\n        △ Less",
        "authors":"['Blake VanBerlo', 'Alexander Wong', 'Jesse Hoey', 'Robert Arntfield']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How do Large Language Models Understand Relevance? A Mechanistic Interpretability Perspective",
        "abstract":"Recent studies have shown that large language models (LLMs) can assess relevance and support information retrieval (IR) tasks such as document ranking and relevance judgment generation. However, the internal mechanisms by which off-the-shelf LLMs understand and operationalize relevance remain largely unexplored. In this paper, we systematically investigate how different LLM modules contribute to relevance judgment through the lens of mechanistic interpretability. Using activation patching techniques, we analyze the roles of various model components and identify a multi-stage, progressive process in generating either pointwise or pairwise relevance judgment. Specifically, LLMs first extract query and document information in the early layers, then process relevance information according to instructions in the middle layers, and finally utilize specific attention heads in the later layers to generate relevance judgments in the required format. Our findings provide insights into the mechanisms underlying relevance assessment in LLMs, offering valuable implications for future research on leveraging LLMs for IR tasks.\n        △ Less",
        "authors":"['Qi Liu', 'Jiaxin Mao', 'Ji-Rong Wen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fast Adaptation with Behavioral Foundation Models",
        "abstract":"Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful paradigm for pretraining behavioral foundation models (BFMs), enabling agents to solve a wide range of downstream tasks specified via reward functions in a zero-shot fashion, i.e., without additional test-time learning or planning. This is achieved by learning self-supervised task embeddings alongside corresponding near-optimal behaviors and incorporating an inference procedure to directly retrieve the latent task embedding and associated policy for any given reward function. Despite promising results, zero-shot policies are often suboptimal due to errors induced by the unsupervised training process, the embedding, and the inference procedure. In this paper, we focus on devising fast adaptation strategies to improve the zero-shot performance of BFMs in a few steps of online interaction with the environment while avoiding any performance drop during the adaptation process. Notably, we demonstrate that existing BFMs learn a set of skills containing more performant policies than those identified by their inference procedure, making them well-suited for fast adaptation. Motivated by this observation, we propose both actor-critic and actor-only fast adaptation strategies that search in the low-dimensional task-embedding space of the pre-trained BFM to rapidly improve the performance of its zero-shot policies on any downstream task. Notably, our approach mitigates the initial \"unlearning\" phase commonly observed when fine-tuning pre-trained RL models. We evaluate our fast adaptation strategies on top of four state-of-the-art zero-shot RL methods in multiple navigation and locomotion domains. Our results show that they achieve 10-40% improvement over their zero-shot performance in a few tens of episodes, outperforming existing baselines.\n        △ Less",
        "authors":"['Harshit Sikchi', 'Andrea Tirinzoni', 'Ahmed Touati', 'Yingchen Xu', 'Anssi Kanervisto', 'Scott Niekum', 'Amy Zhang', 'Alessandro Lazaric', 'Matteo Pirotta']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows",
        "abstract":"Many real-world applications of flow-based generative models desire a diverse set of samples that cover multiple modes of the target distribution. However, the predominant approach for obtaining diverse sets is not sample-efficient, as it involves independently obtaining many samples from the source distribution and mapping them through the flow until the desired mode coverage is achieved. As an alternative to repeated sampling, we introduce DiverseFlow: a training-free approach to improve the diversity of flow models. Our key idea is to employ a determinantal point process to induce a coupling between the samples that drives diversity under a fixed sampling budget. In essence, DiverseFlow allows exploration of more variations in a learned flow model with fewer samples. We demonstrate the efficacy of our method for tasks where sample-efficient diversity is desirable, such as text-guided image generation with polysemous words, inverse problems like large-hole inpainting, and class-conditional image synthesis.\n        △ Less",
        "authors":"['Mashrur M. Morshed', 'Vishnu Boddeti']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning",
        "abstract":"Recent advances in inference-time compute have significantly improved performance on complex tasks by generating long chains of thought (CoTs) using Large Reasoning Models (LRMs). However, this improved accuracy comes at the cost of high inference latency due to the length of generated reasoning sequences and the autoregressive nature of decoding. Our key insight in tackling these overheads is that LRM inference, and the reasoning that it embeds, is highly tolerant of approximations: complex tasks are typically broken down into simpler steps, each of which brings utility based on the semantic insight it provides for downstream steps rather than the exact tokens it generates. Accordingly, we introduce SpecReason, a system that automatically accelerates LRM inference by using a lightweight model to (speculatively) carry out simpler intermediate reasoning steps and reserving the costly base model only to assess (and potentially correct) the speculated outputs. Importantly, SpecReason's focus on exploiting the semantic flexibility of thinking tokens in preserving final-answer accuracy is complementary to prior speculation techniques, most notably speculative decoding, which demands token-level equivalence at each step. Across a variety of reasoning benchmarks, SpecReason achieves 1.5-2.5$\\times$ speedup over vanilla LRM inference while improving accuracy by 1.0-9.9\\%. Compared to speculative decoding without SpecReason, their combination yields an additional 19.4-44.2\\% latency reduction. We open-source SpecReason at https:\/\/github.com\/ruipeterpan\/specreason.\n        △ Less",
        "authors":"['Rui Pan', 'Yinwei Dai', 'Zhihao Zhang', 'Gabriele Oliaro', 'Zhihao Jia', 'Ravi Netravali']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust Hallucination Detection in LLMs via Adaptive Token Selection",
        "abstract":"Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches.\n        △ Less",
        "authors":"['Mengjia Niu', 'Hamed Haddadi', 'Guansong Pang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Review of HPC-Accelerated CFD in National Security and Defense",
        "abstract":"Using High-Performance Computing (HPC), Computational Fluid Dynamics (CFD) now serves as an essential component in defense-related national security applications including missile interception and hypersonic propulsion as well as naval stealth optimization and urban hazard dispersion. This review combines two decades of open-source and public-domain research on HPC-accelerated CFD in defense, addressing three key questions: Which security-sensitive simulations have utilized open-source CFD frameworks such as OpenFOAM, SU2 and ADflow? Which HPC techniques, such as MPI domain decomposition and GPU acceleration together with hybrid parallelism best enhance open-source frameworks to manage large defense CFD simulations? Which technological advancements and research voids currently drive the directional development of the field? Examining several research studies sourced from NASA, DoD HPC centers, and academic institutions, scientific contributions have been classified into air, maritime, and space domains. Modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers show how custom open-source solutions support workflows with rapid completion of multi-million cell simulations. The conclusion highlights new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to deliver practical insights for improving CFD use in defense research and development.\n        △ Less",
        "authors":"['James Afful']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks",
        "abstract":"Motivated by the growing demand for low-precision arithmetic in computational science, we exploit lower-precision emulation in Python -- widely regarded as the dominant programming language for numerical analysis and machine learning. Low-precision training has revolutionized deep learning by enabling more efficient computation and reduced memory and energy consumption while maintaining model fidelity. To better enable numerical experimentation with and exploration of low precision computation, we developed the Pychop library, which supports customizable floating-point formats and a comprehensive set of rounding modes in Python, allowing users to benefit from fast, low-precision emulation in numerous applications. Pychop also introduces interfaces for both PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural network training and inference with unparalleled flexibility.\n  In this paper, we offer a comprehensive exposition of the design, implementation, validation, and practical application of Pychop, establishing it as a foundational tool for advancing efficient mixed-precision algorithms. Furthermore, we present empirical results on low-precision emulation for image classification and object detection using published datasets, illustrating the sensitivity of the use of low precision and offering valuable insights into its impact. Pychop enables in-depth investigations into the effects of numerical precision, facilitates the development of novel hardware accelerators, and integrates seamlessly into existing deep learning workflows. Software and experimental code are publicly available at https:\/\/github.com\/inEXASCALE\/pychop.\n        △ Less",
        "authors":"['Erin Carson', 'Xinye Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting",
        "abstract":"Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness.\n        △ Less",
        "authors":"['Wanna Cui', 'Peizheng Wang', 'Faliang Yin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Smoothed Distance Kernels for MMDs and Applications in Wasserstein Gradient Flows",
        "abstract":"Negative distance kernels $K(x,y) := - \\|x-y\\|$ were used in the definition of maximum mean discrepancies (MMDs) in statistics and lead to favorable numerical results in various applications. In particular, so-called slicing techniques for handling high-dimensional kernel summations profit from the simple parameter-free structure of the distance kernel. However, due to its non-smoothness in $x=y$, most of the classical theoretical results, e.g. on Wasserstein gradient flows of the corresponding MMD functional do not longer hold true. In this paper, we propose a new kernel which keeps the favorable properties of the negative distance kernel as being conditionally positive definite of order one with a nearly linear increase towards infinity and a simple slicing structure, but is Lipschitz differentiable now. Our construction is based on a simple 1D smoothing procedure of the absolute value function followed by a Riemann-Liouville fractional integral transform. Numerical results demonstrate that the new kernel performs similarly well as the negative distance kernel in gradient descent methods, but now with theoretical guarantees.\n        △ Less",
        "authors":"['Nicolaj Rux', 'Michael Quellmalz', 'Gabriele Steidl']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Performance of Rank-One Tensor Approximation on Incomplete Data",
        "abstract":"We are interested in the estimation of a rank-one tensor signal when only a portion $\\varepsilon$ of its noisy observation is available. We show that the study of this problem can be reduced to that of a random matrix model whose spectral analysis gives access to the reconstruction performance. These results shed light on and specify the loss of performance induced by an artificial reduction of the memory cost of a tensor via the deletion of a random part of its entries.\n        △ Less",
        "authors":"['Hugo Lebeau']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A System for Comprehensive Assessment of RAG Frameworks",
        "abstract":"Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for enhancing the factual accuracy and contextual relevance of Large Language Models (LLMs) by integrating retrieval mechanisms. However, existing evaluation frameworks fail to provide a holistic black-box approach to assessing RAG systems, especially in real-world deployment scenarios. To address this gap, we introduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a modular and flexible evaluation framework designed to benchmark deployed RAG applications systematically. SCARF provides an end-to-end, black-box evaluation methodology, enabling a limited-effort comparison across diverse RAG frameworks. Our framework supports multiple deployment configurations and facilitates automated testing across vector databases and LLM serving strategies, producing a detailed performance report. Moreover, SCARF integrates practical considerations such as response coherence, providing a scalable and adaptable solution for researchers and industry professionals evaluating RAG applications. Using the REST APIs interface, we demonstrate how SCARF can be applied to real-world scenarios, showcasing its flexibility in assessing different RAG frameworks and configurations. SCARF is available at GitHub repository.\n        △ Less",
        "authors":"['Mattia Rengo', 'Senad Beadini', 'Domenico Alfano', 'Roberto Abbruzzese']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations",
        "abstract":"Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{https:\/\/github.com\/limchaos\/Likelihood-OOD.git}{\\texttt{https:\/\/github.com\/limchaos\/Likelihood-OOD.git}}$.\n        △ Less",
        "authors":"['Yifan Ding', 'Arturas Aleksandrauskas', 'Amirhossein Ahmadian', 'Jonas Unger', 'Fredrik Lindsten', 'Gabriel Eilertsen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Detection of Fast Moving Celestial Objects Using a Mixture of Experts and Physical-Inspired Neural Network",
        "abstract":"Fast moving celestial objects are characterized by velocities across the celestial sphere that significantly differ from the motions of background stars. In observational images, these objects exhibit distinct shapes, contrasting with the typical appearances of stars. Depending on the observational method employed, these celestial entities may be designated as near-Earth objects or asteroids. Historically, fast moving celestial objects have been observed using ground-based telescopes, where the relative stability of stars and Earth facilitated effective image differencing techniques alongside traditional fast moving celestial object detection and classification algorithms. However, the growing prevalence of space-based telescopes, along with their diverse observational modes, produces images with different properties, rendering conventional methods less effective. This paper presents a novel algorithm for detecting fast moving celestial objects within star fields. Our approach enhances state-of-the-art fast moving celestial object detection neural networks by transforming them into physical-inspired neural networks. These neural networks leverage the point spread function of the telescope and the specific observational mode as prior information; they can directly identify moving fast moving celestial objects within star fields without requiring additional training, thereby addressing the limitations of traditional techniques. Additionally, all neural networks are integrated using the mixture of experts technique, forming a comprehensive fast moving celestial object detection algorithm. We have evaluated our algorithm using simulated observational data that mimics various observations carried out by space based telescope scenarios and real observation images. Results demonstrate that our method effectively detects fast moving celestial objects across different observational modes.\n        △ Less",
        "authors":"['Peng Jia', 'Ge Li', 'Bafeng Cheng', 'Yushan Li', 'Rongyu Sun']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data over dialogue: Why artificial intelligence is unlikely to humanise medicine",
        "abstract":"Recently, a growing number of experts in artificial intelligence (AI) and medicine have be-gun to suggest that the use of AI systems, particularly machine learning (ML) systems, is likely to humanise the practice of medicine by substantially improving the quality of clinician-patient relationships. In this thesis, however, I argue that medical ML systems are more likely to negatively impact these relationships than to improve them. In particular, I argue that the use of medical ML systems is likely to comprise the quality of trust, care, empathy, understanding, and communication between clinicians and patients.\n        △ Less",
        "authors":"['Joshua Hatherley']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Search-contempt: a hybrid MCTS algorithm for training AlphaZero-like engines with better computational efficiency",
        "abstract":"AlphaZero in 2017 was able to master chess and other games without human knowledge by playing millions of games against itself (self-play), with a computation budget running in the tens of millions of dollars. It used a variant of the Monte Carlo Tree Search (MCTS) algorithm, known as PUCT. This paper introduces search-contempt, a novel hybrid variant of the MCTS algorithm that fundamentally alters the distribution of positions generated in self-play, preferring more challenging positions. In addition, search-contempt has been shown to give a big boost in strength for engines in Odds Chess (where one side receives an unfavorable position from the start). More significantly, it opens up the possibility of training a self-play based engine, in a much more computationally efficient manner with the number of training games running into hundreds of thousands, costing tens of thousands of dollars (instead of tens of millions of training games costing millions of dollars required by AlphaZero). This means that it may finally be possible to train such a program from zero on a standard consumer GPU even with a very limited compute, cost, or time budget.\n        △ Less",
        "authors":"['Ameya Joshi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Gradient-based Sample Selection for Faster Bayesian Optimization",
        "abstract":"Bayesian optimization (BO) is an effective technique for black-box optimization. However, its applicability is typically limited to moderate-budget problems due to the cubic complexity in computing the Gaussian process (GP) surrogate model. In large-budget scenarios, directly employing the standard GP model faces significant challenges in computational time and resource requirements. In this paper, we propose a novel approach, gradient-based sample selection Bayesian Optimization (GSSBO), to enhance the computational efficiency of BO. The GP model is constructed on a selected set of samples instead of the whole dataset. These samples are selected by leveraging gradient information to maintain diversity and representation. We provide a theoretical analysis of the gradient-based sample selection strategy and obtain explicit sublinear regret bounds for our proposed framework. Extensive experiments on synthetic and real-world tasks demonstrate that our approach significantly reduces the computational cost of GP fitting in BO while maintaining optimization performance comparable to baseline methods.\n        △ Less",
        "authors":"['Qiyu Wei', 'Haowei Wang', 'Zirui Cao', 'Songhao Wang', 'Richard Allmendinger', 'Mauricio A Álvarez']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Harnessing Equivariance: Modeling Turbulence with Graph Neural Networks",
        "abstract":"This work proposes a novel methodology for turbulence modeling in Large Eddy Simulation (LES) based on Graph Neural Networks (GNNs), which embeds the discrete rotational, reflectional and translational symmetries of the Navier-Stokes equations into the model architecture. In addition, suitable invariant input and output spaces are derived that allow the GNN models to be embedded seamlessly into the LES framework to obtain a symmetry-preserving simulation setup. The suitability of the proposed approach is investigated for two canonical test cases: Homogeneous Isotropic Turbulence (HIT) and turbulent channel flow. For both cases, GNN models are trained successfully in actual simulations using Reinforcement Learning (RL) to ensure that the models are consistent with the underlying LES formulation and discretization. It is demonstrated for the HIT case that the resulting GNN-based LES scheme recovers rotational and reflectional equivariance up to machine precision in actual simulations. At the same time, the stability and accuracy remain on par with non-symmetry-preserving machine learning models that fail to obey these properties. The same modeling strategy translates well to turbulent channel flow, where the GNN model successfully learns the more complex flow physics and is able to recover the turbulent statistics and Reynolds stresses. It is shown that the GNN model learns a zonal modeling strategy with distinct behaviors in the near-wall and outer regions. The proposed approach thus demonstrates the potential of GNNs for turbulence modeling, especially in the context of LES and RL.\n        △ Less",
        "authors":"['Marius Kurz', 'Andrea Beck', 'Benjamin Sanderse']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Novel Deep Learning Approach for Emulating Computationally Expensive Postfire Debris Flows",
        "abstract":"Traditional physics-based models of geophysical flows, such as debris flows and landslides that pose significant risks to human lives and infrastructure are computationally expensive, limiting their utility for large-scale parameter sweeps, uncertainty quantification, inversions or real-time applications. This study presents an efficient alternative, a deep learning-based surrogate model built using a modified U-Net architecture to predict the dynamics of runoff-generated debris flows across diverse terrain based on data from physics based simulations. The study area is divided into smaller patches for localized predictions using a patch-predict-stitch methodology (complemented by limited global data to accelerate training). The patches are then combined to reconstruct spatially continuous flow maps, ensuring scalability for large domains. To enable fast training using limited expensive simulations, the deep learning model was trained on data from an ensemble of physics based simulations using parameters generated via Latin Hypercube Sampling and validated on unseen parameter sets and terrain, achieving maximum pointwise errors below 10% and robust generalization. Uncertainty quantification using Monte Carlo methods are enabled using the validated surrogate, which can facilitate probabilistic hazard assessments. This study highlights the potential of deep learning surrogates as powerful tools for geophysical flow analysis, enabling computationally efficient and reliable probabilistic hazard map predictions.\n        △ Less",
        "authors":"['Palak Patel', 'Luke McGuire', 'Abani Patra']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Quantum Machine Learning: Unveiling Trends, Impacts through Bibliometric Analysis",
        "abstract":"Quantum Machine Learning (QML) is the intersection of two revolutionary fields: quantum computing and machine learning. It promises to unlock unparalleled capabilities in data analysis, model building, and problem-solving by harnessing the unique properties of quantum mechanics. This research endeavors to conduct a comprehensive bibliometric analysis of scientific information pertaining to QML covering the period from 2000 to 2023. An extensive dataset comprising 9493 scholarly works is meticulously examined to unveil notable trends, impact factors, and funding patterns within the domain. Additionally, the study employs bibliometric mapping techniques to visually illustrate the network relationships among key countries, institutions, authors, patent citations and significant keywords in QML research. The analysis reveals a consistent growth in publications over the examined period. The findings highlight the United States and China as prominent contributors, exhibiting substantial publication and citation metrics. Notably, the study concludes that QML, as a research subject, is currently in a formative stage, characterized by robust scholarly activity and ongoing development.\n        △ Less",
        "authors":"['Riya Bansal', 'Nikhil Kumar Rajput']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Relaxing the Markov Requirements on Reinforcement Learning Under Weak Partial Ignorability",
        "abstract":"Incomplete data, confounding effects, and violations of the Markov property are interrelated problems which are ubiquitous in Reinforcement Learning applications. We introduce the concept of ``partial ignorabilty\" and leverage it to establish a novel convergence theorem for adaptive Reinforcement Learning. This theoretical result relaxes the Markov assumption on the stochastic process underlying conventional $Q$-learning, deploying a generalized form of the Robbins-Monro stochastic approximation theorem to establish optimality. This result has clear downstream implications for most active subfields of Reinforcement Learning, with clear paths for extension to the field of Causal Inference.\n        △ Less",
        "authors":"['MaryLena Bleile']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security",
        "abstract":"Financial instability has become a significant issue in today's society. While research typically focuses on financial aspects, there is a tendency to overlook time-related aspects of unstable work schedules. The inability to rely on consistent work schedules leads to burnout, work-family conflicts, and financial shocks that directly impact workers' income and assets. Unforeseen fluctuations in earnings pose challenges in financial planning, affecting decisions on savings and spending and ultimately undermining individuals' long-term financial stability and well-being.\n  This issue is particularly evident in sectors where workers experience frequently changing schedules without sufficient notice, including those in the food service and retail sectors, part-time and hourly workers, and individuals with lower incomes. These groups are already more financially vulnerable, and the unpredictable nature of their schedules exacerbates their financial fragility.\n  Our objective is to understand how unforeseen fluctuations in earnings exacerbate financial fragility by investigating the extent to which individuals' financial management depends on their ability to anticipate and plan for the future. To address this question, we develop a simulation framework that models how individuals optimize utility amidst financial uncertainty and the imperative to avoid financial ruin. We employ online learning techniques, specifically adapting workers' consumption policies based on evolving information about their work schedules.\n  With this framework, we show both theoretically and empirically how a worker's capacity to anticipate schedule changes enhances their long-term utility. Conversely, the inability to predict future events can worsen workers' instability. Moreover, our framework enables us to explore interventions to mitigate the problem of schedule uncertainty and evaluate their effectiveness.\n        △ Less",
        "authors":"['Pegah Nokhiz', 'Aravinda Kanchana Ruwanpathirana', 'Aditya Bhaskara', 'Suresh Venkatasubramanian']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Merging Embedded Topics with Optimal Transport for Online Topic Modeling on Data Streams",
        "abstract":"Topic modeling is a key component in unsupervised learning, employed to identify topics within a corpus of textual data. The rapid growth of social media generates an ever-growing volume of textual data daily, making online topic modeling methods essential for managing these data streams that continuously arrive over time. This paper introduces a novel approach to online topic modeling named StreamETM. This approach builds on the Embedded Topic Model (ETM) to handle data streams by merging models learned on consecutive partial document batches using unbalanced optimal transport. Additionally, an online change point detection algorithm is employed to identify shifts in topics over time, enabling the identification of significant changes in the dynamics of text streams. Numerical experiments on simulated and real-world data show StreamETM outperforming competitors.\n        △ Less",
        "authors":"['Federica Granese', 'Benjamin Navet', 'Serena Villata', 'Charles Bouveyron']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TOCALib: Optimal control library with interpolation for bimanual manipulation and obstacles avoidance",
        "abstract":"The paper presents a new approach for constructing a library of optimal trajectories for two robotic manipulators, Two-Arm Optimal Control and Avoidance Library (TOCALib). The optimisation takes into account kinodynamic and other constraints within the FROST framework. The novelty of the method lies in the consideration of collisions using the DCOL method, which allows obtaining symbolic expressions for assessing the presence of collisions and using them in gradient-based optimization control methods. The proposed approach allowed the implementation of complex bimanual manipulations. In this paper we used Mobile Aloha as an example of TOCALib application. The approach can be extended to other bimanual robots, as well as to gait control of bipedal robots. It can also be used to construct training data for machine learning tasks for manipulation.\n        △ Less",
        "authors":"['Yulia Danik', 'Dmitry Makarov', 'Aleksandra Arkhipova', 'Sergei Davidenko', 'Aleksandr Panov']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conformalized Generative Bayesian Imaging: An Uncertainty Quantification Framework for Computational Imaging",
        "abstract":"Uncertainty quantification plays an important role in achieving trustworthy and reliable learning-based computational imaging. Recent advances in generative modeling and Bayesian neural networks have enabled the development of uncertainty-aware image reconstruction methods. Current generative model-based methods seek to quantify the inherent (aleatoric) uncertainty on the underlying image for given measurements by learning to sample from the posterior distribution of the underlying image. On the other hand, Bayesian neural network-based approaches aim to quantify the model (epistemic) uncertainty on the parameters of a deep neural network-based reconstruction method by approximating the posterior distribution of those parameters. Unfortunately, an ongoing need for an inversion method that can jointly quantify complex aleatoric uncertainty and epistemic uncertainty patterns still persists. In this paper, we present a scalable framework that can quantify both aleatoric and epistemic uncertainties. The proposed framework accepts an existing generative model-based posterior sampling method as an input and introduces an epistemic uncertainty quantification capability through Bayesian neural networks with latent variables and deep ensembling. Furthermore, by leveraging the conformal prediction methodology, the proposed framework can be easily calibrated to ensure rigorous uncertainty quantification. We evaluated the proposed framework on magnetic resonance imaging, computed tomography, and image inpainting problems and showed that the epistemic and aleatoric uncertainty estimates produced by the proposed framework display the characteristic features of true epistemic and aleatoric uncertainties. Furthermore, our results demonstrated that the use of conformal prediction on top of the proposed framework enables marginal coverage guarantees consistent with frequentist principles.\n        △ Less",
        "authors":"['Canberk Ekmekci', 'Mujdat Cetin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Distilling Knowledge from Heterogeneous Architectures for Semantic Segmentation",
        "abstract":"Current knowledge distillation (KD) methods for semantic segmentation focus on guiding the student to imitate the teacher's knowledge within homogeneous architectures. However, these methods overlook the diverse knowledge contained in architectures with different inductive biases, which is crucial for enabling the student to acquire a more precise and comprehensive understanding of the data during distillation. To this end, we propose for the first time a generic knowledge distillation method for semantic segmentation from a heterogeneous perspective, named HeteroAKD. Due to the substantial disparities between heterogeneous architectures, such as CNN and Transformer, directly transferring cross-architecture knowledge presents significant challenges. To eliminate the influence of architecture-specific information, the intermediate features of both the teacher and student are skillfully projected into an aligned logits space. Furthermore, to utilize diverse knowledge from heterogeneous architectures and deliver customized knowledge required by the student, a teacher-student knowledge mixing mechanism (KMM) and a teacher-student knowledge evaluation mechanism (KEM) are introduced. These mechanisms are performed by assessing the reliability and its discrepancy between heterogeneous teacher-student knowledge. Extensive experiments conducted on three main-stream benchmarks using various teacher-student pairs demonstrate that our HeteroAKD outperforms state-of-the-art KD methods in facilitating distillation between heterogeneous architectures.\n        △ Less",
        "authors":"['Yanglin Huang', 'Kai Hu', 'Yuan Zhang', 'Zhineng Chen', 'Xieping Gao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Clicks, comments, consequences: Are content creators' socio-structural and platform characteristics shaping the exposure to negative sentiment, offensive language, and hate speech on YouTube?",
        "abstract":"Receiving negative sentiment, offensive comments, or even hate speech is a constant part of the working experience of content creators (CCs) on YouTube - a growing occupational group in the platform economy. This study investigates how socio-structural characteristics such as the age, gender, and race of CCs but also platform features including the number of subscribers, community strength, and the channel topic shape differences in the occurrence of these phenomena on that platform. Drawing on a random sample of n=3,695 YouTube channels from German-speaking countries, we conduct a comprehensive analysis combining digital trace data, enhanced with hand-coded variables to include socio-structural characteristics in social media data. Publicly visible negative sentiment, offensive language, and hate speech are detected with machine- and deep-learning methods using N=40,000,000 comments. Contrary to existing studies our findings indicate that female content creators are confronted with less negative communication. Notably, our analysis reveals that while BIPoC, who work as CCs, receive significantly more negative sentiment, they aren't exposed to more offensive comments or hate speech. Additionally, platform characteristics also play a crucial role, as channels publishing content on conspiracy theories or politics are more frequently subject to negative communication.\n        △ Less",
        "authors":"['Sarah Weißmann', 'Aaron Philipp', 'Roland Verwiebe', 'Chiara Osorio Krauter', 'Nina-Sophie Fritsch', 'Claudia Buder']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data Requirement Goal Modeling for Machine Learning Systems",
        "abstract":"Machine Learning (ML) has been integrated into various software and systems. Two main components are essential for training an ML model: the training data and the ML algorithm. Given the critical role of data in ML system development, it has become increasingly important to assess the quality of data attributes and ensure that the data meets specific requirements before its utilization. This work proposes an approach to guide non-experts in identifying data requirements for ML systems using goal modeling. In this approach, we first develop the Data Requirement Goal Model (DRGM) by surveying the white literature to identify and categorize the issues and challenges faced by data scientists and requirement engineers working on ML-related projects. An initial DRGM was built to accommodate common tasks that would generalize across projects. Then, based on insights from both white and gray literature, a customization mechanism is built to help adjust the tasks, KPIs, and goals' importance of different elements within the DRGM. The generated model can aid its users in evaluating different datasets using GRL evaluation strategies. We then validate the approach through two illustrative examples based on real-world projects. The results from the illustrative examples demonstrate that the data requirements identified by the proposed approach align with the requirements of real-world projects, demonstrating the practicality and effectiveness of the proposed framework. The proposed dataset selection customization mechanism and the proposed DRGM are helpful in guiding non-experts in identifying the data requirements for machine learning systems tailored to a specific ML problem. This approach also aids in evaluating different dataset alternatives to choose the optimum dataset for the problem. For future work, we recommend implementing tool support to generate the DRGM based on a chatbot interface.\n        △ Less",
        "authors":"['Asma Yamani', 'Nadeen AlAmoudi', 'Salma Albilali', 'Malak Baslyman', 'Jameleddine Hassine']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ms-Mamba: Multi-scale Mamba for Time-Series Forecasting",
        "abstract":"The problem of Time-series Forecasting is generally addressed by recurrent, Transformer-based and the recently proposed Mamba-based architectures. However, existing architectures generally process their input at a single temporal scale, which may be sub-optimal for many tasks where information changes over multiple time scales. In this paper, we introduce a novel architecture called Multi-scale Mamba (ms-Mamba) to address this gap. ms-Mamba incorporates multiple temporal scales by using multiple Mamba blocks with different sampling rates ($Δ$s). Our experiments on many benchmarks demonstrate that ms-Mamba outperforms state-of-the-art approaches, including the recently proposed Transformer-based and Mamba-based models.\n        △ Less",
        "authors":"['Yusuf Meric Karadag', 'Sinan Kalkan', 'Ipek Gursel Dino']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Prediction of Usage Probabilities of Shopping-Mall Corridors Using Heterogeneous Graph Neural Networks",
        "abstract":"We present a method based on graph neural network (GNN) for prediction of probabilities of usage of shopping-mall corridors. The heterogeneous graph network of shops and corridor paths are obtained from floorplans of the malls by creating vector layers for corridors, shops and entrances. These are subsequently assimilated into nodes and edges of graphs. The prediction of the usage probability is based on the shop features, namely, the area and usage categories they fall into, and on the graph connecting these shops, corridor junctions and entrances by corridor paths. Though the presented method is applicable for training on datasets obtained from a field survey or from pedestrian-detecting sensors, the target data of the supervised deep-learning work flow in this work are obtained from a probability method. We also include a context-specific representation learning of latent features. The usage-probability prediction is made on each edge, which is a connection by a section of corridor path between the adjacent nodes representing the shops or corridor points. To create a feature for each edge, the hidden-layer feature vectors acquired in the message-passing GNN layers at the nodes of each edge are averaged and concatenated with the vector obtained by their multiplication. These edge-features are then passed to multilayer perceptrons (MLP) to make the final prediction of usage probability on each edge. The samples of synthetic learning dataset for each shopping mall are obtained by changing the shops' usage and area categories, and by subsequently feeding the graph into the probability model.\n  When including different shopping malls in a single dataset, we also propose to consider graph-level features to inform the model with specific identifying features of each mall.\n        △ Less",
        "authors":"['Malik M Barakathullah', 'Immanuel Koh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Large Language Models through Neuro-Symbolic Integration and Ontological Reasoning",
        "abstract":"Large Language Models (LLMs) demonstrate impressive capabilities in natural language processing but suffer from inaccuracies and logical inconsistencies known as hallucinations. This compromises their reliability, especially in domains requiring factual accuracy. We propose a neuro-symbolic approach integrating symbolic ontological reasoning and machine learning methods to enhance the consistency and reliability of LLM outputs. Our workflow utilizes OWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking, and a lightweight machine learning model (logistic regression) for mapping natural language statements into logical forms compatible with the ontology. When inconsistencies between LLM outputs and the ontology are detected, the system generates explanatory feedback to guide the LLM towards a corrected, logically coherent response in an iterative refinement loop. We present a working Python prototype demonstrating this pipeline. Experimental results in a defined domain suggest significant improvements in semantic coherence and factual accuracy of LLM outputs, showcasing the potential of combining LLM fluency with the rigor of formal semantics.\n        △ Less",
        "authors":"['Ruslan Idelfonso Magana Vsevolodovna', 'Marco Monti']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Predicting the Lifespan of Industrial Printheads with Survival Analysis",
        "abstract":"Accurately predicting the lifespan of critical device components is essential for maintenance planning and production optimization, making it a topic of significant interest in both academia and industry. In this work, we investigate the use of survival analysis for predicting the lifespan of production printheads developed by Canon Production Printing. Specifically, we focus on the application of five techniques to estimate survival probabilities and failure rates: the Kaplan-Meier estimator, Cox proportional hazard model, Weibull accelerated failure time model, random survival forest, and gradient boosting. The resulting estimates are further refined using isotonic regression and subsequently aggregated to determine the expected number of failures. The predictions are then validated against real-world ground truth data across multiple time windows to assess model reliability. Our quantitative evaluation using three performance metrics demonstrates that survival analysis outperforms industry-standard baseline methods for printhead lifespan prediction.\n        △ Less",
        "authors":"['Dan Parii', 'Evelyne Janssen', 'Guangzhi Tang', 'Charalampos Kouzinopoulos', 'Marcin Pietrasik']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Kernel Logistic Regression Learning for High-Capacity Hopfield Networks",
        "abstract":"Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron ratio around 0.14). We propose Kernel Logistic Regression (KLR) learning. Unlike linear methods, KLR uses kernels to implicitly map patterns to high-dimensional feature space, enhancing separability. By learning dual variables, KLR dramatically improves storage capacity, achieving perfect recall even when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and enhances noise robustness. KLR demonstrably outperforms Hebbian and linear logistic regression approaches.\n        △ Less",
        "authors":"['Akira Tamamori']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Learning Meets Teleconnections: Improving S2S Predictions for European Winter Weather",
        "abstract":"Predictions on subseasonal-to-seasonal (S2S) timescales--ranging from two weeks to two month--are crucial for early warning systems but remain challenging owing to chaos in the climate system. Teleconnections, such as the stratospheric polar vortex (SPV) and Madden-Julian Oscillation (MJO), offer windows of enhanced predictability, however, their complex interactions remain underutilized in operational forecasting. Here, we developed and evaluated deep learning architectures to predict North Atlantic-European (NAE) weather regimes, systematically assessing the role of remote drivers in improving S2S forecast skill of deep learning models. We implemented (1) a Long Short-term Memory (LSTM) network predicting the NAE regimes of the next six weeks based on previous regimes, (2) an Index-LSTM incorporating SPV and MJO indices, and (3) a ViT-LSTM using a Vision Transformer to directly encode stratospheric wind and tropical outgoing longwave radiation fields. These models are compared with operational hindcasts as well as other AI models. Our results show that leveraging teleconnection information enhances skill at longer lead times. Notably, the ViT-LSTM outperforms ECMWF's subseasonal hindcasts beyond week 4 by improving Scandinavian Blocking (SB) and Atlantic Ridge (AR) predictions. Analysis of high-confidence predictions reveals that NAO-, SB, and AR opportunity forecasts can be associated with SPV variability and MJO phase patterns aligning with established pathways, also indicating new patterns. Overall, our work demonstrates that encoding physically meaningful climate fields can enhance S2S prediction skill, advancing AI-driven subseasonal forecast. Moreover, the experiments highlight the potential of deep learning methods as investigative tools, providing new insights into atmospheric dynamics and predictability.\n        △ Less",
        "authors":"['Philine L. Bommer', 'Marlene Kretschmer', 'Fiona R. Spuler', 'Kirill Bykov', 'Marina M. -C. Höhne']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beating Transformers using Synthetic Cognition",
        "abstract":"The road to Artificial General Intelligence goes through the generation of episodic reactive behaviors, where the Transformer architecture has been proven to be the state-of-the-art. However, they still fail to develop reasoning. Recently, a novel approach for developing cognitive architectures, called Synthetic Cognition, has been proposed and implemented to develop instantaneous reactive behavior. In this study, we aim to explore the use of Synthetic Cognition to develop episodic reactive behaviors. We propose a mechanism to deal with sequences for the recent implementation of Synthetic Cognition, and test it against DNA foundation models in DNA sequence classification tasks. In our experiments, our proposal clearly outperforms the DNA foundation models, obtaining the best score on more benchmark tasks than the alternatives. Thus, we achieve two goals: expanding Synthetic Cognition to deal with sequences, and beating the Transformer architecture for sequence classification.\n        △ Less",
        "authors":"['Alfredo Ibias', 'Miguel Rodriguez-Galindo', 'Hector Antona', 'Guillem Ramirez-Miranda', 'Enric Guinovart']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CTSR: Cartesian tensor-based sparse regression for data-driven discovery of high-dimensional invariant governing equations",
        "abstract":"Accurate and concise governing equations are crucial for understanding system dynamics. Recently, data-driven methods such as sparse regression have been employed to automatically uncover governing equations from data, representing a significant shift from traditional first-principles modeling. However, most existing methods focus on scalar equations, limiting their applicability to simple, low-dimensional scenarios, and failing to ensure rotation and reflection invariance without incurring significant computational cost or requiring additional prior knowledge. This paper proposes a Cartesian tensor-based sparse regression (CTSR) technique to accurately and efficiently uncover complex, high-dimensional governing equations while ensuring invariance. Evaluations on two two-dimensional (2D) and two three-dimensional (3D) test cases demonstrate that the proposed method achieves superior accuracy and efficiency compared to the conventional technique.\n        △ Less",
        "authors":"['Boqian Zhang', 'Juanmian Lei', 'Guoyou Sun', 'Shuaibing Ding', 'Jian Guo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conditional Conformal Risk Adaptation",
        "abstract":"Uncertainty quantification is becoming increasingly important in image segmentation, especially for high-stakes applications like medical imaging. While conformal risk control generalizes conformal prediction beyond standard miscoverage to handle various loss functions such as false negative rate, its application to segmentation often yields inadequate conditional risk control: some images experience very high false negative rates while others have negligibly small ones. We develop Conformal Risk Adaptation (CRA), which introduces a new score function for creating adaptive prediction sets that significantly improve conditional risk control for segmentation tasks. We establish a novel theoretical framework that demonstrates a fundamental connection between conformal risk control and conformal prediction through a weighted quantile approach, applicable to any score function. To address the challenge of poorly calibrated probabilities in segmentation models, we introduce a specialized probability calibration framework that enhances the reliability of pixel-wise inclusion estimates. Using these calibrated probabilities, we propose Calibrated Conformal Risk Adaptation (CCRA) and a stratified variant (CCRA-S) that partitions images based on their characteristics and applies group-specific thresholds to further enhance conditional risk control. Our experiments on polyp segmentation demonstrate that all three methods (CRA, CCRA, and CCRA-S) provide valid marginal risk control and deliver more consistent conditional risk control across diverse images compared to standard approaches, offering a principled approach to uncertainty quantification that is particularly valuable for high-stakes and personalized segmentation applications.\n        △ Less",
        "authors":"['Rui Luo', 'Zhixin Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Stochastic Smoothed Primal-Dual Algorithms for Nonconvex Optimization with Linear Inequality Constraints",
        "abstract":"We propose smoothed primal-dual algorithms for solving stochastic and smooth nonconvex optimization problems with linear inequality constraints. Our algorithms are single-loop and only require a single stochastic gradient based on one sample at each iteration. A distinguishing feature of our algorithm is that it is based on an inexact gradient descent framework for the Moreau envelope, where the gradient of the Moreau envelope is estimated using one step of a stochastic primal-dual augmented Lagrangian method. To handle inequality constraints and stochasticity, we combine the recently established global error bounds in constrained optimization with a Moreau envelope-based analysis of stochastic proximal algorithms. For obtaining $\\varepsilon$-stationary points, we establish the optimal $O(\\varepsilon^{-4})$ sample complexity guarantee for our algorithms and provide extensions to stochastic linear constraints. We also show how to improve this complexity to $O(\\varepsilon^{-3})$ by using variance reduction and the expected smoothness assumption. Unlike existing methods, the iterations of our algorithms are free of subproblems, large batch sizes or increasing penalty parameters and use dual variable updates to ensure feasibility.\n        △ Less",
        "authors":"['Ruichuan Huang', 'Jiawei Zhang', 'Ahmet Alacaoglu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases",
        "abstract":"Heart diseases constitute the main cause of international human defunction. According to the World Health Organization (WHO), approximately 18 million deaths happen each year due to precisely heart diseases. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid and effective prediction. In this work, an automatic system which analyses in real-time echocardiography video sequences is proposed for the challenging and more specific task of prediction of heart failure times. This system is based on a novel deep learning framework, and works in two stages. The first one transforms the data included in a database of echocardiography video sequences into a machine learning-compatible collection of annotated images which can be used in the training phase of any kind of machine learning-based framework, including a deep learning one. This initial stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage is focused on building and training a Vision Transformer (ViT). Self-supervised learning (SSL) methods, which have been so far barely explored in the literature about heart failure prediction, are applied to effectively train the ViT from scratch, even with scarce databases of echocardiograms. The designed neural network analyses images from echocardiography sequences to estimate the time in which a heart failure will happen. The results obtained show the efficacy of the HODMD algorithm and the superiority of the proposed system with respect to several established ViT and Convolutional Neural Network (CNN) architectures.\n        △ Less",
        "authors":"['Andrés Bell-Navas', 'María Villalba-Orero', 'Enrique Lara-Pezzi', 'Jesús Garicano-Mena', 'Soledad Le Clainche']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering",
        "abstract":"Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating translations of long, complex passages, and a more ``pragmatic'' approach that assesses how accurately key information is conveyed by a translation in context is needed. We introduce TREQA (Translation Evaluation via Question-Answering), a framework that extrinsically evaluates translation quality by assessing how accurately candidate translations answer reading comprehension questions that target key information in the original source or reference texts. In challenging domains that require long-range understanding, such as literary texts, we show that TREQA is competitive with and, in some cases, outperforms state-of-the-art neural and LLM-based metrics in ranking alternative paragraph-level translations, despite never being explicitly optimized to correlate with human judgments. Furthermore, the generated questions and answers offer interpretability: empirical analysis shows that they effectively target translation errors identified by experts in evaluated datasets. Our code is available at https:\/\/github.com\/deep-spin\/treqa\n        △ Less",
        "authors":"['Patrick Fernandes', 'Sweta Agrawal', 'Emmanouil Zaranis', 'André F. T. Martins', 'Graham Neubig']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Privacy-Preserving Vertical K-Means Clustering",
        "abstract":"Clustering is a fundamental data processing task used for grouping records based on one or more features. In the vertically partitioned setting, data is distributed among entities, with each holding only a subset of those features. A key challenge in this scenario is that computing distances between records requires access to all distributed features, which may be privacy-sensitive and cannot be directly shared with other parties. The goal is to compute the joint clusters while preserving the privacy of each entity's dataset. Existing solutions using secret sharing or garbled circuits implement privacy-preserving variants of Lloyd's algorithm but incur high communication costs, scaling as O(nkt), where n is the number of data points, k the number of clusters, and t the number of rounds. These methods become impractical for large datasets or several parties, limiting their use to LAN settings only. On the other hand, a different line of solutions rely on differential privacy (DP) to outsource the local features of the parties to a central server. However, they often significantly degrade the utility of the clustering outcome due to excessive noise. In this work, we propose a novel solution based on homomorphic encryption and DP, reducing communication complexity to O(n+kt). In our method, parties securely outsource their features once, allowing a computing party to perform clustering operations under encryption. DP is applied only to the clusters' centroids, ensuring privacy with minimal impact on utility. Our solution clusters 100,000 two-dimensional points into five clusters using only 73MB of communication, compared to 101GB for existing works, and completes in just under 3 minutes on a 100Mbps network, whereas existing works take over 1 day. This makes our solution practical even for WAN deployments, all while maintaining accuracy comparable to plaintext k-means algorithms.\n        △ Less",
        "authors":"['Federico Mazzone', 'Trevor Brown', 'Florian Kerschbaum', 'Kevin H. Wilson', 'Maarten Everts', 'Florian Hahn', 'Andreas Peter']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs",
        "abstract":"We benchmark foundation models image embeddings for classification and retrieval in e-Commerce, evaluating their suitability for real-world applications. Our study spans embeddings from pre-trained convolutional and transformer models trained via supervised, self-supervised, and text-image contrastive learning. We assess full fine-tuning and transfer learning (top-tuning) on six diverse e-Commerce datasets: fashion, consumer goods, cars, food, and retail. Results show full fine-tuning consistently performs well, while text-image and self-supervised embeddings can match its performance with less training. While supervised embeddings remain stable across architectures, SSL and contrastive embeddings vary significantly, often benefiting from top-tuning. Top-tuning emerges as an efficient alternative to full fine-tuning, reducing computational costs. We also explore cross-tuning, noting its impact depends on dataset characteristics. Our findings offer practical guidelines for embedding selection and fine-tuning strategies, balancing efficiency and performance.\n        △ Less",
        "authors":"['Urszula Czerwinska', 'Cenk Bircanoglu', 'Jeremy Chamoux']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diffusion Transformers for Tabular Data Time Series Generation",
        "abstract":"Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin.\n        △ Less",
        "authors":"['Fabrizio Garuti', 'Enver Sangineto', 'Simone Luetto', 'Lorenzo Forni', 'Rita Cucchiara']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PhaseGen: A Diffusion-Based Approach for Complex-Valued MRI Data Generation",
        "abstract":"Magnetic resonance imaging (MRI) raw data, or k-Space data, is complex-valued, containing both magnitude and phase information. However, clinical and existing Artificial Intelligence (AI)-based methods focus only on magnitude images, discarding the phase data despite its potential for downstream tasks, such as tumor segmentation and classification. In this work, we introduce $\\textit{PhaseGen}$, a novel complex-valued diffusion model for generating synthetic MRI raw data conditioned on magnitude images, commonly used in clinical practice. This enables the creation of artificial complex-valued raw data, allowing pretraining for models that require k-Space information. We evaluate PhaseGen on two tasks: skull-stripping directly in k-Space and MRI reconstruction using the publicly available FastMRI dataset. Our results show that training with synthetic phase data significantly improves generalization for skull-stripping on real-world data, with an increased segmentation accuracy from $41.1\\%$ to $80.1\\%$, and enhances MRI reconstruction when combined with limited real-world data. This work presents a step forward in utilizing generative AI to bridge the gap between magnitude-based datasets and the complex-valued nature of MRI raw data. This approach allows researchers to leverage the vast amount of avaliable image domain data in combination with the information-rich k-Space data for more accurate and efficient diagnostic tasks. We make our code publicly $\\href{https:\/\/github.com\/TIO-IKIM\/PhaseGen}{\\text{available here}}$.\n        △ Less",
        "authors":"['Moritz Rempe', 'Fabian Hörst', 'Helmut Becker', 'Marco Schlimbach', 'Lukas Rotkopf', 'Kevin Kröninger', 'Jens Kleesiek']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using LLMs for Analyzing AIS Data",
        "abstract":"Recent research in Large Language Models (LLMs), has had a profound impact across various fields, including mobility data science. This paper explores the and experiment with different approaches to using LLMs for analyzing AIS data. We propose a set of carefully designed queries to assess the reasoning capabilities of LLMs in this kind of tasks. Further, we experiment with four different methods: (1) using LLMs as a natural language interface to a spatial database, (2) reasoning on raw data, (3) reasoning on compressed trajectories, and (4) reasoning on semantic trajectories. We investigate the strengths and weaknesses for the four methods, and discuss the findings. The goal is to provide valuable insights for both researchers and practitioners on selecting the most appropriate LLM-based method depending on their specific data analysis objectives.\n        △ Less",
        "authors":"['Gaspard Merten', 'Gilles Dejaegere', 'Mahmoud Sakr']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SydneyScapes: Image Segmentation for Australian Environments",
        "abstract":"Autonomous Vehicles (AVs) are being partially deployed and tested across various global locations, including China, the USA, Germany, France, Japan, Korea, and the UK, but with limited demonstrations in Australia. The integration of machine learning (ML) into AV perception systems highlights the need for locally labelled datasets to develop and test algorithms in specific environments. To address this, we introduce SydneyScapes - a dataset tailored for computer vision tasks of image semantic, instance, and panoptic segmentation. This dataset, collected from Sydney and surrounding cities in New South Wales (NSW), Australia, consists of 756 images with high-quality pixel-level annotations. It is designed to assist AV industry and researchers by providing annotated data and tools for algorithm development, testing, and deployment in the Australian context. Additionally, we offer benchmarking results using state-of-the-art algorithms to establish reference points for future research and development. The dataset is publicly available at https:\/\/hdl.handle.net\/2123\/33051.\n        △ Less",
        "authors":"['Hongyu Lyu', 'Julie Stephany Berrio', 'Mao Shan', 'Stewart Worrall']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs",
        "abstract":"We present a design called \\emph{Proof of Gradient Optimization} (PoGO) for blockchain consensus, where miners produce verifiable evidence of training large-scale machine-learning models. Building on previous work, we incorporate \\emph{quantized gradients} (4-bit precision) to reduce storage and computation requirements, while still preserving the ability of verifiers to check that real progress has been made on lowering the model's loss. Additionally, we employ Merkle proofs over the full 32-bit model to handle large parameter sets and to enable random leaf checks with minimal on-chain data. We illustrate these ideas using GPT-3 (175B parameters) as a reference example and also refer to smaller but high-performance models (e.g., \\emph{Gemma~3} with 27B parameters). We provide an empirical cost analysis showing that verification is significantly cheaper than training, thanks in part to quantization and sampling. We also discuss the necessity of longer block times (potentially hours) when incorporating meaningful training steps, the trade-offs when using specialized GPU hardware, and how binary diffs may incrementally optimize updates. Finally, we note that fine-tuning can be handled in a similar manner, merely changing the dataset and the manner of sampling but preserving the overall verification flow. Our protocol allows verifiers to issue either \\emph{positive} or \\emph{negative} attestations; these are aggregated at finalization to either confirm the update or slash the miner.\n        △ Less",
        "authors":"['José I. Orlicki']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation",
        "abstract":"AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft. Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences.\n        △ Less",
        "authors":"['Tuhin Chakrabarty', 'Philippe Laban', 'Chien-Sheng Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A taxonomy of epistemic injustice in the context of AI and the case for generative hermeneutical erasure",
        "abstract":"Whether related to machine learning models' epistemic opacity, algorithmic classification systems' discriminatory automation of testimonial prejudice, the distortion of human beliefs via the 'hallucinations' of generative AI, the inclusion of the global South in global AI governance, the execution of bureaucratic violence via algorithmic systems, or located in the interaction with conversational artificial agents epistemic injustice related to AI is a growing concern. Based on a proposed general taxonomy of epistemic injustice, this paper first sketches a taxonomy of the types of epistemic injustice in the context of AI, relying on the work of scholars from the fields of philosophy of technology, political philosophy and social epistemology. Secondly, an additional perspective on epistemic injustice in the context of AI: generative hermeneutical erasure. I argue that this injustice that can come about through the application of Large Language Models (LLMs) and contend that generative AI, when being deployed outside of its Western space of conception, can have effects of conceptual erasure, particularly in the epistemic domain, followed by forms of conceptual disruption caused by a mismatch between AI system and the interlocutor in terms of conceptual frameworks. AI systems' 'view from nowhere' epistemically inferiorizes non-Western epistemologies and thereby contributes to the erosion of their epistemic particulars, gradually contributing to hermeneutical erasure. This work's relevance lies in proposal of a taxonomy that allows epistemic injustices to be mapped in the AI domain and the proposal of a novel form of AI-related epistemic injustice.\n        △ Less",
        "authors":"['Warmhold Jan Thomas Mollema']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data",
        "abstract":"Outlier detection in high-dimensional tabular data is challenging since data is often distributed across multiple lower-dimensional subspaces -- a phenomenon known as the Multiple Views effect (MV). This effect led to a large body of research focused on mining such subspaces, known as subspace selection. However, as the precise nature of the MV effect was not well understood, traditional methods had to rely on heuristic-driven search schemes that struggle to accurately capture the true structure of the data. Properly identifying these subspaces is critical for unsupervised tasks such as outlier detection or clustering, where misrepresenting the underlying data structure can hinder the performance. We introduce Myopic Subspace Theory (MST), a new theoretical framework that mathematically formulates the Multiple Views effect and writes subspace selection as a stochastic optimization problem. Based on MST, we introduce V-GAN, a generative method trained to solve such an optimization problem. This approach avoids any exhaustive search over the feature space while ensuring that the intrinsic data structure is preserved. Experiments on 42 real-world datasets show that using V-GAN subspaces to build ensemble methods leads to a significant increase in one-class classification performance -- compared to existing subspace selection, feature selection, and embedding methods. Further experiments on synthetic data show that V-GAN identifies subspaces more accurately while scaling better than other relevant subspace selection methods. These results confirm the theoretical guarantees of our approach and also highlight its practical viability in high-dimensional settings.\n        △ Less",
        "authors":"['Jose Cribeiro-Ramallo', 'Federico Matteucci', 'Paul Enciu', 'Alexander Jenke', 'Vadim Arzamasov', 'Thorsten Strufe', 'Klemens Böhm']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable",
        "abstract":"Modern large language foundation models (LLM) have now entered the daily lives of millions of users. We ask a natural question whether it is possible to customize LLM for every user or every task. From system and industrial economy consideration, general continue-training or fine-tuning still require substantial computation and memory of training GPU nodes, whereas most inference nodes under deployment, possibly with lower-end GPUs, are configured to make forward pass fastest possible. We propose a framework to take full advantages of existing LLMs and systems of online service. We train an additional branch of transformer blocks on the final-layer embedding of pretrained LLMs, which is the base, then a carry-on module merge the base models to compose a customized LLM. We can mix multiple layers, or multiple LLMs specialized in different domains such as chat, coding, math, to form a new mixture of LLM that best fit a new task. As the base model don't need to update parameters, we are able to outsource most computation of the training job on inference nodes, and only train a lightweight carry-on on training nodes, where we consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM. We tested Qwen and DeepSeek opensourced models for continue-pretraining and got faster loss convergence. We use it to improve solving math questions with extremely small computation and model size, with 1000 data samples of chain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on, and the results are promising.\n        △ Less",
        "authors":"['Jianqiao Wangni']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving",
        "abstract":"Large language model (LLM) inference serving systems are essential to various LLM-based applications. As demand for LLM services continues to grow, scaling these systems to handle high request rates while meeting latency Service-Level Objectives (SLOs), referred to as effective throughput, becomes critical. However, existing systems often struggle to improve effective throughput, primarily due to a significant decline in Time To First Token (TTFT) SLO attainment. We identify two major causes of this bottleneck: (1) memory-intensive KV cache that limits batch size expansion under GPU memory constraints, and (2) rigid batch composition enforced by the default First-Come-First-Serve scheduling policy. In this paper, we introduce Apt-Serve, a scalable framework designed to enhance effective throughput in LLM inference serving. Apt-Serve features a new hybrid cache scheme that combines KV cache with a memory-efficient hidden cache for reusable input hidden state vectors, allowing large batch sizes and improving request concurrency. Based on the hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism that dynamically optimizes batch composition. We formally define the adaptive scheduling optimization problem and propose an efficient algorithm with theoretical guarantees. Extensive evaluations on three real-world datasets and LLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up to 8.8x improvement in effective throughput compared to the state-of-the-art inference serving systems.\n        △ Less",
        "authors":"['Shihong Gao', 'Xin Zhang', 'Yanyan Shen', 'Lei Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Mechanism-Learning Deeply Coupled Model for Remote Sensing Retrieval of Global Land Surface Temperature",
        "abstract":"Land surface temperature (LST) retrieval from remote sensing data is pivotal for analyzing climate processes and surface energy budgets. However, LST retrieval is an ill-posed inverse problem, which becomes particularly severe when only a single band is available. In this paper, we propose a deeply coupled framework integrating mechanistic modeling and machine learning to enhance the accuracy and generalizability of single-channel LST retrieval. Training samples are generated using a physically-based radiative transfer model and a global collection of 5810 atmospheric profiles. A physics-informed machine learning framework is proposed to systematically incorporate the first principles from classical physical inversion models into the learning workflow, with optimization constrained by radiative transfer equations. Global validation demonstrated a 30% reduction in root-mean-square error versus standalone methods. Under extreme humidity, the mean absolute error decreased from 4.87 K to 2.29 K (53% improvement). Continental-scale tests across five continents confirmed the superior generalizability of this model.\n        △ Less",
        "authors":"['Tian Xie', 'Menghui Jiang', 'Huanfeng Shen', 'Huifang Li', 'Chao Zeng', 'Xiaobin Guan', 'Jun Ma', 'Guanhao Zhang', 'Liangpei Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security",
        "abstract":"Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks remains a critical challenge in cybersecurity. This research introduces a hybrid deep learning model combining Gated Recurrent Units (GRUs) and a Neural Turing Machine (NTM) for enhanced intrusion detection. Trained on the UNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential data processing and an NTM for long-term pattern recognition. The proposed approach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS traffic. These findings offer promising advancements in real-time threat detection and contribute to improved network security across various domains.\n        △ Less",
        "authors":"['Caroline Panggabean', 'Chandrasekar Venkatachalam', 'Priyanka Shah', 'Sincy John', 'Renuka Devi P', 'Shanmugavalli Venkatachalam']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Traversal Learning Coordination For Lossless And Efficient Distributed Learning",
        "abstract":"In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an accuracy drop during aggregation due to its averaging function, while SL and SFL face increased loss due to the independent gradient updates on each split network. TL adopts a unique strategy where the model traverses the nodes during forward propagation (FP) and performs backward propagation (BP) on the orchestrator, effectively implementing centralized learning (CL) principles within a distributed environment. The orchestrator is tasked with generating virtual batches and planning the sequential node visits of the model during FP, aligning them with the ordered index of the data within these batches. We conducted experiments on six datasets representing diverse characteristics across various domains. Our evaluation demonstrates that TL is on par with classic CL approaches in terms of accurate inference, thereby offering a viable and robust solution for DL tasks. TL outperformed other DL methods and improved accuracy by 7.85% for independent and identically distributed (IID) datasets, macro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text classification, and AUC by 3.88% and 4.54% for medical and financial datasets, respectively. By effectively preserving data privacy while maintaining performance, TL represents a significant advancement in DL methodologies.\n        △ Less",
        "authors":"['Erdenebileg Batbaatar', 'Jeonggeol Kim', 'Yongcheol Kim', 'Young Yoon']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying",
        "abstract":"Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies.\n        △ Less",
        "authors":"['Shichen Li', 'Chenhui Shao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
        "abstract":"Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https:\/\/github.com\/juzhengz\/LoRI\n        △ Less",
        "authors":"['Juzheng Zhang', 'Jiacheng You', 'Ashwinee Panda', 'Tom Goldstein']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Unifying and extending Diffusion Models through PDEs for solving Inverse Problems",
        "abstract":"Diffusion models have emerged as powerful generative tools with applications in computer vision and scientific machine learning (SciML), where they have been used to solve large-scale probabilistic inverse problems. Traditionally, these models have been derived using principles of variational inference, denoising, statistical signal processing, and stochastic differential equations. In contrast to the conventional presentation, in this study we derive diffusion models using ideas from linear partial differential equations and demonstrate that this approach has several benefits that include a constructive derivation of the forward and reverse processes, a unified derivation of multiple formulations and sampling strategies, and the discovery of a new class of models. We also apply the conditional version of these models to solving canonical conditional density estimation problems and challenging inverse problems. These problems help establish benchmarks for systematically quantifying the performance of different formulations and sampling strategies in this study, and for future studies. Finally, we identify and implement a mechanism through which a single diffusion model can be applied to measurements obtained from multiple measurement operators. Taken together, the contents of this manuscript provide a new understanding and several new directions in the application of diffusion models to solving physics-based inverse problems.\n        △ Less",
        "authors":"['Agnimitra Dasgupta', 'Alexsander Marciano da Cunha', 'Ali Fardisi', 'Mehrnegar Aminy', 'Brianna Binder', 'Bryan Shaddy', 'Assad A Oberai']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conditional Data Synthesis Augmentation",
        "abstract":"Reliable machine learning and statistical analysis rely on diverse, well-distributed training data. However, real-world datasets are often limited in size and exhibit underrepresentation across key subpopulations, leading to biased predictions and reduced performance, particularly in supervised tasks such as classification. To address these challenges, we propose Conditional Data Synthesis Augmentation (CoDSA), a novel framework that leverages generative models, such as diffusion models, to synthesize high-fidelity data for improving model performance across multimodal domains including tabular, textual, and image data. CoDSA generates synthetic samples that faithfully capture the conditional distributions of the original data, with a focus on under-sampled or high-interest regions. Through transfer learning, CoDSA fine-tunes pre-trained generative models to enhance the realism of synthetic data and increase sample density in sparse areas. This process preserves inter-modal relationships, mitigates data imbalance, improves domain adaptation, and boosts generalization. We also introduce a theoretical framework that quantifies the statistical accuracy improvements enabled by CoDSA as a function of synthetic sample volume and targeted region allocation, providing formal guarantees of its effectiveness. Extensive experiments demonstrate that CoDSA consistently outperforms non-adaptive augmentation strategies and state-of-the-art baselines in both supervised and unsupervised settings.\n        △ Less",
        "authors":"['Xinyu Tian', 'Xiaotong Shen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for Fighting Games",
        "abstract":"Deep reinforcement learning (DRL) has effectively enhanced gameplay experiences and game design across various game genres. However, few studies on fighting game agents have focused explicitly on enhancing player enjoyment, a critical factor for both developers and players. To address this gap and establish a practical baseline for designing enjoyability-focused agents, we propose a two-tier agent (TTA) system and conducted experiments in the classic fighting game Street Fighter II. The first tier of TTA employs a task-oriented network architecture, modularized reward functions, and hybrid training to produce diverse and skilled DRL agents. In the second tier of TTA, a Large Language Model Hyper-Agent, leveraging players' playing data and feedback, dynamically selects suitable DRL opponents. In addition, we investigate and model several key factors that affect the enjoyability of the opponent. The experiments demonstrate improvements from 64. 36% to 156. 36% in the execution of advanced skills over baseline methods. The trained agents also exhibit distinct game-playing styles. Additionally, we conducted a small-scale user study, and the overall enjoyment in the player's feedback validates the effectiveness of our TTA system.\n        △ Less",
        "authors":"['Shouren Wang', 'Zehua Jiang', 'Fernando Sliva', 'Sam Earle', 'Julian Togelius']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Role of Machine Learning in Reducing Healthcare Costs: The Impact of Medication Adherence and Preventive Care on Hospitalization Expenses",
        "abstract":"This study reveals the important role of prevention care and medication adherence in reducing hospitalizations. By using a structured dataset of 1,171 patients, four machine learning models Logistic Regression, Gradient Boosting, Random Forest, and Artificial Neural Networks are applied to predict five-year hospitalization risk, with the Gradient Boosting model achieving the highest accuracy of 81.2%. The result demonstrated that patients with high medication adherence and consistent preventive care can reduce 38.3% and 37.7% in hospitalization risk. The finding also suggests that targeted preventive care can have positive Return on Investment (ROI), and therefore ML models can effectively direct personalized interventions and contribute to long-term medical savings.\n        △ Less",
        "authors":"['Yixin Zhang', 'Yisong Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability",
        "abstract":"Recent advancements in multi-modal models have significantly improved vision-language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution images, and offer limited interpretability in attention mechanisms. To address these challenges, we introduce RadZero, a novel similarity-based cross-attention framework for vision-language alignment in radiology with zero-shot multi-task capability. RadZero leverages large language models to extract minimal semantic sentences from radiology reports and employs a multi-positive contrastive learning strategy to effectively capture relationships between images and multiple relevant textual descriptions. It also utilizes a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, RadZero enables zero-shot inference with similarity probability for classification and pixel-level cross-modal similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, cross-modal similarity map analysis highlights its potential for improving explainability in vision-language alignment. Additionally, qualitative evaluation demonstrates RadZero's capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging.\n        △ Less",
        "authors":"['Jonggwon Park', 'Soobum Kim', 'Byungmu Yoon', 'Kyoyun Choi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction",
        "abstract":"Automated radiology report generation (RRG) holds potential to reduce radiologists' workload, especially as recent advancements in large language models (LLMs) enable the development of multimodal models for chest X-ray (CXR) report generation. However, multimodal LLMs (MLLMs) are resource-intensive, requiring vast datasets and substantial computational cost for training. To address these challenges, we propose a retrieval-augmented generation approach that leverages multimodal retrieval and LLMs to generate radiology reports while mitigating hallucinations and reducing computational demands. Our method uses LLMs to extract key phrases from radiology reports, effectively focusing on essential diagnostic information. Through exploring effective training strategies, including image encoder structure search, adding noise to text embeddings, and additional training objectives, we combine complementary pre-trained image encoders and adopt contrastive learning between text and semantic image embeddings. We evaluate our approach on MIMIC-CXR dataset, achieving state-of-the-art results on CheXbert metrics and competitive RadGraph F1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method demonstrates robust generalization for multi-view RRG, making it suitable for comprehensive clinical applications.\n        △ Less",
        "authors":"['Kyoyun Choi', 'Byungmu Yoon', 'Soobum Kim', 'Jonggwon Park']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Selection for Recommendation Systems",
        "abstract":"We present the construction of a multi-selection model to answer differentially private queries in the context of recommendation systems. The server sends back multiple recommendations and a ``local model'' to the user, which the user can run locally on its device to select the item that best fits its private features. We study a setup where the server uses a deep neural network (trained on the Movielens 25M dataset as the ground truth for movie recommendation. In the multi-selection paradigm, the average recommendation utility is approximately 97\\% of the optimal utility (as determined by the ground truth neural network) while maintaining a local differential privacy guarantee with $ε$ ranging around 1 with respect to feature vectors of neighboring users. This is in comparison to an average recommendation utility of 91\\% in the non-multi-selection regime under the same constraints.\n        △ Less",
        "authors":"['Sahasrajit Sarmasarkar', 'Zhihao Jiang', 'Ashish Goel', 'Aleksandra Korolova', 'Kamesh Munagala']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models",
        "abstract":"We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for Target Speaker Extraction (TSE) based on the LauraGPT backbone. It employs a small-scale auto-regressive decoder-only language model which takes the continuous representations for both the mixture and the reference speeches and produces the first few layers of the target speech's discrete codec representations. In addition, a one-step encoder-only language model reconstructs the sum of the predicted codec embeddings using both the mixture and the reference information. Our approach achieves superior or comparable performance to existing generative and discriminative TSE models. To the best of our knowledge, LauraTSE is the first single-task TSE model to leverage an auto-regressive decoder-only language model as the backbone.\n        △ Less",
        "authors":"['Beilong Tang', 'Bang Zeng', 'Ming Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MicroNAS: An Automated Framework for Developing a Fall Detection System",
        "abstract":"This work presents MicroNAS, an automated neural architecture search tool specifically designed to create models optimized for microcontrollers with small memory resources. The ESP32 microcontroller, with 320 KB of memory, is used as the target platform. The artificial intelligence contribution lies in a novel method for optimizing convolutional neural network and gated recurrent unit architectures by considering the memory size of the target microcontroller as a guide. A comparison is made between memory-driven model optimization and traditional two-stage methods, which use pruning, to show the effectiveness of the proposed framework. To demonstrate the engineering application of MicroNAS, a fall detection system (FDS) for lower-limb amputees is developed as a pilot study. A critical challenge in fall detection studies, class imbalance in the dataset, is addressed. The results show that MicroNAS models achieved higher F1-scores than alternative approaches, such as ensemble methods and H2O Automated Machine Learning, presenting a significant step forward in real-time FDS development. Biomechanists using body-worn sensors for activity detection can adopt the open-source code to design machine learning models tailored for microcontroller platforms with limited memory.\n        △ Less",
        "authors":"['Seyed Mojtaba Mohasel', 'John Sheppard', 'Lindsey K. Molina', 'Richard R. Neptune', 'Shane R. Wurdeman', 'Corey A. Pew']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Automating quantum feature map design via large language models",
        "abstract":"Quantum feature maps are a key component of quantum machine learning, encoding classical data into quantum states to exploit the expressive power of high-dimensional Hilbert spaces. Despite their theoretical promise, designing quantum feature maps that offer practical advantages over classical methods remains an open challenge. In this work, we propose an agentic system that autonomously generates, evaluates, and refines quantum feature maps using large language models. The system consists of five component: Generation, Storage, Validation, Evaluation, and Review. Using these components, it iteratively improves quantum feature maps. Experiments on the MNIST dataset show that it can successfully discover and refine feature maps without human intervention. The best feature map generated outperforms existing quantum baselines and achieves competitive accuracy compared to classical kernels across MNIST, Fashion-MNIST, and CIFAR-10. Our approach provides a framework for exploring dataset-adaptive quantum features and highlights the potential of LLM-driven automation in quantum algorithm design.\n        △ Less",
        "authors":"['Kenya Sakka', 'Kosuke Mitarai', 'Keisuke Fujii']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple Generative Method",
        "abstract":"Climate science studies the structure and dynamics of Earth's climate system and seeks to understand how climate changes over time, where the data is usually stored in the format of time series, recording the climate features, geolocation, time attributes, etc. Recently, much research attention has been paid to the climate benchmarks. In addition to the most common task of weather forecasting, several pioneering benchmark works are proposed for extending the modality, such as domain-specific applications like tropical cyclone intensity prediction and flash flood damage estimation, or climate statement and confidence level in the format of natural language. To further motivate the artificial general intelligence development for climate science, in this paper, we first contribute a multi-modal climate benchmark, i.e., ClimateBench-M, which aligns (1) the time series climate data from ERA5, (2) extreme weather events data from NOAA, and (3) satellite image data from NASA HLS based on a unified spatial-temporal granularity. Second, under each data modality, we also propose a simple but strong generative method that could produce competitive performance in weather forecasting, thunderstorm alerts, and crop segmentation tasks in the proposed ClimateBench-M. The data and code of ClimateBench-M are publicly available at https:\/\/github.com\/iDEA-iSAIL-Lab-UIUC\/ClimateBench-M.\n        △ Less",
        "authors":"['Dongqi Fu', 'Yada Zhu', 'Zhining Liu', 'Lecheng Zheng', 'Xiao Lin', 'Zihao Li', 'Liri Fang', 'Katherine Tieu', 'Onkar Bhardwaj', 'Kommy Weldemariam', 'Hanghang Tong', 'Hendrik Hamann', 'Jingrui He']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"State Estimation Using Particle Filtering in Adaptive Machine Learning Methods: Integrating Q-Learning and NEAT Algorithms with Noisy Radar Measurements",
        "abstract":"Reliable state estimation is essential for autonomous systems operating in complex, noisy environments. Classical filtering approaches, such as the Kalman filter, can struggle when facing nonlinear dynamics or non-Gaussian noise, and even more flexible particle filters often encounter sample degeneracy or high computational costs in large-scale domains. Meanwhile, adaptive machine learning techniques, including Q-learning and neuroevolutionary algorithms such as NEAT, rely heavily on accurate state feedback to guide learning; when sensor data are imperfect, these methods suffer from degraded convergence and suboptimal performance. In this paper, we propose an integrated framework that unifies particle filtering with Q-learning and NEAT to explicitly address the challenge of noisy measurements. By refining radar-based observations into reliable state estimates, our particle filter drives more stable policy updates (in Q-learning) or controller evolution (in NEAT), allowing both reinforcement learning and neuroevolution to converge faster, achieve higher returns or fitness, and exhibit greater resilience to sensor uncertainty. Experiments on grid-based navigation and a simulated car environment highlight consistent gains in training stability, final performance, and success rates over baselines lacking advanced filtering. Altogether, these findings underscore that accurate state estimation is not merely a preprocessing step, but a vital component capable of substantially enhancing adaptive machine learning in real-world applications plagued by sensor noise.\n        △ Less",
        "authors":"['Wonjin Song', 'Feng Bao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Task-Circuit Quantization: Leveraging Knowledge Localization and Interpretability for Compression",
        "abstract":"Post-training quantization (PTQ) reduces a model's memory footprint by mapping full precision weights into low bit weights without costly retraining, but can degrade its downstream performance especially in low 2- to 3-bit settings. We develop a new mixed-precision PTQ approach, Task-Circuit Quantization (TaCQ), that draws parallels to automated circuit discovery, directly conditioning the quantization process on specific weight circuits -- which we define as sets of weights associated with downstream task performance. These weights are kept as 16-bit weights, while others are quantized, maintaining performance while only adding a marginal memory cost. Specifically, TaCQ contrasts unquantized model weights with a uniformly-quantized model to estimate the expected change in weights due to quantization and uses gradient information to predict the resulting impact on task performance, allowing us to preserve task-specific weights. We compare TaCQ-based quantization to existing mixed-precision quantization methods when conditioning both on general-purpose and task-specific data. Across QA, math reasoning, and text-to-SQL tasks for both Llama-3 and Qwen2.5, we find that TaCQ outperforms baselines using the same calibration data and a lower weight budget, achieving major improvements in the 2 and 3-bit regime. With only 3.1 bits we are able to recover 96% of Llama-3-8B-Instruct's unquantized 16-bit MMLU performance, obtaining a 5.25% absolute improvement over SPQR. We also observe consistently large gains over existing methods in the 2-bit regime, with an average gain of 14.74% over the strongest baseline, SliM-LLM. Moreover, we observe a 7.20% gain without conditioning on specific tasks, showing TaCQ's ability to identify important weights is not limited to task-conditioned settings.\n        △ Less",
        "authors":"['Hanqi Xiao', 'Yi-Lin Sung', 'Elias Stengel-Eskin', 'Mohit Bansal']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Min-Max Optimisation for Nonconvex-Nonconcave Functions Using a Random Zeroth-Order Extragradient Algorithm",
        "abstract":"This study explores the performance of the random Gaussian smoothing Zeroth-Order ExtraGradient (ZO-EG) scheme considering min-max optimisation problems with possibly NonConvex-NonConcave (NC-NC) objective functions. We consider both unconstrained and constrained, differentiable and non-differentiable settings. We discuss the min-max problem from the point of view of variational inequalities. For the unconstrained problem, we establish the convergence of the ZO-EG algorithm to the neighbourhood of an $ε$-stationary point of the NC-NC objective function, whose radius can be controlled under a variance reduction scheme, along with its complexity. For the constrained problem, we introduce the new notion of proximal variational inequalities and give examples of functions satisfying this property. Moreover, we prove analogous results to the unconstrained case for the constrained problem. For the non-differentiable case, we prove the convergence of the ZO-EG algorithm to a neighbourhood of an $ε$-stationary point of the smoothed version of the objective function, where the radius of the neighbourhood can be controlled, which can be related to the ($δ,ε$)-Goldstein stationary point of the original objective function.\n        △ Less",
        "authors":"['Amir Ali Farzin', 'Yuen Man Pun', 'Philipp Braun', 'Antoine Lesage-landry', 'Youssef Diouane', 'Iman Shames']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PROPEL: Supervised and Reinforcement Learning for Large-Scale Supply Chain Planning",
        "abstract":"This paper considers how to fuse Machine Learning (ML) and optimization to solve large-scale Supply Chain Planning (SCP) optimization problems. These problems can be formulated as MIP models which feature both integer (non-binary) and continuous variables, as well as flow balance and capacity constraints. This raises fundamental challenges for existing integrations of ML and optimization that have focused on binary MIPs and graph problems. To address these, the paper proposes PROPEL, a new framework that combines optimization with both supervised and Deep Reinforcement Learning (DRL) to reduce the size of search space significantly. PROPEL uses supervised learning, not to predict the values of all integer variables, but to identify the variables that are fixed to zero in the optimal solution, leveraging the structure of SCP applications. PROPEL includes a DRL component that selects which fixed-at-zero variables must be relaxed to improve solution quality when the supervised learning step does not produce a solution with the desired optimality tolerance. PROPEL has been applied to industrial supply chain planning optimizations with millions of variables. The computational results show dramatic improvements in solution times and quality, including a 60% reduction in primal integral and an 88% primal gap reduction, and improvement factors of up to 13.57 and 15.92, respectively.\n        △ Less",
        "authors":"['Vahid Eghbal Akhlaghi', 'Reza Zandehshahvar', 'Pascal Van Hentenryck']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ChronoFormer: Time-Aware Transformer Architectures for Structured Clinical Event Modeling",
        "abstract":"The temporal complexity of electronic health record (EHR) data presents significant challenges for predicting clinical outcomes using machine learning. This paper proposes ChronoFormer, an innovative transformer based architecture specifically designed to encode and leverage temporal dependencies in longitudinal patient data. ChronoFormer integrates temporal embeddings, hierarchical attention mechanisms, and domain specific masking techniques. Extensive experiments conducted on three benchmark tasks mortality prediction, readmission prediction, and long term comorbidity onset demonstrate substantial improvements over current state of the art methods. Furthermore, detailed analyses of attention patterns underscore ChronoFormer's capability to capture clinically meaningful long range temporal relationships.\n        △ Less",
        "authors":"['Yuanyun Zhang', 'Shi Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Minimum width for universal approximation using squashable activation functions",
        "abstract":"The exact minimum width that allows for universal approximation of unbounded-depth networks is known only for ReLU and its variants. In this work, we study the minimum width of networks using general activation functions. Specifically, we focus on squashable functions that can approximate the identity function and binary step function by alternatively composing with affine transformations. We show that for networks using a squashable activation function to universally approximate $L^p$ functions from $[0,1]^{d_x}$ to $\\mathbb R^{d_y}$, the minimum width is $\\max\\{d_x,d_y,2\\}$ unless $d_x=d_y=1$; the same bound holds for $d_x=d_y=1$ if the activation function is monotone. We then provide sufficient conditions for squashability and show that all non-affine analytic functions and a class of piecewise functions are squashable, i.e., our minimum width result holds for those general classes of activation functions.\n        △ Less",
        "authors":"['Jonghyun Shin', 'Namjun Kim', 'Geonho Hwang', 'Sejun Park']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs",
        "abstract":"The adaptation of large language models (LLMs) to time series forecasting poses unique challenges, as time series data is continuous in nature, while LLMs operate on discrete tokens. Despite the success of LLMs in natural language processing (NLP) and other structured domains, aligning time series data with language-based representations while maintaining both predictive accuracy and interpretability remains a significant hurdle. Existing methods have attempted to reprogram time series data into text-based forms, but these often fall short in delivering meaningful, interpretable results. In this paper, we propose a multi-level text alignment framework for time series forecasting using LLMs that not only improves prediction accuracy but also enhances the interpretability of time series representations. Our method decomposes time series into trend, seasonal, and residual components, which are then reprogrammed into component-specific text representations. We introduce a multi-level alignment mechanism, where component-specific embeddings are aligned with pre-trained word tokens, enabling more interpretable forecasts. Experiments on multiple datasets demonstrate that our method outperforms state-of-the-art models in accuracy while providing good interpretability.\n        △ Less",
        "authors":"['Taibiao Zhao', 'Xiaobing Chen', 'Mingxuan Sun']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Throughput-Optimal Scheduling Algorithms for LLM Inference and AI Agents",
        "abstract":"As demand for Large Language Models (LLMs) and AI agents rapidly grows, optimizing systems for efficient LLM inference becomes critical. While significant efforts have targeted system-level engineering, little is explored through a mathematical modeling and queuing perspective.\n  In this paper, we aim to develop the queuing fundamentals for LLM inference, bridging the gap between queuing and LLM system communities. In particular, we study the throughput aspect in LLM inference systems. We prove that a large class of 'work-conserving' scheduling algorithms can achieve maximum throughput for both individual requests and AI agent workloads, highlighting 'work-conserving' as a key design principle in practice. Evaluations of real-world systems show that Orca and Sarathi-serve are throughput-optimal, reassuring practitioners, while FastTransformer and vanilla vLLM are not maximally stable and should be used with caution.\n  Our results highlight the substantial benefits queuing community can offer in improving LLM inference systems and call for more interdisciplinary developments.\n        △ Less",
        "authors":"['Yueying Li', 'Jim Dai', 'Tianyi Peng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging deep learning for plant disease identification: a bibliometric analysis in SCOPUS from 2018 to 2024",
        "abstract":"This work aimed to present a bibliometric analysis of deep learning research for plant disease identification, with a special focus on generative modeling. A thorough analysis of SCOPUS-sourced bibliometric data from 253 documents was performed. Key performance metrics such as accuracy, precision, recall, and F1-score were analyzed for generative modeling. The findings highlighted significant contributions from some authors Too and Arnal Barbedo, whose works had notable citation counts, suggesting their influence on the academic community. Co-authorship networks revealed strong collaborative clusters, while keyword analysis identified emerging research gaps. This study highlights the role of collaboration and citation metrics in shaping research directions and enhancing the impact of scholarly work in applications of deep learning to plant disease identification. Future research should explore the methodologies of highly cited studies to inform best practices and policy-making.\n        △ Less",
        "authors":"['Enow Takang Achuo Albert', 'Ngalle Hermine Bille', 'Ngonkeu Mangaptche Eddy Leonard']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning to erase quantum states: thermodynamic implications of quantum learning theory",
        "abstract":"The energy cost of erasing quantum states depends on our knowledge of the states. We show that learning algorithms can acquire such knowledge to erase many copies of an unknown state at the optimal energy cost. This is proved by showing that learning can be made fully reversible and has no fundamental energy cost itself. With simple counting arguments, we relate the energy cost of erasing quantum states to their complexity, entanglement, and magic. We further show that the constructed erasure protocol is computationally efficient when learning is efficient. Conversely, under standard cryptographic assumptions, we prove that the optimal energy cost cannot be achieved efficiently in general. These results also enable efficient work extraction based on learning. Together, our results establish a concrete connection between quantum learning theory and thermodynamics, highlighting the physical significance of learning processes and enabling efficient learning-based protocols for thermodynamic tasks.\n        △ Less",
        "authors":"['Haimeng Zhao', 'Yuzhen Zhang', 'John Preskill']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FLASH: Flexible Learning of Adaptive Sampling from History in Temporal Graph Neural Networks",
        "abstract":"Aggregating temporal signals from historic interactions is a key step in future link prediction on dynamic graphs. However, incorporating long histories is resource-intensive. Hence, temporal graph neural networks (TGNNs) often rely on historical neighbors sampling heuristics such as uniform sampling or recent neighbors selection. These heuristics are static and fail to adapt to the underlying graph structure. We introduce FLASH, a learnable and graph-adaptive neighborhood selection mechanism that generalizes existing heuristics. FLASH integrates seamlessly into TGNNs and is trained end-to-end using a self-supervised ranking loss. We provide theoretical evidence that commonly used heuristics hinders TGNNs performance, motivating our design. Extensive experiments across multiple benchmarks demonstrate consistent and significant performance improvements for TGNNs equipped with FLASH.\n        △ Less",
        "authors":"['Or Feldman', 'Krishna Sri Ipsit Mantri', 'Carola-Bibiane Schönlieb', 'Chaim Baskin', 'Moshe Eliasof']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Objaverse++: Curated 3D Object Dataset with Quality Annotations",
        "abstract":"This paper presents Objaverse++, a curated subset of Objaverse enhanced with detailed attribute annotations by human experts. Recent advances in 3D content generation have been driven by large-scale datasets such as Objaverse, which contains over 800,000 3D objects collected from the Internet. Although Objaverse represents the largest available 3D asset collection, its utility is limited by the predominance of low-quality models. To address this limitation, we manually annotate 10,000 3D objects with detailed attributes, including aesthetic quality scores, texture color classifications, multi-object composition flags, transparency characteristics, etc. Then, we trained a neural network capable of annotating the tags for the rest of the Objaverse dataset. Through experiments and a user study on generation results, we demonstrate that models pre-trained on our quality-focused subset achieve better performance than those trained on the larger dataset of Objaverse in image-to-3D generation tasks. In addition, by comparing multiple subsets of training data filtered by our tags, our results show that the higher the data quality, the faster the training loss converges. These findings suggest that careful curation and rich annotation can compensate for the raw dataset size, potentially offering a more efficient path to develop 3D generative models. We release our enhanced dataset of approximately 500,000 curated 3D models to facilitate further research on various downstream tasks in 3D computer vision. In the near future, we aim to extend our annotations to cover the entire Objaverse dataset.\n        △ Less",
        "authors":"['Chendi Lin', 'Heshan Liu', 'Qunshu Lin', 'Zachary Bright', 'Shitao Tang', 'Yihui He', 'Minghao Liu', 'Ling Zhu', 'Cindy Le']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bregman-Hausdorff divergence: strengthening the connections between computational geometry and machine learning",
        "abstract":"The purpose of this paper is twofold. On a technical side, we propose an extension of the Hausdorff distance from metric spaces to spaces equipped with asymmetric distance measures. Specifically, we focus on the family of Bregman divergences, which includes the popular Kullback--Leibler divergence (also known as relative entropy).\n  As a proof of concept, we use the resulting Bregman--Hausdorff divergence to compare two collections of probabilistic predictions produced by different machine learning models trained using the relative entropy loss. The algorithms we propose are surprisingly efficient even for large inputs with hundreds of dimensions.\n  In addition to the introduction of this technical concept, we provide a survey. It outlines the basics of Bregman geometry, as well as computational geometry algorithms. We focus on algorithms that are compatible with this geometry and are relevant for machine learning.\n        △ Less",
        "authors":"['Tuyen Pham', 'Hana Dal Poz Kouřimská', 'Hubert Wagner']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Gendered Algorithm: Navigating Financial Inclusion & Equity in AI-facilitated Access to Credit",
        "abstract":"A growing trend in financial technology (fintech) is the use of mobile phone data and machine learning (ML) to provide credit scores- and subsequently, opportunities to access loans- to groups left out of traditional banking. This paper draws on interview data with leaders, investors, and data scientists at fintech companies developing ML-based alternative lending apps in low- and middle-income countries to explore financial inclusion and gender implications. More specifically, it examines how the underlying logics, design choices, and management decisions of ML-based alternative lending tools by fintechs embed or challenge gender biases, and consequently influence gender equity in access to finance. Findings reveal developers follow 'gender blind' approaches, grounded in beliefs that ML is objective and data reflects the truth. This leads to a lack of grappling with the ways data, features for creditworthiness, and access to apps are gendered. Overall, tools increase access to finance, but not gender equitably: Interviewees report less women access loans and receive lower amounts than men, despite being better repayers. Fintechs identify demand- and supply-side reasons for gender differences, but frame them as outside their responsibility. However, that women are observed as better repayers reveals a market inefficiency and potential discriminatory effect, further linked to profit optimization objectives. This research introduces the concept of encoded gender norms, whereby without explicit attention to the gendered nature of data and algorithmic design, AI tools reproduce existing inequalities. In doing so, they reinforce gender norms as self-fulfilling prophecies. The idea that AI is inherently objective and, when left alone, 'fair', is seductive and misleading. In reality, algorithms reflect the perspectives, priorities, and values of the people and institutions that design them.\n        △ Less",
        "authors":"['Genevieve Smith']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Follow-the-Perturbed-Leader Achieves Best-of-Both-Worlds for the m-Set Semi-Bandit Problems",
        "abstract":"We consider a common case of the combinatorial semi-bandit problem, the $m$-set semi-bandit, where the learner exactly selects $m$ arms from the total $d$ arms. In the adversarial setting, the best regret bound, known to be $\\mathcal{O}(\\sqrt{nmd})$ for time horizon $n$, is achieved by the well-known Follow-the-Regularized-Leader (FTRL) policy, which, however, requires to explicitly compute the arm-selection probabilities by solving optimizing problems at each time step and sample according to it. This problem can be avoided by the Follow-the-Perturbed-Leader (FTPL) policy, which simply pulls the $m$ arms that rank among the $m$ smallest (estimated) loss with random perturbation. In this paper, we show that FTPL with a Fréchet perturbation also enjoys the optimal regret bound $\\mathcal{O}(\\sqrt{nmd})$ in the adversarial setting and achieves best-of-both-world regret bounds, i.e., achieves a logarithmic regret for the stochastic setting.\n        △ Less",
        "authors":"['Jingxin Zhan', 'Zhihua Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data Fusion of Deep Learned Molecular Embeddings for Property Prediction",
        "abstract":"Data-driven approaches such as deep learning can result in predictive models for material properties with exceptional accuracy and efficiency. However, in many problems data is sparse, severely limiting their accuracy and applicability. To improve predictions, techniques such as transfer learning and multi-task learning have been used. The performance of multi-task learning models depends on the strength of the underlying correlations between tasks and the completeness of the dataset. We find that standard multi-task models tend to underperform when trained on sparse datasets with weakly correlated properties. To address this gap, we use data fusion techniques to combine the learned molecular embeddings of various single-task models and trained a multi-task model on this combined embedding. We apply this technique to a widely used benchmark dataset of quantum chemistry data for small molecules as well as a newly compiled sparse dataset of experimental data collected from literature and our own quantum chemistry and thermochemical calculations. The results show that the fused, multi-task models outperform standard multi-task models for sparse datasets and can provide enhanced prediction on data-limited properties compared to single-task models.\n        △ Less",
        "authors":"['Robert J Appleton', 'Brian C Barnes', 'Alejandro Strachan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Scalable Approach to Clustering Embedding Projections",
        "abstract":"Interactive visualization of embedding projections is a useful technique for understanding data and evaluating machine learning models. Labeling data within these visualizations is critical for interpretation, as labels provide an overview of the projection and guide user navigation. However, most methods for producing labels require clustering the points, which can be computationally expensive as the number of points grows. In this paper, we describe an efficient clustering approach using kernel density estimation in the projected 2D space instead of points. This algorithm can produce high-quality cluster regions from a 2D density map in a few hundred milliseconds, orders of magnitude faster than current approaches. We contribute the design of the algorithm, benchmarks, and applications that demonstrate the utility of the algorithm, including labeling and summarization.\n        △ Less",
        "authors":"['Donghao Ren', 'Fred Hohman', 'Dominik Moritz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Multi-Phase Analysis of Blood Culture Stewardship: Machine Learning Prediction, Expert Recommendation Assessment, and LLM Automation",
        "abstract":"Blood cultures are often over ordered without clear justification, straining healthcare resources and contributing to inappropriate antibiotic use pressures worsened by the global shortage. In study of 135483 emergency department (ED) blood culture orders, we developed machine learning (ML) models to predict the risk of bacteremia using structured electronic health record (EHR) data and provider notes via a large language model (LLM). The structured models AUC improved from 0.76 to 0.79 with note embeddings and reached 0.81 with added diagnosis codes. Compared to an expert recommendation framework applied by human reviewers and an LLM-based pipeline, our ML approach offered higher specificity without compromising sensitivity. The recommendation framework achieved sensitivity 86%, specificity 57%, while the LLM maintained high sensitivity (96%) but over classified negatives, reducing specificity (16%). These findings demonstrate that ML models integrating structured and unstructured data can outperform consensus recommendations, enhancing diagnostic stewardship beyond existing standards of care.\n        △ Less",
        "authors":"['Fatemeh Amrollahi', 'Nicholas Marshall', 'Fateme Nateghi Haredasht', 'Kameron C Black', 'Aydin Zahedivash', 'Manoj V Maddali', 'Stephen P. Ma', 'Amy Chang', 'MD Phar Stanley C Deresinski', 'Mary Kane Goldstein', 'Steven M. Asch', 'Niaz Banaei', 'Jonathan H Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Agentic SLMs: Hunting Down Test Smells",
        "abstract":"Test smells can compromise the reliability of test suites and hinder software maintenance. Although several strategies exist for detecting test smells, few address their removal. Traditional methods often rely on static analysis or machine learning, requiring significant effort and expertise. This study evaluates LLAMA 3.2 3B, GEMMA 2 9B, DEEPSEEK-R1 14B, and PHI 4 14B - small, open language models - for automating the detection and refactoring of test smells through agent-based workflows. We explore workflows with one, two, and four agents across 150 instances of 5 common test smell types extracted from real-world Java projects. Unlike prior approaches, ours is easily extensible to new smells via natural language definitions and generalizes to Python and Golang. All models detected nearly all test smell instances (pass@5 of 96% with four agents), with PHI 4 14B achieving the highest refactoring accuracy (pass@5 of 75.3%). Analyses were computationally inexpensive and ran efficiently on a consumer-grade hardware. Notably, PHI 4 14B with four agents performed within 5% of proprietary models such as O1-MINI, O3-MINI-HIGH, and GEMINI 2.5 PRO EXPERIMENTAL using a single agent. Multi-agent setups outperformed single-agent ones in three out of five test smell types, highlighting their potential to improve software quality with minimal developer effort. For the Assertion Roulette smell, however, a single agent performed better. To assess practical relevance, we submitted 10 pull requests with PHI 4 14B - generated code to open-source projects. Five were merged, one was rejected, and four remain under review, demonstrating the approach's real-world applicability.\n        △ Less",
        "authors":"['Rian Melo', 'Pedro Simões', 'Rohit Gheyi', \"Marcelo d'Amorim\", 'Márcio Ribeiro', 'Gustavo Soares', 'Eduardo Almeida', 'Elvys Soares']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Language Modeling for the Future of Finance: A Quantitative Survey into Metrics, Tasks, and Data Opportunities",
        "abstract":"Recent advances in language modeling have led to growing interest in applying Natural Language Processing (NLP) techniques to financial problems, enabling new approaches to analysis and decision-making. To systematically examine this trend, we review 374 NLP research papers published between 2017 and 2024 across 38 conferences and workshops, with a focused analysis of 221 papers that directly address finance-related tasks. We evaluate these papers across 11 qualitative and quantitative dimensions, identifying key trends such as the increasing use of general-purpose language models, steady progress in sentiment analysis and information extraction, and emerging efforts around explainability and privacy-preserving methods. We also discuss the use of evaluation metrics, highlighting the importance of domain-specific ones to complement standard machine learning metrics. Our findings emphasize the need for more accessible, adaptive datasets and highlight the significance of incorporating financial crisis periods to strengthen model robustness under real-world conditions. This survey provides a structured overview of NLP research applied to finance and offers practical insights for researchers and practitioners working at this intersection.\n        △ Less",
        "authors":"['Nikita Tatarinov', 'Siddhant Sukhani', 'Agam Shah', 'Sudheer Chava']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evaluating Parameter-Based Training Performance of Neural Networks and Variational Quantum Circuits",
        "abstract":"In recent years, neural networks (NNs) have driven significant advances in machine learning. However, as tasks grow more complex, NNs often require large numbers of trainable parameters, which increases computational and energy demands. Variational quantum circuits (VQCs) offer a promising alternative: they leverage quantum mechanics to capture intricate relationships and typically need fewer parameters. In this work, we evaluate NNs and VQCs on simple supervised and reinforcement learning tasks, examining models with different parameter sizes. We simulate VQCs and execute selected parts of the training process on real quantum hardware to approximate actual training times. Our results show that VQCs can match NNs in performance while using significantly fewer parameters, despite longer training durations. As quantum technology and algorithms advance, and VQC architectures improve, we posit that VQCs could become advantageous for certain machine learning tasks.\n        △ Less",
        "authors":"['Michael Kölle', 'Alexander Feist', 'Jonas Stein', 'Sebastian Wölckert', 'Claudia Linnhoff-Popien']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adapting to Online Distribution Shifts in Deep Learning: A Black-Box Approach",
        "abstract":"We study the well-motivated problem of online distribution shift in which the data arrive in batches and the distribution of each batch can change arbitrarily over time. Since the shifts can be large or small, abrupt or gradual, the length of the relevant historical data to learn from may vary over time, which poses a major challenge in designing algorithms that can automatically adapt to the best ``attention span'' while remaining computationally efficient. We propose a meta-algorithm that takes any network architecture and any Online Learner (OL) algorithm as input and produces a new algorithm which provably enhances the performance of the given OL under non-stationarity. Our algorithm is efficient (it requires maintaining only $O(\\log(T))$ OL instances) and adaptive (it automatically chooses OL instances with the ideal ``attention'' length at every timestamp). Experiments on various real-world datasets across text and image modalities show that our method consistently improves the accuracy of user specified OL algorithms for classification tasks. Key novel algorithmic ingredients include a \\emph{multi-resolution instance} design inspired by wavelet theory and a cross-validation-through-time technique. Both could be of independent interest.\n        △ Less",
        "authors":"['Dheeraj Baby', 'Boran Han', 'Shuai Zhang', 'Cuixiong Hu', 'Yuyang Wang', 'Yu-Xiang Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Better Decisions through the Right Causal World Model",
        "abstract":"Reinforcement learning (RL) agents have shown remarkable performances in various environments, where they can discover effective policies directly from sensory inputs. However, these agents often exploit spurious correlations in the training data, resulting in brittle behaviours that fail to generalize to new or slightly modified environments. To address this, we introduce the Causal Object-centric Model Extraction Tool (COMET), a novel algorithm designed to learn the exact interpretable causal world models (CWMs). COMET first extracts object-centric state descriptions from observations and identifies the environment's internal states related to the depicted objects' properties. Using symbolic regression, it models object-centric transitions and derives causal relationships governing object dynamics. COMET further incorporates large language models (LLMs) for semantic inference, annotating causal variables to enhance interpretability.\n  By leveraging these capabilities, COMET constructs CWMs that align with the true causal structure of the environment, enabling agents to focus on task-relevant features. The extracted CWMs mitigate the danger of shortcuts, permitting the development of RL systems capable of better planning and decision-making across dynamic scenarios. Our results, validated in Atari environments such as Pong and Freeway, demonstrate the accuracy and robustness of COMET, highlighting its potential to bridge the gap between object-centric reasoning and causal inference in reinforcement learning.\n        △ Less",
        "authors":"['Elisabeth Dillies', 'Quentin Delfosse', 'Jannis Blüml', 'Raban Emunds', 'Florian Peter Busch', 'Kristian Kersting']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Resource-efficient Inference with Foundation Model Programs",
        "abstract":"The inference-time resource costs of large language and vision models present a growing challenge in production deployments. We propose the use of foundation model programs, i.e., programs that can invoke foundation models with varying resource costs and performance, as an approach to this problem. Specifically, we present a method that translates a task into a program, then learns a policy for resource allocation that, on each input, selects foundation model \"backends\" for each program module. The policy uses smaller, cheaper backends to handle simpler subtasks, while allowing more complex subtasks to leverage larger, more capable models. We evaluate the method on two new \"streaming\" visual question-answering tasks in which a system answers a question on a sequence of inputs, receiving ground-truth feedback after each answer. Compared to monolithic multi-modal models, our implementation achieves up to 98% resource savings with minimal accuracy loss, demonstrating its potential for scalable and resource-efficient multi-modal inference.\n        △ Less",
        "authors":"['Lunyiu Nie', 'Zhimin Ding', 'Kevin Yu', 'Marco Cheung', 'Chris Jermaine', 'Swarat Chaudhuri']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A new training approach for text classification in Mental Health: LatentGLoss",
        "abstract":"This study presents a multi-stage approach to mental health classification by leveraging traditional machine learning algorithms, deep learning architectures, and transformer-based models. A novel data set was curated and utilized to evaluate the performance of various methods, starting with conventional classifiers and advancing through neural networks. To broaden the architectural scope, recurrent neural networks (RNNs) such as LSTM and GRU were also evaluated to explore their effectiveness in modeling sequential patterns in the data. Subsequently, transformer models such as BERT were fine-tuned to assess the impact of contextual embeddings in this domain. Beyond these baseline evaluations, the core contribution of this study lies in a novel training strategy involving a dual-model architecture composed of a teacher and a student network. Unlike standard distillation techniques, this method does not rely on soft label transfer; instead, it facilitates information flow through both the teacher model's output and its latent representations by modifying the loss function. The experimental results highlight the effectiveness of each modeling stage and demonstrate that the proposed loss function and teacher-student interaction significantly enhance the model's learning capacity in mental health prediction tasks.\n        △ Less",
        "authors":"['Korhan Sevinç']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Prototype-Based Continual Learning with Label-free Replay Buffer and Cluster Preservation Loss",
        "abstract":"Continual learning techniques employ simple replay sample selection processes and use them during subsequent tasks. Typically, they rely on labeled data. In this paper, we depart from this by automatically selecting prototypes stored without labels, preserving cluster structures in the latent space across tasks. By eliminating label dependence in the replay buffer and introducing cluster preservation loss, it is demonstrated that the proposed method can maintain essential information from previously encountered tasks while ensuring adaptation to new tasks. \"Push-away\" and \"pull-toward\" mechanisms over previously learned prototypes are also introduced for class-incremental and domain-incremental scenarios. These mechanisms ensure the retention of previously learned information as well as adaptation to new classes or domain shifts. The proposed method is evaluated on several benchmarks, including SplitCIFAR100, SplitImageNet32, SplitTinyImageNet, and SplitCaltech256 for class-incremental, as well as R-MNIST and CORe50 for domain-incremental setting using pre-extracted DINOv2 features. Experimental results indicate that the label-free replay-based technique outperforms state-of-the-art continual learning methods and, in some cases, even surpasses offline learning. An unsupervised variant of the proposed technique for the class-incremental setting, avoiding labels use even on incoming data, also demonstrated competitive performance, outperforming particular supervised baselines in some cases. These findings underscore the effectiveness of the proposed framework in retaining prior information and facilitating continual adaptation.\n        △ Less",
        "authors":"['Agil Aghasanli', 'Yi Li', 'Plamen Angelov']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Earth-like planet predictor: A machine learning approach",
        "abstract":"Searching for planets analogous to Earth in terms of mass and equilibrium temperature is currently the first step in the quest for habitable conditions outside our Solar System and, ultimately, the search for life in the universe. Future missions such as PLATO or LIFE will begin to detect and characterise these small, cold planets, dedicating significant observation time to them. The aim of this work is to predict which stars are most likely to host an Earth-like planet (ELP) to avoid blind searches, minimises detection times, and thus maximises the number of detections. Using a previous study on correlations between the presence of an ELP and the properties of its system, we trained a Random Forest to recognise and classify systems as 'hosting an ELP' or 'not hosting an ELP'. The Random Forest was trained and tested on populations of synthetic planetary systems derived from the Bern model, and then applied to real observed systems. The tests conducted on the machine learning (ML) model yield precision scores of up to 0.99, indicating that 99% of the systems identified by the model as having ELPs possess at least one. Among the few real observed systems that have been tested, 44 have been selected as having a high probability of hosting an ELP, and a quick study of the stability of these systems confirms that the presence of an Earth-like planet within them would leave them stable. The excellent results obtained from the tests conducted on the ML model demonstrate its ability to recognise the typical architectures of systems with or without ELPs within populations derived from the Bern model. If we assume that the Bern model adequately describes the architecture of real systems, then such a tool can prove indispensable in the search for Earth-like planets. A similar approach could be applied to other planetary system formation models to validate those predictions.\n        △ Less",
        "authors":"['Jeanne Davoult', 'Romain Eltschinger', 'Yann Alibert']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reservoir Computing with a Single Oscillating Gas Bubble: Emphasizing the Chaotic Regime",
        "abstract":"The rising computational and energy demands of artificial intelligence systems urge the exploration of alternative software and hardware solutions that exploit physical effects for computation. According to machine learning theory, a neural network-based computational system must exhibit nonlinearity to effectively model complex patterns and relationships. This requirement has driven extensive research into various nonlinear physical systems to enhance the performance of neural networks. In this paper, we propose and theoretically validate a reservoir computing system based on a single bubble trapped within a bulk of liquid. By applying an external acoustic pressure wave to both encode input information and excite the complex nonlinear dynamics, we showcase the ability of this single-bubble reservoir computing system to forecast complex benchmarking time series and undertake classification tasks with high accuracy. Specifically, we demonstrate that a chaotic physical regime of bubble oscillation proves to be the most effective for this kind of computations.\n        △ Less",
        "authors":"['Hend Abdel-Ghani', 'A. H. Abbas', 'Ivan S. Maksymov']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Machine Learning Techniques in Intrusion Detection Systems for Internet of Things",
        "abstract":"As the Internet of Things (IoT) continues to expand, ensuring the security of connected devices has become increasingly critical. Traditional Intrusion Detection Systems (IDS) often fall short in managing the dynamic and large-scale nature of IoT networks. This paper explores how Machine Learning (ML) and Deep Learning (DL) techniques can significantly enhance IDS performance in IoT environments. We provide a thorough overview of various IDS deployment strategies and categorize the types of intrusions common in IoT systems. A range of ML methods -- including Support Vector Machines, Naive Bayes, K-Nearest Neighbors, Decision Trees, and Random Forests -- are examined alongside advanced DL models such as LSTM, CNN, Autoencoders, RNNs, and Deep Belief Networks. Each technique is evaluated based on its accuracy, efficiency, and suitability for real-world IoT applications. We also address major challenges such as high false positive rates, data imbalance, encrypted traffic analysis, and the resource constraints of IoT devices. In addition, we highlight the emerging role of Generative AI and Large Language Models (LLMs) in improving threat detection, automating responses, and generating intelligent security policies. Finally, we discuss ethical and privacy concerns, underscoring the need for responsible and transparent implementation. This paper aims to provide a comprehensive framework for developing adaptive, intelligent, and secure IDS solutions tailored for the evolving landscape of IoT.\n        △ Less",
        "authors":"['Saeid Jamshidi', 'Amin Nikanjam', 'Nafi Kawser Wazed', 'Foutse Khomh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evolutionary algorithms meet self-supervised learning: a comprehensive survey",
        "abstract":"The number of studies that combine Evolutionary Machine Learning and self-supervised learning has been growing steadily in recent years. Evolutionary Machine Learning has been shown to help automate the design of machine learning algorithms and to lead to more reliable solutions. Self-supervised learning, on the other hand, has produced good results in learning useful features when labelled data is limited. This suggests that the combination of these two areas can help both in shaping evolutionary processes and in automating the design of deep neural networks, while also reducing the need for labelled data. Still, there are no detailed reviews that explain how Evolutionary Machine Learning and self-supervised learning can be used together. To help with this, we provide an overview of studies that bring these areas together. Based on this growing interest and the range of existing works, we suggest a new sub-area of research, which we call Evolutionary Self-Supervised Learning and introduce a taxonomy for it. Finally, we point out some of the main challenges and suggest directions for future research to help Evolutionary Self-Supervised Learning grow and mature as a field.\n        △ Less",
        "authors":"['Adriano Vinhas', 'João Correia', 'Penousal Machado']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MESA: Text-Driven Terrain Generation Using Latent Diffusion and Global Copernicus Data",
        "abstract":"Terrain modeling has traditionally relied on procedural techniques, which often require extensive domain expertise and handcrafted rules. In this paper, we present MESA - a novel data-centric alternative by training a diffusion model on global remote sensing data. This approach leverages large-scale geospatial information to generate high-quality terrain samples from text descriptions, showcasing a flexible and scalable solution for terrain generation. The model's capabilities are demonstrated through extensive experiments, highlighting its ability to generate realistic and diverse terrain landscapes. The dataset produced to support this work, the Major TOM Core-DEM extension dataset, is released openly as a comprehensive resource for global terrain data. The results suggest that data-driven models, trained on remote sensing data, can provide a powerful tool for realistic terrain modeling and generation.\n        △ Less",
        "authors":"['Paul Borne--Pons', 'Mikolaj Czerkawski', 'Rosalie Martin', 'Romain Rouffet']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog",
        "abstract":"We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated subject tagging for scientific and technical records in English and German using the GND taxonomy. Participants developed LLM-based systems to recommend top-k subjects, evaluated through quantitative metrics (precision, recall, F1-score) and qualitative assessments by subject specialists. Results highlight the effectiveness of LLM ensembles, synthetic data generation, and multilingual processing, offering insights into applying LLMs for digital library classification.\n        △ Less",
        "authors":"[\"Jennifer D'Souza\", 'Sameer Sadruddin', 'Holger Israel', 'Mathias Begoin', 'Diana Slawig']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HypoEval: Hypothesis-Guided Evaluation for Natural Language Generation",
        "abstract":"Large language models (LLMs) have demonstrated great potential for automating the evaluation of natural language generation. Previous frameworks of LLM-as-a-judge fall short in two ways: they either use zero-shot setting without consulting any human input, which leads to low alignment, or fine-tune LLMs on labeled data, which requires a non-trivial number of samples. Moreover, previous methods often provide little reasoning behind automated evaluations. In this paper, we propose HypoEval, Hypothesis-guided Evaluation framework, which first uses a small corpus of human evaluations to generate more detailed rubrics for human judgments and then incorporates a checklist-like approach to combine LLM's assigned scores on each decomposed dimension to acquire overall scores. With only 30 human evaluations, HypoEval achieves state-of-the-art performance in alignment with both human rankings (Spearman correlation) and human scores (Pearson correlation), on average outperforming G-Eval by 11.86% and fine-tuned Llama-3.1-8B-Instruct with at least 3 times more human evaluations by 11.95%. Furthermore, we conduct systematic studies to assess the robustness of HypoEval, highlighting its effectiveness as a reliable and interpretable automated evaluation framework.\n        △ Less",
        "authors":"['Mingxuan Li', 'Hanchen Li', 'Chenhao Tan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Trustworthy AI Must Account for Intersectionality",
        "abstract":"Trustworthy AI encompasses many aspirational aspects for aligning AI systems with human values, including fairness, privacy, robustness, explainability, and uncertainty quantification. However, efforts to enhance one aspect often introduce unintended trade-offs that negatively impact others, making it challenging to improve all aspects simultaneously. In this position paper, we review notable approaches to these five aspects and systematically consider every pair, detailing the negative interactions that can arise. For example, applying differential privacy to model training can amplify biases in the data, undermining fairness. Drawing on these findings, we take the position that addressing trustworthiness along each axis in isolation is insufficient. Instead, research on Trustworthy AI must account for intersectionality between aspects and adopt a holistic view across all relevant axes at once. To illustrate our perspective, we provide guidance on how researchers can work towards integrated trustworthiness, a case study on how intersectionality applies to the financial industry, and alternative views to our position.\n        △ Less",
        "authors":"['Jesse C. Cresswell']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"R2E-Gym: Procedural Environments and Hybrid Verifiers for Scaling Open-Weights SWE Agents",
        "abstract":"Improving open-source models on real-world SWE tasks (solving GITHUB issues) faces two key challenges: 1) scalable curation of execution environments to train these models, and, 2) optimal scaling of test-time compute. We introduce AgentGym, the largest procedurally-curated executable gym environment for training real-world SWE-agents, consisting of more than 8.7K tasks. AgentGym is powered by two main contributions: 1) SYNGEN: a synthetic data curation recipe that enables scalable curation of executable environments using test-generation and back-translation directly from commits, thereby reducing reliance on human-written issues or unit tests. We show that this enables more scalable training leading to pass@1 performance of 34.4% on SWE-Bench Verified benchmark with our 32B model. 2) Hybrid Test-time Scaling: we provide an in-depth analysis of two test-time scaling axes; execution-based and execution-free verifiers, demonstrating that they exhibit complementary strengths and limitations. Test-based verifiers suffer from low distinguishability, while execution-free verifiers are biased and often rely on stylistic features. Surprisingly, we find that while each approach individually saturates around 42-43%, significantly higher gains can be obtained by leveraging their complementary strengths. Overall, our approach achieves 51% on the SWE-Bench Verified benchmark, reflecting a new state-of-the-art for open-weight SWE-agents and for the first time showing competitive performance with proprietary models such as o1, o1-preview and sonnet-3.5-v2 (with tools). We will open-source our environments, models, and agent trajectories.\n        △ Less",
        "authors":"['Naman Jain', 'Jaskirat Singh', 'Manish Shetty', 'Liang Zheng', 'Koushik Sen', 'Ion Stoica']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Object Tracking for Collision Avoidance Using Multiple Cameras in Open RAN Networks",
        "abstract":"This paper deals with the multi-object detection and tracking problem, within the scope of open Radio Access Network (RAN), for collision avoidance in vehicular scenarios. To this end, a set of distributed intelligent agents collocated with cameras are considered. The fusion of detected objects is done at an edge service, considering Open RAN connectivity. Then, the edge service predicts the objects trajectories for collision avoidance. Compared to the related work a more realistic Open RAN network is implemented and multiple cameras are used.\n        △ Less",
        "authors":"['Jordi Serra', 'Anton Aguilar', 'Ebrahim Abu-Helalah', 'Raúl Parada', 'Paolo Dini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-based identification and support of at-risk students: A case study of the Moroccan education system",
        "abstract":"Student dropout is a global issue influenced by personal, familial, and academic factors, with varying rates across countries. This paper introduces an AI-driven predictive modeling approach to identify students at risk of dropping out using advanced machine learning techniques. The goal is to enable timely interventions and improve educational outcomes. Our methodology is adaptable across different educational systems and levels. By employing a rigorous evaluation framework, we assess model performance and use Shapley Additive exPlanations (SHAP) to identify key factors influencing predictions. The approach was tested on real data provided by the Moroccan Ministry of National Education, achieving 88% accuracy, 88% recall, 86% precision, and an AUC of 87%. These results highlight the effectiveness of the AI models in identifying at-risk students. The framework is adaptable, incorporating historical data for both short and long-term detection, offering a comprehensive solution to the persistent challenge of student dropout.\n        △ Less",
        "authors":"['Ismail Elbouknify', 'Ismail Berrada', 'Loubna Mekouar', 'Youssef Iraqi', 'El Houcine Bergou', 'Hind Belhabib', 'Younes Nail', 'Souhail Wardi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Holistic Capability Preservation: Towards Compact Yet Comprehensive Reasoning Models",
        "abstract":"This technical report presents Ring-Lite-Distill, a lightweight reasoning model derived from our open-source Mixture-of-Experts (MoE) Large Language Models (LLMs) Ling-Lite. This study demonstrates that through meticulous high-quality data curation and ingenious training paradigms, the compact MoE model Ling-Lite can be further trained to achieve exceptional reasoning capabilities, while maintaining its parameter-efficient architecture with only 2.75 billion activated parameters, establishing an efficient lightweight reasoning architecture. In particular, in constructing this model, we have not merely focused on enhancing advanced reasoning capabilities, exemplified by high-difficulty mathematical problem solving, but rather aimed to develop a reasoning model with more comprehensive competency coverage. Our approach ensures coverage across reasoning tasks of varying difficulty levels while preserving generic capabilities, such as instruction following, tool use, and knowledge retention. We show that, Ring-Lite-Distill's reasoning ability reaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general capabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The models are accessible at https:\/\/huggingface.co\/inclusionAI\n        △ Less",
        "authors":"['Ling Team', 'Caizhi Tang', 'Chilin Fu', 'Chunwei Wu', 'Jia Guo', 'Jianwen Wang', 'Jingyu Hu', 'Liang Jiang', 'Meng Li', 'Peng Jiao', 'Pingping Liu', 'Shaomian Zheng', 'Shiwei Liang', 'Shuaicheng Li', 'Yalin Zhang', 'Yingting Wu', 'Yongkang Liu', 'Zhenyu Huang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GAAPO: Genetic Algorithmic Applied to Prompt Optimization",
        "abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, with their performance heavily dependent on the quality of input prompts. While prompt engineering has proven effective, it typically relies on manual adjustments, making it time-consuming and potentially suboptimal. This paper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a novel hybrid optimization framework that leverages genetic algorithm principles to evolve prompts through successive generations. Unlike traditional genetic approaches that rely solely on mutation and crossover operations, GAAPO integrates multiple specialized prompt generation strategies within its evolutionary framework. Through extensive experimentation on diverse datasets including ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important point for the future development of automatic prompt optimization methods: importance of the tradeoff between the population size and the number of generations, effect of selection methods on stability results, capacity of different LLMs and especially reasoning models to be able to automatically generate prompts from similar queries... Furthermore, we provide insights into the relative effectiveness of different prompt generation strategies and their evolution across optimization phases. These findings contribute to both the theoretical understanding of prompt optimization and practical applications in improving LLM performance.\n        △ Less",
        "authors":"['Xavier Sécheresse', 'Jacques-Yves Guilbert--Ly', 'Antoine Villedieu de Torcy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PLM-eXplain: Divide and Conquer the Protein Embedding Space",
        "abstract":"Protein language models (PLMs) have revolutionised computational biology through their ability to generate powerful sequence representations for diverse prediction tasks. However, their black-box nature limits biological interpretation and translation to actionable insights. We present an explainable adapter layer - PLM-eXplain (PLM-X), that bridges this gap by factoring PLM embeddings into two components: an interpretable subspace based on established biochemical features, and a residual subspace that preserves the model's predictive power. Using embeddings from ESM2, our adapter incorporates well-established properties, including secondary structure and hydropathy while maintaining high performance. We demonstrate the effectiveness of our approach across three protein-level classification tasks: prediction of extracellular vesicle association, identification of transmembrane helices, and prediction of aggregation propensity. PLM-X enables biological interpretation of model decisions without sacrificing accuracy, offering a generalisable solution for enhancing PLM interpretability across various downstream applications. This work addresses a critical need in computational biology by providing a bridge between powerful deep learning models and actionable biological insights.\n        △ Less",
        "authors":"['Jan van Eck', 'Dea Gogishvili', 'Wilson Silva', 'Sanne Abeln']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Compound Fault Diagnosis for Train Transmission Systems Using Deep Learning with Fourier-enhanced Representation",
        "abstract":"Fault diagnosis prevents train disruptions by ensuring the stability and reliability of their transmission systems. Data-driven fault diagnosis models have several advantages over traditional methods in terms of dealing with non-linearity, adaptability, scalability, and automation. However, existing data-driven models are trained on separate transmission components and only consider single faults due to the limitations of existing datasets. These models will perform worse in scenarios where components operate with each other at the same time, affecting each component's vibration signals. To address some of these challenges, we propose a frequency domain representation and a 1-dimensional convolutional neural network for compound fault diagnosis and applied it on the PHM Beijing 2024 dataset, which includes 21 sensor channels, 17 single faults, and 42 compound faults from 4 interacting components, that is, motor, gearbox, left axle box, and right axle box. Our proposed model achieved 97.67% and 93.93% accuracies on the test set with 17 single faults and on the test set with 42 compound faults, respectively.\n        △ Less",
        "authors":"['Jonathan Adam Rico', 'Nagarajan Raghavan', 'Senthilnath Jayavelu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Sturm--Liouville: From Sample-Based to 1D Regularization with Learnable Orthogonal Basis Functions",
        "abstract":"Although Artificial Neural Networks (ANNs) have achieved remarkable success across various tasks, they still suffer from limited generalization. We hypothesize that this limitation arises from the traditional sample-based (0--dimensionnal) regularization used in ANNs. To overcome this, we introduce \\textit{Deep Sturm--Liouville} (DSL), a novel function approximator that enables continuous 1D regularization along field lines in the input space by integrating the Sturm--Liouville Theorem (SLT) into the deep learning framework. DSL defines field lines traversing the input space, along which a Sturm--Liouville problem is solved to generate orthogonal basis functions, enforcing implicit regularization thanks to the desirable properties of SLT. These basis functions are linearly combined to construct the DSL approximator. Both the vector field and basis functions are parameterized by neural networks and learned jointly. We demonstrate that the DSL formulation naturally arises when solving a Rank-1 Parabolic Eigenvalue Problem. DSL is trained efficiently using stochastic gradient descent via implicit differentiation. DSL achieves competitive performance and demonstrate improved sample efficiency on diverse multivariate datasets including high-dimensional image datasets such as MNIST and CIFAR-10.\n        △ Less",
        "authors":"['David Vigouroux', 'Joseba Dalmau', 'Louis Béthune', 'Victor Boutin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can SGD Select Good Fishermen? Local Convergence under Self-Selection Biases and Beyond",
        "abstract":"We revisit the problem of estimating $k$ linear regressors with self-selection bias in $d$ dimensions with the maximum selection criterion, as introduced by Cherapanamjeri, Daskalakis, Ilyas, and Zampetakis [CDIZ23, STOC'23]. Our main result is a $\\operatorname{poly}(d,k,1\/\\varepsilon) + {k}^{O(k)}$ time algorithm for this problem, which yields an improvement in the running time of the algorithms of [CDIZ23] and [GM24, arXiv]. We achieve this by providing the first local convergence algorithm for self-selection, thus resolving the main open question of [CDIZ23].\n  To obtain this algorithm, we reduce self-selection to a seemingly unrelated statistical problem called coarsening. Coarsening occurs when one does not observe the exact value of the sample but only some set (a subset of the sample space) that contains the exact value. Inference from coarse samples arises in various real-world applications due to rounding by humans and algorithms, limited precision of instruments, and lag in multi-agent systems.\n  Our reduction to coarsening is intuitive and relies on the geometry of the self-selection problem, which enables us to bypass the limitations of previous analytic approaches. To demonstrate its applicability, we provide a local convergence algorithm for linear regression under another self-selection criterion, which is related to second-price auction data. Further, we give the first polynomial time local convergence algorithm for coarse Gaussian mean estimation given samples generated from a convex partition. Previously, only a sample-efficient algorithm was known due to Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21, COLT'21].\n        △ Less",
        "authors":"['Alkis Kalavasis', 'Anay Mehrotra', 'Felix Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SolRPDS: A Dataset for Analyzing Rug Pulls in Solana Decentralized Finance",
        "abstract":"Rug pulls in Solana have caused significant damage to users interacting with Decentralized Finance (DeFi). A rug pull occurs when developers exploit users' trust and drain liquidity from token pools on Decentralized Exchanges (DEXs), leaving users with worthless tokens. Although rug pulls in Ethereum and Binance Smart Chain (BSC) have gained attention recently, analysis of rug pulls in Solana remains largely under-explored. In this paper, we introduce SolRPDS (Solana Rug Pull Dataset), the first public rug pull dataset derived from Solana's transactions. We examine approximately four years of DeFi data (2021-2024) that covers suspected and confirmed tokens exhibiting rug pull patterns. The dataset, derived from 3.69 billion transactions, consists of 62,895 suspicious liquidity pools. The data is annotated for inactivity states, which is a key indicator, and includes several detailed liquidity activities such as additions, removals, and last interaction as well as other attributes such as inactivity periods and withdrawn token amounts, to help identify suspicious behavior. Our preliminary analysis reveals clear distinctions between legitimate and fraudulent liquidity pools and we found that 22,195 tokens in the dataset exhibit rug pull patterns during the examined period. SolRPDS can support a wide range of future research on rug pulls including the development of data-driven and heuristic-based solutions for real-time rug pull detection and mitigation.\n        △ Less",
        "authors":"['Abdulrahman Alhaidari', 'Bhavani Kalal', 'Balaji Palanisamy', 'Shamik Sural']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Embedding Reliability Verification Constraints into Generation Expansion Planning",
        "abstract":"Generation planning approaches face challenges in managing the incompatible mathematical structures between stochastic production simulations for reliability assessment and optimization models for generation planning, which hinders the integration of reliability constraints. This study proposes an approach to embedding reliability verification constraints into generation expansion planning by leveraging a weighted oblique decision tree (WODT) technique. For each planning year, a generation mix dataset, labeled with reliability assessment simulations, is generated. An WODT model is trained using this dataset. Reliability-feasible regions are extracted via depth-first search technique and formulated as disjunctive constraints. These constraints are then transformed into mixed-integer linear form using a convex hull modeling technique and embedded into a unit commitment-integrated generation expansion planning model. The proposed approach is validated through a long-term generation planning case study for the Electric Reliability Council of Texas (ERCOT) region, demonstrating its effectiveness in achieving reliable and optimal planning solutions.\n        △ Less",
        "authors":"['Peng Liu', 'Lian Cheng', 'Benjamin P. Omell', 'Anthony P. Burgard']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EqualizeIR: Mitigating Linguistic Biases in Retrieval Models",
        "abstract":"This study finds that existing information retrieval (IR) models show significant biases based on the linguistic complexity of input queries, performing well on linguistically simpler (or more complex) queries while underperforming on linguistically more complex (or simpler) queries. To address this issue, we propose EqualizeIR, a framework to mitigate linguistic biases in IR models. EqualizeIR uses a linguistically biased weak learner to capture linguistic biases in IR datasets and then trains a robust model by regularizing and refining its predictions using the biased weak learner. This approach effectively prevents the robust model from overfitting to specific linguistic patterns in data. We propose four approaches for developing linguistically-biased models. Extensive experiments on several datasets show that our method reduces performance disparities across linguistically simple and complex queries, while improving overall retrieval performance.\n        △ Less",
        "authors":"['Jiali Cheng', 'Hadi Amiri']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DashCLIP: Leveraging multimodal models for generating semantic embeddings for DoorDash",
        "abstract":"Despite the success of vision-language models in various generative tasks, obtaining high-quality semantic representations for products and user intents is still challenging due to the inability of off-the-shelf models to capture nuanced relationships between the entities. In this paper, we introduce a joint training framework for product and user queries by aligning uni-modal and multi-modal encoders through contrastive learning on image-text data. Our novel approach trains a query encoder with an LLM-curated relevance dataset, eliminating the reliance on engagement history. These embeddings demonstrate strong generalization capabilities and improve performance across applications, including product categorization and relevance prediction. For personalized ads recommendation, a significant uplift in the click-through rate and conversion rate after the deployment further confirms the impact on key business metrics. We believe that the flexibility of our framework makes it a promising solution toward enriching the user experience across the e-commerce landscape.\n        △ Less",
        "authors":"['Omkar Gurjar', 'Kin Sum Liu', 'Praveen Kolli', 'Utsaw Kumar', 'Mandar Rahurkar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Guarding Digital Privacy: Exploring User Profiling and Security Enhancements",
        "abstract":"User profiling, the practice of collecting user information for personalized recommendations, has become widespread, driving progress in technology. However, this growth poses a threat to user privacy, as devices often collect sensitive data without their owners' awareness. This article aims to consolidate knowledge on user profiling, exploring various approaches and associated challenges. Through the lens of two companies sharing user data and an analysis of 18 popular Android applications in India across various categories, including $\\textit{Social, Education, Entertainment, Travel, Shopping and Others}$, the article unveils privacy vulnerabilities. Further, the article propose an enhanced machine learning framework, employing decision trees and neural networks, that improves state-of-the-art classifiers in detecting personal information exposure. Leveraging the XAI (explainable artificial intelligence) algorithm LIME (Local Interpretable Model-agnostic Explanations), it enhances interpretability, crucial for reliably identifying sensitive data. Results demonstrate a noteworthy performance boost, achieving a $75.01\\%$ accuracy with a reduced training time of $3.62$ seconds for neural networks. Concluding, the paper suggests research directions to strengthen digital security measures.\n        △ Less",
        "authors":"['Rishika Kohli', 'Shaifu Gupta', 'Manoj Singh Gaur']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Relevance Isn't All You Need: Scaling RAG Systems With Inference-Time Compute Via Multi-Criteria Reranking",
        "abstract":"Modern Large Language Model (LLM) systems typically rely on Retrieval Augmented Generation (RAG) which aims to gather context that is useful for response generation. These RAG systems typically optimize strictly towards retrieving context that is maximally relevant to the query. However, conventional theory suggests that retrieval systems which seek to maximize context relevance without any additional explicit criteria can create information bottlenecks. We reaffirm this finding in the modern age of LLM's by showing that in standard RAG pipelines, maximizing for context relevance alone can degrade downstream response quality. In response, we show evaluations of existing RAG methods which account for both context relevance and answer quality. These evaluations introduce a novel finding that existing RAG systems scale poorly with inference time compute usage when considering our combined metric. We introduce \"RErank BEyond reLevance (REBEL)\", which enables RAG systems to scale with inference-time compute via injection of multi-criteria optimization using Chain-of-Thought prompting (and optionally Multi-Turn dialogue). Ultimately, this enables a new performance\/speed tradeoff curve, where RAG systems are able to achieve both higher relevance of retrieved contexts and superior answer quality as inference time increases. Code for the implementation of our method in llama-index can be found at the following PR: https:\/\/github.com\/run-llama\/llama_index\/pull\/17590. Code for running experiments using this llama-index implementation can be found at https:\/\/github.com\/microsoft\/REBEL.\n        △ Less",
        "authors":"['Will LeVine', 'Bijan Varjavand']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Behavior Importance-Aware Graph Neural Architecture Search for Cross-Domain Recommendation",
        "abstract":"Cross-domain recommendation (CDR) mitigates data sparsity and cold-start issues in recommendation systems. While recent CDR approaches using graph neural networks (GNNs) capture complex user-item interactions, they rely on manually designed architectures that are often suboptimal and labor-intensive. Additionally, extracting valuable behavioral information from source domains to improve target domain recommendations remains challenging. To address these challenges, we propose Behavior importance-aware Graph Neural Architecture Search (BiGNAS), a framework that jointly optimizes GNN architecture and data importance for CDR. BiGNAS introduces two key components: a Cross-Domain Customized Supernetwork and a Graph-Based Behavior Importance Perceptron. The supernetwork, as a one-shot, retrain-free module, automatically searches the optimal GNN architecture for each domain without the need for retraining. The perceptron uses auxiliary learning to dynamically assess the importance of source domain behaviors, thereby improving target domain recommendations. Extensive experiments on benchmark CDR datasets and a large-scale industry advertising dataset demonstrate that BiGNAS consistently outperforms state-of-the-art baselines. To the best of our knowledge, this is the first work to jointly optimize GNN architecture and behavior data importance for cross-domain recommendation.\n        △ Less",
        "authors":"['Chendi Ge', 'Xin Wang', 'Ziwei Zhang', 'Yijian Qin', 'Hong Chen', 'Haiyang Wu', 'Yang Zhang', 'Yuekui Yang', 'Wenwu Zhu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
        "abstract":"Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models.\n        △ Less",
        "authors":"['Nikhil Shivakumar Nayak', 'Krishnateja Killamsetty', 'Ligong Han', 'Abhishek Bhandwaldar', 'Prateek Chanda', 'Kai Xu', 'Hao Wang', 'Aldo Pareja', 'Oleg Silkin', 'Mustafa Eyceoz', 'Akash Srivastava']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Motion Simulator: Pushing the Limit of World Models in Reinforcement Learning",
        "abstract":"An embodied system must not only model the patterns of the external world but also understand its own motion dynamics. A motion dynamic model is essential for efficient skill acquisition and effective planning. In this work, we introduce the neural motion simulator (MoSim), a world model that predicts the future physical state of an embodied system based on current observations and actions. MoSim achieves state-of-the-art performance in physical state prediction and provides competitive performance across a range of downstream tasks. This works shows that when a world model is accurate enough and performs precise long-horizon predictions, it can facilitate efficient skill acquisition in imagined worlds and even enable zero-shot reinforcement learning. Furthermore, MoSim can transform any model-free reinforcement learning (RL) algorithm into a model-based approach, effectively decoupling physical environment modeling from RL algorithm development. This separation allows for independent advancements in RL algorithms and world modeling, significantly improving sample efficiency and enhancing generalization capabilities. Our findings highlight that world models for motion dynamics is a promising direction for developing more versatile and capable embodied systems.\n        △ Less",
        "authors":"['Chenjie Hao', 'Weyl Lu', 'Yifan Xu', 'Yubei Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Are We Done with Object-Centric Learning?",
        "abstract":"Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called Object-Centric Classification with Applied Masks (OCCAM), demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available here: https:\/\/github.com\/AlexanderRubinstein\/OCCAM.\n        △ Less",
        "authors":"['Alexander Rubinstein', 'Ameya Prabhu', 'Matthias Bethge', 'Seong Joon Oh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AssistanceZero: Scalably Solving Assistance Games",
        "abstract":"Assistance games are a promising alternative to reinforcement learning from human feedback (RLHF) for training AI assistants. Assistance games resolve key drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly modeling the interaction between assistant and user as a two-player game where the assistant cannot observe their shared goal. Despite their potential, assistance games have only been explored in simple settings. Scaling them to more complex environments is difficult because it requires both solving intractable decision-making problems under uncertainty and accurately modeling human users' behavior. We present the first scalable approach to solving assistance games and apply it to a new, challenging Minecraft-based assistance game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends AlphaZero with a neural network that predicts human actions and rewards, enabling it to plan under uncertainty. We show that AssistanceZero outperforms model-free RL algorithms and imitation learning in the Minecraft-based assistance game. In a human study, our AssistanceZero-trained assistant significantly reduces the number of actions participants take to complete building tasks in Minecraft. Our results suggest that assistance games are a tractable framework for training effective AI assistants in complex environments. Our code and models are available at https:\/\/github.com\/cassidylaidlaw\/minecraft-building-assistance-game.\n        △ Less",
        "authors":"['Cassidy Laidlaw', 'Eli Bronstein', 'Timothy Guo', 'Dylan Feng', 'Lukas Berglund', 'Justin Svegliato', 'Stuart Russell', 'Anca Dragan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility",
        "abstract":"Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.\n        △ Less",
        "authors":"['Andreas Hochlehnert', 'Hardik Bhatnagar', 'Vishaal Udandarao', 'Samuel Albanie', 'Ameya Prabhu', 'Matthias Bethge']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Identifying Unknown Stochastic Dynamics via Finite expression methods",
        "abstract":"Modeling stochastic differential equations (SDEs) is crucial for understanding complex dynamical systems in various scientific fields. Recent methods often employ neural network-based models, which typically represent SDEs through a combination of deterministic and stochastic terms. However, these models usually lack interpretability and have difficulty generalizing beyond their training domain. This paper introduces the Finite Expression Method (FEX), a symbolic learning approach designed to derive interpretable mathematical representations of the deterministic component of SDEs. For the stochastic component, we integrate FEX with advanced generative modeling techniques to provide a comprehensive representation of SDEs. The numerical experiments on linear, nonlinear, and multidimensional SDEs demonstrate that FEX generalizes well beyond the training domain and delivers more accurate long-term predictions compared to neural network-based methods. The symbolic expressions identified by FEX not only improve prediction accuracy but also offer valuable scientific insights into the underlying dynamics of the systems, paving the way for new scientific discoveries.\n        △ Less",
        "authors":"['Senwei Liang', 'Chunmei Wang', 'Xingjian Xu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning",
        "abstract":"Despite great performance on Olympiad-level reasoning problems, frontier large language models can still struggle on high school math when presented with novel problems outside standard benchmarks. Going beyond final accuracy, we propose a deductive consistency metric to analyze chain-of-thought output from language models (LMs).Formally, deductive reasoning involves two subtasks: understanding a set of input premises and inferring the conclusions that follow from them. The proposed metric studies LMs' performance on these subtasks, with the goal of explaining LMs' reasoning errors on novel problems: how well do LMs understand input premises with increasing context lengths, and how well can they infer conclusions over multiple reasoning hops? Since existing benchmarks may be memorized, we develop a pipeline to evaluate LMs' deductive consistency on novel, perturbed versions of benchmark problems. On novel grade school math problems (GSM-8k), we find that LMs are fairly robust to increasing number of input premises, but suffer significant accuracy decay as the number of reasoning hops is increased. Interestingly, these errors are masked in the original benchmark as all models achieve near 100% accuracy. As we increase the number of solution steps using a synthetic dataset, prediction over multiple hops still remains the major source of error compared to understanding input premises. Other factors, such as shifts in language style or natural propagation of early errors do not explain the trends. Our analysis provides a new view to characterize LM reasoning -- as computations over a window of input premises and reasoning hops -- that can provide unified evaluation across problem domains.\n        △ Less",
        "authors":"['Atharva Pandey', 'Kshitij Dubey', 'Rahul Sharma', 'Amit Sharma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detecting AI-generated Artwork",
        "abstract":"The high efficiency and quality of artwork generated by Artificial Intelligence (AI) has created new concerns and challenges for human artists. In particular, recent improvements in generative AI have made it difficult for people to distinguish between human-generated and AI-generated art. In this research, we consider the potential utility of various types of Machine Learning (ML) and Deep Learning (DL) models in distinguishing AI-generated artwork from human-generated artwork. We focus on three challenging artistic styles, namely, baroque, cubism, and expressionism. The learning models we test are Logistic Regression (LR), Support Vector Machine (SVM), Multilayer Perceptron (MLP), and Convolutional Neural Network (CNN). Our best experimental results yield a multiclass accuracy of 0.8208 over six classes, and an impressive accuracy of 0.9758 for the binary classification problem of distinguishing AI-generated from human-generated art.\n        △ Less",
        "authors":"['Meien Li', 'Mark Stamp']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Downstream Analysis in Genome Sequencing: Species Classification While Basecalling",
        "abstract":"The ability to quickly and accurately identify microbial species in a sample, known as metagenomic profiling, is critical across various fields, from healthcare to environmental science. This paper introduces a novel method to profile signals coming from sequencing devices in parallel with determining their nucleotide sequences, a process known as basecalling, via a multi-objective deep neural network for simultaneous basecalling and multi-class genome classification. We introduce a new loss strategy where losses for basecalling and classification are back-propagated separately, with model weights combined for the shared layers, and a pre-configured ranking strategy allowing top-K species accuracy, giving users flexibility to choose between higher accuracy or higher speed at identifying the species. We achieve state-of-the-art basecalling accuracies, while classification accuracies meet and exceed the results of state-of-the-art binary classifiers, attaining an average of 92.5%\/98.9% accuracy at identifying the top-1\/3 species among a total of 17 genomes in the Wick bacterial dataset. The work presented here has implications for future studies in metagenomic profiling by accelerating the bottleneck step of matching the DNA sequence to the correct genome.\n        △ Less",
        "authors":"['Riselda Kodra', 'Hadjer Benmeziane', 'Irem Boybat', 'William Andrew Simon']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generalized Semantic Contrastive Learning via Embedding Side Information for Few-Shot Object Detection",
        "abstract":"The objective of few-shot object detection (FSOD) is to detect novel objects with few training samples. The core challenge of this task is how to construct a generalized feature space for novel categories with limited data on the basis of the base category space, which could adapt the learned detection model to unknown scenarios. However, limited by insufficient samples for novel categories, two issues still exist: (1) the features of the novel category are easily implicitly represented by the features of the base category, leading to inseparable classifier boundaries, (2) novel categories with fewer data are not enough to fully represent the distribution, where the model fine-tuning is prone to overfitting. To address these issues, we introduce the side information to alleviate the negative influences derived from the feature space and sample viewpoints and formulate a novel generalized feature representation learning method for FSOD. Specifically, we first utilize embedding side information to construct a knowledge matrix to quantify the semantic relationship between the base and novel categories. Then, to strengthen the discrimination between semantically similar categories, we further develop contextual semantic supervised contrastive learning which embeds side information. Furthermore, to prevent overfitting problems caused by sparse samples, a side-information guided region-aware masked module is introduced to augment the diversity of samples, which finds and abandons biased information that discriminates between similar categories via counterfactual explanation, and refines the discriminative representation space further. Extensive experiments using ResNet and ViT backbones on PASCAL VOC, MS COCO, LVIS V1, FSOD-1K, and FSVOD-500 benchmarks demonstrate that our model outperforms the previous state-of-the-art methods, significantly improving the ability of FSOD in most shots\/splits.\n        △ Less",
        "authors":"['Ruoyu Chen', 'Hua Zhang', 'Jingzhi Li', 'Li Liu', 'Zhen Huang', 'Xiaochun Cao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"$Π$-NeSy: A Possibilistic Neuro-Symbolic Approach",
        "abstract":"In this article, we introduce a neuro-symbolic approach that combines a low-level perception task performed by a neural network with a high-level reasoning task performed by a possibilistic rule-based system. The goal is to be able to derive for each input instance the degree of possibility that it belongs to a target (meta-)concept. This (meta-)concept is connected to intermediate concepts by a possibilistic rule-based system. The probability of each intermediate concept for the input instance is inferred using a neural network. The connection between the low-level perception task and the high-level reasoning task lies in the transformation of neural network outputs modeled by probability distributions (through softmax activation) into possibility distributions. The use of intermediate concepts is valuable for the explanation purpose: using the rule-based system, the classification of an input instance as an element of the (meta-)concept can be justified by the fact that intermediate concepts have been recognized.\n  From the technical side, our contribution consists of the design of efficient methods for defining the matrix relation and the equation system associated with a possibilistic rule-based system. The corresponding matrix and equation are key data structures used to perform inferences from a possibilistic rule-based system and to learn the values of the rule parameters in such a system according to a training data sample. Furthermore, leveraging recent results on the handling of inconsistent systems of fuzzy relational equations, an approach for learning rule parameters according to multiple training data samples is presented. Experiments carried out on the MNIST addition problems and the MNIST Sudoku puzzles problems highlight the effectiveness of our approach compared with state-of-the-art neuro-symbolic ones.\n        △ Less",
        "authors":"['Ismaïl Baaj', 'Pierre Marquis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning",
        "abstract":"Recent advancements in large language models have significantly improved their reasoning abilities, particularly through techniques involving search and backtracking. Backtracking naturally scales test-time compute by enabling sequential, linearized exploration via long chain-of-thought (CoT) generation. However, this is not the only strategy for scaling test-time compute: parallel sampling with best-of-n selection provides an alternative that generates diverse solutions simultaneously. Despite the growing adoption of sequential search, its advantages over parallel sampling--especially under a fixed compute budget remain poorly understood. In this paper, we systematically compare these two approaches on two challenging reasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential search underperforms parallel sampling on CountDown but outperforms it on Sudoku, suggesting that backtracking is not universally beneficial. We identify two factors that can cause backtracking to degrade performance: (1) training on fixed search traces can lock models into suboptimal strategies, and (2) explicit CoT supervision can discourage \"implicit\" (non-verbalized) reasoning. Extending our analysis to reinforcement learning (RL), we show that models with backtracking capabilities benefit significantly from RL fine-tuning, while models without backtracking see limited, mixed gains. Together, these findings challenge the assumption that backtracking universally enhances LLM reasoning, instead revealing a complex interaction between task structure, training data, model scale, and learning paradigm.\n        △ Less",
        "authors":"['Tian Qin', 'David Alvarez-Melis', 'Samy Jelassi', 'Eran Malach']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Identifying Key Challenges of Hardness-Based Resampling",
        "abstract":"Performance gap across classes remains a persistent challenge in machine learning, often attributed to variations in class hardness. One way to quantify class hardness is through sample complexity - the minimum number of samples required to effectively learn a given class. Sample complexity theory suggests that class hardness is driven by differences in the amount of data required for generalization. That is, harder classes need substantially more samples to achieve generalization. Therefore, hardness-based resampling is a promising approach to mitigate these performance disparities. While resampling has been studied extensively in data-imbalanced settings, its impact on balanced datasets remains unexplored.\n  This raises the fundamental question whether resampling is effective because it addresses data imbalance or hardness imbalance. We begin addressing this question by introducing class imbalance into balanced datasets and evaluate its effect on performance disparities. We oversample hard classes and undersample easy classes to bring hard classes closer to their sample complexity requirements while maintaining a constant dataset size for fairness. We estimate class-level hardness using the Area Under the Margin (AUM) hardness estimator and leverage it to compute resampling ratios. Using these ratios, we perform hardness-based resampling on the well-known CIFAR-10 and CIFAR-100 datasets.\n  Contrary to theoretical expectations, our results show that hardness-based resampling does not meaningfully affect class-wise performance disparities. To explain this discrepancy, we conduct detailed analyses to identify key challenges unique to hardness-based imbalance, distinguishing it from traditional data-based imbalance. Our insights help explain why theoretical sample complexity expectations fail to translate into practical performance gains and we provide guidelines for future research.\n        △ Less",
        "authors":"['Pawel Pukowski', 'Venet Osmani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using ML filters to help automated vulnerability repairs: when it helps and when it doesn't",
        "abstract":"[Context:] The acceptance of candidate patches in automated program repair has been typically based on testing oracles. Testing requires typically a costly process of building the application while ML models can be used to quickly classify patches, thus allowing more candidate patches to be generated in a positive feedback loop. [Problem:] If the model predictions are unreliable (as in vulnerability detection) they can hardly replace the more reliable oracles based on testing. [New Idea:] We propose to use an ML model as a preliminary filter of candidate patches which is put in front of a traditional filter based on testing. [Preliminary Results:] We identify some theoretical bounds on the precision and recall of the ML algorithm that makes such operation meaningful in practice. With these bounds and the results published in the literature, we calculate how fast some of state-of-the art vulnerability detectors must be to be more effective over a traditional AVR pipeline such as APR4Vuln based just on testing.\n        △ Less",
        "authors":"['Maria Camporese', 'Fabio Massacci']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adapting GT2-FLS for Uncertainty Quantification: A Blueprint Calibration Strategy",
        "abstract":"Uncertainty Quantification (UQ) is crucial for deploying reliable Deep Learning (DL) models in high-stakes applications. Recently, General Type-2 Fuzzy Logic Systems (GT2-FLSs) have been proven to be effective for UQ, offering Prediction Intervals (PIs) to capture uncertainty. However, existing methods often struggle with computational efficiency and adaptability, as generating PIs for new coverage levels $(φ_d)$ typically requires retraining the model. Moreover, methods that directly estimate the entire conditional distribution for UQ are computationally expensive, limiting their scalability in real-world scenarios. This study addresses these challenges by proposing a blueprint calibration strategy for GT2-FLSs, enabling efficient adaptation to any desired $φ_d$ without retraining. By exploring the relationship between $α$-plane type reduced sets and uncertainty coverage, we develop two calibration methods: a lookup table-based approach and a derivative-free optimization algorithm. These methods allow GT2-FLSs to produce accurate and reliable PIs while significantly reducing computational overhead. Experimental results on high-dimensional datasets demonstrate that the calibrated GT2-FLS achieves superior performance in UQ, highlighting its potential for scalable and practical applications.\n        △ Less",
        "authors":"['Yusuf Guven', 'Tufan Kumbasar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FAME: Introducing Fuzzy Additive Models for Explainable AI",
        "abstract":"In this study, we introduce the Fuzzy Additive Model (FAM) and FAM with Explainability (FAME) as a solution for Explainable Artificial Intelligence (XAI). The family consists of three layers: (1) a Projection Layer that compresses the input space, (2) a Fuzzy Layer built upon Single Input-Single Output Fuzzy Logic Systems (SFLS), where SFLS functions as subnetworks within an additive index model, and (3) an Aggregation Layer. This architecture integrates the interpretability of SFLS, which uses human-understandable if-then rules, with the explainability of input-output relationships, leveraging the additive model structure. Furthermore, using SFLS inherently addresses issues such as the curse of dimensionality and rule explosion. To further improve interpretability, we propose a method for sculpting antecedent space within FAM, transforming it into FAME. We show that FAME captures the input-output relationships with fewer active rules, thus improving clarity. To learn the FAM family, we present a deep learning framework. Through the presented comparative results, we demonstrate the promising potential of FAME in reducing model complexity while retaining interpretability, positioning it as a valuable tool for XAI.\n        △ Less",
        "authors":"['Omer Bahadir Gokmen', 'Yusuf Guven', 'Tufan Kumbasar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Assumption-free fidelity bounds for hardware noise characterization",
        "abstract":"In the Quantum Supremacy regime, quantum computers may overcome classical machines on several tasks if we can estimate, mitigate, or correct unavoidable hardware noise. Estimating the error requires classical simulations, which become unfeasible in the Quantum Supremacy regime. We leverage Machine Learning data-driven approaches and Conformal Prediction, a Machine Learning uncertainty quantification tool known for its mild assumptions and finite-sample validity, to find theoretically valid upper bounds of the fidelity between noiseless and noisy outputs of quantum devices. Under reasonable extrapolation assumptions, the proposed scheme applies to any Quantum Computing hardware, does not require modeling the device's noise sources, and can be used when classical simulations are unavailable, e.g. in the Quantum Supremacy regime.\n        △ Less",
        "authors":"['Nicolo Colombo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Signal Compression using RAMAN tinyML Accelerator for BCI Applications",
        "abstract":"High-quality, multi-channel neural recording is indispensable for neuroscience research and clinical applications. Large-scale brain recordings often produce vast amounts of data that must be wirelessly transmitted for subsequent offline analysis and decoding, especially in brain-computer interfaces (BCIs) utilizing high-density intracortical recordings with hundreds or thousands of electrodes. However, transmitting raw neural data presents significant challenges due to limited communication bandwidth and resultant excessive heating. To address this challenge, we propose a neural signal compression scheme utilizing Convolutional Autoencoders (CAEs), which achieves a compression ratio of up to 150 for compressing local field potentials (LFPs). The CAE encoder section is implemented on RAMAN, an energy-efficient tinyML accelerator designed for edge computing, and subsequently deployed on an Efinix Ti60 FPGA with 37.3k LUTs and 8.6k register utilization. RAMAN leverages sparsity in activation and weights through zero skipping, gating, and weight compression techniques. Additionally, we employ hardware-software co-optimization by pruning CAE encoder model parameters using a hardware-aware balanced stochastic pruning strategy, resolving workload imbalance issues and eliminating indexing overhead to reduce parameter storage requirements by up to 32.4%. Using the proposed compact depthwise separable convolutional autoencoder (DS-CAE) model, the compressed neural data from RAMAN is reconstructed offline with superior signal-to-noise and distortion ratios (SNDR) of 22.6 dB and 27.4 dB, along with R2 scores of 0.81 and 0.94, respectively, evaluated on two monkey neural recordings.\n        △ Less",
        "authors":"['Adithya Krishna', 'Sohan Debnath', 'André van Schaik', 'Mahesh Mehendale', 'Chetan Singh Thakur']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration",
        "abstract":"Open-set semantic mapping is crucial for open-world robots. Current mapping approaches either are limited by the depth range or only map beyond-range entities in constrained settings, where overall they fail to combine within-range and beyond-range observations. Furthermore, these methods make a trade-off between fine-grained semantics and efficiency. We introduce RayFronts, a unified representation that enables both dense and beyond-range efficient semantic mapping. RayFronts encodes task-agnostic open-set semantics to both in-range voxels and beyond-range rays encoded at map boundaries, empowering the robot to reduce search volumes significantly and make informed decisions both within & beyond sensory range, while running at 8.84 Hz on an Orin AGX. Benchmarking the within-range semantics shows that RayFronts's fine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation performance while improving throughput by 16.5x. Traditionally, online mapping performance is entangled with other system components, complicating evaluation. We propose a planner-agnostic evaluation framework that captures the utility for online beyond-range search and exploration, and show RayFronts reduces search volume 2.2x more efficiently than the closest online baselines.\n        △ Less",
        "authors":"['Omar Alama', 'Avigyan Bhattacharya', 'Haoyang He', 'Seungchan Kim', 'Yuheng Qiu', 'Wenshan Wang', 'Cherie Ho', 'Nikhil Keetha', 'Sebastian Scherer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dissimilar Batch Decompositions of Random Datasets",
        "abstract":"For better learning, large datasets are often split into small batches and fed sequentially to the predictive model. In this paper, we study such batch decompositions from a probabilistic perspective. We assume that data points (possibly corrupted) are drawn independently from a given space and define a concept of similarity between two data points. We then consider decompositions that restrict the amount of similarity within each batch and obtain high probability bounds for the minimum size. We demonstrate an inherent tradeoff between relaxing the similarity constraint and the overall size and also use martingale methods to obtain bounds for the maximum size of data subsets with a given similarity.\n        △ Less",
        "authors":"['Ghurumuruhan Ganesan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Metabolic Syndrome Prediction with Hybrid Data Balancing and Counterfactuals",
        "abstract":"Metabolic Syndrome (MetS) is a cluster of interrelated risk factors that significantly increases the risk of cardiovascular diseases and type 2 diabetes. Despite its global prevalence, accurate prediction of MetS remains challenging due to issues such as class imbalance, data scarcity, and methodological inconsistencies in existing studies. In this paper, we address these challenges by systematically evaluating and optimizing machine learning (ML) models for MetS prediction, leveraging advanced data balancing techniques and counterfactual analysis. Multiple ML models, including XGBoost, Random Forest, TabNet, etc., were trained and compared under various data balancing techniques such as random oversampling (ROS), SMOTE, ADASYN, and CTGAN. Additionally, we introduce MetaBoost, a novel hybrid framework that integrates SMOTE, ADASYN, and CTGAN, optimizing synthetic data generation through weighted averaging and iterative weight tuning to enhance the model's performance (achieving a 1.14% accuracy improvement over individual balancing techniques). A comprehensive counterfactual analysis is conducted to quantify feature-level changes required to shift individuals from high-risk to low-risk categories. The results indicate that blood glucose (50.3%) and triglycerides (46.7%) were the most frequently modified features, highlighting their clinical significance in MetS risk reduction. Additionally, probabilistic analysis shows elevated blood glucose (85.5% likelihood) and triglycerides (74.9% posterior probability) as the strongest predictors. This study not only advances the methodological rigor of MetS prediction but also provides actionable insights for clinicians and researchers, highlighting the potential of ML in mitigating the public health burden of metabolic syndrome.\n        △ Less",
        "authors":"['Sanyam Paresh Shah', 'Abdullah Mamun', 'Shovito Barua Soumma', 'Hassan Ghasemzadeh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Free Random Projection for In-Context Reinforcement Learning",
        "abstract":"Hierarchical inductive biases are hypothesized to promote generalizable policies in reinforcement learning, as demonstrated by explicit hyperbolic latent representations and architectures. Therefore, a more flexible approach is to have these biases emerge naturally from the algorithm. We introduce Free Random Projection, an input mapping grounded in free probability theory that constructs random orthogonal matrices where hierarchical structure arises inherently. The free random projection integrates seamlessly into existing in-context reinforcement learning frameworks by encoding hierarchical organization within the input space without requiring explicit architectural modifications. Empirical results on multi-environment benchmarks show that free random projection consistently outperforms the standard random projection, leading to improvements in generalization. Furthermore, analyses within linearly solvable Markov decision processes and investigations of the spectrum of kernel random matrices reveal the theoretical underpinnings of free random projection's enhanced performance, highlighting its capacity for effective adaptation in hierarchically structured state spaces.\n        △ Less",
        "authors":"['Tomohiro Hayase', 'Benoît Collins', 'Nakamasa Inoue']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Artificial Intelligence for Pediatric Height Prediction Using Large-Scale Longitudinal Body Composition Data",
        "abstract":"This study developed an accurate artificial intelligence model for predicting future height in children and adolescents using anthropometric and body composition data from the GP Cohort Study (588,546 measurements from 96,485 children aged 7-18). The model incorporated anthropometric measures, body composition, standard deviation scores, and growth velocity parameters, with performance evaluated using RMSE, MAE, and MAPE. Results showed high accuracy with males achieving average RMSE, MAE, and MAPE of 2.51 cm, 1.74 cm, and 1.14%, and females showing 2.28 cm, 1.68 cm, and 1.13%, respectively. Explainable AI approaches identified height SDS, height velocity, and soft lean mass velocity as crucial predictors. The model generated personalized growth curves by estimating individual-specific height trajectories, offering a robust tool for clinical decision support, early identification of growth disorders, and optimization of growth outcomes.\n        △ Less",
        "authors":"['Dohyun Chun', 'Hae Woon Jung', 'Jongho Kang', 'Woo Young Jang', 'Jihun Kim']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RNN-Transducer-based Losses for Speech Recognition on Noisy Targets",
        "abstract":"Training speech recognition systems on noisy transcripts is a significant challenge in industrial pipelines, where datasets are enormous and ensuring accurate transcription for every instance is difficult. In this work, we introduce novel loss functions to mitigate the impact of transcription errors in RNN-Transducer models. Our Star-Transducer loss addresses deletion errors by incorporating \"skip frame\" transitions in the loss lattice, restoring over 90% of the system's performance compared to models trained with accurate transcripts. The Bypass-Transducer loss uses \"skip token\" transitions to tackle insertion errors, recovering more than 60% of the quality. Finally, the Target-Robust Transducer loss merges these approaches, offering robust performance against arbitrary errors. Experimental results demonstrate that the Target-Robust Transducer loss significantly improves RNN-T performance on noisy data by restoring over 70% of the quality compared to well-transcribed data.\n        △ Less",
        "authors":"['Vladimir Bataev']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Computation Pruning for the Forgetting Transformer",
        "abstract":"The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https:\/\/github.com\/zhixuan-lin\/arctic-fox.\n        △ Less",
        "authors":"['Zhixuan Lin', 'Johan Obando-Ceron', 'Xu Owen He', 'Aaron Courville']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ASRL:A robust loss function with potential for development",
        "abstract":"In this article, we proposed a partition:wise robust loss function based on the previous robust loss function. The characteristics of this loss function are that it achieves high robustness and a wide range of applicability through partition-wise design and adaptive parameter adjustment. Finally, the advantages and development potential of this loss function were verified by applying this loss function to the regression question and using five different datasets (with different dimensions, different sample numbers, and different fields) to compare with the other loss functions. The results of multiple experiments have proven the advantages of our loss function .\n        △ Less",
        "authors":"['Chenyu Hui', 'Anran Zhang', 'Xintong Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RO-FIGS: Efficient and Expressive Tree-Based Ensembles for Tabular Data",
        "abstract":"Tree-based models are often robust to uninformative features and can accurately capture non-smooth, complex decision boundaries. Consequently, they often outperform neural network-based models on tabular datasets at a significantly lower computational cost. Nevertheless, the capability of traditional tree-based ensembles to express complex relationships efficiently is limited by using a single feature to make splits. To improve the efficiency and expressiveness of tree-based methods, we propose Random Oblique Fast Interpretable Greedy-Tree Sums (RO-FIGS). RO-FIGS builds on Fast Interpretable Greedy-Tree Sums, and extends it by learning trees with oblique or multivariate splits, where each split consists of a linear combination learnt from random subsets of features. This helps uncover interactions between features and improves performance. The proposed method is suitable for tabular datasets with both numerical and categorical features. We evaluate RO-FIGS on 22 real-world tabular datasets, demonstrating superior performance and much smaller models over other tree- and neural network-based methods. Additionally, we analyse their splits to reveal valuable insights into feature interactions, enriching the information learnt from SHAP summary plots, and thereby demonstrating the enhanced interpretability of RO-FIGS models. The proposed method is well-suited for applications, where balance between accuracy and interpretability is essential.\n        △ Less",
        "authors":"['Urška Matjašec', 'Nikola Simidjievski', 'Mateja Jamnik']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Importance of Being Discrete: Measuring the Impact of Discretization in End-to-End Differentially Private Synthetic Data",
        "abstract":"Differentially Private (DP) generative marginal models are often used in the wild to release synthetic tabular datasets in lieu of sensitive data while providing formal privacy guarantees. These models approximate low-dimensional marginals or query workloads; crucially, they require the training data to be pre-discretized, i.e., continuous values need to first be partitioned into bins. However, as the range of values (or their domain) is often inferred directly from the training data, with the number of bins and bin edges typically defined arbitrarily, this approach can ultimately break end-to-end DP guarantees and may not always yield optimal utility.\n  In this paper, we present an extensive measurement study of four discretization strategies in the context of DP marginal generative models. More precisely, we design DP versions of three discretizers (uniform, quantile, and k-means) and reimplement the PrivTree algorithm. We find that optimizing both the choice of discretizer and bin count can improve utility, on average, by almost 30% across six DP marginal models, compared to the default strategy and number of bins, with PrivTree being the best-performing discretizer in the majority of cases. We demonstrate that, while DP generative models with non-private discretization remain vulnerable to membership inference attacks, applying DP during discretization effectively mitigates this risk. Finally, we propose an optimized approach for automatically selecting the optimal number of bins, achieving high utility while reducing both privacy budget consumption and computational overhead.\n        △ Less",
        "authors":"['Georgi Ganev', 'Meenatchi Sundaram Muthu Selva Annamalai', 'Sofiane Mahiou', 'Emiliano De Cristofaro']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Analysis of Temporal Dropout in Earth Observation Time Series for Regression Tasks",
        "abstract":"Missing instances in time series data impose a significant challenge to deep learning models, particularly in regression tasks. In the Earth Observation field, satellite failure or cloud occlusion frequently results in missing time-steps, introducing uncertainties in the predicted output and causing a decline in predictive performance. While many studies address missing time-steps through data augmentation to improve model robustness, the uncertainty arising at the input level is commonly overlooked. To address this gap, we introduce Monte Carlo Temporal Dropout (MC-TD), a method that explicitly accounts for input-level uncertainty by randomly dropping time-steps during inference using a predefined dropout ratio, thereby simulating the effect of missing data. To bypass the need for costly searches for the optimal dropout ratio, we extend this approach with Monte Carlo Concrete Temporal Dropout (MC-ConcTD), a method that learns the optimal dropout distribution directly. Both MC-TD and MC-ConcTD are applied during inference, leveraging Monte Carlo sampling for uncertainty quantification. Experiments on three EO time-series datasets demonstrate that MC-ConcTD improves predictive performance and uncertainty calibration compared to existing approaches. Additionally, we highlight the advantages of adaptive dropout tuning over manual selection, making uncertainty quantification more robust and accessible for EO applications.\n        △ Less",
        "authors":"['Miro Miranda', 'Francisco Mena', 'Andreas Dengel']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation",
        "abstract":"In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at https:\/\/emmanuelleb985.github.io\/ukbob , and the filtered labels will be made available with the UK Biobank.\n        △ Less",
        "authors":"['Emmanuelle Bourigault', 'Amir Jamaludin', 'Abdullah Hamdi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs",
        "abstract":"This paper presents MedSegFactory, a versatile medical synthesis framework that generates high-quality paired medical images and segmentation masks across modalities and tasks. It aims to serve as an unlimited data repository, supplying image-mask pairs to enhance existing segmentation tools. The core of MedSegFactory is a dual-stream diffusion model, where one stream synthesizes medical images and the other generates corresponding segmentation masks. To ensure precise alignment between image-mask pairs, we introduce Joint Cross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic cross-conditioning between streams. This bidirectional interaction allows both representations to guide each other's generation, enhancing consistency between generated pairs. MedSegFactory unlocks on-demand generation of paired medical images and segmentation masks through user-defined prompts that specify the target labels, imaging modalities, anatomical regions, and pathological conditions, facilitating scalable and high-quality data generation. This new paradigm of medical image synthesis enables seamless integration into diverse medical imaging workflows, enhancing both efficiency and accuracy. Extensive experiments show that MedSegFactory generates data of superior quality and usability, achieving competitive or state-of-the-art performance in 2D and 3D segmentation tasks while addressing data scarcity and regulatory constraints.\n        △ Less",
        "authors":"['Jiawei Mao', 'Yuhan Wang', 'Yucheng Tang', 'Daguang Xu', 'Kang Wang', 'Yang Yang', 'Zongwei Zhou', 'Yuyin Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-Driven Consensus: Modeling Multi-Agent Networks with Long-Range Interactions through path-Laplacian Matrices",
        "abstract":"Extended connectivity in graphs can be analyzed through k-path Laplacian matrices, which permit the capture of long-range interactions in various real-world networked systems such as social, transportation, and multi-agent networks. In this work, we present several alternative methods based on machine learning methods (LSTM, xLSTM, Transformer, XGBoost, and ConvLSTM) to predict the final consensus value based on directed networks (Erdös-Renyi, Watts-Strogatz, and Barabási-Albert) and on the initial state. We highlight how different k-hop interactions affect the performance of the tested methods. This framework opens new avenues for analyzing multi-scale diffusion processes in large-scale, complex networks.\n        △ Less",
        "authors":"['Yusef Ahsini', 'Belén Reverte', 'J. Alberto Conejero']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CRYSIM: Prediction of Symmetric Structures of Large Crystals with GPU-based Ising Machines",
        "abstract":"Solving black-box optimization problems with Ising machines is increasingly common in materials science. However, their application to crystal structure prediction (CSP) is still ineffective due to symmetry agnostic encoding of atomic coordinates. We introduce CRYSIM, an algorithm that encodes the space group, the Wyckoff positions combination, and coordinates of independent atomic sites as separate variables. This encoding reduces the search space substantially by exploiting the symmetry in space groups. When CRYSIM is interfaced to Fixstars Amplify, a GPU-based Ising machine, its prediction performance was competitive with CALYPSO and Bayesian optimization for crystals containing more than 150 atoms in a unit cell. Although it is not realistic to interface CRYSIM to current small-scale quantum devices, it has the potential to become the standard CSP algorithm in the coming quantum age.\n        △ Less",
        "authors":"['Chen Liang', 'Diptesh Das', 'Jiang Guo', 'Ryo Tamura', 'Zetian Mao', 'Koji Tsuda']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Machine Learning (ML) based Reduced Order Modeling (ROM) for linear and non-linear solid and structural mechanics",
        "abstract":"Multiple model reduction techniques have been proposed to tackle linear and non linear problems. Intrusive model order reduction techniques exhibit high accuracy levels, however, they are rarely used as a standalone industrial tool, because of the required high level knowledge involved in the construction and usage of these techniques. Moreover, the computation time benefit is compromised for highly nonlinear problems. On the other hand, non-intrusive methods often struggle with accuracy in nonlinear cases, typically requiring a large design of experiment and a large number of snapshots achieve a reliable performance. However, generating the stiffness matrix in a non-intrusive approach presents an optimal way to align accuracy with efficiency, allying the advantages of both intrusive and non-intrusive methods.This work introduces a lightly intrusive model order reduction technique that employs machine learning within a Proper Orthogonal Decomposition framework to achieve this alliance. By leveraging outputs from commercial full-order models, this method constructs a reduced-order model that operates effectively without requiring expert user intervention. The proposed technique has the possibility to approximate linear non affine as well as non linear terms. It is showcased for linear and nonlinear structural mechanics problems.\n        △ Less",
        "authors":"['Mikhael Tannous', 'Chady Ghnatios', 'Eivind Fonn', 'Trond Kvamsdal', 'Francisco Chinesta']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models",
        "abstract":"Recent studies have introduced various approaches for prompt-tuning black-box vision-language models, referred to as black-box prompt-tuning (BBPT). While BBPT has demonstrated considerable potential, it is often found that many existing methods require an excessive number of queries (i.e., function evaluations), which poses a significant challenge in real-world scenarios where the number of allowed queries is limited. To tackle this issue, we propose Zeroth-order Intrinsic-dimensional Prompt-tuning (ZIP), a novel approach that enables efficient and robust prompt optimization in a purely black-box setting. The key idea of ZIP is to reduce the problem dimensionality and the variance of zeroth-order gradient estimates, such that the training is done fast with far less queries. We achieve this by re-parameterizing prompts in low-rank representations and designing intrinsic-dimensional clipping of estimated gradients. We evaluate ZIP on 13+ vision-language tasks in standard benchmarks and show that it achieves an average improvement of approximately 6% in few-shot accuracy and 48% in query efficiency compared to the best-performing alternative BBPT methods, establishing a new state of the art. Our ablation analysis further shows that the proposed clipping mechanism is robust and nearly optimal, without the need to manually select the clipping threshold, matching the result of expensive hyperparameter search.\n        △ Less",
        "authors":"['Seonghwan Park', 'Jaehyeon Jeong', 'Yongjun Kim', 'Jaeho Lee', 'Namhoon Lee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Locally Linear Embedding",
        "abstract":"Manifold learning techniques, such as Locally linear embedding (LLE), are designed to preserve the local neighborhood structures of high-dimensional data during dimensionality reduction. Traditional LLE employs Euclidean distance to define neighborhoods, which can struggle to capture the intrinsic geometric relationships within complex data. A novel approach, Adaptive locally linear embedding(ALLE), is introduced to address this limitation by incorporating a dynamic, data-driven metric that enhances topological preservation. This method redefines the concept of proximity by focusing on topological neighborhood inclusion rather than fixed distances. By adapting the metric based on the local structure of the data, it achieves superior neighborhood preservation, particularly for datasets with complex geometries and high-dimensional structures. Experimental results demonstrate that ALLE significantly improves the alignment between neighborhoods in the input and feature spaces, resulting in more accurate and topologically faithful embeddings. This approach advances manifold learning by tailoring distance metrics to the underlying data, providing a robust solution for capturing intricate relationships in high-dimensional datasets.\n        △ Less",
        "authors":"['Ali Goli', 'Mahdieh Alizadeh', 'Hadi Sadoghi Yazdi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Regret Bounds for Robust Online Decision Making",
        "abstract":"We propose a framework which generalizes \"decision making with structured observations\" by allowing robust (i.e. multivalued) models. In this framework, each model associates each decision with a convex set of probability distributions over outcomes. Nature can choose distributions out of this set in an arbitrary (adversarial) manner, that can be nonoblivious and depend on past history. The resulting framework offers much greater generality than classical bandits and reinforcement learning, since the realizability assumption becomes much weaker and more realistic. We then derive a theory of regret bounds for this framework. Although our lower and upper bounds are not tight, they are sufficient to fully characterize power-law learnability. We demonstrate this theory in two special cases: robust linear bandits and tabular robust online reinforcement learning. In both cases, we derive regret bounds that improve state-of-the-art (except that we do not address computational efficiency).\n        △ Less",
        "authors":"['Alexander Appel', 'Vanessa Kosoy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Neural Koopman Operator-based Economic Model Predictive Control of Shipboard Carbon Capture System",
        "abstract":"Shipboard carbon capture is a promising solution to help reduce carbon emissions in international shipping. In this work, we propose a data-driven dynamic modeling and economic predictive control approach within the Koopman framework. This integrated modeling and control approach is used to achieve safe and energy-efficient process operation of shipboard post-combustion carbon capture plants. Specifically, we propose a deep neural Koopman operator modeling approach, based on which a Koopman model with time-varying model parameters is established. This Koopman model predicts the overall economic operational cost and key system outputs, based on accessible partial state measurements. By leveraging this learned model, a constrained economic predictive control scheme is developed. Despite time-varying parameters involved in the formulated model, the formulated optimization problem associated with the economic predictive control design is convex, and it can be solved efficiently during online control implementations. Extensive tests are conducted on a high-fidelity simulation environment for shipboard post-combustion carbon capture processes. Four ship operational conditions are taken into account. The results show that the proposed method significantly improves the overall economic operational performance and carbon capture rate. Additionally, the proposed method guarantees safe operation by ensuring that hard constraints on the system outputs are satisfied.\n        △ Less",
        "authors":"['Minghao Han', 'Xunyuan Yin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hybrid CNN with Chebyshev Polynomial Expansion for Medical Image Analysis",
        "abstract":"Lung cancer remains one of the leading causes of cancer-related mortality worldwide, with early and accurate diagnosis playing a pivotal role in improving patient outcomes. Automated detection of pulmonary nodules in computed tomography (CT) scans is a challenging task due to variability in nodule size, shape, texture, and location. Traditional Convolutional Neural Networks (CNNs) have shown considerable promise in medical image analysis; however, their limited ability to capture fine-grained spatial-spectral variations restricts their performance in complex diagnostic scenarios. In this study, we propose a novel hybrid deep learning architecture that incorporates Chebyshev polynomial expansions into CNN layers to enhance expressive power and improve the representation of underlying anatomical structures. The proposed Chebyshev-CNN leverages the orthogonality and recursive properties of Chebyshev polynomials to extract high-frequency features and approximate complex nonlinear functions with greater fidelity. The model is trained and evaluated on benchmark lung cancer imaging datasets, including LUNA16 and LIDC-IDRI, achieving superior performance in classifying pulmonary nodules as benign or malignant. Quantitative results demonstrate significant improvements in accuracy, sensitivity, and specificity compared to traditional CNN-based approaches. This integration of polynomial-based spectral approximation within deep learning provides a robust framework for enhancing automated medical diagnostics and holds potential for broader applications in clinical decision support systems.\n        △ Less",
        "authors":"['Abhinav Roy', 'Bhavesh Gyanchandani', 'Aditya Oza']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mass Balance Approximation of Unfolding Improves Potential-Like Methods for Protein Stability Predictions",
        "abstract":"The prediction of protein stability changes following single-point mutations plays a pivotal role in computational biology, particularly in areas like drug discovery, enzyme reengineering, and genetic disease analysis. Although deep-learning strategies have pushed the field forward, their use in standard workflows remains limited due to resource demands. Conversely, potential-like methods are fast, intuitive, and efficient. Yet, these typically estimate Gibbs free energy shifts without considering the free-energy variations in the unfolded protein state, an omission that may breach mass balance and diminish accuracy. This study shows that incorporating a mass-balance correction (MBC) to account for the unfolded state significantly enhances these methods. While many machine learning models partially model this balance, our analysis suggests that a refined representation of the unfolded state may improve the predictive performance.\n        △ Less",
        "authors":"['Ivan Rossi', 'Guido Barducci', 'Tiziana Sanavia', 'Paola Turina', 'Emidio Capriotti', 'Piero Fariselli']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust Classification with Noisy Labels Based on Posterior Maximization",
        "abstract":"Designing objective functions robust to label noise is crucial for real-world classification algorithms. In this paper, we investigate the robustness to label noise of an $f$-divergence-based class of objective functions recently proposed for supervised classification, herein referred to as $f$-PML. We show that, in the presence of label noise, any of the $f$-PML objective functions can be corrected to obtain a neural network that is equal to the one learned with the clean dataset. Additionally, we propose an alternative and novel correction approach that, during the test phase, refines the posterior estimated by the neural network trained in the presence of label noise. Then, we demonstrate that, even if the considered $f$-PML objective functions are not symmetric, they are robust to symmetric label noise for any choice of $f$-divergence, without the need for any correction approach. This allows us to prove that the cross-entropy, which belongs to the $f$-PML class, is robust to symmetric label noise. Finally, we show that such a class of objective functions can be used together with refined training strategies, achieving competitive performance against state-of-the-art techniques of classification with label noise.\n        △ Less",
        "authors":"['Nicola Novello', 'Andrea M. Tonello']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations",
        "abstract":"Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1 (671B). In this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term few-shot expert localization, with only a few demonstrations, the model consistently activates a sparse and stable subset of experts. Building on this observation, we propose a simple yet effective pruning framework, EASY-EP, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts. EASY-EP comprises two key components: output-aware expert importance assessment and expert-level token contribution estimation. The former evaluates the importance of each expert for the current token by considering the gating scores and magnitudes of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities after and before routed experts. Experiments show that our method can achieve comparable performances and $2.99\\times$ throughput under the same memory budget with full DeepSeek-R1 with only half the experts. Our code is available at https:\/\/github.com\/RUCAIBox\/EASYEP.\n        △ Less",
        "authors":"['Zican Dong', 'Han Peng', 'Peiyu Liu', 'Wayne Xin Zhao', 'Dong Wu', 'Feng Xiao', 'Zhifeng Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beware of \"Explanations\" of AI",
        "abstract":"Understanding the decisions made and actions taken by increasingly complex AI system remains a key challenge. This has led to an expanding field of research in explainable artificial intelligence (XAI), highlighting the potential of explanations to enhance trust, support adoption, and meet regulatory standards. However, the question of what constitutes a \"good\" explanation is dependent on the goals, stakeholders, and context. At a high level, psychological insights such as the concept of mental model alignment can offer guidance, but success in practice is challenging due to social and technical factors. As a result of this ill-defined nature of the problem, explanations can be of poor quality (e.g. unfaithful, irrelevant, or incoherent), potentially leading to substantial risks. Instead of fostering trust and safety, poorly designed explanations can actually cause harm, including wrong decisions, privacy violations, manipulation, and even reduced AI adoption. Therefore, we caution stakeholders to beware of explanations of AI: while they can be vital, they are not automatically a remedy for transparency or responsible AI adoption, and their misuse or limitations can exacerbate harm. Attention to these caveats can help guide future research to improve the quality and impact of AI explanations.\n        △ Less",
        "authors":"['David Martens', 'Galit Shmueli', 'Theodoros Evgeniou', 'Kevin Bauer', 'Christian Janiesch', 'Stefan Feuerriegel', 'Sebastian Gabel', 'Sofie Goethals', 'Travis Greene', 'Nadja Klein', 'Mathias Kraus', 'Niklas Kühl', 'Claudia Perlich', 'Wouter Verbeke', 'Alona Zharova', 'Patrick Zschech', 'Foster Provost']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring",
        "abstract":"Effective and rapid evaluation of pavement surface condition is critical for prioritizing maintenance, ensuring transportation safety, and minimizing vehicle wear and tear. While conventional manual inspections suffer from subjectivity, existing machine learning-based methods are constrained by their reliance on large and high-quality labeled datasets, which require significant resources and limit adaptability across varied road conditions. The revolutionary advancements in Large Language Models (LLMs) present significant potential for overcoming these challenges. In this study, we propose an innovative automated zero-shot learning approach that leverages the image recognition and natural language understanding capabilities of LLMs to assess road conditions effectively. Multiple LLM-based assessment models were developed, employing prompt engineering strategies aligned with the Pavement Surface Condition Index (PSCI) standards. These models' accuracy and reliability were evaluated against official PSCI results, with an optimized model ultimately selected. Extensive tests benchmarked the optimized model against evaluations from various levels experts using Google Street View road images. The results reveal that the LLM-based approach can effectively assess road conditions, with the optimized model -employing comprehensive and structured prompt engineering strategies -outperforming simpler configurations by achieving high accuracy and consistency, even surpassing expert evaluations. Moreover, successfully applying the optimized model to Google Street View images demonstrates its potential for future city-scale deployments. These findings highlight the transformative potential of LLMs in automating road damage evaluations and underscore the pivotal role of detailed prompt engineering in achieving reliable assessments.\n        △ Less",
        "authors":"['Shuoshuo Xu', 'Kai Zhao', 'James Loney', 'Zili Li', 'Andrea Visentin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hybrid machine learning models based on physical patterns to accelerate CFD simulations: a short guide on autoregressive models",
        "abstract":"Accurate modeling of the complex dynamics of fluid flows is a fundamental challenge in computational physics and engineering. This study presents an innovative integration of High-Order Singular Value Decomposition (HOSVD) with Long Short-Term Memory (LSTM) architectures to address the complexities of reduced-order modeling (ROM) in fluid dynamics. HOSVD improves the dimensionality reduction process by preserving multidimensional structures, surpassing the limitations of Singular Value Decomposition (SVD). The methodology is tested across numerical and experimental data sets, including two- and three-dimensional (2D and 3D) cylinder wake flows, spanning both laminar and turbulent regimes. The emphasis is also on exploring how the depth and complexity of LSTM architectures contribute to improving predictive performance. Simpler architectures with a single dense layer effectively capture the periodic dynamics, demonstrating the network's ability to model non-linearities and chaotic dynamics. The addition of extra layers provides higher accuracy at minimal computational cost. These additional layers enable the network to expand its representational capacity, improving the prediction accuracy and reliability. The results demonstrate that HOSVD outperforms SVD in all tested scenarios, as evidenced by using different error metrics. Efficient mode truncation by HOSVD-based models enables the capture of complex temporal patterns, offering reliable predictions even in challenging, noise-influenced data sets. The findings underscore the adaptability and robustness of HOSVD-LSTM architectures, offering a scalable framework for modeling fluid dynamics.\n        △ Less",
        "authors":"['Arindam Sengupta', 'Rodrigo Abadía-Heredia', 'Ashton Hetherington', 'José Miguel Pérez', 'Soledad Le Clainche']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FedMerge: Federated Personalization via Model Merging",
        "abstract":"One global model in federated learning (FL) might not be sufficient to serve many clients with non-IID tasks and distributions. While there has been advances in FL to train multiple global models for better personalization, they only provide limited choices to clients so local finetuning is still indispensable. In this paper, we propose a novel ``FedMerge'' approach that can create a personalized model per client by simply merging multiple global models with automatically optimized and customized weights. In FedMerge, a few global models can serve many non-IID clients, even without further local finetuning. We formulate this problem as a joint optimization of global models and the merging weights for each client. Unlike existing FL approaches where the server broadcasts one or multiple global models to all clients, the server only needs to send a customized, merged model to each client. Moreover, instead of periodically interrupting the local training and re-initializing it to a global model, the merged model aligns better with each client's task and data distribution, smoothening the local-global gap between consecutive rounds caused by client drift. We evaluate FedMerge on three different non-IID settings applied to different domains with diverse tasks and data types, in which FedMerge consistently outperforms existing FL approaches, including clustering-based and mixture-of-experts (MoE) based methods.\n        △ Less",
        "authors":"['Shutong Chen', 'Tianyi Zhou', 'Guodong Long', 'Jing Jiang', 'Chengqi Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Deployment of Spiking Neural Networks on SpiNNaker2 for DVS Gesture Recognition Using Neuromorphic Intermediate Representation",
        "abstract":"Spiking Neural Networks (SNNs) are highly energy-efficient during inference, making them particularly suitable for deployment on neuromorphic hardware. Their ability to process event-driven inputs, such as data from dynamic vision sensors (DVS), further enhances their applicability to edge computing tasks. However, the resource constraints of edge hardware necessitate techniques like weight quantization, which reduce the memory footprint of SNNs while preserving accuracy. Despite its importance, existing quantization methods typically focus on synaptic weights quantization without taking account of other critical parameters, such as scaling neuron firing thresholds.\n  To address this limitation, we present the first benchmark for the DVS gesture recognition task using SNNs optimized for the many-core neuromorphic chip SpiNNaker2. Our study evaluates two quantization pipelines for fixed-point computations. The first approach employs post training quantization (PTQ) with percentile-based threshold scaling, while the second uses quantization aware training (QAT) with adaptive threshold scaling. Both methods achieve accurate 8-bit on-chip inference, closely approximating 32-bit floating-point performance. Additionally, our baseline SNNs perform competitively against previously reported results without specialized techniques. These models are deployed on SpiNNaker2 using the neuromorphic intermediate representation (NIR). Ultimately, we achieve 94.13% classification accuracy on-chip, demonstrating the SpiNNaker2's potential for efficient, low-energy neuromorphic computing.\n        △ Less",
        "authors":"['Sirine Arfa', 'Bernhard Vogginger', 'Chen Liu', 'Johannes Partzsch', 'Mark Schone', 'Christian Mayr']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PETNet -- Coincident Particle Event Detection using Spiking Neural Networks",
        "abstract":"Spiking neural networks (SNN) hold the promise of being a more biologically plausible, low-energy alternative to conventional artificial neural networks. Their time-variant nature makes them particularly suitable for processing time-resolved, sparse binary data. In this paper, we investigate the potential of leveraging SNNs for the detection of photon coincidences in positron emission tomography (PET) data. PET is a medical imaging technique based on injecting a patient with a radioactive tracer and detecting the emitted photons. One central post-processing task for inferring an image of the tracer distribution is the filtering of invalid hits occurring due to e.g. absorption or scattering processes. Our approach, coined PETNet, interprets the detector hits as a binary-valued spike train and learns to identify photon coincidence pairs in a supervised manner. We introduce a dedicated multi-objective loss function and demonstrate the effects of explicitly modeling the detector geometry on simulation data for two use-cases. Our results show that PETNet can outperform the state-of-the-art classical algorithm with a maximal coincidence detection $F_1$ of 95.2%. At the same time, PETNet is able to predict photon coincidences up to 36 times faster than the classical approach, highlighting the great potential of SNNs in particle physics applications.\n        △ Less",
        "authors":"['Jan Debus', 'Charlotte Debus', 'Günther Dissertori', 'Markus Götz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Plastic tensor networks for interpretable generative modeling",
        "abstract":"A structural optimization scheme for a single-layer nonnegative adaptive tensor tree (NATT) that models a target probability distribution is proposed. The NATT scheme, by construction, has the advantage that it is interpretable as a probabilistic graphical model. We consider the NATT scheme and a recently proposed Born machine adaptive tensor tree (BMATT) optimization scheme and demonstrate their effectiveness on a variety of generative modeling tasks where the objective is to infer the hidden structure of a provided dataset. Our results show that in terms of minimizing the negative log-likelihood, the single-layer scheme has model performance comparable to the Born machine scheme, though not better. The tasks include deducing the structure of binary bitwise operations, learning the internal structure of random Bayesian networks given only visible sites, and a real-world example related to hierarchical clustering where a cladogram is constructed from mitochondrial DNA sequences. In doing so, we also show the importance of the choice of network topology and the versatility of a least-mutual information criterion in selecting a candidate structure for a tensor tree, as well as discuss aspects of these tensor tree generative models including their information content and interpretability.\n        △ Less",
        "authors":"['Katsuya O. Akamatsu', 'Kenji Harada', 'Tsuyoshi Okubo', 'Naoki Kawashima']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning global control of underactuated systems with Model-Based Reinforcement Learning",
        "abstract":"This short paper describes our proposed solution for the third edition of the \"AI Olympics with RealAIGym\" competition, held at ICRA 2025. We employed Monte-Carlo Probabilistic Inference for Learning Control (MC-PILCO), an MBRL algorithm recognized for its exceptional data efficiency across various low-dimensional robotic tasks, including cart-pole, ball \\& plate, and Furuta pendulum systems. MC-PILCO optimizes a system dynamics model using interaction data, enabling policy refinement through simulation rather than direct system data optimization. This approach has proven highly effective in physical systems, offering greater data efficiency than Model-Free (MF) alternatives. Notably, MC-PILCO has previously won the first two editions of this competition, demonstrating its robustness in both simulated and real-world environments. Besides briefly reviewing the algorithm, we discuss the most critical aspects of the MC-PILCO implementation in the tasks at hand: learning a global policy for the pendubot and acrobot systems.\n        △ Less",
        "authors":"['Niccolò Turcato', 'Marco Calì', 'Alberto Dalla Libera', 'Giulio Giacomuzzo', 'Ruggero Carli', 'Diego Romeres']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Large-Scale (Semi-)Automated Security Assessment of Consumer IoT Devices -- A Roadmap",
        "abstract":"The Internet of Things (IoT) has rapidly expanded across various sectors, with consumer IoT devices - such as smart thermostats and security cameras - experiencing growth. Although these devices improve efficiency and promise additional comfort, they also introduce new security challenges. Common and easy-to-explore vulnerabilities make IoT devices prime targets for malicious actors. Upcoming mandatory security certifications offer a promising way to mitigate these risks by enforcing best practices and providing transparency. Regulatory bodies are developing IoT security frameworks, but a universal standard for large-scale systematic security assessment is lacking. Existing manual testing approaches are expensive, limiting their efficacy in the diverse and rapidly evolving IoT domain. This paper reviews current IoT security challenges and assessment efforts, identifies gaps, and proposes a roadmap for scalable, automated security assessment, leveraging a model-based testing approach and machine learning techniques to strengthen consumer IoT security.\n        △ Less",
        "authors":"['Pascal Schöttle', 'Matthias Janetschek', 'Florian Merkle', 'Martin Nocker', 'Christoph Egger']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Clustering and novel class recognition: evaluating bioacoustic deep learning feature extractors",
        "abstract":"In computational bioacoustics, deep learning models are composed of feature extractors and classifiers. The feature extractors generate vector representations of the input sound segments, called embeddings, which can be input to a classifier. While benchmarking of classification scores provides insights into specific performance statistics, it is limited to species that are included in the models' training data. Furthermore, it makes it impossible to compare models trained on very different taxonomic groups. This paper aims to address this gap by analyzing the embeddings generated by the feature extractors of 15 bioacoustic models spanning a wide range of setups (model architectures, training data, training paradigms). We evaluated and compared different ways in which models structure embedding spaces through clustering and kNN classification, which allows us to focus our comparison on feature extractors independent of their classifiers. We believe that this approach lets us evaluate the adaptability and generalization potential of models going beyond the classes they were trained on.\n        △ Less",
        "authors":"['Vincent S. Kather', 'Burooj Ghani', 'Dan Stowell']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers",
        "abstract":"Transformers have driven remarkable breakthroughs in natural language processing and computer vision, yet their standard attention mechanism still imposes O(N^2) complexity, hindering scalability to longer sequences. We introduce Circular-convolutional ATtention (CAT), a Fourier-based approach that efficiently applies circular convolutions to reduce complexity without sacrificing representational power. CAT achieves O(NlogN) computations, requires fewer learnable parameters by streamlining fully-connected layers, and introduces no heavier operations, resulting in consistent accuracy improvements and about a 10% speedup in naive PyTorch implementations on large-scale benchmarks such as ImageNet-1k and WikiText-103. Grounded in an engineering-isomorphism framework, CAT's design not only offers practical efficiency and ease of implementation but also provides insights to guide the development of next-generation, high-performance Transformer architectures. Finally, our ablation studies highlight the key conditions underlying CAT's success, shedding light on broader principles for scalable attention mechanisms.\n        △ Less",
        "authors":"['Yoshihiro Yamada']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Benchmarking Convolutional Neural Network and Graph Neural Network based Surrogate Models on a Real-World Car External Aerodynamics Dataset",
        "abstract":"Aerodynamic optimization is crucial for developing eco-friendly, aerodynamic, and stylish cars, which requires close collaboration between aerodynamicists and stylists, a collaboration impaired by the time-consuming nature of aerodynamic simulations. Surrogate models offer a viable solution to reduce this overhead, but they are untested in real-world aerodynamic datasets. We present a comparative evaluation of two surrogate modeling approaches for predicting drag on a real-world dataset: a Convolutional Neural Network (CNN) model that uses a signed distance field as input and a commercial tool based on Graph Neural Networks (GNN) that directly processes a surface mesh. In contrast to previous studies based on datasets created from parameterized geometries, our dataset comprises 343 geometries derived from 32 baseline vehicle geometries across five distinct car projects, reflecting the diverse, free-form modifications encountered in the typical vehicle development process. Our results show that the CNN-based method achieves a mean absolute error of 2.3 drag counts, while the GNN-based method achieves 3.8. Both methods achieve approximately 77% accuracy in predicting the direction of drag change relative to the baseline geometry. While both methods effectively capture the broader trends between baseline groups (set of samples derived from a single baseline geometry), they struggle to varying extents in capturing the finer intra-baseline group variations. In summary, our findings suggest that aerodynamicists can effectively use both methods to predict drag in under two minutes, which is at least 600 times faster than performing a simulation. However, there remains room for improvement in capturing the finer details of the geometry.\n        △ Less",
        "authors":"['Sam Jacob Jacob', 'Markus Mrosek', 'Carsten Othmer', 'Harald Köstler']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning",
        "abstract":"Hyperparameter optimisation (HPO) is crucial for achieving strong performance in reinforcement learning (RL), as RL algorithms are inherently sensitive to hyperparameter settings. Probabilistic Curriculum Learning (PCL) is a curriculum learning strategy designed to improve RL performance by structuring the agent's learning process, yet effective hyperparameter tuning remains challenging and computationally demanding. In this paper, we provide an empirical analysis of hyperparameter interactions and their effects on the performance of a PCL algorithm within standard RL tasks, including point-maze navigation and DC motor control. Using the AlgOS framework integrated with Optuna's Tree-Structured Parzen Estimator (TPE), we present strategies to refine hyperparameter search spaces, enhancing optimisation efficiency. Additionally, we introduce a novel SHAP-based interpretability approach tailored specifically for analysing hyperparameter impacts, offering clear insights into how individual hyperparameters and their interactions influence RL performance. Our work contributes practical guidelines and interpretability tools that significantly improve the effectiveness and computational feasibility of hyperparameter optimisation in reinforcement learning.\n        △ Less",
        "authors":"['Llewyn Salt', 'Marcus Gallagher']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Learning for Cardiovascular Risk Assessment: Proxy Features from Carotid Sonography as Predictors of Arterial Damage",
        "abstract":"In this study, hypertension is utilized as an indicator of individual vascular damage. This damage can be identified through machine learning techniques, providing an early risk marker for potential major cardiovascular events and offering valuable insights into the overall arterial condition of individual patients. To this end, the VideoMAE deep learning model, originally developed for video classification, was adapted by finetuning for application in the domain of ultrasound imaging. The model was trained and tested using a dataset comprising over 31,000 carotid sonography videos sourced from the Gutenberg Health Study (15,010 participants), one of the largest prospective population health studies. This adaptation facilitates the classification of individuals as hypertensive or non-hypertensive (75.7% validation accuracy), functioning as a proxy for detecting visual arterial damage. We demonstrate that our machine learning model effectively captures visual features that provide valuable insights into an individual's overall cardiovascular health.\n        △ Less",
        "authors":"['Christoph Balada', 'Aida Romano-Martinez', 'Vincent ten Cate', 'Katharina Geschke', 'Jonas Tesarz', 'Paul Claßen', 'Alexander K. Schuster', 'Dativa Tibyampansha', 'Karl-Patrik Kresoja', 'Philipp S. Wild', 'Sheraz Ahmed', 'Andreas Dengel']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SEE: Continual Fine-tuning with Sequential Ensemble of Experts",
        "abstract":"Continual fine-tuning of large language models (LLMs) suffers from catastrophic forgetting. Rehearsal-based methods mitigate this problem by retaining a small set of old data. Nevertheless, they still suffer inevitable performance loss. Although training separate experts for each task can help prevent forgetting, effectively assembling them remains a challenge. Some approaches use routers to assign tasks to experts, but in continual learning, they often require retraining for optimal performance. To address these challenges, we introduce the Sequential Ensemble of Experts (SEE) framework. SEE removes the need for an additional router, allowing each expert to independently decide whether a query should be handled. The framework employs distributed routing, and during continual fine-tuning, SEE only requires the training of new experts for incoming tasks rather than retraining the entire system. Experiments reveal that the SEE outperforms prior approaches, including multi-task learning, in continual fine-tuning. It also demonstrates remarkable generalization ability, as the expert can effectively identify out-of-distribution queries, which can then be directed to a more generalized model for resolution. This work highlights the promising potential of integrating routing and response mechanisms within each expert, paving the way for the future of distributed model ensembling.\n        △ Less",
        "authors":"['Zhilin Wang', 'Yafu Li', 'Xiaoye Qu', 'Yu Cheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust and Noise-resilient Long-Term Prediction of Spatiotemporal Data Using Variational Mode Graph Neural Networks with 3D Attention",
        "abstract":"This paper focuses on improving the robustness of spatiotemporal long-term prediction using a variational mode graph convolutional network (VMGCN) by introducing 3D channel attention. The deep learning network for this task relies on historical data inputs, yet real-time data can be corrupted by sensor noise, altering its distribution. We model this noise as independent and identically distributed (i.i.d.) Gaussian noise and incorporate it into the LargeST traffic volume dataset, resulting in data with both inherent and additive noise components. Our approach involves decomposing the corrupted signal into modes using variational mode decomposition, followed by feeding the data into a learning pipeline for prediction. We integrate a 3D attention mechanism encompassing spatial, temporal, and channel attention. The spatial and temporal attention modules learn their respective correlations, while the channel attention mechanism is used to suppress noise and highlight the significant modes in the spatiotemporal signals. Additionally, a learnable soft thresholding method is implemented to exclude unimportant modes from the feature vector, and a feature reduction method based on the signal-to-noise ratio (SNR) is applied. We compare the performance of our approach against baseline models, demonstrating that our method achieves superior long-term prediction accuracy, robustness to noise, and improved performance with mode truncation compared to the baseline models. The code of the paper is available at https:\/\/github.com\/OsamaAhmad369\/VMGCN.\n        △ Less",
        "authors":"['Osama Ahmad', 'Zubair Khalid']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bridging the Gap Between Preference Alignment and Machine Unlearning",
        "abstract":"Despite advances in Preference Alignment (PA) for Large Language Models (LLMs), mainstream methods like Reinforcement Learning with Human Feedback (RLHF) face notable challenges. These approaches require high-quality datasets of positive preference examples, which are costly to obtain and computationally intensive due to training instability, limiting their use in low-resource scenarios. LLM unlearning technique presents a promising alternative, by directly removing the influence of negative examples. However, current research has primarily focused on empirical validation, lacking systematic quantitative analysis. To bridge this gap, we propose a framework to explore the relationship between PA and LLM unlearning. Specifically, we introduce a bi-level optimization-based method to quantify the impact of unlearning specific negative examples on PA performance. Our analysis reveals that not all negative examples contribute equally to alignment improvement when unlearned, and the effect varies significantly across examples. Building on this insight, we pose a crucial question: how can we optimally select and weight negative examples for unlearning to maximize PA performance? To answer this, we propose a framework called Unlearning to Align (U2A), which leverages bi-level optimization to efficiently select and unlearn examples for optimal PA performance. We validate the proposed method through extensive experiments, with results confirming its effectiveness.\n        △ Less",
        "authors":"['Xiaohua Feng', 'Yuyuan Li', 'Huwei Ji', 'Jiaming Zhang', 'Li Zhang', 'Tianyu Du', 'Chaochao Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty",
        "abstract":"Driven by privacy protection laws and regulations, unlearning in Large Language Models (LLMs) is gaining increasing attention. However, current research often neglects the interpretability of the unlearning process, particularly concerning sample-level unlearning difficulty. Existing studies typically assume a uniform unlearning difficulty across samples. This simplification risks attributing the performance of unlearning algorithms to sample selection rather than the algorithm's design, potentially steering the development of LLM unlearning in the wrong direction. Thus, we investigate the relationship between LLM unlearning and sample characteristics, with a focus on unlearning difficulty. Drawing inspiration from neuroscience, we propose a Memory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level unlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of hard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an $\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning algorithms, which prioritizes easily forgettable samples, thereby improving unlearning efficiency and effectiveness. We validate the proposed metric and method using public benchmarks and datasets, with results confirming its effectiveness.\n        △ Less",
        "authors":"['Xiaohua Feng', 'Yuyuan Li', 'Chengye Wang', 'Junlin Liu', 'Li Zhang', 'Chaochao Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GRAIN: Multi-Granular and Implicit Information Aggregation Graph Neural Network for Heterophilous Graphs",
        "abstract":"Graph neural networks (GNNs) have shown significant success in learning graph representations. However, recent studies reveal that GNNs often fail to outperform simple MLPs on heterophilous graph tasks, where connected nodes may differ in features or labels, challenging the homophily assumption. Existing methods addressing this issue often overlook the importance of information granularity and rarely consider implicit relationships between distant nodes. To overcome these limitations, we propose the Granular and Implicit Graph Network (GRAIN), a novel GNN model specifically designed for heterophilous graphs. GRAIN enhances node embeddings by aggregating multi-view information at various granularity levels and incorporating implicit data from distant, non-neighboring nodes. This approach effectively integrates local and global information, resulting in smoother, more accurate node representations. We also introduce an adaptive graph information aggregator that efficiently combines multi-granularity and implicit data, significantly improving node representation quality, as shown by experiments on 13 datasets covering varying homophily and heterophily. GRAIN consistently outperforms 12 state-of-the-art models, excelling on both homophilous and heterophilous graphs.\n        △ Less",
        "authors":"['Songwei Zhao', 'Yuan Jiang', 'Zijing Zhang', 'Yang Yu', 'Hechang Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection",
        "abstract":"Unsupervised multivariate time series anomaly detection (UMTSAD) plays a critical role in various domains, including finance, networks, and sensor systems. In recent years, due to the outstanding performance of deep learning in general sequential tasks, many models have been specialized for deep UMTSAD tasks and have achieved impressive results, particularly those based on the Transformer and self-attention mechanisms. However, the sequence anomaly association assumptions underlying these models are often limited to specific predefined patterns and scenarios, such as concentrated or peak anomaly patterns. These limitations hinder their ability to generalize to diverse anomaly situations, especially where the lack of labels poses significant challenges. To address these issues, we propose AMAD, which integrates \\textbf{A}uto\\textbf{M}asked Attention for UMTS\\textbf{AD} scenarios. AMAD introduces a novel structure based on the AutoMask mechanism and an attention mixup module, forming a simple yet generalized anomaly association representation framework. This framework is further enhanced by a Max-Min training strategy and a Local-Global contrastive learning approach. By combining multi-scale feature extraction with automatic relative association modeling, AMAD provides a robust and adaptable solution to UMTSAD challenges. Extensive experimental results demonstrate that the proposed model achieving competitive performance results compared to SOTA benchmarks across a variety of datasets.\n        △ Less",
        "authors":"['Tiange Huang', 'Yongjun Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FACT: Multinomial Misalignment Classification for Point Cloud Registration",
        "abstract":"We present FACT, a method for predicting alignment quality (i.e., registration error) of registered lidar point cloud pairs. This is useful e.g. for quality assurance of large, automatically registered 3D models. FACT extracts local features from a registered pair and processes them with a point transformer-based network to predict a misalignment class. We generalize prior work that study binary alignment classification of registration errors, by recasting it as multinomial misalignment classification. To achieve this, we introduce a custom regression-by-classification loss function that combines the cross-entropy and Wasserstein losses, and demonstrate that it outperforms both direct regression and prior binary classification. FACT successfully classifies point-cloud pairs registered with both the classical ICP and GeoTransformer, while other choices, such as standard point-cloud-quality metrics and registration residuals are shown to be poor choices for predicting misalignment. On a synthetically perturbed point-cloud task introduced by the CorAl method, we show that FACT achieves substantially better performance than CorAl. Finally, we demonstrate how FACT can assist experts in correcting misaligned point-cloud maps. Our code is available at https:\/\/github.com\/LudvigDillen\/FACT_for_PCMC.\n        △ Less",
        "authors":"['Ludvig Dillén', 'Per-Erik Forssén', 'Johan Edstedt']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Quantum neural networks facilitating quantum state classification",
        "abstract":"The classification of quantum states into distinct classes poses a significant challenge. In this study, we address this problem using quantum neural networks in combination with a problem-inspired circuit and customised as well as predefined ansätz. To facilitate the resource-efficient quantum state classification, we construct the dataset of quantum states using the proposed problem-inspired circuit. The problem-inspired circuit incorporates two-qubit parameterised unitary gates of varying entangling power, which is further integrated with the ansätz, developing an entire quantum neural network. To demonstrate the capability of the selected ansätz, we visualise the mitigated barren plateaus. The designed quantum neural network demonstrates the efficiency in binary and multi-class classification tasks. This work establishes a foundation for the classification of multi-qubit quantum states and offers the potential for generalisation to multi-qubit pure quantum states.\n        △ Less",
        "authors":"['Diksha Sharma', 'Vivek Balasaheb Sabale', 'Thirumalai M.', 'Atul Kumar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Wanting to be Understood",
        "abstract":"This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand and to be understood even in the absence of extrinsic rewards. Through simulations of the perceptual crossing paradigm, we explore the effect of various internal reward functions in reinforcement learning agents. The drive to understand is implemented as an active inference type artificial curiosity reward, whereas the drive to be understood is implemented through intrinsic rewards for imitation, influence\/impressionability, and sub-reaction time anticipation of the other. Results indicate that while artificial curiosity alone does not lead to a preference for social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize interaction. We demonstrate that this intrinsic motivation can facilitate cooperation in tasks where only one agent receives extrinsic reward for the behaviour of the other.\n        △ Less",
        "authors":"['Chrisantha Fernando', 'Dylan Banarse', 'Simon Osindero']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Disentangle and Regularize: Sign Language Production with Articulator-Based Disentanglement and Channel-Aware Regularization",
        "abstract":"In this work, we propose a simple gloss-free, transformer-based sign language production (SLP) framework that directly maps spoken-language text to sign pose sequences. We first train a pose autoencoder that encodes sign poses into a compact latent space using an articulator-based disentanglement strategy, where features corresponding to the face, right hand, left hand, and body are modeled separately to promote structured and interpretable representation learning. Next, a non-autoregressive transformer decoder is trained to predict these latent representations from sentence-level text embeddings. To guide this process, we apply channel-aware regularization by aligning predicted latent distributions with priors extracted from the ground-truth encodings using a KL-divergence loss. The contribution of each channel to the loss is weighted according to its associated articulator region, enabling the model to account for the relative importance of different articulators during training. Our approach does not rely on gloss supervision or pretrained models, and achieves state-of-the-art results on the PHOENIX14T dataset using only a modest training set.\n        △ Less",
        "authors":"['Sumeyye Meryem Tasyurek', 'Tugce Kiziltepe', 'Hacer Yalim Keles']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InteractRank: Personalized Web-Scale Search Pre-Ranking with Cross Interaction Features",
        "abstract":"Modern search systems use a multi-stage architecture to deliver personalized results efficiently. Key stages include retrieval, pre-ranking, full ranking, and blending, which refine billions of items to top selections. The pre-ranking stage, vital for scoring and filtering hundreds of thousands of items down to a few thousand, typically relies on two tower models due to their computational efficiency, despite often lacking in capturing complex interactions. While query-item cross interaction features are paramount for full ranking, integrating them into pre-ranking models presents efficiency-related challenges. In this paper, we introduce InteractRank, a novel two tower pre-ranking model with robust cross interaction features used at Pinterest. By incorporating historical user engagement-based query-item interactions in the scoring function along with the two tower dot product, InteractRank significantly boosts pre-ranking performance with minimal latency and computation costs. In real-world A\/B experiments at Pinterest, InteractRank improves the online engagement metric by 6.5% over a BM25 baseline and by 3.7% over a vanilla two tower baseline. We also highlight other components of InteractRank, like real-time user-sequence modeling, and analyze their contributions through offline ablation studies. The code for InteractRank is available at https:\/\/github.com\/pinterest\/atg-research\/tree\/main\/InteractRank.\n        △ Less",
        "authors":"['Sujay Khandagale', 'Bhawna Juneja', 'Prabhat Agarwal', 'Aditya Subramanian', 'Jaewon Yang', 'Yuting Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"NAPER: Fault Protection for Real-Time Resource-Constrained Deep Neural Networks",
        "abstract":"Fault tolerance in Deep Neural Networks (DNNs) deployed on resource-constrained systems presents unique challenges for high-accuracy applications with strict timing requirements. Memory bit-flips can severely degrade DNN accuracy, while traditional protection approaches like Triple Modular Redundancy (TMR) often sacrifice accuracy to maintain reliability, creating a three-way dilemma between reliability, accuracy, and timeliness. We introduce NAPER, a novel protection approach that addresses this challenge through ensemble learning. Unlike conventional redundancy methods, NAPER employs heterogeneous model redundancy, where diverse models collectively achieve higher accuracy than any individual model. This is complemented by an efficient fault detection mechanism and a real-time scheduler that prioritizes meeting deadlines by intelligently scheduling recovery operations without interrupting inference. Our evaluations demonstrate NAPER's superiority: 40% faster inference in both normal and fault conditions, maintained accuracy 4.2% higher than TMR-based strategies, and guaranteed uninterrupted operation even during fault recovery. NAPER effectively balances the competing demands of accuracy, reliability, and timeliness in real-time DNN applications\n        △ Less",
        "authors":"['Rian Adam Rajagede', 'Muhammad Husni Santriaji', 'Muhammad Arya Fikriansyah', 'Hilal Hudan Nuha', 'Yanjie Fu', 'Yan Solihin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CAFE-AD: Cross-Scenario Adaptive Feature Enhancement for Trajectory Planning in Autonomous Driving",
        "abstract":"Imitation learning based planning tasks on the nuPlan dataset have gained great interest due to their potential to generate human-like driving behaviors. However, open-loop training on the nuPlan dataset tends to cause causal confusion during closed-loop testing, and the dataset also presents a long-tail distribution of scenarios. These issues introduce challenges for imitation learning. To tackle these problems, we introduce CAFE-AD, a Cross-Scenario Adaptive Feature Enhancement for Trajectory Planning in Autonomous Driving method, designed to enhance feature representation across various scenario types. We develop an adaptive feature pruning module that ranks feature importance to capture the most relevant information while reducing the interference of noisy information during training. Moreover, we propose a cross-scenario feature interpolation module that enhances scenario information to introduce diversity, enabling the network to alleviate over-fitting in dominant scenarios. We evaluate our method CAFE-AD on the challenging public nuPlan Test14-Hard closed-loop simulation benchmark. The results demonstrate that CAFE-AD outperforms state-of-the-art methods including rule-based and hybrid planners, and exhibits the potential in mitigating the impact of long-tail distribution within the dataset. Additionally, we further validate its effectiveness in real-world environments. The code and models will be made available at https:\/\/github.com\/AlniyatRui\/CAFE-AD.\n        △ Less",
        "authors":"['Junrui Zhang', 'Chenjie Wang', 'Jie Peng', 'Haoyu Li', 'Jianmin Ji', 'Yu Zhang', 'Yanyong Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bypassing Safety Guardrails in LLMs Using Humor",
        "abstract":"In this paper, we show it is possible to bypass the safety guardrails of large language models (LLMs) through a humorous prompt including the unsafe request. In particular, our method does not edit the unsafe request and follows a fixed template -- it is simple to implement and does not need additional LLMs to craft prompts. Extensive experiments show the effectiveness of our method across different LLMs. We also show that both removing and adding more humor to our method can reduce its effectiveness -- excessive humor possibly distracts the LLM from fulfilling its unsafe request. Thus, we argue that LLM jailbreaking occurs when there is a proper balance between focus on the unsafe request and presence of humor.\n        △ Less",
        "authors":"['Pedro Cisneros-Velarde']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure",
        "abstract":"Financial scenario simulation is essential for risk management and portfolio optimization, yet it remains challenging especially in high-dimensional and small data settings common in finance. We propose a diffusion factor model that integrates latent factor structure into generative diffusion processes, bridging econometrics with modern generative AI to address the challenges of the curse of dimensionality and data scarcity in financial simulation. By exploiting the low-dimensional factor structure inherent in asset returns, we decompose the score function--a key component in diffusion models--using time-varying orthogonal projections, and this decomposition is incorporated into the design of neural network architectures. We derive rigorous statistical guarantees, establishing nonasymptotic error bounds for both score estimation at O(d^{5\/2} n^{-2\/(k+5)}) and generated distribution at O(d^{5\/4} n^{-1\/2(k+5)}), primarily driven by the intrinsic factor dimension k rather than the number of assets d, surpassing the dimension-dependent limits in the classical nonparametric statistics literature and making the framework viable for markets with thousands of assets. Numerical studies confirm superior performance in latent subspace recovery under small data regimes. Empirical analysis demonstrates the economic significance of our framework in constructing mean-variance optimal portfolios and factor portfolios. This work presents the first theoretical integration of factor structure with diffusion models, offering a principled approach for high-dimensional financial simulation with limited data.\n        △ Less",
        "authors":"['Minshuo Chen', 'Renyuan Xu', 'Yumin Xu', 'Ruixun Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TabKAN: Advancing Tabular Data Analysis using Kolmograv-Arnold Network",
        "abstract":"Tabular data analysis presents unique challenges due to its heterogeneous feature types, missing values, and complex interactions. While traditional machine learning methods, such as gradient boosting, often outperform deep learning approaches, recent advancements in neural architectures offer promising alternatives. This paper introduces TabKAN, a novel framework that advances tabular data modeling using Kolmogorov-Arnold Networks (KANs). Unlike conventional deep learning models, KANs leverage learnable activation functions on edges, enhancing both interpretability and training efficiency. Our contributions include: (1) the introduction of modular KAN-based architectures tailored for tabular data analysis, (2) the development of a transfer learning framework for KAN models, enabling effective knowledge transfer between domains, (3) the development of model-specific interpretability for tabular data learning, reducing reliance on post hoc and model-agnostic analysis, and (4) comprehensive evaluation of vanilla supervised learning across binary and multi-class classification tasks. Through extensive benchmarking on diverse public datasets, TabKAN demonstrates superior performance in supervised learning while significantly outperforming classical and Transformer-based models in transfer learning scenarios. Our findings highlight the advantage of KAN-based architectures in efficiently transferring knowledge across domains, bridging the gap between traditional machine learning and deep learning for structured data.\n        △ Less",
        "authors":"['Ali Eslamian', 'Alireza Afzal Aghaei', 'Qiang Cheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Lugha-Llama: Adapting Large Language Models for African Languages",
        "abstract":"Large language models (LLMs) have achieved impressive results in a wide range of natural language applications. However, they often struggle to recognize low-resource languages, in particular African languages, which are not well represented in large training corpora. In this paper, we consider how to adapt LLMs to low-resource African languages. We find that combining curated data from African languages with high-quality English educational texts results in a training mix that substantially improves the model's performance on these languages. On the challenging IrokoBench dataset, our models consistently achieve the best performance amongst similarly sized baselines, particularly on knowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the cross-lingual question answering benchmark AfriQA, our models outperform the base model by over 10%. To better understand the role of English data during training, we translate a subset of 200M tokens into Swahili language and perform an analysis which reveals that the content of these data is primarily responsible for the strong performance. We release our models and data to encourage future research on African languages.\n        △ Less",
        "authors":"['Happy Buzaaba', 'Alexander Wettig', 'David Ifeoluwa Adelani', 'Christiane Fellbaum']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Flexible Graph Similarity Computation With A Proactive Optimization Strategy",
        "abstract":"Graph Edit Distance (GED) is an important similarity measure in graph retrieval, which quantifies the minimum cost of transforming one graph into another through edit operations, and offers flexibility by allowing customizable operation costs. Recent learning-based approaches approximate GEDs with the distances between representations in vector spaces. However, these methods often struggle with varying operation costs due to neglecting the impact of these costs on determining optimal graph mappings. Furthermore, they rely on isolated node distances as guidance, necessitating inefficient reactive refinements of mappings. To address these issues, we propose Graph Edit Network (GEN), a novel learning-based approach for flexible GED computation. By identifying the limitations of existing methods in capturing flexibility of GED, we introduce a principled yet simple solution that incorporates the operation costs before establishing mappings. To improve matching efficiency, we propose a strategy that proactively optimizes guidance from a graph perspective. This strategy initializes guidance as each node's alignment difficulty and captures the interdependencies between matches within and across graphs through a difficulty propagation mechanism, enabling more informed decisions. As a result, GEN selects optimal matches in a single step, minimizing the need for costly refinements. Results on real-world and synthetic datasets demonstrate the effectiveness, time efficiency, and adaptability of GEN, achieving up to 37.8\\% error reduction and 72.7\\% inference time reduction compared with state-of-the-art models, while performing robustly under varying cost settings and graph sizes.\n        △ Less",
        "authors":"['Zhouyang Liu', 'Ning Liu', 'Yixin Chen', 'Jiezhong He', 'Dongsheng Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"WaveHiTS: Wavelet-Enhanced Hierarchical Time Series Modeling for Wind Direction Nowcasting in Eastern Inner Mongolia",
        "abstract":"Wind direction forecasting plays a crucial role in optimizing wind energy production, but faces significant challenges due to the circular nature of directional data, error accumulation in multi-step forecasting, and complex meteorological interactions. This paper presents a novel model, WaveHiTS, which integrates wavelet transform with Neural Hierarchical Interpolation for Time Series to address these challenges. Our approach decomposes wind direction into U-V components, applies wavelet transform to capture multi-scale frequency patterns, and utilizes a hierarchical structure to model temporal dependencies at multiple scales, effectively mitigating error propagation. Experiments conducted on real-world meteorological data from Inner Mongolia, China demonstrate that WaveHiTS significantly outperforms deep learning models (RNN, LSTM, GRU), transformer-based approaches (TFT, Informer, iTransformer), and hybrid models (EMD-LSTM). The proposed model achieves RMSE values of approximately 19.2°-19.4° compared to 56°-64° for deep learning recurrent models, maintaining consistent accuracy across all forecasting steps up to 60 minutes ahead. Moreover, WaveHiTS demonstrates superior robustness with vector correlation coefficients (VCC) of 0.985-0.987 and hit rates of 88.5%-90.1%, substantially outperforming baseline models. Ablation studies confirm that each component-wavelet transform, hierarchical structure, and U-V decomposition-contributes meaningfully to overall performance. These improvements in wind direction nowcasting have significant implications for enhancing wind turbine yaw control efficiency and grid integration of wind energy.\n        △ Less",
        "authors":"['Hailong Shu', 'Weiwei Song', 'Yue Wang', 'Jiping Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Power of the Pareto Front: Balancing Uncertain Rewards for Adaptive Experimentation in scanning probe microscopy",
        "abstract":"Automated experimentation has the potential to revolutionize scientific discovery, but its effectiveness depends on well-defined optimization targets, which are often uncertain or probabilistic in real-world settings. In this work, we demonstrate the application of Multi-Objective Bayesian Optimization (MOBO) to balance multiple, competing rewards in autonomous experimentation. Using scanning probe microscopy (SPM) imaging, one of the most widely used and foundational SPM modes, we show that MOBO can optimize imaging parameters to enhance measurement quality, reproducibility, and efficiency. A key advantage of this approach is the ability to compute and analyze the Pareto front, which not only guides optimization but also provides physical insights into the trade-offs between different objectives. Additionally, MOBO offers a natural framework for human-in-the-loop decision-making, enabling researchers to fine-tune experimental trade-offs based on domain expertise. By standardizing high-quality, reproducible measurements and integrating human input into AI-driven optimization, this work highlights MOBO as a powerful tool for advancing autonomous scientific discovery.\n        △ Less",
        "authors":"['Yu Liu', 'Sergei V. Kalinin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Controller Distillation Reduces Fragile Brain-Body Co-Adaptation and Enables Migrations in MAP-Elites",
        "abstract":"Brain-body co-optimization suffers from fragile co-adaptation where brains become over-specialized for particular bodies, hindering their ability to transfer well to others. Evolutionary algorithms tend to discard such low-performing solutions, eliminating promising morphologies. Previous work considered applying MAP-Elites, where niche descriptors are based on morphological features, to promote better search over morphology space. In this work, we show that this approach still suffers from fragile co-adaptation: where a core mechanism of MAP-Elites, creating stepping stones through solutions that migrate from one niche to another, is disrupted. We suggest that this disruption occurs because the body mutations that move an offspring to a new morphological niche break the robots' fragile brain-body co-adaptation and thus significantly decrease the performance of those potential solutions -- reducing their likelihood of outcompeting an existing elite in that new niche. We utilize a technique, we call Pollination, that periodically replaces the controllers of certain solutions with a distilled controller with better generalization across morphologies to reduce fragile brain-body co-adaptation and thus promote MAP-Elites migrations. Pollination increases the success of body mutations and the number of migrations, resulting in better quality-diversity metrics. We believe we develop important insights that could apply to other domains where MAP-Elites is used.\n        △ Less",
        "authors":"['Alican Mertan', 'Nick Cheney']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?",
        "abstract":"We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.\n        △ Less",
        "authors":"['Chenrui Fan', 'Ming Li', 'Lichao Sun', 'Tianyi Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GTS-LUM: Reshaping User Behavior Modeling with LLMs in Telecommunications Industry",
        "abstract":"As telecommunication service providers shifting their focus to analyzing user behavior for package design and marketing interventions, a critical challenge lies in developing a unified, end-to-end framework capable of modeling long-term and periodic user behavior sequences with diverse time granularities, multi-modal data inputs, and heterogeneous labels. This paper introduces GTS-LUM, a novel user behavior model that redefines modeling paradigms in telecommunication settings. GTS-LUM adopts a (multi-modal) encoder-adapter-LLM decoder architecture, enhanced with several telecom-specific innovations. Specifically, the model incorporates an advanced timestamp processing method to handle varying time granularities. It also supports multi-modal data inputs -- including structured tables and behavior co-occurrence graphs -- and aligns these with semantic information extracted by a tokenizer using a Q-former structure. Additionally, GTS-LUM integrates a front-placed target-aware mechanism to highlight historical behaviors most relevant to the target. Extensive experiments on industrial dataset validate the effectiveness of this end-to-end framework and also demonstrate that GTS-LUM outperforms LLM4Rec approaches which are popular in recommendation systems, offering an effective and generalizing solution for user behavior modeling in telecommunications.\n        △ Less",
        "authors":"['Liu Shi', 'Tianwu Zhou', 'Wei Xu', 'Li Liu', 'Zhexin Cui', 'Shaoyi Liang', 'Haoxing Niu', 'Yichong Tian', 'Jianwei Guo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data-driven Fuzzy Control for Time-Optimal Aggressive Trajectory Following",
        "abstract":"Optimal trajectories that minimize a user-defined cost function in dynamic systems require the solution of a two-point boundary value problem. The optimization process yields an optimal control sequence that depends on the initial conditions and system parameters. However, the optimal sequence may result in undesirable behavior if the system's initial conditions and parameters are erroneous. This work presents a data-driven fuzzy controller synthesis framework that is guided by a time-optimal trajectory for multicopter tracking problems. In particular, we consider an aggressive maneuver consisting of a mid-air flip and generate a time-optimal trajectory by numerically solving the two-point boundary value problem. A fuzzy controller consisting of a stabilizing controller near hover conditions and an autoregressive moving average (ARMA) controller, trained to mimic the time-optimal aggressive trajectory, is constructed using the Takagi-Sugeno fuzzy framework.\n        △ Less",
        "authors":"['August Phelps', 'Juan Augusto Paredes Salazar', 'Ankit Goel']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Continuous-Variable Quantum Encoding Techniques: A Comparative Study of Embedding Techniques and Their Impact on Machine Learning Performance",
        "abstract":"This study explores the intersection of continuous-variable quantum computing (CVQC) and classical machine learning, focusing on CVQC data encoding techniques, including Displacement encoding and squeezing encoding, alongside Instantaneous Quantum Polynomial (IQP) encoding from discrete quantum computing. We perform an extensive empirical analysis to assess the impact of these encoding methods on classical machine learning models, such as Logistic Regression, Support Vector Machines, K-Nearest Neighbors, and ensemble methods like Random Forest and LightGBM. Our findings indicate that CVQC-based encoding methods significantly enhance feature expressivity, resulting in improved classification accuracy and F1 scores, especially in high-dimensional and complex datasets. However, these improvements come with varying computational costs, which depend on the complexity of the encoding and the architecture of the machine learning models. Additionally, we examine the trade-off between quantum expressibility and classical learnability, offering valuable insights into the practical feasibility of incorporating these quantum encodings into real-world applications. This study contributes to the growing body of research on quantum-classical hybrid learning, emphasizing the role of CVQC in advancing quantum data representation and its integration into classical machine learning workflows.\n        △ Less",
        "authors":"['Minati Rath', 'Hema Date']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploiting Meta-Learning-based Poisoning Attacks for Graph Link Prediction",
        "abstract":"Link prediction in graph data utilizes various algorithms and machine learning\/deep learning models to predict potential relationships between graph nodes. This technique has found widespread use in numerous real-world applications, including recommendation systems, community networks, and biological structures. However, recent research has highlighted the vulnerability of link prediction models to adversarial attacks, such as poisoning and evasion attacks. Addressing the vulnerability of these models is crucial to ensure stable and robust performance in link prediction applications. While many works have focused on enhancing the robustness of the Graph Convolution Network (GCN) model, the Variational Graph Auto-Encoder (VGAE), a sophisticated model for link prediction, has not been thoroughly investigated in the context of graph adversarial attacks. To bridge this gap, this article proposes an unweighted graph poisoning attack approach using meta-learning techniques to undermine VGAE's link prediction performance. We conducted comprehensive experiments on diverse datasets to evaluate the proposed method and its parameters, comparing it with existing approaches in similar settings. Our results demonstrate that our approach significantly diminishes link prediction performance and outperforms other state-of-the-art methods.\n        △ Less",
        "authors":"['Mingchen Li', 'Di Zhuang', 'Keyu Chen', 'Dumindu Samaraweera', 'Morris Chang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sparsified-Learning for Heavy-Tailed Locally Stationary Processes",
        "abstract":"Sparsified Learning is ubiquitous in many machine learning tasks. It aims to regularize the objective function by adding a penalization term that considers the constraints made on the learned parameters. This paper considers the problem of learning heavy-tailed LSP. We develop a flexible and robust sparse learning framework capable of handling heavy-tailed data with locally stationary behavior and propose concentration inequalities. We further provide non-asymptotic oracle inequalities for different types of sparsity, including $\\ell_1$-norm and total variation penalization for the least square loss.\n        △ Less",
        "authors":"['Yingjie Wang', 'Mokhtar Z. Alaya', 'Salim Bouzebda', 'Xinsheng Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Fair Learning: A Unified Framework for Fine-tuning Representations with Sufficient Networks",
        "abstract":"Ensuring fairness in machine learning is a critical and challenging task, as biased data representations often lead to unfair predictions. To address this, we propose Deep Fair Learning, a framework that integrates nonlinear sufficient dimension reduction with deep learning to construct fair and informative representations. By introducing a novel penalty term during fine-tuning, our method enforces conditional independence between sensitive attributes and learned representations, addressing bias at its source while preserving predictive performance. Unlike prior methods, it supports diverse sensitive attributes, including continuous, discrete, binary, or multi-group types. Experiments on various types of data structure show that our approach achieves a superior balance between fairness and utility, significantly outperforming state-of-the-art baselines.\n        △ Less",
        "authors":"['Enze Shi', 'Linglong Kong', 'Bei Jiang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Analyzing Examinee Comments using DistilBERT and Machine Learning to Ensure Quality Control in Exam Content",
        "abstract":"This study explores using Natural Language Processing (NLP) to analyze candidate comments for identifying problematic test items. We developed and validated machine learning models that automatically identify relevant negative feedback, evaluated approaches of incorporating psychometric features enhances model performance, and compared NLP-flagged items with traditionally flagged items. Results demonstrate that candidate feedback provides valuable complementary information to statistical methods, potentially improving test validity while reducing manual review burden. This research offers testing organizations an efficient mechanism to incorporate direct candidate experience into quality assurance processes.\n        △ Less",
        "authors":"['Ye', 'Ma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Intelligent VR Training: A Physiological Adaptation Framework for Cognitive Load and Stress Detection",
        "abstract":"Adaptive Virtual Reality (VR) systems have the potential to enhance training and learning experiences by dynamically responding to users' cognitive states. This research investigates how eye tracking and heart rate variability (HRV) can be used to detect cognitive load and stress in VR environments, enabling real-time adaptation. The study follows a three-phase approach: (1) conducting a user study with the Stroop task to label cognitive load data and train machine learning models to detect high cognitive load, (2) fine-tuning these models with new users and integrating them into an adaptive VR system that dynamically adjusts training difficulty based on physiological signals, and (3) developing a privacy-aware approach to detect high cognitive load and compare this with the adaptive VR in Phase two. This research contributes to affective computing and adaptive VR using physiological sensing, with applications in education, training, and healthcare. Future work will explore scalability, real-time inference optimization, and ethical considerations in physiological adaptive VR.\n        △ Less",
        "authors":"['Mahsa Nasri']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Neural Architecture Search with Model-Agnostic Meta Learning",
        "abstract":"Federated Learning (FL) often struggles with data heterogeneity due to the naturally uneven distribution of user data across devices. Federated Neural Architecture Search (NAS) enables collaborative search for optimal model architectures tailored to heterogeneous data to achieve higher accuracy. However, this process is time-consuming due to extensive search space and retraining. To overcome this, we introduce FedMetaNAS, a framework that integrates meta-learning with NAS within the FL context to expedite the architecture search by pruning the search space and eliminating the retraining stage. Our approach first utilizes the Gumbel-Softmax reparameterization to facilitate relaxation of the mixed operations in the search space. We then refine the local search process by incorporating Model-Agnostic Meta-Learning, where a task-specific learner adapts both weights and architecture parameters (alphas) for individual tasks, while a meta learner adjusts the overall model weights and alphas based on the gradient information from task learners. Following the meta-update, we propose soft pruning using the same trick on search space to gradually sparsify the architecture, ensuring that the performance of the chosen architecture remains robust after pruning which allows for immediate use of the model without retraining. Experimental evaluations demonstrate that FedMetaNAS significantly accelerates the search process by more than 50\\% with higher accuracy compared to FedNAS.\n        △ Less",
        "authors":"['Xinyuan Huang', 'Jiechao Gao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can you Finetune your Binoculars? Embedding Text Watermarks into the Weights of Large Language Models",
        "abstract":"The indistinguishability of AI-generated content from human text raises challenges in transparency and accountability. While several methods exist to watermark models behind APIs, embedding watermark strategies directly into model weights that are later reflected in the outputs of the model is challenging. In this study we propose a strategy to finetune a pair of low-rank adapters of a model, one serving as the text-generating model, and the other as the detector, so that a subtle watermark is embedded into the text generated by the first model and simultaneously optimized for detectability by the second. In this way, the watermarking strategy is fully learned end-to-end. This process imposes an optimization challenge, as balancing watermark robustness, naturalness, and task performance requires trade-offs. We discuss strategies on how to optimize this min-max objective and present results showing the effect of this modification to instruction finetuning.\n        △ Less",
        "authors":"['Fay Elhassan', 'Niccolò Ajroldi', 'Antonio Orvieto', 'Jonas Geiping']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Classifying Subjective Time Perception in a Multi-robot Control Scenario Using Eye-tracking Information",
        "abstract":"As automation and mobile robotics reshape work environments, rising expectations for productivity increase cognitive demands on human operators, leading to potential stress and cognitive overload. Accurately assessing an operator's mental state is critical for maintaining performance and well-being. We use subjective time perception, which can be altered by stress and cognitive load, as a sensitive, low-latency indicator of well-being and cognitive strain. Distortions in time perception can affect decision-making, reaction times, and overall task effectiveness, making it a valuable metric for adaptive human-swarm interaction systems.\n  We study how human physiological signals can be used to estimate a person's subjective time perception in a human-swarm interaction scenario as example. A human operator needs to guide and control a swarm of small mobile robots. We obtain eye-tracking data that is classified for subjective time perception based on questionnaire data. Our results show that we successfully estimate a person's time perception from eye-tracking data. The approach can profit from individual-based pretraining using only 30 seconds of data. In future work, we aim for robots that respond to human operator needs by automatically classifying physiological data in a closed control loop.\n        △ Less",
        "authors":"['Till Aust', 'Julian Kaduk', 'Heiko Hamann']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph Neural Network-Based Distributed Optimal Control for Linear Networked Systems: An Online Distributed Training Approach",
        "abstract":"In this paper, we consider the distributed optimal control problem for linear networked systems. In particular, we are interested in learning distributed optimal controllers using graph recurrent neural networks (GRNNs). Most of the existing approaches result in centralized optimal controllers with offline training processes. However, as the increasing demand of network resilience, the optimal controllers are further expected to be distributed, and are desirable to be trained in an online distributed fashion, which are also the main contributions of our work. To solve this problem, we first propose a GRNN-based distributed optimal control method, and we cast the problem as a self-supervised learning problem. Then, the distributed online training is achieved via distributed gradient computation, and inspired by the (consensus-based) distributed optimization idea, a distributed online training optimizer is designed. Furthermore, the local closed-loop stability of the linear networked system under our proposed GRNN-based controller is provided by assuming that the nonlinear activation function of the GRNN-based controller is both local sector-bounded and slope-restricted. The effectiveness of our proposed method is illustrated by numerical simulations using a specifically developed simulator.\n        △ Less",
        "authors":"['Zihao Song', 'Panos J. Antsaklis', 'Hai Lin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"S'MoRE: Structural Mixture of Residual Experts for LLM Fine-tuning",
        "abstract":"Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) architectures enhance model capacity at the cost of more & under-utilized parameters. To address these limitations, we propose Structural Mixture of Residual Experts (S'MoRE), a novel framework that seamlessly integrates the efficiency of LoRA with the flexibility of MoE. Specifically, S'MoRE employs hierarchical low-rank decomposition of expert weights, yielding residuals of varying orders interconnected in a multi-layer structure. By routing input tokens through sub-trees of residuals, S'MoRE emulates the capacity of many experts by instantiating and assembling just a few low-rank matrices. We craft the inter-layer propagation of S'MoRE's residuals as a special type of Graph Neural Network (GNN), and prove that under similar parameter budget, S'MoRE improves \"structural flexibility\" of traditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and empirical results demonstrate that S'MoRE achieves superior fine-tuning performance, offering a transformative approach for efficient LLM adaptation.\n        △ Less",
        "authors":"['Hanqing Zeng', 'Yinglong Xia', 'Zhuokai Zhao', 'Gilbert Jiang', 'Qiang Zhang', 'Jiayi Liu', 'Lizhu Zhang', 'Xiangjun Fan', 'Benyu Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
        "abstract":"Speculative decoding (SD) has been shown to reduce the latency of autoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing throughput and therefore reducing the cost per token requires decoding with large batch sizes. Recent work shows that SD can accelerate decoding with large batch sizes too if the context is sufficiently long and the draft model's KV cache is sparse. We introduce SPIRe, a draft model that combines static sparse attention, pruned initialization, and feedback memory to increase the modeled throughput of speculative decoding by over 100% compared to speculation with a much smaller draft model and by over 35% compared to the strong baseline of sparse self-speculation. Our approach is particularly effective when context lengths vary significantly across requests.\n        △ Less",
        "authors":"['Sanjit Neelam', 'Daniel Heinlein', 'Vaclav Cvicek', 'Akshay Mishra', 'Reiner Pope']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Releasing Differentially Private Event Logs Using Generative Models",
        "abstract":"In recent years, the industry has been witnessing an extended usage of process mining and automated event data analysis. Consequently, there is a rising significance in addressing privacy apprehensions related to the inclusion of sensitive and private information within event data utilized by process mining algorithms. State-of-the-art research mainly focuses on providing quantifiable privacy guarantees, e.g., via differential privacy, for trace variants that are used by the main process mining techniques, e.g., process discovery. However, privacy preservation techniques designed for the release of trace variants are still insufficient to meet all the demands of industry-scale utilization. Moreover, ensuring privacy guarantees in situations characterized by a high occurrence of infrequent trace variants remains a challenging endeavor. In this paper, we introduce two novel approaches for releasing differentially private trace variants based on trained generative models. With TraVaG, we leverage \\textit{Generative Adversarial Networks} (GANs) to sample from a privatized implicit variant distribution. Our second method employs \\textit{Denoising Diffusion Probabilistic Models} that reconstruct artificial trace variants from noise via trained Markov chains. Both methods offer industry-scale benefits and elevate the degree of privacy assurances, particularly in scenarios featuring a substantial prevalence of infrequent variants. Also, they overcome the shortcomings of conventional privacy preservation techniques, such as bounding the length of variants and introducing fake variants. Experimental results on real-life event data demonstrate that our approaches surpass state-of-the-art techniques in terms of privacy guarantees and utility preservation.\n        △ Less",
        "authors":"['Frederik Wangelik', 'Majid Rafiei', 'Mahsa Pourbafrani', 'Wil M. P. van der Aalst']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Unifying Autoregressive and Diffusion-Based Sequence Generation",
        "abstract":"We present significant extensions to diffusion-based sequence generation models, blurring the line with autoregressive language models. We introduce hyperschedules, which assign distinct noise schedules to individual token positions, generalizing both autoregressive models (e.g., GPT) and conventional diffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two hybrid token-wise noising processes that interpolate between absorbing and uniform processes, enabling the model to fix past mistakes, and we introduce a novel inference algorithm that leverages this new feature in a simplified context inspired from MDLM. To support efficient training and inference, we design attention masks compatible with KV-caching. Our methods achieve state-of-the-art perplexity and generate diverse, high-quality sequences across standard benchmarks, suggesting a promising path for autoregressive diffusion-based sequence generation.\n        △ Less",
        "authors":"['Nima Fathi', 'Torsten Scholak', 'Pierre-André Noël']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PEEL the Layers and Find Yourself: Revisiting Inference-time Data Leakage for Residual Neural Networks",
        "abstract":"This paper explores inference-time data leakage risks of deep neural networks (NNs), where a curious and honest model service provider is interested in retrieving users' private data inputs solely based on the model inference results. Particularly, we revisit residual NNs due to their popularity in computer vision and our hypothesis that residual blocks are a primary cause of data leakage owing to the use of skip connections. By formulating inference-time data leakage as a constrained optimization problem, we propose a novel backward feature inversion method, \\textbf{PEEL}, which can effectively recover block-wise input features from the intermediate output of residual NNs. The surprising results in high-quality input data recovery can be explained by the intuition that the output from these residual blocks can be considered as a noisy version of the input and thus the output retains sufficient information for input recovery. We demonstrate the effectiveness of our layer-by-layer feature inversion method on facial image datasets and pre-trained classifiers. Our results show that PEEL outperforms the state-of-the-art recovery methods by an order of magnitude when evaluated by mean squared error (MSE). The code is available at \\href{https:\/\/github.com\/Huzaifa-Arif\/PEEL}{https:\/\/github.com\/Huzaifa-Arif\/PEEL}\n        △ Less",
        "authors":"['Huzaifa Arif', 'Keerthiram Murugesan', 'Payel Das', 'Alex Gittens', 'Pin-Yu Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Understanding Machine Unlearning Through the Lens of Mode Connectivity",
        "abstract":"Machine Unlearning aims to remove undesired information from trained models without requiring full retraining from scratch. Despite recent advancements, their underlying loss landscapes and optimization dynamics received less attention. In this paper, we investigate and analyze machine unlearning through the lens of mode connectivity - the phenomenon where independently trained models can be connected by smooth low-loss paths in the parameter space. We define and study mode connectivity in unlearning across a range of overlooked conditions, including connections between different unlearning methods, models trained with and without curriculum learning, and models optimized with first-order and secondorder techniques. Our findings show distinct patterns of fluctuation of different evaluation metrics along the curve, as well as the mechanistic (dis)similarity between unlearning methods. To the best of our knowledge, this is the first study on mode connectivity in the context of machine unlearning.\n        △ Less",
        "authors":"['Jiali Cheng', 'Hadi Amiri']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Physical spline for denoising object trajectory data by combining splines, ML feature regression and model knowledge",
        "abstract":"This article presents a method for estimating the dynamic driving states (position, velocity, acceleration and heading) from noisy measurement data. The proposed approach is effective with both complete and partial observations, producing refined trajectory signals with kinematic consistency, ensuring that velocity is the integral of acceleration and position is the integral of velocity. Additionally, the method accounts for the constraint that vehicles can only move in the direction of their orientation. The method is implemented as a configurable python library that also enables trajectory estimation solely based on position data. Regularization is applied to prevent extreme state variations. A key application is enhancing recorded trajectory data for use as reference inputs in machine learning models. At the end, the article presents the results of the method along with a comparison to ground truth data.\n        △ Less",
        "authors":"['Jonas Torzewski']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Low Rank Learning for Offline Query Optimization",
        "abstract":"Recent deployments of learned query optimizers use expensive neural networks and ad-hoc search policies. To address these issues, we introduce \\textsc{LimeQO}, a framework for offline query optimization leveraging low-rank learning to efficiently explore alternative query plans with minimal resource usage. By modeling the workload as a partially observed, low-rank matrix, we predict unobserved query plan latencies using purely linear methods, significantly reducing computational overhead compared to neural networks. We formalize offline exploration as an active learning problem, and present simple heuristics that reduces a 3-hour workload to 1.5 hours after just 1.5 hours of exploration. Additionally, we propose a transductive Tree Convolutional Neural Network (TCNN) that, despite higher computational costs, achieves the same workload reduction with only 0.5 hours of exploration. Unlike previous approaches that place expensive neural networks directly in the query processing ``hot'' path, our approach offers a low-overhead solution and a no-regressions guarantee, all without making assumptions about the underlying DBMS. The code is available in \\href{https:\/\/github.com\/zixy17\/LimeQO}{https:\/\/github.com\/zixy17\/LimeQO}.\n        △ Less",
        "authors":"['Zixuan Yi', 'Yao Tian', 'Zachary G. Ives', 'Ryan Marcus']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sharpness-Aware Parameter Selection for Machine Unlearning",
        "abstract":"It often happens that some sensitive personal information, such as credit card numbers or passwords, are mistakenly incorporated in the training of machine learning models and need to be removed afterwards. The removal of such information from a trained model is a complex task that needs to partially reverse the training process. There have been various machine unlearning techniques proposed in the literature to address this problem. Most of the proposed methods revolve around removing individual data samples from a trained model. Another less explored direction is when features\/labels of a group of data samples need to be reverted. While the existing methods for these tasks do the unlearning task by updating the whole set of model parameters or only the last layer of the model, we show that there are a subset of model parameters that have the largest contribution in the unlearning target features. More precisely, the model parameters with the largest corresponding diagonal value in the Hessian matrix (computed at the learned model parameter) have the most contribution in the unlearning task. By selecting these parameters and updating them during the unlearning stage, we can have the most progress in unlearning. We provide theoretical justifications for the proposed strategy by connecting it to sharpness-aware minimization and robust unlearning. We empirically show the effectiveness of the proposed strategy in improving the efficacy of unlearning with a low computational cost.\n        △ Less",
        "authors":"['Saber Malekmohammadi', 'Hong kyu Lee', 'Li Xiong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Zero Body Problem: Probing LLM Use of Sensory Language",
        "abstract":"Sensory language expresses embodied experiences ranging from taste and sound to excitement and stomachache. This language is of interest to scholars from a wide range of domains including robotics, narratology, linguistics, and cognitive science. In this work, we explore whether language models, which are not embodied, can approximate human use of embodied language. We extend an existing corpus of parallel human and model responses to short story prompts with an additional 18,000 stories generated by 18 popular models. We find that all models generate stories that differ significantly from human usage of sensory language, but the direction of these differences varies considerably between model families. Namely, Gemini models use significantly more sensory language than humans along most axes whereas most models from the remaining five families use significantly less. Linear probes run on five models suggest that they are capable of identifying sensory language. However, we find preliminary evidence suggesting that instruction tuning may discourage usage of sensory language. Finally, to support further work, we release our expanded story dataset.\n        △ Less",
        "authors":"['Rebecca M. M. Hicke', 'Sil Hamilton', 'David Mimno']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards practicable Machine Learning development using AI Engineering Blueprints",
        "abstract":"The implementation of artificial intelligence (AI) in business applications holds considerable promise for significant improvements. The development of AI systems is becoming increasingly complex, thereby underscoring the growing importance of AI engineering and MLOps techniques. Small and medium-sized enterprises (SMEs) face considerable challenges when implementing AI in their products or processes. These enterprises often lack the necessary resources and expertise to develop, deploy, and operate AI systems that are tailored to address their specific problems.\n  Given the lack of studies on the application of AI engineering practices, particularly in the context of SMEs, this paper proposes a research plan designed to develop blueprints for the creation of proprietary machine learning (ML) models using AI engineering and MLOps practices. These blueprints enable SMEs to develop, deploy, and operate AI systems by providing reference architectures and suitable automation approaches for different types of ML.\n  The efficacy of the blueprints is assessed through their application to a series of field projects. This process gives rise to further requirements and additional development loops for the purpose of generalization. The benefits of using the blueprints for organizations are demonstrated by observing the process of developing ML models and by conducting interviews with the developers.\n        △ Less",
        "authors":"['Nicolas Weeger', 'Annika Stiehl', 'Jóakim vom Kistowski', 'Stefan Geißelsöder', 'Christian Uhl']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL",
        "abstract":"To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work we present novel theoretical results that provide a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setup: the bound, based on a `maximum policy ratio' that is computed with respect to a `safe' base policy, can also be more generally applied to temporally-extended properties (beyond safety) and to robust control problems. We thus present SPoRt, which also provides a data-driven approach for obtaining such a bound for the base policy, based on scenario theory, and which includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. Hence, SPoRt enables the user to trade off safety guarantees in exchange for task-specific performance. Accordingly, we present experimental results demonstrating this trade-off, as well as a comparison of the theoretical bound to posterior bounds based on empirical violation rates.\n        △ Less",
        "authors":"['Jacques Cloete', 'Nikolaus Vertovec', 'Alessandro Abate']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep spatio-temporal point processes: Advances and new directions",
        "abstract":"Spatio-temporal point processes (STPPs) model discrete events distributed in time and space, with important applications in areas such as criminology, seismology, epidemiology, and social networks. Traditional models often rely on parametric kernels, limiting their ability to capture heterogeneous, nonstationary dynamics. Recent innovations integrate deep neural architectures -- either by modeling the conditional intensity function directly or by learning flexible, data-driven influence kernels, substantially broadening their expressive power. This article reviews the development of the deep influence kernel approach, which enjoys statistical explainability, since the influence kernel remains in the model to capture the spatiotemporal propagation of event influence and its impact on future events, while also possessing strong expressive power, thereby benefiting from both worlds. We explain the main components in developing deep kernel point processes, leveraging tools such as functional basis decomposition and graph neural networks to encode complex spatial or network structures, as well as estimation using both likelihood-based and likelihood-free methods, and address computational scalability for large-scale data. We also discuss the theoretical foundation of kernel identifiability. Simulated and real-data examples highlight applications to crime analysis, earthquake aftershock prediction, and sepsis prediction modeling, and we conclude by discussing promising directions for the field.\n        △ Less",
        "authors":"['Xiuyuan Cheng', 'Zheng Dong', 'Yao Xie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From Broadcast to Minimap: Achieving State-of-the-Art SoccerNet Game State Reconstruction",
        "abstract":"Game State Reconstruction (GSR), a critical task in Sports Video Understanding, involves precise tracking and localization of all individuals on the football field-players, goalkeepers, referees, and others - in real-world coordinates. This capability enables coaches and analysts to derive actionable insights into player movements, team formations, and game dynamics, ultimately optimizing training strategies and enhancing competitive advantage. Achieving accurate GSR using a single-camera setup is highly challenging due to frequent camera movements, occlusions, and dynamic scene content. In this work, we present a robust end-to-end pipeline for tracking players across an entire match using a single-camera setup. Our solution integrates a fine-tuned YOLOv5m for object detection, a SegFormer-based camera parameter estimator, and a DeepSORT-based tracking framework enhanced with re-identification, orientation prediction, and jersey number recognition. By ensuring both spatial accuracy and temporal consistency, our method delivers state-of-the-art game state reconstruction, securing first place in the SoccerNet Game State Reconstruction Challenge 2024 and significantly outperforming competing methods.\n        △ Less",
        "authors":"['Vladimir Golovkin', 'Nikolay Nemtsev', 'Vasyl Shandyba', 'Oleg Udin', 'Nikita Kasatkin', 'Pavel Kononov', 'Anton Afanasiev', 'Sergey Ulasen', 'Andrei Boiarov']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Information-Geometric Approach to Artificial Curiosity",
        "abstract":"Learning in environments with sparse rewards remains a fundamental challenge in reinforcement learning. Artificial curiosity addresses this limitation through intrinsic rewards to guide exploration, however, the precise formulation of these rewards has remained elusive. Ideally, such rewards should depend on the agent's information about the environment, remaining agnostic to the representation of the information -- an invariance central to information geometry. Leveraging information geometry, we show that invariance under congruent Markov morphisms and the agent-environment interaction, uniquely constrains intrinsic rewards to concave functions of the reciprocal occupancy. Additional geometrically motivated restrictions effectively limits the candidates to those determined by a real parameter that governs the occupancy space geometry. Remarkably, special values of this parameter are found to correspond to count-based and maximum entropy exploration, revealing a geometric exploration-exploitation trade-off. This framework provides important constraints to the engineering of intrinsic reward while integrating foundational exploration methods into a single, cohesive model.\n        △ Less",
        "authors":"['Alexander Nedergaard', 'Pablo A. Morales']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Geometric-Aware Perspective and Beyond: Hybrid Quantum-Classical Machine Learning Methods",
        "abstract":"Geometric Machine Learning (GML) has shown that respecting non-Euclidean geometry in data spaces can significantly improve performance over naive Euclidean assumptions. In parallel, Quantum Machine Learning (QML) has emerged as a promising paradigm that leverages superposition, entanglement, and interference within quantum state manifolds for learning tasks. This paper offers a unifying perspective by casting QML as a specialized yet more expressive branch of GML. We argue that quantum states, whether pure or mixed, reside on curved manifolds (e.g., projective Hilbert spaces or density-operator manifolds), mirroring how covariance matrices inhabit the manifold of symmetric positive definite (SPD) matrices or how image sets occupy Grassmann manifolds. However, QML also benefits from purely quantum properties, such as entanglement-induced curvature, that can yield richer kernel structures and more nuanced data embeddings.\n  We illustrate these ideas with published and newly discussed results, including hybrid classical -quantum pipelines for diabetic foot ulcer classification and structural health monitoring. Despite near-term hardware limitations that constrain purely quantum solutions, hybrid architectures already demonstrate tangible benefits by combining classical manifold-based feature extraction with quantum embeddings. We present a detailed mathematical treatment of the geometrical underpinnings of quantum states, emphasizing parallels to classical Riemannian geometry and manifold-based optimization. Finally, we outline open research challenges and future directions, including Quantum Large Language Models (LLMs), quantum reinforcement learning, and emerging hardware approaches, demonstrating how synergizing GML and QML principles can unlock the next generation of machine intelligence.\n        △ Less",
        "authors":"['Azadeh Alavia', 'Hossein Akhoundib', 'Fatemeh Kouchmeshkib', 'Mojtaba Mahmoodianc', 'Sanduni Jayasinghec', 'Yongli Rena', 'Abdolrahman Alavi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Physics-informed KAN PointNet: Deep learning for simultaneous solutions to inverse problems in incompressible flow on numerous irregular geometries",
        "abstract":"Kolmogorov-Arnold Networks (KANs) have gained attention as a promising alternative to traditional Multilayer Perceptrons (MLPs) for deep learning applications in computational physics, especially within the framework of physics-informed neural networks (PINNs). Physics-informed Kolmogorov-Arnold Networks (PIKANs) and their variants have been introduced and evaluated to solve inverse problems. However, similar to PINNs, current versions of PIKANs are limited to obtaining solutions for a single computational domain per training run; consequently, a new geometry requires retraining the model from scratch. Physics-informed PointNet (PIPN) was introduced to address this limitation for PINNs. In this work, we introduce physics-informed Kolmogorov-Arnold PointNet (PI-KAN-PointNet) to extend this capability to PIKANs. PI-KAN-PointNet enables the simultaneous solution of an inverse problem over multiple irregular geometries within a single training run, reducing computational costs. We construct KANs using Jacobi polynomials and investigate their performance by considering Jacobi polynomials of different degrees and types in terms of both computational cost and prediction accuracy. As a benchmark test case, we consider natural convection in a square enclosure with a cylinder, where the cylinder's shape varies across a dataset of 135 geometries. We compare the performance of PI-KAN-PointNet with that of PIPN (i.e., physics-informed PointNet with MLPs) and observe that, with approximately an equal number of trainable parameters and similar computational cost, PI-KAN-PointNet provides more accurate predictions. Finally, we explore the combination of KAN and MLP in constructing a physics-informed PointNet. Our findings indicate that a physics-informed PointNet model employing MLP layers as the encoder and KAN layers as the decoder represents the optimal configuration among all models investigated.\n        △ Less",
        "authors":"['Ali Kashefi', 'Tapan Mukerji']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MM-STFlowNet: A Transportation Hub-Oriented Multi-Mode Passenger Flow Prediction Method via Spatial-Temporal Dynamic Graph Modeling",
        "abstract":"Accurate and refined passenger flow prediction is essential for optimizing the collaborative management of multiple collection and distribution modes in large-scale transportation hubs. Traditional methods often focus only on the overall passenger volume, neglecting the interdependence between different modes within the hub. To address this limitation, we propose MM-STFlowNet, a comprehensive multi-mode prediction framework grounded in dynamic spatial-temporal graph modeling. Initially, an integrated temporal feature processing strategy is implemented using signal decomposition and convolution techniques to address data spikes and high volatility. Subsequently, we introduce the Spatial-Temporal Dynamic Graph Convolutional Recurrent Network (STDGCRN) to capture detailed spatial-temporal dependencies across multiple traffic modes, enhanced by an adaptive channel attention mechanism. Finally, the self-attention mechanism is applied to incorporate various external factors, further enhancing prediction accuracy. Experiments on a real-world dataset from Guangzhounan Railway Station in China demonstrate that MM-STFlowNet achieves state-of-the-art performance, particularly during peak periods, providing valuable insight for transportation hub management.\n        △ Less",
        "authors":"['Ronghui Zhang', 'Wenbin Xing', 'Mengran Li', 'Zihan Wang', 'Junzhou Chen', 'Xiaolei Ma', 'Zhiyuan Liu', 'Zhengbing He']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs",
        "abstract":"Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models.\n        △ Less",
        "authors":"['Bailey J. Eccles', 'Leon Wong', 'Blesson Varghese']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hybrid Temporal Differential Consistency Autoencoder for Efficient and Sustainable Anomaly Detection in Cyber-Physical Systems",
        "abstract":"Cyberattacks on critical infrastructure, particularly water distribution systems, have increased due to rapid digitalization and the integration of IoT devices and industrial control systems (ICS). These cyber-physical systems (CPS) introduce new vulnerabilities, requiring robust and automated intrusion detection systems (IDS) to mitigate potential threats. This study addresses key challenges in anomaly detection by leveraging time correlations in sensor data, integrating physical principles into machine learning models, and optimizing computational efficiency for edge applications. We build upon the concept of temporal differential consistency (TDC) loss to capture the dynamics of the system, ensuring meaningful relationships between dynamic states. Expanding on this foundation, we propose a hybrid autoencoder-based approach, referred to as hybrid TDC-AE, which extends TDC by incorporating both deterministic nodes and conventional statistical nodes. This hybrid structure enables the model to account for non-deterministic processes. Our approach achieves state-of-the-art classification performance while improving time to detect anomalies by 3%, outperforming the BATADAL challenge leader without requiring domain-specific knowledge, making it broadly applicable. Additionally, it maintains the computational efficiency of conventional autoencoders while reducing the number of fully connected layers, resulting in a more sustainable and efficient solution. The method demonstrates how leveraging physics-inspired consistency principles enhances anomaly detection and strengthens the resilience of cyber-physical systems.\n        △ Less",
        "authors":"['Michael Somma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching",
        "abstract":"Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.\n        △ Less",
        "authors":"['Yanhao Dong', 'Yubo Miao', 'Weinan Li', 'Xiao Zheng', 'Chao Wang', 'Feng Lyu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DeepGDel: Deep Learning-based Gene Deletion Prediction Framework for Growth-Coupled Production in Genome-Scale Metabolic Models",
        "abstract":"In genome-scale constraint-based metabolic models, gene deletion strategies are crucial for achieving growth-coupled production, where cell growth and target metabolite production are simultaneously achieved. While computational methods for calculating gene deletions have been widely explored and contribute to developing gene deletion strategy databases, current approaches are limited in leveraging new data-driven paradigms, such as machine learning, for more efficient strain design. Therefore, it is necessary to propose a fundamental framework for this objective. In this study, we first formulate the problem of gene deletion strategy prediction and then propose a framework for predicting gene deletion strategies for growth-coupled production in genome-scale metabolic models. The proposed framework leverages deep learning algorithms to learn and integrate sequential gene and metabolite data representation, enabling the automatic gene deletion strategy prediction. Computational experiment results demonstrate the feasibility of the proposed framework, showing substantial improvements over the baseline method. Specifically, the proposed framework achieves a 17.64%, 27.15%, and 18.07% increase in overall accuracy across three metabolic models of different scales under study, while maintaining balanced precision and recall in predicting gene deletion statuses. The source code and examples for the framework are publicly available at https:\/\/github.com\/MetNetComp\/DeepGDel.\n        △ Less",
        "authors":"['Ziwei Yang', 'Takeyuki Tamura']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DMol: A Schedule-Driven Diffusion Model for Highly Efficient and Versatile Molecule Generation",
        "abstract":"We introduce a new graph diffusion model for small molecule generation, \\emph{DMol}, which outperforms the state-of-the-art DiGress model in terms of validity by roughly $1.5\\%$ across all benchmarking datasets while reducing the number of diffusion steps by at least $10$-fold, and the running time to roughly one half. The performance improvements are a result of a careful change in the objective function and a ``graph noise\" scheduling approach which, at each diffusion step, allows one to only change a subset of nodes of varying size in the molecule graph. Another relevant property of the method is that it can be easily combined with junction-tree-like graph representations that arise by compressing a collection of relevant ring structures into supernodes. Unlike classical junction-tree techniques that involve VAEs and require complicated reconstruction steps, compressed DMol directly performs graph diffusion on a graph that compresses only a carefully selected set of frequent carbon rings into supernodes, which results in straightforward sample generation. This compressed DMol method offers additional validity improvements over generic DMol of roughly $2\\%$, increases the novelty of the method, and further improves the running time due to reductions in the graph size.\n        △ Less",
        "authors":"['Peizhi Niu', 'Yu-Hsiang Wang', 'Vishal Rana', 'Chetan Rupakheti', 'Abhishek Pandey', 'Olgica Milenkovic']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rethinking RoPE: A Mathematical Blueprint for N-dimensional Positional Encoding",
        "abstract":"Rotary Position Embedding (RoPE) is widely adopted in Transformers due to its ability to encode relative positions with high efficiency and extrapolation capability. However, existing RoPE variants lack a unified theoretical foundation, especially in higher dimensions. In this paper, we propose a systematic mathematical framework for RoPE grounded in Lie group and Lie algebra theory. We identify two core properties of RoPE, named relativity and reversibility, and derive general constraints and constructions for valid RoPE in 1D, 2D, and N-dimensional (ND). We prove that RoPE must lie in the basis of a maximal abelian subalgebra (MASA) of the special orthogonal Lie algebra, and show that standard RoPE corresponds to the maximal toral subalgebra. Furthermore, we propose to model inter-dimensional interactions by learning an orthogonal basis transformation. Our framework unifies and explains existing RoPE designs, while enabling principled extensions to new modalities and tasks.\n        △ Less",
        "authors":"['Haiping Liu', 'Hongpeng Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights",
        "abstract":"The rapid adoption of large language models (LLMs) has led to significant energy consumption and carbon emissions, posing a critical challenge to the sustainability of generative AI technologies. This paper explores the integration of energy-efficient optimization techniques in the deployment of LLMs to address these environmental concerns. We present a case study and framework that demonstrate how strategic quantization and local inference techniques can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45\\% post quantization, making them particularly suitable for resource-constrained environments. The findings provide actionable insights for achieving sustainability in AI while maintaining high levels of accuracy and responsiveness.\n        △ Less",
        "authors":"['Tahniat Khan', 'Soroor Motie', 'Sedef Akinli Kocak', 'Shaina Raza']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Predicting Survivability of Cancer Patients with Metastatic Patterns Using Explainable AI",
        "abstract":"Cancer remains a leading global health challenge and a major cause of mortality. This study leverages machine learning (ML) to predict the survivability of cancer patients with metastatic patterns using the comprehensive MSK-MET dataset, which includes genomic and clinical data from 25,775 patients across 27 cancer types. We evaluated five ML models-XGBoost, Naïve Bayes, Decision Tree, Logistic Regression, and Random Fores using hyperparameter tuning and grid search. XGBoost emerged as the best performer with an area under the curve (AUC) of 0.82. To enhance model interpretability, SHapley Additive exPlanations (SHAP) were applied, revealing key predictors such as metastatic site count, tumor mutation burden, fraction of genome altered, and organ-specific metastases. Further survival analysis using Kaplan-Meier curves, Cox Proportional Hazards models, and XGBoost Survival Analysis identified significant predictors of patient outcomes, offering actionable insights for clinicians. These findings could aid in personalized prognosis and treatment planning, ultimately improving patient care.\n        △ Less",
        "authors":"['Polycarp Nalela', 'Deepthi Rao', 'Praveen Rao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Well2Flow: Reconstruction of reservoir states from sparse wells using score-based generative models",
        "abstract":"This study investigates the use of score-based generative models for reservoir simulation, with a focus on reconstructing spatially varying permeability and saturation fields in saline aquifers, inferred from sparse observations at two well locations. By modeling the joint distribution of permeability and saturation derived from high-fidelity reservoir simulations, the proposed neural network is trained to learn the complex spatiotemporal dynamics governing multiphase fluid flow in porous media. During inference, the framework effectively reconstructs both permeability and saturation fields by conditioning on sparse vertical profiles extracted from well log data. This approach introduces a novel methodology for incorporating physical constraints and well log guidance into generative models, significantly enhancing the accuracy and physical plausibility of the reconstructed subsurface states. Furthermore, the framework demonstrates strong generalization capabilities across varying geological scenarios, highlighting its potential for practical deployment in data-scarce reservoir management tasks.\n        △ Less",
        "authors":"['Shiqin Zeng', 'Haoyun Li', 'Abhinav Prakash Gahlot', 'Felix J. Herrmann']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging State Space Models in Long Range Genomics",
        "abstract":"Long-range dependencies are critical for understanding genomic structure and function, yet most conventional methods struggle with them. Widely adopted transformer-based models, while excelling at short-context tasks, are limited by the attention module's quadratic computational complexity and inability to extrapolate to sequences longer than those seen in training. In this work, we explore State Space Models (SSMs) as a promising alternative by benchmarking two SSM-inspired architectures, Caduceus and Hawk, on long-range genomics modeling tasks under conditions parallel to a 50M parameter transformer baseline. We discover that SSMs match transformer performance and exhibit impressive zero-shot extrapolation across multiple tasks, handling contexts 10 to 100 times longer than those seen during training, indicating more generalizable representations better suited for modeling the long and complex human genome. Moreover, we demonstrate that these models can efficiently process sequences of 1M tokens on a single GPU, allowing for modeling entire genomic regions at once, even in labs with limited compute. Our findings establish SSMs as efficient and scalable for long-context genomic analysis.\n        △ Less",
        "authors":"['Matvei Popov', 'Aymen Kallala', 'Anirudha Ramesh', 'Narimane Hennouni', 'Shivesh Khaitan', 'Rick Gentry', 'Alain-Sam Cohen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions",
        "abstract":"Understanding and mitigating biases is critical for the adoption of large language models (LLMs) in high-stakes decision-making. We introduce Admissions and Hiring, decision tasks with hypothetical applicant profiles where a person's race can be inferred from their name, as simplified test beds for racial bias. We show that Gemma 2B Instruct and LLaMA 3.2 3B Instruct exhibit strong biases. Gemma grants admission to 26% more White than Black applicants, and LLaMA hires 60% more Asian than White applicants. We demonstrate that these biases are resistant to prompt engineering: multiple prompting strategies all fail to promote fairness. In contrast, using distributed alignment search, we can identify \"race subspaces\" within model activations and intervene on them to debias model decisions. Averaging the representation across all races within the subspaces reduces Gemma's bias by 37-57%. Finally, we examine the generalizability of Gemma's race subspaces, and find limited evidence for generalization, where changing the prompt format can affect the race representation. Our work suggests mechanistic approaches may provide a promising venue for improving the fairness of LLMs, but a universal race representation remains elusive.\n        △ Less",
        "authors":"['Dang Nguyen', 'Chenhao Tan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Going beyond explainability in multi-modal stroke outcome prediction models",
        "abstract":"Aim: This study aims to enhance interpretability and explainability of multi-modal prediction models integrating imaging and tabular patient data.\n  Methods: We adapt the xAI methods Grad-CAM and Occlusion to multi-modal, partly interpretable deep transformation models (dTMs). DTMs combine statistical and deep learning approaches to simultaneously achieve state-of-the-art prediction performance and interpretable parameter estimates, such as odds ratios for tabular features. Based on brain imaging and tabular data from 407 stroke patients, we trained dTMs to predict functional outcome three months after stroke. We evaluated the models using different discriminatory metrics. The adapted xAI methods were used to generated explanation maps for identification of relevant image features and error analysis.\n  Results: The dTMs achieve state-of-the-art prediction performance, with area under the curve (AUC) values close to 0.8. The most important tabular predictors of functional outcome are functional independence before stroke and NIHSS on admission, a neurological score indicating stroke severity. Explanation maps calculated from brain imaging dTMs for functional outcome highlighted critical brain regions such as the frontal lobe, which is known to be linked to age which in turn increases the risk for unfavorable outcomes. Similarity plots of the explanation maps revealed distinct patterns which give insight into stroke pathophysiology, support developing novel predictors of stroke outcome and enable to identify false predictions.\n  Conclusion: By adapting methods for explanation maps to dTMs, we enhanced the explainability of multi-modal and partly interpretable prediction models. The resulting explanation maps facilitate error analysis and support hypothesis generation regarding the significance of specific image regions in outcome prediction.\n        △ Less",
        "authors":"['Jonas Brändli', 'Maurice Schneeberger', 'Lisa Herzog', 'Loran Avci', 'Nordin Dari', 'Martin Häansel', 'Hakim Baazaoui', 'Pascal Bühler', 'Susanne Wegener', 'Beate Sick']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ternarization of Vision Language Models for use on edge devices",
        "abstract":"We propose a process to compress a pre-trained Vision Language Model into a ternary version of itself instead of training a ternary model from scratch. A new initialization scheme from pre-trained weights based on the k-means algorithm is proposed to reduce the ternarization time. We implement different custom operators for executing the ternary model on the TensorFlow Lite Engine. We compare the original model with its ternary and binary versions in terms of memory consumption, inference speed and perplexity. We find that the ternary model using our custom ternary matrix multiplication operator provides a good compromise in term of memory usage and perplexity, while having the fastest token generation speed.\n        △ Less",
        "authors":"['Ben Crulis', 'Cyril De Runz', 'Barthelemy Serres', 'Gilles Venturini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generative AI Enhanced Financial Risk Management Information Retrieval",
        "abstract":"Risk management in finance involves recognizing, evaluating, and addressing financial risks to maintain stability and ensure regulatory compliance. Extracting relevant insights from extensive regulatory documents is a complex challenge requiring advanced retrieval and language models. This paper introduces RiskData, a dataset specifically curated for finetuning embedding models in risk management, and RiskEmbed, a finetuned embedding model designed to improve retrieval accuracy in financial question-answering systems. The dataset is derived from 94 regulatory guidelines published by the Office of the Superintendent of Financial Institutions (OSFI) from 1991 to 2024. We finetune a state-of-the-art sentence BERT embedding model to enhance domain-specific retrieval performance typically for Retrieval-Augmented Generation (RAG) systems. Experimental results demonstrate that RiskEmbed significantly outperforms general-purpose and financial embedding models, achieving substantial improvements in ranking metrics. By open-sourcing both the dataset and the model, we provide a valuable resource for financial institutions and researchers aiming to develop more accurate and efficient risk management AI solutions.\n        △ Less",
        "authors":"['Amin Haeri', 'Jonathan Vitrano', 'Mahdi Ghelichi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Different Paths, Same Destination: Designing New Physics-Inspired Dynamical Systems with Engineered Stability to Minimize the Ising Hamiltonian",
        "abstract":"Oscillator Ising machines (OIMs) represent an exemplar case of using physics-inspired non-linear dynamical systems to solve computationally challenging combinatorial optimization problems (COPs). The computational performance of such systems is highly sensitive to the underlying dynamical properties, the topology of the input graph, and their relative compatibility. In this work, we explore the concept of designing different dynamical systems that minimize the same objective function but exhibit drastically different dynamical properties. Our goal is to leverage this diversification in dynamics to reduce the sensitivity of the computational performance to the underlying graph, and subsequently, enhance the overall effectiveness of such physics-based computational methods. To this end, we introduce a novel dynamical system, the Dynamical Ising Machine (DIM), which, like the OIM, minimizes the Ising Hamiltonian but offers significantly different dynamical properties. We analyze the characteristic properties of the DIM and compare them with those of the OIM. We also show that the relative performance of each model is dependent on the input graph. Our work illustrates that using multiple dynamical systems with varying properties to solve the same COP enables an effective method that is less sensitive to the input graph, while producing robust solutions.\n        △ Less",
        "authors":"['E. M. H. E. B. Ekanayake', 'N. Shukla']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Joint Group Profiling and Recommendation via Deep Neural Network-based Multi-Task Learning",
        "abstract":"Group recommender systems aim to generate recommendations that align with the collective preferences of a group, introducing challenges that differ significantly from those in individual recommendation scenarios. This paper presents Joint Group Profiling and Recommendation via Deep Neural Network-based Multi-Task Learning, a framework that unifies group profiling and recommendation tasks within a single model. By jointly learning these tasks, the model develops a deeper understanding of group dynamics, leading to improved recommendation accuracy. The shared representations between the two tasks facilitate the discovery of latent features essential to both, resulting in richer and more informative group embeddings. To further enhance performance, an attention mechanism is integrated to dynamically evaluate the relevance of different group features and item attributes, ensuring the model prioritizes the most impactful information. Experiments and evaluations on real-world datasets demonstrate that our multi-task learning approach consistently outperforms baseline models in terms of accuracy, validating its effectiveness and robustness.\n        △ Less",
        "authors":"['Ngoc Luyen Le', 'Marie-Hélène Abel']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning through Bayesian Optimization",
        "abstract":"Large Language Models (LLMs) can encode complex relationships in their latent spaces, yet harnessing them for optimization under uncertainty remains challenging. We address this gap with a novel architecture that reframes LLM finetuning as Gaussian process (GP) marginal likelihood optimization via deep kernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs to preserve the benefits of both - LLMs to provide a rich and flexible input space for Bayesian optimization and - GPs to model this space with predictive uncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction optimization, our method nearly doubles the discovery rate of high-performing reactions compared to static LLM embeddings (from 24% to 43% coverage of the top 5% reactions in just 50 optimization iterations). We also observe a 14% improvement over domain-specific representations without requiring specialized features. Extensive empirical evaluation across 19 benchmarks - ranging from general chemistry to reaction and molecular property optimization - demonstrates our method's robustness, generality, and consistent improvements across: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder), (3) pretraining domains (chemistry-related or general-purpose) and (4) hyperparameter settings (tuned once on a single dataset). Finally, we explain these improvements: joint LLM-GP optimization through marginal likelihood implicitly performs contrastive learning, aligning representations to produce (1) better-structured embedding spaces, (2) improved uncertainty calibration, and (3) more efficient sampling - without requiring any external loss. This work provides both practical advances in sample-efficient optimization and insights into what makes effective Bayesian optimization.\n        △ Less",
        "authors":"['Bojana Ranković', 'Philippe Schwaller']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
        "abstract":"Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.\n        △ Less",
        "authors":"['Gleb Rodionov', 'Roman Garipov', 'Alina Shutova', 'George Yakushev', 'Vage Egiazarian', 'Anton Sinitsin', 'Denis Kuznedelev', 'Dan Alistarh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fractal and Regular Geometry of Deep Neural Networks",
        "abstract":"We study the geometric properties of random neural networks by investigating the boundary volumes of their excursion sets for different activation functions, as the depth increases. More specifically, we show that, for activations which are not very regular (e.g., the Heaviside step function), the boundary volumes exhibit fractal behavior, with their Hausdorff dimension monotonically increasing with the depth. On the other hand, for activations which are more regular (e.g., ReLU, logistic and $\\tanh$), as the depth increases, the expected boundary volumes can either converge to zero, remain constant or diverge exponentially, depending on a single spectral parameter which can be easily computed. Our theoretical results are confirmed in some numerical experiments based on Monte Carlo simulations.\n        △ Less",
        "authors":"['Simmaco Di Lillo', 'Domenico Marinucci', 'Michele Salvi', 'Stefano Vigogna']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Decentralized Federated Domain Generalization with Style Sharing: A Formal Modeling and Convergence Analysis",
        "abstract":"Much of the federated learning (FL) literature focuses on settings where local dataset statistics remain the same between training and testing time. Recent advances in domain generalization (DG) aim to use data from source (training) domains to train a model that generalizes well to data from unseen target (testing) domains. In this paper, we are motivated by two major gaps in existing work on FL and DG: (1) the lack of formal mathematical analysis of DG objectives and training processes; and (2) DG research in FL being limited to the conventional star-topology architecture. Addressing the second gap, we develop $\\textit{Decentralized Federated Domain Generalization with Style Sharing}$ ($\\texttt{StyleDDG}$), a fully decentralized DG algorithm designed to allow devices in a peer-to-peer network to achieve DG based on sharing style information inferred from their datasets. Additionally, we fill the first gap by providing the first systematic approach to mathematically analyzing style-based DG training optimization. We cast existing centralized DG algorithms within our framework, and employ their formalisms to model $\\texttt{StyleDDG}$. Based on this, we obtain analytical conditions under which a sub-linear convergence rate of $\\texttt{StyleDDG}$ can be obtained. Through experiments on two popular DG datasets, we demonstrate that $\\texttt{StyleDDG}$ can obtain significant improvements in accuracy across target domains with minimal added communication overhead compared to decentralized gradient methods that do not employ style sharing.\n        △ Less",
        "authors":"['Shahryar Zehtabi', 'Dong-Jun Han', 'Seyyedali Hosseinalipour', 'Christopher G. Brinton']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
        "abstract":"While decoder-only large language models (LLMs) have shown impressive results, encoder-decoder models are still widely adopted in real-world applications for their inference efficiency and richer encoder representation. In this paper, we study a novel problem: adapting pretrained decoder-only LLMs to encoder-decoder, with the goal of leveraging the strengths of both approaches to achieve a more favorable quality-efficiency trade-off. We argue that adaptation not only enables inheriting the capability of decoder-only LLMs but also reduces the demand for computation compared to pretraining from scratch. We rigorously explore different pretraining objectives and parameter initialization\/optimization techniques. Through extensive experiments based on Gemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to 1.6B), we demonstrate the effectiveness of adaptation and the advantage of encoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs achieve comparable (often better) pretraining performance but substantially better finetuning performance than their decoder-only counterpart. For example, Gemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning. Encoder-decoder adaptation also allows for flexible combination of different-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B by $>$3\\%. The adapted encoder representation also yields better results on SuperGLUE. We will release our checkpoints to facilitate future research.\n        △ Less",
        "authors":"['Biao Zhang', 'Fedor Moiseev', 'Joshua Ainslie', 'Paul Suganthan', 'Min Ma', 'Surya Bhupatiraju', 'Fede Lebron', 'Orhan Firat', 'Armand Joulin', 'Zhe Dong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs",
        "abstract":"The increasing adoption of web crawling opt-outs by copyright holders of online content raises critical questions about the impact of data compliance on large language model (LLM) performance. However, little is known about how these restrictions (and the resultant filtering of pretraining datasets) affect the capabilities of models trained using these corpora. In this work, we conceptualize this effect as the $\\textit{data compliance gap}$ (DCG), which quantifies the performance difference between models trained on datasets that comply with web crawling opt-outs, and those that do not. We measure the data compliance gap in two settings: pretraining models from scratch and continual pretraining from existing compliant models (simulating a setting where copyrighted data could be integrated later in pretraining). Our experiments with 1.5B models show that, as of January 2025, compliance with web data opt-outs does not degrade general knowledge acquisition (close to 0\\% DCG). However, in specialized domains such as biomedical research, excluding major publishers leads to performance declines. These findings suggest that while general-purpose LLMs can be trained to perform equally well using fully open data, performance in specialized domains may benefit from access to high-quality copyrighted sources later in training. Our study provides empirical insights into the long-debated trade-off between data compliance and downstream model performance, informing future discussions on AI training practices and policy decisions.\n        △ Less",
        "authors":"['Dongyang Fan', 'Vinko Sabolčec', 'Matin Ansaripour', 'Ayush Kumar Tarun', 'Martin Jaggi', 'Antoine Bosselut', 'Imanol Schlag']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From 128K to 4M: Efficient Training of Ultra-Long Context Large Language Models",
        "abstract":"Long-context capabilities are essential for a wide range of applications, including document and video understanding, in-context learning, and inference-time scaling, all of which require models to process and reason over long sequences of text and multimodal data. In this work, we introduce a efficient training recipe for building ultra-long context LLMs from aligned instruct model, pushing the boundaries of context lengths from 128K to 1M, 2M, and 4M tokens. Our approach leverages efficient continued pretraining strategies to extend the context window and employs effective instruction tuning to maintain the instruction-following and reasoning abilities. Our UltraLong-8B, built on Llama3.1-Instruct with our recipe, achieves state-of-the-art performance across a diverse set of long-context benchmarks. Importantly, models trained with our approach maintain competitive performance on standard benchmarks, demonstrating balanced improvements for both long and short context tasks. We further provide an in-depth analysis of key design choices, highlighting the impacts of scaling strategies and data composition. Our findings establish a robust framework for efficiently scaling context lengths while preserving general model capabilities. We release all model weights at: https:\/\/ultralong.github.io\/.\n        △ Less",
        "authors":"['Chejian Xu', 'Wei Ping', 'Peng Xu', 'Zihan Liu', 'Boxin Wang', 'Mohammad Shoeybi', 'Bo Li', 'Bryan Catanzaro']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"NNN: Next-Generation Neural Networks for Marketing Mix Modeling",
        "abstract":"We present NNN, a Transformer-based neural network approach to Marketing Mix Modeling (MMM) designed to address key limitations of traditional methods. Unlike conventional MMMs which rely on scalar inputs and parametric decay functions, NNN uses rich embeddings to capture both quantitative and qualitative aspects of marketing and organic channels (e.g., search queries, ad creatives). This, combined with its attention mechanism, enables NNN to model complex interactions, capture long-term effects, and potentially improve sales attribution accuracy. We show that L1 regularization permits the use of such expressive models in typical data-constrained settings. Evaluating NNN on simulated and real-world data demonstrates its efficacy, particularly through considerable improvement in predictive power. Beyond attribution, NNN provides valuable, complementary insights through model probing, such as evaluating keyword or creative effectiveness, enhancing model interpretability.\n        △ Less",
        "authors":"['Thomas Mulc', 'Mike Anderson', 'Paul Cubre', 'Huikun Zhang', 'Ivy Liu', 'Saket Kumar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Need for zkSpeed: Accelerating HyperPlonk for Zero-Knowledge Proofs",
        "abstract":"Zero-Knowledge Proofs (ZKPs) are rapidly gaining importance in privacy-preserving and verifiable computing. ZKPs enable a proving party to prove the truth of a statement to a verifying party without revealing anything else. ZKPs have applications in blockchain technologies, verifiable machine learning, and electronic voting, but have yet to see widespread adoption due to the computational complexity of the proving process. Recent works have accelerated the key primitives of state-of-the-art ZKP protocols on GPU and ASIC. However, the protocols accelerated thus far face one of two challenges: they either require a trusted setup for each application, or they generate larger proof sizes with higher verification costs, limiting their applicability in scenarios with numerous verifiers or strict verification time constraints. This work presents an accelerator, zkSpeed, for HyperPlonk, a state-of-the-art ZKP protocol that supports both one-time, universal setup and small proof sizes for typical ZKP applications in publicly verifiable, consensus-based systems. We accelerate the entire protocol, including two major primitives: SumCheck and Multi-scalar Multiplications (MSMs). We develop a full-chip architecture using 366.46 mm$^2$ and 2 TB\/s of bandwidth to accelerate the entire proof generation process, achieving geometric mean speedups of 801$\\times$ over CPU baselines.\n        △ Less",
        "authors":"['Alhad Daftardar', 'Jianqiao Mo', 'Joey Ah-kiow', 'Benedikt Bünz', 'Ramesh Karri', 'Siddharth Garg', 'Brandon Reagen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Work Capacity of Channels with Memory: Maximum Extractable Work in Percept-Action Loops",
        "abstract":"Predicting future observations plays a central role in machine learning, biology, economics, and many other fields. It lies at the heart of organizational principles such as the variational free energy principle and has even been shown -- based on the second law of thermodynamics -- to be necessary for reaching the fundamental energetic limits of sequential information processing. While the usefulness of the predictive paradigm is undisputed, complex adaptive systems that interact with their environment are more than just predictive machines: they have the power to act upon their environment and cause change. In this work, we develop a framework to analyze the thermodynamics of information processing in percept-action loops -- a model of agent-environment interaction -- allowing us to investigate the thermodynamic implications of actions and percepts on equal footing. To this end, we introduce the concept of work capacity -- the maximum rate at which an agent can expect to extract work from its environment. Our results reveal that neither of two previously established design principles for work-efficient agents -- maximizing predictive power and forgetting past actions -- remains optimal in environments where actions have observable consequences. Instead, a trade-off emerges: work-efficient agents must balance prediction and forgetting, as remembering past actions can reduce the available free energy. This highlights a fundamental departure from the thermodynamics of passive observation, suggesting that prediction and energy efficiency may be at odds in active learning systems.\n        △ Less",
        "authors":"['Lukas J. Fiderer', 'Paul C. Barth', 'Isaac D. Smith', 'Hans J. Briegel']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An experimental survey and Perspective View on Meta-Learning for Automated Algorithms Selection and Parametrization",
        "abstract":"Considerable progress has been made in the recent literature studies to tackle the Algorithms Selection and Parametrization (ASP) problem, which is diversified in multiple meta-learning setups. Yet there is a lack of surveys and comparative evaluations that critically analyze, summarize and assess the performance of existing methods. In this paper, we provide an overview of the state of the art in this continuously evolving field. The survey sheds light on the motivational reasons for pursuing classifiers selection through meta-learning. In this regard, Automated Machine Learning (AutoML) is usually treated as an ASP problem under the umbrella of the democratization of machine learning. Accordingly, AutoML makes machine learning techniques accessible to domain scientists who are interested in applying advanced analytics but lack the required expertise. It can ease the task of manually selecting ML algorithms and tuning related hyperparameters. We comprehensively discuss the different phases of classifiers selection based on a generic framework that is formed as an outcome of reviewing prior works. Subsequently, we propose a benchmark knowledge base of 4 millions previously learned models and present extensive comparative evaluations of the prominent methods for classifiers selection based on 08 classification algorithms and 400 benchmark datasets. The comparative study quantitatively assesses the performance of algorithms selection methods along while emphasizing the strengths and limitations of existing studies.\n        △ Less",
        "authors":"['Moncef Garouani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TxGemma: Efficient and Agentic LLMs for Therapeutics",
        "abstract":"Therapeutic development is a costly and high-risk endeavor that is often plagued by high failure rates. To address this, we introduce TxGemma, a suite of efficient, generalist large language models (LLMs) capable of therapeutic property prediction as well as interactive reasoning and explainability. Unlike task-specific models, TxGemma synthesizes information from diverse sources, enabling broad application across the therapeutic development pipeline. The suite includes 2B, 9B, and 27B parameter models, fine-tuned from Gemma-2 on a comprehensive dataset of small molecules, proteins, nucleic acids, diseases, and cell lines. Across 66 therapeutic development tasks, TxGemma achieved superior or comparable performance to the state-of-the-art generalist model on 64 (superior on 45), and against state-of-the-art specialist models on 50 (superior on 26). Fine-tuning TxGemma models on therapeutic downstream tasks, such as clinical trial adverse event prediction, requires less training data than fine-tuning base LLMs, making TxGemma suitable for data-limited applications. Beyond these predictive capabilities, TxGemma features conversational models that bridge the gap between general LLMs and specialized property predictors. These allow scientists to interact in natural language, provide mechanistic reasoning for predictions based on molecular structure, and engage in scientific discussions. Building on this, we further introduce Agentic-Tx, a generalist therapeutic agentic system powered by Gemini 2.5 that reasons, acts, manages diverse workflows, and acquires external domain knowledge. Agentic-Tx surpasses prior leading models on the Humanity's Last Exam benchmark (Chemistry & Biology) with 52.3% relative improvement over o3-mini (high) and 26.7% over o3-mini (high) on GPQA (Chemistry) and excels with improvements of 6.3% (ChemBench-Preference) and 2.4% (ChemBench-Mini) over o3-mini (high).\n        △ Less",
        "authors":"['Eric Wang', 'Samuel Schmidgall', 'Paul F. Jaeger', 'Fan Zhang', 'Rory Pilgrim', 'Yossi Matias', 'Joelle Barral', 'David Fleet', 'Shekoofeh Azizi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Heuristic Methods are Good Teachers to Distill MLPs for Graph Link Prediction",
        "abstract":"Link prediction is a crucial graph-learning task with applications including citation prediction and product recommendation. Distilling Graph Neural Networks (GNNs) teachers into Multi-Layer Perceptrons (MLPs) students has emerged as an effective approach to achieve strong performance and reducing computational cost by removing graph dependency. However, existing distillation methods only use standard GNNs and overlook alternative teachers such as specialized model for link prediction (GNN4LP) and heuristic methods (e.g., common neighbors). This paper first explores the impact of different teachers in GNN-to-MLP distillation. Surprisingly, we find that stronger teachers do not always produce stronger students: MLPs distilled from GNN4LP can underperform those distilled from simpler GNNs, while weaker heuristic methods can teach MLPs to near-GNN performance with drastically reduced training costs. Building on these insights, we propose Ensemble Heuristic-Distilled MLPs (EHDM), which eliminates graph dependencies while effectively integrating complementary signals via a gating mechanism. Experiments on ten datasets show an average 7.93% improvement over previous GNN-to-MLP approaches with 1.95-3.32 times less training time, indicating EHDM is an efficient and effective link prediction method.\n        △ Less",
        "authors":"['Zongyue Qin', 'Shichang Zhang', 'Mingxuan Ju', 'Tong Zhao', 'Neil Shah', 'Yizhou Sun']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"WoundAmbit: Bridging State-of-the-Art Semantic Segmentation and Real-World Wound Care",
        "abstract":"Chronic wounds affect a large population, particularly the elderly and diabetic patients, who often exhibit limited mobility and co-existing health conditions. Automated wound monitoring via mobile image capture can reduce in-person physician visits by enabling remote tracking of wound size. Semantic segmentation is key to this process, yet wound segmentation remains underrepresented in medical imaging research. To address this, we benchmark state-of-the-art deep learning models from general-purpose vision, medical imaging, and top methods from public wound challenges. For fair comparison, we standardize training, data augmentation, and evaluation, conducting cross-validationto minimize partitioning bias. We also assess real-world deployment aspects, including generalization to an out-of-distribution wound dataset, computational efficiency, and interpretability. Additionally, we propose a reference object-based approach to convert AI-generated masks into clinically relevant wound size estimates, and evaluate this, along with mask quality, for the best models based on physician assessments. Overall, the transformer-based TransNeXt showed the highest levels of generalizability. Despite variations in inference times, all models processed at least one image per second on the CPU, which is deemed adequate for the intended application. Interpretability analysis typically revealed prominent activations in wound regions, emphasizing focus on clinically relevant features. Expert evaluation showed high mask approval for all analyzed models, with VWFormer and ConvNeXtS backbone performing the best. Size retrieval accuracy was similar across models, and predictions closely matched expert annotations. Finally, we demonstrate how our AI-driven wound size estimation framework, WoundAmbit, can be integrated into a custom telehealth system. Our code will be made available on GitHub upon publication.\n        △ Less",
        "authors":"['Vanessa Borst', 'Timo Dittus', 'Tassilo Dege', 'Astrid Schmieder', 'Samuel Kounev']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Self-Supervised Framework for Space Object Behaviour Characterisation",
        "abstract":"Foundation Models, pre-trained on large unlabelled datasets before task-specific fine-tuning, are increasingly being applied to specialised domains. Recent examples include ClimaX for climate and Clay for satellite Earth observation, but a Foundation Model for Space Object Behavioural Analysis has not yet been developed. As orbital populations grow, automated methods for characterising space object behaviour are crucial for space safety. We present a Space Safety and Sustainability Foundation Model focusing on space object behavioural analysis using light curves (LCs). We implemented a Perceiver-Variational Autoencoder (VAE) architecture, pre-trained with self-supervised reconstruction and masked reconstruction on 227,000 LCs from the MMT-9 observatory. The VAE enables anomaly detection, motion prediction, and LC generation. We fine-tuned the model for anomaly detection & motion prediction using two independent LC simulators (CASSANDRA and GRIAL respectively), using CAD models of boxwing, Sentinel-3, SMOS, and Starlink platforms. Our pre-trained model achieved a reconstruction error of 0.01%, identifying potentially anomalous light curves through reconstruction difficulty. After fine-tuning, the model scored 88% and 82% accuracy, with 0.90 and 0.95 ROC AUC scores respectively in both anomaly detection and motion mode prediction (sun-pointing, spin, etc.). Analysis of high-confidence anomaly predictions on real data revealed distinct patterns including characteristic object profiles and satellite glinting. Here, we demonstrate how self-supervised learning can simultaneously enable anomaly detection, motion prediction, and synthetic data generation from rich representations learned in pre-training. Our work therefore supports space safety and sustainability through automated monitoring and simulation capabilities.\n        △ Less",
        "authors":"['Ian Groves', 'Andrew Campbell', 'James Fernandes', 'Diego Ramírez Rodríguez', 'Paul Murray', 'Massimiliano Vasile', 'Victoria Nockles']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Modality Sensing in mmWave Beamforming for Connected Vehicles Using Deep Learning",
        "abstract":"Beamforming techniques are considered as essential parts to compensate severe path losses in millimeter-wave (mmWave) communications. In particular, these techniques adopt large antenna arrays and formulate narrow beams to obtain satisfactory received powers. However, performing accurate beam alignment over narrow beams for efficient link configuration by traditional standard defined beam selection approaches, which mainly rely on channel state information and beam sweeping through exhaustive searching, imposes computational and communications overheads. And, such resulting overheads limit their potential use in vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communications involving highly dynamic scenarios. In comparison, utilizing out-of-band contextual information, such as sensing data obtained from sensor devices, provides a better alternative to reduce overheads. This paper presents a deep learning-based solution for utilizing the multi-modality sensing data for predicting the optimal beams having sufficient mmWave received powers so that the best V2I and V2V line-of-sight links can be ensured proactively. The proposed solution has been tested on real-world measured mmWave sensing and communication data, and the results show that it can achieve up to 98.19% accuracies while predicting top-13 beams. Correspondingly, when compared to existing been sweeping approach, the beam sweeping searching space and time overheads are greatly shortened roughly by 79.67% and 91.89%, respectively which confirm a promising solution for beamforming in mmWave enabled communications.\n        △ Less",
        "authors":"['Muhammad Baqer Mollah', 'Honggang Wang', 'Mohammad Ataul Karim', 'Hua Fang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups",
        "abstract":"Large Language Models (LLMs) have been shown to demonstrate imbalanced biases against certain groups. However, the study of unprovoked targeted attacks by LLMs towards at-risk populations remains underexplored. Our paper presents three novel contributions: (1) the explicit evaluation of LLM-generated attacks on highly vulnerable mental health groups; (2) a network-based framework to study the propagation of relative biases; and (3) an assessment of the relative degree of stigmatization that emerges from these attacks. Our analysis of a recently released large-scale bias audit dataset reveals that mental health entities occupy central positions within attack narrative networks, as revealed by a significantly higher mean centrality of closeness (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7). Drawing from sociological foundations of stigmatization theory, our stigmatization analysis indicates increased labeling components for mental health disorder-related targets relative to initial targets in generation chains. Taken together, these insights shed light on the structural predilections of large language models to heighten harmful discourse and highlight the need for suitable approaches for mitigation.\n        △ Less",
        "authors":"['Rijul Magu', 'Arka Dutta', 'Sean Kim', 'Ashiqur R. KhudaBukhsh', 'Munmun De Choudhury']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hall Effect Thruster Forecasting using a Topological Approach for Data Assimilation",
        "abstract":"Hall Effect Thrusters (HETs) are electric thrusters that eject heavy ionized gas particles from the spacecraft to generate thrust. Although traditionally they were used for station keeping, recently They have been used for interplanetary space missions due to their high delta-V potential and their operational longevity in contrast to other thrusters, e.g., chemical. However, the operation of HETs involves complex processes such as ionization of gases, strong magnetic fields, and complicated solar panel power supply interactions. Therefore, their operation is extremely difficult to model thus necessitating Data Assimilation (DA) approaches for estimating and predicting their operational states. Because HET's operating environment is often noisy with non-Gaussian sources, this significantly limits applicable DA tools. We describe a topological approach for data assimilation that bypasses these limitations that does not depend on the noise model, and utilize it to forecast spatiotemporal plume field states of HETs. Our approach is a generalization of the Topological Approach for Data Assimilation (TADA) method that allows including different forecast functions. We show how TADA can be combined with the Long Short-Term Memory network for accurate forecasting. We then apply our approach to high-fidelity Hall Effect Thruster (HET) simulation data from the Air Force Research Laboratory (AFRL) rocket propulsion division where we demonstrate the forecast resiliency of TADA on noise contaminated, high-dimensional data.\n        △ Less",
        "authors":"['Max M. Chumley', 'Firas A. Khasawneh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adversarial Training of Reward Models",
        "abstract":"Reward modeling has emerged as a promising approach for the scalable alignment of language models. However, contemporary reward models (RMs) often lack robustness, awarding high rewards to low-quality, out-of-distribution (OOD) samples. This can lead to reward hacking, where policies exploit unintended shortcuts to maximize rewards, undermining alignment. To address this challenge, we introduce Adv-RM, a novel adversarial training framework that automatically identifies adversarial examples -- responses that receive high rewards from the target RM but are OOD and of low quality. By leveraging reinforcement learning, Adv-RM trains a policy to generate adversarial examples that reliably expose vulnerabilities in large state-of-the-art reward models such as Nemotron 340B RM. Incorporating these adversarial examples into the reward training process improves the robustness of RMs, mitigating reward hacking and enhancing downstream performance in RLHF. We demonstrate that Adv-RM significantly outperforms conventional RM training, increasing stability and enabling more effective RLHF training in both synthetic and real-data settings.\n        △ Less",
        "authors":"['Alexander Bukharin', 'Haifeng Qian', 'Shengyang Sun', 'Adithya Renduchintala', 'Soumye Singhal', 'Zhilin Wang', 'Oleksii Kuchaiev', 'Olivier Delalleau', 'Tuo Zhao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accelerating Vehicle Routing via AI-Initialized Genetic Algorithms",
        "abstract":"Vehicle Routing Problems (VRP) are an extension of the Traveling Salesperson Problem and are a fundamental NP-hard challenge in combinatorial optimization. Solving VRP in real-time at large scale has become critical in numerous applications, from growing markets like last-mile delivery to emerging use-cases like interactive logistics planning. Such applications involve solving similar problem instances repeatedly, yet current state-of-the-art solvers treat each instance on its own without leveraging previous examples. We introduce a novel optimization framework that uses a reinforcement learning agent - trained on prior instances - to quickly generate initial solutions, which are then further optimized by genetic algorithms. Our framework, Evolutionary Algorithm with Reinforcement Learning Initialization (EARLI), consistently outperforms current state-of-the-art solvers across various time scales. For example, EARLI handles vehicle routing with 500 locations within 1s, 10x faster than current solvers for the same solution quality, enabling applications like real-time and interactive routing. EARLI can generalize to new data, as demonstrated on real e-commerce delivery data of a previously unseen city. Our hybrid framework presents a new way to combine reinforcement learning and genetic algorithms, paving the road for closer interdisciplinary collaboration between AI and optimization communities towards real-time optimization in diverse domains.\n        △ Less",
        "authors":"['Ido Greenberg', 'Piotr Sielski', 'Hugo Linsenmaier', 'Rajesh Gandham', 'Shie Mannor', 'Alex Fender', 'Gal Chechik', 'Eli Meirom']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robo-taxi Fleet Coordination at Scale via Reinforcement Learning",
        "abstract":"Fleets of robo-taxis offering on-demand transportation services, commonly known as Autonomous Mobility-on-Demand (AMoD) systems, hold significant promise for societal benefits, such as reducing pollution, energy consumption, and urban congestion. However, orchestrating these systems at scale remains a critical challenge, with existing coordination algorithms often failing to exploit the systems' full potential. This work introduces a novel decision-making framework that unites mathematical modeling with data-driven techniques. In particular, we present the AMoD coordination problem through the lens of reinforcement learning and propose a graph network-based framework that exploits the main strengths of graph representation learning, reinforcement learning, and classical operations research tools. Extensive evaluations across diverse simulation fidelities and scenarios demonstrate the flexibility of our approach, achieving superior system performance, computational efficiency, and generalizability compared to prior methods. Finally, motivated by the need to democratize research efforts in this area, we release publicly available benchmarks, datasets, and simulators for network-level coordination alongside an open-source codebase designed to provide accessible simulation platforms and establish a standardized validation process for comparing methodologies. Code available at: https:\/\/github.com\/StanfordASL\/RL4AMOD\n        △ Less",
        "authors":"['Luigi Tresca', 'Carolin Schmidt', 'James Harrison', 'Filipe Rodrigues', 'Gioele Zardini', 'Daniele Gammelli', 'Marco Pavone']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Axis-Aligned Subspaces for High-Dimensional Bayesian Optimization with Group Testing",
        "abstract":"Bayesian optimization (BO ) is an effective method for optimizing expensive-to-evaluate black-box functions. While high-dimensional problems can be particularly challenging, due to the multitude of parameter choices and the potentially high number of data points required to fit the model, this limitation can be addressed if the problem satisfies simplifying assumptions. Axis-aligned subspace approaches, where few dimensions have a significant impact on the objective, motivated several algorithms for high-dimensional BO . However, the validity of this assumption is rarely verified, and the assumption is rarely exploited to its full extent. We propose a group testing ( GT) approach to identify active variables to facilitate efficient optimization in these domains. The proposed algorithm, Group Testing Bayesian Optimization (GTBO), first runs a testing phase where groups of variables are systematically selected and tested on whether they influence the objective, then terminates once active dimensions are identified. To that end, we extend the well-established GT theory to functions over continuous domains. In the second phase, GTBO guides optimization by placing more importance on the active dimensions. By leveraging the axis-aligned subspace assumption, GTBO outperforms state-of-the-art methods on benchmarks satisfying the assumption of axis-aligned subspaces, while offering improved interpretability.\n        △ Less",
        "authors":"['Erik Hellsten', 'Carl Hvarfner', 'Leonard Papenmeier', 'Luigi Nardi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Uncertainty-Aware Hybrid Machine Learning in Virtual Sensors for Vehicle Sideslip Angle Estimation",
        "abstract":"Precise vehicle state estimation is crucial for safe and reliable autonomous driving. The number of measurable states and their precision offered by the onboard vehicle sensor system are often constrained by cost. For instance, measuring critical quantities such as the Vehicle Sideslip Angle (VSA) poses significant commercial challenges using current optical sensors. This paper addresses these limitations by focusing on the development of high-performance virtual sensors to enhance vehicle state estimation for active safety. The proposed Uncertainty-Aware Hybrid Learning (UAHL) architecture integrates a machine learning model with vehicle motion models to estimate VSA directly from onboard sensor data. A key aspect of the UAHL architecture is its focus on uncertainty quantification for individual model estimates and hybrid fusion. These mechanisms enable the dynamic weighting of uncertainty-aware predictions from machine learning and vehicle motion models to produce accurate and reliable hybrid VSA estimates. This work also presents a novel dataset named Real-world Vehicle State Estimation Dataset (ReV-StED), comprising synchronized measurements from advanced vehicle dynamic sensors. The experimental results demonstrate the superior performance of the proposed method for VSA estimation, highlighting UAHL as a promising architecture for advancing virtual sensors and enhancing active safety in autonomous vehicles.\n        △ Less",
        "authors":"['Abinav Kalyanasundaram', 'Karthikeyan Chandra Sekaran', 'Philipp Stauber', 'Michael Lange', 'Wolfgang Utschick', 'Michael Botsch']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Varroa destructor mite detection using a narrow spectra illumination",
        "abstract":"This paper focuses on the development and modification of a beehive monitoring device and Varroa destructor detection on the bees with the help of hyperspectral imagery while utilizing a U-net, semantic segmentation architecture, and conventional computer vision methods. The main objectives were to collect a dataset of bees and mites, and propose the computer vision model which can achieve the detection between bees and mites.\n        △ Less",
        "authors":"['Samuel Bielik', 'Simon Bilik']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Nonuniform-Tensor-Parallelism: Mitigating GPU failure impact for Scaled-up LLM Training",
        "abstract":"LLM training is scaled up to 10Ks of GPUs by a mix of data-(DP) and model-parallel (MP) execution. Critical to achieving efficiency is tensor-parallel (TP; a form of MP) execution within tightly-coupled subsets of GPUs, referred to as a scale-up domain, and the larger the scale-up domain the better the performance. New datacenter architectures are emerging with more GPUs able to be tightly-coupled in a scale-up domain, such as moving from 8 GPUs to 72 GPUs connected via NVLink. Unfortunately, larger scale-up domains increase the blast-radius of failures, with a failure of single GPU potentially impacting TP execution on the full scale-up domain, which can degrade overall LLM training throughput dramatically. With as few as 0.1% of GPUs being in a failed state, a high TP-degree job can experience nearly 10% reduction in LLM training throughput. We propose nonuniform-tensor-parallelism (NTP) to mitigate this amplified impact of GPU failures. In NTP, a DP replica that experiences GPU failures operates at a reduced TP degree, contributing throughput equal to the percentage of still-functional GPUs. We also propose a rack-design with improved electrical and thermal capabilities in order to sustain power-boosting of scale-up domains that have experienced failures; combined with NTP, this can allow the DP replica with the reduced TP degree (i.e., with failed GPUs) to keep up with the others, thereby achieving near-zero throughput loss for large-scale LLM training.\n        △ Less",
        "authors":"['Daiyaan Arfeen', 'Dheevatsa Mudigere', 'Ankit More', 'Bhargava Gopireddy', 'Ahmet Inci', 'Gregory R. Ganger']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MCAT: Visual Query-Based Localization of Standard Anatomical Clips in Fetal Ultrasound Videos Using Multi-Tier Class-Aware Token Transformer",
        "abstract":"Accurate standard plane acquisition in fetal ultrasound (US) videos is crucial for fetal growth assessment, anomaly detection, and adherence to clinical guidelines. However, manually selecting standard frames is time-consuming and prone to intra- and inter-sonographer variability. Existing methods primarily rely on image-based approaches that capture standard frames and then classify the input frames across different anatomies. This ignores the dynamic nature of video acquisition and its interpretation. To address these challenges, we introduce Multi-Tier Class-Aware Token Transformer (MCAT), a visual query-based video clip localization (VQ-VCL) method, to assist sonographers by enabling them to capture a quick US sweep. By then providing a visual query of the anatomy they wish to analyze, MCAT returns the video clip containing the standard frames for that anatomy, facilitating thorough screening for potential anomalies. We evaluate MCAT on two ultrasound video datasets and a natural image VQ-VCL dataset based on Ego4D. Our model outperforms state-of-the-art methods by 10% and 13% mIoU on the ultrasound datasets and by 5.35% mIoU on the Ego4D dataset, using 96% fewer tokens. MCAT's efficiency and accuracy have significant potential implications for public health, especially in low- and middle-income countries (LMICs), where it may enhance prenatal care by streamlining standard plane acquisition, simplifying US-based screening, diagnosis and allowing sonographers to examine more patients.\n        △ Less",
        "authors":"['Divyanshu Mishra', 'Pramit Saha', 'He Zhao', 'Netzahualcoyotl Hernandez-Cruz', 'Olga Patey', 'Aris Papageorghiou', 'J. Alison Noble']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accurate Ab-initio Neural-network Solutions to Large-Scale Electronic Structure Problems",
        "abstract":"We present finite-range embeddings (FiRE), a novel wave function ansatz for accurate large-scale ab-initio electronic structure calculations. Compared to contemporary neural-network wave functions, FiRE reduces the asymptotic complexity of neural-network variational Monte Carlo (NN-VMC) by $\\sim n_\\text{el}$, the number of electrons. By restricting electron-electron interactions within the neural network, FiRE accelerates all key operations -- sampling, pseudopotentials, and Laplacian computations -- resulting in a real-world $10\\times$ acceleration in now-feasible 180-electron calculations. We validate our method's accuracy on various challenging systems, including biochemical compounds, conjugated hydrocarbons, and organometallic compounds. On these systems, FiRE's energies are consistently within chemical accuracy of the most reliable data, including experiments, even in cases where high-accuracy methods such as CCSD(T), AFQMC, or contemporary NN-VMC fall short. With these improvements in both runtime and accuracy, FiRE represents a new `gold-standard' method for fast and accurate large-scale ab-initio calculations, potentially enabling new computational studies in fields like quantum chemistry, solid-state physics, and material design.\n        △ Less",
        "authors":"['Michael Scherbela', 'Nicholas Gao', 'Philipp Grohs', 'Stephan Günnemann']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Collaborative Prediction: Tractable Information Aggregation via Agreement",
        "abstract":"We give efficient \"collaboration protocols\" through which two parties, who observe different features about the same instances, can interact to arrive at predictions that are more accurate than either could have obtained on their own. The parties only need to iteratively share and update their own label predictions-without either party ever having to share the actual features that they observe. Our protocols are efficient reductions to the problem of learning on each party's feature space alone, and so can be used even in settings in which each party's feature space is illegible to the other-which arises in models of human\/AI interaction and in multi-modal learning. The communication requirements of our protocols are independent of the dimensionality of the data. In an online adversarial setting we show how to give regret bounds on the predictions that the parties arrive at with respect to a class of benchmark policies defined on the joint feature space of the two parties, despite the fact that neither party has access to this joint feature space. We also give simpler algorithms for the same task in the batch setting in which we assume that there is a fixed but unknown data distribution. We generalize our protocols to a decision theoretic setting with high dimensional outcome spaces, where parties communicate only \"best response actions.\"\n  Our theorems give a computationally and statistically tractable generalization of past work on information aggregation amongst Bayesians who share a common and correct prior, as part of a literature studying \"agreement\" in the style of Aumann's agreement theorem. Our results require no knowledge of (or even the existence of) a prior distribution and are computationally efficient. Nevertheless we show how to lift our theorems back to this classical Bayesian setting, and in doing so, give new information aggregation theorems for Bayesian agreement.\n        △ Less",
        "authors":"['Natalie Collina', 'Ira Globus-Harris', 'Surbhi Goel', 'Varun Gupta', 'Aaron Roth', 'Mirah Shi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PINP: Physics-Informed Neural Predictor with latent estimation of fluid flows",
        "abstract":"Accurately predicting fluid dynamics and evolution has been a long-standing challenge in physical sciences. Conventional deep learning methods often rely on the nonlinear modeling capabilities of neural networks to establish mappings between past and future states, overlooking the fluid dynamics, or only modeling the velocity field, neglecting the coupling of multiple physical quantities. In this paper, we propose a new physics-informed learning approach that incorporates coupled physical quantities into the prediction process to assist with forecasting. Central to our method lies in the discretization of physical equations, which are directly integrated into the model architecture and loss function. This integration enables the model to provide robust, long-term future predictions. By incorporating physical equations, our model demonstrates temporal extrapolation and spatial generalization capabilities. Experimental results show that our approach achieves the state-of-the-art performance in spatiotemporal prediction across both numerical simulations and real-world extreme-precipitation nowcasting benchmarks.\n        △ Less",
        "authors":"['Huaguan Chen', 'Yang Liu', 'Hao Sun']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Explainable AI for building energy retrofitting under data scarcity",
        "abstract":"Enhancing energy efficiency in residential buildings is a crucial step toward mitigating climate change and reducing greenhouse gas emissions. Retrofitting existing buildings, which account for a significant portion of energy consumption, is critical particularly in regions with outdated and inefficient building stocks. This study presents an Artificial Intelligence (AI) and Machine Learning (ML)-based framework to recommend energy efficiency measures for residential buildings, leveraging accessible building characteristics to achieve energy class targets. Using Latvia as a case study, the methodology addresses challenges associated with limited datasets, class imbalance and data scarcity. The proposed approach integrates Conditional Tabular Generative Adversarial Networks (CTGAN) to generate synthetic data, enriching and balancing the dataset. A Multi-Layer Perceptron (MLP) model serves as the predictive model performing multi-label classification to predict appropriate retrofit strategies. Explainable Artificial Intelligence (XAI), specifically SHapley Additive exPlanations (SHAP), ensures transparency and trust by identifying key features that influence recommendations and guiding feature engineering choices for improved reliability and performance. The evaluation of the approach shows that it notably overcomes data limitations, achieving improvements up to 54% in precision, recall and F1 score. Although this study focuses on Latvia, the methodology is adaptable to other regions, underscoring the potential of AI in reducing the complexity and cost of building energy retrofitting overcoming data limitations. By facilitating decision-making processes and promoting stakeholders engagement, this work supports the global transition toward sustainable energy use in the residential building sector.\n        △ Less",
        "authors":"['Panagiota Rempi', 'Sotiris Pelekis', 'Alexandros Menelaos Tzortzis', 'Evangelos Karakolis', 'Christos Ntanos', 'Dimitris Askounis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Trust-Region Twisted Policy Improvement",
        "abstract":"Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep reinforcement learning (RL). However, scaling MCTS to parallel compute has proven challenging in practice which has motivated alternative planners like sequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters for smoothing through a reformulation of RL as a policy inference problem. Yet, persisting design choices of these particle filters often conflict with the aim of online planning in RL, which is to obtain a policy improvement at the start of planning. Drawing inspiration from MCTS, we tailor SMC planners specifically for RL by improving data generation within the planner through constrained action sampling and explicit terminal state handling, as well as improving policy and value target estimation. This leads to our Trust-Region Twisted SMC (TRT-SMC), which shows improved runtime and sample-efficiency over baseline MCTS and SMC methods in both discrete and continuous domains.\n        △ Less",
        "authors":"['Joery A. de Vries', 'Jinke He', 'Yaniv Oren', 'Matthijs T. J. Spaan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhanced Anomaly Detection for Capsule Endoscopy Using Ensemble Learning Strategies",
        "abstract":"Capsule endoscopy is a method to capture images of the gastrointestinal tract and screen for diseases which might remain hidden if investigated with standard endoscopes. Due to the limited size of a video capsule, embedding AI models directly into the capsule demands careful consideration of the model size and thus complicates anomaly detection in this field. Furthermore, the scarcity of available data in this domain poses an ongoing challenge to achieving effective anomaly detection. Thus, this work introduces an ensemble strategy to address this challenge in anomaly detection tasks in video capsule endoscopies, requiring only a small number of individual neural networks during both the training and inference phases. Ensemble learning combines the predictions of multiple independently trained neural networks. This has shown to be highly effective in enhancing both the accuracy and robustness of machine learning models. However, this comes at the cost of higher memory usage and increased computational effort, which quickly becomes prohibitive in many real-world applications. Instead of applying the same training algorithm to each individual network, we propose using various loss functions, drawn from the anomaly detection field, to train each network. The methods are validated on the two largest publicly available datasets for video capsule endoscopy images, the Galar and the Kvasir-Capsule dataset. We achieve an AUC score of 76.86% on the Kvasir-Capsule and an AUC score of 76.98% on the Galar dataset. Our approach outperforms current baselines with significantly fewer parameters across all models, which is a crucial step towards incorporating artificial intelligence into capsule endoscopies.\n        △ Less",
        "authors":"['Julia Werner', 'Christoph Gerum', 'Jorg Nick', 'Maxime Le Floch', 'Franz Brinkmann', 'Jochen Hampe', 'Oliver Bringmann']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Confidence Regularized Masked Language Modeling using Text Length",
        "abstract":"Masked language modeling is a widely used method for learning language representations, where the model predicts a randomly masked word in each input. However, this approach typically considers only a single correct answer during training, ignoring the variety of plausible alternatives that humans might choose. This issue becomes more pronounced when the input text is short, as the possible word distribution tends to have higher entropy, potentially causing the model to become overconfident in its predictions. To mitigate this, we propose a novel confidence regularizer that adaptively adjusts the regularization strength based on the input length. Experiments on the GLUE and SQuAD benchmarks show that our method improves both accuracy and expected calibration error\n        △ Less",
        "authors":"['Seunghyun Ji', 'Soowon Lee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Information-Theoretic Reward Decomposition for Generalizable RLHF",
        "abstract":"A generalizable reward model is crucial in Reinforcement Learning from Human Feedback (RLHF) as it enables correctly evaluating unseen prompt-response pairs. However, existing reward models lack this ability, as they are typically trained by increasing the reward gap between chosen and rejected responses, while overlooking the prompts that the responses are conditioned on. Consequently, when the trained reward model is evaluated on prompt-response pairs that lie outside the data distribution, neglecting the effect of prompts may result in poor generalization of the reward model. To address this issue, we decompose the reward value into two independent components: prompt-free reward and prompt-related reward. Prompt-free reward represents the evaluation that is determined only by responses, while the prompt-related reward reflects the reward that derives from both the prompt and the response. We extract these two components from an information-theoretic perspective, which requires no extra models. Subsequently, we propose a new reward learning algorithm by prioritizing data samples based on their prompt-free reward values. Through toy examples, we demonstrate that the extracted prompt-free and prompt-related rewards effectively characterize two parts of the reward model. Further, standard evaluations show that our method improves both the alignment performance and the generalization capability of the reward model.\n        △ Less",
        "authors":"['Liyuan Mao', 'Haoran Xu', 'Amy Zhang', 'Weinan Zhang', 'Chenjia Bai']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust Statistics vs. Machine Learning vs. Bayesian Inference: Insights into Handling Faulty GNSS Measurements in Field Robotics",
        "abstract":"This paper presents research findings on handling faulty measurements (i.e., outliers) of global navigation satellite systems (GNSS) for robot localization under adverse signal conditions in field applications, where raw GNSS data are frequently corrupted due to environmental interference such as multipath, signal blockage, or non-line-of-sight conditions. In this context, we investigate three strategies applied specifically to GNSS pseudorange observations: robust statistics for error mitigation, machine learning for faulty measurement prediction, and Bayesian inference for noise distribution approximation. Since previous studies have provided limited insight into the theoretical foundations and practical evaluations of these three methodologies within a unified problem statement (i.e., state estimation using ranging sensors), we conduct extensive experiments using real-world sensor data collected in diverse urban environments. Our goal is to examine both established techniques and newly proposed methods, thereby advancing the understanding of how to handle faulty range measurements, such as GNSS, for robust, long-term robot localization. In addition to presenting successful results, this work highlights critical observations and open questions to motivate future research in robust state estimation.\n        △ Less",
        "authors":"['Haoming Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optuna vs Code Llama: Are LLMs a New Paradigm for Hyperparameter Tuning?",
        "abstract":"Optimal hyperparameter selection is critical for maximizing neural network performance, especially as models grow in complexity. This work investigates the viability of leveraging large language models (LLMs) for hyperparameter optimization by fine-tuning a parameter-efficient version of Code Llama using LoRA. The adapted LLM is capable of generating accurate and efficient hyperparameter recommendations tailored to diverse neural network architectures. Unlike traditional approaches such as Optuna, which rely on computationally intensive trial-and-error procedures, our method achieves competitive or superior results in terms of Root Mean Square Error (RMSE) while significantly reducing computational overhead. Our findings demonstrate that LLM-based optimization not only matches the performance of state-of-the-art techniques like Tree-structured Parzen Estimators (TPE) but also substantially accelerates the tuning process. This positions LLMs as a promising alternative for rapid experimentation, particularly in resource-constrained environments such as edge devices and mobile platforms, where computational efficiency is essential. In addition to improved efficiency, the method offers time savings and consistent performance across various tasks, highlighting its robustness and generalizability. All generated hyperparameters are included in the LEMUR Neural Network (NN) Dataset, which is publicly available and serves as an open-source benchmark for hyperparameter optimization research.\n        △ Less",
        "authors":"['Roman Kochnev', 'Arash Torabi Goodarzi', 'Zofia Antonina Bentyn', 'Dmitry Ignatov', 'Radu Timofte']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning-enhanced electronic skin for tactile sensing on deformable surface based on electrical impedance tomography",
        "abstract":"Electrical Impedance Tomography (EIT)-based tactile sensors offer cost-effective and scalable solutions for robotic sensing, especially promising for soft robots. However a major issue of EIT-based tactile sensors when applied in highly deformable objects is their performance degradation due to surface deformations. This limitation stems from their inherent sensitivity to strain, which is particularly exacerbated in soft bodies, thus requiring dedicated data interpretation to disentangle the parameter being measured and the signal deriving from shape changes. This has largely limited their practical implementations. This paper presents a machine learning-assisted tactile sensing approach to address this challenge by tracking surface deformations and segregating this contribution in the signal readout during tactile sensing. We first capture the deformations of the target object, followed by tactile reconstruction using a deep learning model specifically designed to process and fuse EIT data and deformation information. Validations using numerical simulations achieved high correlation coefficients (0.9660 - 0.9999), peak signal-to-noise ratios (28.7221 - 55.5264 dB) and low relative image errors (0.0107 - 0.0805). Experimental validations, using a hydrogel-based EIT e-skin under various deformation scenarios, further demonstrated the effectiveness of the proposed approach in real-world settings. The findings could underpin enhanced tactile interaction in soft and highly deformable robotic applications.\n        △ Less",
        "authors":"['Huazhi Dong', 'Xiaopeng Wu', 'Delin Hu', 'Zhe Liu', 'Francesco Giorgio-Serchi', 'Yunjie Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Smart Exploration in Reinforcement Learning using Bounded Uncertainty Models",
        "abstract":"Reinforcement learning (RL) is a powerful tool for decision-making in uncertain environments, but it often requires large amounts of data to learn an optimal policy. We propose using prior model knowledge to guide the exploration process to speed up this learning process. This model knowledge comes in the form of a model set to which the true transition kernel and reward function belong. We optimize over this model set to obtain upper and lower bounds on the Q-function, which are then used to guide the exploration of the agent. We provide theoretical guarantees on the convergence of the Q-function to the optimal Q-function under the proposed class of exploring policies. Furthermore, we also introduce a data-driven regularized version of the model set optimization problem that ensures the convergence of the class of exploring policies to the optimal policy. Lastly, we show that when the model set has a specific structure, namely the bounded-parameter MDP (BMDP) framework, the regularized model set optimization problem becomes convex and simple to implement. In this setting, we also show that we obtain finite-time convergence to the optimal policy under additional assumptions. We demonstrate the effectiveness of the proposed exploration strategy in a simulation study. The results indicate that the proposed method can significantly speed up the learning process in reinforcement learning.\n        △ Less",
        "authors":"['J. S. van Hulst', 'W. P. M. H. Heemels', 'D. J. Antunes']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MLPROP -- an open interactive web interface for thermophysical property prediction with machine learning",
        "abstract":"Machine learning (ML) enables the development of powerful methods for predicting thermophysical properties with unprecedented scope and accuracy. However, technical barriers like cumbersome implementation in established workflows hinder their application in practice. With MLPROP, we provide an interactive web interface for directly applying advanced ML methods to predict thermophysical properties without requiring ML expertise, thereby substantially increasing the accessibility of novel models. MLPROP currently includes models for predicting the vapor pressure of pure components (GRAPPA), activity coefficients and vapor-liquid equilibria in binary mixtures (UNIFAC 2.0, mod. UNIFAC 2.0, and HANNA), and a routine to fit NRTL parameters to the model predictions. MLPROP will be continuously updated and extended and is accessible free of charge via https:\/\/ml-prop.mv.rptu.de\/. MLPROP removes the barrier to learning and experimenting with new ML-based methods for predicting thermophysical properties. The source code of all models is available as open source, which allows integration into existing workflows.\n        △ Less",
        "authors":"['Marco Hoffmann', 'Thomas Specht', 'Nicolas Hayer', 'Hans Hasse', 'Fabian Jirasek']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Autoencoder-Based Detection of Anomalous Stokes V Spectra in the Flare-Producing Active Region 13663 Using Hinode\/SP Observations",
        "abstract":"Detecting unusual signals in observational solar spectra is crucial for understanding the features associated with impactful solar events, such as solar flares. However, existing spectral analysis techniques face challenges, particularly when relying on pre-defined, physics-based calculations to process large volumes of noisy and complex observational data. To address these limitations, we applied deep learning to detect anomalies in the Stokes V spectra from the Hinode\/SP instrument. Specifically, we developed an autoencoder model for spectral compression, which serves as an anomaly detection method. Our model effectively identifies anomalous spectra within spectro-polarimetric maps captured prior to the onset of the X1.3 flare on May 5, 2024, in NOAA AR 13663. These atypical spectral points exhibit highly complex profiles and spatially align with polarity inversion lines in magnetogram images, indicating their potential as sites of magnetic energy storage and possible triggers for flares. Notably, the detected anomalies are highly localized, making them particularly challenging to identify in magnetogram images using current manual methods.\n        △ Less",
        "authors":"['Jargalmaa Batmunkh', 'Yusuke Iida', 'Takayoshi Oba']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Drought forecasting using a hybrid neural architecture for integrating time series and static data",
        "abstract":"Reliable forecasting is critical for early warning systems and adaptive drought management. Most previous deep learning approaches focus solely on homogeneous regions and rely on single-structured data. This paper presents a hybrid neural architecture that integrates time series and static data, achieving state-of-the-art performance on the DroughtED dataset. Our results illustrate the potential of designing neural models for the treatment of heterogeneous data in climate related tasks and present reliable prediction of USDM categories, an expert-informed drought metric. Furthermore, this work validates the potential of DroughtED for enabling location-agnostic training of deep learning models.\n        △ Less",
        "authors":"['Julian Agudelo', 'Vincent Guigue', 'Cristina Manfredotti', 'Hadrien Piot']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Unsupervised Location Mapping for Narrative Corpora",
        "abstract":"This work presents the task of unsupervised location mapping, which seeks to map the trajectory of an individual narrative on a spatial map of locations in which a large set of narratives take place. Despite the fundamentality and generality of the task, very little work addressed the spatial mapping of narrative texts. The task consists of two parts: (1) inducing a ``map'' with the locations mentioned in a set of texts, and (2) extracting a trajectory from a single narrative and positioning it on the map. Following recent advances in increasing the context length of large language models, we propose a pipeline for this task in a completely unsupervised manner without predefining the set of labels. We test our method on two different domains: (1) Holocaust testimonies and (2) Lake District writing, namely multi-century literature on travels in the English Lake District. We perform both intrinsic and extrinsic evaluations for the task, with encouraging results, thereby setting a benchmark and evaluation practices for the task, as well as highlighting challenges.\n        △ Less",
        "authors":"['Eitan Wagner', 'Renana Keydar', 'Omri Abend']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AEGIS: Human Attention-based Explainable Guidance for Intelligent Vehicle Systems",
        "abstract":"Improving decision-making capabilities in Autonomous Intelligent Vehicles (AIVs) has been a heated topic in recent years. Despite advancements, training machines to capture regions of interest for comprehensive scene understanding, like human perception and reasoning, remains a significant challenge. This study introduces a novel framework, Human Attention-based Explainable Guidance for Intelligent Vehicle Systems (AEGIS). AEGIS utilizes human attention, converted from eye-tracking, to guide reinforcement learning (RL) models to identify critical regions of interest for decision-making. AEGIS uses a pre-trained human attention model to guide RL models to identify critical regions of interest for decision-making. By collecting 1.2 million frames from 20 participants across six scenarios, AEGIS pre-trains a model to predict human attention patterns.\n        △ Less",
        "authors":"['Zhuoli Zhuang', 'Cheng-You Lu', 'Yu-Cheng Fred Chang', 'Yu-Kai Wang', 'Thomas Do', 'Chin-Teng Lin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CKGAN: Training Generative Adversarial Networks Using Characteristic Kernel Integral Probability Metrics",
        "abstract":"In this paper, we propose CKGAN, a novel generative adversarial network (GAN) variant based on an integral probability metrics framework with characteristic kernel (CKIPM). CKIPM, as a distance between two probability distributions, is designed to optimize the lowerbound of the maximum mean discrepancy (MMD) in a reproducing kernel Hilbert space, and thus can be used to train GANs. CKGAN mitigates the notorious problem of mode collapse by mapping the generated images back to random noise. To save the effort of selecting the kernel function manually, we propose a soft selection method to automatically learn a characteristic kernel function. The experimental evaluation conducted on a set of synthetic and real image benchmarks (MNIST, CelebA, etc.) demonstrates that CKGAN generally outperforms other MMD-based GANs. The results also show that at the cost of moderately more training time, the automatically selected kernel function delivers very close performance to the best of manually fine-tuned one on real image benchmarks and is able to improve the performances of other MMD-based GANs.\n        △ Less",
        "authors":"['Kuntian Zhang', 'Simin Yu', 'Yaoshu Wang', 'Makoto Onizuka', 'Chuan Xiao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evaluation of the impact of expert knowledge: How decision support scores impact the effectiveness of automatic knowledge-driven feature engineering (aKDFE)",
        "abstract":"Adverse Drug Events (ADEs), harmful medication effects, pose significant healthcare challenges, impacting patient safety and costs. This study evaluates automatic Knowledge-Driven Feature Engineering (aKDFE) for improved ADE prediction from Electronic Health Record (EHR) data, comparing it with automated event-based Knowledge Discovery in Databases (KDD). We investigated how incorporating domain-specific ADE risk scores for prolonged heart QT interval, extracted from the Janusmed Riskprofile (Janusmed) Clinical Decision Support System (CDSS), affects prediction performance using EHR data and medication handling events. Results indicate that, while aKDFE step 1 (event-based feature generation) alone did not significantly improve ADE prediction performance, aKDFE step 2 (patient-centric transformation) enhances the prediction performance. High Area Under the Receiver Operating Characteristic curve (AUROC) values suggest strong feature correlations to the outcome, aligning with the predictive power of patients' prior healthcare history for ADEs. Statistical analysis did not confirm that incorporating the Janusmed information (i) risk scores and (ii) medication route of administration into the model's feature set enhanced predictive performance. However, the patient-centric transformation applied by aKDFE proved to be a highly effective feature engineering approach. Limitations include a single-project focus, potential bias from machine learning pipeline methods, and reliance on AUROC. In conclusion, aKDFE, particularly with patient-centric transformation, improves ADE prediction from EHR data. Future work will explore attention-based models, event feature sequences, and automatic methods for incorporating domain knowledge into the aKDFE framework.\n        △ Less",
        "authors":"['Olof Björneld', 'Tora Hammar', 'Daniel Nilsson', 'Alisa Lincke', 'Welf Löwe']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Uncovering Fairness through Data Complexity as an Early Indicator",
        "abstract":"Fairness constitutes a concern within machine learning (ML) applications. Currently, there is no study on how disparities in classification complexity between privileged and unprivileged groups could influence the fairness of solutions, which serves as a preliminary indicator of potential unfairness. In this work, we investigate this gap, specifically, we focus on synthetic datasets designed to capture a variety of biases ranging from historical bias to measurement and representational bias to evaluate how various complexity metrics differences correlate with group fairness metrics. We then apply association rule mining to identify patterns that link disproportionate complexity differences between groups with fairness-related outcomes, offering data-centric indicators to guide bias mitigation. Our findings are also validated by their application in real-world problems, providing evidence that quantifying group-wise classification complexity can uncover early indicators of potential fairness challenges. This investigation helps practitioners to proactively address bias in classification tasks.\n        △ Less",
        "authors":"['Juliett Suárez Ferreira', 'Marija Slavkovik', 'Jorge Casillas']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep RL-based Autonomous Navigation of Micro Aerial Vehicles (MAVs) in a complex GPS-denied Indoor Environment",
        "abstract":"The Autonomy of Unmanned Aerial Vehicles (UAVs) in indoor environments poses significant challenges due to the lack of reliable GPS signals in enclosed spaces such as warehouses, factories, and indoor facilities. Micro Aerial Vehicles (MAVs) are preferred for navigating in these complex, GPS-denied scenarios because of their agility, low power consumption, and limited computational capabilities. In this paper, we propose a Reinforcement Learning based Deep-Proximal Policy Optimization (D-PPO) algorithm to enhance realtime navigation through improving the computation efficiency. The end-to-end network is trained in 3D realistic meta-environments created using the Unreal Engine. With these trained meta-weights, the MAV system underwent extensive experimental trials in real-world indoor environments. The results indicate that the proposed method reduces computational latency by 91\\% during training period without significant degradation in performance. The algorithm was tested on a DJI Tello drone, yielding similar results.\n        △ Less",
        "authors":"['Amit Kumar Singh', 'Prasanth Kumar Duba', 'P. Rajalakshmi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"High-Resource Translation:Turning Abundance into Accessibility",
        "abstract":"This paper presents a novel approach to constructing an English-to-Telugu translation model by leveraging transfer learning techniques and addressing the challenges associated with low-resource languages. Utilizing the Bharat Parallel Corpus Collection (BPCC) as the primary dataset, the model incorporates iterative backtranslation to generate synthetic parallel data, effectively augmenting the training dataset and enhancing the model's translation capabilities. The research focuses on a comprehensive strategy for improving model performance through data augmentation, optimization of training parameters, and the effective use of pre-trained models. These methodologies aim to create a robust translation system that can handle diverse sentence structures and linguistic nuances in both English and Telugu. This work highlights the significance of innovative data handling techniques and the potential of transfer learning in overcoming limitations posed by sparse datasets in low-resource languages. The study contributes to the field of machine translation and seeks to improve communication between English and Telugu speakers in practical contexts.\n        △ Less",
        "authors":"['Abhiram Reddy Yanampally']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PRIMEDrive-CoT: A Precognitive Chain-of-Thought Framework for Uncertainty-Aware Object Interaction in Driving Scene Scenario",
        "abstract":"Driving scene understanding is a critical real-world problem that involves interpreting and associating various elements of a driving environment, such as vehicles, pedestrians, and traffic signals. Despite advancements in autonomous driving, traditional pipelines rely on deterministic models that fail to capture the probabilistic nature and inherent uncertainty of real-world driving. To address this, we propose PRIMEDrive-CoT, a novel uncertainty-aware model for object interaction and Chain-of-Thought (CoT) reasoning in driving scenarios. In particular, our approach combines LiDAR-based 3D object detection with multi-view RGB references to ensure interpretable and reliable scene understanding. Uncertainty and risk assessment, along with object interactions, are modelled using Bayesian Graph Neural Networks (BGNNs) for probabilistic reasoning under ambiguous conditions. Interpretable decisions are facilitated through CoT reasoning, leveraging object dynamics and contextual cues, while Grad-CAM visualizations highlight attention regions. Extensive evaluations on the DriveCoT dataset demonstrate that PRIMEDrive-CoT outperforms state-of-the-art CoT and risk-aware models.\n        △ Less",
        "authors":"['Sriram Mandalika', 'Lalitha V', 'Athira Nambiar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Intrinsic Saliency Guided Trunk-Collateral Network for Unsupervised Video Object Segmentation",
        "abstract":"Recent unsupervised video object segmentation (UVOS) methods predominantly adopt the motion-appearance paradigm. Mainstream motion-appearance approaches use either the two-encoder structure to separately encode motion and appearance features, or the single-encoder structure for joint encoding. However, these methods fail to properly balance the motion-appearance relationship. Consequently, even with complex fusion modules for motion-appearance integration, the extracted suboptimal features degrade the models' overall performance. Moreover, the quality of optical flow varies across scenarios, making it insufficient to rely solely on optical flow to achieve high-quality segmentation results. To address these challenges, we propose the Intrinsic Saliency guided Trunk-Collateral Net}work (ISTC-Net), which better balances the motion-appearance relationship and incorporates model's intrinsic saliency information to enhance segmentation performance. Specifically, considering that optical flow maps are derived from RGB images, they share both commonalities and differences. We propose a novel Trunk-Collateral structure. The shared trunk backbone captures the motion-appearance commonality, while the collateral branch learns the uniqueness of motion features. Furthermore, an Intrinsic Saliency guided Refinement Module (ISRM) is devised to efficiently leverage the model's intrinsic saliency information to refine high-level features, and provide pixel-level guidance for motion-appearance fusion, thereby enhancing performance without additional input. Experimental results show that ISTC-Net achieved state-of-the-art performance on three UVOS datasets (89.2% J&F on DAVIS-16, 76% J on YouTube-Objects, 86.4% J on FBMS) and four standard video salient object detection (VSOD) benchmarks with the notable increase, demonstrating its effectiveness and superiority over previous methods.\n        △ Less",
        "authors":"['Xiangyu Zheng', 'Wanyun Li', 'Songcheng He', 'Xiaoqiang Li', 'We Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference",
        "abstract":"The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https:\/\/github.com\/PKU-SEC-Lab\/HybriMoE.\n        △ Less",
        "authors":"['Shuzhang Zhong', 'Yanfan Sun', 'Ling Liang', 'Runsheng Wang', 'Ru Huang', 'Meng Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Why do zeroes happen? A model-based approach for demand classification",
        "abstract":"Effective demand forecasting is critical for inventory management, production planning, and decision making across industries. Selecting the appropriate model and suitable features to efficiently capture patterns in the data is one of the main challenges in demand forecasting. In reality, this becomes even more complicated when the recorded sales have zeroes, which can happen naturally or due to some anomalies, such as stockouts and recording errors. Mistreating the zeroes can lead to the application of inappropriate forecasting methods, and thus leading to poor decision making. Furthermore, the demand itself can have different fundamental characteristics, and being able to distinguish one type from another might bring substantial benefits in terms of accuracy and thus decision making. We propose a two-stage model-based classification framework that in the first step, identifies artificially occurring zeroes, and then classifies demand to one of the possible types: regular\/intermittent, intermittent smooth\/lumpy, fractional\/count. The framework utilises statistical modelling and information criteria to detect anomalous zeroes and then classify demand into those categories. We then argue that different types of demand need different features, and show empirically that they tend to increase the accuracy of the forecasting methods compared to those applied directly to the dataset without the generated features and the two-stage framework. Our general practical recommendation based on that is to use the mixture approach for intermittent demand, capturing the demand sizes and demand probability separately, as it seems to improve the accuracy of different forecasting approaches.\n        △ Less",
        "authors":"['Ivan Svetunkov', 'Anna Sroginis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"To Give or Not to Give? The Impacts of Strategically Withheld Recourse",
        "abstract":"Individuals often aim to reverse undesired outcomes in interactions with automated systems, like loan denials, by either implementing system-recommended actions (recourse), or manipulating their features. While providing recourse benefits users and enhances system utility, it also provides information about the decision process that can be used for more effective strategic manipulation, especially when the individuals collectively share such information with each other.\n  We show that this tension leads rational utility-maximizing systems to frequently withhold recourse, resulting in decreased population utility, particularly impacting sensitive groups.\n  To mitigate these effects, we explore the role of recourse subsidies, finding them effective in increasing the provision of recourse actions by rational systems, as well as lowering the potential social cost and mitigating unfairness caused by recourse withholding.\n        △ Less",
        "authors":"['Yatong Chen', 'Andrew Estornell', 'Yevgeniy Vorobeychik', 'Yang Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Actuarial Learning for Pension Fund Mortality Forecasting",
        "abstract":"For the assessment of the financial soundness of a pension fund, it is necessary to take into account mortality forecasting so that longevity risk is consistently incorporated into future cash flows. In this article, we employ machine learning models applied to actuarial science ({\\it actuarial learning}) to make mortality predictions for a relevant sample of pension funds' participants. Actuarial learning represents an emerging field that involves the application of machine learning (ML) and artificial intelligence (AI) techniques in actuarial science. This encompasses the use of algorithms and computational models to analyze large sets of actuarial data, such as regression trees, random forest, boosting, XGBoost, CatBoost, and neural networks (eg. FNN, LSTM, and MHA). Our results indicate that some ML\/AI algorithms present competitive out-of-sample performance when compared to the classical Lee-Carter model. This may indicate interesting alternatives for consistent liability evaluation and effective pension fund risk management.\n        △ Less",
        "authors":"['Eduardo Fraga L. de Melo', 'Helton Graziadei', 'Rodrigo Targino']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Energy-Conserving Neural Network Closure Model for Long-Time Accurate and Stable LES",
        "abstract":"Machine learning-based closure models for LES have shown promise in capturing complex turbulence dynamics but often suffer from instabilities and physical inconsistencies. In this work, we develop a novel skew-symmetric neural architecture as closure model that enforces stability while preserving key physical conservation laws. Our approach leverages a discretization that ensures mass, momentum, and energy conservation, along with a face-averaging filter to maintain mass conservation in coarse-grained velocity fields. We compare our model against several conventional data-driven closures (including unconstrained convolutional neural networks), and the physics-based Smagorinsky model. Performance is evaluated on decaying turbulence and Kolmogorov flow for multiple coarse-graining factors. In these test cases we observe that unconstrained machine learning models suffer from numerical instabilities. In contrast, our skew-symmetric model remains stable across all tests, though at the cost of increased dissipation. Despite this trade-off, we demonstrate that our model still outperforms the Smagorinsky model in unseen scenarios. These findings highlight the potential of structure-preserving machine learning closures for reliable long-time LES.\n        △ Less",
        "authors":"['Toby van Gastelen', 'Wouter Edeling', 'Benjamin Sanderse']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PathGPT: Leveraging Large Language Models for Personalized Route Generation",
        "abstract":"The proliferation of GPS enabled devices has led to the accumulation of a substantial corpus of historical trajectory data. By leveraging these data for training machine learning models,researchers have devised novel data-driven methodologies that address the personalized route recommendation (PRR) problem. In contrast to conventional algorithms such as Dijkstra shortest path algorithm,these novel algorithms possess the capacity to discern and learn patterns within the data,thereby facilitating the generation of more personalized paths. However,once these models have been trained,their application is constrained to the generation of routes that align with their training patterns. This limitation renders them less adaptable to novel scenarios and the deployment of multiple machine learning models might be necessary to address new possible scenarios,which can be costly as each model must be trained separately. Inspired by recent advances in the field of Large Language Models (LLMs),we leveraged their natural language understanding capabilities to develop a unified model to solve the PRR problem while being seamlessly adaptable to new scenarios without additional training. To accomplish this,we combined the extensive knowledge LLMs acquired during training with further access to external hand-crafted context information,similar to RAG (Retrieved Augmented Generation) systems,to enhance their ability to generate paths according to user-defined requirements. Extensive experiments on different datasets show a considerable uplift in LLM performance on the PRR problem.\n        △ Less",
        "authors":"['Steeve Cuthbert Marcelyn', 'Yucen Gao', 'Yuzhe Zhang', 'Xiaofeng Gao', 'Guihai Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Substructure-Aware Expert Model for Molecular Property Prediction",
        "abstract":"Molecular property prediction is essential for applications such as drug discovery and toxicity assessment. While Graph Neural Networks (GNNs) have shown promising results by modeling molecules as molecular graphs, their reliance on data-driven learning limits their ability to generalize, particularly in the presence of data imbalance and diverse molecular substructures. Existing methods often overlook the varying contributions of different substructures to molecular properties, treating them uniformly. To address these challenges, we propose ASE-Mol, a novel GNN-based framework that leverages a Mixture-of-Experts (MoE) approach for molecular property prediction. ASE-Mol incorporates BRICS decomposition and significant substructure awareness to dynamically identify positive and negative substructures. By integrating a MoE architecture, it reduces the adverse impact of negative motifs while improving adaptability to positive motifs. Experimental results on eight benchmark datasets demonstrate that ASE-Mol achieves state-of-the-art performance, with significant improvements in both accuracy and interpretability.\n        △ Less",
        "authors":"['Tianyi Jiang', 'Zeyu Wang', 'Shanqing Yu', 'Qi Xuan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Momentum Boosted Episodic Memory for Improving Learning in Long-Tailed RL Environments",
        "abstract":"Traditional Reinforcement Learning (RL) algorithms assume the distribution of the data to be uniform or mostly uniform. However, this is not the case with most real-world applications like autonomous driving or in nature where animals roam. Some experiences are encountered frequently, and most of the remaining experiences occur rarely; the resulting distribution is called Zipfian. Taking inspiration from the theory of complementary learning systems, an architecture for learning from Zipfian distributions is proposed where important long tail trajectories are discovered in an unsupervised manner. The proposal comprises an episodic memory buffer containing a prioritised memory module to ensure important rare trajectories are kept longer to address the Zipfian problem, which needs credit assignment to happen in a sample efficient manner. The experiences are then reinstated from episodic memory and given weighted importance forming the trajectory to be executed. Notably, the proposed architecture is modular, can be incorporated in any RL architecture and yields improved performance in multiple Zipfian tasks over traditional architectures. Our method outperforms IMPALA by a significant margin on all three tasks and all three evaluation metrics (Zipfian, Uniform, and Rare Accuracy) and also gives improvements on most Atari environments that are considered challenging\n        △ Less",
        "authors":"['Dolton Fernandes', 'Pramod Kaushik', 'Harsh Shukla', 'Bapi Raju Surampudi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Unlearning Made Practical: Seamless Integration via Negated Pseudo-Gradients",
        "abstract":"The right to be forgotten is a fundamental principle of privacy-preserving regulations and extends to Machine Learning (ML) paradigms such as Federated Learning (FL). While FL enhances privacy by enabling collaborative model training without sharing private data, trained models still retain the influence of training data. Federated Unlearning (FU) methods recently proposed often rely on impractical assumptions for real-world FL deployments, such as storing client update histories or requiring access to a publicly available dataset. To address these constraints, this paper introduces a novel method that leverages negated Pseudo-gradients Updates for Federated Unlearning (PUF). Our approach only uses standard client model updates, anyway employed during regular FL rounds, and interprets them as pseudo-gradients. When a client needs to be forgotten, we apply the negated of their pseudo-gradients, appropriately scaled, to the global model. Unlike state-of-the-art mechanisms, PUF seamlessly integrates with FL workflows, incurs no additional computational and communication overhead beyond standard FL rounds, and supports concurrent unlearning requests. We extensively evaluated the proposed method on two well-known benchmark image classification datasets (CIFAR-10 and CIFAR-100) and a real-world medical imaging dataset for segmentation (ProstateMRI), using three different neural architectures: two residual networks and a vision transformer. The experimental results across various settings demonstrate that PUF achieves state-of-the-art forgetting effectiveness and recovery time, without relying on any additional assumptions, thus underscoring its practical applicability.\n        △ Less",
        "authors":"['Alessio Mora', 'Carlo Mazzocca', 'Rebecca Montanari', 'Paolo Bellavista']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization",
        "abstract":"While large language models (LLMs) have demonstrated exceptional capabilities in challenging tasks such as mathematical reasoning, existing methods to enhance reasoning ability predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data after pre-training. However, these approaches critically depend on external supervisions--such as human labelled reasoning traces, verified golden answers, or pre-trained reward models--which limits scalability and practical applicability. In this work, we propose Entropy Minimized Policy Optimization (EMPO), which makes an early attempt at fully unsupervised LLM reasoning incentivization. EMPO does not require any supervised information for incentivizing reasoning capabilities (i.e., neither verifiable reasoning traces, problems with golden answers, nor additional pre-trained reward models). By continuously minimizing the predictive entropy of LLMs on unlabeled user queries in a latent semantic space, EMPO enables purely self-supervised evolution of reasoning capabilities with strong flexibility and practicality. Our experiments demonstrate competitive performance of EMPO on both mathematical reasoning and free-form commonsense reasoning tasks. Specifically, without any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B Base from 30.7\\% to 48.1\\% on mathematical benchmarks and improves truthfulness accuracy of Qwen2.5-7B Instruct from 87.16\\% to 97.25\\% on TruthfulQA.\n        △ Less",
        "authors":"['Qingyang Zhang', 'Haitao Wu', 'Changqing Zhang', 'Peilin Zhao', 'Yatao Bian']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization",
        "abstract":"The integration of large language models (LLMs) into information retrieval systems introduces new attack surfaces, particularly for adversarial ranking manipulations. We present StealthRank, a novel adversarial ranking attack that manipulates LLM-driven product recommendation systems while maintaining textual fluency and stealth. Unlike existing methods that often introduce detectable anomalies, StealthRank employs an energy-based optimization framework combined with Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text sequences embedded within product descriptions that subtly yet effectively influence LLM ranking mechanisms. We evaluate StealthRank across multiple LLMs, demonstrating its ability to covertly boost the ranking of target products while avoiding explicit manipulation traces that can be easily detected. Our results show that StealthRank consistently outperforms state-of-the-art adversarial ranking baselines in both effectiveness and stealth, highlighting critical vulnerabilities in LLM-driven recommendation systems.\n        △ Less",
        "authors":"['Yiming Tang', 'Yi Fan', 'Chenxiao Yu', 'Tiankai Yang', 'Yue Zhao', 'Xiyang Hu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Storybooth: Training-free Multi-Subject Consistency for Improved Visual Storytelling",
        "abstract":"Training-free consistent text-to-image generation depicting the same subjects across different images is a topic of widespread recent interest. Existing works in this direction predominantly rely on cross-frame self-attention; which improves subject-consistency by allowing tokens in each frame to pay attention to tokens in other frames during self-attention computation. While useful for single subjects, we find that it struggles when scaling to multiple characters. In this work, we first analyze the reason for these limitations. Our exploration reveals that the primary-issue stems from self-attention-leakage, which is exacerbated when trying to ensure consistency across multiple-characters. This happens when tokens from one subject pay attention to other characters, causing them to appear like each other (e.g., a dog appearing like a duck). Motivated by these findings, we propose StoryBooth: a training-free approach for improving multi-character consistency. In particular, we first leverage multi-modal chain-of-thought reasoning and region-based generation to apriori localize the different subjects across the desired story outputs. The final outputs are then generated using a modified diffusion model which consists of two novel layers: 1) a bounded cross-frame self-attention layer for reducing inter-character attention leakage, and 2) token-merging layer for improving consistency of fine-grain subject details. Through both qualitative and quantitative results we find that the proposed approach surpasses prior state-of-the-art, exhibiting improved consistency across both multiple-characters and fine-grain subject details.\n        △ Less",
        "authors":"['Jaskirat Singh', 'Junshen Kevin Chen', 'Jonas Kohler', 'Michael Cohen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Temporal Dynamic Embedding for Irregularly Sampled Time Series",
        "abstract":"In several practical applications, particularly healthcare, clinical data of each patient is individually recorded in a database at irregular intervals as required. This causes a sparse and irregularly sampled time series, which makes it difficult to handle as a structured representation of the prerequisites of neural network models. We therefore propose temporal dynamic embedding (TDE), which enables neural network models to receive data that change the number of variables over time. TDE regards each time series variable as an embedding vector evolving over time, instead of a conventional fixed structured representation, which causes a critical missing problem. For each time step, TDE allows for the selective adoption and aggregation of only observed variable subsets and represents the current status of patient based on current observations. The experiment was conducted on three clinical datasets: PhysioNet 2012, MIMIC-III, and PhysioNet 2019. The TDE model performed competitively or better than the imputation-based baseline and several recent state-of-the-art methods with reduced training runtime.\n        △ Less",
        "authors":"['Mincheol Kim', 'Soo-Yong Shin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AiGAS-dEVL-RC: An Adaptive Growing Neural Gas Model for Recurrently Drifting Unsupervised Data Streams",
        "abstract":"Concept drift and extreme verification latency pose significant challenges in data stream learning, particularly when dealing with recurring concept changes in dynamic environments. This work introduces a novel method based on the Growing Neural Gas (GNG) algorithm, designed to effectively handle abrupt recurrent drifts while adapting to incrementally evolving data distributions (incremental drifts). Leveraging the self-organizing and topological adaptability of GNG, the proposed approach maintains a compact yet informative memory structure, allowing it to efficiently store and retrieve knowledge of past or recurring concepts, even under conditions of delayed or sparse stream supervision. Our experiments highlight the superiority of our approach over existing data stream learning methods designed to cope with incremental non-stationarities and verification latency, demonstrating its ability to quickly adapt to new drifts, robustly manage recurring patterns, and maintain high predictive accuracy with a minimal memory footprint. Unlike other techniques that fail to leverage recurring knowledge, our proposed approach is proven to be a robust and efficient online learning solution for unsupervised drifting data flows.\n        △ Less",
        "authors":"['Maria Arostegi', 'Miren Nekane Bilbao', 'Jesus L. Lobo', 'Javier Del Ser']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Addressing Class Imbalance with Probabilistic Graphical Models and Variational Inference",
        "abstract":"This study proposes a method for imbalanced data classification based on deep probabilistic graphical models (DPGMs) to solve the problem that traditional methods have insufficient learning ability for minority class samples. To address the classification bias caused by class imbalance, we introduce variational inference optimization probability modeling, which enables the model to adaptively adjust the representation ability of minority classes and combines the class-aware weight adjustment strategy to enhance the classifier's sensitivity to minority classes. In addition, we combine the adversarial learning mechanism to generate minority class samples in the latent space so that the model can better characterize the category boundary in the high-dimensional feature space. The experiment is evaluated on the Kaggle \"Credit Card Fraud Detection\" dataset and compared with a variety of advanced imbalanced classification methods (such as GAN-based sampling, BRF, XGBoost-Cost Sensitive, SAAD, HAN). The results show that the method in this study has achieved the best performance in AUC, Precision, Recall and F1-score indicators, effectively improving the recognition rate of minority classes and reducing the false alarm rate. This method can be widely used in imbalanced classification tasks such as financial fraud detection, medical diagnosis, and anomaly detection, providing a new solution for related research.\n        △ Less",
        "authors":"['Yujia Lou', 'Jie Liu', 'Yuan Sheng', 'Jiawei Wang', 'Yiwei Zhang', 'Yaokun Ren']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Interpretable Non-linear Survival Analysis with Evolutionary Symbolic Regression",
        "abstract":"Survival Regression (SuR) is a key technique for modeling time to event in important applications such as clinical trials and semiconductor manufacturing. Currently, SuR algorithms belong to one of three classes: non-linear black-box -- allowing adaptability to many datasets but offering limited interpretability (e.g., tree ensembles); linear glass-box -- being easier to interpret but limited to modeling only linear interactions (e.g., Cox proportional hazards); and non-linear glass-box -- allowing adaptability and interpretability, but empirically found to have several limitations (e.g., explainable boosting machines, survival trees). In this work, we investigate whether Symbolic Regression (SR), i.e., the automated search of mathematical expressions from data, can lead to non-linear glass-box survival models that are interpretable and accurate. We propose an evolutionary, multi-objective, and multi-expression implementation of SR adapted to SuR. Our empirical results on five real-world datasets show that SR consistently outperforms traditional glass-box methods for SuR in terms of accuracy per number of dimensions in the model, while exhibiting comparable accuracy with black-box methods. Furthermore, we offer qualitative examples to assess the interpretability potential of SR models for SuR. Code at: https:\/\/github.com\/lurovi\/SurvivalMultiTree-pyNSGP.\n        △ Less",
        "authors":"['Luigi Rovito', 'Marco Virgolin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-Driven Prognostics for State of Health Prediction in Li-ion Batteries: A Comprehensive Analysis with Validation",
        "abstract":"This paper presents a comprehensive review of AI-driven prognostics for State of Health (SoH) prediction in lithium-ion batteries. We compare the effectiveness of various AI algorithms, including FFNN, LSTM, and BiLSTM, across multiple datasets (CALCE, NASA, UDDS) and scenarios (e.g., varying temperatures and driving conditions). Additionally, we analyze the factors influencing SoH fluctuations, such as temperature and charge-discharge rates, and validate our findings through simulations. The results demonstrate that BiLSTM achieves the highest accuracy, with an average RMSE reduction of 15% compared to LSTM, highlighting its robustness in real-world applications.\n        △ Less",
        "authors":"['Tianqi Ding', 'Dawei Xiang', 'Tianyao Sun', 'YiJiashum Qi', 'Zunduo Zhao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Single-Agent vs. Multi-Agent LLM Strategies for Automated Student Reflection Assessment",
        "abstract":"We explore the use of Large Language Models (LLMs) for automated assessment of open-text student reflections and prediction of academic performance. Traditional methods for evaluating reflections are time-consuming and may not scale effectively in educational settings. In this work, we employ LLMs to transform student reflections into quantitative scores using two assessment strategies (single-agent and multi-agent) and two prompting techniques (zero-shot and few-shot). Our experiments, conducted on a dataset of 5,278 reflections from 377 students over three academic terms, demonstrate that the single-agent with few-shot strategy achieves the highest match rate with human evaluations. Furthermore, models utilizing LLM-assessed reflection scores outperform baselines in both at-risk student identification and grade prediction tasks. These findings suggest that LLMs can effectively automate reflection assessment, reduce educators' workload, and enable timely support for students who may need additional assistance. Our work emphasizes the potential of integrating advanced generative AI technologies into educational practices to enhance student engagement and academic success.\n        △ Less",
        "authors":"['Gen Li', 'Li Chen', 'Cheng Tang', 'Valdemar Švábenský', 'Daisuke Deguchi', 'Takayoshi Yamashita', 'Atsushi Shimada']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Automated Archival Descriptions with Federated Intelligence of LLMs",
        "abstract":"Enforcing archival standards requires specialized expertise, and manually creating metadata descriptions for archival materials is a tedious and error-prone task. This work aims at exploring the potential of agentic AI and large language models (LLMs) in addressing the challenges of implementing a standardized archival description process. To this end, we introduce an agentic AI-driven system for automated generation of high-quality metadata descriptions of archival materials. We develop a federated optimization approach that unites the intelligence of multiple LLMs to construct optimal archival metadata. We also suggest methods to overcome the challenges associated with using LLMs for consistent metadata generation. To evaluate the feasibility and effectiveness of our techniques, we conducted extensive experiments using a real-world dataset of archival materials, which covers a variety of document types and data formats. The evaluation results demonstrate the feasibility of our techniques and highlight the superior performance of the federated optimization approach compared to single-model solutions in metadata quality and reliability.\n        △ Less",
        "authors":"['Jinghua Groppe', 'Andreas Marquet', 'Annabel Walz', 'Sven Groppe']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diabetic Retinopathy Detection Based on Convolutional Neural Networks with SMOTE and CLAHE Techniques Applied to Fundus Images",
        "abstract":"Diabetic retinopathy (DR) is one of the major complications in diabetic patients' eyes, potentially leading to permanent blindness if not detected timely. This study aims to evaluate the accuracy of artificial intelligence (AI) in diagnosing DR. The method employed is the Synthetic Minority Over-sampling Technique (SMOTE) algorithm, applied to identify DR and its severity stages from fundus images using the public dataset \"APTOS 2019 Blindness Detection.\" Literature was reviewed via ScienceDirect, ResearchGate, Google Scholar, and IEEE Xplore. Classification results using Convolutional Neural Network (CNN) showed the best performance for the binary classes normal (0) and DR (1) with an accuracy of 99.55%, precision of 99.54%, recall of 99.54%, and F1-score of 99.54%. For the multiclass classification No_DR (0), Mild (1), Moderate (2), Severe (3), Proliferate_DR (4), the accuracy was 95.26%, precision 95.26%, recall 95.17%, and F1-score 95.23%. Evaluation using the confusion matrix yielded results of 99.68% for binary classification and 96.65% for multiclass. This study highlights the significant potential in enhancing the accuracy of DR diagnosis compared to traditional human analysis\n        △ Less",
        "authors":"['Sidhiq Mardianta', 'Affandy', 'Catur Supriyanto', 'Catur Supriyanto', 'Adi Wijaya']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Architecture independent generalization bounds for overparametrized deep ReLU networks",
        "abstract":"We prove that overparametrized neural networks are able to generalize with a test error that is independent of the level of overparametrization, and independent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds that only depend on the metric geometry of the test and training sets, on the regularity properties of the activation function, and on the operator norms of the weights and norms of biases. For overparametrized deep ReLU networks with a training sample size bounded by the input space dimension, we explicitly construct zero loss minimizers without use of gradient descent, and prove that the generalization error is independent of the network architecture.\n        △ Less",
        "authors":"['Thomas Chen', 'Chun-Kai Kevin Chien', 'Patricia Muñoz Ewald', 'Andrew G. Moore']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"kNN-SVC: Robust Zero-Shot Singing Voice Conversion with Additive Synthesis and Concatenation Smoothness Optimization",
        "abstract":"Robustness is critical in zero-shot singing voice conversion (SVC). This paper introduces two novel methods to strengthen the robustness of the kNN-VC framework for SVC. First, kNN-VC's core representation, WavLM, lacks harmonic emphasis, resulting in dull sounds and ringing artifacts. To address this, we leverage the bijection between WavLM, pitch contours, and spectrograms to perform additive synthesis, integrating the resulting waveform into the model to mitigate these issues. Second, kNN-VC overlooks concatenative smoothness, a key perceptual factor in SVC. To enhance smoothness, we propose a new distance metric that filters out unsuitable kNN candidates and optimize the summing weights of the candidates during inference. Although our techniques are built on the kNN-VC framework for implementation convenience, they are broadly applicable to general concatenative neural synthesis models. Experimental results validate the effectiveness of these modifications in achieving robust SVC. Demo: http:\/\/knnsvc.com Code: https:\/\/github.com\/SmoothKen\/knn-svc\n        △ Less",
        "authors":"['Keren Shao', 'Ke Chen', 'Matthew Baas', 'Shlomo Dubnov']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dual Boost-Driven Graph-Level Clustering Network",
        "abstract":"Graph-level clustering remains a pivotal yet formidable challenge in graph learning. Recently, the integration of deep learning with representation learning has demonstrated notable advancements, yielding performance enhancements to a certain degree. However, existing methods suffer from at least one of the following issues: 1. the original graph structure has noise, and 2. during feature propagation and pooling processes, noise is gradually aggregated into the graph-level embeddings through information propagation. Consequently, these two limitations mask clustering-friendly information, leading to suboptimal graph-level clustering performance. To this end, we propose a novel Dual Boost-Driven Graph-Level Clustering Network (DBGCN) to alternately promote graph-level clustering and filtering out interference information in a unified framework. Specifically, in the pooling step, we evaluate the contribution of features at the global and optimize them using a learnable transformation matrix to obtain high-quality graph-level representation, such that the model's reasoning capability can be improved. Moreover, to enable reliable graph-level clustering, we first identify and suppress information detrimental to clustering by evaluating similarities between graph-level representations, providing more accurate guidance for multi-view fusion. Extensive experiments demonstrated that DBGCN outperforms the state-of-the-art graph-level clustering methods on six benchmark datasets.\n        △ Less",
        "authors":"['John Smith', 'Wenxuan Tu', 'Junlong Wu', 'Wenxin Zhang', 'Jingxin Liu', 'Haotian Wang', 'Jieren Cheng', 'Huajie Lei', 'Guangzhen Yao', 'Lingren Wang', 'Mengfei Li', 'Renda Han', 'Yu Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Curved representational Bregman divergences and their applications",
        "abstract":"By analogy to curved exponential families, we define curved Bregman divergences as restrictions of Bregman divergences to sub-dimensional parameter subspaces, and prove that the barycenter of a finite weighted parameter set with respect to a curved Bregman divergence amounts to the Bregman projection onto the subspace induced by the constraint of the barycenter with respect to the unconstrained full Bregman divergence. We demonstrate the significance of curved Bregman divergences with two examples: (1) symmetrized Bregman divergences and (2) the Kullback-Leibler divergence between circular complex normal distributions. We then consider monotonic embeddings to define representational curved Bregman divergences and show that the $α$-divergences are representational curved Bregman divergences with respect to $α$-embeddings of the probability simplex into the positive measure cone. As an application, we report an efficient method to calculate the intersection of a finite set of $α$-divergence spheres.\n        △ Less",
        "authors":"['Frank Nielsen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Measuring Déjà vu Memorization Efficiently",
        "abstract":"Recent research has shown that representation learning models may accidentally memorize their training data. For example, the déjà vu method shows that for certain representation learning models and training images, it is sometimes possible to correctly predict the foreground label given only the representation of the background - better than through dataset-level correlations. However, their measurement method requires training two models - one to estimate dataset-level correlations and the other to estimate memorization. This multiple model setup becomes infeasible for large open-source models. In this work, we propose alternative simple methods to estimate dataset-level correlations, and show that these can be used to approximate an off-the-shelf model's memorization ability without any retraining. This enables, for the first time, the measurement of memorization in pre-trained open-source image representation and vision-language representation models. Our results show that different ways of measuring memorization yield very similar aggregate results. We also find that open-source models typically have lower aggregate memorization than similar models trained on a subset of the data. The code is available both for vision and vision language models.\n        △ Less",
        "authors":"['Narine Kokhlikyan', 'Bargav Jayaraman', 'Florian Bordes', 'Chuan Guo', 'Kamalika Chaudhuri']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Lattice: Learning to Efficiently Compress the Memory",
        "abstract":"Attention mechanisms have revolutionized sequence learning but suffer from quadratic computational complexity. This paper introduces Lattice, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity. We formulate this compression as an online optimization problem and derive a dynamic memory update rule based on a single gradient descent step. The resulting recurrence features a state- and input-dependent gating mechanism, offering an interpretable memory update process. The core innovation is the orthogonal update: each memory slot is updated exclusively with information orthogonal to its current state hence incorporation of only novel, non-redundant data, which minimizes the interference with previously stored information. The experimental results show that Lattice achieves the best perplexity compared to all baselines across diverse context lengths, with performance improvement becoming more pronounced as the context length increases.\n        △ Less",
        "authors":"['Mahdi Karami', 'Vahab Mirrokni']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Effective Method for Inverse Ising Problem under Missing Observations in Restricted Boltzmann Machines",
        "abstract":"Restricted Boltzmann machines (RBMs) are energy-based models analogous to the Ising model and are widely applied in statistical machine learning. The standard inverse Ising problem with a complete dataset requires computing both data and model expectations and is computationally challenging because model expectations have a combinatorial explosion. Furthermore, in many applications, the available datasets are partially incomplete, making it difficult to compute even data expectations. In this study, we propose a approximation framework for these expectations in the practical inverse Ising problems that integrates mean-field approximation or persistent contrastive divergence to generate refined initial points and spatial Monte Carlo integration to enhance estimator accuracy. We demonstrate that the proposed method effectively and accurately tunes the model parameters in comparison to the conventional method.\n        △ Less",
        "authors":"['Kaiji Sekimoto', 'Muneki Yasuda']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TAGC: Optimizing Gradient Communication in Distributed Transformer Training",
        "abstract":"The increasing complexity of large language models (LLMs) necessitates efficient training strategies to mitigate the high computational costs associated with distributed training. A significant bottleneck in this process is gradient synchronization across multiple GPUs, particularly in the zero-redundancy parallelism mode. In this paper, we introduce Transformer-Aware Gradient Compression (TAGC), an optimized gradient compression algorithm designed specifically for transformer-based models. TAGC extends the lossless homomorphic compression method by adapting it for sharded models and incorporating transformer-specific optimizations, such as layer-selective compression and dynamic sparsification. Our experimental results demonstrate that TAGC accelerates training by up to 15% compared to the standard Fully Sharded Data Parallel (FSDP) approach, with minimal impact on model quality. We integrate TAGC into the PyTorch FSDP framework, the implementation is publicly available at https:\/\/github.com\/ipolyakov\/TAGC.\n        △ Less",
        "authors":"['Igor Polyakov', 'Alexey Dukhanov', 'Egor Spirin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Multi-Modal AI System for Screening Mammography: Integrating 2D and 3D Imaging to Improve Breast Cancer Detection in a Prospective Clinical Study",
        "abstract":"Although digital breast tomosynthesis (DBT) improves diagnostic performance over full-field digital mammography (FFDM), false-positive recalls remain a concern in breast cancer screening. We developed a multi-modal artificial intelligence system integrating FFDM, synthetic mammography, and DBT to provide breast-level predictions and bounding-box localizations of suspicious findings. Our AI system, trained on approximately 500,000 mammography exams, achieved 0.945 AUROC on an internal test set. It demonstrated capacity to reduce recalls by 31.7% and radiologist workload by 43.8% while maintaining 100% sensitivity, underscoring its potential to improve clinical workflows. External validation confirmed strong generalizability, reducing the gap to a perfect AUROC by 35.31%-69.14% relative to strong baselines. In prospective deployment across 18 sites, the system reduced recall rates for low-risk cases. An improved version, trained on over 750,000 exams with additional labels, further reduced the gap by 18.86%-56.62% across large external datasets. Overall, these results underscore the importance of utilizing all available imaging modalities, demonstrate the potential for clinical impact, and indicate feasibility of further reduction of the test error with increased training set when using large-capacity neural networks.\n        △ Less",
        "authors":"['Jungkyu Park', 'Jan Witowski', 'Yanqi Xu', 'Hari Trivedi', 'Judy Gichoya', 'Beatrice Brown-Mulry', 'Malte Westerhoff', 'Linda Moy', 'Laura Heacock', 'Alana Lewin', 'Krzysztof J. Geras']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"To Start Up a Start-Up$-$Embedding Strategic Demand Development in Operational On-Demand Fulfillment via Reinforcement Learning with Information Shaping",
        "abstract":"The last few years have witnessed rapid growth in the on-demand delivery market, with many start-ups entering the field. However, not all of these start-ups have succeeded due to various reasons, among others, not being able to establish a large enough customer base. In this paper, we address this problem that many on-demand transportation start-ups face: how to establish themselves in a new market. When starting, such companies often have limited fleet resources to serve demand across a city. Depending on the use of the fleet, varying service quality is observed in different areas of the city, and in turn, the service quality impacts the respective growth of demand in each area. Thus, operational fulfillment decisions drive the longer-term demand development. To integrate strategic demand development into real-time fulfillment operations, we propose a two-step approach. First, we derive analytical insights into optimal allocation decisions for a stylized problem. Second, we use these insights to shape the training data of a reinforcement learning strategy for operational real-time fulfillment. Our experiments demonstrate that combining operational efficiency with long-term strategic planning is highly advantageous. Further, we show that the careful shaping of training data is essential for the successful development of demand.\n        △ Less",
        "authors":"['Xinwei Chen', 'Marlin W. Ulmer', 'Barrett W. Thomas']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning",
        "abstract":"Recent advances in large-scale generative language models have shown that reasoning capabilities can significantly improve model performance across a variety of tasks. However, the impact of reasoning on a model's ability to mitigate stereotypical responses remains largely underexplored. In this work, we investigate the crucial relationship between a model's reasoning ability and fairness, and ask whether improved reasoning capabilities can mitigate harmful stereotypical responses, especially those arising due to shallow or flawed reasoning. We conduct a comprehensive evaluation of multiple open-source LLMs, and find that larger models with stronger reasoning abilities exhibit substantially lower stereotypical bias on existing fairness benchmarks. Building on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning, a novel approach that extracts structured reasoning traces from advanced reasoning models and infuses them into models that lack such capabilities. We use only general-purpose reasoning and do not require any fairness-specific supervision for bias mitigation. Notably, we see that models fine-tuned using ReGiFT not only improve fairness relative to their non-reasoning counterparts but also outperform advanced reasoning models on fairness benchmarks. We also analyze how variations in the correctness of the reasoning traces and their length influence model fairness and their overall performance. Our findings highlight that enhancing reasoning capabilities is an effective, fairness-agnostic strategy for mitigating stereotypical bias caused by reasoning flaws.\n        △ Less",
        "authors":"['Sanchit Kabra', 'Akshita Jha', 'Chandan K. Reddy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Maternal and Fetal Health Status Assessment by Using Machine Learning on Optical 3D Body Scans",
        "abstract":"Monitoring maternal and fetal health during pregnancy is crucial for preventing adverse outcomes. While tests such as ultrasound scans offer high accuracy, they can be costly and inconvenient. Telehealth and more accessible body shape information provide pregnant women with a convenient way to monitor their health. This study explores the potential of 3D body scan data, captured during the 18-24 gestational weeks, to predict adverse pregnancy outcomes and estimate clinical parameters. We developed a novel algorithm with two parallel streams which are used for extract body shape features: one for supervised learning to extract sequential abdominal circumference information, and another for unsupervised learning to extract global shape descriptors, alongside a branch for demographic data.\n  Our results indicate that 3D body shape can assist in predicting preterm labor, gestational diabetes mellitus (GDM), gestational hypertension (GH), and in estimating fetal weight. Compared to other machine learning models, our algorithm achieved the best performance, with prediction accuracies exceeding 88% and fetal weight estimation accuracy of 76.74% within a 10% error margin, outperforming conventional anthropometric methods by 22.22%.\n        △ Less",
        "authors":"['Ruting Cheng', 'Yijiang Zheng', 'Boyuan Feng', 'Chuhui Qiu', 'Zhuoxin Long', 'Joaquin A. Calderon', 'Xiaoke Zhang', 'Jaclyn M. Phillips', 'James K. Hahn']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Model-Agnostic Policy Explanations with Large Language Models",
        "abstract":"Intelligent agents, such as robots, are increasingly deployed in real-world, human-centric environments. To foster appropriate human trust and meet legal and ethical standards, these agents must be able to explain their behavior. However, state-of-the-art agents are typically driven by black-box models like deep neural networks, limiting their interpretability. We propose a method for generating natural language explanations of agent behavior based only on observed states and actions -- without access to the agent's underlying model. Our approach learns a locally interpretable surrogate model of the agent's behavior from observations, which then guides a large language model to generate plausible explanations with minimal hallucination. Empirical results show that our method produces explanations that are more comprehensible and correct than those from baselines, as judged by both language models and human evaluators. Furthermore, we find that participants in a user study more accurately predicted the agent's future actions when given our explanations, suggesting improved understanding of agent behavior.\n        △ Less",
        "authors":"['Zhang Xi-Jia', 'Yue Guo', 'Shufei Chen', 'Simon Stepputtis', 'Matthew Gombolay', 'Katia Sycara', 'Joseph Campbell']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Technical Report: Full Version of Analyzing and Optimizing Perturbation of DP-SGD Geometrically",
        "abstract":"Differential privacy (DP) has become a prevalent privacy model in a wide range of machine learning tasks, especially after the debut of DP-SGD. However, DP-SGD, which directly perturbs gradients in the training iterations, fails to mitigate the negative impacts of noise on gradient direction. As a result, DP-SGD is often inefficient. Although various solutions (e.g., clipping to reduce the sensitivity of gradients and amplifying privacy bounds to save privacy budgets) are proposed to trade privacy for model efficiency, the root cause of its inefficiency is yet unveiled.\n  In this work, we first generalize DP-SGD and theoretically derive the impact of DP noise on the training process. Our analysis reveals that, in terms of a perturbed gradient, only the noise on direction has eminent impact on the model efficiency while that on magnitude can be mitigated by optimization techniques, i.e., fine-tuning gradient clipping and learning rate. Besides, we confirm that traditional DP introduces biased noise on the direction when adding unbiased noise to the gradient itself. Overall, the perturbation of DP-SGD is actually sub-optimal from a geometric perspective. Motivated by this, we design a geometric perturbation strategy GeoDP within the DP framework, which perturbs the direction and the magnitude of a gradient, respectively. By directly reducing the noise on the direction, GeoDP mitigates the negative impact of DP noise on model efficiency with the same DP guarantee. Extensive experiments on two public datasets (i.e., MNIST and CIFAR-10), one synthetic dataset and three prevalent models (i.e., Logistic Regression, CNN and ResNet) confirm the effectiveness and generality of our strategy.\n        △ Less",
        "authors":"['Jiawei Duan', 'Haibo Hu', 'Qingqing Ye', 'Xinyue Sun']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FedEFC: Federated Learning Using Enhanced Forward Correction Against Noisy Labels",
        "abstract":"Federated Learning (FL) is a powerful framework for privacy-preserving distributed learning. It enables multiple clients to collaboratively train a global model without sharing raw data. However, handling noisy labels in FL remains a major challenge due to heterogeneous data distributions and communication constraints, which can severely degrade model performance. To address this issue, we propose FedEFC, a novel method designed to tackle the impact of noisy labels in FL. FedEFC mitigates this issue through two key techniques: (1) prestopping, which prevents overfitting to mislabeled data by dynamically halting training at an optimal point, and (2) loss correction, which adjusts model updates to account for label noise. In particular, we develop an effective loss correction tailored to the unique challenges of FL, including data heterogeneity and decentralized training. Furthermore, we provide a theoretical analysis, leveraging the composite proper loss property, to demonstrate that the FL objective function under noisy label distributions can be aligned with the clean label distribution. Extensive experimental results validate the effectiveness of our approach, showing that it consistently outperforms existing FL techniques in mitigating the impact of noisy labels, particularly under heterogeneous data settings (e.g., achieving up to 41.64% relative performance improvement over the existing loss correction method).\n        △ Less",
        "authors":"['Seunghun Yu', 'Jin-Hyun Ahn', 'Joonhyuk Kang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fairness in Machine Learning-based Hand Load Estimation: A Case Study on Load Carriage Tasks",
        "abstract":"Predicting external hand load from sensor data is essential for ergonomic exposure assessments, as obtaining this information typically requires direct observation or supplementary data. While machine learning methods have been used to estimate external hand load from worker postures or force exertion data, our findings reveal systematic bias in these predictions due to individual differences such as age and biological sex. To explore this issue, we examined bias in hand load prediction by varying the sex ratio in the training dataset. We found substantial sex disparity in predictive performance, especially when the training dataset is more sex-imbalanced. To address this bias, we developed and evaluated a fair predictive model for hand load estimation that leverages a Variational Autoencoder (VAE) with feature disentanglement. This approach is designed to separate sex-agnostic and sex-specific latent features, minimizing feature overlap. The disentanglement capability enables the model to make predictions based solely on sex-agnostic features of motion patterns, ensuring fair prediction for both biological sexes. Our proposed fair algorithm outperformed conventional machine learning methods (e.g., Random Forests) in both fairness and predictive accuracy, achieving a lower mean absolute error (MAE) difference across male and female sets and improved fairness metrics such as statistical parity (SP) and positive and negative residual differences (PRD and NRD), even when trained on imbalanced sex datasets. These findings emphasize the importance of fairness-aware machine learning algorithms to prevent potential disadvantages in workplace health and safety for certain worker populations.\n        △ Less",
        "authors":"['Arafat Rahman', 'Sol Lim', 'Seokhyun Chung']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Impact of Language Nuances on Sentiment Analysis with Large Language Models: Paraphrasing, Sarcasm, and Emojis",
        "abstract":"Large Language Models (LLMs) have demonstrated impressive performance across various tasks, including sentiment analysis. However, data quality--particularly when sourced from social media--can significantly impact their accuracy. This research explores how textual nuances, including emojis and sarcasm, affect sentiment analysis, with a particular focus on improving data quality through text paraphrasing techniques. To address the lack of labeled sarcasm data, the authors created a human-labeled dataset of 5929 tweets that enabled the assessment of LLM in various sarcasm contexts. The results show that when topic-specific datasets, such as those related to nuclear power, are used to finetune LLMs these models are not able to comprehend accurate sentiment in presence of sarcasm due to less diverse text, requiring external interventions like sarcasm removal to boost model accuracy. Sarcasm removal led to up to 21% improvement in sentiment accuracy, as LLMs trained on nuclear power-related content struggled with sarcastic tweets, achieving only 30% accuracy. In contrast, LLMs trained on general tweet datasets, covering a broader range of topics, showed considerable improvements in predicting sentiment for sarcastic tweets (60% accuracy), indicating that incorporating general text data can enhance sarcasm detection. The study also utilized adversarial text augmentation, showing that creating synthetic text variants by making minor changes significantly increased model robustness and accuracy for sarcastic tweets (approximately 85%). Additionally, text paraphrasing of tweets with fragmented language transformed around 40% of the tweets with low-confidence labels into high-confidence ones, improving LLMs sentiment analysis accuracy by 6%.\n        △ Less",
        "authors":"['Naman Bhargava', 'Mohammed I. Radaideh', 'O Hwang Kwon', 'Aditi Verma', 'Majdi I. Radaideh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DEL: Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding",
        "abstract":"Speculative Decoding (SD) is a widely used approach to accelerate the inference of large language models (LLMs) without reducing generation quality. It operates by first using a compact model to draft multiple tokens efficiently, followed by parallel verification using the target LLM. This approach leads to faster inference compared to auto-regressive decoding. While there are multiple approaches to create a draft model, one promising approach is to use early-exit methods. These methods draft candidate tokens by using a subset of layers of the primary model and applying the remaining layers for verification, allowing a single model to handle both drafting and verification. While this technique reduces memory usage and computational cost, its performance relies on the choice of the exit layer for drafting and the number of tokens drafted (speculation length) in each SD round. Prior works use hyperparameter exploration to statically select these values. However, our evaluations show that these hyperparameter values are task-specific, and even within a task they are dependent on the current sequence context. We introduce DEL, a plug-and-play method that adaptively selects the exit layer and speculation length during inference. DEL dynamically tracks the token acceptance rate if the tokens are drafted at each layer of an LLM and uses that knowledge to heuristically select the optimal exit layer and speculation length. Our experiments across a broad range of models and downstream tasks show that DEL achieves overall speedups of $2.16\\times$$\\sim$$2.50\\times$ over vanilla auto-regressive decoding and improves upon the state-of-the-art SD methods by up to $0.27\\times$.\n        △ Less",
        "authors":"['Hossein Entezari Zarch', 'Lei Gao', 'Chaoyi Jiang', 'Murali Annavaram']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-fidelity Reinforcement Learning Control for Complex Dynamical Systems",
        "abstract":"Controlling instabilities in complex dynamical systems is challenging in scientific and engineering applications. Deep reinforcement learning (DRL) has seen promising results for applications in different scientific applications. The many-query nature of control tasks requires multiple interactions with real environments of the underlying physics. However, it is usually sparse to collect from the experiments or expensive to simulate for complex dynamics. Alternatively, controlling surrogate modeling could mitigate the computational cost issue. However, a fast and accurate learning-based model by offline training makes it very hard to get accurate pointwise dynamics when the dynamics are chaotic. To bridge this gap, the current work proposes a multi-fidelity reinforcement learning (MFRL) framework that leverages differentiable hybrid models for control tasks, where a physics-based hybrid model is corrected by limited high-fidelity data. We also proposed a spectrum-based reward function for RL learning. The effect of the proposed framework is demonstrated on two complex dynamics in physics. The statistics of the MFRL control result match that computed from many-query evaluations of the high-fidelity environments and outperform other SOTA baselines.\n        △ Less",
        "authors":"['Luning Sun', 'Xin-Yang Liu', 'Siyan Zhao', 'Aditya Grover', 'Jian-Xun Wang', 'Jayaraman J. Thiagarajan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Finding Fantastic Experts in MoEs: A Unified Study for Expert Dropping Strategies and Observations",
        "abstract":"Sparsely activated Mixture-of-Experts (SMoE) has shown promise in scaling up the learning capacity of neural networks. However, vanilla SMoEs have issues such as expert redundancy and heavy memory requirements, making them inefficient and non-scalable, especially for resource-constrained scenarios. Expert-level sparsification of SMoEs involves pruning the least important experts to address these limitations. In this work, we aim to address three questions: (1) What is the best recipe to identify the least knowledgeable subset of experts that can be dropped with minimal impact on performance? (2) How should we perform expert dropping (one-shot or iterative), and what correction measures can we undertake to minimize its drastic impact on SMoE subnetwork capabilities? (3) What capabilities of full-SMoEs are severely impacted by the removal of the least dominant experts, and how can we recover them? Firstly, we propose MoE Experts Compression Suite (MC-Suite), which is a collection of some previously explored and multiple novel recipes to provide a comprehensive benchmark for estimating expert importance from diverse perspectives, as well as unveil numerous valuable insights for SMoE experts. Secondly, unlike prior works with a one-shot expert pruning approach, we explore the benefits of iterative pruning with the re-estimation of the MC-Suite criterion. Moreover, we introduce the benefits of task-agnostic fine-tuning as a correction mechanism during iterative expert dropping, which we term MoE Lottery Subnetworks. Lastly, we present an experimentally validated conjecture that, during expert dropping, SMoEs' instruction-following capabilities are predominantly hurt, which can be restored to a robust level subject to external augmentation of instruction-following capabilities using k-shot examples and supervised fine-tuning.\n        △ Less",
        "authors":"['Ajay Jaiswal', 'Jianyu Wang', 'Yixiao Li', 'Pingzhi Li', 'Tianlong Chen', 'Zhangyang Wang', 'Chong Wang', 'Ruoming Pang', 'Xianzhi Du']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning",
        "abstract":"Episodic tasks in Reinforcement Learning (RL) often pose challenges due to sparse reward signals and high-dimensional state spaces, which hinder efficient learning. Additionally, these tasks often feature hidden \"trap states\" -- irreversible failures that prevent task completion but do not provide explicit negative rewards to guide agents away from repeated errors. To address these issues, we propose Time-Weighted Contrastive Reward Learning (TW-CRL), an Inverse Reinforcement Learning (IRL) framework that leverages both successful and failed demonstrations. By incorporating temporal information, TW-CRL learns a dense reward function that identifies critical states associated with success or failure. This approach not only enables agents to avoid trap states but also encourages meaningful exploration beyond simple imitation of expert trajectories. Empirical evaluations on navigation tasks and robotic manipulation benchmarks demonstrate that TW-CRL surpasses state-of-the-art methods, achieving improved efficiency and robustness.\n        △ Less",
        "authors":"['Yuxuan Li', 'Ning Yang', 'Stephen Xia']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Gaze-Guided Learning: Avoiding Shortcut Bias in Visual Classification",
        "abstract":"Inspired by human visual attention, deep neural networks have widely adopted attention mechanisms to learn locally discriminative attributes for challenging visual classification tasks. However, existing approaches primarily emphasize the representation of such features while neglecting their precise localization, which often leads to misclassification caused by shortcut biases. This limitation becomes even more pronounced when models are evaluated on transfer or out-of-distribution datasets. In contrast, humans are capable of leveraging prior object knowledge to quickly localize and compare fine-grained attributes, a capability that is especially crucial in complex and high-variance classification scenarios. Motivated by this, we introduce Gaze-CIFAR-10, a human gaze time-series dataset, along with a dual-sequence gaze encoder that models the precise sequential localization of human attention on distinct local attributes. In parallel, a Vision Transformer (ViT) is employed to learn the sequential representation of image content. Through cross-modal fusion, our framework integrates human gaze priors with machine-derived visual sequences, effectively correcting inaccurate localization in image feature representations. Extensive qualitative and quantitative experiments demonstrate that gaze-guided cognitive cues significantly enhance classification accuracy.\n        △ Less",
        "authors":"['Jiahang Li', 'Shibo Xue', 'Yong Su']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BoolE: Exact Symbolic Reasoning via Boolean Equality Saturation",
        "abstract":"Boolean symbolic reasoning for gate-level netlists is a critical step in verification, logic and datapath synthesis, and hardware security. Specifically, reasoning datapath and adder tree in bit-blasted Boolean networks is particularly crucial for verification and synthesis, and challenging. Conventional approaches either fail to accurately (exactly) identify the function blocks of the designs in gate-level netlist with structural hashing and symbolic propagation, or their reasoning performance is highly sensitive to structure modifications caused by technology mapping or logic optimization. This paper introduces BoolE, an exact symbolic reasoning framework for Boolean netlists using equality saturation. BoolE optimizes scalability and performance by integrating domain-specific Boolean ruleset for term rewriting. We incorporate a novel extraction algorithm into BoolE to enhance its structural insight and computational efficiency, which adeptly identifies and captures multi-input, multi-output high-level structures (e.g., full adder) in the reconstructed e-graph.\n  Our experiments show that BoolE surpasses state-of-the-art symbolic reasoning baselines, including the conventional functional approach (ABC) and machine learning-based method (Gamora). Specifically, we evaluated its performance on various multiplier architecture with different configurations. Our results show that BoolE identifies $3.53\\times$ and $3.01\\times$ more exact full adders than ABC in carry-save array and Booth-encoded multipliers, respectively. Additionally, we integrated BoolE into multiplier formal verification tasks, where it significantly accelerates the performance of traditional formal verification tools using computer algebra, demonstrated over four orders of magnitude runtime improvements.\n        △ Less",
        "authors":"['Jiaqi Yin', 'Zhan Song', 'Chen Chen', 'Qihao Hu', 'Cunxi Yu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Lightweight Large Vision-language Model for Multimodal Medical Images",
        "abstract":"Medical Visual Question Answering (VQA) enhances clinical decision-making by enabling systems to interpret medical images and answer clinical queries. However, developing efficient, high-performance VQA models is challenging due to the complexity of medical imagery and diverse modalities. In this paper, we introduce a lightweight, multimodal VQA model integrating BiomedCLIP for image feature extraction and LLaMA-3 for text processing. Designed for medical VQA tasks, our model achieves state-of-the-art performance on the OmniMedVQA dataset. With approximately 8 billion parameters, it requires only two NVIDIA 40 GB A100 GPUs, demonstrating superior efficiency over larger models. Our results show 73.4% accuracy for open-end questions, surpassing existing models and validating its potential for real-world medical applications. Key contributions include a specialized multimodal VQA model, a resource-efficient architecture, and strong performance in answering open-ended clinical questions.\n        △ Less",
        "authors":"['Belal Alsinglawi', 'Chris McCarthy', 'Sara Webb', 'Christopher Fluke', 'Navid Toosy Saidy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cross-functional transferability in universal machine learning interatomic potentials",
        "abstract":"The rapid development of universal machine learning interatomic potentials (uMLIPs) has demonstrated the possibility for generalizable learning of the universal potential energy surface. In principle, the accuracy of uMLIPs can be further improved by bridging the model from lower-fidelity datasets to high-fidelity ones. In this work, we analyze the challenge of this transfer learning problem within the CHGNet framework. We show that significant energy scale shifts and poor correlations between GGA and r$^2$SCAN pose challenges to cross-functional data transferability in uMLIPs. By benchmarking different transfer learning approaches on the MP-r$^2$SCAN dataset of 0.24 million structures, we demonstrate the importance of elemental energy referencing in the transfer learning of uMLIPs. By comparing the scaling law with and without the pre-training on a low-fidelity dataset, we show that significant data efficiency can still be achieved through transfer learning, even with a target dataset of sub-million structures. We highlight the importance of proper transfer learning and multi-fidelity learning in creating next-generation uMLIPs on high-fidelity data.\n        △ Less",
        "authors":"['Xu Huang', 'Bowen Deng', 'Peichen Zhong', 'Aaron D. Kaplan', 'Kristin A. Persson', 'Gerbrand Ceder']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From Fairness to Truthfulness: Rethinking Data Valuation Design",
        "abstract":"As large language models increasingly rely on external data sources, fairly compensating data contributors has become a central concern. In this paper, we revisit the design of data markets through a game-theoretic lens, where data owners face private, heterogeneous costs for data sharing. We show that commonly used valuation methods--such as Leave-One-Out and Data Shapley--fail to ensure truthful reporting of these costs, leading to inefficient market outcomes. To address this, we adapt well-established payment rules from mechanism design, namely Myerson and Vickrey-Clarke-Groves (VCG), to the data market setting. We demonstrate that the Myerson payment is the minimal truthful payment mechanism, optimal from the buyer's perspective, and that VCG and Myerson payments coincide in unconstrained allocation settings. Our findings highlight the importance of incorporating incentive compatibility into data valuation, paving the way for more robust and efficient data markets.\n        △ Less",
        "authors":"['Dongyang Fan', 'Tyler J. Rotello', 'Sai Praneeth Karimireddy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Hierarchical Reinforcement Learning for Adaptive Traffic Signal Control",
        "abstract":"Multi-agent reinforcement learning (MARL) has shown promise for adaptive traffic signal control (ATSC), enabling multiple intersections to coordinate signal timings in real time. However, in large-scale settings, MARL faces constraints due to extensive data sharing and communication requirements. Federated learning (FL) mitigates these challenges by training shared models without directly exchanging raw data, yet traditional FL methods such as FedAvg struggle with highly heterogeneous intersections. Different intersections exhibit varying traffic patterns, demands, and road structures, so performing FedAvg across all agents is inefficient. To address this gap, we propose Hierarchical Federated Reinforcement Learning (HFRL) for ATSC. HFRL employs clustering-based or optimization-based techniques to dynamically group intersections and perform FedAvg independently within groups of intersections with similar characteristics, enabling more effective coordination and scalability than standard FedAvg. Our experiments on synthetic and real-world traffic networks demonstrate that HFRL not only outperforms both decentralized and standard federated RL approaches but also identifies suitable grouping patterns based on network structure or traffic demand, resulting in a more robust framework for distributed, heterogeneous systems.\n        △ Less",
        "authors":"['Yongjie Fu', 'Lingyun Zhong', 'Zifan Li', 'Xuan Di']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Riemannian Geometry for the classification of brain states with intracortical brain-computer interfaces",
        "abstract":"This study investigates the application of Riemannian geometry-based methods for brain decoding using invasive electrophysiological recordings. Although previously employed in non-invasive, the utility of Riemannian geometry for invasive datasets, which are typically smaller and scarcer, remains less explored. Here, we propose a Minimum Distance to Mean (MDM) classifier using a Riemannian geometry approach based on covariance matrices extracted from intracortical Local Field Potential (LFP) recordings across various regions during different brain state dynamics. For benchmarking, we evaluated the performance of our approach against Convolutional Neural Networks (CNNs) and Euclidean MDM classifiers. Our results indicate that the Riemannian geometry-based classification not only achieves a superior mean F1 macro-averaged score across different channel configurations but also requires up to two orders of magnitude less computational training time. Additionally, the geometric framework reveals distinct spatial contributions of brain regions across varying brain states, suggesting a state-dependent organization that traditional time series-based methods often fail to capture. Our findings align with previous studies supporting the efficacy of geometry-based methods and extending their application to invasive brain recordings, highlighting their potential for broader clinical use, such as brain computer interface applications.\n        △ Less",
        "authors":"['Arnau Marin-Llobet', 'Arnau Manasanch', 'Sergio Sanchez-Manso', 'Lluc Tresserras', 'Xinhe Zhang', 'Yining Hua', 'Hao Zhao', 'Melody Torao-Angosto', 'Maria V Sanchez-Vives', 'Leonardo Dalla Porta']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FORCE: Feature-Oriented Representation with Clustering and Explanation",
        "abstract":"Learning about underlying patterns in data using latent unobserved structures to improve the accuracy of predictive models has become an active avenue of deep learning research. Most approaches cluster the original features to capture certain latent structures. However, the information gained in the process can often be implicitly derived by sufficiently complex models. Thus, such approaches often provide minimal benefits. We propose a SHAP (Shapley Additive exPlanations) based supervised deep learning framework FORCE which relies on two-stage usage of SHAP values in the neural network architecture, (i) an additional latent feature to guide model training, based on clustering SHAP values, and (ii) initiating an attention mechanism within the architecture using latent information. This approach gives a neural network an indication about the effect of unobserved values that modify feature importance for an observation. The proposed framework is evaluated on three real life datasets. Our results demonstrate that FORCE led to dramatic improvements in overall performance as compared to networks that did not incorporate the latent feature and attention framework (e.g., F1 score for presence of heart disease 0.80 vs 0.72). Using cluster assignments and attention based on SHAP values guides deep learning, enhancing latent pattern learning and overall discriminative capability.\n        △ Less",
        "authors":"['Rishav Mukherjee', 'Jeffrey Ahearn Thompson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Reinforcement Finetuning via Adaptive Curriculum Learning",
        "abstract":"Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.\n        △ Less",
        "authors":"['Taiwei Shi', 'Yiyang Wu', 'Linxin Song', 'Tianyi Zhou', 'Jieyu Zhao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evaluating the Generalization Capabilities of Large Language Models on Code Reasoning",
        "abstract":"We assess how the code reasoning abilities of large language models (LLMs) generalize to different kinds of programs. We present techniques for obtaining in- and out-of-distribution programs with different characteristics: code sampled from a domain-specific language, code automatically generated by an LLM, code collected from competitive programming contests, and mutated versions of these programs. We also present an experimental methodology for evaluating LLM generalization by comparing their performance on these programs. We perform an extensive evaluation across 10 state-of-the-art models from the past year, obtaining insights into their generalization capabilities over time and across different classes of programs. Our results highlight that while earlier models exhibit behavior consistent with pattern matching, the latest models exhibit strong generalization abilities on code reasoning.\n        △ Less",
        "authors":"['Rem Yang', 'Julian Dai', 'Nikos Vasilakis', 'Martin Rinard']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"L3GS: Layered 3D Gaussian Splats for Efficient 3D Scene Delivery",
        "abstract":"Traditional 3D content representations include dense point clouds that consume large amounts of data and hence network bandwidth, while newer representations such as neural radiance fields suffer from poor frame rates due to their non-standard volumetric rendering pipeline. 3D Gaussian splats (3DGS) can be seen as a generalization of point clouds that meet the best of both worlds, with high visual quality and efficient rendering for real-time frame rates. However, delivering 3DGS scenes from a hosting server to client devices is still challenging due to high network data consumption (e.g., 1.5 GB for a single scene). The goal of this work is to create an efficient 3D content delivery framework that allows users to view high quality 3D scenes with 3DGS as the underlying data representation. The main contributions of the paper are: (1) Creating new layered 3DGS scenes for efficient delivery, (2) Scheduling algorithms to choose what splats to download at what time, and (3) Trace-driven experiments from users wearing virtual reality headsets to evaluate the visual quality and latency. Our system for Layered 3D Gaussian Splats delivery L3GS demonstrates high visual quality, achieving 16.9% higher average SSIM compared to baselines, and also works with other compressed 3DGS representations.\n        △ Less",
        "authors":"['Yi-Zhen Tsai', 'Xuechen Zhang', 'Zheng Li', 'Jiasi Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Prism: Dynamic and Flexible Benchmarking of LLMs Code Generation with Monte Carlo Tree Search",
        "abstract":"The rapid advancement of Large Language Models (LLMs) has outpaced traditional evaluation methods. Static benchmarks fail to capture the depth and breadth of LLM capabilities and eventually become obsolete, while most dynamic approaches either rely too heavily on LLM-based evaluation or remain constrained by predefined test sets. We introduce Prism, a flexible, dynamic benchmarking framework designed for comprehensive LLM assessment. Prism builds on three key components: (1) a tree-based state representation that models evaluation as a Markov Decision Process, (2) a Monte Carlo Tree Search algorithm adapted to uncover challenging evaluation scenarios, and (3) a multi-agent evaluation pipeline that enables simultaneous assessment of diverse capabilities. To ensure robust evaluation, Prism integrates structural measurements of tree exploration patterns with performance metrics across difficulty levels, providing detailed diagnostics of error patterns, test coverage, and solution approaches. Through extensive experiments on five state-of-the-art LLMs, we analyze how model architecture and scale influence code generation performance across varying task difficulties. Our results demonstrate Prism's effectiveness as a dynamic benchmark that evolves with model advancements while offering deeper insights into their limitations.\n        △ Less",
        "authors":"['Vahid Majdinasab', 'Amin Nikanjam', 'Foutse Khomh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural network-enhanced integrators for simulating ordinary differential equations",
        "abstract":"Numerous applications necessitate the computation of numerical solutions to differential equations across a wide range of initial conditions and system parameters, which feeds the demand for efficient yet accurate numerical integration methods.This study proposes a neural network (NN) enhancement of classical numerical integrators. NNs are trained to learn integration errors, which are then used as additive correction terms in numerical schemes. The performance of these enhanced integrators is compared with well-established methods through numerical studies, with a particular emphasis on computational efficiency. Analytical properties are examined in terms of local errors and backward error analysis. Embedded Runge-Kutta schemes are then employed to develop enhanced integrators that mitigate generalization risk, ensuring that the neural network's evaluation in previously unseen regions of the state space does not destabilize the integrator. It is guaranteed that the enhanced integrators perform at least as well as the desired classical Runge-Kutta schemes. The effectiveness of the proposed approaches is demonstrated through extensive numerical studies using a realistic model of a wind turbine, with parameters derived from the established simulation framework OpenFast.\n        △ Less",
        "authors":"['Amine Othmane', 'Kathrin Flaßkamp']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimal Bayesian Affine Estimator and Active Learning for the Wiener Model",
        "abstract":"This paper presents a Bayesian estimation framework for Wiener models, focusing on learning nonlinear output functions under known linear state dynamics. We derive a closed-form optimal affine estimator for the unknown parameters, characterized by the so-called \"dynamic basis statistics (DBS).\" Several features of the proposed estimator are studied, including Bayesian unbiasedness, closed-form posterior statistics, error monotonicity in trajectory length, and consistency condition (also known as persistent excitation). In the special case of Fourier basis functions, we demonstrate that the closed-form description is computationally available, as the Fourier DBS enjoys explicit expression. Furthermore, we identify an inherent inconsistency in single-trajectory measurements, regardless of input excitation. Leveraging the closed-form estimation error, we develop an active learning algorithm synthesizing input signals to minimize estimation error. Numerical experiments validate the efficacy of our approach, showing significant improvements over traditional regularized least-squares methods.\n        △ Less",
        "authors":"['Sasan Vakili', 'Manuel Mazo Jr.', 'Peyman Mohajerin Esfahani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph Databases",
        "abstract":"Large language models have shown remarkable language processing and reasoning ability but are prone to hallucinate when asked about private data. Retrieval-augmented generation (RAG) retrieves relevant data that fit into an LLM's context window and prompts the LLM for an answer. GraphRAG extends this approach to structured Knowledge Graphs (KGs) and questions regarding entities multiple hops away. The majority of recent GraphRAG methods either overlook the retrieval step or have ad hoc retrieval processes that are abstract or inefficient. This prevents them from being adopted when the KGs are stored in graph databases supporting graph query languages. In this work, we present GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate provably correct Cypher queries to retrieve high-quality subgraph contexts and produce accurate answers. Our method is the first such solution that can be taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks suggest that our method is sample-efficient and scales with the availability of training data. Our method achieves significantly better results than all state-of-the-art models across all four standard metrics on two challenging Q\\&As on large text-attributed KGs.\n        △ Less",
        "authors":"['Alfred Clemedtson', 'Borun Shi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph Neural Networks for Enhancing Ensemble Forecasts of Extreme Rainfall",
        "abstract":"Climate change is increasing the occurrence of extreme precipitation events, threatening infrastructure, agriculture, and public safety. Ensemble prediction systems provide probabilistic forecasts but exhibit biases and difficulties in capturing extreme weather. While post-processing techniques aim to enhance forecast accuracy, they rarely focus on precipitation, which exhibits complex spatial dependencies and tail behavior. Our novel framework leverages graph neural networks to post-process ensemble forecasts, specifically modeling the extremes of the underlying distribution. This allows to capture spatial dependencies and improves forecast accuracy for extreme events, thus leading to more reliable forecasts and mitigating risks of extreme precipitation and flooding.\n        △ Less",
        "authors":"['Christopher Bülte', 'Sohir Maskey', 'Philipp Scholl', 'Jonas von Berg', 'Gitta Kutyniok']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Quantum Mechanics and Neural Networks",
        "abstract":"We demonstrate that any Euclidean-time quantum mechanical theory may be represented as a neural network, ensured by the Kosambi-Karhunen-Loève theorem, mean-square path continuity, and finite two-point functions. The additional constraint of reflection positivity, which is related to unitarity, may be achieved by a number of mechanisms, such as imposing neural network parameter space splitting or the Markov property. Non-differentiability of the networks is related to the appearance of non-trivial commutators. Neural networks acting on Markov processes are no longer Markov, but still reflection positive, which facilitates the definition of deep neural network quantum systems. We illustrate these principles in several examples using numerical implementations, recovering classic quantum mechanical results such as Heisenberg uncertainty, non-trivial commutators, and the spectrum.\n        △ Less",
        "authors":"['Christian Ferko', 'James Halverson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Intermediate Layer Classifiers for OOD generalization",
        "abstract":"Deep classifiers are known to be sensitive to data distribution shifts, primarily due to their reliance on spurious correlations in training data. It has been suggested that these classifiers can still find useful features in the network's last layer that hold up under such shifts. In this work, we question the use of last-layer representations for out-of-distribution (OOD) generalisation and explore the utility of intermediate layers. To this end, we introduce \\textit{Intermediate Layer Classifiers} (ILCs). We discover that intermediate layer representations frequently offer substantially better generalisation than those from the penultimate layer. In many cases, zero-shot OOD generalisation using earlier-layer representations approaches the few-shot performance of retraining on penultimate layer representations. This is confirmed across multiple datasets, architectures, and types of distribution shifts. Our analysis suggests that intermediate layers are less sensitive to distribution shifts compared to the penultimate layer. These findings highlight the importance of understanding how information is distributed across network layers and its role in OOD generalisation, while also pointing to the limits of penultimate layer representation utility. Code is available at https:\/\/github.com\/oshapio\/intermediate-layer-generalization\n        △ Less",
        "authors":"['Arnas Uselis', 'Seong Joon Oh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Large-Scale Classification of Shortwave Communication Signals with Machine Learning",
        "abstract":"This paper presents a deep learning approach to the classification of 160 shortwave radio signals. It addresses the typical challenges of the shortwave spectrum, which are the large number of different signal types, the presence of various analog modulations and ionospheric propagation. As a classifier a deep convolutional neural network is used, that is trained to recognize 160 typical shortwave signal classes. The approach is blind and therefore does not require preknowledge or special preprocessing of the signal and no manual design of discriminative features for each signal class. The network is trained on a large number of synthetically generated signals and high quality recordings. Finally, the network is evaluated on real-world radio signals obtained from globally deployed receiver hardware and achieves up to 90% accuracy for an observation time of only 1 second.\n        △ Less",
        "authors":"['Stefan Scholl']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GraphPINE: Graph Importance Propagation for Interpretable Drug Response Prediction",
        "abstract":"Explainability is necessary for many tasks in biomedical research. Recent explainability methods have focused on attention, gradient, and Shapley value. These do not handle data with strong associated prior knowledge and fail to constrain explainability results based on known relationships between predictive features.\n  We propose GraphPINE, a graph neural network (GNN) architecture leveraging domain-specific prior knowledge to initialize node importance optimized during training for drug response prediction. Typically, a manual post-prediction step examines literature (i.e., prior knowledge) to understand returned predictive features. While node importance can be obtained for gradient and attention after prediction, node importance from these methods lacks complementary prior knowledge; GraphPINE seeks to overcome this limitation. GraphPINE differs from other GNN gating methods by utilizing an LSTM-like sequential format. We introduce an importance propagation layer that unifies 1) updates for feature matrix and node importance and 2) uses GNN-based graph propagation of feature values. This initialization and updating mechanism allows for informed feature learning and improved graph representation.\n  We apply GraphPINE to cancer drug response prediction using drug screening and gene data collected for over 5,000 gene nodes included in a gene-gene graph with a drug-target interaction (DTI) graph for initial importance. The gene-gene graph and DTIs were obtained from curated sources and weighted by article count discussing relationships between drugs and genes. GraphPINE achieves a PR-AUC of 0.894 and ROC-AUC of 0.796 across 952 drugs. Code is available at https:\/\/anonymous.4open.science\/r\/GraphPINE-40DE.\n        △ Less",
        "authors":"['Yoshitaka Inoue', 'Tianfan Fu', 'Augustin Luna']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Survey on Algorithms for multi-index models",
        "abstract":"We review the literature on algorithms for estimating the index space in a multi-index model. The primary focus is on computationally efficient (polynomial-time) algorithms in Gaussian space, the assumptions under which consistency is guaranteed by these methods, and their sample complexity. In many cases, a gap is observed between the sample complexity of the best known computationally efficient methods and the information-theoretical minimum. We also review algorithms based on estimating the span of gradients using nonparametric methods, and algorithms based on fitting neural networks using gradient descent\n        △ Less",
        "authors":"['Joan Bruna', 'Daniel Hsu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Behavior-Based Knowledge Representation Improves Prediction of Players' Moves in Chess by 25%",
        "abstract":"Predicting player behavior in strategic games, especially complex ones like chess, presents a significant challenge. The difficulty arises from several factors. First, the sheer number of potential outcomes stemming from even a single position, starting from the initial setup, makes forecasting a player's next move incredibly complex. Second, and perhaps even more challenging, is the inherent unpredictability of human behavior. Unlike the optimized play of engines, humans introduce a layer of variability due to differing playing styles and decision-making processes. Each player approaches the game with a unique blend of strategic thinking, tactical awareness, and psychological tendencies, leading to diverse and often unexpected actions. This stylistic variation, combined with the capacity for creativity and even irrational moves, makes predicting human play difficult. Chess, a longstanding benchmark of artificial intelligence research, has seen significant advancements in tools and automation. Engines like Deep Blue, AlphaZero, and Stockfish can defeat even the most skilled human players. However, despite their exceptional ability to outplay top-level grandmasters, predicting the moves of non-grandmaster players, who comprise most of the global chess community -- remains complicated for these engines. This paper proposes a novel approach combining expert knowledge with machine learning techniques to predict human players' next moves. By applying feature engineering grounded in domain expertise, we seek to uncover the patterns in the moves of intermediate-level chess players, particularly during the opening phase of the game. Our methodology offers a promising framework for anticipating human behavior, advancing both the fields of AI and human-computer interaction.\n        △ Less",
        "authors":"['Benny Skidanov', 'Daniel Erbesfeld', 'Gera Weiss', 'Achiya Elyasaf']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EP-Diffuser: An Efficient Diffusion Model for Traffic Scene Generation and Prediction via Polynomial Representations",
        "abstract":"As the prediction horizon increases, predicting the future evolution of traffic scenes becomes increasingly difficult due to the multi-modal nature of agent motion. Most state-of-the-art (SotA) prediction models primarily focus on forecasting the most likely future. However, for the safe operation of autonomous vehicles, it is equally important to cover the distribution for plausible motion alternatives. To address this, we introduce EP-Diffuser, a novel parameter-efficient diffusion-based generative model designed to capture the distribution of possible traffic scene evolutions. Conditioned on road layout and agent history, our model acts as a predictor and generates diverse, plausible scene continuations. We benchmark EP-Diffuser against two SotA models in terms of accuracy and plausibility of predictions on the Argoverse 2 dataset. Despite its significantly smaller model size, our approach achieves both highly accurate and plausible traffic scene predictions. We further evaluate model generalization ability in an out-of-distribution (OoD) test setting using Waymo Open dataset and show superior robustness of our approach. The code and model checkpoints can be found here: https:\/\/github.com\/continental\/EP-Diffuser.\n        △ Less",
        "authors":"['Yue Yao', 'Mohamed-Khalil Bouzidi', 'Daniel Goehring', 'Joerg Reichardt']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Less but Better: Parameter-Efficient Fine-Tuning of Large Language Models for Personality Detection",
        "abstract":"Personality detection automatically identifies an individual's personality from various data sources, such as social media texts. However, as the parameter scale of language models continues to grow, the computational cost becomes increasingly difficult to manage. Fine-tuning also grows more complex, making it harder to justify the effort and reliably predict outcomes. We introduce a novel parameter-efficient fine-tuning framework, PersLLM, to address these challenges. In PersLLM, a large language model (LLM) extracts high-dimensional representations from raw data and stores them in a dynamic memory layer. PersLLM then updates the downstream layers with a replaceable output network, enabling flexible adaptation to various personality detection scenarios. By storing the features in the memory layer, we eliminate the need for repeated complex computations by the LLM. Meanwhile, the lightweight output network serves as a proxy for evaluating the overall effectiveness of the framework, improving the predictability of results. Experimental results on key benchmark datasets like Kaggle and Pandora show that PersLLM significantly reduces computational cost while maintaining competitive performance and strong adaptability.\n        △ Less",
        "authors":"['Lingzhi Shen', 'Yunfei Long', 'Xiaohao Cai', 'Guanming Chen', 'Imran Razzak', 'Shoaib Jameel']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling",
        "abstract":"The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed $100,000$ tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.\n        △ Less",
        "authors":"['Benjamin Lipkin', 'Benjamin LeBrun', 'Jacob Hoover Vigly', 'João Loula', 'David R. MacIver', 'Li Du', 'Jason Eisner', 'Ryan Cotterell', 'Vikash Mansinghka', \"Timothy J. O'Donnell\", 'Alexander K. Lew', 'Tim Vieira']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TRATSS: Transformer-Based Task Scheduling System for Autonomous Vehicles",
        "abstract":"Efficient scheduling remains a critical challenge in various domains, requiring solutions to complex NP-hard optimization problems to achieve optimal resource allocation and maximize productivity. In this paper, we introduce a framework called Transformer-Based Task Scheduling System (TRATSS), designed to address the intricacies of single agent scheduling in graph-based environments. By integrating the latest advancements in reinforcement learning and transformer architecture, TRATSS provides a novel system that outputs optimized task scheduling decisions while dynamically adapting to evolving task requirements and resource availability. Leveraging the self-attention mechanism in transformers, TRATSS effectively captures complex task dependencies, thereby providing solutions with enhanced resource utilization and task completion efficiency. Experimental evaluations on benchmark datasets demonstrate TRATSS's effectiveness in providing high-quality solutions to scheduling problems that involve multiple action profiles.\n        △ Less",
        "authors":"['Yazan Youssef', 'Paulo Ricardo Marques de Araujo', 'Aboelmagd Noureldin', 'Sidney Givigi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Role of Environment Access in Agnostic Reinforcement Learning",
        "abstract":"We study Reinforcement Learning (RL) in environments with large state spaces, where function approximation is required for sample-efficient learning. Departing from a long history of prior work, we consider the weakest possible form of function approximation, called agnostic policy learning, where the learner seeks to find the best policy in a given class $Π$, with no guarantee that $Π$ contains an optimal policy for the underlying task. Although it is known that sample-efficient agnostic policy learning is not possible in the standard online RL setting without further assumptions, we investigate the extent to which this can be overcome with stronger forms of access to the environment. Specifically, we show that: 1. Agnostic policy learning remains statistically intractable when given access to a local simulator, from which one can reset to any previously seen state. This result holds even when the policy class is realizable, and stands in contrast to a positive result of [MFR24] showing that value-based learning under realizability is tractable with local simulator access. 2. Agnostic policy learning remains statistically intractable when given online access to a reset distribution with good coverage properties over the state space (the so-called $μ$-reset setting). We also study stronger forms of function approximation for policy learning, showing that PSDP [BKSN03] and CPI [KL02] provably fail in the absence of policy completeness. 3. On a positive note, agnostic policy learning is statistically tractable for Block MDPs with access to both of the above reset models. We establish this via a new algorithm that carefully constructs a policy emulator: a tabular MDP with a small state space that approximates the value functions of all policies $π\\in Π$. These values are approximated without any explicit value function class.\n        △ Less",
        "authors":"['Akshay Krishnamurthy', 'Gene Li', 'Ayush Sekhari']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Handling Weather Uncertainty in Air Traffic Prediction through an Inverse Approach",
        "abstract":"Adverse weather conditions, particularly convective phenomena, pose significant challenges to Air Traffic Management, often requiring real-time rerouting decisions that impact efficiency and safety. This study introduces a 3-D Gaussian Mixture Model to predict long lead-time flight trajectory changes, incorporating comprehensive weather and traffic data. Utilizing high-resolution meteorological datasets, including convective weather maps and wind data, alongside traffic records, the model demonstrates robust performance in forecasting reroutes up to 60 minutes. The novel 3-D Gaussian Mixture Model framework employs a probabilistic approach to capture uncertainty while providing accurate forecasts of altitude, latitude, and longitude. Extensive evaluation revealed a Mean Absolute Percentage Error below 0.02 across varying lead times, highlighting the model's accuracy and scalability. By integrating explainability techniques such as the Vanilla Gradient algorithm, the study provides insights into feature contributions, showing that they contribute to improving Air Traffic Management strategies to mitigate weather-induced disruptions.\n        △ Less",
        "authors":"['G. Lancia', 'D. Falanga', 'S. Alam', 'G. Lulli']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Nature-Inspired Colony of Artificial Intelligence System with Fast, Detailed, and Organized Learner Agents for Enhancing Diversity and Quality",
        "abstract":"The concepts of convolutional neural networks (CNNs) and multi-agent systems are two important areas of research in artificial intelligence (AI). In this paper, we present an approach that builds a CNN-based colony of AI agents to serve as a single system and perform multiple tasks (e.g., predictions or classifications) in an environment. The proposed system impersonates the natural environment of a biological system, like an ant colony or a human colony. The proposed colony of AI that is defined as a role-based system uniquely contributes to accomplish tasks in an environment by incorporating AI agents that are fast learners, detailed learners, and organized learners. These learners can enhance their localized learning and their collective decisions as a single system of colony of AI agents. This approach also enhances the diversity and quality of the colony of AI with the help of Genetic Algorithms and their crossover and mutation mechanisms. The evolution of fast, detailed, and organized learners in the colony of AI is achieved by introducing a unique one-to-one mapping between these learners and the pretrained VGG16, VGG19, and ResNet50 models, respectively. This role-based approach creates two parent-AI agents using the AI models through the processes, called the intra- and inter-marriage of AI, so that they can share their learned knowledge (weights and biases) based on a probabilistic rule and produce diversified child-AI agents to perform new tasks. This process will form a colony of AI that consists of families of multi-model and mixture-model AI agents to improve diversity and quality. Simulations show that the colony of AI, built using the VGG16, VGG19, and ResNet50 models, can provide a single system that generates child-AI agents of excellent predictive performance, ranging between 82% and 95% of F1-scores, to make diversified collective and quality decisions on a task.\n        △ Less",
        "authors":"['Shan Suthaharan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Of All StrIPEs: Investigating Structure-informed Positional Encoding for Efficient Music Generation",
        "abstract":"While music remains a challenging domain for generative models like Transformers, a two-pronged approach has recently proved successful: inserting musically-relevant structural information into the positional encoding (PE) module and using kernel approximation techniques based on Random Fourier Features (RFF) to lower the computational cost from quadratic to linear. Yet, it is not clear how such RFF-based efficient PEs compare with those based on rotation matrices, such as Rotary Positional Encoding (RoPE). In this paper, we present a unified framework based on kernel methods to analyze both families of efficient PEs. We use this framework to develop a novel PE method called RoPEPool, capable of extracting causal relationships from temporal sequences. Using RFF-based PEs and rotation-based PEs, we demonstrate how seemingly disparate PEs can be jointly studied by considering the content-context interactions they induce. For empirical validation, we use a symbolic music generation task, namely, melody harmonization. We show that RoPEPool, combined with highly-informative structural priors, outperforms all methods.\n        △ Less",
        "authors":"['Manvi Agarwal', 'Changhong Wang', 'Gael Richard']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Find A Winning Sign: Sign Is All We Need to Win the Lottery",
        "abstract":"The Lottery Ticket Hypothesis (LTH) posits the existence of a sparse subnetwork (a.k.a. winning ticket) that can generalize comparably to its over-parameterized counterpart when trained from scratch. The common approach to finding a winning ticket is to preserve the original strong generalization through Iterative Pruning (IP) and transfer information useful for achieving the learned generalization by applying the resulting sparse mask to an untrained network. However, existing IP methods still struggle to generalize their observations beyond ad-hoc initialization and small-scale architectures or datasets, or they bypass these challenges by applying their mask to trained weights instead of initialized ones. In this paper, we demonstrate that the parameter sign configuration plays a crucial role in conveying useful information for generalization to any randomly initialized network. Through linear mode connectivity analysis, we observe that a sparse network trained by an existing IP method can retain its basin of attraction if its parameter signs and normalization layer parameters are preserved. To take a step closer to finding a winning ticket, we alleviate the reliance on normalization layer parameters by preventing high error barriers along the linear path between the sparse network trained by our method and its counterpart with initialized normalization layer parameters. Interestingly, across various architectures and datasets, we observe that any randomly initialized network can be optimized to exhibit low error barriers along the linear path to the sparse network trained by our method by inheriting its sparsity and parameter sign information, potentially achieving performance comparable to the original. The code is available at https:\/\/github.com\/JungHunOh\/AWS\\_ICLR2025.git\n        △ Less",
        "authors":"['Junghun Oh', 'Sungyong Baik', 'Kyoung Mu Lee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DyTTP: Trajectory Prediction with Normalization-Free Transformers",
        "abstract":"Accurate trajectory prediction is a cornerstone for the safe operation of autonomous driving systems, where understanding the dynamic behavior of surrounding agents is crucial. Transformer-based architectures have demonstrated significant promise in capturing complex spatio-temporality dependencies. However, their reliance on normalization layers can lead to computation overhead and training instabilities. In this work, we present a two-fold approach to address these challenges. First, we integrate DynamicTanh (DyT), which is the latest method to promote transformers, into the backbone, replacing traditional layer normalization. This modification simplifies the network architecture and improves the stability of the inference. We are the first work to deploy the DyT to the trajectory prediction task. Complementing this, we employ a snapshot ensemble strategy to further boost trajectory prediction performance. Using cyclical learning rate scheduling, multiple model snapshots are captured during a single training run. These snapshots are then aggregated via simple averaging at inference time, allowing the model to benefit from diverse hypotheses without incurring substantial additional computational cost. Extensive experiments on Argoverse datasets demonstrate that our combined approach significantly improves prediction accuracy, inference speed and robustness in diverse driving scenarios. This work underscores the potential of normalization-free transformer designs augmented with lightweight ensemble techniques in advancing trajectory forecasting for autonomous vehicles.\n        △ Less",
        "authors":"['Yunxiang Liu', 'Hongkuo Niu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Learning for Double Auction",
        "abstract":"Auctions are important mechanisms extensively implemented in various markets, e.g., search engines' keyword auctions, antique auctions, etc. Finding an optimal auction mechanism is extremely difficult due to the constraints of imperfect information, incentive compatibility (IC), and individual rationality (IR). In addition to the traditional economic methods, some recently attempted to find the optimal (single) auction using deep learning methods. Unlike those attempts focusing on single auctions, we develop deep learning methods for double auctions, where imperfect information exists on both the demand and supply sides. The previous attempts on single auction cannot directly apply to our contexts and those attempts additionally suffer from limited generalizability, inefficiency in ensuring the constraints, and learning fluctuations. We innovate in designing deep learning models for solving the more complex problem and additionally addressing the previous models' three limitations. Specifically, we achieve generalizability by leveraging a transformer-based architecture to model market participants as sequences for varying market sizes; we utilize the numerical features of the constraints and pre-treat them for a higher learning efficiency; we develop a gradient-conflict-elimination scheme to address the problem of learning fluctuation. Extensive experimental evaluations demonstrate the superiority of our approach to classical and machine learning baselines.\n        △ Less",
        "authors":"['Jiayin Liu', 'Chenglong Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Achieving binary weight and activation for LLMs using Post-Training Quantization",
        "abstract":"Quantizing large language models (LLMs) to 1-bit precision significantly reduces computational costs, but existing quantization techniques suffer from noticeable performance degradation when using weight and activation precisions below 4 bits (W4A4). In this paper, we propose a post-training quantization framework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit with an additional 1 bit for fine-grain grouping and activations are quantized to 1 bit with a 4-fold increase in the number of channels. For weight quantization, we propose utilizing Hessian-aware fine-grained grouping along with an EM-based quantization scheme. For activation quantization, we decompose INT4-quantized activations into a 4 * INT1 format equivalently and simultaneously smooth the scaling factors based on quantization errors, which further reduces the quantization errors in activations. Our method surpasses state-of-the-art (SOTA) LLM quantization baselines on W2A4 across multiple tasks, pushing the boundaries of existing LLM quantization methods toward fully binarized models.\n        △ Less",
        "authors":"['Siqing Song', 'Chuang Wang', 'Ruiqi Wang', 'Yi Yang', 'Xuyao Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Non-linear Phillips Curve for India: Evidence from Explainable Machine Learning",
        "abstract":"The conventional linear Phillips curve model, while widely used in policymaking, often struggles to deliver accurate forecasts in the presence of structural breaks and inherent nonlinearities. This paper addresses these limitations by leveraging machine learning methods within a New Keynesian Phillips Curve framework to forecast and explain headline inflation in India, a major emerging economy. Our analysis demonstrates that machine learning-based approaches significantly outperform standard linear models in forecasting accuracy. Moreover, by employing explainable machine learning techniques, we reveal that the Phillips curve relationship in India is highly nonlinear, characterized by thresholds and interaction effects among key variables. Headline inflation is primarily driven by inflation expectations, followed by past inflation and the output gap, while supply shocks, except rainfall, exert only a marginal influence. These findings highlight the ability of machine learning models to improve forecast accuracy and uncover complex, nonlinear dynamics in inflation data, offering valuable insights for policymakers.\n        △ Less",
        "authors":"['Shovon Sengupta', 'Bhanu Pratap', 'Amit Pawar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hyperflows: Pruning Reveals the Importance of Weights",
        "abstract":"Network pruning is used to reduce inference latency and power consumption in large neural networks. However, most existing methods struggle to accurately assess the importance of individual weights due to their inherent interrelatedness, leading to poor performance, especially at extreme sparsity levels. We introduce Hyperflows, a dynamic pruning approach that estimates each weight's importance by observing the network's gradient response to the weight's removal. A global pressure term continuously drives all weights toward pruning, with those critical for accuracy being automatically regrown based on their flow, the aggregated gradient signal when they are absent. We explore the relationship between final sparsity and pressure, deriving power-law equations similar to those found in neural scaling laws. Empirically, we demonstrate state-of-the-art results with ResNet-50 and VGG-19 on CIFAR-10 and CIFAR-100.\n        △ Less",
        "authors":"['Eugen Barbulescu', 'Antonio Alexoaie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Structuring Multiple Simple Cycle Reservoirs with Particle Swarm Optimization",
        "abstract":"Reservoir Computing (RC) is a time-efficient computational paradigm derived from Recurrent Neural Networks (RNNs). The Simple Cycle Reservoir (SCR) is an RC model that stands out for its minimalistic design, offering extremely low construction complexity and proven capability of universally approximating time-invariant causal fading memory filters, even in the linear dynamics regime. This paper introduces Multiple Simple Cycle Reservoirs (MSCRs), a multi-reservoir framework that extends Echo State Networks (ESNs) by replacing a single large reservoir with multiple interconnected SCRs. We demonstrate that optimizing MSCR using Particle Swarm Optimization (PSO) outperforms existing multi-reservoir models, achieving competitive predictive performance with a lower-dimensional state space. By modeling interconnections as a weighted Directed Acyclic Graph (DAG), our approach enables flexible, task-specific network topology adaptation. Numerical simulations on three benchmark time-series prediction tasks confirm these advantages over rival algorithms. These findings highlight the potential of MSCR-PSO as a promising framework for optimizing multi-reservoir systems, providing a foundation for further advancements and applications of interconnected SCRs for developing efficient AI devices.\n        △ Less",
        "authors":"['Ziqiang Li', 'Robert Simon Fong', 'Kantaro Fujiwara', 'Kazuyuki Aihara', 'Gouhei Tanaka']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Thanos: A Block-wise Pruning Algorithm for Efficient Large Language Model Compression",
        "abstract":"This paper presents Thanos, a novel weight-pruning algorithm designed to reduce the memory footprint and enhance the computational efficiency of large language models (LLMs) by removing redundant weights while maintaining accuracy. Thanos introduces a block-wise pruning strategy with adaptive masks that dynamically adjust to weight importance, enabling flexible sparsity patterns and structured formats, such as $n:m$ sparsity, optimized for hardware acceleration. Experimental evaluations demonstrate that Thanos achieves state-of-the-art performance in structured pruning and outperforms existing methods in unstructured pruning. By providing an efficient and adaptable approach to model compression, Thanos offers a practical solution for deploying large models in resource-constrained environments.\n        △ Less",
        "authors":"['Ivan Ilin', 'Peter Richtarik']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ZeroED: Hybrid Zero-shot Error Detection through Large Language Model Reasoning",
        "abstract":"Error detection (ED) in tabular data is crucial yet challenging due to diverse error types and the need for contextual understanding. Traditional ED methods often rely heavily on manual criteria and labels, making them labor-intensive. Large language models (LLM) can minimize human effort but struggle with errors requiring a comprehensive understanding of data context. In this paper, we propose ZeroED, a novel hybrid zero-shot error detection framework, which combines LLM reasoning ability with the manual label-based ED pipeline. ZeroED operates in four steps, i.e., feature representation, error labeling, training data construction, and detector training. Initially, to enhance error distinction, ZeroED generates rich data representations using error reason-aware binary features, pre-trained embeddings, and statistical features. Then, ZeroED employs LLM to label errors holistically through in-context learning, guided by a two-step reasoning process for detailed error detection guidelines. To reduce token costs, LLMs are applied only to representative data selected via clustering-based sampling. High-quality training data is constructed through in-cluster label propagation and LLM augmentation with verification. Finally, a classifier is trained to detect all errors. Extensive experiments on seven public datasets demonstrate that, ZeroED substantially outperforms state-of-the-art methods by a maximum 30% improvement in F1 score and up to 90% token cost reduction.\n        △ Less",
        "authors":"['Wei Ni', 'Kaihang Zhang', 'Xiaoye Miao', 'Xiangyu Zhao', 'Yangyang Wu', 'Yaoshu Wang', 'Jianwei Yin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Divergent Paths: Separating Homophilic and Heterophilic Learning for Enhanced Graph-level Representations",
        "abstract":"Graph Convolutional Networks (GCNs) are predominantly tailored for graphs displaying homophily, where similar nodes connect, but often fail on heterophilic graphs. The strategy of adopting distinct approaches to learn from homophilic and heterophilic components in node-level tasks has been widely discussed and proven effective both theoretically and experimentally. However, in graph-level tasks, research on this topic remains notably scarce. Addressing this gap, our research conducts an analysis on graphs with nodes' category ID available, distinguishing intra-category and inter-category components as embodiment of homophily and heterophily, respectively. We find while GCNs excel at extracting information within categories, they frequently capture noise from inter-category components. Consequently, it is crucial to employ distinct learning strategies for intra- and inter-category elements. To alleviate this problem, we separately learn the intra- and inter-category parts by a combination of an intra-category convolution (IntraNet) and an inter-category high-pass graph convolution (InterNet). Our IntraNet is supported by sophisticated graph preprocessing steps and a novel category-based graph readout function. For the InterNet, we utilize a high-pass filter to amplify the node disparities, enhancing the recognition of details in the high-frequency components. The proposed approach, DivGNN, combines the IntraNet and InterNet with a gated mechanism and substantially improves classification performance on graph-level tasks, surpassing traditional GNN baselines in effectiveness.\n        △ Less",
        "authors":"['Han Lei', 'Jiaxing Xu', 'Xia Dong', 'Yiping Ke']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AROMA: Autonomous Rank-one Matrix Adaptation",
        "abstract":"As large language models continue to grow in size, parameter-efficient fine-tuning (PEFT) has become increasingly crucial. While low-rank adaptation (LoRA) offers a solution through low-rank updates, its static rank allocation may yield suboptimal results. Adaptive low-rank adaptation (AdaLoRA) improves this with dynamic allocation but remains sensitive to initial and target rank configurations. We introduce AROMA, a framework that automatically constructs layer-specific updates by iteratively building up rank-one components with very few trainable parameters that gradually diminish to zero. Unlike existing methods that employ rank reduction mechanisms, AROMA introduces a dual-loop architecture for rank growth. The inner loop extracts information from each rank-one subspace, while the outer loop determines the number of rank-one subspaces, i.e., the optimal rank. We reset optimizer states to maintain subspace independence. AROMA significantly reduces parameters compared to LoRA and AdaLoRA while achieving superior performance on natural language understanding and commonsense reasoning tasks, offering new insights into adaptive PEFT. The code is available at \\href{https:\/\/github.com\/ShuDun23\/AROMA}{AROMA}.\n        △ Less",
        "authors":"['Hao Nan Sheng', 'Zhi-yong Wang', 'Mingrui Yang', 'Hing Cheung So']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MASS: MoErging through Adaptive Subspace Selection",
        "abstract":"Model merging has recently emerged as a lightweight alternative to ensembling, combining multiple fine-tuned models into a single set of parameters with no additional training overhead. Yet, existing merging methods fall short of matching the full accuracy of separately fine-tuned endpoints. We present MASS (MoErging through Adaptive Subspace Selection), a new approach that closes this gap by unifying multiple fine-tuned models while retaining near state-of-the-art performance across tasks. Building on the low-rank decomposition of per-task updates, MASS stores only the most salient singular components for each task and merges them into a shared model. At inference time, a non-parametric, data-free router identifies which subspace (or combination thereof) best explains an input's intermediate features and activates the corresponding task-specific block. This procedure is fully training-free and introduces only a two-pass inference overhead plus a ~2 storage factor compared to a single pretrained model, irrespective of the number of tasks. We evaluate MASS on CLIP-based image classification using ViT-B-16, ViT-B-32 and ViT-L-14 for benchmarks of 8, 14 and 20 tasks respectively, establishing a new state-of-the-art. Most notably, MASS recovers up to ~98% of the average accuracy of individual fine-tuned models, making it a practical alternative to ensembling at a fraction of the storage cost.\n        △ Less",
        "authors":"['Donato Crisostomi', 'Alessandro Zirilli', 'Antonio Andrea Gargiulo', 'Maria Sofia Bucarelli', 'Simone Scardapane', 'Fabrizio Silvestri', 'Iacopo Masi', 'Emanuele Rodolà']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Three-Factor Learning in Spiking Neural Networks: An Overview of Methods and Trends from a Machine Learning Perspective",
        "abstract":"Three-factor learning rules in Spiking Neural Networks (SNNs) have emerged as a crucial extension to traditional Hebbian learning and Spike-Timing-Dependent Plasticity (STDP), incorporating neuromodulatory signals to improve adaptation and learning efficiency. These mechanisms enhance biological plausibility and facilitate improved credit assignment in artificial neural systems. This paper takes a view on this topic from a machine learning perspective, providing an overview of recent advances in three-factor learning, discusses theoretical foundations, algorithmic implementations, and their relevance to reinforcement learning and neuromorphic computing. In addition, we explore interdisciplinary approaches, scalability challenges, and potential applications in robotics, cognitive modeling, and AI systems. Finally, we highlight key research gaps and propose future directions for bridging the gap between neuroscience and artificial intelligence.\n        △ Less",
        "authors":"['Szymon Mazurek', 'Jakub Caputa', 'Jan K. Argasiński', 'Maciej Wielgosz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving Early Prediction of Type 2 Diabetes Mellitus with ECG-DiaNet: A Multimodal Neural Network Leveraging Electrocardiogram and Clinical Risk Factors",
        "abstract":"Type 2 Diabetes Mellitus (T2DM) remains a global health challenge, underscoring the need for early and accurate risk prediction. This study presents ECG-DiaNet, a multimodal deep learning model that integrates electrocardiogram (ECG) features with clinical risk factors (CRFs) to enhance T2DM onset prediction. Using data from Qatar Biobank (QBB), we trained and validated models on a development cohort (n=2043) and evaluated performance on a longitudinal test set (n=395) with five-year follow-up. ECG-DiaNet outperformed unimodal ECG-only and CRF-only models, achieving a higher AUROC (0.845 vs 0.8217) than the CRF-only model, with statistical significance (DeLong p<0.001). Reclassification metrics further confirmed improvements: Net Reclassification Improvement (NRI=0.0153) and Integrated Discrimination Improvement (IDI=0.0482). Risk stratification into low-, medium-, and high-risk groups showed ECG-DiaNet achieved superior positive predictive value (PPV) in high-risk individuals. The model's reliance on non-invasive and widely available ECG signals supports its feasibility in clinical and community health settings. By combining cardiac electrophysiology and systemic risk profiles, ECG-DiaNet addresses the multifactorial nature of T2DM and supports precision prevention. These findings highlight the value of multimodal AI in advancing early detection and prevention strategies for T2DM, particularly in underrepresented Middle Eastern populations.\n        △ Less",
        "authors":"['Farida Mohsen', 'Zubair Shah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Quantum Adaptive Self-Attention for Quantum Transformer Models",
        "abstract":"Transformer models have revolutionized sequential learning across various domains, yet their self-attention mechanism incurs quadratic computational cost, posing limitations for real-time and resource-constrained tasks. To address this, we propose Quantum Adaptive Self-Attention (QASA), a novel hybrid architecture that enhances classical Transformer models with a quantum attention mechanism. QASA replaces dot-product attention with a parameterized quantum circuit (PQC) that adaptively captures inter-token relationships in the quantum Hilbert space. Additionally, a residual quantum projection module is introduced before the feedforward network to further refine temporal features. Our design retains classical efficiency in earlier layers while injecting quantum expressiveness in the final encoder block, ensuring compatibility with current NISQ hardware. Experiments on synthetic time-series tasks demonstrate that QASA achieves faster convergence and superior generalization compared to both standard Transformers and reduced classical variants. Preliminary complexity analysis suggests potential quantum advantages in gradient computation, opening new avenues for efficient quantum deep learning models.\n        △ Less",
        "authors":"['Chi-Sheng Chen', 'En-Jui Kuo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Impact of Price Inflation on Algorithmic Collusion Through Reinforcement Learning Agents",
        "abstract":"Algorithmic pricing is increasingly shaping market competition, raising concerns about its potential to compromise competitive dynamics. While prior work has shown that reinforcement learning (RL)-based pricing algorithms can lead to tacit collusion, less attention has been given to the role of macroeconomic factors in shaping these dynamics. This study examines the role of inflation in influencing algorithmic collusion within competitive markets. By incorporating inflation shocks into a RL-based pricing model, we analyze whether agents adapt their strategies to sustain supra-competitive profits. Our findings indicate that inflation reduces market competitiveness by fostering implicit coordination among agents, even without direct collusion. However, despite achieving sustained higher profitability, agents fail to develop robust punishment mechanisms to deter deviations from equilibrium strategies. The results suggest that inflation amplifies non-competitive dynamics in algorithmic pricing, emphasizing the need for regulatory oversight in markets where AI-driven pricing is prevalent.\n        △ Less",
        "authors":"['Sebastián Tinoco', 'Andrés Abeliuk', 'Javier Ruiz del Solar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Level Generation with Constrained Expressive Range",
        "abstract":"Expressive range analysis is a visualization-based technique used to evaluate the performance of generative models, particularly in game level generation. It typically employs two quantifiable metrics to position generated artifacts on a 2D plot, offering insight into how content is distributed within a defined metric space. In this work, we use the expressive range of a generator as the conceptual space of possible creations. Inspired by the quality diversity paradigm, we explore this space to generate levels. To do so, we use a constraint-based generator that systematically traverses and generates levels in this space. To train the constraint-based generator we use different tile patterns to learn from the initial example levels. We analyze how different patterns influence the exploration of the expressive range. Specifically, we compare the exploration process based on time, the number of successful and failed sample generations, and the overall interestingness of the generated levels. Unlike typical quality diversity approaches that rely on random generation and hope to get good coverage of the expressive range, this approach systematically traverses the grid ensuring more coverage. This helps create unique and interesting game levels while also improving our understanding of the generator's strengths and limitations.\n        △ Less",
        "authors":"['Mahsa Bazzaz', 'Seth Cooper']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"VALUE: Value-Aware Large Language Model for Query Rewriting via Weighted Trie in Sponsored Search",
        "abstract":"In the realm of sponsored search advertising, matching advertisements with the search intent of a user's query is crucial. Query-to-bidwords(i.e. bidding keywords) rewriting is a vital technique that has garnered significant attention. Recently, with the prevalence of LLMs, generative retrieval methods have proven effective in producing high-relevance rewrites. However, we have identified a significant limitation in existing approaches: While fine-tuning LLMs for specific domains enhances semantic relevance, these models have no perception of the intrinsic value of their generated outputs, such as commercial value. Therefore, after SFT, a RLHF phase is often employed to address this issue. Nevertheless, traditional preference alignment methods often face challenges in aligning fine-grained values and are susceptible to overfitting, which diminishes the effectiveness and quality of the generated results. To address these challenges, we propose VALUE(Value-Aware Large language model for qUery rewriting via wEighted trie), the first framework that ensures the generation of high-value and highly relevant bidwords. Our approach utilizes weighted trie, an innovative modification of the traditional trie data structure. By modulating the LLM's output probability distribution with value information from the trie during decoding process, we constrain the generation space and guide the trajectory of text production. Offline experiments demonstrate the effectiveness of our method in semantic matching and preference alignment, showing a remarkable improvement in the value attribute by more than fivefold. Online A\/B tests further revealed that our Revenue Per Mille (RPM) metric increased by 1.64%. VALUE has been deployed on our advertising system since October 2024 and served the Double Eleven promotions, the biggest shopping carnival in China.\n        △ Less",
        "authors":"['Boyang Zuo', 'Xiao Zhang', 'Feng Li', 'Pengjie Wang', 'Jian Xu', 'Bo Zheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Document clustering with evolved multiword search queries",
        "abstract":"Text clustering holds significant value across various domains due to its ability to identify patterns and group related information. Current approaches which rely heavily on a computed similarity measure between documents are often limited in accuracy and interpretability. We present a novel approach to the problem based on a set of evolved search queries. Clusters are formed as the set of documents matched by a single search query in the set of queries. The queries are optimized to maximize the number of documents returned and to minimize the overlap between clusters (documents returned by more than one query). Where queries contain more than one word they are interpreted disjunctively. We have found it useful to assign one word to be the root and constrain the query construction such that the set of documents returned by any additional query words intersect with the set returned by the root word. Not all documents in a collection are returned by any of the search queries in a set, so once the search query evolution is completed a second stage is performed whereby a KNN algorithm is applied to assign all unassigned documents to their nearest cluster. We describe the method and present results using 8 text datasets comparing effectiveness with well-known existing algorithms. We note that as well as achieving the highest accuracy on these datasets the search query format provides the qualitative benefits of being interpretable and modifiable whilst providing a causal explanation of cluster construction.\n        △ Less",
        "authors":"['Laurence Hirsch', 'Robin Hirsch', 'Bayode Ogunleye']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Multi-Task Learning via Generalist Recommender",
        "abstract":"Multi-task learning (MTL) is a common machine learning technique that allows the model to share information across different tasks and improve the accuracy of recommendations for all of them. Many existing MTL implementations suffer from scalability issues as the training and inference performance can degrade with the increasing number of tasks, which can limit production use case scenarios for MTL-based recommender systems. Inspired by the recent advances of large language models, we developed an end-to-end efficient and scalable Generalist Recommender (GRec). GRec takes comprehensive data signals by utilizing NLP heads, parallel Transformers, as well as a wide and deep structure to process multi-modal inputs. These inputs are then combined and fed through a newly proposed task-sentence level routing mechanism to scale the model capabilities on multiple tasks without compromising performance. Offline evaluations and online experiments show that GRec significantly outperforms our previous recommender solutions. GRec has been successfully deployed on one of the largest telecom websites and apps, effectively managing high volumes of online traffic every day.\n        △ Less",
        "authors":"['Luyang Wang', 'Cangcheng Tang', 'Chongyang Zhang', 'Jun Ruan', 'Kai Huang', 'Jason Dai']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On Synthesizing Data for Context Attribution in Question Answering",
        "abstract":"Question Answering (QA) accounts for a significant portion of LLM usage \"in the wild\". However, LLMs sometimes produce false or misleading responses, also known as \"hallucinations\". Therefore, grounding the generated answers in contextually provided information -- i.e., providing evidence for the generated text -- is paramount for LLMs' trustworthiness. Providing this information is the task of context attribution. In this paper, we systematically study LLM-based approaches for this task, namely we investigate (i) zero-shot inference, (ii) LLM ensembling, and (iii) fine-tuning of small LMs on synthetic data generated by larger LLMs. Our key contribution is SynQA: a novel generative strategy for synthesizing context attribution data. Given selected context sentences, an LLM generates QA pairs that are supported by these sentences. This leverages LLMs' natural strengths in text generation while ensuring clear attribution paths in the synthetic training data. We show that the attribution data synthesized via SynQA is highly effective for fine-tuning small LMs for context attribution in different QA tasks and domains. Finally, with a user study, we validate the usefulness of small LMs (fine-tuned on synthetic data from SynQA) in context attribution for QA.\n        △ Less",
        "authors":"['Gorjan Radevski', 'Kiril Gashteovski', 'Shahbaz Syed', 'Christopher Malon', 'Sebastien Nicolas', 'Chia-Chien Hung', 'Timo Sztyler', 'Verena Heußer', 'Wiem Ben Rim', 'Masafumi Enomoto', 'Kunihiro Takeoka', 'Masafumi Oyamada', 'Goran Glavaš', 'Carolin Lawrence']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Systematic Survey on Federated Sequential Recommendation",
        "abstract":"Sequential recommendation is an advanced recommendation technique that utilizes the sequence of user behaviors to generate personalized suggestions by modeling the temporal dependencies and patterns in user preferences. However, it requires a server to centrally collect users' data, which poses a threat to the data privacy of different users. In recent years, federated learning has emerged as a distributed architecture that allows participants to train a global model while keeping their private data locally. This survey pioneers Federated Sequential Recommendation (FedSR), where each user joins as a participant in federated training to achieve a recommendation service that balances data privacy and model performance. We begin with an introduction to the background and unique challenges of FedSR. Then, we review existing solutions from two levels, each of which includes two specific techniques. Additionally, we discuss the critical challenges and future research directions in FedSR.\n        △ Less",
        "authors":"['Yichen Li', 'Qiyu Qin', 'Gaoyang Zhu', 'Wenchao Xu', 'Haozhao Wang', 'Yuhua Li', 'Rui Zhang', 'Ruixuan Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Gaussian Mixture Flow Matching Models",
        "abstract":"Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE\/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\\times$256.\n        △ Less",
        "authors":"['Hansheng Chen', 'Kai Zhang', 'Hao Tan', 'Zexiang Xu', 'Fujun Luan', 'Leonidas Guibas', 'Gordon Wetzstein', 'Sai Bi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dimension-Free Convergence of Diffusion Models for Approximate Gaussian Mixtures",
        "abstract":"Diffusion models are distinguished by their exceptional generative performance, particularly in producing high-quality samples through iterative denoising. While current theory suggests that the number of denoising steps required for accurate sample generation should scale linearly with data dimension, this does not reflect the practical efficiency of widely used algorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper investigates the effectiveness of diffusion models in sampling from complex high-dimensional distributions that can be well-approximated by Gaussian Mixture Models (GMMs). For these distributions, our main result shows that DDPM takes at most $\\widetilde{O}(1\/\\varepsilon)$ iterations to attain an $\\varepsilon$-accurate distribution in total variation (TV) distance, independent of both the ambient dimension $d$ and the number of components $K$, up to logarithmic factors. Furthermore, this result remains robust to score estimation errors. These findings highlight the remarkable effectiveness of diffusion models in high-dimensional settings given the universal approximation capability of GMMs, and provide theoretical insights into their practical success.\n        △ Less",
        "authors":"['Gen Li', 'Changxiao Cai', 'Yuting Wei']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dion: A Communication-Efficient Optimizer for Large Models",
        "abstract":"Training large AI models efficiently requires distributing computation across multiple accelerators, but this often incurs significant communication overhead -- especially during gradient synchronization. We introduce Dion, a communication-efficient optimizer that retains the synchronous semantics of standard distributed training (e.g., DDP, FSDP) while substantially reducing I\/O costs. Unlike conventional optimizers that synchronize full gradient matrices, Dion leverages orthonormalized updates with device-local momentum buffers, eliminating the need for full gradient exchange. It further supports an efficient sharding strategy that avoids reconstructing large matrices during training.\n        △ Less",
        "authors":"['Kwangjun Ahn', 'Byron Xu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using Physiological Measures, Gaze, and Facial Expressions to Model Human Trust in a Robot Partner",
        "abstract":"With robots becoming increasingly prevalent in various domains, it has become crucial to equip them with tools to achieve greater fluency in interactions with humans. One of the promising areas for further exploration lies in human trust. A real-time, objective model of human trust could be used to maximize productivity, preserve safety, and mitigate failure. In this work, we attempt to use physiological measures, gaze, and facial expressions to model human trust in a robot partner. We are the first to design an in-person, human-robot supervisory interaction study to create a dedicated trust dataset. Using this dataset, we train machine learning algorithms to identify the objective measures that are most indicative of trust in a robot partner, advancing trust prediction in human-robot interactions. Our findings indicate that a combination of sensor modalities (blood volume pulse, electrodermal activity, skin temperature, and gaze) can enhance the accuracy of detecting human trust in a robot partner. Furthermore, the Extra Trees, Random Forest, and Decision Trees classifiers exhibit consistently better performance in measuring the person's trust in the robot partner. These results lay the groundwork for constructing a real-time trust model for human-robot interaction, which could foster more efficient interactions between humans and robots.\n        △ Less",
        "authors":"['Haley N. Green', 'Tariq Iqbal']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Covariant Gradient Descent",
        "abstract":"We present a manifestly covariant formulation of the gradient descent method, ensuring consistency across arbitrary coordinate systems and general curved trainable spaces. The optimization dynamics is defined using a covariant force vector and a covariant metric tensor, both computed from the first and second statistical moments of the gradients. These moments are estimated through time-averaging with an exponential weight function, which preserves linear computational complexity. We show that commonly used optimization methods such as RMSProp, Adam and AdaBelief correspond to special limits of the covariant gradient descent (CGD) and demonstrate how these methods can be further generalized and improved.\n        △ Less",
        "authors":"['Dmitry Guskov', 'Vitaly Vanchurin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Aggregating time-series and image data: functors and double functors",
        "abstract":"Aggregation of time-series or image data over subsets of the domain is a fundamental task in data science. We show that many known aggregation operations can be interpreted as (double) functors on appropriate (double) categories. Such functorial aggregations are amenable to parallel implementation via straightforward extensions of Blelloch's parallel scan algorithm. In addition to providing a unified viewpoint on existing operations, it allows us to propose new aggregation operations for time-series and image data.\n        △ Less",
        "authors":"['Joscha Diehl']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AnomalousNet: A Hybrid Approach with Attention U-Nets and Change Point Detection for Accurate Characterization of Anomalous Diffusion in Video Data",
        "abstract":"Anomalous diffusion occurs in a wide range of systems, including protein transport within cells, animal movement in complex habitats, pollutant dispersion in groundwater, and nanoparticle motion in synthetic materials. Accurately estimating the anomalous diffusion exponent and the diffusion coefficient from the particle trajectories is essential to distinguish between sub-diffusive, super-diffusive, or normal diffusion regimes. These estimates provide a deeper insight into the underlying dynamics of the system, facilitating the identification of particle behaviors and the detection of changes in diffusion states. However, analyzing short and noisy video data, which often yield incomplete and heterogeneous trajectories, poses a significant challenge for traditional statistical approaches. We introduce a data-driven method that integrates particle tracking, an attention\n  U-Net architecture, and a change-point detection algorithm to address these issues. This approach not only infers the anomalous diffusion parameters with high accuracy but also identifies temporal transitions between different states, even in the presence of noise and limited temporal resolution. Our methodology demonstrated strong performance in the 2nd Anomalous Diffusion (AnDi) Challenge benchmark within the top submissions for video tasks.\n        △ Less",
        "authors":"['Yusef Ahsini', 'Marc Escoto', 'J. Alberto Conejero']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models",
        "abstract":"Large Language Models (LLMs) have emerged as powerful tools for generating coherent text, understanding context, and performing reasoning tasks. However, they struggle with temporal reasoning, which requires processing time-related information such as event sequencing, durations, and inter-temporal relationships. These capabilities are critical for applications including question answering, scheduling, and historical analysis. In this paper, we introduce TISER, a novel framework that enhances the temporal reasoning abilities of LLMs through a multi-stage process that combines timeline construction with iterative self-reflection. Our approach leverages test-time scaling to extend the length of reasoning traces, enabling models to capture complex temporal dependencies more effectively. This strategy not only boosts reasoning accuracy but also improves the traceability of the inference process. Experimental results demonstrate state-of-the-art performance across multiple benchmarks, including out-of-distribution test sets, and reveal that TISER enables smaller open-source models to surpass larger closed-weight models on challenging temporal reasoning tasks.\n        △ Less",
        "authors":"['Adrián Bazaga', 'Rexhina Blloshmi', 'Bill Byrne', 'Adrià de Gispert']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adversarial KA",
        "abstract":"Regarding the representation theorem of Kolmogorov and Arnold (KA) as an algorithm for representing or «expressing» functions, we test its robustness by analyzing its ability to withstand adversarial attacks. We find KA to be robust to countable collections of continuous adversaries, but unearth a question about the equi-continuity of the outer functions that, so far, obstructs taking limits and defeating continuous groups of adversaries. This question on the regularity of the outer functions is relevant to the debate over the applicability of KA to the general theory of NNs.\n        △ Less",
        "authors":"['Sviatoslav Dzhenzher', 'Michael H. Freedman']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Explaining Low Perception Model Competency with High-Competency Counterfactuals",
        "abstract":"There exist many methods to explain how an image classification model generates its decision, but very little work has explored methods to explain why a classifier might lack confidence in its prediction. As there are various reasons the classifier might lose confidence, it would be valuable for this model to not only indicate its level of uncertainty but also explain why it is uncertain. Counterfactual images have been used to visualize changes that could be made to an image to generate a different classification decision. In this work, we explore the use of counterfactuals to offer an explanation for low model competency--a generalized form of predictive uncertainty that measures confidence. Toward this end, we develop five novel methods to generate high-competency counterfactual images, namely Image Gradient Descent (IGD), Feature Gradient Descent (FGD), Autoencoder Reconstruction (Reco), Latent Gradient Descent (LGD), and Latent Nearest Neighbors (LNN). We evaluate these methods across two unique datasets containing images with six known causes for low model competency and find Reco, LGD, and LNN to be the most promising methods for counterfactual generation. We further evaluate how these three methods can be utilized by pre-trained Multimodal Large Language Models (MLLMs) to generate language explanations for low model competency. We find that the inclusion of a counterfactual image in the language model query greatly increases the ability of the model to generate an accurate explanation for the cause of low model competency, thus demonstrating the utility of counterfactual images in explaining low perception model competency.\n        △ Less",
        "authors":"['Sara Pohland', 'Claire Tomlin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PEAKS: Selecting Key Training Examples Incrementally via Prediction Error Anchored by Kernel Similarity",
        "abstract":"As deep learning continues to be driven by ever-larger datasets, understanding which examples are most important for generalization has become a critical question. While progress in data selection continues, emerging applications require studying this problem in dynamic contexts. To bridge this gap, we pose the Incremental Data Selection (IDS) problem, where examples arrive as a continuous stream, and need to be selected without access to the full data source. In this setting, the learner must incrementally build a training dataset of predefined size while simultaneously learning the underlying task. We find that in IDS, the impact of a new sample on the model state depends fundamentally on both its geometric relationship in the feature space and its prediction error. Leveraging this insight, we propose PEAKS (Prediction Error Anchored by Kernel Similarity), an efficient data selection method tailored for IDS. Our comprehensive evaluations demonstrate that PEAKS consistently outperforms existing selection strategies. Furthermore, PEAKS yields increasingly better performance returns than random selection as training data size grows on real-world datasets.\n        △ Less",
        "authors":"['Mustafa Burak Gurbuz', 'Xingyu Zheng', 'Constantine Dovrolis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Texture2LoD3: Enabling LoD3 Building Reconstruction With Panoramic Images",
        "abstract":"Despite recent advancements in surface reconstruction, Level of Detail (LoD) 3 building reconstruction remains an unresolved challenge. The main issue pertains to the object-oriented modelling paradigm, which requires georeferencing, watertight geometry, facade semantics, and low-poly representation -- Contrasting unstructured mesh-oriented models. In Texture2LoD3, we introduce a novel method leveraging the ubiquity of 3D building model priors and panoramic street-level images, enabling the reconstruction of LoD3 building models. We observe that prior low-detail building models can serve as valid planar targets for ortho-rectifying street-level panoramic images. Moreover, deploying segmentation on accurately textured low-level building surfaces supports maintaining essential georeferencing, watertight geometry, and low-poly representation for LoD3 reconstruction. In the absence of LoD3 validation data, we additionally introduce the ReLoD3 dataset, on which we experimentally demonstrate that our method leads to improved facade segmentation accuracy by 11% and can replace costly manual projections. We believe that Texture2LoD3 can scale the adoption of LoD3 models, opening applications in estimating building solar potential or enhancing autonomous driving simulations. The project website, code, and data are available here: https:\/\/wenzhaotang.github.io\/Texture2LoD3\/.\n        △ Less",
        "authors":"['Wenzhao Tang', 'Weihang Li', 'Xiucheng Liang', 'Olaf Wysocki', 'Filip Biljecki', 'Christoph Holst', 'Boris Jutzi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PINNverse: Accurate parameter estimation in differential equations from noisy data with constrained physics-informed neural networks",
        "abstract":"Parameter estimation for differential equations from measured data is an inverse problem prevalent across quantitative sciences. Physics-Informed Neural Networks (PINNs) have emerged as effective tools for solving such problems, especially with sparse measurements and incomplete system information. However, PINNs face convergence issues, stability problems, overfitting, and complex loss function design. Here we introduce PINNverse, a training paradigm that addresses these limitations by reformulating the learning process as a constrained differential optimization problem. This approach achieves a dynamic balance between data loss and differential equation residual loss during training while preventing overfitting. PINNverse combines the advantages of PINNs with the Modified Differential Method of Multipliers to enable convergence on any point on the Pareto front. We demonstrate robust and accurate parameter estimation from noisy data in four classical ODE and PDE models from physics and biology. Our method enables accurate parameter inference also when the forward problem is expensive to solve.\n        △ Less",
        "authors":"['Marius Almanstötter', 'Roman Vetter', 'Dagmar Iber']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Embedded Federated Feature Selection with Dynamic Sparse Training: Balancing Accuracy-Cost Tradeoffs",
        "abstract":"Federated Learning (FL) enables multiple resource-constrained edge devices with varying levels of heterogeneity to collaboratively train a global model. However, devices with limited capacity can create bottlenecks and slow down model convergence. One effective approach to addressing this issue is to use an efficient feature selection method, which reduces overall resource demands by minimizing communication and computation costs, thereby mitigating the impact of struggling nodes. Existing federated feature selection (FFS) methods are either considered as a separate step from FL or rely on a third party. These approaches increase computation and communication overhead, making them impractical for real-world high-dimensional datasets. To address this, we present \\textit{Dynamic Sparse Federated Feature Selection} (DSFFS), the first innovative embedded FFS that is efficient in both communication and computation. In the proposed method, feature selection occurs simultaneously with model training. During training, input-layer neurons, their connections, and hidden-layer connections are dynamically pruned and regrown, eliminating uninformative features. This process enhances computational efficiency on devices, improves network communication efficiency, and boosts global model performance. Several experiments are conducted on nine real-world datasets of varying dimensionality from diverse domains, including biology, image, speech, and text. The results under a realistic non-iid data distribution setting show that our approach achieves a better trade-off between accuracy, computation, and communication costs by selecting more informative features compared to other state-of-the-art FFS methods.\n        △ Less",
        "authors":"['Afsaneh Mahanipour', 'Hana Khamfroush']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Learning for Medical Image Classification: A Comprehensive Benchmark",
        "abstract":"The federated learning paradigm is wellsuited for the field of medical image analysis, as it can effectively cope with machine learning on isolated multicenter data while protecting the privacy of participating parties. However, current research on optimization algorithms in federated learning often focuses on limited datasets and scenarios, primarily centered around natural images, with insufficient comparative experiments in medical contexts. In this work, we conduct a comprehensive evaluation of several state-of-the-art federated learning algorithms in the context of medical imaging. We conduct a fair comparison of classification models trained using various federated learning algorithms across multiple medical imaging datasets. Additionally, we evaluate system performance metrics, such as communication cost and computational efficiency, while considering different federated learning architectures. Our findings show that medical imaging datasets pose substantial challenges for current federated learning optimization algorithms. No single algorithm consistently delivers optimal performance across all medical federated learning scenarios, and many optimization algorithms may underperform when applied to these datasets. Our experiments provide a benchmark and guidance for future research and application of federated learning in medical imaging contexts. Furthermore, we propose an efficient and robust method that combines generative techniques using denoising diffusion probabilistic models with label smoothing to augment datasets, widely enhancing the performance of federated learning on classification tasks across various medical imaging datasets. Our code will be released on GitHub, offering a reliable and comprehensive benchmark for future federated learning studies in medical imaging.\n        △ Less",
        "authors":"['Zhekai Zhou', 'Guibo Luo', 'Mingzhi Chen', 'Zhenyu Weng', 'Yuesheng Zhu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"IAEmu: Learning Galaxy Intrinsic Alignment Correlations",
        "abstract":"The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing analyses, arise from correlations in galaxy shapes driven by tidal interactions and galaxy formation processes. Accurate IA modeling is essential for robust cosmological inference, but current approaches rely on perturbative methods that break down on nonlinear scales or on expensive simulations. We introduce IAEmu, a neural network-based emulator that predicts the galaxy position-position ($ξ$), position-orientation ($ω$), and orientation-orientation ($η$) correlation functions and their uncertainties using mock catalogs based on the halo occupation distribution (HOD) framework. Compared to simulations, IAEmu achieves ~3% average error for $ξ$ and ~5% for $ω$, while capturing the stochasticity of $η$ without overfitting. The emulator provides both aleatoric and epistemic uncertainties, helping identify regions where predictions may be less reliable. We also demonstrate generalization to non-HOD alignment signals by fitting to IllustrisTNG hydrodynamical simulation data. As a fully differentiable neural network, IAEmu enables $\\sim$10,000$\\times$ speed-ups in mapping HOD parameters to correlation functions on GPUs, compared to CPU-based simulations. This acceleration facilitates inverse modeling via gradient-based sampling, making IAEmu a powerful surrogate model for galaxy bias and IA studies with direct applications to Stage IV weak lensing surveys.\n        △ Less",
        "authors":"['Sneh Pandya', 'Yuanyuan Yang', 'Nicholas Van Alfen', 'Jonathan Blazek', 'Robin Walters']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mapping biodiversity at very-high resolution in Europe",
        "abstract":"This paper describes a cascading multimodal pipeline for high-resolution biodiversity mapping across Europe, integrating species distribution modeling, biodiversity indicators, and habitat classification. The proposed pipeline first predicts species compositions using a deep-SDM, a multimodal model trained on remote sensing, climate time series, and species occurrence data at 50x50m resolution. These predictions are then used to generate biodiversity indicator maps and classify habitats with Pl@ntBERT, a transformer-based LLM designed for species-to-habitat mapping. With this approach, continental-scale species distribution maps, biodiversity indicator maps, and habitat maps are produced, providing fine-grained ecological insights. Unlike traditional methods, this framework enables joint modeling of interspecies dependencies, bias-aware training with heterogeneous presence-absence data, and large-scale inference from multi-source remote sensing inputs.\n        △ Less",
        "authors":"['César Leblanc', 'Lukas Picek', 'Benjamin Deneu', 'Pierre Bonnet', 'Maximilien Servajean', 'Rémi Palard', 'Alexis Joly']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hybrid machine learning data assimilation for marine biogeochemistry",
        "abstract":"Marine biogeochemistry models are critical for forecasting, as well as estimating ecosystem responses to climate change and human activities. Data assimilation (DA) improves these models by aligning them with real-world observations, but marine biogeochemistry DA faces challenges due to model complexity, strong nonlinearity, and sparse, uncertain observations. Existing DA methods applied to marine biogeochemistry struggle to update unobserved variables effectively, while ensemble-based methods are computationally too expensive for high-complexity marine biogeochemistry models. This study demonstrates how machine learning (ML) can improve marine biogeochemistry DA by learning statistical relationships between observed and unobserved variables. We integrate ML-driven balancing schemes into a 1D prototype of a system used to forecast marine biogeochemistry in the North-West European Shelf seas. ML is applied to predict (i) state-dependent correlations from free-run ensembles and (ii), in an ``end-to-end'' fashion, analysis increments from an Ensemble Kalman Filter. Our results show that ML significantly enhances updates for previously not-updated variables when compared to univariate schemes akin to those used operationally. Furthermore, ML models exhibit moderate transferability to new locations, a crucial step toward scaling these methods to 3D operational systems. We conclude that ML offers a clear pathway to overcome current computational bottlenecks in marine biogeochemistry DA and that refining transferability, optimizing training data sampling, and evaluating scalability for large-scale marine forecasting, should be future research priorities.\n        △ Less",
        "authors":"['Ieuan Higgs', 'Ross Bannister', 'Jozef Skákala', 'Alberto Carrassi', 'Stefano Ciavatta']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A moving target in AI-assisted decision-making: Dataset shift, model updating, and the problem of update opacity",
        "abstract":"Machine learning (ML) systems are vulnerable to performance decline over time due to dataset shift. To address this problem, experts often suggest that ML systems should be regularly updated to ensure ongoing performance stability. Some scholarly literature has begun to address the epistemic and ethical challenges associated with different updating methodologies. Thus far, however, little attention has been paid to the impact of model updating on the ML-assisted decision-making process itself, particularly in the AI ethics and AI epistemology literatures. This article aims to address this gap in the literature. It argues that model updating introduces a new sub-type of opacity into ML-assisted decision-making -- update opacity -- that occurs when users cannot understand how or why an update has changed the reasoning or behaviour of an ML system. This type of opacity presents a variety of distinctive epistemic and safety concerns that available solutions to the black box problem in ML are largely ill-equipped to address. A variety of alternative strategies may be developed or pursued to address the problem of update opacity more directly, including bi-factual explanations, dynamic model reporting, and update compatibility. However, each of these strategies presents its own risks or carries significant limitations. Further research will be needed to address the epistemic and safety concerns associated with model updating and update opacity going forward.\n        △ Less",
        "authors":"['Joshua Hatherley']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Resource-Efficient Beam Prediction in mmWave Communications with Multimodal Realistic Simulation Framework",
        "abstract":"Beamforming is a key technology in millimeter-wave (mmWave) communications that improves signal transmission by optimizing directionality and intensity. However, conventional channel estimation methods, such as pilot signals or beam sweeping, often fail to adapt to rapidly changing communication environments. To address this limitation, multimodal sensing-aided beam prediction has gained significant attention, using various sensing data from devices such as LiDAR, radar, GPS, and RGB images to predict user locations or network conditions. Despite its promising potential, the adoption of multimodal sensing-aided beam prediction is hindered by high computational complexity, high costs, and limited datasets. Thus, in this paper, a resource-efficient learning approach is proposed to transfer knowledge from a multimodal network to a monomodal (radar-only) network based on cross-modal relational knowledge distillation (CRKD), while reducing computational overhead and preserving predictive accuracy. To enable multimodal learning with realistic data, a novel multimodal simulation framework is developed while integrating sensor data generated from the autonomous driving simulator CARLA with MATLAB-based mmWave channel modeling, and reflecting real-world conditions. The proposed CRKD achieves its objective by distilling relational information across different feature spaces, which enhances beam prediction performance without relying on expensive sensor data. Simulation results demonstrate that CRKD efficiently distills multimodal knowledge, allowing a radar-only model to achieve $94.62\\%$ of the teacher performance. In particular, this is achieved with just $10\\%$ of the teacher network's parameters, thereby significantly reducing computational complexity and dependence on multimodal sensor data.\n        △ Less",
        "authors":"['Yu Min Park', 'Yan Kyaw Tun', 'Walid Saad', 'Choong Seon Hong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Training state-of-the-art pathology foundation models with orders of magnitude less data",
        "abstract":"The field of computational pathology has recently seen rapid advances driven by the development of modern vision foundation models (FMs), typically trained on vast collections of pathology images. Recent studies demonstrate that increasing the training data set and model size and integrating domain-specific image processing techniques can significantly enhance the model's performance on downstream tasks. Building on these insights, our work incorporates several recent modifications to the standard DINOv2 framework from the literature to optimize the training of pathology FMs. We also apply a post-training procedure for fine-tuning models on higher-resolution images to further enrich the information encoded in the embeddings. We present three novel pathology FMs trained on up to two orders of magnitude fewer WSIs than those used to train other state-of-the-art FMs while demonstrating a comparable or superior performance on downstream tasks. Even the model trained on TCGA alone (12k WSIs) outperforms most existing FMs and, on average, matches Virchow2, the second-best FM published to date. This suggests that there still remains a significant potential for further improving the models and algorithms used to train pathology FMs to take full advantage of the vast data collections.\n        △ Less",
        "authors":"['Mikhail Karasikov', 'Joost van Doorn', 'Nicolas Känzig', 'Melis Erdal Cesur', 'Hugo Mark Horlings', 'Robert Berke', 'Fei Tang', 'Sebastian Otálora']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval",
        "abstract":"Generative information retrieval (GenIR) is a promising neural retrieval paradigm that formulates document retrieval as a document identifier (docid) generation task, allowing for end-to-end optimization toward a unified global retrieval objective. However, existing GenIR models suffer from token-level misalignment, where models trained to predict the next token often fail to capture document-level relevance effectively. While reinforcement learning-based methods, such as reinforcement learning from relevance feedback (RLRF), aim to address this misalignment through reward modeling, they introduce significant complexity, requiring the optimization of an auxiliary reward function followed by reinforcement fine-tuning, which is computationally expensive and often unstable. To address these challenges, we propose direct document relevance optimization (DDRO), which aligns token-level docid generation with document-level relevance estimation through direct optimization via pairwise ranking, eliminating the need for explicit reward modeling and reinforcement learning. Experimental results on benchmark datasets, including MS MARCO document and Natural Questions, show that DDRO outperforms reinforcement learning-based methods, achieving a 7.4% improvement in MRR@10 for MS MARCO and a 19.9% improvement for Natural Questions. These findings highlight DDRO's potential to enhance retrieval effectiveness with a simplified optimization approach. By framing alignment as a direct optimization problem, DDRO simplifies the ranking optimization pipeline of GenIR models while offering a viable alternative to reinforcement learning-based methods.\n        △ Less",
        "authors":"['Kidist Amde Mekonnen', 'Yubao Tang', 'Maarten de Rijke']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BRIDGES: Bridging Graph Modality and Large Language Models within EDA Tasks",
        "abstract":"While many EDA tasks already involve graph-based data, existing LLMs in EDA primarily either represent graphs as sequential text, or simply ignore graph-structured data that might be beneficial like dataflow graphs of RTL code. Recent studies have found that LLM performance suffers when graphs are represented as sequential text, and using additional graph information significantly boosts performance. To address these challenges, we introduce BRIDGES, a framework designed to incorporate graph modality into LLMs for EDA tasks. BRIDGES integrates an automated data generation workflow, a solution that combines graph modality with LLM, and a comprehensive evaluation suite. First, we establish an LLM-driven workflow to generate RTL and netlist-level data, converting them into dataflow and netlist graphs with function descriptions. This workflow yields a large-scale dataset comprising over 500,000 graph instances and more than 1.5 billion tokens. Second, we propose a lightweight cross-modal projector that encodes graph representations into text-compatible prompts, enabling LLMs to effectively utilize graph data without architectural modifications. Experimental results demonstrate 2x to 10x improvements across multiple tasks compared to text-only baselines, including accuracy in design retrieval, type prediction and perplexity in function description, with negligible computational overhead (<1% model weights increase and <30% additional runtime overhead). Even without additional LLM finetuning, our results outperform text-only by a large margin. We plan to release BRIDGES, including the dataset, models, and training flow.\n        △ Less",
        "authors":"['Wei Li', 'Yang Zou', 'Christopher Ellis', 'Ruben Purdy', 'Shawn Blanton', 'José M. F. Moura']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning symmetries in datasets",
        "abstract":"We investigate how symmetries present in datasets affect the structure of the latent space learned by Variational Autoencoders (VAEs). By training VAEs on data originating from simple mechanical systems and particle collisions, we analyze the organization of the latent space through a relevance measure that identifies the most meaningful latent directions. We show that when symmetries or approximate symmetries are present, the VAE self-organizes its latent space, effectively compressing the data along a reduced number of latent variables. This behavior captures the intrinsic dimensionality determined by the symmetry constraints and reveals hidden relations among the features. Furthermore, we provide a theoretical analysis of a simple toy model, demonstrating how, under idealized conditions, the latent space aligns with the symmetry directions of the data manifold. We illustrate these findings with examples ranging from two-dimensional datasets with $O(2)$ symmetry to realistic datasets from electron-positron and proton-proton collisions. Our results highlight the potential of unsupervised generative models to expose underlying structures in data and offer a novel approach to symmetry discovery without explicit supervision.\n        △ Less",
        "authors":"['Veronica Sanz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Attention-Based Multiscale Temporal Fusion Network for Uncertain-Mode Fault Diagnosis in Multimode Processes",
        "abstract":"Fault diagnosis in multimode processes plays a critical role in ensuring the safe operation of industrial systems across multiple modes. It faces a great challenge yet to be addressed - that is, the significant distributional differences among monitoring data from multiple modes make it difficult for the models to extract shared feature representations related to system health conditions. In response to this problem, this paper introduces a novel method called attention-based multiscale temporal fusion network. The multiscale depthwise convolution and gated recurrent unit are employed to extract multiscale contextual local features and long-short-term features. Instance normalization is applied to suppress mode-specific information. Furthermore, a temporal attention mechanism is designed to focus on critical time points with higher cross-mode shared information, thereby enhancing the accuracy of fault diagnosis. The proposed model is applied to Tennessee Eastman process dataset and three-phase flow facility dataset. The experiments demonstrate that the proposed model achieves superior diagnostic performance and maintains a small model size. The source code will be available on GitHub at https:\/\/github.com\/GuangqiangLi\/AMTFNet.\n        △ Less",
        "authors":"['Guangqiang Li', 'M. Amine Atoui', 'Xiangshun Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Machine learning interatomic potential can infer electrical response",
        "abstract":"Modeling the response of material and chemical systems to electric fields remains a longstanding challenge. Machine learning interatomic potentials (MLIPs) offer an efficient and scalable alternative to quantum mechanical methods but do not by themselves incorporate electrical response. Here, we show that polarization and Born effective charge (BEC) tensors can be directly extracted from long-range MLIPs within the Latent Ewald Summation (LES) framework, solely by learning from energy and force data. Using this approach, we predict the infrared spectra of bulk water under zero or finite external electric fields, ionic conductivities of high-pressure superionic ice, and the phase transition and hysteresis in ferroelectric PbTiO$_3$ perovskite. This work thus extends the capability of MLIPs to predict electrical response--without training on charges or polarization or BECs--and enables accurate modeling of electric-field-driven processes in diverse systems at scale.\n        △ Less",
        "authors":"['Peichen Zhong', 'Dongjin Kim', 'Daniel S. King', 'Bingqing Cheng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RLBayes: a Bayesian Network Structure Learning Algorithm via Reinforcement Learning-Based Search Strategy",
        "abstract":"The score-based structure learning of Bayesian network (BN) is an effective way to learn BN models, which are regarded as some of the most compelling probabilistic graphical models in the field of representation and reasoning under uncertainty. However, the search space of structure learning grows super-exponentially as the number of variables increases, which makes BN structure learning an NP-hard problem, as well as a combination optimization problem (COP). Despite the successes of many heuristic methods on it, the results of the structure learning of BN are usually unsatisfactory. Inspired by Q-learning, in this paper, a Bayesian network structure learning algorithm via reinforcement learning-based (RL-based) search strategy is proposed, namely RLBayes. The method borrows the idea of RL and tends to record and guide the learning process by a dynamically maintained Q-table. By creating and maintaining the dynamic Q-table, RLBayes achieve storing the unlimited search space within limited space, thereby achieving the structure learning of BN via Q-learning. Not only is it theoretically proved that RLBayes can converge to the global optimal BN structure, but also it is experimentally proved that RLBayes has a better effect than almost all other heuristic search algorithms.\n        △ Less",
        "authors":"['Mingcan Wang', 'Junchang Xin', 'Luxuan Qu', 'Qi Chen', 'Zhiqiong Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DDPM Score Matching and Distribution Learning",
        "abstract":"Score estimation is the backbone of score-based generative models (SGMs), especially denoising diffusion probabilistic models (DDPMs). A key result in this area shows that with accurate score estimates, SGMs can efficiently generate samples from any realistic data distribution (Chen et al., ICLR'23; Lee et al., ALT'23). This distribution learning result, where the learned distribution is implicitly that of the sampler's output, does not explain how score estimation relates to classical tasks of parameter and density estimation.\n  This paper introduces a framework that reduces score estimation to these two tasks, with various implications for statistical and computational learning theory:\n  Parameter Estimation: Koehler et al. (ICLR'23) demonstrate that a score-matching variant is statistically inefficient for the parametric estimation of multimodal densities common in practice. In contrast, we show that under mild conditions, denoising score-matching in DDPMs is asymptotically efficient.\n  Density Estimation: By linking generation to score estimation, we lift existing score estimation guarantees to $(ε,δ)$-PAC density estimation, i.e., a function approximating the target log-density within $ε$ on all but a $δ$-fraction of the space. We provide (i) minimax rates for density estimation over Hölder classes and (ii) a quasi-polynomial PAC density estimation algorithm for the classical Gaussian location mixture model, building on and addressing an open problem from Gatmiry et al. (arXiv'24).\n  Lower Bounds for Score Estimation: Our framework offers the first principled method to prove computational lower bounds for score estimation across general distributions. As an application, we establish cryptographic lower bounds for score estimation in general Gaussian mixture models, conceptually recovering Song's (NeurIPS'24) result and advancing his key open problem.\n        △ Less",
        "authors":"['Sinho Chewi', 'Alkis Kalavasis', 'Anay Mehrotra', 'Omar Montasser']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SparsyFed: Sparse Adaptive Federated Training",
        "abstract":"Sparse training is often adopted in cross-device federated learning (FL) environments where constrained devices collaboratively train a machine learning model on private data by exchanging pseudo-gradients across heterogeneous networks. Although sparse training methods can reduce communication overhead and computational burden in FL, they are often not used in practice for the following key reasons: (1) data heterogeneity makes it harder for clients to reach consensus on sparse models compared to dense ones, requiring longer training; (2) methods for obtaining sparse masks lack adaptivity to accommodate very heterogeneous data distributions, crucial in cross-device FL; and (3) additional hyperparameters are required, which are notably challenging to tune in FL. This paper presents SparsyFed, a practical federated sparse training method that critically addresses the problems above. Previous works have only solved one or two of these challenges at the expense of introducing new trade-offs, such as clients' consensus on masks versus sparsity pattern adaptivity. We show that SparsyFed simultaneously (1) can produce 95% sparse models, with negligible degradation in accuracy, while only needing a single hyperparameter, (2) achieves a per-round weight regrowth 200 times smaller than previous methods, and (3) allows the sparse masks to adapt to highly heterogeneous data distributions and outperform all baselines under such conditions.\n        △ Less",
        "authors":"['Adriano Guastella', 'Lorenzo Sani', 'Alex Iacob', 'Alessio Mora', 'Paolo Bellavista', 'Nicholas D. Lane']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Reinforcement Learning Method for Environments with Stochastic Variables: Post-Decision Proximal Policy Optimization with Dual Critic Networks",
        "abstract":"This paper presents Post-Decision Proximal Policy Optimization (PDPPO), a novel variation of the leading deep reinforcement learning method, Proximal Policy Optimization (PPO). The PDPPO state transition process is divided into two steps: a deterministic step resulting in the post-decision state and a stochastic step leading to the next state. Our approach incorporates post-decision states and dual critics to reduce the problem's dimensionality and enhance the accuracy of value function estimation. Lot-sizing is a mixed integer programming problem for which we exemplify such dynamics. The objective of lot-sizing is to optimize production, delivery fulfillment, and inventory levels in uncertain demand and cost parameters. This paper evaluates the performance of PDPPO across various environments and configurations. Notably, PDPPO with a dual critic architecture achieves nearly double the maximum reward of vanilla PPO in specific scenarios, requiring fewer episode iterations and demonstrating faster and more consistent learning across different initializations. On average, PDPPO outperforms PPO in environments with a stochastic component in the state transition. These results support the benefits of using a post-decision state. Integrating this post-decision state in the value function approximation leads to more informed and efficient learning in high-dimensional and stochastic environments.\n        △ Less",
        "authors":"['Leonardo Kanashiro Felizardo', 'Edoardo Fadda', 'Paolo Brandimarte', 'Emilio Del-Moral-Hernandez', 'Mariá Cristina Vasconcelos Nascimento']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Pr$εε$mpt: Sanitizing Sensitive Prompts for LLMs",
        "abstract":"The rise of large language models (LLMs) has introduced new privacy challenges, particularly during inference where sensitive information in prompts may be exposed to proprietary LLM APIs. In this paper, we address the problem of formally protecting the sensitive information contained in a prompt while maintaining response quality. To this end, first, we introduce a cryptographically inspired notion of a prompt sanitizer which transforms an input prompt to protect its sensitive tokens. Second, we propose Pr$εε$mpt, a novel system that implements a prompt sanitizer. Pr$εε$mpt categorizes sensitive tokens into two types: (1) those where the LLM's response depends solely on the format (such as SSNs, credit card numbers), for which we use format-preserving encryption (FPE); and (2) those where the response depends on specific values, (such as age, salary) for which we apply metric differential privacy (mDP). Our evaluation demonstrates that Pr$εε$mpt is a practical method to achieve meaningful privacy guarantees, while maintaining high utility compared to unsanitized prompts, and outperforming prior methods\n        △ Less",
        "authors":"['Amrita Roy Chowdhury', 'David Glukhov', 'Divyam Anshumaan', 'Prasad Chalasani', 'Nicolas Papernot', 'Somesh Jha', 'Mihir Bellare']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Online Cluster-Based Parameter Control for Metaheuristic",
        "abstract":"The concept of parameter setting is a crucial and significant process in metaheuristics since it can majorly impact their performance. It is a highly complex and challenging procedure since it requires a deep understanding of the optimization algorithm and the optimization problem at hand. In recent years, the upcoming rise of autonomous decision systems has attracted ongoing scientific interest in this direction, utilizing a considerable number of parameter-tuning methods. There are two types of methods: offline and online. Online methods usually excel in complex real-world problems, as they can offer dynamic parameter control throughout the execution of the algorithm. The present work proposes a general-purpose online parameter-tuning method called Cluster-Based Parameter Adaptation (CPA) for population-based metaheuristics. The main idea lies in the identification of promising areas within the parameter search space and in the generation of new parameters around these areas. The method's validity has been demonstrated using the differential evolution algorithm and verified in established test suites of low- and high-dimensional problems. The obtained results are statistically analyzed and compared with state-of-the-art algorithms, including advanced auto-tuning approaches. The analysis reveals the promising solid CPA's performance as well as its robustness under a variety of benchmark problems and dimensions.\n        △ Less",
        "authors":"['Vasileios A. Tatsis', 'Dimos Ioannidis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Unifying Physics- and Data-Driven Modeling via Novel Causal Spatiotemporal Graph Neural Network for Interpretable Epidemic Forecasting",
        "abstract":"Accurate epidemic forecasting is crucial for effective disease control and prevention. Traditional compartmental models often struggle to estimate temporally and spatially varying epidemiological parameters, while deep learning models typically overlook disease transmission dynamics and lack interpretability in the epidemiological context. To address these limitations, we propose a novel Causal Spatiotemporal Graph Neural Network (CSTGNN), a hybrid framework that integrates a Spatio-Contact SIR model with Graph Neural Networks (GNNs) to capture the spatiotemporal propagation of epidemics. Inter-regional human mobility exhibits continuous and smooth spatiotemporal patterns, leading to adjacent graph structures that share underlying mobility dynamics. To model these dynamics, we employ an adaptive static connectivity graph to represent the stable components of human mobility and utilize a temporal dynamics model to capture fluctuations within these patterns. By integrating the adaptive static connectivity graph with the temporal dynamics graph, we construct a dynamic graph that encapsulates the comprehensive properties of human mobility networks. Additionally, to capture temporal trends and variations in infectious disease spread, we introduce a temporal decomposition model to handle temporal dependence. This model is then integrated with a dynamic graph convolutional network for epidemic forecasting. We validate our model using real-world datasets at the provincial level in China and the state level in Germany. Extensive studies demonstrate that our method effectively models the spatiotemporal dynamics of infectious diseases, providing a valuable tool for forecasting and intervention strategies. Furthermore, analysis of the learned parameters offers insights into disease transmission mechanisms, enhancing the interpretability and practical applicability of our model.\n        △ Less",
        "authors":"['Shuai Han', 'Lukas Stelz', 'Thomas R. Sokolowski', 'Kai Zhou', 'Horst Stöcker']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Optimal Heterogeneous Client Sampling in Multi-Model Federated Learning",
        "abstract":"Federated learning (FL) allows edge devices to collaboratively train models without sharing local data. As FL gains popularity, clients may need to train multiple unrelated FL models, but communication constraints limit their ability to train all models simultaneously. While clients could train FL models sequentially, opportunistically having FL clients concurrently train different models -- termed multi-model federated learning (MMFL) -- can reduce the overall training time. Prior work uses simple client-to-model assignments that do not optimize the contribution of each client to each model over the course of its training. Prior work on single-model FL shows that intelligent client selection can greatly accelerate convergence, but naïve extensions to MMFL can violate heterogeneous resource constraints at both the server and the clients. In this work, we develop a novel convergence analysis of MMFL with arbitrary client sampling methods, theoretically demonstrating the strengths and limitations of previous well-established gradient-based methods. Motivated by this analysis, we propose MMFL-LVR, a loss-based sampling method that minimizes training variance while explicitly respecting communication limits at the server and reducing computational costs at the clients. We extend this to MMFL-StaleVR, which incorporates stale updates for improved efficiency and stability, and MMFL-StaleVRE, a lightweight variant suitable for low-overhead deployment. Experiments show our methods improve average accuracy by up to 19.1% over random sampling, with only a 5.4% gap from the theoretical optimum (full client participation).\n        △ Less",
        "authors":"['Haoran Zhang', 'Zejun Gong', 'Zekai Li', 'Marie Siew', 'Carlee Joe-Wong', 'Rachid El-Azouzi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Interpretable Style Takagi-Sugeno-Kang Fuzzy Clustering",
        "abstract":"Clustering is an efficient and essential technique for exploring latent knowledge of data. However, limited attention has been given to the interpretability of the clusters detected by most clustering algorithms. In addition, due to the homogeneity of data, different groups of data have their own homogeneous styles. In this paper, the above two aspects are considered, and an interpretable style Takagi-Sugeno-Kang (TSK) fuzzy clustering (IS-TSK-FC) algorithm is proposed. The clustering behavior of IS-TSK-FC is fully guided by the TSK fuzzy inference on fuzzy rules. In particular, samples are grouped into clusters represented by the corresponding consequent vectors of all fuzzy rules learned in an unsupervised manner. This can explain how the clusters are generated in detail, thus making the underlying decision-making process of the IS-TSK-FC interpretable. Moreover, a series of style matrices are introduced to facilitate the consequents of fuzzy rules in IS-TSK-FC by capturing the styles of clusters as well as the nuances between different styles. Consequently, all the fuzzy rules in IS-TSK-FC have powerful data representation capability. After determining the antecedents of all the fuzzy rules, the optimization problem of IS-TSK-FC can be iteratively solved in an alternation manner. The effectiveness of IS-TSK-FC as an interpretable clustering tool is validated through extensive experiments on benchmark datasets with unknown implicit\/explicit styles. Specially, the superior clustering performance of IS-TSK-FC is demonstrated on case studies where different groups of data present explicit styles. The source code of IS-TSK-FC can be downloaded from https:\/\/github.com\/gusuhang10\/IS-TSK-FC.\n        △ Less",
        "authors":"['Suhang Gu', 'Ye Wang', 'Yongxin Chou', 'Jinliang Cong', 'Mingli Lu', 'Zhuqing Jiao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Balancing Robustness and Efficiency in Embedded DNNs Through Activation Function Selection",
        "abstract":"Machine learning-based embedded systems for safety-critical applications, such as aerospace and autonomous driving, must be robust to perturbations caused by soft errors. As transistor geometries shrink and voltages decrease, modern electronic devices become more susceptible to background radiation, increasing the concern about failures produced by soft errors. The resilience of deep neural networks (DNNs) to these errors depends not only on target device technology but also on model structure and the numerical representation and arithmetic precision of their parameters. Compression techniques like pruning and quantization, used to reduce memory footprint and computational complexity, alter both model structure and representation, affecting soft error robustness. In this regard, although often overlooked, the choice of activation functions (AFs) impacts not only accuracy and trainability but also compressibility and error resilience. This paper explores the use of bounded AFs to enhance robustness against parameter perturbations, while evaluating their effects on model accuracy, compressibility, and computational load with a technology-agnostic approach. We focus on encoder-decoder convolutional models developed for semantic segmentation of hyperspectral images with application to autonomous driving systems. Experiments are conducted on an AMD-Xilinx's KV260 SoM.\n        △ Less",
        "authors":"['Jon Gutiérrez Zaballa', 'Koldo Basterretxea', 'Javier Echanobe']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning",
        "abstract":"Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design.\n        △ Less",
        "authors":"['Anja Surina', 'Amin Mansouri', 'Lars Quaedvlieg', 'Amal Seddas', 'Maryna Viazovska', 'Emmanuel Abbe', 'Caglar Gulcehre']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SpeakEasy: Enhancing Text-to-Speech Interactions for Expressive Content Creation",
        "abstract":"Novice content creators often invest significant time recording expressive speech for social media videos. While recent advancements in text-to-speech (TTS) technology can generate highly realistic speech in various languages and accents, many struggle with unintuitive or overly granular TTS interfaces. We propose simplifying TTS generation by allowing users to specify high-level context alongside their script. Our Wizard-of-Oz system, SpeakEasy, leverages user-provided context to inform and influence TTS output, enabling iterative refinement with high-level feedback. This approach was informed by two 8-subject formative studies: one examining content creators' experiences with TTS, and the other drawing on effective strategies from voice actors. Our evaluation shows that participants using SpeakEasy were more successful in generating performances matching their personal standards, without requiring significantly more effort than leading industry interfaces.\n        △ Less",
        "authors":"['Stephen Brade', 'Sam Anderson', 'Rithesh Kumar', 'Zeyu Jin', 'Anh Truong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"State Tuning: State-based Test-Time Scaling on RWKV-7",
        "abstract":"Test-time scaling has emerged as a prominent research direction in machine learning, enabling models to enhance their expressive capabilities during inference.Transformers, renowned for striking a delicate balance between efficiency and expressiveness, have benefited from test-time scaling techniques that leverage an expanding key-value (KV) cache to significantly improve performance.In this paper, we introduce a novel state-based approach to test-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7 model.By exploiting the unique strengths of RWKV-7, our method achieves state-of-the-art performance on the target task without altering the model's pre-trained weights. Our approach centers on three key innovations. First, we develop an observer framework that allows a smaller model to replicate and learn the state dynamics of the RWKV-7 model. Second, we employ a kernel method to dynamically upscale the state size, enhancing the model's capacity to capture intricate patterns. Third, we integrate Decorrelated Backpropagation (DBP) to optimize the upscaled state matrix, thereby improving convergence and expressivity. By tuning only the state matrix, we demonstrate that a smaller model can outperform larger models on the given task. This method preserves the efficiency of the original RWKV-7 architecture while harnessing the power of test-time scaling to deliver superior results. Our findings underscore the potential of state tuning as an effective strategy for advancing model performance in resource-constrained settings. Our code is https:\/\/github.com\/TorchRWKV\/flash-linear-attention.\n        △ Less",
        "authors":"['Liu Xiao', 'Li Zhiyuan', 'Lin Yueyu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Climplicit: Climatic Implicit Embeddings for Global Ecological Tasks",
        "abstract":"Deep learning on climatic data holds potential for macroecological applications. However, its adoption remains limited among scientists outside the deep learning community due to storage, compute, and technical expertise barriers. To address this, we introduce Climplicit, a spatio-temporal geolocation encoder pretrained to generate implicit climatic representations anywhere on Earth. By bypassing the need to download raw climatic rasters and train feature extractors, our model uses x3500 less disk space and significantly reduces computational needs for downstream tasks. We evaluate our Climplicit embeddings on biomes classification, species distribution modeling, and plant trait regression. We find that single-layer probing our Climplicit embeddings consistently performs better or on par with training a model from scratch on downstream tasks and overall better than alternative geolocation encoding models.\n        △ Less",
        "authors":"['Johannes Dollinger', 'Damien Robert', 'Elena Plekhanova', 'Lukas Drees', 'Jan Dirk Wegner']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-Driven Tactical Communications and Networking for Defense: A Survey and Emerging Trends",
        "abstract":"The integration of Artificial Intelligence (AI) in military communications and networking is reshaping modern defense strategies, enhancing secure data exchange, real-time situational awareness, and autonomous decision-making. This survey explores how AI-driven technologies improve tactical communication networks, radar-based data transmission, UAV-assisted relay systems, and electronic warfare resilience. The study highlights AI applications in adaptive signal processing, multi-agent coordination for network optimization, radar-assisted target tracking, and AI-driven electronic countermeasures. Our work introduces a novel three-criteria evaluation methodology. It systematically assesses AI applications based on general system objectives, communications constraints in the military domain, and critical tactical environmental factors. We analyze key AI techniques for different types of learning applied to multi-domain network interoperability and distributed data information fusion in military operations. We also address challenges such as adversarial AI threats, the real-time adaptability of autonomous communication networks, and the limitations of current AI models under battlefield conditions. Finally, we discuss emerging trends in self-healing networks, AI-augmented decision support systems, and intelligent spectrum allocation. We provide a structured roadmap for future AI-driven defense communications and networking research.\n        △ Less",
        "authors":"['Victor Monzon Baeza', 'Raúl Parada', 'Laura Concha Salor', 'Carlos Monzo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MIAT: Maneuver-Intention-Aware Transformer for Spatio-Temporal Trajectory Prediction",
        "abstract":"Accurate vehicle trajectory prediction is critical for safe and efficient autonomous driving, especially in mixed traffic environments with both human-driven and autonomous vehicles. However, uncertainties introduced by inherent driving behaviors -- such as acceleration, deceleration, and left and right maneuvers -- pose significant challenges for reliable trajectory prediction. We introduce a Maneuver-Intention-Aware Transformer (MIAT) architecture, which integrates a maneuver intention awareness mechanism with spatiotemporal interaction modeling to enhance long-horizon trajectory predictions. We systematically investigate the impact of varying awareness of maneuver intention on both short- and long-horizon trajectory predictions. Evaluated on the real-world NGSIM dataset and benchmarked against various transformer- and LSTM-based methods, our approach achieves an improvement of up to 4.7% in short-horizon predictions and a 1.6% in long-horizon predictions compared to other intention-aware benchmark methods. Moreover, by leveraging an intention awareness control mechanism, MIAT realizes an 11.1% performance boost in long-horizon predictions, with a modest drop in short-horizon performance.\n        △ Less",
        "authors":"['Chandra Raskoti', 'Iftekharul Islam', 'Xuan Wang', 'Weizi Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Not All Data Are Unlearned Equally",
        "abstract":"Machine unlearning is concerned with the task of removing knowledge learned from particular data points from a trained model. In the context of large language models (LLMs), unlearning has recently received increased attention, particularly for removing knowledge about named entities from models for privacy purposes. While various approaches have been proposed to address the unlearning problem, most existing approaches treat all data points to be unlearned equally, i.e., unlearning that Montreal is a city in Canada is treated exactly the same as unlearning the phone number of the first author of this paper. In this work, we show that this all data is equal assumption does not hold for LLM unlearning. We study how the success of unlearning depends on the frequency of the knowledge we want to unlearn in the pre-training data of a model and find that frequency strongly affects unlearning, i.e., more frequent knowledge is harder to unlearn. Additionally, we uncover a misalignment between probability and generation-based evaluations of unlearning and show that this problem worsens as models become larger. Overall, our experiments highlight the need for better evaluation practices and novel methods for LLM unlearning that take the training data of models into account.\n        △ Less",
        "authors":"['Aravind Krishnan', 'Siva Reddy', 'Marius Mosbach']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Attention-Augmented Inverse Reinforcement Learning with Graph Convolutions for Multi-Agent Task Allocation",
        "abstract":"This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\n  Multi-agent task allocation (MATA) plays a vital role in cooperative multi-agent systems, with significant implications for applications such as logistics, search and rescue, and robotic coordination. Although traditional deep reinforcement learning (DRL) methods have been shown to be promising, their effectiveness is hindered by a reliance on manually designed reward functions and inefficiencies in dynamic environments. In this paper, an inverse reinforcement learning (IRL)-based framework is proposed, in which multi-head self-attention (MHSA) and graph attention mechanisms are incorporated to enhance reward function learning and task execution efficiency. Expert demonstrations are utilized to infer optimal reward densities, allowing dependence on handcrafted designs to be reduced and adaptability to be improved. Extensive experiments validate the superiority of the proposed method over widely used multi-agent reinforcement learning (MARL) algorithms in terms of both cumulative rewards and task execution efficiency.\n        △ Less",
        "authors":"['Huilin Yin', 'Zhikun Yang', 'Linchuan Zhang', 'Daniel Watzenig']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph-based Diffusion Model for Collaborative Filtering",
        "abstract":"Recently, diffusion-based recommendation methods have achieved impressive results. However, existing approaches predominantly treat each user's historical interactions as independent training samples, overlooking the potential of higher-order collaborative signals between users and items. Such signals, which encapsulate richer and more nuanced relationships, can be naturally captured using graph-based data structures. To address this limitation, we extend diffusion-based recommendation methods to the graph domain by directly modeling user-item bipartite graphs with diffusion models. This enables better modeling of the higher-order connectivity inherent in complex interaction dynamics. However, this extension introduces two primary challenges: (1) Noise Heterogeneity, where interactions are influenced by various forms of continuous and discrete noise, and (2) Relation Explosion, referring to the high computational costs of processing large-scale graphs. To tackle these challenges, we propose a Graph-based Diffusion Model for Collaborative Filtering (GDMCF). To address noise heterogeneity, we introduce a multi-level noise corruption mechanism that integrates both continuous and discrete noise, effectively simulating real-world interaction complexities. To mitigate relation explosion, we design a user-active guided diffusion process that selectively focuses on the most meaningful edges and active users, reducing inference costs while preserving the graph's topological integrity. Extensive experiments on three benchmark datasets demonstrate that GDMCF consistently outperforms state-of-the-art methods, highlighting its effectiveness in capturing higher-order collaborative signals and improving recommendation performance.\n        △ Less",
        "authors":"['Xuan Zhang', 'Xiang Deng', 'Hongxing Yuan', 'Chunyu Wei', 'Yushun Fan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-level Neural Networks for high-dimensional parametric obstacle problems",
        "abstract":"A new method to solve computationally challenging (random) parametric obstacle problems is developed and analyzed, where the parameters can influence the related partial differential equation (PDE) and determine the position and surface structure of the obstacle. As governing equation, a stationary elliptic diffusion problem is assumed. The high-dimensional solution of the obstacle problem is approximated by a specifically constructed convolutional neural network (CNN). This novel algorithm is inspired by a finite element constrained multigrid algorithm to represent the parameter to solution map. This has two benefits: First, it allows for efficient practical computations since multi-level data is used as an explicit output of the NN thanks to an appropriate data preprocessing. This improves the efficacy of the training process and subsequently leads to small errors in the natural energy norm. Second, the comparison of the CNN to a multigrid algorithm provides means to carry out a complete a priori convergence and complexity analysis of the proposed NN architecture. Numerical experiments illustrate a state-of-the-art performance for this challenging problem.\n        △ Less",
        "authors":"['Martin Eigel', 'Cosmas Heiß', 'Janina E. Schütte']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Concept Extraction for Time Series with ECLAD-ts",
        "abstract":"Convolutional neural networks (CNNs) for time series classification (TSC) are being increasingly used in applications ranging from quality prediction to medical diagnosis. The black box nature of these models makes understanding their prediction process difficult. This issue is crucial because CNNs are prone to learning shortcuts and biases, compromising their robustness and alignment with human expectations. To assess whether such mechanisms are being used and the associated risk, it is essential to provide model explanations that reflect the inner workings of the model. Concept Extraction (CE) methods offer such explanations, but have mostly been developed for the image domain so far, leaving a gap in the time series domain. In this work, we present a CE and localization method tailored to the time series domain, based on the ideas of CE methods for images. We propose the novel method ECLAD-ts, which provides post-hoc global explanations based on how the models encode subsets of the input at different levels of abstraction. For this, concepts are produced by clustering timestep-wise aggregations of CNN activation maps, and their importance is computed based on their impact on the prediction process. We evaluate our method on synthetic and natural datasets. Furthermore, we assess the advantages and limitations of CE in time series through empirical results. Our results show that ECLAD-ts effectively explains models by leveraging their internal representations, providing useful insights about their prediction process.\n        △ Less",
        "authors":"['Antonia Holzapfel', 'Andres Felipe Posada-Moreno', 'Sebastian Trimpe']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mixture-of-Personas Language Models for Population Simulation",
        "abstract":"Advances in Large Language Models (LLMs) paved the way for their emerging applications in various domains, such as human behavior simulations, where LLMs could augment human-generated data in social science research and machine learning model training. However, pretrained LLMs often fail to capture the behavioral diversity of target populations due to the inherent variability across individuals and groups. To address this, we propose \\textit{Mixture of Personas} (MoP), a \\textit{probabilistic} prompting method that aligns the LLM responses with the target population. MoP is a contextual mixture model, where each component is an LM agent characterized by a persona and an exemplar representing subpopulation behaviors. The persona and exemplar are randomly chosen according to the learned mixing weights to elicit diverse LLM responses during simulation. MoP is flexible, requires no model finetuning, and is transferable across base models. Experiments for synthetic data generation show that MoP outperforms competing methods in alignment and diversity metrics.\n        △ Less",
        "authors":"['Ngoc Bui', 'Hieu Trung Nguyen', 'Shantanu Kumar', 'Julian Theodore', 'Weikang Qiu', 'Viet Anh Nguyen', 'Rex Ying']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Joint Pedestrian and Vehicle Traffic Optimization in Urban Environments using Reinforcement Learning",
        "abstract":"Reinforcement learning (RL) holds significant promise for adaptive traffic signal control. While existing RL-based methods demonstrate effectiveness in reducing vehicular congestion, their predominant focus on vehicle-centric optimization leaves pedestrian mobility needs and safety challenges unaddressed. In this paper, we present a deep RL framework for adaptive control of eight traffic signals along a real-world urban corridor, jointly optimizing both pedestrian and vehicular efficiency. Our single-agent policy is trained using real-world pedestrian and vehicle demand data derived from Wi-Fi logs and video analysis. The results demonstrate significant performance improvements over traditional fixed-time signals, reducing average wait times per pedestrian and per vehicle by up to 67% and 52%, respectively, while simultaneously decreasing total accumulated wait times for both groups by up to 67% and 53%. Additionally, our results demonstrate generalization capabilities across varying traffic demands, including conditions entirely unseen during training, validating RL's potential for developing transportation systems that serve all road users.\n        △ Less",
        "authors":"['Bibek Poudel', 'Xuan Wang', 'Weizi Li', 'Lei Zhu', 'Kevin Heaslip']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deconstructing Jazz Piano Style Using Machine Learning",
        "abstract":"Artistic style has been studied for centuries, and recent advances in machine learning create new possibilities for understanding it computationally. However, ensuring that machine-learning models produce insights aligned with the interests of practitioners and critics remains a significant challenge. Here, we focus on musical style, which benefits from a rich theoretical and mathematical analysis tradition. We train a variety of supervised-learning models to identify 20 iconic jazz musicians across a carefully curated dataset of 84 hours of recordings, and interpret their decision-making processes. Our models include a novel multi-input architecture that enables four musical domains (melody, harmony, rhythm, and dynamics) to be analysed separately. These models enable us to address fundamental questions in music theory and also advance the state-of-the-art in music performer identification (94% accuracy across 20 classes). We release open-source implementations of our models and an accompanying web application for exploring musical styles.\n        △ Less",
        "authors":"['Huw Cheston', 'Reuben Bance', 'Peter M. C. Harrison']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Stacking Variational Bayesian Monte Carlo",
        "abstract":"Variational Bayesian Monte Carlo (VBMC) is a sample-efficient method for approximate Bayesian inference with computationally expensive likelihoods. While VBMC's local surrogate approach provides stable approximations, its conservative exploration strategy and limited evaluation budget can cause it to miss regions of complex posteriors. In this work, we introduce Stacking Variational Bayesian Monte Carlo (S-VBMC), a method that constructs global posterior approximations by merging independent VBMC runs through a principled and inexpensive post-processing step. Our approach leverages VBMC's mixture posterior representation and per-component evidence estimates, requiring no additional likelihood evaluations while being naturally parallelizable. We demonstrate S-VBMC's effectiveness on two synthetic problems designed to challenge VBMC's exploration capabilities and two real-world applications from computational neuroscience, showing substantial improvements in posterior approximation quality across all cases.\n        △ Less",
        "authors":"['Francesco Silvestrin', 'Chengkun Li', 'Luigi Acerbi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SurvSurf: a partially monotonic neural network for first-hitting time prediction of intermittently observed discrete and continuous sequential events",
        "abstract":"We propose a neural-network based survival model (SurvSurf) specifically designed for direct and simultaneous probabilistic prediction of the first hitting time of sequential events from baseline. Unlike existing models, SurvSurf is theoretically guaranteed to never violate the monotonic relationship between the cumulative incidence functions of sequential events, while allowing nonlinear influence from predictors. It also incorporates implicit truths for unobserved intermediate events in model fitting, and supports both discrete and continuous time and events. We also identified a variant of the Integrated Brier Score (IBS) that showed robust correlation with the mean squared error (MSE) between the true and predicted probabilities by accounting for implied truths about the missing intermediate events. We demonstrated the superiority of SurvSurf compared to modern and traditional predictive survival models in two simulated datasets and two real-world datasets, using MSE, the more robust IBS and by measuring the extent of monotonicity violation.\n        △ Less",
        "authors":"['Yichen Kelly Chen', 'Sören Dittmer', 'Kinga Bernatowicz', 'Josep Arús-Pous', 'Kamen Bliznashki', 'John Aston', 'James H. F. Rudd', 'Carola-Bibiane Schönlieb', 'James Jones', 'Michael Roberts']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transforming Future Data Center Operations and Management via Physical AI",
        "abstract":"Data centers (DCs) as mission-critical infrastructures are pivotal in powering the growth of artificial intelligence (AI) and the digital economy. The evolution from Internet DC to AI DC has introduced new challenges in operating and managing data centers for improved business resilience and reduced total cost of ownership. As a result, new paradigms, beyond the traditional approaches based on best practices, must be in order for future data centers. In this research, we propose and develop a novel Physical AI (PhyAI) framework for advancing DC operations and management. Our system leverages the emerging capabilities of state-of-the-art industrial products and our in-house research and development. Specifically, it presents three core modules, namely: 1) an industry-grade in-house simulation engine to simulate DC operations in a highly accurate manner, 2) an AI engine built upon NVIDIA PhysicsNemo for the training and evaluation of physics-informed machine learning (PIML) models, and 3) a digital twin platform built upon NVIDIA Omniverse for our proposed 5-tier digital twin framework. This system presents a scalable and adaptable solution to digitalize, optimize, and automate future data center operations and management, by enabling real-time digital twins for future data centers. To illustrate its effectiveness, we present a compelling case study on building a surrogate model for predicting the thermal and airflow profiles of a large-scale DC in a real-time manner. Our results demonstrate its superior performance over traditional time-consuming Computational Fluid Dynamics\/Heat Transfer (CFD\/HT) simulation, with a median absolute temperature prediction error of 0.18 °C. This emerging approach would open doors to several potential research directions for advancing Physical AI in future DC operations.\n        △ Less",
        "authors":"['Zhiwei Cao', 'Minghao Li', 'Feng Lin', 'Jimin Jia', 'Yonggang Wen', 'Jianxiong Yin', 'Simon See']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Domain-Based Taxonomy of Jailbreak Vulnerabilities in Large Language Models",
        "abstract":"The study of large language models (LLMs) is a key area in open-world machine learning. Although LLMs demonstrate remarkable natural language processing capabilities, they also face several challenges, including consistency issues, hallucinations, and jailbreak vulnerabilities. Jailbreaking refers to the crafting of prompts that bypass alignment safeguards, leading to unsafe outputs that compromise the integrity of LLMs. This work specifically focuses on the challenge of jailbreak vulnerabilities and introduces a novel taxonomy of jailbreak attacks grounded in the training domains of LLMs. It characterizes alignment failures through generalization, objectives, and robustness gaps. Our primary contribution is a perspective on jailbreak, framed through the different linguistic domains that emerge during LLM training and alignment. This viewpoint highlights the limitations of existing approaches and enables us to classify jailbreak attacks on the basis of the underlying model deficiencies they exploit. Unlike conventional classifications that categorize attacks based on prompt construction methods (e.g., prompt templating), our approach provides a deeper understanding of LLM behavior. We introduce a taxonomy with four categories -- mismatched generalization, competing objectives, adversarial robustness, and mixed attacks -- offering insights into the fundamental nature of jailbreak vulnerabilities. Finally, we present key lessons derived from this taxonomic study.\n        △ Less",
        "authors":"['Carlos Peláez-González', 'Andrés Herrera-Poyatos', 'Cristina Zuheros', 'David Herrera-Poyatos', 'Virilo Tejedor', 'Francisco Herrera']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Visual Text Grounding of Multimodal Large Language Model",
        "abstract":"Despite the existing evolution of Multimodal Large Language Models (MLLMs), a non-neglectable limitation remains in their struggle with visual text grounding, especially in text-rich images of documents. Document images, such as scanned forms and infographics, highlight critical challenges due to their complex layouts and textual content. However, current benchmarks do not fully address these challenges, as they mostly focus on visual grounding on natural images, rather than text-rich document images. Thus, to bridge this gap, we introduce TRIG, a novel task with a newly designed instruction dataset for benchmarking and improving the Text-Rich Image Grounding capabilities of MLLMs in document question-answering. Specifically, we propose an OCR-LLM-human interaction pipeline to create 800 manually annotated question-answer pairs as a benchmark and a large-scale training set of 90$ synthetic data based on four diverse datasets. A comprehensive evaluation of various MLLMs on our proposed benchmark exposes substantial limitations in their grounding capability on text-rich images. In addition, we propose two simple and effective TRIG methods based on general instruction tuning and plug-and-play efficient embedding, respectively. By finetuning MLLMs on our synthetic dataset, they promisingly improve spatial reasoning and grounding capabilities.\n        △ Less",
        "authors":"['Ming Li', 'Ruiyi Zhang', 'Jian Chen', 'Jiuxiang Gu', 'Yufan Zhou', 'Franck Dernoncourt', 'Wanrong Zhu', 'Tianyi Zhou', 'Tong Sun']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ensuring Safety in an Uncertain Environment: Constrained MDPs via Stochastic Thresholds",
        "abstract":"This paper studies constrained Markov decision processes (CMDPs) with constraints against stochastic thresholds, aiming at safety of reinforcement learning in unknown and uncertain environments. We leverage a Growing-Window estimator sampling from interactions with the uncertain and dynamic environment to estimate the thresholds, based on which we design Stochastic Pessimistic-Optimistic Thresholding (SPOT), a novel model-based primal-dual algorithm for multiple constraints against stochastic thresholds. SPOT enables reinforcement learning under both pessimistic and optimistic threshold settings. We prove that our algorithm achieves sublinear regret and constraint violation; i.e., a reward regret of $\\tilde{\\mathcal{O}}(\\sqrt{T})$ while allowing an $\\tilde{\\mathcal{O}}(\\sqrt{T})$ constraint violation over $T$ episodes. The theoretical guarantees show that our algorithm achieves performance comparable to that of an approach relying on fixed and clear thresholds. To the best of our knowledge, SPOT is the first reinforcement learning algorithm that realises theoretical guaranteed performance in an uncertain environment where even thresholds are unknown.\n        △ Less",
        "authors":"['Qian Zuo', 'Fengxiang He']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Unified Pairwise Framework for RLHF: Bridging Generative Reward Modeling and Policy Optimization",
        "abstract":"Reinforcement Learning from Human Feedback (RLHF) has emerged as a important paradigm for aligning large language models (LLMs) with human preferences during post-training. This framework typically involves two stages: first, training a reward model on human preference data, followed by optimizing the language model using reinforcement learning algorithms. However, current RLHF approaches may constrained by two limitations. First, existing RLHF frameworks often rely on Bradley-Terry models to assign scalar rewards based on pairwise comparisons of individual responses. However, this approach imposes significant challenges on reward model (RM), as the inherent variability in prompt-response pairs across different contexts demands robust calibration capabilities from the RM. Second, reward models are typically initialized from generative foundation models, such as pre-trained or supervised fine-tuned models, despite the fact that reward models perform discriminative tasks, creating a mismatch. This paper introduces Pairwise-RL, a RLHF framework that addresses these challenges through a combination of generative reward modeling and a pairwise proximal policy optimization (PPO) algorithm. Pairwise-RL unifies reward model training and its application during reinforcement learning within a consistent pairwise paradigm, leveraging generative modeling techniques to enhance reward model performance and score calibration. Experimental evaluations demonstrate that Pairwise-RL outperforms traditional RLHF frameworks across both internal evaluation datasets and standard public benchmarks, underscoring its effectiveness in improving alignment and model behavior.\n        △ Less",
        "authors":"['Wenyuan Xu', 'Xiaochen Zuo', 'Chao Xin', 'Yu Yue', 'Lin Yan', 'Yonghui Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Llama walks into the 'Bar': Efficient Supervised Fine-Tuning for Legal Reasoning in the Multi-state Bar Exam",
        "abstract":"Legal reasoning tasks present unique challenges for large language models (LLMs) due to the complexity of domain-specific knowledge and reasoning processes. This paper investigates how effectively smaller language models (Llama 2 7B and Llama 3 8B) can be fine-tuned with a limited dataset of 1,514 Multi-state Bar Examination (MBE) questions to improve legal question answering accuracy. We evaluate these models on the 2022 MBE questions licensed from JD Advising, the same dataset used in the 'GPT-4 passes the Bar exam' study. Our methodology involves collecting approximately 200 questions per legal domain across 7 domains. We distill the dataset using Llama 3 (70B) to transform explanations into a structured IRAC (Issue, Rule, Application, Conclusion) format as a guided reasoning process to see if it results in better performance over the non-distilled dataset. We compare the non-fine-tuned models against their supervised fine-tuned (SFT) counterparts, trained for different sample sizes per domain, to study the effect on accuracy and prompt adherence. We also analyse option selection biases and their mitigation following SFT. In addition, we consolidate the performance across multiple variables: prompt type (few-shot vs zero-shot), answer ordering (chosen-option first vs generated-explanation first), response format (Numbered list vs Markdown vs JSON), and different decoding temperatures. Our findings show that domain-specific SFT helps some model configurations achieve close to human baseline performance, despite limited computational resources and a relatively small dataset. We release both the gathered SFT dataset and the family of Supervised Fine-tuned (SFT) adapters optimised for MBE performance. This establishes a practical lower bound on resources needed towards achieving effective legal question answering in smaller LLMs.\n        △ Less",
        "authors":"['Rean Fernandes', 'André Biedenkapp', 'Frank Hutter', 'Noor Awad']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Constrained Gaussian Process Motion Planning via Stein Variational Newton Inference",
        "abstract":"Gaussian Process Motion Planning (GPMP) is a widely used framework for generating smooth trajectories within a limited compute time--an essential requirement in many robotic applications. However, traditional GPMP approaches often struggle with enforcing hard nonlinear constraints and rely on Maximum a Posteriori (MAP) solutions that disregard the full Bayesian posterior. This limits planning diversity and ultimately hampers decision-making. Recent efforts to integrate Stein Variational Gradient Descent (SVGD) into motion planning have shown promise in handling complex constraints. Nonetheless, these methods still face persistent challenges, such as difficulties in strictly enforcing constraints and inefficiencies when the probabilistic inference problem is poorly conditioned. To address these issues, we propose a novel constrained Stein Variational Gaussian Process Motion Planning (cSGPMP) framework, incorporating a GPMP prior specifically designed for trajectory optimization under hard constraints. Our approach improves the efficiency of particle-based inference while explicitly handling nonlinear constraints. This advancement significantly broadens the applicability of GPMP to motion planning scenarios demanding robust Bayesian inference, strict constraint adherence, and computational efficiency within a limited time. We validate our method on standard benchmarks, achieving an average success rate of 98.57% across 350 planning tasks, significantly outperforming competitive baselines. This demonstrates the ability of our method to discover and use diverse trajectory modes, enhancing flexibility and adaptability in complex environments, and delivering significant improvements over standard baselines without incurring major computational costs.\n        △ Less",
        "authors":"['Jiayun Li', 'Kay Pompetzki', 'An Thai Le', 'Haolei Tong', 'Jan Peters', 'Georgia Chalvatzaki']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Boosting Relational Deep Learning with Pretrained Tabular Models",
        "abstract":"Relational databases, organized into tables connected by primary-foreign key relationships, are a common format for organizing data. Making predictions on relational data often involves transforming them into a flat tabular format through table joins and feature engineering, which serve as input to tabular methods. However, designing features that fully capture complex relational patterns remains challenging. Graph Neural Networks (GNNs) offer a compelling alternative by inherently modeling these relationships, but their time overhead during inference limits their applicability for real-time scenarios. In this work, we aim to bridge this gap by leveraging existing feature engineering efforts to enhance the efficiency of GNNs in relational databases. Specifically, we use GNNs to capture complex relationships within relational databases, patterns that are difficult to featurize, while employing engineered features to encode temporal information, thereby avoiding the need to retain the entire historical graph and enabling the use of smaller, more efficient graphs. Our \\textsc{LightRDL} approach not only improves efficiency, but also outperforms existing models. Experimental results on the RelBench benchmark demonstrate that our framework achieves up to $33\\%$ performance improvement and a $526\\times$ inference speedup compared to GNNs, making it highly suitable for real-time inference.\n        △ Less",
        "authors":"['Veronica Lachi', 'Antonio Longa', 'Beatrice Bevilacqua', 'Bruno Lepri', 'Andrea Passerini', 'Bruno Ribeiro']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration",
        "abstract":"Retrieval-Augmented Generation (RAG) systems often struggle to handle multi-hop question-answering tasks accurately due to irrelevant context retrieval and limited complex reasoning capabilities. We introduce Collab-RAG, a collaborative training framework that leverages mutual enhancement between a white-box small language model (SLM) and a blackbox large language model (LLM) for RAG. Specifically, the SLM decomposes complex queries into simpler sub-questions, thus enhancing the accuracy of the retrieval and facilitating more effective reasoning by the black-box LLM. Concurrently, the black-box LLM provides feedback signals to improve the SLM's decomposition capability. We observe that Collab-RAG relies solely on supervision from an affordable black-box LLM without additional distillation from frontier LLMs, yet demonstrates strong generalization across multiple black-box LLMs. Experimental evaluations across five multi-hop QA datasets demonstrate that Collab-RAG substantially outperforms existing black-box-only and SLM fine-tuning baselines by 1.8%-14.2% on average. In particular, our fine-tuned 3B SLM surpasses a frozen 32B LLM in question decomposition, highlighting the efficiency of Collab-RAG in improving reasoning and retrieval for complex questions. The code of Collab-RAG is available on https:\/\/github.com\/ritaranx\/Collab-RAG\/.\n        △ Less",
        "authors":"['Ran Xu', 'Wenqi Shi', 'Yuchen Zhuang', 'Yue Yu', 'Joyce C. Ho', 'Haoyu Wang', 'Carl Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"IterMask3D: Unsupervised Anomaly Detection and Segmentation with Test-Time Iterative Mask Refinement in 3D Brain MR",
        "abstract":"Unsupervised anomaly detection and segmentation methods train a model to learn the training distribution as 'normal'. In the testing phase, they identify patterns that deviate from this normal distribution as 'anomalies'. To learn the `normal' distribution, prevailing methods corrupt the images and train a model to reconstruct them. During testing, the model attempts to reconstruct corrupted inputs based on the learned 'normal' distribution. Deviations from this distribution lead to high reconstruction errors, which indicate potential anomalies. However, corrupting an input image inevitably causes information loss even in normal regions, leading to suboptimal reconstruction and an increased risk of false positives. To alleviate this, we propose IterMask3D, an iterative spatial mask-refining strategy designed for 3D brain MRI. We iteratively spatially mask areas of the image as corruption and reconstruct them, then shrink the mask based on reconstruction error. This process iteratively unmasks 'normal' areas to the model, whose information further guides reconstruction of 'normal' patterns under the mask to be reconstructed accurately, reducing false positives. In addition, to achieve better reconstruction performance, we also propose using high-frequency image content as additional structural information to guide the reconstruction of the masked area. Extensive experiments on the detection of both synthetic and real-world imaging artifacts, as well as segmentation of various pathological lesions across multiple MRI sequences, consistently demonstrate the effectiveness of our proposed method.\n        △ Less",
        "authors":"['Ziyun Liang', 'Xiaoqing Guo', 'Wentian Xu', 'Yasin Ibrahim', 'Natalie Voets', 'Pieter M Pretorius', 'J. Alison Noble', 'Konstantinos Kamnitsas']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AlgOS: Algorithm Operating System",
        "abstract":"Algorithm Operating System (AlgOS) is an unopinionated, extensible, modular framework for algorithmic implementations. AlgOS offers numerous features: integration with Optuna for automated hyperparameter tuning; automated argument parsing for generic command-line interfaces; automated registration of new classes; and a centralised database for logging experiments and studies. These features are designed to reduce the overhead of implementing new algorithms and to standardise the comparison of algorithms. The standardisation of algorithmic implementations is crucial for reproducibility and reliability in research. AlgOS combines Abstract Syntax Trees with a novel implementation of the Observer pattern to control the logical flow of algorithmic segments.\n        △ Less",
        "authors":"['Llewyn Salt', 'Marcus Gallagher']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Large Language Models for Cost-Effective, Multilingual Depression Detection and Severity Assessment",
        "abstract":"Depression is a prevalent mental health disorder that is difficult to detect early due to subjective symptom assessments. Recent advancements in large language models have offered efficient and cost-effective approaches for this objective. In this study, we evaluated the performance of four LLMs in depression detection using clinical interview data. We selected the best performing model and further tested it in the severity evaluation scenario and knowledge enhanced scenario. The robustness was evaluated in complex diagnostic scenarios using a dataset comprising 51074 statements from six different mental disorders. We found that DeepSeek V3 is the most reliable and cost-effective model for depression detection, performing well in both zero-shot and few-shot scenarios, with zero-shot being the most efficient choice. The evaluation of severity showed low agreement with the human evaluator, particularly for mild depression. The model maintains stably high AUCs for detecting depression in complex diagnostic scenarios. These findings highlight DeepSeek V3s strong potential for text-based depression detection in real-world clinical applications. However, they also underscore the need for further refinement in severity assessment and the mitigation of potential biases to enhance clinical reliability.\n        △ Less",
        "authors":"['Longdi Xian', 'Jianzhang Ni', 'Mingzhu Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SoK: LLM-based Log Parsing",
        "abstract":"Log data, generated by software systems, provides crucial insights for tasks like monitoring, root cause analysis, and anomaly detection. Due to the vast volume of logs, automated log parsing is essential to transform semi-structured log messages into structured representations. Traditional log parsing techniques often require manual configurations, such as defining log formats or labeling data, which limits scalability and usability. Recent advances in large language models (LLMs) have introduced the new research field of LLM-based log parsing, offering potential improvements in automation and adaptability. Despite promising results, there is no structured overview of these approaches since this is a relatively new research field with the earliest advances published in late 2023. This paper systematically reviews 29 LLM-based log parsing methods, comparing their capabilities, limitations, and reliance on manual effort. We analyze the learning and prompt-engineering paradigms employed, efficiency- and effectiveness-enhancing techniques, and the role of LLMs in the parsing process. We aggregate the results of the survey in a large table comprising the characterizing features of LLM-based log parsing approaches and derive the general process of LLM-based log parsing, incorporating all reviewed approaches in a single flow chart. Additionally, we benchmark seven open-source LLM-based log parsers on public datasets and critically assess their reproducibility. Our findings summarize the advances of this new research field and provide insights for researchers and practitioners seeking efficient and user-friendly log parsing solutions, with all code and results made publicly available for transparency.\n        △ Less",
        "authors":"['Viktor Beck', 'Max Landauer', 'Markus Wurzenberger', 'Florian Skopik', 'Andreas Rauber']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Closed-Loop Neural Operator-Based Observer of Traffic Density",
        "abstract":"We consider the problem of traffic density estimation with sparse measurements from stationary roadside sensors. Our approach uses Fourier neural operators to learn macroscopic traffic flow dynamics from high-fidelity microscopic-level simulations. During inference, the operator functions as an open-loop predictor of traffic evolution. To close the loop, we couple the open-loop operator with a correction operator that combines the predicted density with sparse measurements from the sensors. Simulations with the SUMO software indicate that, compared to open-loop observers, the proposed closed-loop observer exhibit classical closed-loop properties such as robustness to noise and ultimate boundedness of the error. This shows the advantages of combining learned physics with real-time corrections, and opens avenues for accurate, efficient, and interpretable data-driven observers.\n        △ Less",
        "authors":"['Alice Harting', 'Karl Henrik Johansson', 'Matthieu Barreau']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FedSAUC: A Similarity-Aware Update Control for Communication-Efficient Federated Learning in Edge Computing",
        "abstract":"Federated learning is a distributed machine learning framework to collaboratively train a global model without uploading privacy-sensitive data onto a centralized server. Usually, this framework is applied to edge devices such as smartphones, wearable devices, and Internet of Things (IoT) devices which closely collect information from users. However, these devices are mostly battery-powered. The update procedure of federated learning will constantly consume the battery power and the transmission bandwidth. In this work, we propose an update control for federated learning, FedSAUC, by considering the similarity of users' behaviors (models). At the server side, we exploit clustering algorithms to group devices with similar models. Then we select some representatives for each cluster to update information to train the model. We also implemented a testbed prototyping on edge devices for validating the performance. The experimental results show that this update control will not affect the training accuracy in the long run.\n        △ Less",
        "authors":"['Ming-Lun Lee', 'Han-Chang Chou', 'Yan-Ann Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
        "abstract":"Adversarial patch attacks pose a major threat to vision systems by embedding localized perturbations that mislead deep models. Traditional defense methods often require retraining or fine-tuning, making them impractical for real-world deployment. We propose a training-free Visual Retrieval-Augmented Generation (VRAG) framework that integrates Vision-Language Models (VLMs) for adversarial patch detection. By retrieving visually similar patches and images that resemble stored attacks in a continuously expanding database, VRAG performs generative reasoning to identify diverse attack types, all without additional training or fine-tuning. We extensively evaluate open-source large-scale VLMs, including Qwen-VL-Plus, Qwen2.5-VL-72B, and UI-TARS-72B-DPO, alongside Gemini-2.0, a closed-source model. Notably, the open-source UI-TARS-72B-DPO model achieves up to 95 percent classification accuracy, setting a new state-of-the-art for open-source adversarial patch detection. Gemini-2.0 attains the highest overall accuracy, 98 percent, but remains closed-source. Experimental results demonstrate VRAG's effectiveness in identifying a variety of adversarial patches with minimal human annotation, paving the way for robust, practical defenses against evolving adversarial patch attacks.\n        △ Less",
        "authors":"['Roie Kazoom', 'Raz Lapid', 'Moshe Sipper', 'Ofer Hadar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Nonlocal techniques for the analysis of deep ReLU neural network approximations",
        "abstract":"Recently, Daubechies, DeVore, Foucart, Hanin, and Petrova introduced a system of piece-wise linear functions, which can be easily reproduced by artificial neural networks with the ReLU activation function and which form a Riesz basis of $L_2([0,1])$. This work was generalized by two of the authors to the multivariate setting. We show that this system serves as a Riesz basis also for Sobolev spaces $W^s([0,1]^d)$ and Barron classes ${\\mathbb B}^s([0,1]^d)$ with smoothness $0<s<1$. We apply this fact to re-prove some recent results on the approximation of functions from these classes by deep neural networks. Our proof method avoids using local approximations and allows us to track also the implicit constants as well as to show that we can avoid the curse of dimension. Moreover, we also study how well one can approximate Sobolev and Barron functions by ANNs if only function values are known.\n        △ Less",
        "authors":"['Cornelia Schneider', 'Mario Ullrich', 'Jan Vybiral']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Attentional Graph Meta-Learning for Indoor Localization Using Extremely Sparse Fingerprints",
        "abstract":"Fingerprint-based indoor localization is often labor-intensive due to the need for dense grids and repeated measurements across time and space. Maintaining high localization accuracy with extremely sparse fingerprints remains a persistent challenge. Existing benchmark methods primarily rely on the measured fingerprints, while neglecting valuable spatial and environmental characteristics. In this paper, we propose a systematic integration of an Attentional Graph Neural Network (AGNN) model, capable of learning spatial adjacency relationships and aggregating information from neighboring fingerprints, and a meta-learning framework that utilizes datasets with similar environmental characteristics to enhance model training. To minimize the labor required for fingerprint collection, we introduce two novel data augmentation strategies: 1) unlabeled fingerprint augmentation using moving platforms, which enables the semi-supervised AGNN model to incorporate information from unlabeled fingerprints, and 2) synthetic labeled fingerprint augmentation through environmental digital twins, which enhances the meta-learning framework through a practical distribution alignment, which can minimize the feature discrepancy between synthetic and real-world fingerprints effectively. By integrating these novel modules, we propose the Attentional Graph Meta-Learning (AGML) model. This novel model combines the strengths of the AGNN model and the meta-learning framework to address the challenges posed by extremely sparse fingerprints. To validate our approach, we collected multiple datasets from both consumer-grade WiFi devices and professional equipment across diverse environments. Extensive experiments conducted on both synthetic and real-world datasets demonstrate that the AGML model-based localization method consistently outperforms all baseline methods using sparse fingerprints across all evaluated metrics.\n        △ Less",
        "authors":"['Wenzhong Yan', 'Feng Yin', 'Jun Gao', 'Ao Wang', 'Yang Tian', 'Ruizhi Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sparse Optimization for Transfer Learning: A L0-Regularized Framework for Multi-Source Domain Adaptation",
        "abstract":"This paper explores transfer learning in heterogeneous multi-source environments with distributional divergence between target and auxiliary domains. To address challenges in statistical bias and computational efficiency, we propose a Sparse Optimization for Transfer Learning (SOTL) framework based on L0-regularization. The method extends the Joint Estimation Transferred from Strata (JETS) paradigm with two key innovations: (1) L0-constrained exact sparsity for parameter space compression and complexity reduction, and (2) refining optimization focus to emphasize target parameters over redundant ones. Simulations show that SOTL significantly improves both estimation accuracy and computational speed, especially under adversarial auxiliary domain conditions. Empirical validation on the Community and Crime benchmarks demonstrates the statistical robustness of the SOTL method in cross-domain transfer.\n        △ Less",
        "authors":"['Chenqi Gong', 'Hu Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Topological Schrödinger Bridge Matching",
        "abstract":"Given two boundary distributions, the Schrödinger Bridge (SB) problem seeks the ``most likely`` random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows. In this work, we propose the Topological Schrödinger Bridge problem (TSBP) for matching signal distributions on a topological domain. We set the reference process to follow some linear tractable topology-aware stochastic dynamics such as topological heat diffusion. For the case of Gaussian boundary distributions, we derive a closed-form topological SB (TSB) in terms of its time-marginal and stochastic differential. In the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns. Building on these results, we develop TSB-based models for matching topological signals by parameterizing the unknowns in the optimal process as (topological) neural networks and learning them through likelihood training. We validate the theoretical results and demonstrate the practical applications of TSB-based models on both synthetic and real-world networks, emphasizing the role of topology. Additionally, we discuss the connections of TSB-based models to other emerging models, and outline future directions for topological signal matching.\n        △ Less",
        "authors":"['Maosheng Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TabRep: a Simple and Effective Continuous Representation for Training Tabular Diffusion Models",
        "abstract":"Diffusion models have been the predominant generative model for tabular data generation. However, they face the conundrum of modeling under a separate versus a unified data representation. The former encounters the challenge of jointly modeling all multi-modal distributions of tabular data in one model. While the latter alleviates this by learning a single representation for all features, it currently leverages sparse suboptimal encoding heuristics and necessitates additional computation costs. In this work, we address the latter by presenting TabRep, a tabular diffusion architecture trained with a unified continuous representation. To motivate the design of our representation, we provide geometric insights into how the data manifold affects diffusion models. The key attributes of our representation are composed of its density, flexibility to provide ample separability for nominal features, and ability to preserve intrinsic relationships. Ultimately, TabRep provides a simple yet effective approach for training tabular diffusion models under a continuous data manifold. Our results showcase that TabRep achieves superior performance across a broad suite of evaluations. It is the first to synthesize tabular data that exceeds the downstream quality of the original datasets while preserving privacy and remaining computationally efficient.\n        △ Less",
        "authors":"['Jacob Si', 'Zijing Ou', 'Mike Qu', 'Zhengrui Xiang', 'Yingzhen Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Playing Non-Embedded Card-Based Games with Reinforcement Learning",
        "abstract":"Significant progress has been made in AI for games, including board games, MOBA, and RTS games. However, complex agents are typically developed in an embedded manner, directly accessing game state information, unlike human players who rely on noisy visual data, leading to unfair competition. Developing complex non-embedded agents remains challenging, especially in card-based RTS games with complex features and large state spaces. We propose a non-embedded offline reinforcement learning training strategy using visual inputs to achieve real-time autonomous gameplay in the RTS game Clash Royale. Due to the lack of a object detection dataset for this game, we designed an efficient generative object detection dataset for training. We extract features using state-of-the-art object detection and optical character recognition models. Our method enables real-time image acquisition, perception feature fusion, decision-making, and control on mobile devices, successfully defeating built-in AI opponents. All code is open-sourced at https:\/\/github.com\/wty-yy\/katacr.\n        △ Less",
        "authors":"['Tianyang Wu', 'Lipeng Wan', 'Yuhang Wang', 'Qiang Wan', 'Xuguang Lan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Feedback-Enhanced Hallucination-Resistant Vision-Language Model for Real-Time Scene Understanding",
        "abstract":"Real-time scene comprehension is a key advance in artificial intelligence, enhancing robotics, surveillance, and assistive tools. However, hallucination remains a challenge. AI systems often misinterpret visual inputs, detecting nonexistent objects or describing events that never happened. These errors, far from minor, threaten reliability in critical areas like security and autonomous navigation where accuracy is essential.\n  Our approach tackles this by embedding self-awareness into the AI. Instead of trusting initial outputs, our framework continuously assesses them in real time, adjusting confidence thresholds dynamically. When certainty falls below a solid benchmark, it suppresses unreliable claims. Combining YOLOv5's object detection strength with VILA1.5-3B's controlled language generation, we tie descriptions to confirmed visual data. Strengths include dynamic threshold tuning for better accuracy, evidence-based text to reduce hallucination, and real-time performance at 18 frames per second.\n  This feedback-driven design cuts hallucination by 37 percent over traditional methods. Fast, flexible, and reliable, it excels in applications from robotic navigation to security monitoring, aligning AI perception with reality.\n        △ Less",
        "authors":"['Zahir Alsulaimawi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bidirectional Hierarchical Protein Multi-Modal Representation Learning",
        "abstract":"Protein representation learning is critical for numerous biological tasks. Recently, large transformer-based protein language models (pLMs) pretrained on large scale protein sequences have demonstrated significant success in sequence-based tasks. However, pLMs lack structural information. Conversely, graph neural networks (GNNs) designed to leverage 3D structural information have shown promising generalization in protein-related prediction tasks, but their effectiveness is often constrained by the scarcity of labeled structural data. Recognizing that sequence and structural representations are complementary perspectives of the same protein entity, we propose a multimodal bidirectional hierarchical fusion framework to effectively merge these modalities. Our framework employs attention and gating mechanisms to enable effective interaction between pLMs-generated sequential representations and GNN-extracted structural features, improving information exchange and enhancement across layers of the neural network. Based on the framework, we further introduce local Bi-Hierarchical Fusion with gating and global Bi-Hierarchical Fusion with multihead self-attention approaches. Through extensive experiments on a diverse set of protein-related tasks, our method demonstrates consistent improvements over strong baselines and existing fusion techniques in a variety of protein representation learning benchmarks, including react (enzyme\/EC classification), model quality assessment (MQA), protein-ligand binding affinity prediction (LBA), protein-protein binding site prediction (PPBS), and B cell epitopes prediction (BCEs). Our method establishes a new state-of-the-art for multimodal protein representation learning, emphasizing the efficacy of BIHIERARCHICAL FUSION in bridging sequence and structural modalities.\n        △ Less",
        "authors":"['Xuefeng Liu', 'Songhao Jiang', 'Chih-chan Tien', 'Jinbo Xu', 'Rick Stevens']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"KunPeng: A Global Ocean Environmental Model",
        "abstract":"Inspired by the similarity of the atmosphere-ocean physical coupling mechanism, this study innovatively migrates meteorological large-model techniques to the ocean domain, constructing the KunPeng global ocean environmental prediction model. Aimed at the discontinuous characteristics of marine space, we propose a terrain-adaptive mask constraint mechanism to mitigate effectively training divergence caused by abrupt gradients at land-sea boundaries. To fully integrate far-, medium-, and close-range marine features, a longitude-cyclic deformable convolution network (LC-DCN) is employed to enhance the dynamic receptive field, achieving refined modeling of multi-scale oceanic characteristics. A Deformable Convolution-enhanced Multi-Step Prediction module (DC-MTP) is employed to strengthen temporal dependency feature extraction capabilities. Experimental results demonstrate that this model achieves an average ACC of 0.80 in 15-day global predictions at 0.25$^\\circ$ resolution, outperforming comparative models by 0.01-0.08. The average mean squared error (MSE) is 0.41 (representing a 5%-31% reduction) and the average mean absolute error (MAE) is 0.44 (0.6%-21% reduction) compared to other models. Significant improvements are particularly observed in sea surface parameter prediction, deep-sea region characterization, and current velocity field forecasting. Through a horizontal comparison of the applicability of operators at different scales in the marine domain, this study reveals that local operators significantly outperform global operators under slow-varying oceanic processes, demonstrating the effectiveness of dynamic feature pyramid representations in predicting marine physical parameters.\n        △ Less",
        "authors":"['Yi Zhao', 'Jiaqi Li', 'Haitao Xia', 'Tianjiao Zhang', 'Zerong Zeng', 'Tianyu Ren', 'Yucheng Zhang', 'Chao Zhu', 'Shengtong Xu', 'Hongchun Yuan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Continuous Locomotive Crowd Behavior Generation",
        "abstract":"Modeling and reproducing crowd behaviors are important in various domains including psychology, robotics, transport engineering and virtual environments. Conventional methods have focused on synthesizing momentary scenes, which have difficulty in replicating the continuous nature of real-world crowds. In this paper, we introduce a novel method for automatically generating continuous, realistic crowd trajectories with heterogeneous behaviors and interactions among individuals. We first design a crowd emitter model. To do this, we obtain spatial layouts from single input images, including a segmentation map, appearance map, population density map and population probability, prior to crowd generation. The emitter then continually places individuals on the timeline by assigning independent behavior characteristics such as agents' type, pace, and start\/end positions using diffusion models. Next, our crowd simulator produces their long-term locomotions. To simulate diverse actions, it can augment their behaviors based on a Markov chain. As a result, our overall framework populates the scenes with heterogeneous crowd behaviors by alternating between the proposed emitter and simulator. Note that all the components in the proposed framework are user-controllable. Lastly, we propose a benchmark protocol to evaluate the realism and quality of the generated crowds in terms of the scene-level population dynamics and the individual-level trajectory accuracy. We demonstrate that our approach effectively models diverse crowd behavior patterns and generalizes well across different geographical environments. Code is publicly available at https:\/\/github.com\/InhwanBae\/CrowdES .\n        △ Less",
        "authors":"['Inhwan Bae', 'Junoh Lee', 'Hae-Gon Jeon']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Investigating Popularity Bias Amplification in Recommender Systems Employed in the Entertainment Domain",
        "abstract":"Recommender systems have become an integral part of our daily online experience by analyzing past user behavior to suggest relevant content in entertainment domains such as music, movies, and books. Today, they are among the most widely used applications of AI and machine learning. Consequently, regulations and guidelines for trustworthy AI, such as the European AI Act, which addresses issues like bias and fairness, are highly relevant to the design, development, and evaluation of recommender systems. One particularly important type of bias in this context is popularity bias, which results in the unfair underrepresentation of less popular content in recommendation lists. This work summarizes our research on investigating the amplification of popularity bias in recommender systems within the entertainment sector. Analyzing datasets from three entertainment domains, music, movies, and anime, we demonstrate that an item's recommendation frequency is positively correlated with its popularity. As a result, user groups with little interest in popular content receive less accurate recommendations compared to those who prefer widely popular items. Furthermore, we aim to better understand the connection between recommendation accuracy, calibration quality of algorithms, and popularity bias amplification.\n        △ Less",
        "authors":"['Dominik Kowald']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Vision Transformers with Autoencoders and Explainable AI for Cancer Patient Risk Stratification Using Whole Slide Imaging",
        "abstract":"Cancer remains one of the leading causes of mortality worldwide, necessitating accurate diagnosis and prognosis. Whole Slide Imaging (WSI) has become an integral part of clinical workflows with advancements in digital pathology. While various studies have utilized WSIs, their extracted features may not fully capture the most relevant pathological information, and their lack of interpretability limits clinical adoption.\n  In this paper, we propose PATH-X, a framework that integrates Vision Transformers (ViT) and Autoencoders with SHAP (Shapley Additive Explanations) to enhance model explainability for patient stratification and risk prediction using WSIs from The Cancer Genome Atlas (TCGA). A representative image slice is selected from each WSI, and numerical feature embeddings are extracted using Google's pre-trained ViT. These features are then compressed via an autoencoder and used for unsupervised clustering and classification tasks. Kaplan-Meier survival analysis is applied to evaluate stratification into two and three risk groups. SHAP is used to identify key contributing features, which are mapped onto histopathological slices to provide spatial context.\n  PATH-X demonstrates strong performance in breast and glioma cancers, where a sufficient number of WSIs enabled robust stratification. However, performance in lung cancer was limited due to data availability, emphasizing the need for larger datasets to enhance model reliability and clinical applicability.\n        △ Less",
        "authors":"['Ahmad Hussein', 'Mukesh Prasad', 'Ali Anaissi', 'Ali Braytee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MedGNN: Capturing the Links Between Urban Characteristics and Medical Prescriptions",
        "abstract":"Understanding how urban socio-demographic and environmental factors relate with health is essential for public health and urban planning. However, traditional statistical methods struggle with nonlinear effects, while machine learning models often fail to capture geographical (nearby areas being more similar) and topological (unequal connectivity between places) effects in an interpretable way. To address this, we propose MedGNN, a spatio-topologically explicit framework that constructs a 2-hop spatial graph, integrating positional and locational node embeddings with urban characteristics in a graph neural network. Applied to MEDSAT, a comprehensive dataset covering over 150 environmental and socio-demographic factors and six prescription outcomes (depression, anxiety, diabetes, hypertension, asthma, and opioids) across 4,835 Greater London neighborhoods, MedGNN improved predictions by over 25% on average compared to baseline methods. Using depression prescriptions as a case study, we analyzed graph embeddings via geographical principal component analysis, identifying findings that: align with prior research (e.g., higher antidepressant prescriptions among older and White populations), contribute to ongoing debates (e.g., greenery linked to higher and NO2 to lower prescriptions), and warrant further study (e.g., canopy evaporation correlated with fewer prescriptions). These results demonstrate MedGNN's potential, and more broadly, of carefully applied machine learning, to advance transdisciplinary public health research.\n        △ Less",
        "authors":"['Minwei Zhao', 'Sanja Scepanovic', 'Stephen Law', 'Daniele Quercia', 'Ivica Obadic']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TathyaNyaya and FactLegalLlama: Advancing Factual Judgment Prediction and Explanation in the Indian Legal Context",
        "abstract":"In the landscape of Fact-based Judgment Prediction and Explanation (FJPE), reliance on factual data is essential for developing robust and realistic AI-driven decision-making tools. This paper introduces TathyaNyaya, the largest annotated dataset for FJPE tailored to the Indian legal context, encompassing judgments from the Supreme Court of India and various High Courts. Derived from the Hindi terms \"Tathya\" (fact) and \"Nyaya\" (justice), the TathyaNyaya dataset is uniquely designed to focus on factual statements rather than complete legal texts, reflecting real-world judicial processes where factual data drives outcomes. Complementing this dataset, we present FactLegalLlama, an instruction-tuned variant of the LLaMa-3-8B Large Language Model (LLM), optimized for generating high-quality explanations in FJPE tasks. Finetuned on the factual data in TathyaNyaya, FactLegalLlama integrates predictive accuracy with coherent, contextually relevant explanations, addressing the critical need for transparency and interpretability in AI-assisted legal systems. Our methodology combines transformers for binary judgment prediction with FactLegalLlama for explanation generation, creating a robust framework for advancing FJPE in the Indian legal domain. TathyaNyaya not only surpasses existing datasets in scale and diversity but also establishes a benchmark for building explainable AI systems in legal analysis. The findings underscore the importance of factual precision and domain-specific tuning in enhancing predictive performance and interpretability, positioning TathyaNyaya and FactLegalLlama as foundational resources for AI-assisted legal decision-making.\n        △ Less",
        "authors":"['Shubham Kumar Nigam', 'Balaramamahanthi Deepak Patnaik', 'Shivam Mishra', 'Noel Shallum', 'Kripabandhu Ghosh', 'Arnab Bhattacharya']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use",
        "abstract":"Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.\n        △ Less",
        "authors":"['Anna Goldie', 'Azalia Mirhoseini', 'Hao Zhou', 'Irene Cai', 'Christopher D. Manning']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs",
        "abstract":"The proliferation of Large Language Models (LLMs) accessed via black-box APIs introduces a significant trust challenge: users pay for services based on advertised model capabilities (e.g., size, performance), but providers may covertly substitute the specified model with a cheaper, lower-quality alternative to reduce operational costs. This lack of transparency undermines fairness, erodes trust, and complicates reliable benchmarking. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. This paper formalizes the problem of model substitution detection in LLM APIs. We systematically evaluate existing verification techniques, including output-based statistical tests, benchmark evaluations, and log probability analysis, under various realistic attack scenarios like model quantization, randomized substitution, and benchmark evasion. Our findings reveal the limitations of methods relying solely on text outputs, especially against subtle or adaptive attacks. While log probability analysis offers stronger guarantees when available, its accessibility is often limited. We conclude by discussing the potential of hardware-based solutions like Trusted Execution Environments (TEEs) as a pathway towards provable model integrity, highlighting the trade-offs between security, performance, and provider adoption. Code is available at https:\/\/github.com\/sunblaze-ucb\/llm-api-audit\n        △ Less",
        "authors":"['Will Cai', 'Tianneng Shi', 'Xuandong Zhao', 'Dawn Song']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AdvKT: An Adversarial Multi-Step Training Framework for Knowledge Tracing",
        "abstract":"Knowledge Tracing (KT) monitors students' knowledge states and simulates their responses to question sequences. Existing KT models typically follow a single-step training paradigm, which leads to discrepancies with the multi-step inference process required in real-world simulations, resulting in significant error accumulation. This accumulation of error, coupled with the issue of data sparsity, can substantially degrade the performance of recommendation models in the intelligent tutoring systems. To address these challenges, we propose a novel Adversarial Multi-Step Training Framework for Knowledge Tracing (AdvKT), which, for the first time, focuses on the multi-step KT task. More specifically, AdvKT leverages adversarial learning paradigm involving a generator and a discriminator. The generator mimics high-reward responses, effectively reducing error accumulation across multiple steps, while the discriminator provides feedback to generate synthetic data. Additionally, we design specialized data augmentation techniques to enrich the training data with realistic variations, ensuring that the model generalizes well even in scenarios with sparse data. Experiments conducted on four real-world datasets demonstrate the superiority of AdvKT over existing KT models, showcasing its ability to address both error accumulation and data sparsity issues effectively.\n        △ Less",
        "authors":"['Lingyue Fu', 'Ting Long', 'Jianghao Lin', 'Wei Xia', 'Xinyi Dai', 'Ruiming Tang', 'Yasheng Wang', 'Weinan Zhang', 'Yong Yu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important",
        "abstract":"The increasing size of the Key-Value (KV) cache during the Large Language Models long-context inference is the main obstacle for its balance between the deployment cost and task accuracy. To reduce the KV cache size in such scenarios, most previous efforts leveraged on the attention weight to evict non-critical cache tokens. But there is a trade-off in those methods, they usually require major modifiation of the inference infrastructure and significant computation overhead. Base on the fact that the Large Lanuage models are autoregresssive models, we propose {\\it LagKV}, a KV allocation strategy only relying on straight forward comparison among KV themself. It is a totally attention free method which offers easy integration to the main stream inference platform and comparable performance comparing to other complicated KV compression methods. Results on LongBench and PasskeyRetrieval show that, our approach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx 90\\%$ of the original model performance for $8\\times$. Especially in the 64-digit passkey retrieval task, our mehod outperforms the attention weight based method $H_2O$ over $60\\%$ with same compression ratios. Our code is available at \\url{https:\/\/github.com\/AI-Lab-China-Merchants-Bank\/LagKV}.\n        △ Less",
        "authors":"['Manlai Liang', 'JiaMing Zhang', 'Xiong Li', 'Jinlong Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Provable Failure of Language Models in Learning Majority Boolean Logic via Gradient Descent",
        "abstract":"Recent advancements in Transformer-based architectures have led to impressive breakthroughs in natural language processing tasks, with models such as GPT-4, Claude, and Gemini demonstrating human-level reasoning abilities. However, despite their high performance, concerns remain about the inherent limitations of these models, especially when it comes to learning basic logical functions. While complexity-theoretic analyses indicate that Transformers can represent simple logic functions (e.g., $\\mathsf{AND}$, $\\mathsf{OR}$, and majority gates) by its nature of belonging to the $\\mathsf{TC}^0$ class, these results assume ideal parameter settings and do not account for the constraints imposed by gradient descent-based training methods. In this work, we investigate whether Transformers can truly learn simple majority functions when trained using gradient-based methods. We focus on a simplified variant of the Transformer architecture and consider both $n=\\mathrm{poly}(d)$ and $n=\\exp(Ω(d))$ number of training samples, where each sample is a $d$-size binary string paired with the output of a basic majority function. Our analysis demonstrates that even after $\\mathrm{poly}(d)$ gradient queries, the generalization error of the Transformer model still remains substantially large, growing exponentially with $d$. This work highlights fundamental optimization challenges in training Transformers for the simplest logical reasoning tasks and provides new insights into their theoretical limitations.\n        △ Less",
        "authors":"['Bo Chen', 'Zhenmei Shi', 'Zhao Song', 'Jiahao Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Large-Scale Mixed-Traffic and Intersection Control using Multi-agent Reinforcement Learning",
        "abstract":"Traffic congestion remains a significant challenge in modern urban networks. Autonomous driving technologies have emerged as a potential solution. Among traffic control methods, reinforcement learning has shown superior performance over traffic signals in various scenarios. However, prior research has largely focused on small-scale networks or isolated intersections, leaving large-scale mixed traffic control largely unexplored. This study presents the first attempt to use decentralized multi-agent reinforcement learning for large-scale mixed traffic control in which some intersections are managed by traffic signals and others by robot vehicles. Evaluating a real-world network in Colorado Springs, CO, USA with 14 intersections, we measure traffic efficiency via average waiting time of vehicles at intersections and the number of vehicles reaching their destinations within a time window (i.e., throughput). At 80% RV penetration rate, our method reduces waiting time from 6.17 s to 5.09 s and increases throughput from 454 vehicles per 500 seconds to 493 vehicles per 500 seconds, outperforming the baseline of fully signalized intersections. These findings suggest that integrating reinforcement learning-based control large-scale traffic can improve overall efficiency and may inform future urban planning strategies.\n        △ Less",
        "authors":"['Songyang Liu', 'Muyang Fan', 'Weizi Li', 'Jing Du', 'Shuai Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Learning over 5G, WiFi, and Ethernet: Measurements and Evaluation",
        "abstract":"Federated Learning (FL) deployments using IoT devices is an area that is poised to significantly benefit from advances in NextG wireless. In this paper, we deploy a FL application using a 5G-NR Standalone (SA) testbed with open-source and Commercial Off-the-Shelf (COTS) components. The 5G testbed architecture consists of a network of resource-constrained edge devices, namely Raspberry Pi's, and a central server equipped with a Software Defined Radio (SDR) and running O-RAN software. Our testbed allows edge devices to communicate with the server using WiFi and Ethernet, instead of 5G. FL is deployed using the Flower FL framework, for which we developed a comprehensive instrumentation tool to collect and analyze diverse communications and machine learning performance metrics including: model aggregation time, downlink transmission time, training time, and uplink transmission time. Leveraging these measurements, we perform a comparative analysis of the FL application across three network interfaces: 5G, WiFi, and Ethernet. Our experimental results suggest that, on 5G, the uplink model transfer time is a significant factor in convergence time of FL. In particular, we find that the 5G uplink contributes to roughly 23% of the duration of one average communication round when using all edge devices in our testbed. When comparing the uplink time of the 5G testbed, we find that it is 33.3x higher than Ethernet and 17.8x higher than WiFi. Our results also suggest that 5G exacerbates the well-known straggler effect. For reproducibility, we have open-sourced our FL application, instrumentation tools, and testbed configuration.\n        △ Less",
        "authors":"['Robert J. Hayek', 'Joaquin Chung', 'Kayla Comer', 'Chandra R. Murthy', 'Rajkumar Kettimuthu', 'Igor Kadota']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sparsity-Aware Communication for Distributed Graph Neural Network Training",
        "abstract":"Graph Neural Networks (GNNs) are a computationally efficient method to learn embeddings and classifications on graph data. However, GNN training has low computational intensity, making communication costs the bottleneck for scalability. Sparse-matrix dense-matrix multiplication (SpMM) is the core computational operation in full-graph training of GNNs. Previous work parallelizing this operation focused on sparsity-oblivious algorithms, where matrix elements are communicated regardless of the sparsity pattern. This leads to a predictable communication pattern that can be overlapped with computation and enables the use of collective communication operations at the expense of wasting significant bandwidth by communicating unnecessary data. We develop sparsity-aware algorithms that tackle the communication bottlenecks in GNN training with three novel approaches. First, we communicate only the necessary matrix elements. Second, we utilize a graph partitioning model to reorder the matrix and drastically reduce the amount of communicated elements. Finally, we address the high load imbalance in communication with a tailored partitioning model, which minimizes both the total communication volume and the maximum sending volume. We further couple these sparsity-exploiting approaches with a communication-avoiding approach (1.5D parallel SpMM) in which submatrices are replicated to reduce communication. We explore the tradeoffs of these combined optimizations and show up to 14X improvement on 256 GPUs and on some instances reducing communication to almost zero resulting in a communication-free parallel training relative to a popular GNN framework based on communication-oblivious SpMM.\n        △ Less",
        "authors":"['Ujjaini Mukhodopadhyay', 'Alok Tripathy', 'Oguz Selvitopi', 'Katherine Yelick', 'Aydin Buluc']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scaling Graph Neural Networks for Particle Track Reconstruction",
        "abstract":"Particle track reconstruction is an important problem in high-energy physics (HEP), necessary to study properties of subatomic particles. Traditional track reconstruction algorithms scale poorly with the number of particles within the accelerator. The Exa.TrkX project, to alleviate this computational burden, introduces a pipeline that reduces particle track reconstruction to edge classification on a graph, and uses graph neural networks (GNNs) to produce particle tracks. However, this GNN-based approach is memory-prohibitive and skips graphs that would exceed GPU memory. We introduce improvements to the Exa.TrkX pipeline to train on samples of input particle graphs, and show that these improvements generalize to higher precision and recall. In addition, we adapt performance optimizations, introduced for GNN training, to fit our augmented Exa.TrkX pipeline. These optimizations provide a $2\\times$ speedup over our baseline implementation in PyTorch Geometric.\n        △ Less",
        "authors":"['Alok Tripathy', 'Alina Lazar', 'Xiangyang Ju', 'Paolo Calafiura', 'Katherine Yelick', 'Aydin Buluc']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"asKAN: Active Subspace embedded Kolmogorov-Arnold Network",
        "abstract":"The Kolmogorov-Arnold Network (KAN) has emerged as a promising neural network architecture for small-scale AI+Science applications. However, it suffers from inflexibility in modeling ridge functions, which is widely used in representing the relationships in physical systems. This study investigates this inflexibility through the lens of the Kolmogorov-Arnold theorem, which starts the representation of multivariate functions from constructing the univariate components rather than combining the independent variables. Our analysis reveals that incorporating linear combinations of independent variables can substantially simplify the network architecture in representing the ridge functions. Inspired by this finding, we propose active subspace embedded KAN (asKAN), a hierarchical framework that synergizes KAN's function representation with active subspace methodology. The architecture strategically embeds active subspace detection between KANs, where the active subspace method is used to identify the primary ridge directions and the independent variables are adaptively projected onto these critical dimensions. The proposed asKAN is implemented in an iterative way without increasing the number of neurons in the original KAN. The proposed method is validated through function fitting, solving the Poisson equation, and reconstructing sound field. Compared with KAN, asKAN significantly reduces the error using the same network architecture. The results suggest that asKAN enhances the capability of KAN in fitting and solving equations in the form of ridge functions.\n        △ Less",
        "authors":"['Zhiteng Zhou', 'Zhaoyue Xu', 'Yi Liu', 'Shizhao Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Interval-Valued Time Series Classification Using $D_K$-Distance",
        "abstract":"In recent years, modeling and analysis of interval-valued time series have garnered increasing attention in econometrics, finance, and statistics. However, these studies have predominantly focused on statistical inference in the forecasting of univariate and multivariate interval-valued time series, overlooking another important aspect: classification. In this paper, we introduce a classification approach that treats intervals as unified entities, applicable to both univariate and multivariate interval-valued time series. Specifically, we first extend the point-valued time series imaging methods to interval-valued scenarios using the $D_K$-distance, enabling the imaging of interval-valued time series. Then, we employ suitable deep learning model for classification on the obtained imaging dataset, aiming to achieve classification for interval-valued time series. In theory, we derived a sharper excess risk bound for deep multiclassifiers based on offset Rademacher complexity. Finally, we validate the superiority of the proposed method through comparisons with various existing point-valued time series classification methods in both simulation studies and real data applications.\n        △ Less",
        "authors":"['Wan Tian', 'Zhongfeng Qin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Simultaneous Approach for Training Neural Differential-Algebraic Systems of Equations",
        "abstract":"Scientific machine learning is an emerging field that broadly describes the combination of scientific computing and machine learning to address challenges in science and engineering. Within the context of differential equations, this has produced highly influential methods, such as neural ordinary differential equations (NODEs). Recent works extend this line of research to consider neural differential-algebraic systems of equations (DAEs), where some unknown relationships within the DAE are learned from data. Training neural DAEs, similarly to neural ODEs, is computationally expensive, as it requires the solution of a DAE for every parameter update. Further, the rigorous consideration of algebraic constraints is difficult within common deep learning training algorithms such as stochastic gradient descent. In this work, we apply the simultaneous approach to neural DAE problems, resulting in a fully discretized nonlinear optimization problem, which is solved to local optimality and simultaneously obtains the neural network parameters and the solution to the corresponding DAE. We extend recent work demonstrating the simultaneous approach for neural ODEs, by presenting a general framework to solve neural DAEs, with explicit consideration of hybrid models, where some components of the DAE are known, e.g. physics-informed constraints. Furthermore, we present a general strategy for improving the performance and convergence of the nonlinear programming solver, based on solving an auxiliary problem for initialization and approximating Hessian terms. We achieve promising results in terms of accuracy, model generalizability and computational cost, across different problem settings such as sparse data, unobserved states and multiple trajectories. Lastly, we provide several promising future directions to improve the scalability and robustness of our approach.\n        △ Less",
        "authors":"['Laurens R. Lueg', 'Victor Alves', 'Daniel Schicksnus', 'John R. Kitchin', 'Carl D. Laird', 'Lorenz T. Biegler']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Classification of ADHD and Healthy Children Using EEG Based Multi-Band Spatial Features Enhancement",
        "abstract":"Attention Deficit Hyperactivity Disorder (ADHD) is a common neurodevelopmental disorder in children, characterized by difficulties in attention, hyperactivity, and impulsivity. Early and accurate diagnosis of ADHD is critical for effective intervention and management. Electroencephalogram (EEG) signals have emerged as a non-invasive and efficient tool for ADHD detection due to their high temporal resolution and ability to capture neural dynamics. In this study, we propose a method for classifying ADHD and healthy children using EEG data from the benchmark dataset. There were 61 children with ADHD and 60 healthy children, both boys and girls, aged 7 to 12. The EEG signals, recorded from 19 channels, were processed to extract Power Spectral Density (PSD) and Spectral Entropy (SE) features across five frequency bands, resulting in a comprehensive 190-dimensional feature set. To evaluate the classification performance, a Support Vector Machine (SVM) with the RBF kernel demonstrated the best performance with a mean cross-validation accuracy of 99.2\\% and a standard deviation of 0.0079, indicating high robustness and precision. These results highlight the potential of spatial features in conjunction with machine learning for accurately classifying ADHD using EEG data. This work contributes to developing non-invasive, data-driven tools for early diagnosis and assessment of ADHD in children.\n        △ Less",
        "authors":"['Md Bayazid Hossain', 'Md Anwarul Islam Himel', 'Md Abdur Rahim', 'Shabbir Mahmood', 'Abu Saleh Musa Miah', 'Jungpil Shin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"N-TORC: Native Tensor Optimizer for Real-time Constraints",
        "abstract":"Compared to overlay-based tensor architectures like VTA or Gemmini, compilers that directly translate machine learning models into a dataflow architecture as HLS code, such as HLS4ML and FINN, generally can achieve lower latency by generating customized matrix-vector multipliers and memory structures tailored to the specific fundamental tensor operations required by each layer. However, this approach has significant drawbacks: the compilation process is highly time-consuming and the resulting deployments have unpredictable area and latency, making it impractical to constrain the latency while simultaneously minimizing area. Currently, no existing methods address this type of optimization. In this paper, we present N-TORC (Native Tensor Optimizer for Real-Time Constraints), a novel approach that utilizes data-driven performance and resource models to optimize individual layers of a dataflow architecture. When combined with model hyperparameter optimization, N-TORC can quickly generate architectures that satisfy latency constraints while simultaneously optimizing for both accuracy and resource cost (i.e. offering a set of optimal trade-offs between cost and accuracy). To demonstrate its effectiveness, we applied this framework to a cyber-physical application, DROPBEAR (Dynamic Reproduction of Projectiles in Ballistic Environments for Advanced Research). N-TORC's HLS4ML performance and resource models achieve higher accuracy than prior efforts, and its Mixed Integer Program (MIP)-based solver generates equivalent solutions to a stochastic search in 1000X less time.\n        △ Less",
        "authors":"['Suyash Vardhan Singh', 'Iftakhar Ahmad', 'David Andrews', 'Miaoqing Huang', 'Austin R. J. Downey', 'Jason D. Bakos']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ACE-RLHF: Automated Code Evaluation and Socratic Feedback Generation Tool using Large Language Models and Reinforcement Learning with Human Feedback",
        "abstract":"Automated Program Repair tools are developed for generating feedback and suggesting a repair method for erroneous code. State of the art (SOTA) code repair methods rely on data-driven approaches and often fail to deliver solution for complicated programming questions. To interpret the natural language of unprecedented programming problems, using Large Language Models (LLMs) for code-feedback generation is crucial. LLMs generate more comprehensible feedback than compiler-generated error messages, and Reinforcement Learning with Human Feedback (RLHF) further enhances quality by integrating human-in-the-loop which helps novice students to lean programming from scratch interactively. We are applying RLHF fine-tuning technique for an expected Socratic response such as a question with hint to solve the programming issue. We are proposing code feedback generation tool by fine-tuning LLM with RLHF, Automated Code Evaluation with RLHF (ACE-RLHF), combining two open-source LLM models with two different SOTA optimization techniques. The quality of feedback is evaluated on two benchmark datasets containing basic and competition-level programming questions where the later is proposed by us. We achieved 2-5% higher accuracy than RL-free SOTA techniques using Llama-3-7B-Proximal-policy optimization in automated evaluation and similar or slightly higher accuracy compared to reward model-free RL with AI Feedback (RLAIF). We achieved almost 40% higher accuracy with GPT-3.5 Best-of-n optimization while performing manual evaluation.\n        △ Less",
        "authors":"['Tasnia Rahman', 'Sathish A. P. Kumar', 'Sumit Jha', 'Arvind Ramanathan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EquiCPI: SE(3)-Equivariant Geometric Deep Learning for Structure-Aware Prediction of Compound-Protein Interactions",
        "abstract":"Accurate prediction of compound-protein interactions (CPI) remains a cornerstone challenge in computational drug discovery. While existing sequence-based approaches leverage molecular fingerprints or graph representations, they critically overlook three-dimensional (3D) structural determinants of binding affinity. To bridge this gap, we present EquiCPI, an end-to-end geometric deep learning framework that synergizes first-principles structural modeling with SE(3)-equivariant neural networks. Our pipeline transforms raw sequences into 3D atomic coordinates via ESMFold for proteins and DiffDock-L for ligands, followed by physics-guided conformer re-ranking and equivariant feature learning. At its core, EquiCPI employs SE(3)-equivariant message passing over atomic point clouds, preserving symmetry under rotations, translations, and reflections, while hierarchically encoding local interaction patterns through tensor products of spherical harmonics. The proposed model is evaluated on BindingDB (affinity prediction) and DUD-E (virtual screening), EquiCPI achieves performance on par with or exceeding the state-of-the-art deep learning competitors.\n        △ Less",
        "authors":"['Ngoc-Quang Nguyen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sub-Clustering for Class Distance Recalculation in Long-Tailed Drug Classification",
        "abstract":"In the real world, long-tailed data distributions are prevalent, making it challenging for models to effectively learn and classify tail classes. However, we discover that in the field of drug chemistry, certain tail classes exhibit higher identifiability during training due to their unique molecular structural features, a finding that significantly contrasts with the conventional understanding that tail classes are generally difficult to identify. Existing imbalance learning methods, such as resampling and cost-sensitive reweighting, overly rely on sample quantity priors, causing models to excessively focus on tail classes at the expense of head class performance. To address this issue, we propose a novel method that breaks away from the traditional static evaluation paradigm based on sample size. Instead, we establish a dynamical inter-class separability metric using feature distances between different classes. Specifically, we employ a sub-clustering contrastive learning approach to thoroughly learn the embedding features of each class, and we dynamically compute the distances between class embeddings to capture the relative positional evolution of samples from different classes in the feature space, thereby rebalancing the weights of the classification loss function. We conducted experiments on multiple existing long-tailed drug datasets and achieved competitive results by improving the accuracy of tail classes without compromising the performance of dominant classes.\n        △ Less",
        "authors":"['Yujia Su', 'Xinjie Li', 'Lionel Z. Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Novel Algorithm for Personalized Federated Learning: Knowledge Distillation with Weighted Combination Loss",
        "abstract":"Federated learning (FL) offers a privacy-preserving framework for distributed machine learning, enabling collaborative model training across diverse clients without centralizing sensitive data. However, statistical heterogeneity, characterized by non-independent and identically distributed (non-IID) client data, poses significant challenges, leading to model drift and poor generalization. This paper proposes a novel algorithm, pFedKD-WCL (Personalized Federated Knowledge Distillation with Weighted Combination Loss), which integrates knowledge distillation with bi-level optimization to address non-IID challenges. pFedKD-WCL leverages the current global model as a teacher to guide local models, optimizing both global convergence and local personalization efficiently. We evaluate pFedKD-WCL on the MNIST dataset and a synthetic dataset with non-IID partitioning, using multinomial logistic regression and multilayer perceptron models. Experimental results demonstrate that pFedKD-WCL outperforms state-of-the-art algorithms, including FedAvg, FedProx, Per-FedAvg, and pFedMe, in terms of accuracy and convergence speed.\n        △ Less",
        "authors":"['Hengrui Hu', 'Anai N. Kothari', 'Anjishnu Banerjee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DanceMosaic: High-Fidelity Dance Generation with Multimodal Editability",
        "abstract":"Recent advances in dance generation have enabled automatic synthesis of 3D dance motions. However, existing methods still struggle to produce high-fidelity dance sequences that simultaneously deliver exceptional realism, precise dance-music synchronization, high motion diversity, and physical plausibility. Moreover, existing methods lack the flexibility to edit dance sequences according to diverse guidance signals, such as musical prompts, pose constraints, action labels, and genre descriptions, significantly restricting their creative utility and adaptability. Unlike the existing approaches, DanceMosaic enables fast and high-fidelity dance generation, while allowing multimodal motion editing. Specifically, we propose a multimodal masked motion model that fuses the text-to-motion model with music and pose adapters to learn probabilistic mapping from diverse guidance signals to high-quality dance motion sequences via progressive generative masking training. To further enhance the motion generation quality, we propose multimodal classifier-free guidance and inference-time optimization mechanism that further enforce the alignment between the generated motions and the multimodal guidance. Extensive experiments demonstrate that our method establishes a new state-of-the-art performance in dance generation, significantly advancing the quality and editability achieved by existing approaches.\n        △ Less",
        "authors":"['Foram Niravbhai Shah', 'Parshwa Shah', 'Muhammad Usama Saleem', 'Ekkasit Pinyoanuntapong', 'Pu Wang', 'Hongfei Xue', 'Ahmed Helmy']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exact Unlearning of Finetuning Data via Model Merging at Scale",
        "abstract":"Approximate unlearning has gained popularity as an approach to efficiently update an LLM so that it behaves (roughly) as if it was not trained on a subset of data to begin with. However, existing methods are brittle in practice and can easily be attacked to reveal supposedly unlearned information. To alleviate issues with approximate unlearning, we instead propose SIFT-Masks (SIgn-Fixed Tuning-Masks), an exact unlearning method based on model merging. SIFT-Masks addresses two key limitations of standard model merging: (1) merging a large number of tasks can severely harm utility; and (2) methods that boost utility by sharing extra information across tasks make exact unlearning prohibitively expensive. SIFT-Masks solves these issues by (1) applying local masks to recover task-specific performance; and (2) constraining finetuning to align with a global sign vector as a lightweight approach to determine masks independently before merging. Across four settings where we merge up to 500 models, SIFT-Masks improves accuracy by 5-80% over naive merging and uses up to 250x less compute for exact unlearning compared to other merging baselines.\n        △ Less",
        "authors":"['Kevin Kuo', 'Amrith Setlur', 'Kartik Srinivas', 'Aditi Raghunathan', 'Virginia Smith']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SiameseDuo++: Active Learning from Data Streams with Dual Augmented Siamese Networks",
        "abstract":"Data stream mining, also known as stream learning, is a growing area which deals with learning from high-speed arriving data. Its relevance has surged recently due to its wide range of applicability, such as, critical infrastructure monitoring, social media analysis, and recommender systems. The design of stream learning methods faces significant research challenges; from the nonstationary nature of the data (referred to as concept drift) and the fact that data streams are typically not annotated with the ground truth, to the requirement that such methods should process large amounts of data in real-time with limited memory. This work proposes the SiameseDuo++ method, which uses active learning to automatically select instances for a human expert to label according to a budget. Specifically, it incrementally trains two siamese neural networks which operate in synergy, augmented by generated examples. Both the proposed active learning strategy and augmentation operate in the latent space. SiameseDuo++ addresses the aforementioned challenges by operating with limited memory and limited labelling budget. Simulation experiments show that the proposed method outperforms strong baselines and state-of-the-art methods in terms of learning speed and\/or performance. To promote open science we publicly release our code and datasets.\n        △ Less",
        "authors":"['Kleanthis Malialis', 'Stylianos Filippou', 'Christos G. Panayiotou', 'Marios M. Polycarpou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Tool-as-Interface: Learning Robot Policies from Human Tool Usage through Imitation Learning",
        "abstract":"Tool use is critical for enabling robots to perform complex real-world tasks, and leveraging human tool-use data can be instrumental for teaching robots. However, existing data collection methods like teleoperation are slow, prone to control delays, and unsuitable for dynamic tasks. In contrast, human natural data, where humans directly perform tasks with tools, offers natural, unstructured interactions that are both efficient and easy to collect. Building on the insight that humans and robots can share the same tools, we propose a framework to transfer tool-use knowledge from human data to robots. Using two RGB cameras, our method generates 3D reconstruction, applies Gaussian splatting for novel view augmentation, employs segmentation models to extract embodiment-agnostic observations, and leverages task-space tool-action representations to train visuomotor policies. We validate our approach on diverse real-world tasks, including meatball scooping, pan flipping, wine bottle balancing, and other complex tasks. Our method achieves a 71\\% higher average success rate compared to diffusion policies trained with teleoperation data and reduces data collection time by 77\\%, with some tasks solvable only by our framework. Compared to hand-held gripper, our method cuts data collection time by 41\\%. Additionally, our method bridges the embodiment gap, improves robustness to variations in camera viewpoints and robot configurations, and generalizes effectively across objects and spatial setups.\n        △ Less",
        "authors":"['Haonan Chen', 'Cheng Zhu', 'Yunzhu Li', 'Katherine Driggs-Campbell']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scalable Approximate Algorithms for Optimal Transport Linear Models",
        "abstract":"Recently, linear regression models incorporating an optimal transport (OT) loss have been explored for applications such as supervised unmixing of spectra, music transcription, and mass spectrometry. However, these task-specific approaches often do not generalize readily to a broader class of linear models. In this work, we propose a novel algorithmic framework for solving a general class of non-negative linear regression models with an entropy-regularized OT datafit term, based on Sinkhorn-like scaling iterations. Our framework accommodates convex penalty functions on the weights (e.g. squared-$\\ell_2$ and $\\ell_1$ norms), and admits additional convex loss terms between the transported marginal and target distribution (e.g. squared error or total variation). We derive simple multiplicative updates for common penalty and datafit terms. This method is suitable for large-scale problems due to its simplicity of implementation and straightforward parallelization.\n        △ Less",
        "authors":"['Tomasz Kacprzak', 'Francois Kamper', 'Michael W. Heiss', 'Gianluca Janka', 'Ann M. Dillner', 'Satoshi Takahama']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Modeling of AUV Dynamics with Limited Resources: Efficient Online Learning Using Uncertainty",
        "abstract":"Machine learning proves effective in constructing dynamics models from data, especially for underwater vehicles. Continuous refinement of these models using incoming data streams, however, often requires storage of an overwhelming amount of redundant data. This work investigates the use of uncertainty in the selection of data points to rehearse in online learning when storage capacity is constrained. The models are learned using an ensemble of multilayer perceptrons as they perform well at predicting epistemic uncertainty. We present three novel approaches: the Threshold method, which excludes samples with uncertainty below a specified threshold, the Greedy method, designed to maximize uncertainty among the stored points, and Threshold-Greedy, which combines the previous two approaches. The methods are assessed on data collected by an underwater vehicle Dagon. Comparison with baselines reveals that the Threshold exhibits enhanced stability throughout the learning process and also yields a model with the least cumulative testing loss. We also conducted detailed analyses on the impact of model parameters and storage size on the performance of the models, as well as a comparison of three different uncertainty estimation methods.\n        △ Less",
        "authors":"['Michal Tešnar', 'Bilal Wehbe', 'Matias Valdenegro-Toro']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Your Image Generator Is Your New Private Dataset",
        "abstract":"Generative diffusion models have emerged as powerful tools to synthetically produce training data, offering potential solutions to data scarcity and reducing labelling costs for downstream supervised deep learning applications. However, effectively leveraging text-conditioned image generation for building classifier training sets requires addressing key issues: constructing informative textual prompts, adapting generative models to specific domains, and ensuring robust performance. This paper proposes the Text-Conditioned Knowledge Recycling (TCKR) pipeline to tackle these challenges. TCKR combines dynamic image captioning, parameter-efficient diffusion model fine-tuning, and Generative Knowledge Distillation techniques to create synthetic datasets tailored for image classification. The pipeline is rigorously evaluated on ten diverse image classification benchmarks. The results demonstrate that models trained solely on TCKR-generated data achieve classification accuracies on par with (and in several cases exceeding) models trained on real images. Furthermore, the evaluation reveals that these synthetic-data-trained models exhibit substantially enhanced privacy characteristics: their vulnerability to Membership Inference Attacks is significantly reduced, with the membership inference AUC lowered by 5.49 points on average compared to using real training data, demonstrating a substantial improvement in the performance-privacy trade-off. These findings indicate that high-fidelity synthetic data can effectively replace real data for training classifiers, yielding strong performance whilst simultaneously providing improved privacy protection as a valuable emergent property. The code and trained models are available in the accompanying open-source repository.\n        △ Less",
        "authors":"['Nicolo Resmini', 'Eugenio Lomurno', 'Cristian Sbrolli', 'Matteo Matteucci']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Better Rates for Random Task Orderings in Continual Linear Models",
        "abstract":"We study the common continual learning setup where an overparameterized model is sequentially fitted to a set of jointly realizable tasks. We analyze the forgetting, i.e., loss on previously seen tasks, after $k$ iterations. For linear models, we prove that fitting a task is equivalent to a single stochastic gradient descent (SGD) step on a modified objective. We develop novel last-iterate SGD upper bounds in the realizable least squares setup, and apply them to derive new results for continual learning. Focusing on random orderings over $T$ tasks, we establish universal forgetting rates, whereas existing rates depend on the problem dimensionality or complexity. Specifically, in continual regression with replacement, we improve the best existing rate from $O((d-r)\/k)$ to $O(\\min(k^{-1\/4}, \\sqrt{d-r}\/k, \\sqrt{Tr}\/k))$, where $d$ is the dimensionality and $r$ the average task rank. Furthermore, we establish the first rates for random task orderings without replacement. The obtained rate of $O(\\min(T^{-1\/4}, (d-r)\/T))$ proves for the first time that randomization alone, with no task repetition, can prevent catastrophic forgetting in sufficiently long task sequences. Finally, we prove a similar $O(k^{-1\/4})$ universal rate for the forgetting in continual linear classification on separable data. Our universal rates apply for broader projection methods, such as block Kaczmarz and POCS, illuminating their loss convergence under i.i.d and one-pass orderings.\n        △ Less",
        "authors":"['Itay Evron', 'Ran Levinstein', 'Matan Schliserman', 'Uri Sherman', 'Tomer Koren', 'Daniel Soudry', 'Nathan Srebro']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hierarchical Planning for Complex Tasks with Knowledge Graph-RAG and Symbolic Verification",
        "abstract":"Large Language Models (LLMs) have shown promise as robotic planners but often struggle with long-horizon and complex tasks, especially in specialized environments requiring external knowledge. While hierarchical planning and Retrieval-Augmented Generation (RAG) address some of these challenges, they remain insufficient on their own and a deeper integration is required for achieving more reliable systems. To this end, we propose a neuro-symbolic approach that enhances LLMs-based planners with Knowledge Graph-based RAG for hierarchical plan generation. This method decomposes complex tasks into manageable subtasks, further expanded into executable atomic action sequences. To ensure formal correctness and proper decomposition, we integrate a Symbolic Validator, which also functions as a failure detector by aligning expected and observed world states. Our evaluation against baseline methods demonstrates the consistent significant advantages of integrating hierarchical planning, symbolic verification, and RAG across tasks of varying complexity and different LLMs. Additionally, our experimental setup and novel metrics not only validate our approach for complex planning but also serve as a tool for assessing LLMs' reasoning and compositional capabilities.\n        △ Less",
        "authors":"['Cristina Cornelio', 'Flavio Petruzzellis', 'Pietro Lio']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cramer-Rao Bounds for Laplacian Matrix Estimation",
        "abstract":"In this paper, we analyze the performance of the estimation of Laplacian matrices under general observation models. Laplacian matrix estimation involves structural constraints, including symmetry and null-space properties, along with matrix sparsity. By exploiting a linear reparametrization that enforces the structural constraints, we derive closed-form matrix expressions for the Cramer-Rao Bound (CRB) specifically tailored to Laplacian matrix estimation. We further extend the derivation to the sparsity-constrained case, introducing two oracle CRBs that incorporate prior information of the support set, i.e. the locations of the nonzero entries in the Laplacian matrix. We examine the properties and order relations between the bounds, and provide the associated Slepian-Bangs formula for the Gaussian case. We demonstrate the use of the new CRBs in three representative applications: (i) topology identification in power systems, (ii) graph filter identification in diffused models, and (iii) precision matrix estimation in Gaussian Markov random fields under Laplacian constraints. The CRBs are evaluated and compared with the mean-squared-errors (MSEs) of the constrained maximum likelihood estimator (CMLE), which integrates both equality and inequality constraints along with sparsity constraints, and of the oracle CMLE, which knows the locations of the nonzero entries of the Laplacian matrix. We perform this analysis for the applications of power system topology identification and graphical LASSO, and demonstrate that the MSEs of the estimators converge to the CRB and oracle CRB, given a sufficient number of measurements.\n        △ Less",
        "authors":"['Morad Halihal', 'Tirza Routtenberg', 'H. Vincent Poor']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Advancing Egocentric Video Question Answering with Multimodal Large Language Models",
        "abstract":"Egocentric Video Question Answering (QA) requires models to handle long-horizon temporal reasoning, first-person perspectives, and specialized challenges like frequent camera movement. This paper systematically evaluates both proprietary and open-source Multimodal Large Language Models (MLLMs) on QaEgo4Dv2 - a refined dataset of egocentric videos derived from QaEgo4D. Four popular MLLMs (GPT-4o, Gemini-1.5-Pro, Video-LLaVa-7B and Qwen2-VL-7B-Instruct) are assessed using zero-shot and fine-tuned approaches for both OpenQA and CloseQA settings. We introduce QaEgo4Dv2 to mitigate annotation noise in QaEgo4D, enabling more reliable comparison. Our results show that fine-tuned Video-LLaVa-7B and Qwen2-VL-7B-Instruct achieve new state-of-the-art performance, surpassing previous benchmarks by up to +2.6% ROUGE\/METEOR (for OpenQA) and +13% accuracy (for CloseQA). We also present a thorough error analysis, indicating the model's difficulty in spatial reasoning and fine-grained object recognition - key areas for future improvement.\n        △ Less",
        "authors":"['Alkesh Patel', 'Vibhav Chitalia', 'Yinfei Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A model agnostic eXplainable AI based fuzzy framework for sensor constrained Aerospace maintenance applications",
        "abstract":"Machine Learning methods have extensively evolved to support industrial big data methods and their corresponding need in gas turbine maintenance and prognostics. However, most unsupervised methods need extensively labeled data to perform predictions across many dimensions. The cutting edge of small and medium applications do not necessarily maintain operational sensors and data acquisition with rising costs and diminishing profits. We propose a framework to make sensor maintenance priority decisions using a combination of SHAP, UMAP, Fuzzy C-means clustering. An aerospace jet engine dataset is used as a case study.\n        △ Less",
        "authors":"['Bharadwaj Dogga', 'Anoop Sathyan', 'Kelly Cohen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Empirical Comparison of Text Summarization: A Multi-Dimensional Evaluation of Large Language Models",
        "abstract":"Text summarization is crucial for mitigating information overload across domains like journalism, medicine, and business. This research evaluates summarization performance across 17 large language models (OpenAI, Google, Anthropic, open-source) using a novel multi-dimensional framework. We assessed models on seven diverse datasets (BigPatent, BillSum, CNN\/DailyMail, PubMed, SAMSum, WikiHow, XSum) at three output lengths (50, 100, 150 tokens) using metrics for factual consistency, semantic similarity, lexical overlap, and human-like quality, while also considering efficiency factors. Our findings reveal significant performance differences, with specific models excelling in factual accuracy (deepseek-v3), human-like quality (claude-3-5-sonnet), and processing efficiency\/cost-effectiveness (gemini-1.5-flash, gemini-2.0-flash). Performance varies dramatically by dataset, with models struggling on technical domains but performing well on conversational content. We identified a critical tension between factual consistency (best at 50 tokens) and perceived quality (best at 150 tokens). Our analysis provides evidence-based recommendations for different use cases, from high-stakes applications requiring factual accuracy to resource-constrained environments needing efficient processing. This comprehensive approach enhances evaluation methodology by integrating quality metrics with operational considerations, incorporating trade-offs between accuracy, efficiency, and cost-effectiveness to guide model selection for specific applications.\n        △ Less",
        "authors":"['Anantharaman Janakiraman', 'Behnaz Ghoraani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Consequentialist Critique of Binary Classification Evaluation Practices",
        "abstract":"ML-supported decisions, such as ordering tests or determining preventive custody, often involve binary classification based on probabilistic forecasts. Evaluation frameworks for such forecasts typically consider whether to prioritize independent-decision metrics (e.g., Accuracy) or top-K metrics (e.g., Precision@K), and whether to focus on fixed thresholds or threshold-agnostic measures like AUC-ROC. We highlight that a consequentialist perspective, long advocated by decision theorists, should naturally favor evaluations that support independent decisions using a mixture of thresholds given their prevalence, such as Brier scores and Log loss. However, our empirical analysis reveals a strong preference for top-K metrics or fixed thresholds in evaluations at major conferences like ICML, FAccT, and CHIL. To address this gap, we use this decision-theoretic framework to map evaluation metrics to their optimal use cases, along with a Python package, briertools, to promote the broader adoption of Brier scores. In doing so, we also uncover new theoretical connections, including a reconciliation between the Brier Score and Decision Curve Analysis, which clarifies and responds to a longstanding critique by (Assel, et al. 2017) regarding the clinical utility of proper scoring rules.\n        △ Less",
        "authors":"['Gerardo Flores', 'Abigail Schiff', 'Alyssa H. Smith', 'Julia A Fukuyama', 'Ashia C. Wilson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning",
        "abstract":"Recently, Large Language Models (LLMs) have rapidly evolved, approaching Artificial General Intelligence (AGI) while benefiting from large-scale reinforcement learning to enhance Human Alignment (HA) and Reasoning. Recent reward-based optimization algorithms, such as Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO) have achieved significant performance on reasoning tasks, whereas preference-based optimization algorithms such as Direct Preference Optimization (DPO) significantly improve the performance of LLMs on human alignment. However, despite the strong performance of reward-based optimization methods in alignment tasks , they remain vulnerable to reward hacking. Furthermore, preference-based algorithms (such as Online DPO) haven't yet matched the performance of reward-based optimization algorithms (like PPO) on reasoning tasks, making their exploration in this specific area still a worthwhile pursuit. Motivated by these challenges, we propose the Trust Region Preference Approximation (TRPA) algorithm, which integrates rule-based optimization with preference-based optimization for reasoning tasks. As a preference-based algorithm, TRPA naturally eliminates the reward hacking issue. TRPA constructs preference levels using predefined rules, forms corresponding preference pairs, and leverages a novel optimization algorithm for RL training with a theoretical monotonic improvement guarantee. Experimental results demonstrate that TRPA not only achieves competitive performance on reasoning tasks but also exhibits robust stability. The code of this paper are released and updating on https:\/\/github.com\/XueruiSu\/Trust-Region-Preference-Approximation.git.\n        △ Less",
        "authors":"['Xuerui Su', 'Shufang Xie', 'Guoqing Liu', 'Yingce Xia', 'Renqian Luo', 'Peiran Jin', 'Zhiming Ma', 'Yue Wang', 'Zun Wang', 'Yuting Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hessian of Perplexity for Large Language Models by PyTorch autograd (Open Source)",
        "abstract":"Computing the full Hessian matrix -- the matrix of second-order derivatives for an entire Large Language Model (LLM) is infeasible due to its sheer size. In this technical report, we aim to provide a comprehensive guide on how to accurately compute at least a small portion of the Hessian for LLMs using PyTorch autograd library. We also demonstrate how to compute the full diagonal of the Hessian matrix using multiple samples of vector-Hessian Products (HVPs). We hope that both this guide and the accompanying GitHub code will be valuable resources for practitioners and researchers interested in better understanding the behavior and structure of the Hessian in LLMs.\n        △ Less",
        "authors":"['Ivan Ilin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Active Learning with a Noisy Annotator",
        "abstract":"Active Learning (AL) aims to reduce annotation costs by strategically selecting the most informative samples for labeling. However, most active learning methods struggle in the low-budget regime where only a few labeled examples are available. This issue becomes even more pronounced when annotators provide noisy labels. A common AL approach for the low- and mid-budget regimes focuses on maximizing the coverage of the labeled set across the entire dataset. We propose a novel framework called Noise-Aware Active Sampling (NAS) that extends existing greedy, coverage-based active learning strategies to handle noisy annotations. NAS identifies regions that remain uncovered due to the selection of noisy representatives and enables resampling from these areas. We introduce a simple yet effective noise filtering approach suitable for the low-budget regime, which leverages the inner mechanism of NAS and can be applied for noise filtering before model training. On multiple computer vision benchmarks, including CIFAR100 and ImageNet subsets, NAS significantly improves performance for standard active learning methods across different noise types and rates.\n        △ Less",
        "authors":"['Netta Shafir', 'Guy Hacohen', 'Daphna Weinshall']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Classification View on Meta Learning Bandits",
        "abstract":"Contextual multi-armed bandits are a popular choice to model sequential decision-making. E.g., in a healthcare application we may perform various tests to asses a patient condition (exploration) and then decide on the best treatment to give (exploitation). When humans design strategies, they aim for the exploration to be fast, since the patient's health is at stake, and easy to interpret for a physician overseeing the process. However, common bandit algorithms are nothing like that: The regret caused by exploration scales with $\\sqrt{H}$ over $H$ rounds and decision strategies are based on opaque statistical considerations. In this paper, we use an original classification view to meta learn interpretable and fast exploration plans for a fixed collection of bandits $\\mathbb{M}$. The plan is prescribed by an interpretable decision tree probing decisions' payoff to classify the test bandit. The test regret of the plan in the stochastic and contextual setting scales with $O (λ^{-2} C_λ (\\mathbb{M}) \\log^2 (MH))$, being $M$ the size of $\\mathbb{M}$, $λ$ a separation parameter over the bandits, and $C_λ(\\mathbb{M})$ a novel classification-coefficient that fundamentally links meta learning bandits with classification. Through a nearly matching lower bound, we show that $C_λ(\\mathbb{M})$ inherently captures the complexity of the setting.\n        △ Less",
        "authors":"['Mirco Mutti', 'Jeongyeol Kwon', 'Shie Mannor', 'Aviv Tamar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Directed Graph-alignment Approach for Identification of Gaps in Short Answers",
        "abstract":"In this paper, we have presented a method for identifying missing items known as gaps in the student answers by comparing them against the corresponding model answer\/reference answers, automatically. The gaps can be identified at word, phrase or sentence level. The identified gaps are useful in providing feedback to the students for formative assessment. The problem of gap identification has been modelled as an alignment of a pair of directed graphs representing a student answer and the corresponding model answer for a given question. To validate the proposed approach, the gap annotated student answers considering answers from three widely known datasets in the short answer grading domain, namely, University of North Texas (UNT), SciEntsBank, and Beetle have been developed and this gap annotated student answers' dataset is available at: https:\/\/github.com\/sahuarchana7\/gaps-answers-dataset. Evaluation metrics used in the traditional machine learning tasks have been adopted to evaluate the task of gap identification. Though performance of the proposed approach varies across the datasets and the types of the answers, overall the performance is observed to be promising.\n        △ Less",
        "authors":"['Archana Sahu', 'Plaban Kumar Bhowmick']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI2STOW: End-to-End Deep Reinforcement Learning to Construct Master Stowage Plans under Demand Uncertainty",
        "abstract":"The worldwide economy and environmental sustainability depend on eff icient and reliable supply chains, in which container shipping plays a crucial role as an environmentally friendly mode of transport. Liner shipping companies seek to improve operational efficiency by solving the stowage planning problem. Due to many complex combinatorial aspects, stowage planning is challenging and often decomposed into two NP-hard subproblems: master and slot planning. This article proposes AI2STOW, an end-to-end deep reinforcement learning model with feasibility projection and an action mask to create master plans under demand uncertainty with global objectives and constraints, including paired block stowage patterms. Our experimental results demonstrate that AI2STOW outperforms baseline methods from reinforcement learning and stochastic programming in objective performance and computational efficiency, based on simulated instances reflecting the scale of realistic vessels and operational planning horizons.\n        △ Less",
        "authors":"['Jaike Van Twiller', 'Djordje Grbic', 'Rune Møller Jensen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EclipseNETs: Learning Irregular Small Celestial Body Silhouettes",
        "abstract":"Accurately predicting eclipse events around irregular small bodies is crucial for spacecraft navigation, orbit determination, and spacecraft systems management. This paper introduces a novel approach leveraging neural implicit representations to model eclipse conditions efficiently and reliably. We propose neural network architectures that capture the complex silhouettes of asteroids and comets with high precision. Tested on four well-characterized bodies - Bennu, Itokawa, 67P\/Churyumov-Gerasimenko, and Eros - our method achieves accuracy comparable to traditional ray-tracing techniques while offering orders of magnitude faster performance. Additionally, we develop an indirect learning framework that trains these models directly from sparse trajectory data using Neural Ordinary Differential Equations, removing the requirement to have prior knowledge of an accurate shape model. This approach allows for the continuous refinement of eclipse predictions, progressively reducing errors and improving accuracy as new trajectory data is incorporated.\n        △ Less",
        "authors":"['Giacomo Acciarini', 'Dario Izzo', 'Francesco Biscani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation",
        "abstract":"Unlocking the next generation of biotechnology and therapeutic innovation demands overcoming the inherent complexity and resource-intensity of conventional protein engineering methods. Recent GenAI-powered computational techniques often rely on the availability of the target protein's 3D structures and specific binding sites to generate high-affinity binders, constraints exhibited by models such as AlphaProteo and RFdiffusion. In this work, we explore the use of Protein Language Models (pLMs) for high-affinity binder generation. We introduce Prot42, a novel family of Protein Language Models (pLMs) pretrained on vast amounts of unlabeled protein sequences. By capturing deep evolutionary, structural, and functional insights through an advanced auto-regressive, decoder-only architecture inspired by breakthroughs in natural language processing, Prot42 dramatically expands the capabilities of computational protein design based on language only. Remarkably, our models handle sequences up to 8,192 amino acids, significantly surpassing standard limitations and enabling precise modeling of large proteins and complex multi-domain sequences. Demonstrating powerful practical applications, Prot42 excels in generating high-affinity protein binders and sequence-specific DNA-binding proteins. Our innovative models are publicly available, offering the scientific community an efficient and precise computational toolkit for rapid protein engineering.\n        △ Less",
        "authors":"['Mohammad Amaan Sayeed', 'Engin Tekin', 'Maryam Nadeem', 'Nancy A. ElNaker', 'Aahan Singh', 'Natalia Vassilieva', 'Boulbaba Ben Amor']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Spatial Structure of Mixture-of-Experts in Transformers",
        "abstract":"A common assumption is that MoE routers primarily leverage semantic features for expert selection. However, our study challenges this notion by demonstrating that positional token information also plays a crucial role in routing decisions. Through extensive empirical analysis, we provide evidence supporting this hypothesis, develop a phenomenological explanation of the observed behavior, and discuss practical implications for MoE-based architectures.\n        △ Less",
        "authors":"['Daniel Bershatsky', 'Ivan Oseledets']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evaluation framework for Image Segmentation Algorithms",
        "abstract":"This paper presents a comprehensive evaluation framework for image segmentation algorithms, encompassing naive methods, machine learning approaches, and deep learning techniques. We begin by introducing the fundamental concepts and importance of image segmentation, and the role of interactive segmentation in enhancing accuracy. A detailed background theory section explores various segmentation methods, including thresholding, edge detection, region growing, feature extraction, random forests, support vector machines, convolutional neural networks, U-Net, and Mask R-CNN. The implementation and experimental setup are thoroughly described, highlighting three primary approaches: algorithm assisting user, user assisting algorithm, and hybrid methods. Evaluation metrics such as Intersection over Union (IoU), computation time, and user interaction time are employed to measure performance. A comparative analysis presents detailed results, emphasizing the strengths, limitations, and trade-offs of each method. The paper concludes with insights into the practical applicability of these approaches across various scenarios and outlines future work, focusing on expanding datasets, developing more representative approaches, integrating real-time feedback, and exploring weakly supervised and self-supervised learning paradigms to enhance segmentation accuracy and efficiency. Keywords: Image Segmentation, Interactive Segmentation, Machine Learning, Deep Learning, Computer Vision\n        △ Less",
        "authors":"['Tatiana Merkulova', 'Bharani Jayakumar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence",
        "abstract":"Despite remarkable progress in machine learning, current AI systems continue to fall short of true human-like intelligence. While Large Language Models (LLMs) excel in pattern recognition and response generation, they lack genuine understanding - an essential hallmark of Artificial General Intelligence (AGI). Existing AGI evaluation methods fail to offer a practical, gradual, and informative metric. This paper introduces the Artificial General Intelligence Test Bed (AGITB), comprising twelve rigorous tests that form a signal-processing-level foundation for the potential emergence of cognitive capabilities. AGITB evaluates intelligence through a model's ability to predict binary signals across time without relying on symbolic representations or pretraining. Unlike high-level tests grounded in language or perception, AGITB focuses on core computational invariants reflective of biological intelligence, such as determinism, sensitivity, and generalisation. The test bed assumes no prior bias, operates independently of semantic meaning, and ensures unsolvability through brute force or memorization. While humans pass AGITB by design, no current AI system has met its criteria, making AGITB a compelling benchmark for guiding and recognizing progress toward AGI.\n        △ Less",
        "authors":"['Matej Šprogar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deliberate Planning of 3D Bin Packing on Packing Configuration Trees",
        "abstract":"Online 3D Bin Packing Problem (3D-BPP) has widespread applications in industrial automation. Existing methods usually solve the problem with limited resolution of spatial discretization, and\/or cannot deal with complex practical constraints well. We propose to enhance the practical applicability of online 3D-BPP via learning on a novel hierarchical representation, packing configuration tree (PCT). PCT is a full-fledged description of the state and action space of bin packing which can support packing policy learning based on deep reinforcement learning (DRL). The size of the packing action space is proportional to the number of leaf nodes, making the DRL model easy to train and well-performing even with continuous solution space. We further discover the potential of PCT as tree-based planners in deliberately solving packing problems of industrial significance, including large-scale packing and different variations of BPP setting. A recursive packing method is proposed to decompose large-scale packing into smaller sub-trees while a spatial ensemble mechanism integrates local solutions into global. For different BPP variations with additional decision variables, such as lookahead, buffering, and offline packing, we propose a unified planning framework enabling out-of-the-box problem solving. Extensive evaluations demonstrate that our method outperforms existing online BPP baselines and is versatile in incorporating various practical constraints. The planning process excels across large-scale problems and diverse problem variations. We develop a real-world packing robot for industrial warehousing, with careful designs accounting for constrained placement and transportation stability. Our packing robot operates reliably and efficiently on unprotected pallets at 10 seconds per box. It achieves averagely 19 boxes per pallet with 57.4% space utilization for relatively large-size boxes.\n        △ Less",
        "authors":"['Hang Zhao', 'Juzhan Xu', 'Kexiong Yu', 'Ruizhen Hu', 'Chenyang Zhu', 'Kai Xu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Binned Group Algebra Factorization for Differentially Private Continual Counting",
        "abstract":"We study memory-efficient matrix factorization for differentially private counting under continual observation. While recent work by Henzinger and Upadhyay 2024 introduced a factorization method with reduced error based on group algebra, its practicality in streaming settings remains limited by computational constraints. We present new structural properties of the group algebra factorization, enabling the use of a binning technique from Andersson and Pagh (2024). By grouping similar values in rows, the binning method reduces memory usage and running time to $\\tilde O(\\sqrt{n})$, where $n$ is the length of the input stream, while maintaining a low error. Our work bridges the gap between theoretical improvements in factorization accuracy and practical efficiency in large-scale private learning systems.\n        △ Less",
        "authors":"['Monika Henzinger', 'Nikita P. Kalinin', 'Jalaj Upadhyay']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Human-Level Competitive Pokémon via Scalable Offline Reinforcement Learning with Transformers",
        "abstract":"Competitive Pokémon Singles (CPS) is a popular strategy game where players learn to exploit their opponent based on imperfect information in battles that can last more than one hundred stochastic turns. AI research in CPS has been led by heuristic tree search and online self-play, but the game may also create a platform to study adaptive policies trained offline on large datasets. We develop a pipeline to reconstruct the first-person perspective of an agent from logs saved from the third-person perspective of a spectator, thereby unlocking a dataset of real human battles spanning more than a decade that grows larger every day. This dataset enables a black-box approach where we train large sequence models to adapt to their opponent based solely on their input trajectory while selecting moves without explicit search of any kind. We study a progression from imitation learning to offline RL and offline fine-tuning on self-play data in the hardcore competitive setting of Pokémon's four oldest (and most partially observed) game generations. The resulting agents outperform a recent LLM Agent approach and a strong heuristic search engine. While playing anonymously in online battles against humans, our best agents climb to rankings inside the top 10% of active players.\n        △ Less",
        "authors":"['Jake Grigsby', 'Yuqi Xie', 'Justin Sasek', 'Steven Zheng', 'Yuke Zhu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning",
        "abstract":"Large reasoning models exhibit remarkable reasoning capabilities via long, elaborate reasoning trajectories. Supervised fine-tuning on such reasoning traces, also known as distillation, can be a cost-effective way to boost reasoning capabilities of student models. However, empirical observations reveal that these reasoning trajectories are often suboptimal, switching excessively between different lines of thought, resulting in under-thinking, over-thinking, and even degenerate responses. We introduce Retro-Search, an MCTS-inspired search algorithm, for distilling higher quality reasoning paths from large reasoning models. Retro-Search retrospectively revises reasoning paths to discover better, yet shorter traces, which can then lead to student models with enhanced reasoning capabilities with shorter, thus faster inference. Our approach can enable two use cases: self-improvement, where models are fine-tuned on their own Retro-Search-ed thought traces, and weak-to-strong improvement, where a weaker model revises stronger model's thought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned on its own Retro-Search-ed traces, reduces the average reasoning length by 31.2% while improving performance by 7.7% across seven math benchmarks. For weak-to-strong improvement, we retrospectively revise R1-671B's traces from the OpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x smaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance comparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length and a 2.4% performance improvement compared to fine-tuning on the original OpenThoughts data. Our work counters recently emergent viewpoints that question the relevance of search algorithms in the era of large reasoning models, by demonstrating that there are still opportunities for algorithmic advancements, even for frontier models.\n        △ Less",
        "authors":"['Ximing Lu', 'Seungju Han', 'David Acuna', 'Hyunwoo Kim', 'Jaehun Jung', 'Shrimai Prabhumoye', 'Niklas Muennighoff', 'Mostofa Patwary', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Yejin Choi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Future-Proof Yourself: An AI Era Survival Guide",
        "abstract":"Future-Proof Yourself is a practical guide that helps readers navigate the fast-changing world of artificial intelligence in everyday life. The book begins by explaining how computers learn from data in simple, relatable terms, and gradually introduces the methods used in modern AI. It shows how basic ideas in machine learning evolve into advanced systems that can recognize images, understand language, and even make decisions. The guide also reviews the history of AI and highlights the major breakthroughs that have shaped its growth. Looking ahead, the book explores emerging trends such as the integration of AI with digital twins, wearable devices, and virtual environments. Designed for a general audience, the text avoids heavy technical jargon and presents complex ideas in clear, straightforward language so that anyone can gain a solid understanding of the technology that is set to transform our future.\n        △ Less",
        "authors":"['Taehoon Kim']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From Coarse to Fine: A Physics-Informed Self-Guided Flow Diffusion Model",
        "abstract":"Machine learning methods are widely explored as a promising way to reconstruct high-fidelity computational fluid dynamics (CFD) data from faster-to-compute low-fidelity input. Diffusion models have achieved great success as they can reconstruct high-fidelity data from low-fidelity inputs at arbitrary resolution without re-training. However, most existing approaches assume that low-fidelity data is generated artificially via downsampling high-fidelity data. In reality, low-fidelity data is produced by numerical solvers that use a coarser resolution from the start, leading to substantial differences compared to high-fidelity data, especially in the long-range. Solver-generated low-fidelity data usually sacrifices fine-grained details, such as small-scale vortices compared to high-fidelity ones. To bridge this gap, we propose \\model, a novel diffusion model for reconstruction, where both low- and high-fidelity data are straight from numerical solvers. Our findings show that state-of-the-art models struggle to generate fine-scale details when faced with solver-generated low-fidelity inputs. To address this challenge, we propose an \\textit{Importance Weight} strategy during training that serves as a form of self-guidance, along with a training-free \\textit{Residual Correction} approach during inference that embeds physical insights into the model. Together, these techniques steer the diffusion model toward more accurate reconstructions. Experimental results on four 2D turbulent flow datasets demonstrate the efficacy of our proposed method.\n        △ Less",
        "authors":"['Ruoyan Li', 'Zijie Huang', 'Yizhou Sun', 'Wei Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"StyleRec: A Benchmark Dataset for Prompt Recovery in Writing Style Transformation",
        "abstract":"Prompt Recovery, reconstructing prompts from the outputs of large language models (LLMs), has grown in importance as LLMs become ubiquitous. Most users access LLMs through APIs without internal model weights, relying only on outputs and logits, which complicates recovery. This paper explores a unique prompt recovery task focused on reconstructing prompts for style transfer and rephrasing, rather than typical question-answering. We introduce a dataset created with LLM assistance, ensuring quality through multiple techniques, and test methods like zero-shot, few-shot, jailbreak, chain-of-thought, fine-tuning, and a novel canonical-prompt fallback for poor-performing cases. Our results show that one-shot and fine-tuning yield the best outcomes but highlight flaws in traditional sentence similarity metrics for evaluating prompt recovery. Contributions include (1) a benchmark dataset, (2) comprehensive experiments on prompt recovery strategies, and (3) identification of limitations in current evaluation metrics, all of which advance general prompt recovery research, where the structure of the input prompt is unrestricted.\n        △ Less",
        "authors":"['Shenyang Liu', 'Yang Gao', 'Shaoyan Zhai', 'Liqiang Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How Accurately Do Large Language Models Understand Code?",
        "abstract":"Large Language Models (LLMs) are increasingly used in post-development tasks such as code repair and testing. A key factor in these tasks' success is the model's deep understanding of code. However, the extent to which LLMs truly understand code remains largely unevaluated. Quantifying code comprehension is challenging due to its abstract nature and the lack of a standardized metric. Previously, this was assessed through developer surveys, which are not feasible for evaluating LLMs. Existing LLM benchmarks focus primarily on code generation, fundamentally different from code comprehension. Additionally, fixed benchmarks quickly become obsolete as they become part of the training data. This paper presents the first large-scale empirical investigation into LLMs' ability to understand code. Inspired by mutation testing, we use an LLM's fault-finding ability as a proxy for its deep code understanding. This approach is based on the insight that a model capable of identifying subtle functional discrepancies must understand the code well. We inject faults in real-world programs and ask the LLM to localize them, ensuring the specifications suffice for fault localization. Next, we apply semantic-preserving code mutations (SPMs) to the faulty programs and test whether the LLMs still locate the faults, verifying their confidence in code understanding. We evaluate nine popular LLMs on 600,010 debugging tasks from 670 Java and 637 Python programs. We find that LLMs lose the ability to debug the same bug in 78% of faulty programs when SPMs are applied, indicating a shallow understanding of code and reliance on features irrelevant to semantics. We also find that LLMs understand code earlier in the program better than later. This suggests that LLMs' code comprehension remains tied to lexical and syntactic features due to tokenization designed for natural languages, which overlooks code semantics.\n        △ Less",
        "authors":"['Sabaat Haroon', 'Ahmad Faraz Khan', 'Ahmad Humayun', 'Waris Gill', 'Abdul Haddi Amjad', 'Ali R. Butt', 'Mohammad Taha Khan', 'Muhammad Ali Gulzar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Novel Cholesky Kernel based Support Vector Classifier",
        "abstract":"Support Vector Machine (SVM) is a popular supervised classification model that works by first finding the margin boundaries for the training data classes and then calculating the decision boundary, which is then used to classify the test data. This study demonstrates limitations of traditional support vector classification which uses cartesian coordinate geometry to find the margin and decision boundaries in an input space using only a few support vectors, without considering data variance and correlation. Subsequently, the study proposes a new Cholesky Kernel that adjusts for the effects of variance-covariance structure of the data in the decision boundary equation and margin calculations. The study demonstrates that SVM model is valid only in the Euclidean space, and the Cholesky kernel obtained by decomposing covariance matrix acts as a transformation matrix, which when applied on the original data transforms the data from the input space to the Euclidean space. The effectiveness of the Cholesky kernel based SVM classifier is demonstrated by classifying the Wisconsin Breast Cancer (Diagnostic) Dataset and comparing with traditional SVM approaches. The Cholesky kernel based SVM model shows marked improvement in the precision, recall and F1 scores compared to linear and other kernel SVMs.\n        △ Less",
        "authors":"['Satyajeet Sahoo', 'Jhareswar Maiti']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AutoPDL: Automatic Prompt Optimization for LLM Agents",
        "abstract":"The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and non-transferable across LLMs or tasks. Therefore, this paper proposes AutoPDL, an automated approach to discover good LLM agent configurations. Our method frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and six LLMs (ranging from 8B to 70B parameters) show consistent accuracy gains ($9.5\\pm17.5$ percentage points), up to 68.9pp, and reveal that selected prompting strategies vary across models and tasks.\n        △ Less",
        "authors":"['Claudio Spiess', 'Mandana Vaziri', 'Louis Mandel', 'Martin Hirzel']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"REFORMER: A ChatGPT-Driven Data Synthesis Framework Elevating Text-to-SQL Models",
        "abstract":"The existing Text-to-SQL models suffer from a shortage of training data, inhibiting their ability to fully facilitate the applications of SQL queries in new domains. To address this challenge, various data synthesis techniques have been employed to generate more diverse and higher quality data. In this paper, we propose REFORMER, a framework that leverages ChatGPT's prowess without the need for additional training, to facilitate the synthesis of (question, SQL query) pairs tailored to new domains. Our data augmentation approach is based on a \"retrieve-and-edit\" method, where we generate new questions by filling masked question using explanation of SQL queries with the help of ChatGPT. Furthermore, we demonstrate that cycle consistency remains a valuable method of validation when applied appropriately. Our experimental results show that REFORMER consistently outperforms previous data augmentation methods. To further investigate the power of ChatGPT and create a general data augmentation method, we also generate the new data by paraphrasing the question in the dataset and by paraphrasing the description of a new SQL query that is generated by ChatGPT as well. Our results affirm that paraphrasing questions generated by ChatGPT help augment the original data.\n        △ Less",
        "authors":"['Shenyang Liu', 'Saleh Almohaimeed', 'Liqiang Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Extending Cox Proportional Hazards Model with Symbolic Non-Linear Log-Risk Functions for Survival Analysis",
        "abstract":"The Cox proportional hazards (CPH) model has been widely applied in survival analysis to estimate relative risks across different subjects given multiple covariates. Traditional CPH models rely on a linear combination of covariates weighted with coefficients as the log-risk function, which imposes a strong and restrictive assumption, limiting generalization. Recent deep learning methods enable non-linear log-risk functions. However, they often lack interpretability due to the end-to-end training mechanisms. The implementation of Kolmogorov-Arnold Networks (KAN) offers new possibilities for extending the CPH model with fully transparent and symbolic non-linear log-risk functions. In this paper, we introduce Generalized Cox Proportional Hazards (GCPH) model, a novel method for survival analysis that leverages KAN to enable a non-linear mapping from covariates to survival outcomes in a fully symbolic manner. GCPH maintains the interpretability of traditional CPH models while allowing for the estimation of non-linear log-risk functions. Experiments conducted on both synthetic data and various public benchmarks demonstrate that GCPH achieves competitive performance in terms of prediction accuracy and exhibits superior interpretability compared to current state-of-the-art methods.\n        △ Less",
        "authors":"['Jiaxiang Cheng', 'Guoqiang Hu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DDPT: Diffusion-Driven Prompt Tuning for Large Language Model Code Generation",
        "abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation. However, the quality of the generated code is heavily dependent on the structure and composition of the prompts used. Crafting high-quality prompts is a challenging task that requires significant knowledge and skills of prompt engineering. To advance the automation support for the prompt engineering for LLM-based code generation, we propose a novel solution Diffusion-Driven Prompt Tuning (DDPT) that learns how to generate optimal prompt embedding from Gaussian Noise to automate the prompt engineering for code generation. We evaluate the feasibility of diffusion-based optimization and abstract the optimal prompt embedding as a directional vector toward the optimal embedding. We use the code generation loss given by the LLMs to help the diffusion model capture the distribution of optimal prompt embedding during training. The trained diffusion model can build a path from the noise distribution to the optimal distribution at the sampling phrase, the evaluation result demonstrates that DDPT helps improve the prompt optimization for code generation.\n        △ Less",
        "authors":"['Jinyang Li', 'Sangwon Hyun', 'M. Ali Babar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Tight Regret Bounds for Fixed-Price Bilateral Trade",
        "abstract":"We examine fixed-price mechanisms in bilateral trade through the lens of regret minimization. Our main results are twofold. (i) For independent values, a near-optimal $\\widetildeΘ(T^{2\/3})$ tight bound for $\\textsf{Global Budget Balance}$ fixed-price mechanisms with two-bit\/one-bit feedback. (ii) For correlated\/adversarial values, a near-optimal $Ω(T^{3\/4})$ lower bound for $\\textsf{Global Budget Balance}$ fixed-price mechanisms with two-bit\/one-bit feedback, which improves the best known $Ω(T^{5\/7})$ lower bound obtained in the work \\cite{BCCF24} and, up to polylogarithmic factors, matches the $\\widetilde{\\mathcal{O}}(T^{3 \/ 4})$ upper bound obtained in the same work. Our work in combination with the previous works \\cite{CCCFL24mor, CCCFL24jmlr, AFF24, BCCF24} (essentially) gives a thorough understanding of regret minimization for fixed-price bilateral trade.\n  En route, we have developed two technical ingredients that might be of independent interest: (i) A novel algorithmic paradigm, called $\\textit{fractal elimination}$, to address one-bit feedback and independent values. (ii) A new $\\textit{lower-bound construction}$ with novel proof techniques, to address the $\\textsf{Global Budget Balance}$ constraint and correlated values.\n        △ Less",
        "authors":"['Houshuang Chen', 'Yaonan Jin', 'Pinyan Lu', 'Chihao Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data Scaling Laws for End-to-End Autonomous Driving",
        "abstract":"Autonomous vehicle (AV) stacks have traditionally relied on decomposed approaches, with separate modules handling perception, prediction, and planning. However, this design introduces information loss during inter-module communication, increases computational overhead, and can lead to compounding errors. To address these challenges, recent works have proposed architectures that integrate all components into an end-to-end differentiable model, enabling holistic system optimization. This shift emphasizes data engineering over software integration, offering the potential to enhance system performance by simply scaling up training resources. In this work, we evaluate the performance of a simple end-to-end driving architecture on internal driving datasets ranging in size from 16 to 8192 hours with both open-loop metrics and closed-loop simulations. Specifically, we investigate how much additional training data is needed to achieve a target performance gain, e.g., a 5% improvement in motion prediction accuracy. By understanding the relationship between model performance and training dataset size, we aim to provide insights for data-driven decision-making in autonomous driving development.\n        △ Less",
        "authors":"['Alexander Naumann', 'Xunjiang Gu', 'Tolga Dimlioglu', 'Mariusz Bojarski', 'Alperen Degirmenci', 'Alexander Popov', 'Devansh Bisla', 'Marco Pavone', 'Urs Müller', 'Boris Ivanovic']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hallucination Detection using Multi-View Attention Features",
        "abstract":"This study tackles token-level hallucination detection in outputs of large language models. Previous studies revealed that attention exhibits irregular patterns when hallucination occurs. Inspired by this, we extract features from the attention matrix that provide complementary views of (a) the average attention each token receives, which helps identify whether certain tokens are overly influential or ignored, (b) the diversity of attention each token receives, which reveals whether attention is biased toward specific subsets, and (c) the diversity of tokens a token attends to during generation, which indicates whether the model references a narrow or broad range of information. These features are input to a Transformer-based classifier to conduct token-level classification to identify hallucinated spans. Experimental results indicate that the proposed method outperforms strong baselines on hallucination detection with longer input contexts, i.e., data-to-text and summarization tasks.\n        △ Less",
        "authors":"['Yuya Ogasa', 'Yuki Arase']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Economic Battery Storage Dispatch with Deep Reinforcement Learning from Rule-Based Demonstrations",
        "abstract":"The application of deep reinforcement learning algorithms to economic battery dispatch problems has significantly increased recently. However, optimizing battery dispatch over long horizons can be challenging due to delayed rewards. In our experiments we observe poor performance of popular actor-critic algorithms when trained on yearly episodes with hourly resolution. To address this, we propose an approach extending soft actor-critic (SAC) with learning from demonstrations. The special feature of our approach is that, due to the absence of expert demonstrations, the demonstration data is generated through simple, rule-based policies. We conduct a case study on a grid-connected microgrid and use if-then-else statements based on the wholesale price of electricity to collect demonstrations. These are stored in a separate replay buffer and sampled with linearly decaying probability along with the agent's own experiences. Despite these minimal modifications and the imperfections in the demonstration data, the results show a drastic performance improvement regarding both sample efficiency and final rewards. We further show that the proposed method reliably outperforms the demonstrator and is robust to the choice of rule, as long as the rule is sufficient to guide early training into the right direction.\n        △ Less",
        "authors":"['Manuel Sage', 'Martin Staniszewski', 'Yaoyao Fiona Zhao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Causal Inference Isn't Special: Why It's Just Another Prediction Problem",
        "abstract":"Causal inference is often portrayed as fundamentally distinct from predictive modeling, with its own terminology, goals, and intellectual challenges. But at its core, causal inference is simply a structured instance of prediction under distribution shift. In both cases, we begin with labeled data from a source domain and seek to generalize to a target domain where outcomes are not observed. The key difference is that in causal inference, the labels -- potential outcomes -- are selectively observed based on treatment assignment, introducing bias that must be addressed through assumptions. This perspective reframes causal estimation as a familiar generalization problem and highlights how techniques from predictive modeling, such as reweighting and domain adaptation, apply directly to causal tasks. It also clarifies that causal assumptions are not uniquely strong -- they are simply more explicit. By viewing causal inference through the lens of prediction, we demystify its logic, connect it to familiar tools, and make it more accessible to practitioners and educators alike.\n        △ Less",
        "authors":"['Carlos Fernández-Loría']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Geo-OLM: Enabling Sustainable Earth Observation Studies with Cost-Efficient Open Language Models & State-Driven Workflows",
        "abstract":"Geospatial Copilots hold immense potential for automating Earth observation (EO) and climate monitoring workflows, yet their reliance on large-scale models such as GPT-4o introduces a paradox: tools intended for sustainability studies often incur unsustainable costs. Using agentic AI frameworks in geospatial applications can amass thousands of dollars in API charges or requires expensive, power-intensive GPUs for deployment, creating barriers for researchers, policymakers, and NGOs. Unfortunately, when geospatial Copilots are deployed with open language models (OLMs), performance often degrades due to their dependence on GPT-optimized logic. In this paper, we present Geo-OLM, a tool-augmented geospatial agent that leverages the novel paradigm of state-driven LLM reasoning to decouple task progression from tool calling. By alleviating the workflow reasoning burden, our approach enables low-resource OLMs to complete geospatial tasks more effectively. When downsizing to small models below 7B parameters, Geo-OLM outperforms the strongest prior geospatial baselines by 32.8% in successful query completion rates. Our method performs comparably to proprietary models achieving results within 10% of GPT-4o, while reducing inference costs by two orders of magnitude from \\$500-\\$1000 to under \\$10. We present an in-depth analysis with geospatial downstream benchmarks, providing key insights to help practitioners effectively deploy OLMs for EO applications.\n        △ Less",
        "authors":"['Dimitrios Stamoulis', 'Diana Marculescu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Variational Self-Supervised Learning",
        "abstract":"We present Variational Self-Supervised Learning (VSSL), a novel framework that combines variational inference with self-supervised learning to enable efficient, decoder-free representation learning. Unlike traditional VAEs that rely on input reconstruction via a decoder, VSSL symmetrically couples two encoders with Gaussian outputs. A momentum-updated teacher network defines a dynamic, data-dependent prior, while the student encoder produces an approximate posterior from augmented views. The reconstruction term in the ELBO is replaced with a cross-view denoising objective, preserving the analytical tractability of Gaussian KL divergence. We further introduce cosine-based formulations of KL and log-likelihood terms to enhance semantic alignment in high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and ImageNet-100 show that VSSL achieves competitive or superior performance to leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a scalable, probabilistically grounded approach to learning transferable representations without generative reconstruction, bridging the gap between variational modeling and modern self-supervised techniques.\n        △ Less",
        "authors":"['Mehmet Can Yavuz', 'Berrin Yanikoglu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Survey of Social Cybersecurity: Techniques for Attack Detection, Evaluations, Challenges, and Future Prospects",
        "abstract":"In today's digital era, the Internet, especially social media platforms, plays a significant role in shaping public opinions, attitudes, and beliefs. Unfortunately, the credibility of scientific information sources is often undermined by the spread of misinformation through various means, including technology-driven tools like bots, cyborgs, trolls, sock-puppets, and deep fakes. This manipulation of public discourse serves antagonistic business agendas and compromises civil society. In response to this challenge, a new scientific discipline has emerged: social cybersecurity.\n        △ Less",
        "authors":"['Aos Mulahuwaish', 'Basheer Qolomany', 'Kevin Gyorick', 'Jacques Bou Abdo', 'Mohammed Aledhari', 'Junaid Qadir', 'Kathleen Carley', 'Ala Al-Fuqaha']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization",
        "abstract":"Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems-a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agent frameworks against established human-designed algorithms, revealing key strengths and limitations of current approaches and identifying promising directions for future research. CO-Bench is publicly available at https:\/\/github.com\/sunnweiwei\/CO-Bench.\n        △ Less",
        "authors":"['Weiwei Sun', 'Shengyu Feng', 'Shanda Li', 'Yiming Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Gating is Weighting: Understanding Gated Linear Attention through In-context Learning",
        "abstract":"Linear attention methods offer a compelling alternative to softmax attention due to their efficiency in recurrent decoding. Recent research has focused on enhancing standard linear attention by incorporating gating while retaining its computational benefits. Such Gated Linear Attention (GLA) architectures include competitive models such as Mamba and RWKV. In this work, we investigate the in-context learning capabilities of the GLA model and make the following contributions. We show that a multilayer GLA can implement a general class of Weighted Preconditioned Gradient Descent (WPGD) algorithms with data-dependent weights. These weights are induced by the gating mechanism and the input, enabling the model to control the contribution of individual tokens to prediction. To further understand the mechanics of this weighting, we introduce a novel data model with multitask prompts and characterize the optimization landscape of learning a WPGD algorithm. Under mild conditions, we establish the existence and uniqueness (up to scaling) of a global minimum, corresponding to a unique WPGD solution. Finally, we translate these findings to explore the optimization landscape of GLA and shed light on how gating facilitates context-aware learning and when it is provably better than vanilla linear attention.\n        △ Less",
        "authors":"['Yingcong Li', 'Davoud Ataee Tarzanagh', 'Ankit Singh Rawat', 'Maryam Fazel', 'Samet Oymak']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using ensemble methods of machine learning to predict real estate prices",
        "abstract":"In recent years, machine learning (ML) techniques have become a powerful tool for improving the accuracy of predictions and decision-making. Machine learning technologies have begun to penetrate all areas, including the real estate sector. Correct forecasting of real estate value plays an important role in the buyer-seller chain, because it ensures reasonableness of price expectations based on the offers available in the market and helps to avoid financial risks for both parties of the transaction. Accurate forecasting is also important for real estate investors to make an informed decision on a specific property. This study helps to gain a deeper understanding of how effective and accurate ensemble machine learning methods are in predicting real estate values. The results obtained in the work are quite accurate, as can be seen from the coefficient of determination (R^2), root mean square error (RMSE) and mean absolute error (MAE) calculated for each model. The Gradient Boosting Regressor model provides the highest accuracy, the Extra Trees Regressor, Hist Gradient Boosting Regressor and Random Forest Regressor models give good results. In general, ensemble machine learning techniques can be effectively used to solve real estate valuation. This work forms ideas for future research, which consist in the preliminary processing of the data set by searching and extracting anomalous values, as well as the practical implementation of the obtained results.\n        △ Less",
        "authors":"['Oleh Pastukh', 'Viktor Khomyshyn']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sigma: A dataset for text-to-code semantic parsing with statistical analysis",
        "abstract":"In the domain of semantic parsing, significant progress has been achieved in Text-to-SQL and question-answering tasks, both of which focus on extracting information from data sources in their native formats. However, the inherent constraints of their formal meaning representations, such as SQL programming language or basic logical forms, hinder their ability to analyze data from various perspectives, such as conducting statistical analyses. To address this limitation and inspire research in this field, we design SIGMA, a new dataset for Text-to-Code semantic parsing with statistical analysis. SIGMA comprises 6000 questions with corresponding Python code labels, spanning across 160 databases. Half of the questions involve query types, which return information in its original format, while the remaining 50% are statistical analysis questions, which perform statistical operations on the data. The Python code labels in our dataset cover 4 types of query types and 40 types of statistical analysis patterns. We evaluated the SIGMA dataset using three different baseline models: LGESQL, SmBoP, and SLSQL. The experimental results show that the LGESQL model with ELECTRA outperforms all other models, achieving 83.37% structure accuracy. In terms of execution accuracy, the SmBoP model, when combined with GraPPa and T5, reaches 76.38%.\n        △ Less",
        "authors":"['Saleh Almohaimeed', 'Shenyang Liu', 'May Alsofyani', 'Saad Almohaimeed', 'Liqiang Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generative Market Equilibrium Models with Stable Adversarial Learning via Reinforcement",
        "abstract":"We present a general computational framework for solving continuous-time financial market equilibria under minimal modeling assumptions while incorporating realistic financial frictions, such as trading costs, and supporting multiple interacting agents. Inspired by generative adversarial networks (GANs), our approach employs a novel generative deep reinforcement learning framework with a decoupling feedback system embedded in the adversarial training loop, which we term as the \\emph{reinforcement link}. This architecture stabilizes the training dynamics by incorporating feedback from the discriminator. Our theoretically guided feedback mechanism enables the decoupling of the equilibrium system, overcoming challenges that hinder conventional numerical algorithms. Experimentally, our algorithm not only learns but also provides testable predictions on how asset returns and volatilities emerge from the endogenous trading behavior of market participants, where traditional analytical methods fall short. The design of our model is further supported by an approximation guarantee.\n        △ Less",
        "authors":"['Anastasis Kratsios', 'Xiaofei Shi', 'Qiang Sun', 'Zhanhao Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CATS: Mitigating Correlation Shift for Multivariate Time Series Classification",
        "abstract":"Unsupervised Domain Adaptation (UDA) leverages labeled source data to train models for unlabeled target data. Given the prevalence of multivariate time series (MTS) data across various domains, the UDA task for MTS classification has emerged as a critical challenge. However, for MTS data, correlations between variables often vary across domains, whereas most existing UDA works for MTS classification have overlooked this essential characteristic. To bridge this gap, we introduce a novel domain shift, {\\em correlation shift}, measuring domain differences in multivariate correlation. To mitigate correlation shift, we propose a scalable and parameter-efficient \\underline{C}orrelation \\underline{A}dapter for M\\underline{TS} (CATS). Designed as a plug-and-play technique compatible with various Transformer variants, CATS employs temporal convolution to capture local temporal patterns and a graph attention module to model the changing multivariate correlation. The adapter reweights the target correlations to align the source correlations with a theoretically guaranteed precision. A correlation alignment loss is further proposed to mitigate correlation shift, bypassing the alignment challenge from the non-i.i.d. nature of MTS data. Extensive experiments on four real-world datasets demonstrate that (1) compared with vanilla Transformer-based models, CATS increases over $10\\%$ average accuracy while only adding around $1\\%$ parameters, and (2) all Transformer variants equipped with CATS either reach or surpass state-of-the-art baselines.\n        △ Less",
        "authors":"['Xiao Lin', 'Zhichen Zeng', 'Tianxin Wei', 'Zhining Liu', 'Yuzhong chen', 'Hanghang Tong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Foundation Models for Environmental Science: A Survey of Emerging Frontiers",
        "abstract":"Modeling environmental ecosystems is essential for effective resource management, sustainable development, and understanding complex ecological processes. However, traditional data-driven methods face challenges in capturing inherently complex and interconnected processes and are further constrained by limited observational data in many environmental applications. Foundation models, which leverages large-scale pre-training and universal representations of complex and heterogeneous data, offer transformative opportunities for capturing spatiotemporal dynamics and dependencies in environmental processes, and facilitate adaptation to a broad range of applications. This survey presents a comprehensive overview of foundation model applications in environmental science, highlighting advancements in common environmental use cases including forward prediction, data generation, data assimilation, downscaling, inverse modeling, model ensembling, and decision-making across domains. We also detail the process of developing these models, covering data collection, architecture design, training, tuning, and evaluation. Through discussions on these emerging methods as well as their future opportunities, we aim to promote interdisciplinary collaboration that accelerates advancements in machine learning for driving scientific discovery in addressing critical environmental challenges.\n        △ Less",
        "authors":"['Runlong Yu', 'Shengyu Chen', 'Yiqun Xie', 'Huaxiu Yao', 'Jared Willard', 'Xiaowei Jia']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification Tasks",
        "abstract":"Are traditional classification approaches irrelevant in this era of AI hype? We show that there are multiclass classification problems where predictive models holistically outperform LLM prompt-based frameworks. Given text and images from home-service project descriptions provided by Thumbtack customers, we build embeddings-based softmax models that predict the professional category (e.g., handyman, bathroom remodeling) associated with each problem description. We then compare against prompts that ask state-of-the-art LLM models to solve the same problem. We find that the embeddings approach outperforms the best LLM prompts in terms of accuracy, calibration, latency, and financial cost. In particular, the embeddings approach has 49.5% higher accuracy than the prompting approach, and its superiority is consistent across text-only, image-only, and text-image problem descriptions. Furthermore, it yields well-calibrated probabilities, which we later use as confidence signals to provide contextualized user experience during deployment. On the contrary, prompting scores are overly uninformative. Finally, the embeddings approach is 14 and 81 times faster than prompting in processing images and text respectively, while under realistic deployment assumptions, it can be up to 10 times cheaper. Based on these results, we deployed a variation of the embeddings approach, and through A\/B testing we observed performance consistent with our offline analysis. Our study shows that for multiclass classification problems that can leverage proprietary datasets, an embeddings-based approach may yield unequivocally better results. Hence, scientists, practitioners, engineers, and business leaders can use our study to go beyond the hype and consider appropriate predictive models for their classification use cases.\n        △ Less",
        "authors":"['Marios Kokkodis', 'Richard Demsyn-Jones', 'Vijay Raghavan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ADA-Net: Attention-Guided Domain Adaptation Network with Contrastive Learning for Standing Dead Tree Segmentation Using Aerial Imagery",
        "abstract":"Information on standing dead trees is important for understanding forest ecosystem functioning and resilience but has been lacking over large geographic regions. Climate change has caused large-scale tree mortality events that can remain undetected due to limited data. In this study, we propose a novel method for segmenting standing dead trees using aerial multispectral orthoimages. Because access to annotated datasets has been a significant problem in forest remote sensing due to the need for forest expertise, we introduce a method for domain transfer by leveraging domain adaptation to learn a transformation from a source domain X to target domain Y. In this Image-to-Image translation task, we aim to utilize available annotations in the target domain by pre-training a segmentation network. When images from a new study site without annotations are introduced (source domain X), these images are transformed into the target domain. Then, transfer learning is applied by inferring the pre-trained network on domain-adapted images. In addition to investigating the feasibility of current domain adaptation approaches for this objective, we propose a novel approach called the Attention-guided Domain Adaptation Network (ADA-Net) with enhanced contrastive learning. Accordingly, the ADA-Net approach provides new state-of-the-art domain adaptation performance levels outperforming existing approaches. We have evaluated the proposed approach using two datasets from Finland and the US. The USA images are converted to the Finland domain, and we show that the synthetic USA2Finland dataset exhibits similar characteristics to the Finland domain images. The software implementation is shared at https:\/\/github.com\/meteahishali\/ADA-Net. The data is publicly available at https:\/\/www.kaggle.com\/datasets\/meteahishali\/aerial-imagery-for-standing-dead-tree-segmentation.\n        △ Less",
        "authors":"['Mete Ahishali', 'Anis Ur Rahman', 'Einari Heinaro', 'Samuli Junttila']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving Chronic Kidney Disease Detection Efficiency: Fine Tuned CatBoost and Nature-Inspired Algorithms with Explainable AI",
        "abstract":"Chronic Kidney Disease (CKD) is a major global health issue which is affecting million people around the world and with increasing rate of mortality. Mitigation of progression of CKD and better patient outcomes requires early detection. Nevertheless, limitations lie in traditional diagnostic methods, especially in resource constrained settings. This study proposes an advanced machine learning approach to enhance CKD detection by evaluating four models: Random Forest (RF), Multi-Layer Perceptron (MLP), Logistic Regression (LR), and a fine-tuned CatBoost algorithm. Specifically, among these, the fine-tuned CatBoost model demonstrated the best overall performance having an accuracy of 98.75%, an AUC of 0.9993 and a Kappa score of 97.35% of the studies. The proposed CatBoost model has used a nature inspired algorithm such as Simulated Annealing to select the most important features, Cuckoo Search to adjust outliers and grid search to fine tune its settings in such a way to achieve improved prediction accuracy. Features significance is explained by SHAP-a well-known XAI technique-for gaining transparency in the decision-making process of proposed model and bring up trust in diagnostic systems. Using SHAP, the significant clinical features were identified as specific gravity, serum creatinine, albumin, hemoglobin, and diabetes mellitus. The potential of advanced machine learning techniques in CKD detection is shown in this research, particularly for low income and middle-income healthcare settings where prompt and correct diagnoses are vital. This study seeks to provide a highly accurate, interpretable, and efficient diagnostic tool to add to efforts for early intervention and improved healthcare outcomes for all CKD patients.\n        △ Less",
        "authors":"['Md. Ehsanul Haque', 'S. M. Jahidul Islam', 'Jeba Maliha', 'Md. Shakhauat Hossan Sumon', 'Rumana Sharmin', 'Sakib Rokoni']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LOGLO-FNO: Efficient Learning of Local and Global Features in Fourier Neural Operators",
        "abstract":"Modeling high-frequency information is a critical challenge in scientific machine learning. For instance, fully turbulent flow simulations of Navier-Stokes equations at Reynolds numbers 3500 and above can generate high-frequency signals due to swirling fluid motions caused by eddies and vortices. Faithfully modeling such signals using neural networks depends on accurately reconstructing moderate to high frequencies. However, it has been well known that deep neural nets exhibit the so-called spectral bias toward learning low-frequency components. Meanwhile, Fourier Neural Operators (FNOs) have emerged as a popular class of data-driven models in recent years for solving Partial Differential Equations (PDEs) and for surrogate modeling in general. Although impressive results have been achieved on several PDE benchmark problems, FNOs often perform poorly in learning non-dominant frequencies characterized by local features. This limitation stems from the spectral bias inherent in neural networks and the explicit exclusion of high-frequency modes in FNOs and their variants. Therefore, to mitigate these issues and improve FNO's spectral learning capabilities to represent a broad range of frequency components, we propose two key architectural enhancements: (i) a parallel branch performing local spectral convolutions (ii) a high-frequency propagation module. Moreover, we propose a novel frequency-sensitive loss term based on radially binned spectral errors. This introduction of a parallel branch for local convolutions reduces number of trainable parameters by up to 50% while achieving the accuracy of baseline FNO that relies solely on global convolutions. Experiments on three challenging PDE problems in fluid mechanics and biological pattern formation, and the qualitative and spectral analysis of predictions show the effectiveness of our method over the state-of-the-art neural operator baselines.\n        △ Less",
        "authors":"['Marimuthu Kalimuthu', 'David Holzmüller', 'Mathias Niepert']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Progressive Multi-Source Domain Adaptation for Personalized Facial Expression Recognition",
        "abstract":"Personalized facial expression recognition (FER) involves adapting a machine learning model using samples from labeled sources and unlabeled target domains. Given the challenges of recognizing subtle expressions with considerable interpersonal variability, state-of-the-art unsupervised domain adaptation (UDA) methods focus on the multi-source UDA (MSDA) setting, where each domain corresponds to a specific subject, and improve model accuracy and robustness. However, when adapting to a specific target, the diverse nature of multiple source domains translates to a large shift between source and target data. State-of-the-art MSDA methods for FER address this domain shift by considering all the sources to adapt to the target representations. Nevertheless, adapting to a target subject presents significant challenges due to large distributional differences between source and target domains, often resulting in negative transfer. In addition, integrating all sources simultaneously increases computational costs and causes misalignment with the target. To address these issues, we propose a progressive MSDA approach that gradually introduces information from subjects based on their similarity to the target subject. This will ensure that only the most relevant sources from the target are selected, which helps avoid the negative transfer caused by dissimilar sources. We first exploit the closest sources to reduce the distribution shift with the target and then move towards the furthest while only considering the most relevant sources based on the predetermined threshold. Furthermore, to mitigate catastrophic forgetting caused by the incremental introduction of source subjects, we implemented a density-based memory mechanism that preserves the most relevant historical source samples for adaptation. Our experiments show the effectiveness of our proposed method on pain datasets: Biovid and UNBC-McMaster.\n        △ Less",
        "authors":"['Muhammad Osama Zeeshan', 'Marco Pedersoli', 'Alessandro Lameiras Koerich', 'Eric Grange']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Randomised Postiterations for Calibrated BayesCG",
        "abstract":"The Bayesian conjugate gradient method offers probabilistic solutions to linear systems but suffers from poor calibration, limiting its utility in uncertainty quantification tasks. Recent approaches leveraging postiterations to construct priors have improved computational properties but failed to correct calibration issues. In this work, we propose a novel randomised postiteration strategy that enhances the calibration of the BayesCG posterior while preserving its favourable convergence characteristics. We present theoretical guarantees for the improved calibration, supported by results on the distribution of posterior errors. Numerical experiments demonstrate the efficacy of the method in both synthetic and inverse problem settings, showing enhanced uncertainty quantification and better propagation of uncertainties through computational pipelines.\n        △ Less",
        "authors":"['Niall Vyas', 'Disha Hegde', 'Jon Cockayne']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From Automation to Autonomy in Smart Manufacturing: A Bayesian Optimization Framework for Modeling Multi-Objective Experimentation and Sequential Decision Making",
        "abstract":"Discovering novel materials with desired properties is essential for driving innovation. Industry 4.0 and smart manufacturing have promised transformative advances in this area through real-time data integration and automated production planning and control. However, the reliance on automation alone has often fallen short, lacking the flexibility needed for complex processes. To fully unlock the potential of smart manufacturing, we must evolve from automation to autonomous systems that go beyond rigid programming and can dynamically optimize the search for solutions. Current discovery approaches are often slow, requiring numerous trials to find optimal combinations, and costly, particularly when optimizing multiple properties simultaneously. This paper proposes a Bayesian multi-objective sequential decision-making (BMSDM) framework that can intelligently select experiments as manufacturing progresses, guiding us toward the discovery of optimal design faster and more efficiently. The framework leverages sequential learning through Bayesian Optimization, which iteratively refines a statistical model representing the underlying manufacturing process. This statistical model acts as a surrogate, allowing for efficient exploration and optimization without requiring numerous real-world experiments. This approach can significantly reduce the time and cost of data collection required by traditional experimental designs. The proposed framework is compared with traditional DoE methods and two other multi-objective optimization methods. Using a manufacturing dataset, we evaluate and compare the performance of these approaches across five evaluation metrics. BMSDM comprehensively outperforms the competing methods in multi-objective decision-making scenarios. Our proposed approach represents a significant leap forward in creating an intelligent autonomous platform capable of novel material discovery.\n        △ Less",
        "authors":"['Avijit Saha Asru', 'Hamed Khosravi', 'Imtiaz Ahmed', 'Abdullahil Azeem']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Perils of Label Indeterminacy: A Case Study on Prediction of Neurological Recovery After Cardiac Arrest",
        "abstract":"The design of AI systems to assist human decision-making typically requires the availability of labels to train and evaluate supervised models. Frequently, however, these labels are unknown, and different ways of estimating them involve unverifiable assumptions or arbitrary choices. In this work, we introduce the concept of label indeterminacy and derive important implications in high-stakes AI-assisted decision-making. We present an empirical study in a healthcare context, focusing specifically on predicting the recovery of comatose patients after resuscitation from cardiac arrest. Our study shows that label indeterminacy can result in models that perform similarly when evaluated on patients with known labels, but vary drastically in their predictions for patients where labels are unknown. After demonstrating crucial ethical implications of label indeterminacy in this high-stakes context, we discuss takeaways for evaluation, reporting, and design.\n        △ Less",
        "authors":"['Jakob Schoeffer', 'Maria De-Arteaga', 'Jonathan Elmer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Loss Functions in Deep Learning: A Comprehensive Review",
        "abstract":"Loss functions are at the heart of deep learning, shaping how models learn and perform across diverse tasks. They are used to quantify the difference between predicted outputs and ground truth labels, guiding the optimization process to minimize errors. Selecting the right loss function is critical, as it directly impacts model convergence, generalization, and overall performance across various applications, from computer vision to time series forecasting. This paper presents a comprehensive review of loss functions, covering fundamental metrics like Mean Squared Error and Cross-Entropy to advanced functions such as Adversarial and Diffusion losses. We explore their mathematical foundations, impact on model training, and strategic selection for various applications, including computer vision (Discriminative and generative), tabular data prediction, and time series forecasting. For each of these categories, we discuss the most used loss functions in the recent advancements of deep learning techniques. Also, this review explore the historical evolution, computational efficiency, and ongoing challenges in loss function design, underlining the need for more adaptive and robust solutions. Emphasis is placed on complex scenarios involving multi-modal data, class imbalances, and real-world constraints. Finally, we identify key future directions, advocating for loss functions that enhance interpretability, scalability, and generalization, leading to more effective and resilient deep learning models.\n        △ Less",
        "authors":"['Omar Elharrouss', 'Yasir Mahmood', 'Yassine Bechqito', 'Mohamed Adel Serhani', 'Elarbi Badidi', 'Jamal Riffi', 'Hamid Tairi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"oneDAL Optimization for ARM Scalable Vector Extension: Maximizing Efficiency for High-Performance Data Science",
        "abstract":"The evolution of ARM-based architectures, particularly those incorporating Scalable Vector Extension (SVE), has introduced transformative opportunities for high-performance computing (HPC) and machine learning (ML) workloads. The Unified Acceleration Foundation's (UXL) oneAPI Data Analytics Library (oneDAL) is a widely adopted library for accelerating ML and data analytics workflows, but its reliance on Intel's proprietary Math Kernel Library (MKL) has traditionally limited its compatibility to x86platforms. This paper details the porting of oneDAL to ARM architectures with SVE support, using OpenBLAS as an alternative backend to overcome architectural and performance challenges. Beyond porting, the research introduces novel ARM-specific optimizations, including custom sparse matrix routines, vectorized statistical functions, and a Scalable Vector Extension (SVE)-optimized Support Vector Machine (SVM) algorithm. The SVM enhancements leverage SVE's flexible vector lengths and predicate driven execution, achieving notable performance gains of 22% for the Boser method and 5% for the Thunder method. Benchmarks conducted on ARM SVE-enabled AWSGraviton3 instances showcase up to 200x acceleration in ML training and inference tasks compared to the original scikit-learn implementation on the ARM platform. Moreover, the ARM-optimized oneDAL achieves performance parity with, and in some cases exceeds, the x86 oneDAL implementation (MKL backend) on IceLake x86 systems, which are nearly twice as costly as AWSGraviton3 ARM instances. These findings highlight ARM's potential as a high-performance, energyefficient platform for dataintensive ML applications. By expanding cross-architecture compatibility and contributing to the opensource ecosystem, this work reinforces ARM's position as a competitive alternative in the HPC and ML domains, paving the way for future advancements in dataintensive computing.\n        △ Less",
        "authors":"['Chandan Sharma', 'Rakshith GB', 'Ajay Kumar Patel', 'Dhanus M Lal', 'Darshan Patel', 'Ragesh Hajela', 'Masahiro Doteguchi', 'Priyanka Sharma']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis with Generic Traffic Representation",
        "abstract":"Machine learning (ML) powered network traffic analysis has been widely used for the purpose of threat detection. Unfortunately, their generalization across different tasks and unseen data is very limited. Large language models (LLMs), known for their strong generalization capabilities, have shown promising performance in various domains. However, their application to the traffic analysis domain is limited due to significantly different characteristics of network traffic. To address the issue, in this paper, we propose TrafficLLM, which introduces a dual-stage fine-tuning framework to learn generic traffic representation from heterogeneous raw traffic data. The framework uses traffic-domain tokenization, dual-stage tuning pipeline, and extensible adaptation to help LLM release generalization ability on dynamic traffic analysis tasks, such that it enables traffic detection and traffic generation across a wide range of downstream tasks. We evaluate TrafficLLM across 10 distinct scenarios and 229 types of traffic. TrafficLLM achieves F1-scores of 0.9875 and 0.9483, with up to 80.12% and 33.92% better performance than existing detection and generation methods. It also shows strong generalization on unseen traffic with an 18.6% performance improvement. We further evaluate TrafficLLM in real-world scenarios. The results confirm that TrafficLLM is easy to scale and achieves accurate detection performance on enterprise traffic.\n        △ Less",
        "authors":"['Tianyu Cui', 'Xinjie Lin', 'Sijia Li', 'Miao Chen', 'Qilei Yin', 'Qi Li', 'Ke Xu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Elicitation of Latent Information Using Natural Language",
        "abstract":"Eliciting information to reduce uncertainty about a latent entity is a critical task in many application domains, e.g., assessing individual student learning outcomes, diagnosing underlying diseases, or learning user preferences. Though natural language is a powerful medium for this purpose, large language models (LLMs) and existing fine-tuning algorithms lack mechanisms for strategically gathering information to refine their own understanding of the latent entity. To harness the generalization power and world knowledge of LLMs in developing effective information-gathering strategies, we propose an adaptive elicitation framework that actively reduces uncertainty on the latent entity. Since probabilistic modeling of an abstract latent entity is difficult, our framework adopts a predictive view of uncertainty, using a meta-learned language model to simulate future observations and enable scalable uncertainty quantification over complex natural language. Through autoregressive forward simulation, our model quantifies how new questions reduce epistemic uncertainty, enabling the development of sophisticated information-gathering strategies to choose the most informative next queries. In experiments on the 20 questions game, dynamic opinion polling, and adaptive student assessment, our method consistently outperforms baselines in identifying critical unknowns and improving downstream predictions, illustrating the promise of strategic information gathering in natural language settings.\n        △ Less",
        "authors":"['Jimmy Wang', 'Thomas Zollo', 'Richard Zemel', 'Hongseok Namkoong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Directional Sign Loss: A Topology-Preserving Loss Function that Approximates the Sign of Finite Differences",
        "abstract":"Preserving critical topological features in learned latent spaces is a fundamental challenge in representation learning, particularly for topology-sensitive data. This paper introduces directional sign loss (DSL), a novel loss function that approximates the number of mismatches in the signs of finite differences between corresponding elements of two arrays. By penalizing discrepancies in critical points between input and reconstructed data, DSL encourages autoencoders and other learnable compressors to retain the topological features of the original data. We present the mathematical formulation, complexity analysis, and practical implementation of DSL, comparing its behavior to its non-differentiable counterpart and to other topological measures. Experiments on one-, two-, and three-dimensional data show that combining DSL with traditional loss functions preserves topological features more effectively than traditional losses alone. Moreover, DSL serves as a differentiable, efficient proxy for common topology-based metrics, enabling its use in gradient-based optimization frameworks.\n        △ Less",
        "authors":"['Harvey Dam', 'Tripti Agarwal', 'Ganesh Gopalakrishnan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Effects of Grouped Structural Global Pruning of Vision Transformers on Domain Generalisation",
        "abstract":"With the growing sizes of AI models like large language models (LLMs) and vision transformers, deploying them on devices with limited computational resources is a significant challenge particularly when addressing domain generalisation (DG) tasks. This paper introduces a novel grouped structural pruning method for pre-trained vision transformers (ViT, BeiT, and DeiT), evaluated on the PACS and Office-Home DG benchmarks. Our method uses dependency graph analysis to identify and remove redundant groups of neurons, weights, filters, or attention heads within transformers, using a range of selection metrics. Grouped structural pruning is applied at pruning ratios of 50\\%, 75\\% and 95\\% and the models are then fine-tuned on selected distributions from DG benchmarks to evaluate their overall performance in DG tasks. Results show significant improvements in inference speed and fine-tuning time with minimal trade-offs in accuracy and DG task performance. For instance, on the PACS benchmark, pruning ViT, BeiT, and DeiT models by 50\\% using the Hessian metric resulted in accuracy drops of only -2.94\\%, -1.42\\%, and -1.72\\%, respectively, while achieving speed boosts of 2.5x, 1.81x, and 2.15x. These findings demonstrate the effectiveness of our approach in balancing model efficiency with domain generalisation performance.\n        △ Less",
        "authors":"['Hamza Riaz', 'Alan F. Smeaton']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AiReview: An Open Platform for Accelerating Systematic Reviews with LLMs",
        "abstract":"Systematic reviews are fundamental to evidence-based medicine. Creating one is time-consuming and labour-intensive, mainly due to the need to screen, or assess, many studies for inclusion in the review. Several tools have been developed to streamline this process, mostly relying on traditional machine learning methods. Large language models (LLMs) have shown potential in further accelerating the screening process. However, no tool currently allows end users to directly leverage LLMs for screening or facilitates systematic and transparent usage of LLM-assisted screening methods. This paper introduces (i) an extensible framework for applying LLMs to systematic review tasks, particularly title and abstract screening, and (ii) a web-based interface for LLM-assisted screening. Together, these elements form AiReview-a novel platform for LLM-assisted systematic review creation. AiReview is the first of its kind to bridge the gap between cutting-edge LLM-assisted screening methods and those that create medical systematic reviews. The tool is available at https:\/\/aireview.ielab.io. The source code is also open sourced at https:\/\/github.com\/ielab\/ai-review.\n        △ Less",
        "authors":"['Xinyu Mao', 'Teerapong Leelanupab', 'Martin Potthast', 'Harrisen Scells', 'Guido Zuccon']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Principled Learning for Re-ranking in Recommender Systems",
        "abstract":"As the final stage of recommender systems, re-ranking presents ordered item lists to users that best match their interests. It plays such a critical role and has become a trending research topic with much attention from both academia and industry. Recent advances of re-ranking are focused on attentive listwise modeling of interactions and mutual influences among items to be re-ranked. However, principles to guide the learning process of a re-ranker, and to measure the quality of the output of the re-ranker, have been always missing. In this paper, we study such principles to learn a good re-ranker. Two principles are proposed, including convergence consistency and adversarial consistency. These two principles can be applied in the learning of a generic re-ranker and improve its performance. We validate such a finding by various baseline methods over different datasets.\n        △ Less",
        "authors":"['Qunwei Li', 'Linghui Li', 'Jianbin Lin', 'Wenliang Zhong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AttackLLM: LLM-based Attack Pattern Generation for an Industrial Control System",
        "abstract":"Malicious examples are crucial for evaluating the robustness of machine learning algorithms under attack, particularly in Industrial Control Systems (ICS). However, collecting normal and attack data in ICS environments is challenging due to the scarcity of testbeds and the high cost of human expertise. Existing datasets are often limited by the domain expertise of practitioners, making the process costly and inefficient. The lack of comprehensive attack pattern data poses a significant problem for developing robust anomaly detection methods. In this paper, we propose a novel approach that combines data-centric and design-centric methodologies to generate attack patterns using large language models (LLMs). Our results demonstrate that the attack patterns generated by LLMs not only surpass the quality and quantity of those created by human experts but also offer a scalable solution that does not rely on expensive testbeds or pre-existing attack examples. This multi-agent based approach presents a promising avenue for enhancing the security and resilience of ICS environments.\n        △ Less",
        "authors":"['Chuadhry Mujeeb Ahmed']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Variational autoencoders understand knot topology",
        "abstract":"Supervised machine learning (ML) methods are emerging as valid alternatives to standard mathematical methods for identifying knots in long, collapsed polymers. Here, we introduce a hybrid supervised\/unsupervised ML approach for knot classification based on a variational autoencoder enhanced with a knot type classifier (VAEC). The neat organization of knots in its latent representation suggests that the VAEC, only based on an arbitrary labeling of three-dimensional configurations, has grasped complex topological concepts such as chirality, unknotting number, braid index, and the grouping in families such as achiral, torus, and twist knots. The understanding of topological concepts is confirmed by the ability of the VAEC to distinguish the chirality of knots $9_{42}$ and $10_{71}$ not used for its training and with a notoriously undetected chirality to standard tools. The well-organized latent space is also key for generating configurations with the decoder that reliably preserves the topology of the input ones. Our findings demonstrate the ability of a hybrid supervised-generative ML algorithm to capture different topological features of entangled filaments and to exploit this knowledge to faithfully reconstruct or produce new knotted configurations without simulations.\n        △ Less",
        "authors":"['Anna Braghetto', 'Sumanta Kundu', 'Marco Baiesi', 'Enzo Orlandini']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Digital Gene: Learning about the Physical World through Analytic Concepts",
        "abstract":"Reviewing the progress in artificial intelligence over the past decade, various significant advances (e.g. object detection, image generation, large language models) have enabled AI systems to produce more semantically meaningful outputs and achieve widespread adoption in internet scenarios. Nevertheless, AI systems still struggle when it comes to understanding and interacting with the physical world. This reveals an important issue: relying solely on semantic-level concepts learned from internet data (e.g. texts, images) to understand the physical world is far from sufficient -- machine intelligence currently lacks an effective way to learn about the physical world. This research introduces the idea of analytic concept -- representing the concepts related to the physical world through programs of mathematical procedures, providing machine intelligence a portal to perceive, reason about, and interact with the physical world. Except for detailing the design philosophy and providing guidelines for the application of analytic concepts, this research also introduce about the infrastructure that has been built around analytic concepts. I aim for my research to contribute to addressing these questions: What is a proper abstraction of general concepts in the physical world for machine intelligence? How to systematically integrate structured priors with neural networks to constrain AI systems to comply with physical laws?\n        △ Less",
        "authors":"['Jianhua Sun', 'Cewu Lu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MInCo: Mitigating Information Conflicts in Distracted Visual Model-based Reinforcement Learning",
        "abstract":"Existing visual model-based reinforcement learning (MBRL) algorithms with observation reconstruction often suffer from information conflicts, making it difficult to learn compact representations and hence result in less robust policies, especially in the presence of task-irrelevant visual distractions. In this paper, we first reveal that the information conflicts in current visual MBRL algorithms stem from visual representation learning and latent dynamics modeling with an information-theoretic perspective. Based on this finding, we present a new algorithm to resolve information conflicts for visual MBRL, named MInCo, which mitigates information conflicts by leveraging negative-free contrastive learning, aiding in learning invariant representation and robust policies despite noisy observations. To prevent the dominance of visual representation learning, we introduce time-varying reweighting to bias the learning towards dynamics modeling as training proceeds. We evaluate our method on several robotic control tasks with dynamic background distractions. Our experiments demonstrate that MInCo learns invariant representations against background noise and consistently outperforms current state-of-the-art visual MBRL methods. Code is available at https:\/\/github.com\/ShiguangSun\/minco.\n        △ Less",
        "authors":"['Shiguang Sun', 'Hanbo Zhang', 'Zeyang Liu', 'Xinrui Yang', 'Lipeng Wan', 'Bing Yan', 'Xingyu Chen', 'Xuguang Lan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"OrbitZoo: Multi-Agent Reinforcement Learning Environment for Orbital Dynamics",
        "abstract":"The increasing number of satellites and orbital debris has made space congestion a critical issue, threatening satellite safety and sustainability. Challenges such as collision avoidance, station-keeping, and orbital maneuvering require advanced techniques to handle dynamic uncertainties and multi-agent interactions. Reinforcement learning (RL) has shown promise in this domain, enabling adaptive, autonomous policies for space operations; however, many existing RL frameworks rely on custom-built environments developed from scratch, which often use simplified models and require significant time to implement and validate the orbital dynamics, limiting their ability to fully capture real-world complexities. To address this, we introduce OrbitZoo, a versatile multi-agent RL environment built on a high-fidelity industry standard library, that enables realistic data generation, supports scenarios like collision avoidance and cooperative maneuvers, and ensures robust and accurate orbital dynamics. The environment is validated against a real satellite constellation, Starlink, achieving a Mean Absolute Percentage Error (MAPE) of 0.16% compared to real-world data. This validation ensures reliability for generating high-fidelity simulations and enabling autonomous and independent satellite operations.\n        △ Less",
        "authors":"['Alexandre Oliveira', 'Katarina Dyreby', 'Francisco Caldas', 'Cláudia Soares']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Vehicle Acceleration Prediction Considering Environmental Influence and Individual Driving Behavior",
        "abstract":"Accurate vehicle acceleration prediction is critical for intelligent driving control and energy efficiency management, particularly in environments with complex driving behavior dynamics. This paper proposes a general short-term vehicle acceleration prediction framework that jointly models environmental influence and individual driving behavior. The framework adopts a dual input design by incorporating environmental sequences, constructed from historical traffic variables such as percentile-based speed and acceleration statistics of multiple vehicles at specific spatial locations, capture group-level driving behavior influenced by the traffic environment. In parallel, individual driving behavior sequences represent motion characteristics of the target vehicle prior to the prediction point, reflecting personalized driving styles. These two inputs are processed using an LSTM Seq2Seq model enhanced with an attention mechanism, enabling accurate multi-step acceleration prediction. To demonstrate the effectiveness of the proposed method, an empirical study was conducted using high resolution radar video fused trajectory data collected from the exit section of the Guangzhou Baishi Tunnel. Drivers were clustered into three categories conservative, moderate, and aggressive based on key behavioral indicators, and a dedicated prediction model was trained for each group to account for driver heterogeneity.Experimental results show that the proposed method consistently outperforms four baseline models, yielding a 10.9% improvement in accuracy with the inclusion of historical traffic variables and a 33% improvement with driver classification. Although prediction errors increase with forecast distance, incorporating environment- and behavior-aware features significantly enhances model robustness.\n        △ Less",
        "authors":"['Wenxuan Wang', 'Lexing Zhang', 'Jiale Lei', 'Yin Feng', 'Hengxu Hu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Introducing COGENT3: An AI Architecture for Emergent Cognition",
        "abstract":"This paper presents COGENT3 (or Collective Growth and Entropy-modulated Triads System), a novel approach for emergent cognition integrating pattern formation networks with group influence dynamics. Contrasting with traditional strategies that rely on predetermined architectures, computational structures emerge dynamically in our framework through agent interactions. This enables a more flexible and adaptive system exhibiting characteristics reminiscent of human cognitive processes. The incorporation of temperature modulation and memory effects in COGENT3 closely integrates statistical mechanics, machine learning, and cognitive science.\n        △ Less",
        "authors":"['Eduardo Salazar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Predicting Soil Macronutrient Levels: A Machine Learning Approach Models Trained on pH, Conductivity, and Average Power of Acid-Base Solutions",
        "abstract":"Soil macronutrients, particularly potassium ions (K$^+$), are indispensable for plant health, underpinning various physiological and biological processes, and facilitating the management of both biotic and abiotic stresses. Deficient macronutrient content results in stunted growth, delayed maturation, and increased vulnerability to environmental stressors, thereby accentuating the imperative for precise soil nutrient monitoring. Traditional techniques such as chemical assays, atomic absorption spectroscopy, inductively coupled plasma optical emission spectroscopy, and electrochemical methods, albeit advanced, are prohibitively expensive and time-intensive, thus unsuitable for real-time macronutrient assessment. In this study, we propose an innovative soil testing protocol utilizing a dataset derived from synthetic solutions to model soil behaviour. The dataset encompasses physical properties including conductivity and pH, with a concentration on three key macronutrients: nitrogen (N), phosphorus (P), and potassium (K). Four machine learning algorithms were applied to the dataset, with random forest regressors and neural networks being selected for the prediction of soil nutrient concentrations. Comparative analysis with laboratory soil testing results revealed prediction errors of 23.6% for phosphorus and 16% for potassium using the random forest model, and 26.3% for phosphorus and 21.8% for potassium using the neural network model. This methodology illustrates a cost-effective and efficacious strategy for real-time soil nutrient monitoring, offering substantial advancements over conventional techniques and enhancing the capability to sustain optimal nutrient levels conducive to robust crop growth.\n        △ Less",
        "authors":"['Mridul Kumar', 'Deepali Jain', 'Zeeshan Saifi', 'Soami Daya Krishnananda']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transformer representation learning is necessary for dynamic multi-modal physiological data on small-cohort patients",
        "abstract":"Postoperative delirium (POD), a severe neuropsychiatric complication affecting nearly 50% of high-risk surgical patients, is defined as an acute disorder of attention and cognition, It remains significantly underdiagnosed in the intensive care units (ICUs) due to subjective monitoring methods. Early and accurate diagnosis of POD is critical and achievable. Here, we propose a POD prediction framework comprising a Transformer representation model followed by traditional machine learning algorithms. Our approaches utilizes multi-modal physiological data, including amplitude-integrated electroencephalography (aEEG), vital signs, electrocardiographic monitor data as well as hemodynamic parameters. We curated the first multi-modal POD dataset encompassing two patient types and evaluated the various Transformer architectures for representation learning. Empirical results indicate a consistent improvements of sensitivity and Youden index in patient TYPE I using Transformer representations, particularly our fusion adaptation of Pathformer. By enabling effective delirium diagnosis from postoperative day 1 to 3, our extensive experimental findings emphasize the potential of multi-modal physiological data and highlight the necessity of representation learning via multi-modal Transformer architecture in clinical diagnosis.\n        △ Less",
        "authors":"['Bingxu Wang', 'Yapeng Wang', 'Kunzhi Cai', 'Yuqi Zhang', 'Zeyi Zhou', 'Yachong Guo', 'Wei Wang', 'Qing Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Minimax Optimal Convergence of Gradient Descent in Logistic Regression via Large and Adaptive Stepsizes",
        "abstract":"We study $\\textit{gradient descent}$ (GD) for logistic regression on linearly separable data with stepsizes that adapt to the current risk, scaled by a constant hyperparameter $η$. We show that after at most $1\/γ^2$ burn-in steps, GD achieves a risk upper bounded by $\\exp(-Θ(η))$, where $γ$ is the margin of the dataset. As $η$ can be arbitrarily large, GD attains an arbitrarily small risk $\\textit{immediately after the burn-in steps}$, though the risk evolution may be $\\textit{non-monotonic}$.\n  We further construct hard datasets with margin $γ$, where any batch or online first-order method requires $Ω(1\/γ^2)$ steps to find a linear separator. Thus, GD with large, adaptive stepsizes is $\\textit{minimax optimal}$ among first-order batch methods. Notably, the classical $\\textit{Perceptron}$ (Novikoff, 1962), a first-order online method, also achieves a step complexity of $1\/γ^2$, matching GD even in constants.\n  Finally, our GD analysis extends to a broad class of loss functions and certain two-layer networks.\n        △ Less",
        "authors":"['Ruiqi Zhang', 'Jingfeng Wu', 'Licong Lin', 'Peter L. Bartlett']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PipeDec: Low-Latency Pipeline-based Inference with Dynamic Speculative Decoding towards Large-scale Models",
        "abstract":"Autoregressive large language model inference primarily consists of two stages: pre-filling and decoding. Decoding involves sequential computation for each token, which leads to significant latency. Speculative decoding is a technique that leverages the draft model combined with large model verification to enhance parallelism without sacrificing accuracy. However, existing external prediction methods face challenges in adapting to multi-node serial deployments. While they can maintain speedup under such conditions, the high latency of multi-node deployments ultimately results in low overall efficiency. We propose a speculative decoding framework named PipeDec to address the low global resource utilization of single tasks in pipeline deployments thereby reducing decoding latency. We integrate a draft model into the pipeline of the large model and immediately forward each prediction from the draft model to subsequent pipeline stages. A dynamic prediction tree manages prediction sequences across nodes, enabling efficient updating and pruning. This approach leverages the draft model's predictions to utilize all pipeline nodes for parallel decoding of a single task. Experiments were conducted using LLama3.2 1B as the draft model in conjunction with a 14-stage parallel pipeline to accelerate LLama3.1 70B by six different types of datasets. During the decoding phase of a single task, PipeDec achieved a 4.46x-7.79x speedup compared to traditional pipeline parallelism and a 2.2x-2.69x speedup compared to baseline tree-based speculative decoding methods. The code will be released after the review process.\n        △ Less",
        "authors":"['Haofei Yin', 'Mengbai Xiao', 'Rouzhou Lu', 'Xiao Zhang', 'Dongxiao Yu', 'Guanghui Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Lifting Factor Graphs with Some Unknown Factors for New Individuals",
        "abstract":"Lifting exploits symmetries in probabilistic graphical models by using a representative for indistinguishable objects, allowing to carry out query answering more efficiently while maintaining exact answers. In this paper, we investigate how lifting enables us to perform probabilistic inference for factor graphs containing unknown factors, i.e., factors whose underlying function of potential mappings is unknown. We present the Lifting Factor Graphs with Some Unknown Factors (LIFAGU) algorithm to identify indistinguishable subgraphs in a factor graph containing unknown factors, thereby enabling the transfer of known potentials to unknown potentials to ensure a well-defined semantics of the model and allow for (lifted) probabilistic inference. We further extend LIFAGU to incorporate additional background knowledge about groups of factors belonging to the same individual object. By incorporating such background knowledge, LIFAGU is able to further reduce the ambiguity of possible transfers of known potentials to unknown potentials.\n        △ Less",
        "authors":"['Malte Luttermann', 'Ralf Möller', 'Marcel Gehrke']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Corrected with the Latest Version: Make Robust Asynchronous Federated Learning Possible",
        "abstract":"As an emerging paradigm of federated learning, asynchronous federated learning offers significant speed advantages over traditional synchronous federated learning. Unlike synchronous federated learning, which requires waiting for all clients to complete updates before aggregation, asynchronous federated learning aggregates the models that have arrived in realtime, greatly improving training speed. However, this mechanism also introduces the issue of client model version inconsistency. When the differences between models of different versions during aggregation become too large, it may lead to conflicts, thereby reducing the models accuracy. To address this issue, this paper proposes an asynchronous federated learning version correction algorithm based on knowledge distillation, named FedADT. FedADT applies knowledge distillation before aggregating gradients, using the latest global model to correct outdated information, thus effectively reducing the negative impact of outdated gradients on the training process. Additionally, FedADT introduces an adaptive weighting function that adjusts the knowledge distillation weight according to different stages of training, helps mitigate the misleading effects caused by the poorer performance of the global model in the early stages of training. This method significantly improves the overall performance of asynchronous federated learning without adding excessive computational overhead. We conducted experimental comparisons with several classical algorithms, and the results demonstrate that FedADT achieves significant improvements over other asynchronous methods and outperforms all methods in terms of convergence speed.\n        △ Less",
        "authors":"['Chaoyi Lu', 'Yiding Sun', 'Pengbo Li', 'Zhichuan Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scalable Robust Bayesian Co-Clustering with Compositional ELBOs",
        "abstract":"Co-clustering exploits the duality of instances and features to simultaneously uncover meaningful groups in both dimensions, often outperforming traditional clustering in high-dimensional or sparse data settings. Although recent deep learning approaches successfully integrate feature learning and cluster assignment, they remain susceptible to noise and can suffer from posterior collapse within standard autoencoders. In this paper, we present the first fully variational Co-clustering framework that directly learns row and column clusters in the latent space, leveraging a doubly reparameterized ELBO to improve gradient signal-to-noise separation. Our unsupervised model integrates a Variational Deep Embedding with a Gaussian Mixture Model (GMM) prior for both instances and features, providing a built-in clustering mechanism that naturally aligns latent modes with row and column clusters. Furthermore, our regularized end-to-end noise learning Compositional ELBO architecture jointly reconstructs the data while regularizing against noise through the KL divergence, thus gracefully handling corrupted or missing inputs in a single training pipeline. To counteract posterior collapse, we introduce a scale modification that increases the encoder's latent means only in the reconstruction pathway, preserving richer latent representations without inflating the KL term. Finally, a mutual information-based cross-loss ensures coherent co-clustering of rows and columns. Empirical results on diverse real-world datasets from multiple modalities, numerical, textual, and image-based, demonstrate that our method not only preserves the advantages of prior Co-clustering approaches but also exceeds them in accuracy and robustness, particularly in high-dimensional or noisy settings.\n        △ Less",
        "authors":"['Ashwin Vinod', 'Chandrajit Bajaj']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Among Us: A Sandbox for Agentic Deception",
        "abstract":"Studying deception in AI agents is important and difficult due to the lack of model organisms and sandboxes that elicit the behavior without asking the model to act under specific conditions or inserting intentional backdoors. Extending upon $\\textit{AmongAgents}$, a text-based social-deduction game environment, we aim to fix this by introducing Among Us as a rich sandbox where LLM-agents exhibit human-style deception naturally while they think, speak, and act with other agents or humans. We introduce Deception ELO as an unbounded measure of deceptive capability, suggesting that frontier models win more because they're better at deception, not at detecting it. We evaluate the effectiveness of AI safety techniques (LLM-monitoring of outputs, linear probes on various datasets, and sparse autoencoders) for detecting lying and deception in Among Us, and find that they generalize very well out-of-distribution. We open-source our sandbox as a benchmark for future alignment research and hope that this is a good testbed to improve safety techniques to detect and remove agentically-motivated deception, and to anticipate deceptive abilities in LLMs.\n        △ Less",
        "authors":"['Satvik Golechha', 'Adrià Garriga-Alonso']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep-Learning-Directed Preventive Dynamic Security Control via Coordinated Demand Response",
        "abstract":"Unlike common faults, three-phase short-circuit faults in power systems pose significant challenges. These faults can lead to out-of-step (OOS) conditions and jeopardize the system's dynamic security. The rapid dynamics of these faults often exceed the time of protection actions, thus limiting the effectiveness of corrective schemes. This paper proposes an end-to-end deep-learning-based mechanism, namely, a convolutional neural network with an attention mechanism, to predict OOS conditions early and enhance the system's fault resilience. The results of the study demonstrate the effectiveness of the proposed algorithm in terms of early prediction and robustness against such faults in various operating conditions.\n        △ Less",
        "authors":"['Amin Masoumi', 'Mert Korkali']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Stochastic, Dynamic, Fluid Autonomy in Agentic AI: Implications for Authorship, Inventorship, and Liability",
        "abstract":"Agentic Artificial Intelligence (AI) systems, exemplified by OpenAI's DeepResearch, autonomously pursue goals, adapting strategies through implicit learning. Unlike traditional generative AI, which is reactive to user prompts, agentic AI proactively orchestrates complex workflows. It exhibits stochastic, dynamic, and fluid autonomy: its steps and outputs vary probabilistically (stochastic), it evolves based on prior interactions (dynamic), and it operates with significant independence within human-defined parameters, adapting to context (fluid). While this fosters complex, co-evolutionary human-machine interactions capable of generating uniquely synthesized creative outputs, it also irrevocably blurs boundaries--human and machine contributions become irreducibly entangled in intertwined creative processes. Consequently, agentic AI poses significant challenges to legal frameworks reliant on clear attribution: authorship doctrines struggle to disentangle ownership, intellectual property regimes strain to accommodate recursively blended novelty, and liability models falter as accountability diffuses across shifting loci of control. The central issue is not the legal treatment of human versus machine contributions, but the fundamental unmappability--the practical impossibility in many cases--of accurately attributing specific creative elements to either source. When retroactively parsing contributions becomes infeasible, applying distinct standards based on origin becomes impracticable. Therefore, we argue, legal and policy frameworks may need to treat human and machine contributions as functionally equivalent--not for moral or economic reasons, but as a pragmatic necessity.\n        △ Less",
        "authors":"['Anirban Mukherjee', 'Hannah Hanwen Chang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning-Based Multi-Criteria Decision Model for Site Selection Problems",
        "abstract":"Strategically locating sawmills is critical for the efficiency, profitability, and sustainability of timber supply chains, yet it involves a series of complex decision-making affected by various factors, such as proximity to resources and markets, proximity to roads and rail lines, distance from the urban area, slope, labor market, and existing sawmill data. Although conventional Multi-Criteria Decision-Making (MCDM) approaches utilize these factors while locating facilities, they are susceptible to bias since they rely heavily on expert opinions to determine the relative factor weights. Machine learning (ML) models provide an objective, data-driven alternative for site selection that derives these weights directly from the patterns in large datasets without requiring subjective weighting. Additionally, ML models autonomously identify critical features, eliminating the need for subjective feature selection. In this study, we propose integrated ML and MCDM methods and showcase the utility of this integrated model to improve sawmill location decisions via a case study in Mississippi. This integrated model is flexible and applicable to site selection problems across various industries.\n        △ Less",
        "authors":"['Mahid Ahmed', 'Ali Dogru', 'Chaoyang Zhang', 'Chao Meng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PIORF: Physics-Informed Ollivier-Ricci Flow for Long-Range Interactions in Mesh Graph Neural Networks",
        "abstract":"Recently, data-driven simulators based on graph neural networks have gained attention in modeling physical systems on unstructured meshes. However, they struggle with long-range dependencies in fluid flows, particularly in refined mesh regions. This challenge, known as the 'over-squashing' problem, hinders information propagation. While existing graph rewiring methods address this issue to some extent, they only consider graph topology, overlooking the underlying physical phenomena. We propose Physics-Informed Ollivier-Ricci Flow (PIORF), a novel rewiring method that combines physical correlations with graph topology. PIORF uses Ollivier-Ricci curvature (ORC) to identify bottleneck regions and connects these areas with nodes in high-velocity gradient nodes, enabling long-range interactions and mitigating over-squashing. Our approach is computationally efficient in rewiring edges and can scale to larger simulations. Experimental results on 3 fluid dynamics benchmark datasets show that PIORF consistently outperforms baseline models and existing rewiring methods, achieving up to 26.2 improvement.\n        △ Less",
        "authors":"['Youn-Yeol Yu', 'Jeongwhan Choi', 'Jaehyeon Park', 'Kookjin Lee', 'Noseong Park']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can You Count to Nine? A Human Evaluation Benchmark for Counting Limits in Modern Text-to-Video Models",
        "abstract":"Generative models have driven significant progress in a variety of AI tasks, including text-to-video generation, where models like Video LDM and Stable Video Diffusion can produce realistic, movie-level videos from textual instructions. Despite these advances, current text-to-video models still face fundamental challenges in reliably following human commands, particularly in adhering to simple numerical constraints. In this work, we present T2VCountBench, a specialized benchmark aiming at evaluating the counting capability of SOTA text-to-video models as of 2025. Our benchmark employs rigorous human evaluations to measure the number of generated objects and covers a diverse range of generators, covering both open-source and commercial models. Extensive experiments reveal that all existing models struggle with basic numerical tasks, almost always failing to generate videos with an object count of 9 or fewer. Furthermore, our comprehensive ablation studies explore how factors like video style, temporal dynamics, and multilingual inputs may influence counting performance. We also explore prompt refinement techniques and demonstrate that decomposing the task into smaller subtasks does not easily alleviate these limitations. Our findings highlight important challenges in current text-to-video generation and provide insights for future research aimed at improving adherence to basic numerical constraints.\n        △ Less",
        "authors":"['Xuyang Guo', 'Zekai Huang', 'Jiayan Huo', 'Yingyu Liang', 'Zhenmei Shi', 'Zhao Song', 'Jiahao Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Survey of Pathology Foundation Model: Progress and Future Directions",
        "abstract":"Computational pathology, analyzing whole slide images for automated cancer diagnosis, relies on the multiple instance learning framework where performance heavily depends on the feature extractor and aggregator. Recent Pathology Foundation Models (PFMs), pretrained on large-scale histopathology data, have significantly enhanced capabilities of extractors and aggregators but lack systematic analysis frameworks. This survey presents a hierarchical taxonomy organizing PFMs through a top-down philosophy that can be utilized to analyze FMs in any domain: model scope, model pretraining, and model design. Additionally, we systematically categorize PFM evaluation tasks into slide-level, patch-level, multimodal, and biological tasks, providing comprehensive benchmarking criteria. Our analysis identifies critical challenges in both PFM development (pathology-specific methodology, end-to-end pretraining, data-model scalability) and utilization (effective adaptation, model maintenance), paving the way for future directions in this promising field. Resources referenced in this survey are available at https:\/\/github.com\/BearCleverProud\/AwesomeWSI.\n        △ Less",
        "authors":"['Conghao Xiong', 'Hao Chen', 'Joseph J. Y. Sung']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Memory-Statistics Tradeoff in Continual Learning with Structural Regularization",
        "abstract":"We study the statistical performance of a continual learning problem with two linear regression tasks in a well-specified random design setting. We consider a structural regularization algorithm that incorporates a generalized $\\ell_2$-regularization tailored to the Hessian of the previous task for mitigating catastrophic forgetting. We establish upper and lower bounds on the joint excess risk for this algorithm. Our analysis reveals a fundamental trade-off between memory complexity and statistical efficiency, where memory complexity is measured by the number of vectors needed to define the structural regularization. Specifically, increasing the number of vectors in structural regularization leads to a worse memory complexity but an improved excess risk, and vice versa. Furthermore, our theory suggests that naive continual learning without regularization suffers from catastrophic forgetting, while structural regularization mitigates this issue. Notably, structural regularization achieves comparable performance to joint training with access to both tasks simultaneously. These results highlight the critical role of curvature-aware regularization for continual learning.\n        △ Less",
        "authors":"['Haoran Li', 'Jingfeng Wu', 'Vladimir Braverman']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Disparate Privacy Vulnerability: Targeted Attribute Inference Attacks and Defenses",
        "abstract":"As machine learning (ML) technologies become more prevalent in privacy-sensitive areas like healthcare and finance, eventually incorporating sensitive information in building data-driven algorithms, it is vital to scrutinize whether these data face any privacy leakage risks. One potential threat arises from an adversary querying trained models using the public, non-sensitive attributes of entities in the training data to infer their private, sensitive attributes, a technique known as the attribute inference attack. This attack is particularly deceptive because, while it may perform poorly in predicting sensitive attributes across the entire dataset, it excels at predicting the sensitive attributes of records from a few vulnerable groups, a phenomenon known as disparate vulnerability. This paper illustrates that an adversary can take advantage of this disparity to carry out a series of new attacks, showcasing a threat level beyond previous imagination. We first develop a novel inference attack called the disparity inference attack, which targets the identification of high-risk groups within the dataset. We then introduce two targeted variations of the attribute inference attack that can identify and exploit a vulnerable subset of the training data, marking the first instances of targeted attacks in this category, achieving significantly higher accuracy than untargeted versions. We are also the first to introduce a novel and effective disparity mitigation technique that simultaneously preserves model performance and prevents any risk of targeted attacks.\n        △ Less",
        "authors":"['Ehsanul Kabir', 'Lucas Craig', 'Shagufta Mehnaz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Contrastive and Variational Approaches in Self-Supervised Learning for Complex Data Mining",
        "abstract":"Complex data mining has wide application value in many fields, especially in the feature extraction and classification tasks of unlabeled data. This paper proposes an algorithm based on self-supervised learning and verifies its effectiveness through experiments. The study found that in terms of the selection of optimizer and learning rate, the combination of AdamW optimizer and 0.002 learning rate performed best in all evaluation indicators, indicating that the adaptive optimization method can improve the performance of the model in complex data mining tasks. In addition, the ablation experiment further analyzed the contribution of each module. The results show that contrastive learning, variational modules, and data augmentation strategies play a key role in the generalization ability and robustness of the model. Through the convergence curve analysis of the loss function, the experiment verifies that the method can converge stably during the training process and effectively avoid serious overfitting. Further experimental results show that the model has strong adaptability on different data sets, can effectively extract high-quality features from unlabeled data, and improves classification accuracy. At the same time, under different data distribution conditions, the method can still maintain high detection accuracy, proving its applicability in complex data environments. This study analyzed the role of self-supervised learning methods in complex data mining through systematic experiments and verified its advantages in improving feature extraction quality, optimizing classification performance, and enhancing model stability\n        △ Less",
        "authors":"['Yingbin Liang', 'Lu Dai', 'Shuo Shi', 'Minghao Dai', 'Junliang Du', 'Haige Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Artificial intelligence application in lymphoma diagnosis: from Convolutional Neural Network to Vision Transformer",
        "abstract":"Recently, vision transformers were shown to be capable of outperforming convolutional neural networks when pretrained on sufficiently large datasets. Vision transformer models show good accuracy on large scale datasets, with features of multi-modal training. Due to their promising feature detection, we aim to explore vision transformer models for diagnosis of anaplastic large cell lymphoma versus classical Hodgkin lymphoma using pathology whole slide images of HE slides. We compared the classification performance of the vision transformer to our previously designed convolutional neural network on the same dataset. The dataset includes whole slide images of HE slides for 20 cases, including 10 cases in each diagnostic category. From each whole slide image, 60 image patches having size of 100 by 100 pixels and at magnification of 20 were obtained to yield 1200 image patches, from which 90 percent were used for training, 9 percent for validation, and 10 percent for testing. The test results from the convolutional neural network model had previously shown an excellent diagnostic accuracy of 100 percent. The test results from the vision transformer model also showed a comparable accuracy at 100 percent. To the best of the authors' knowledge, this is the first direct comparison of predictive performance between a vision transformer model and a convolutional neural network model using the same dataset of lymphoma. Overall, convolutional neural network has a more mature architecture than vision transformer and is usually the best choice when large scale pretraining is not an available option. Nevertheless, our current study shows comparable and excellent accuracy of vision transformer compared to that of convolutional neural network even with a relatively small dataset of anaplastic large cell lymphoma and classical Hodgkin lymphoma.\n        △ Less",
        "authors":"['Daniel Rivera', 'Jacob Huddin', 'Alexander Banerjee', 'Rongzhen Zhang', 'Brenda Mai', 'Hanadi El Achi', 'Jacob Armstrong', 'Amer Wahed', 'Andy Nguyen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Comprehensive Survey of Challenges and Opportunities of Few-Shot Learning Across Multiple Domains",
        "abstract":"In a world where new domains are constantly discovered and machine learning (ML) is applied to automate new tasks every day, challenges arise with the number of samples available to train ML models. While the traditional ML training relies heavily on data volume, finding a large dataset with a lot of usable samples is not always easy, and often the process takes time. For instance, when a new human transmissible disease such as COVID-19 breaks out and there is an immediate surge for rapid diagnosis, followed by rapid isolation of infected individuals from healthy ones to contain the spread, there is an immediate need to create tools\/automation using machine learning models. At the early stage of an outbreak, it is not only difficult to obtain a lot of samples, but also difficult to understand the details about the disease, to process the data needed to train a traditional ML model. A solution for this can be a few-shot learning approach. This paper presents challenges and opportunities of few-shot approaches that vary across major domains, i.e., audio, image, text, and their combinations, with their strengths and weaknesses. This detailed understanding can help to adopt appropriate approaches applicable to different domains and applications.\n        △ Less",
        "authors":"['Andrea Gajic', 'Sudip Vhaduri']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Computational Efficient Informative Nonignorable Matrix Completion: A Row- and Column-Wise Matrix U-Statistic Pseudo-Likelihood Approach",
        "abstract":"In this study, we establish a unified framework to deal with the high dimensional matrix completion problem under flexible nonignorable missing mechanisms. Although the matrix completion problem has attracted much attention over the years, there are very sparse works that consider the nonignorable missing mechanism. To address this problem, we derive a row- and column-wise matrix U-statistics type loss function, with the nuclear norm for regularization. A singular value proximal gradient algorithm is developed to solve the proposed optimization problem. We prove the non-asymptotic upper bound of the estimation error's Frobenius norm and show the performance of our method through numerical simulations and real data analysis.\n        △ Less",
        "authors":"['Yuanhong A', 'Guoyu Zhang', 'Yongcheng Zeng', 'Bo Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-resolution Score-Based Variational Graphical Diffusion for Causal Disaster System Modeling and Inference",
        "abstract":"Complex systems with intricate causal dependencies challenge accurate prediction. Effective modeling requires precise physical process representation, integration of interdependent factors, and incorporation of multi-resolution observational data. These systems manifest in both static scenarios with instantaneous causal chains and temporal scenarios with evolving dynamics, complicating modeling efforts. Current methods struggle to simultaneously handle varying resolutions, capture physical relationships, model causal dependencies, and incorporate temporal dynamics, especially with inconsistently sampled data from diverse sources. We introduce Temporal-SVGDM: Score-based Variational Graphical Diffusion Model for Multi-resolution observations. Our framework constructs individual SDEs for each variable at its native resolution, then couples these SDEs through a causal score mechanism where parent nodes inform child nodes' evolution. This enables unified modeling of both immediate causal effects in static scenarios and evolving dependencies in temporal scenarios. In temporal models, state representations are processed through a sequence prediction model to predict future states based on historical patterns and causal relationships. Experiments on real-world datasets demonstrate improved prediction accuracy and causal understanding compared to existing methods, with robust performance under varying levels of background knowledge. Our model exhibits graceful degradation across different disaster types, successfully handling both static earthquake scenarios and temporal hurricane and wildfire scenarios, while maintaining superior performance even with limited data.\n        △ Less",
        "authors":"['Xuechun Li', 'Shan Gao', 'Susu Xu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Spatially-Heterogeneous Causal Bayesian Networks for Seismic Multi-Hazard Estimation: A Variational Approach with Gaussian Processes and Normalizing Flows",
        "abstract":"Post-earthquake hazard and impact estimation are critical for effective disaster response, yet current approaches face significant limitations. Traditional models employ fixed parameters regardless of geographical context, misrepresenting how seismic effects vary across diverse landscapes, while remote sensing technologies struggle to distinguish between co-located hazards. We address these challenges with a spatially-aware causal Bayesian network that decouples co-located hazards by modeling their causal relationships with location-specific parameters. Our framework integrates sensing observations, latent variables, and spatial heterogeneity through a novel combination of Gaussian Processes with normalizing flows, enabling us to capture how same earthquake produces different effects across varied geological and topographical features. Evaluations across three earthquakes demonstrate Spatial-VCBN achieves Area Under the Curve (AUC) improvements of up to 35.2% over existing methods. These results highlight the critical importance of modeling spatial heterogeneity in causal mechanisms for accurate disaster assessment, with direct implications for improving emergency response resource allocation.\n        △ Less",
        "authors":"['Xuechun Li', 'Shan Gao', 'Runyu Gao', 'Susu Xu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Foundation Models for Time Series: A Survey",
        "abstract":"Transformer-based foundation models have emerged as a dominant paradigm in time series analysis, offering unprecedented capabilities in tasks such as forecasting, anomaly detection, classification, trend analysis and many more time series analytical tasks. This survey provides a comprehensive overview of the current state of the art pre-trained foundation models, introducing a novel taxonomy to categorize them across several dimensions. Specifically, we classify models by their architecture design, distinguishing between those leveraging patch-based representations and those operating directly on raw sequences. The taxonomy further includes whether the models provide probabilistic or deterministic predictions, and whether they are designed to work with univariate time series or can handle multivariate time series out of the box. Additionally, the taxonomy encompasses model scale and complexity, highlighting differences between lightweight architectures and large-scale foundation models. A unique aspect of this survey is its categorization by the type of objective function employed during training phase. By synthesizing these perspectives, this survey serves as a resource for researchers and practitioners, providing insights into current trends and identifying promising directions for future research in transformer-based time series modeling.\n        △ Less",
        "authors":"['Siva Rama Krishna Kottapalli', 'Karthik Hubli', 'Sandeep Chandrashekhara', 'Garima Jain', 'Sunayana Hubli', 'Gayathri Botla', 'Ramesh Doddaiah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DiTaiListener: Controllable High Fidelity Listener Video Generation with Diffusion",
        "abstract":"Generating naturalistic and nuanced listener motions for extended interactions remains an open problem. Existing methods often rely on low-dimensional motion codes for facial behavior generation followed by photorealistic rendering, limiting both visual fidelity and expressive richness. To address these challenges, we introduce DiTaiListener, powered by a video diffusion model with multimodal conditions. Our approach first generates short segments of listener responses conditioned on the speaker's speech and facial motions with DiTaiListener-Gen. It then refines the transitional frames via DiTaiListener-Edit for a seamless transition. Specifically, DiTaiListener-Gen adapts a Diffusion Transformer (DiT) for the task of listener head portrait generation by introducing a Causal Temporal Multimodal Adapter (CTM-Adapter) to process speakers' auditory and visual cues. CTM-Adapter integrates speakers' input in a causal manner into the video generation process to ensure temporally coherent listener responses. For long-form video generation, we introduce DiTaiListener-Edit, a transition refinement video-to-video diffusion model. The model fuses video segments into smooth and continuous videos, ensuring temporal consistency in facial expressions and image quality when merging short video segments produced by DiTaiListener-Gen. Quantitatively, DiTaiListener achieves the state-of-the-art performance on benchmark datasets in both photorealism (+73.8% in FID on RealTalk) and motion representation (+6.1% in FD metric on VICO) spaces. User studies confirm the superior performance of DiTaiListener, with the model being the clear preference in terms of feedback, diversity, and smoothness, outperforming competitors by a significant margin.\n        △ Less",
        "authors":"['Maksim Siniukov', 'Di Chang', 'Minh Tran', 'Hongkun Gong', 'Ashutosh Chaubey', 'Mohammad Soleymani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Machine Learning Reviews Composition Dependent Thermal Stability in Halide Perovskites",
        "abstract":"Halide perovskites exhibit unpredictable properties in response to environmental stressors, due to several composition-dependent degradation mechanisms. In this work, we apply data visualization and machine learning (ML) techniques to reveal unexpected correlations between composition, temperature, and material properties while using high throughput, in situ environmental photoluminescence (PL) experiments. Correlation heatmaps show the strong influence of Cs content on film degradation, and dimensionality reduction visualization methods uncover clear composition-based data clusters. An extreme gradient boosting algorithm (XGBoost) effectively forecasts PL features for ten perovskite films with both composition-agnostic (>85% accuracy) and composition-dependent (>75% accuracy) model approaches, while elucidating the relative feature importance of composition (up to 99%). This model validates a previously unseen anti-correlation between Cs content and material thermal stability. Our ML-based framework can be expanded to any perovskite family, significantly reducing the analysis time currently employed to identify stable options for photovoltaics.\n        △ Less",
        "authors":"['Abigail R. Hering', 'Mansha Dubey', 'Elahe Hosseini', 'Meghna Srivastava', 'Yu An', 'Juan-Pablo Correa-Baena', 'Houman Homayoun', 'Marina S. Leite']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving Mixed-Criticality Scheduling with Reinforcement Learning",
        "abstract":"This paper introduces a novel reinforcement learning (RL) approach to scheduling mixed-criticality (MC) systems on processors with varying speeds. Building upon the foundation laid by [1], we extend their work to address the non-preemptive scheduling problem, which is known to be NP-hard. By modeling this scheduling challenge as a Markov Decision Process (MDP), we develop an RL agent capable of generating near-optimal schedules for real-time MC systems. Our RL-based scheduler prioritizes high-critical tasks while maintaining overall system performance.\n  Through extensive experiments, we demonstrate the scalability and effectiveness of our approach. The RL scheduler significantly improves task completion rates, achieving around 80% overall and 85% for high-criticality tasks across 100,000 instances of synthetic data and real data under varying system conditions. Moreover, under stable conditions without degradation, the scheduler achieves 94% overall task completion and 93% for high-criticality tasks. These results highlight the potential of RL-based schedulers in real-time and safety-critical applications, offering substantial improvements in handling complex and dynamic scheduling scenarios.\n        △ Less",
        "authors":"['Muhammad El-Mahdy', 'Nourhan Sakr', 'Rodrigo Carrasco']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Regression Discontinuity Design with Distribution-Valued Outcomes",
        "abstract":"This article introduces Regression Discontinuity Design (RDD) with Distribution-Valued Outcomes (R3D), extending the standard RDD framework to settings where the outcome is a distribution rather than a scalar. Such settings arise when treatment is assigned at a higher level of aggregation than the outcome-for example, when a subsidy is allocated based on a firm-level revenue cutoff while the outcome of interest is the distribution of employee wages within the firm. Since standard RDD methods cannot accommodate such two-level randomness, I propose a novel approach based on random distributions. The target estimand is a \"local average quantile treatment effect\", which averages across random quantiles. To estimate this target, I introduce two related approaches: one that extends local polynomial regression to random quantiles and another based on local Fréchet regression, a form of functional regression. For both estimators, I establish asymptotic normality and develop uniform, debiased confidence bands together with a data-driven bandwidth selection procedure. Simulations validate these theoretical properties and show existing methods to be biased and inconsistent in this setting. I then apply the proposed methods to study the effects of gubernatorial party control on within-state income distributions in the US, using a close-election design. The results suggest a classic equality-efficiency tradeoff under Democratic governorship, driven by reductions in income at the top of the distribution.\n        △ Less",
        "authors":"['David Van Dijcke']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"V-CEM: Bridging Performance and Intervenability in Concept-based Models",
        "abstract":"Concept-based eXplainable AI (C-XAI) is a rapidly growing research field that enhances AI model interpretability by leveraging intermediate, human-understandable concepts. This approach not only enhances model transparency but also enables human intervention, allowing users to interact with these concepts to refine and improve the model's performance. Concept Bottleneck Models (CBMs) explicitly predict concepts before making final decisions, enabling interventions to correct misclassified concepts. While CBMs remain effective in Out-Of-Distribution (OOD) settings with intervention, they struggle to match the performance of black-box models. Concept Embedding Models (CEMs) address this by learning concept embeddings from both concept predictions and input data, enhancing In-Distribution (ID) accuracy but reducing the effectiveness of interventions, especially in OOD scenarios. In this work, we propose the Variational Concept Embedding Model (V-CEM), which leverages variational inference to improve intervention responsiveness in CEMs. We evaluated our model on various textual and visual datasets in terms of ID performance, intervention responsiveness in both ID and OOD settings, and Concept Representation Cohesiveness (CRC), a metric we propose to assess the quality of the concept embedding representations. The results demonstrate that V-CEM retains CEM-level ID performance while achieving intervention effectiveness similar to CBM in OOD settings, effectively reducing the gap between interpretability (intervention) and generalization (performance).\n        △ Less",
        "authors":"['Francesco De Santis', 'Gabriele Ciravegna', 'Philippe Bich', 'Danilo Giordano', 'Tania Cerquitelli']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GREATERPROMPT: A Unified, Customizable, and High-Performing Open-Source Toolkit for Prompt Optimization",
        "abstract":"LLMs have gained immense popularity among researchers and the general public for its impressive capabilities on a variety of tasks. Notably, the efficacy of LLMs remains significantly dependent on the quality and structure of the input prompts, making prompt design a critical factor for their performance. Recent advancements in automated prompt optimization have introduced diverse techniques that automatically enhance prompts to better align model outputs with user expectations. However, these methods often suffer from the lack of standardization and compatibility across different techniques, limited flexibility in customization, inconsistent performance across model scales, and they often exclusively rely on expensive proprietary LLM APIs. To fill in this gap, we introduce GREATERPROMPT, a novel framework that democratizes prompt optimization by unifying diverse methods under a unified, customizable API while delivering highly effective prompts for different tasks. Our framework flexibly accommodates various model scales by leveraging both text feedback-based optimization for larger LLMs and internal gradient-based optimization for smaller models to achieve powerful and precise prompt improvements. Moreover, we provide a user-friendly Web UI that ensures accessibility for non-expert users, enabling broader adoption and enhanced performance across various user groups and application scenarios. GREATERPROMPT is available at https:\/\/github.com\/psunlpgroup\/GreaterPrompt via GitHub, PyPI, and web user interfaces.\n        △ Less",
        "authors":"['Wenliang Zheng', 'Sarkar Snigdha Sarathi Das', 'Yusen Zhang', 'Rui Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Clinical ModernBERT: An efficient and long context encoder for biomedical text",
        "abstract":"We introduce Clinical ModernBERT, a transformer based encoder pretrained on large scale biomedical literature, clinical notes, and medical ontologies, incorporating PubMed abstracts, MIMIC IV clinical data, and medical codes with their textual descriptions. Building on ModernBERT the current state of the art natural language text encoder featuring architectural upgrades such as rotary positional embeddings (RoPE), Flash Attention, and extended context length up to 8,192 tokens our model adapts these innovations specifically for biomedical and clinical domains. Clinical ModernBERT excels at producing semantically rich representations tailored for long context tasks. We validate this both by analyzing its pretrained weights and through empirical evaluation on a comprehensive suite of clinical NLP benchmarks.\n        △ Less",
        "authors":"['Simon A. Lee', 'Anthony Wu', 'Jeffrey N. Chiang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Practical Poisoning Attacks against Retrieval-Augmented Generation",
        "abstract":"Large language models (LLMs) have demonstrated impressive natural language processing abilities but face challenges such as hallucination and outdated knowledge. Retrieval-Augmented Generation (RAG) has emerged as a state-of-the-art approach to mitigate these issues. While RAG enhances LLM outputs, it remains vulnerable to poisoning attacks. Recent studies show that injecting poisoned text into the knowledge database can compromise RAG systems, but most existing attacks assume that the attacker can insert a sufficient number of poisoned texts per query to outnumber correct-answer texts in retrieval, an assumption that is often unrealistic. To address this limitation, we propose CorruptRAG, a practical poisoning attack against RAG systems in which the attacker injects only a single poisoned text, enhancing both feasibility and stealth. Extensive experiments across multiple datasets demonstrate that CorruptRAG achieves higher attack success rates compared to existing baselines.\n        △ Less",
        "authors":"['Baolei Zhang', 'Yuxi Chen', 'Minghong Fang', 'Zhuqing Liu', 'Lihai Nie', 'Tong Li', 'Zheli Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DeepOHeat-v1: Efficient Operator Learning for Fast and Trustworthy Thermal Simulation and Optimization in 3D-IC Design",
        "abstract":"Thermal analysis is crucial in three-dimensional integrated circuit (3D-IC) design due to increased power density and complex heat dissipation paths. Although operator learning frameworks such as DeepOHeat have demonstrated promising preliminary results in accelerating thermal simulation, they face critical limitations in prediction capability for multi-scale thermal patterns, training efficiency, and trustworthiness of results during design optimization. This paper presents DeepOHeat-v1, an enhanced physics-informed operator learning framework that addresses these challenges through three key innovations. First, we integrate Kolmogorov-Arnold Networks with learnable activation functions as trunk networks, enabling an adaptive representation of multi-scale thermal patterns. This approach achieves a $1.25\\times$ and $6.29\\times$ reduction in error in two representative test cases. Second, we introduce a separable training method that decomposes the basis function along the coordinate axes, achieving $62\\times$ training speedup and $31\\times$ GPU memory reduction in our baseline case, and enabling thermal analysis at resolutions previously infeasible due to GPU memory constraints. Third, we propose a confidence score to evaluate the trustworthiness of the predicted results, and further develop a hybrid optimization workflow that combines operator learning with finite difference (FD) using Generalized Minimal Residual (GMRES) method for incremental solution refinement, enabling efficient and trustworthy thermal optimization. Experimental results demonstrate that DeepOHeat-v1 achieves accuracy comparable to optimization using high-fidelity finite difference solvers, while speeding up the entire optimization process by $70.6\\times$ in our test cases, effectively minimizing the peak temperature through optimal placement of heat-generating components.\n        △ Less",
        "authors":"['Xinling Yu', 'Ziyue Liu', 'Hai Li', 'Yixing Li', 'Xin Ai', 'Zhiyu Zeng', 'Ian Young', 'Zheng Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A New Approach to Controlling Linear Dynamical Systems",
        "abstract":"We propose a new method for controlling linear dynamical systems under adversarial disturbances and cost functions. Our algorithm achieves a running time that scales polylogarithmically with the inverse of the stability margin, improving upon prior methods with polynomial dependence maintaining the same regret guarantees. The technique, which may be of independent interest, is based on a novel convex relaxation that approximates linear control policies using spectral filters constructed from the eigenvectors of a specific Hankel matrix.\n        △ Less",
        "authors":"['Anand Brahmbhatt', 'Gon Buzaglo', 'Sofiia Druchyna', 'Elad Hazan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Batch Bayesian Optimization for High-Dimensional Experimental Design: Simulation and Visualization",
        "abstract":"Bayesian Optimization (BO) is increasingly used to guide experimental optimization tasks. To elucidate BO behavior in noisy and high-dimensional settings typical for materials science applications, we perform batch BO of two six-dimensional test functions: an Ackley function representing a needle-in-a-haystack problem and a Hartmann function representing a problem with a false maximum with a value close to the global maximum. We show learning curves, performance metrics, and visualization to effectively track the evolution of optimization in high dimensions and evaluate how they are affected by noise, batch-picking method, choice of acquisition function,and its exploration hyperparameter values. We find that the effects of noise depend on the problem landscape; therefore, prior knowledge of the domain structure and noise level is needed when designing BO. The Ackley function optimization is significantly degraded by noise with a complete loss of ground truth resemblance when noise equals 10 % of the maximum objective value. For the Hartmann function, even in the absence of noise, a significant fraction of the initial samplings identify the false maximum instead of the ground truth maximum as the optimum of the function; with increasing noise, BO remains effective, albeit with increasing probability of landing on the false maximum. This study systematically highlights the critical issues when setting up BO and choosing synthetic data to test experimental design. The results and methodology will facilitate wider utilization of BO in guiding experiments, specifically in high-dimensional settings.\n        △ Less",
        "authors":"['Imon Mia', 'Armi Tiihonen', 'Anna Ernst', 'Anusha Srivastava', 'Tonio Buonassisi', 'William Vandenberghe', 'Julia W. P. Hsu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Analysis of Robustness of a Large Game Corpus",
        "abstract":"Procedural content generation via machine learning (PCGML) in games involves using machine learning techniques to create game content such as maps and levels. 2D tile-based game levels have consistently served as a standard dataset for PCGML because they are a simplified version of game levels while maintaining the specific constraints typical of games, such as being solvable. In this work, we highlight the unique characteristics of game levels, including their structured discrete data nature, the local and global constraints inherent in the games, and the sensitivity of the game levels to small changes in input. We define the robustness of data as a measure of sensitivity to small changes in input that cause a change in output, and we use this measure to analyze and compare these levels to state-of-the-art machine learning datasets, showcasing the subtle differences in their nature. We also constructed a large dataset from four games inspired by popular classic tile-based games that showcase these characteristics and address the challenge of sparse data in PCGML by providing a significantly larger dataset than those currently available.\n        △ Less",
        "authors":"['Mahsa Bazzaz', 'Seth Cooper']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Language Models Are Implicitly Continuous",
        "abstract":"Language is typically modelled with discrete sequences. However, the most successful approaches to language modelling, namely neural networks, are continuous and smooth function approximators. In this work, we show that Transformer-based language models implicitly learn to represent sentences as continuous-time functions defined over a continuous input space. This phenomenon occurs in most state-of-the-art Large Language Models (LLMs), including Llama2, Llama3, Phi3, Gemma, Gemma2, and Mistral, and suggests that LLMs reason about language in ways that fundamentally differ from humans. Our work formally extends Transformers to capture the nuances of time and space continuity in both input and output space. Our results challenge the traditional interpretation of how LLMs understand language, with several linguistic and engineering implications.\n        △ Less",
        "authors":"['Samuele Marro', 'Davide Evangelista', 'X. Angelo Huang', 'Emanuele La Malfa', 'Michele Lombardi', 'Michael Wooldridge']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Have Large Language Models Learned to Reason? A Characterization via 3-SAT Phase Transition",
        "abstract":"Large Language Models (LLMs) have been touted as AI models possessing advanced reasoning abilities. In theory, autoregressive LLMs with Chain-of-Thought (CoT) can perform more serial computations to solve complex reasoning tasks. However, recent studies suggest that, despite this capacity, LLMs do not truly learn to reason but instead fit on statistical features. To study the reasoning capabilities in a principled fashion, we adopt a computational theory perspective and propose an experimental protocol centered on 3-SAT -- the prototypical NP-complete problem lying at the core of logical reasoning and constraint satisfaction tasks. Specifically, we examine the phase transitions in random 3-SAT and characterize the reasoning abilities of state-of-the-art LLMs by varying the inherent hardness of the problem instances. By comparing DeepSeek R1 with other LLMs, our findings reveal two key insights (1) LLM accuracy drops significantly on harder instances, suggesting all current models struggle when statistical shortcuts are unavailable (2) Unlike other LLMs, R1 shows signs of having learned the underlying reasoning. Following a principled experimental protocol, our study moves beyond the benchmark-driven evidence often found in LLM reasoning research. Our findings highlight important gaps and suggest clear directions for future research.\n        △ Less",
        "authors":"['Rishi Hazra', 'Gabriele Venturato', 'Pedro Zuidberg Dos Martires', 'Luc De Raedt']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Random Normed k-Means: A Paradigm-Shift in Clustering within Probabilistic Metric Spaces",
        "abstract":"Existing approaches remain largely constrained by traditional distance metrics, limiting their effectiveness in handling random data. In this work, we introduce the first k-means variant in the literature that operates within a probabilistic metric space, replacing conventional distance measures with a well-defined distance distribution function. This pioneering approach enables more flexible and robust clustering in both deterministic and random datasets, establishing a new foundation for clustering in stochastic environments. By adopting a probabilistic perspective, our method not only introduces a fresh paradigm but also establishes a rigorous theoretical framework that is expected to serve as a key reference for future clustering research involving random data. Extensive experiments on diverse real and synthetic datasets assess our model's effectiveness using widely recognized evaluation metrics, including Silhouette, Davies-Bouldin, Calinski Harabasz, the adjusted Rand index, and distortion. Comparative analyses against established methods such as k-means++, fuzzy c-means, and kernel probabilistic k-means demonstrate the superior performance of our proposed random normed k-means (RNKM) algorithm. Notably, RNKM exhibits a remarkable ability to identify nonlinearly separable structures, making it highly effective in complex clustering scenarios. These findings position RNKM as a groundbreaking advancement in clustering research, offering a powerful alternative to traditional techniques while addressing a long-standing gap in the literature. By bridging probabilistic metrics with clustering, this study provides a foundational reference for future developments and opens new avenues for advanced data analysis in dynamic, data-driven applications.\n        △ Less",
        "authors":"['Abderrafik Laakel Hemdanou', 'Youssef Achtoun', 'Mohammed Lamarti Sefian', 'Ismail Tahiri', 'Abdellatif El Afia']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Exploration-free Method for a Linear Stochastic Bandit Driven by a Linear Gaussian Dynamical System",
        "abstract":"In stochastic multi-armed bandits, a major problem the learner faces is the trade-off between exploration and exploitation. Recently, exploration-free methods -- methods that commit to the action predicted to return the highest reward -- have been studied from the perspective of linear bandits. In this paper, we introduce a linear bandit setting where the reward is the output of a linear Gaussian dynamical system. Motivated by a problem encountered in hyperparameter optimization for reinforcement learning, where the number of actions is much higher than the number of training iterations, we propose Kalman filter Observability Dependent Exploration (KODE), an exploration-free method that utilizes the Kalman filter predictions to select actions. Our major contribution of this work is our analysis of the performance of the proposed method, which is dependent on the observability properties of the underlying linear Gaussian dynamical system. We evaluate KODE via two different metrics: regret, which is the cumulative expected difference between the highest possible reward and the reward sampled by KODE, and action alignment, which measures how closely KODE's chosen action aligns with the linear Gaussian dynamical system's state variable. To provide intuition on the performance, we prove that KODE implicitly encourages the learner to explore actions depending on the observability of the linear Gaussian dynamical system. This method is compared to several well-known stochastic multi-armed bandit algorithms to validate our theoretical results.\n        △ Less",
        "authors":"['Jonathan Gornet', 'Yilin Mo', 'Bruno Sinopoli']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RF-BayesPhysNet: A Bayesian rPPG Uncertainty Estimation Method for Complex Scenarios",
        "abstract":"Remote photoplethysmography (rPPG) technology infers heart rate by capturing subtle color changes in facial skin\n  using a camera, demonstrating great potential in non-contact heart rate measurement. However, measurement\n  accuracy significantly decreases in complex scenarios such as lighting changes and head movements compared\n  to ideal laboratory conditions. Existing deep learning models often neglect the quantification of measurement\n  uncertainty, limiting their credibility in dynamic scenes. To address the issue of insufficient rPPG measurement\n  reliability in complex scenarios, this paper introduces Bayesian neural networks to the rPPG field for the first time,\n  proposing the Robust Fusion Bayesian Physiological Network (RF-BayesPhysNet), which can model both aleatoric\n  and epistemic uncertainty. It leverages variational inference to balance accuracy and computational efficiency.\n  Due to the current lack of uncertainty estimation metrics in the rPPG field, this paper also proposes a new set of\n  methods, using Spearman correlation coefficient, prediction interval coverage, and confidence interval width, to\n  measure the effectiveness of uncertainty estimation methods under different noise conditions. Experiments show\n  that the model, with only double the parameters compared to traditional network models, achieves a MAE of 2.56\n  on the UBFC-RPPG dataset, surpassing most models. It demonstrates good uncertainty estimation capability\n  in no-noise and low-noise conditions, providing prediction confidence and significantly enhancing robustness in\n  real-world applications. We have open-sourced the code at https:\/\/github.com\/AIDC-rPPG\/RF-Net\n        △ Less",
        "authors":"['Rufei Ma', 'Chao Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Opening the Black-Box: Symbolic Regression with Kolmogorov-Arnold Networks for Energy Applications",
        "abstract":"While most modern machine learning methods offer speed and accuracy, few promise interpretability or explainability -- two key features necessary for highly sensitive industries, like medicine, finance, and engineering. Using eight datasets representative of one especially sensitive industry, nuclear power, this work compares a traditional feedforward neural network (FNN) to a Kolmogorov-Arnold Network (KAN). We consider not only model performance and accuracy, but also interpretability through model architecture and explainability through a post-hoc SHAP analysis. In terms of accuracy, we find KANs and FNNs comparable across all datasets, when output dimensionality is limited. KANs, which transform into symbolic equations after training, yield perfectly interpretable models while FNNs remain black-boxes. Finally, using the post-hoc explainability results from Kernel SHAP, we find that KANs learn real, physical relations from experimental data, while FNNs simply produce statistically accurate results. Overall, this analysis finds KANs a promising alternative to traditional machine learning methods, particularly in applications requiring both accuracy and comprehensibility.\n        △ Less",
        "authors":"['Nataly R. Panczyk', 'Omer F. Erdem', 'Majdi I. Radaideh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Stochastic Variational Inference with Tuneable Stochastic Annealing",
        "abstract":"In this paper, we exploit the observation that stochastic variational inference (SVI) is a form of annealing and present a modified SVI approach -- applicable to both large and small datasets -- that allows the amount of annealing done by SVI to be tuned. We are motivated by the fact that, in SVI, the larger the batch size the more approximately Gaussian is the intrinsic noise, but the smaller its variance. This low variance reduces the amount of annealing which is needed to escape bad local optimal solutions. We propose a simple method for achieving both goals of having larger variance noise to escape bad local optimal solutions and more data information to obtain more accurate gradient directions. The idea is to set an actual batch size, which may be the size of the data set, and a smaller effective batch size that matches the larger level of variance at this smaller batch size. The result is an approximation to the maximum entropy stochastic gradient at this variance level. We theoretically motivate our approach for the framework of conjugate exponential family models and illustrate the method empirically on the probabilistic matrix factorization collaborative filter, the Latent Dirichlet Allocation topic model, and the Gaussian mixture model.\n        △ Less",
        "authors":"['John Paisley', 'Ghazal Fazelnia', 'Brian Barr']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient FPGA-accelerated Convolutional Neural Networks for Cloud Detection on CubeSats",
        "abstract":"We present the implementation of four FPGA-accelerated convolutional neural network (CNN) models for onboard cloud detection in resource-constrained CubeSat missions, leveraging Xilinx's Vitis AI (VAI) framework and Deep Learning Processing Unit (DPU), a programmable engine with pre-implemented, parameterizable IP cores optimized for deep neural networks, on a Zynq UltraScale+ MPSoC. This study explores both pixel-wise (Pixel-Net and Patch-Net) and image-wise (U-Net and Scene-Net) models to benchmark trade-offs in accuracy, latency, and model complexity. Applying channel pruning, we achieved substantial reductions in model parameters (up to 98.6%) and floating-point operations (up to 90.7%) with minimal accuracy loss. Furthermore, the VAI tool was used to quantize the models to 8-bit precision, ensuring optimized hardware performance with negligible impact on accuracy. All models retained high accuracy post-FPGA integration, with a cumulative maximum accuracy drop of only 0.6% after quantization and pruning. The image-wise Scene-Net and U-Net models demonstrated strong real-time inference capabilities, achieving frame rates per second of 57.14 and 37.45, respectively, with power consumption of around 2.5 W, surpassing state-of-the-art onboard cloud detection solutions. Our approach underscores the potential of DPU-based hardware accelerators to expand the processing capabilities of small satellites, enabling efficient and flexible onboard CNN-based applications.\n        △ Less",
        "authors":"['Angela Cratere', 'M. Salim Farissi', 'Andrea Carbone', 'Marcello Asciolla', 'Maria Rizzi', \"Francesco Dell'Olio\", 'Augusto Nascetti', 'Dario Spiller']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using Attention Sinks to Identify and Evaluate Dormant Heads in Pretrained LLMs",
        "abstract":"Multi-head attention is foundational to large language models (LLMs), enabling different heads to have diverse focus on relevant input tokens. However, learned behaviors like attention sinks, where the first token receives most attention despite limited semantic importance, challenge our understanding of multi-head attention. To analyze this phenomenon, we propose a new definition for attention heads dominated by attention sinks, known as dormant attention heads. We compare our definition to prior work in a model intervention study where we test whether dormant heads matter for inference by zeroing out the output of dormant attention heads. Using six pretrained models and five benchmark datasets, we find our definition to be more model and dataset-agnostic. Using our definition on most models, more than 4% of a model's attention heads can be zeroed while maintaining average accuracy, and zeroing more than 14% of a model's attention heads can keep accuracy to within 1% of the pretrained model's average accuracy. Further analysis reveals that dormant heads emerge early in pretraining and can transition between dormant and active states during pretraining. Additionally, we provide evidence that they depend on characteristics of the input text.\n        △ Less",
        "authors":"['Pedro Sandoval-Segura', 'Xijun Wang', 'Ashwinee Panda', 'Micah Goldblum', 'Ronen Basri', 'Tom Goldstein', 'David Jacobs']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Concept-based Rubrics Improve LLM Formative Assessment and Data Synthesis",
        "abstract":"Formative assessment in STEM topics aims to promote student learning by identifying students' current understanding, thus targeting how to promote further learning. Previous studies suggest that the assessment performance of current generative large language models (LLMs) on constructed responses to open-ended questions is significantly lower than that of supervised classifiers trained on high-quality labeled data. However, we demonstrate that concept-based rubrics can significantly enhance LLM performance, which narrows the gap between LLMs as off-the shelf assessment tools, and smaller supervised models, which need large amounts of training data. For datasets where concept-based rubrics allow LLMs to achieve strong performance, we show that the concept-based rubrics help the same LLMs generate high quality synthetic data for training lightweight, high-performance supervised models. Our experiments span diverse STEM student response datasets with labels of varying quality, including a new real-world dataset that contains some AI-assisted responses, which introduces additional considerations.\n        △ Less",
        "authors":"['Yuchen Wei', 'Dennis Pearl', 'Matthew Beckman', 'Rebecca J. Passonneau']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HeterMoE: Efficient Training of Mixture-of-Experts Models on Heterogeneous GPUs",
        "abstract":"The Mixture-of-Experts (MoE) architecture has become increasingly popular as a method to scale up large language models (LLMs). To save costs, heterogeneity-aware training solutions have been proposed to utilize GPU clusters made up of both newer and older-generation GPUs. However, existing solutions are agnostic to the performance characteristics of different MoE model components (i.e., attention and expert) and do not fully utilize each GPU's compute capability.\n  In this paper, we introduce HeterMoE, a system to efficiently train MoE models on heterogeneous GPUs. Our key insight is that newer GPUs significantly outperform older generations on attention due to architectural advancements, while older GPUs are still relatively efficient for experts. HeterMoE disaggregates attention and expert computation, where older GPUs are only assigned with expert modules. Through the proposed zebra parallelism, HeterMoE overlaps the computation on different GPUs, in addition to employing an asymmetric expert assignment strategy for fine-grained load balancing to minimize GPU idle time. Our evaluation shows that HeterMoE achieves up to 2.3x speed-up compared to existing MoE training systems, and 1.4x compared to an optimally balanced heterogeneity-aware solution. HeterMoE efficiently utilizes older GPUs by maintaining 95% training throughput on average, even with half of the GPUs in a homogeneous A40 cluster replaced with V100.\n        △ Less",
        "authors":"['Yongji Wu', 'Xueshen Liu', 'Shuowei Jin', 'Ceyu Xu', 'Feng Qian', 'Z. Morley Mao', 'Matthew Lentz', 'Danyang Zhuo', 'Ion Stoica']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CREASE-2D Analysis of Small Angle X-ray Scattering Data from Supramolecular Dipeptide Systems",
        "abstract":"In this paper, we extend a recently developed machine-learning (ML) based CREASE-2D method to analyze the entire two-dimensional (2D) scattering pattern obtained from small angle X-ray scattering measurements of supramolecular dipeptide micellar systems. Traditional analysis of such scattering data would involve use of approximate or incorrect analytical models to fit to azimuthally-averaged 1D scattering patterns that can miss the anisotropic arrangements. Analysis of the 2D scattering profiles of such micellar solutions using CREASE-2D allows us to understand both isotropic and anisotropic structural arrangements that are present in these systems of assembled dipeptides in water and in the presence of added solvents\/salts. CREASE-2D outputs distributions of relevant structural features including ones that cannot be identified with existing analytical models (e.g., assembled tubes, cross-sectional eccentricity, tortuosity, orientational order). The representative three-dimensional (3D) real-space structures for the optimized values of these structural features further facilitate visualization of the structures. Through this detailed interpretation of these 2D SAXS profiles we are able to characterize the shapes of the assembled tube structures as a function of dipeptide chemistry, solution conditions with varying salts and solvents, and relative concentrations of all components. This paper demonstrates how CREASE-2D analysis of entire SAXS profiles can provide an unprecedented level of understanding of structural arrangements which has not been possible through traditional analytical model fits to the 1D SAXS data.\n        △ Less",
        "authors":"['Nitant Gupta', 'Sri V. V. R. Akepati', 'Simona Bianco', 'Jay Shah', 'Dave J. Adams', 'Arthi Jayaraman']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving World Models using Deep Supervision with Linear Probes",
        "abstract":"Developing effective world models is crucial for creating artificial agents that can reason about and navigate complex environments. In this paper, we investigate a deep supervision technique for encouraging the development of a world model in a network trained end-to-end to predict the next observation. While deep supervision has been widely applied for task-specific learning, our focus is on improving the world models. Using an experimental environment based on the Flappy Bird game, where the agent receives only LIDAR measurements as observations, we explore the effect of adding a linear probe component to the network's loss function. This additional term encourages the network to encode a subset of the true underlying world features into its hidden state. Our experiments demonstrate that this supervision technique improves both training and test performance, enhances training stability, and results in more easily decodable world features -- even for those world features which were not included in the training. Furthermore, we observe a reduced distribution drift in networks trained with the linear probe, particularly during high-variability phases of the game (flying between successive pipe encounters). Including the world features loss component roughly corresponded to doubling the model size, suggesting that the linear probe technique is particularly beneficial in compute-limited settings or when aiming to achieve the best performance with smaller models. These findings contribute to our understanding of how to develop more robust and sophisticated world models in artificial agents, paving the way for further advancements in this field.\n        △ Less",
        "authors":"['Andrii Zahorodnii']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can ChatGPT Learn My Life From a Week of First-Person Video?",
        "abstract":"Motivated by recent improvements in generative AI and wearable camera devices (e.g. smart glasses and AI-enabled pins), I investigate the ability of foundation models to learn about the wearer's personal life through first-person camera data. To test this, I wore a camera headset for 54 hours over the course of a week, generated summaries of various lengths (e.g. minute-long, hour-long, and day-long summaries), and fine-tuned both GPT-4o and GPT-4o-mini on the resulting summary hierarchy. By querying the fine-tuned models, we are able to learn what the models learned about me. The results are mixed: Both models learned basic information about me (e.g. approximate age, gender). Moreover, GPT-4o correctly deduced that I live in Pittsburgh, am a PhD student at CMU, am right-handed, and have a pet cat. However, both models also suffered from hallucination and would make up names for the individuals present in the video footage of my life.\n        △ Less",
        "authors":"['Keegan Harris']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models",
        "abstract":"Tree-Ring Watermarking is a significant technique for authenticating AI-generated images. However, its effectiveness in rectified flow-based models remains unexplored, particularly given the inherent challenges of these models with noise latent inversion. Through extensive experimentation, we evaluated and compared the detection and separability of watermarks between SD 2.1 and FLUX.1-dev models. By analyzing various text guidance configurations and augmentation attacks, we demonstrate how inversion limitations affect both watermark recovery and the statistical separation between watermarked and unwatermarked images. Our findings provide valuable insights into the current limitations of Tree-Ring Watermarking in the current SOTA models and highlight the critical need for improved inversion methods to achieve reliable watermark detection and separability. The official implementation, dataset release and all experimental results are available at this \\href{https:\/\/github.com\/dsgiitr\/flux-watermarking}{\\textbf{link}}.\n        △ Less",
        "authors":"['Ved Umrajkar', 'Aakash Kumar Singh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Interpretable Multimodal Learning for Tumor Protein-Metal Binding: Progress, Challenges, and Perspectives",
        "abstract":"In cancer therapeutics, protein-metal binding mechanisms critically govern drug pharmacokinetics and targeting efficacy, thereby fundamentally shaping the rational design of anticancer metallodrugs. While conventional laboratory methods used to study such mechanisms are often costly, low throughput, and limited in capturing dynamic biological processes, machine learning (ML) has emerged as a promising alternative. Despite increasing efforts to develop protein-metal binding datasets and ML algorithms, the application of ML in tumor protein-metal binding remains limited. Key challenges include a shortage of high-quality, tumor-specific datasets, insufficient consideration of multiple data modalities, and the complexity of interpreting results due to the ''black box'' nature of complex ML models. This paper summarizes recent progress and ongoing challenges in using ML to predict tumor protein-metal binding, focusing on data, modeling, and interpretability. We present multimodal protein-metal binding datasets and outline strategies for acquiring, curating, and preprocessing them for training ML models. Moreover, we explore the complementary value provided by different data modalities and examine methods for their integration. We also review approaches for improving model interpretability to support more trustworthy decisions in cancer research. Finally, we offer our perspective on research opportunities and propose strategies to address the scarcity of tumor protein data and the limited number of predictive models for tumor protein-metal binding. We also highlight two promising directions for effective metal-based drug design: integrating protein-protein interaction data to provide structural insights into metal-binding events and predicting structural changes in tumor proteins after metal binding.\n        △ Less",
        "authors":"['Xiaokun Liu', 'Sayedmohammadreza Rastegari', 'Yijun Huang', 'Sxe Chang Cheong', 'Weikang Liu', 'Wenjie Zhao', 'Qihao Tian', 'Hongming Wang', 'Shuo Zhou', 'Yingjie Guo', 'Sina Tabakhi', 'Xianyuan Liu', 'Zheqing Zhu', 'Wei Sang', 'Haiping Lu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Hybrid Wavelet-Fourier Method for Next-Generation Conditional Diffusion Models",
        "abstract":"We present a novel generative modeling framework,Wavelet-Fourier-Diffusion, which adapts the diffusion paradigm to hybrid frequency representations in order to synthesize high-quality, high-fidelity images with improved spatial localization. In contrast to conventional diffusion models that rely exclusively on additive noise in pixel space, our approach leverages a multi-transform that combines wavelet sub-band decomposition with partial Fourier steps. This strategy progressively degrades and then reconstructs images in a hybrid spectral domain during the forward and reverse diffusion processes. By supplementing traditional Fourier-based analysis with the spatial localization capabilities of wavelets, our model can capture both global structures and fine-grained features more effectively. We further extend the approach to conditional image generation by integrating embeddings or conditional features via cross-attention. Experimental evaluations on CIFAR-10, CelebA-HQ, and a conditional ImageNet subset illustrate that our method achieves competitive or superior performance relative to baseline diffusion models and state-of-the-art GANs, as measured by Fréchet Inception Distance (FID) and Inception Score (IS). We also show how the hybrid frequency-based representation improves control over global coherence and fine texture synthesis, paving the way for new directions in multi-scale generative modeling.\n        △ Less",
        "authors":"['Andrew Kiruluta', 'Andreas Lemos']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring Various Sequential Learning Methods for Deformation History Modeling",
        "abstract":"Current neural network (NN) models can learn patterns from data points with historical dependence. Specifically, in natural language processing (NLP), sequential learning has transitioned from recurrence-based architectures to transformer-based architectures. However, it is unknown which NN architectures will perform the best on datasets containing deformation history due to mechanical loading. Thus, this study ascertains the appropriateness of 1D-convolutional, recurrent, and transformer-based architectures for predicting deformation localization based on the earlier states in the form of deformation history. Following this investigation, the crucial incompatibility issues between the mathematical computation of the prediction process in the best-performing NN architectures and the actual values derived from the natural physical properties of the deformation paths are examined in detail.\n        △ Less",
        "authors":"['Muhammed Adil Yatkin', 'Mihkel Korgesaar', 'Jani Romanoff', 'Umit Islak', 'Hasan Kurban']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?",
        "abstract":"Large language models (LLMs) are increasingly contributing to the creation of content on the Internet. This creates a feedback loop as subsequent generations of models will be trained on this generated, synthetic data. This phenomenon is receiving increasing interest, in particular because previous studies have shown that it may lead to distribution shift - models misrepresent and forget the true underlying distributions of human data they are expected to approximate (e.g. resulting in a drastic loss of quality). In this study, we study the impact of human data properties on distribution shift dynamics in iterated training loops. We first confirm that the distribution shift dynamics greatly vary depending on the human data by comparing four datasets (two based on Twitter and two on Reddit). We then test whether data quality may influence the rate of this shift. We find that it does on the twitter, but not on the Reddit datasets. We then focus on a Reddit dataset and conduct a more exhaustive evaluation of a large set of dataset properties. This experiment associated lexical diversity with larger, and semantic diversity with smaller detrimental shifts, suggesting that incorporating text with high lexical (but limited semantic) diversity could exacerbate the degradation of generated text. We then focus on the evolution of political bias, and find that the type of shift observed (bias reduction, amplification or inversion) depends on the political lean of the human (true) distribution. Overall, our work extends the existing literature on the consequences of recursive fine-tuning by showing that this phenomenon is highly dependent on features of the human data on which training occurs. This suggests that different parts of internet (e.g. GitHub, Reddit) may undergo different types of shift depending on their properties.\n        △ Less",
        "authors":"['Grgur Kovač', 'Jérémy Perez', 'Rémy Portelas', 'Peter Ford Dominey', 'Pierre-Yves Oudeyer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs",
        "abstract":"Self-driving laboratories have begun to replace human experimenters in performing single experimental skills or predetermined experimental protocols. However, as the pace of idea iteration in scientific research has been intensified by Artificial Intelligence, the demand for rapid design of new protocols for new discoveries become evident. Efforts to automate protocol design have been initiated, but the capabilities of knowledge-based machine designers, such as Large Language Models, have not been fully elicited, probably for the absence of a systematic representation of experimental knowledge, as opposed to isolated, flatten pieces of information. To tackle this issue, we propose a multi-faceted, multi-scale representation, where instance actions, generalized operations, and product flow models are hierarchically encapsulated using Domain-Specific Languages. We further develop a data-driven algorithm based on non-parametric modeling that autonomously customizes these representations for specific domains. The proposed representation is equipped with various machine designers to manage protocol design tasks, including planning, modification, and adjustment. The results demonstrate that the proposed method could effectively complement Large Language Models in the protocol design process, serving as an auxiliary module in the realm of machine-assisted scientific exploration.\n        △ Less",
        "authors":"['Yu-Zhe Shi', 'Mingchen Liu', 'Fanxu Meng', 'Qiao Xu', 'Zhangqian Bi', 'Kun He', 'Lecheng Ruan', 'Qining Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Offline and Distributional Reinforcement Learning for Wireless Communications",
        "abstract":"The rapid growth of heterogeneous and massive wireless connectivity in 6G networks demands intelligent solutions to ensure scalability, reliability, privacy, ultra-low latency, and effective control. Although artificial intelligence (AI) and machine learning (ML) have demonstrated their potential in this domain, traditional online reinforcement learning (RL) and deep RL methods face limitations in real-time wireless networks. For instance, these methods rely on online interaction with the environment, which might be unfeasible, costly, or unsafe. In addition, they cannot handle the inherent uncertainties in real-time wireless applications. We focus on offline and distributional RL, two advanced RL techniques that can overcome these challenges by training on static datasets and accounting for network uncertainties. We introduce a novel framework that combines offline and distributional RL for wireless communication applications. Through case studies on unmanned aerial vehicle (UAV) trajectory optimization and radio resource management (RRM), we demonstrate that our proposed Conservative Quantile Regression (CQR) algorithm outperforms conventional RL approaches regarding convergence speed and risk management. Finally, we discuss open challenges and potential future directions for applying these techniques in 6G networks, paving the way for safer and more efficient real-time wireless systems.\n        △ Less",
        "authors":"['Eslam Eldeeb', 'Hirley Alves']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"What Large Language Models Do Not Talk About: An Empirical Study of Moderation and Censorship Practices",
        "abstract":"Large Language Models (LLMs) are increasingly deployed as gateways to information, yet their content moderation practices remain underexplored. This work investigates the extent to which LLMs refuse to answer or omit information when prompted on political topics. To do so, we distinguish between hard censorship (i.e., generated refusals, error messages, or canned denial responses) and soft censorship (i.e., selective omission or downplaying of key elements), which we identify in LLMs' responses when asked to provide information on a broad range of political figures. Our analysis covers 14 state-of-the-art models from Western countries, China, and Russia, prompted in all six official United Nations (UN) languages. Our analysis suggests that although censorship is observed across the board, it is predominantly tailored to an LLM provider's domestic audience and typically manifests as either hard censorship or soft censorship (though rarely both concurrently). These findings underscore the need for ideological and geographic diversity among publicly available LLMs, and greater transparency in LLM moderation strategies to facilitate informed user choices. All data are made freely available.\n        △ Less",
        "authors":"['Sander Noels', 'Guillaume Bied', 'Maarten Buyl', 'Alexander Rogiers', 'Yousra Fettach', 'Jefrey Lijffijt', 'Tijl De Bie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Semantic-guided Representation Learning for Multi-Label Recognition",
        "abstract":"Multi-label Recognition (MLR) involves assigning multiple labels to each data instance in an image, offering advantages over single-label classification in complex scenarios. However, it faces the challenge of annotating all relevant categories, often leading to uncertain annotations, such as unseen or incomplete labels. Recent Vision and Language Pre-training (VLP) based methods have made significant progress in tackling zero-shot MLR tasks by leveraging rich vision-language correlations. However, the correlation between multi-label semantics has not been fully explored, and the learned visual features often lack essential semantic information. To overcome these limitations, we introduce a Semantic-guided Representation Learning approach (SigRL) that enables the model to learn effective visual and textual representations, thereby improving the downstream alignment of visual images and categories. Specifically, we first introduce a graph-based multi-label correlation module (GMC) to facilitate information exchange between labels, enriching the semantic representation across the multi-label texts. Next, we propose a Semantic Visual Feature Reconstruction module (SVFR) to enhance the semantic information in the visual representation by integrating the learned textual representation during reconstruction. Finally, we optimize the image-text matching capability of the VLP model using both local and global features to achieve zero-shot MLR. Comprehensive experiments are conducted on several MLR benchmarks, encompassing both zero-shot MLR (with unseen labels) and single positive multi-label learning (with limited labels), demonstrating the superior performance of our approach compared to state-of-the-art methods. The code is available at https:\/\/github.com\/MVL-Lab\/SigRL.\n        △ Less",
        "authors":"['Ruhui Zhang', 'Hezhe Qiao', 'Pengcheng Xu', 'Mingsheng Shang', 'Lin Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Decision SpikeFormer: Spike-Driven Transformer for Decision Making",
        "abstract":"Offline reinforcement learning (RL) enables policy training solely on pre-collected data, avoiding direct environment interaction - a crucial benefit for energy-constrained embodied AI applications. Although Artificial Neural Networks (ANN)-based methods perform well in offline RL, their high computational and energy demands motivate exploration of more efficient alternatives. Spiking Neural Networks (SNNs) show promise for such tasks, given their low power consumption. In this work, we introduce DSFormer, the first spike-driven transformer model designed to tackle offline RL via sequence modeling. Unlike existing SNN transformers focused on spatial dimensions for vision tasks, we develop Temporal Spiking Self-Attention (TSSA) and Positional Spiking Self-Attention (PSSA) in DSFormer to capture the temporal and positional dependencies essential for sequence modeling in RL. Additionally, we propose Progressive Threshold-dependent Batch Normalization (PTBN), which combines the benefits of LayerNorm and BatchNorm to preserve temporal dependencies while maintaining the spiking nature of SNNs. Comprehensive results in the D4RL benchmark show DSFormer's superiority over both SNN and ANN counterparts, achieving 78.4% energy savings, highlighting DSFormer's advantages not only in energy efficiency but also in competitive performance. Code and models are public at https:\/\/wei-nijuan.github.io\/DecisionSpikeFormer.\n        △ Less",
        "authors":"['Wei Huang', 'Qinying Gu', 'Nanyang Ye']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Experimental Study on Time Series Analysis of Lower Limb Rehabilitation Exercise Data Driven by Novel Model Architecture and Large Models",
        "abstract":"This study investigates the application of novel model architectures and large-scale foundational models in temporal series analysis of lower limb rehabilitation motion data, aiming to leverage advancements in machine learning and artificial intelligence to empower active rehabilitation guidance strategies for post-stroke patients in limb motor function recovery. Utilizing the SIAT-LLMD dataset of lower limb movement data proposed by the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, we systematically elucidate the implementation and analytical outcomes of the innovative xLSTM architecture and the foundational model Lag-Llama in short-term temporal prediction tasks involving joint kinematics and dynamics parameters. The research provides novel insights for AI-enabled medical rehabilitation applications, demonstrating the potential of cutting-edge model architectures and large-scale models in rehabilitation medicine temporal prediction. These findings establish theoretical foundations for future applications of personalized rehabilitation regimens, offering significant implications for the development of customized therapeutic interventions in clinical practice.\n        △ Less",
        "authors":"['Hengyu Lin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CSF: Fixed-outline Floorplanning Based on the Conjugate Subgradient Algorithm Assisted by Q-Learning",
        "abstract":"To perform the fixed-outline floorplanning problem efficiently, we propose to solve the original nonsmooth analytic optimization model via the conjugate subgradient algorithm (CSA), which is further accelerated by adaptively regulating the step size with the assistance of Q-learning. The objective for global floorplanning is a weighted sum of the half-perimeter wirelength, the overlapping area and the out-of-bound width, and the legalization is implemented by optimizing the weighted sum of the overlapping area and the out-of-bound width. Meanwhile, we also propose two improved variants for the legalizaiton algorithm based on constraint graphs (CGs). Experimental results demonstrate that the CSA assisted by Q-learning (CSAQ) can address both global floorplanning and legalization efficiently, and the two stages jointly contribute to competitive results on the optimization of wirelength. Meanwhile, the improved CG-based legalization methods also outperforms the original one in terms of runtime and success rate.\n        △ Less",
        "authors":"['Huabin Cheng', 'Rujie Chen', 'Yu Chen', 'Wei Zhang', 'Ning Xu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Outlook Towards Deployable Continual Learning for Particle Accelerators",
        "abstract":"Particle Accelerators are high power complex machines. To ensure uninterrupted operation of these machines, thousands of pieces of equipment need to be synchronized, which requires addressing many challenges including design, optimization and control, anomaly detection and machine protection. With recent advancements, Machine Learning (ML) holds promise to assist in more advance prognostics, optimization, and control. While ML based solutions have been developed for several applications in particle accelerators, only few have reached deployment and even fewer to long term usage, due to particle accelerator data distribution drifts caused by changes in both measurable and non-measurable parameters. In this paper, we identify some of the key areas within particle accelerators where continual learning can allow maintenance of ML model performance with distribution drifts. Particularly, we first discuss existing applications of ML in particle accelerators, and their limitations due to distribution drift. Next, we review existing continual learning techniques and investigate their potential applications to address data distribution drifts in accelerators. By identifying the opportunities and challenges in applying continual learning, this paper seeks to open up the new field and inspire more research efforts towards deployable continual learning for particle accelerators.\n        △ Less",
        "authors":"['Kishansingh Rajput', 'Sen Lin', 'Auralee Edelen', 'Willem Blokland', 'Malachi Schram']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DP-LET: An Efficient Spatio-Temporal Network Traffic Prediction Framework",
        "abstract":"Accurately predicting spatio-temporal network traffic is essential for dynamically managing computing resources in modern communication systems and minimizing energy consumption. Although spatio-temporal traffic prediction has received extensive research attention, further improvements in prediction accuracy and computational efficiency remain necessary. In particular, existing decomposition-based methods or hybrid architectures often incur heavy overhead when capturing local and global feature correlations, necessitating novel approaches that optimize accuracy and complexity. In this paper, we propose an efficient spatio-temporal network traffic prediction framework, DP-LET, which consists of a data processing module, a local feature enhancement module, and a Transformer-based prediction module. The data processing module is designed for high-efficiency denoising of network data and spatial decoupling. In contrast, the local feature enhancement module leverages multiple Temporal Convolutional Networks (TCNs) to capture fine-grained local features. Meanwhile, the prediction module utilizes a Transformer encoder to model long-term dependencies and assess feature relevance. A case study on real-world cellular traffic prediction demonstrates the practicality of DP-LET, which maintains low computational complexity while achieving state-of-the-art performance, significantly reducing MSE by 31.8% and MAE by 23.1% compared to baseline models.\n        △ Less",
        "authors":"['Xintong Wang', 'Haihan Nan', 'Ruidong Li', 'Huaming Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sample, Don't Search: Rethinking Test-Time Alignment for Language Models",
        "abstract":"Increasing test-time computation has emerged as a promising direction for improving language model performance, particularly in scenarios where model finetuning is impractical or impossible due to computational constraints or private model weights. However, existing test-time search methods using a reward model (RM) often degrade in quality as compute scales, due to the over-optimization of what are inherently imperfect reward proxies. We introduce QAlign, a new test-time alignment approach. As we scale test-time compute, QAlign converges to sampling from the optimal aligned distribution for each individual prompt. By adopting recent advances in Markov chain Monte Carlo for text generation, our method enables better-aligned outputs without modifying the underlying model or even requiring logit access. We demonstrate the effectiveness of QAlign on mathematical reasoning benchmarks (GSM8K and GSM-Symbolic) using a task-specific RM, showing consistent improvements over existing test-time compute methods like best-of-n and majority voting. Furthermore, when applied with more realistic RMs trained on the Tulu 3 preference dataset, QAlign outperforms direct preference optimization (DPO), best-of-n, majority voting, and weighted majority voting on a diverse range of datasets (GSM8K, MATH500, IFEval, MMLU-Redux, and TruthfulQA). A practical solution to aligning language models at test time using additional computation without degradation, our approach expands the limits of the capability that can be obtained from off-the-shelf language models without further training.\n        △ Less",
        "authors":"['Gonçalo Faria', 'Noah A. Smith']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detecting Plant VOC Traces Using Indoor Air Quality Sensors",
        "abstract":"In the era of growing interest in healthy buildings and smart homes, the importance of sustainable, health conscious indoor environments is paramount. Smart tools, especially VOC sensors, are crucial for monitoring indoor air quality, yet interpreting signals from various VOC sources remains challenging. A promising approach involves understanding how indoor plants respond to environmental conditions. Plants produce terpenes, a type of VOC, when exposed to abiotic and biotic stressors - including pathogens, predators, light, and temperature - offering a novel pathway for monitoring indoor air quality. While prior work often relies on specialized laboratory sensors, our research leverages readily available commercial sensors to detect and classify plant emitted VOCs that signify changes in indoor conditions. We quantified the sensitivity of these sensors by measuring 16 terpenes in controlled experiments, then identified and tested the most promising terpenes in realistic environments. We also examined physics based models to map VOC responses but found them lacking for real world complexity. Consequently, we trained machine learning models to classify terpenes using commercial sensors and identified optimal sensor placement. To validate this approach, we analyzed emissions from a living basil plant, successfully detecting terpene output. Our findings establish a foundation for overcoming challenges in plant VOC detection, paving the way for advanced plant based sensors to enhance indoor environmental quality in future smart buildings.\n        △ Less",
        "authors":"['Seyed Hamidreza Nabaei', 'Ryan Lenfant', 'Viswajith Govinda Rajan', 'Dong Chen', 'Michael P. Timko', 'Bradford Campbell', 'Arsalan Heydarian']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning",
        "abstract":"Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset.\n        △ Less",
        "authors":"['Kai Ye', 'Hongyi Zhou', 'Jin Zhu', 'Francesco Quinzan', 'Chengchung Shi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training",
        "abstract":"Federated Active Learning (FAL) has emerged as a promising framework to leverage large quantities of unlabeled data across distributed clients while preserving data privacy. However, real-world deployments remain limited by high annotation costs and communication-intensive sampling processes, particularly in a cross-silo setting, when clients possess substantial local datasets. This paper addresses the crucial question: What is the best practice to reduce communication costs in human-in-the-loop learning with minimal annotator effort? Existing FAL methods typically rely on iterative annotation processes that separate active sampling from federated updates, leading to multiple rounds of expensive communication and annotation. In response, we introduce FAST, a two-pass FAL framework that harnesses foundation models for weak labeling in a preliminary pass, followed by a refinement pass focused exclusively on the most uncertain samples. By leveraging representation knowledge from foundation models and integrating refinement steps into a streamlined workflow, FAST substantially reduces the overhead incurred by iterative active sampling. Extensive experiments on diverse medical and natural image benchmarks demonstrate that FAST outperforms existing FAL methods by an average of 4.36% while reducing communication rounds eightfold under a limited 5% labeling budget.\n        △ Less",
        "authors":"['Haoyuan Li', 'Mathias Funk', 'Jindong Wang', 'Aaqib Saeed']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Study on Adversarial Robustness of Discriminative Prototypical Learning",
        "abstract":"Deep neural networks demonstrate significant vulnerability to adversarial perturbations, posing risks for critical applications. Current adversarial training methods predominantly focus on robustness against attacks without explicitly leveraging geometric structures in the latent space, usually resulting in reduced accuracy on the original clean data. To address these issues, we propose a novel adversarial training framework named Adversarial Deep Positive-Negative Prototypes (Adv-DPNP), which integrates disriminative prototype-based learning with adversarial training. Adv-DPNP uses unified class prototypes serving dual roles as classifier weights and robust anchors, enhancing both intra-class compactness and inter-class separation in the latent space. Moreover, a novel dual-branch training mechanism maintains stable prototypes by updating them exclusively with clean data; while the feature extractor layers are learned using both clean and adversarial data to remain invariant against adversarial perturbations. In addition, our approach utilizes a composite loss function combining positive prototype alignment, negative prototype repulsion, and consistency regularization to further enhance discrimination, adversarial robustness, and clean accuracy. Extensive experiments conducted on standard benchmark datasets confirm the effectiveness of Adv-DPNP compared to state-of-the-art methods, achieving higher clean accuracy and competitive robustness under adversarial perturbations and common corruptions. Our code is available at https:\/\/github.com\/fum-rpl\/adv-dpnp\n        △ Less",
        "authors":"['Ramin Zarei Sabzevar', 'Hamed Mohammadzadeh', 'Tahmineh Tavakoli', 'Ahad Harati']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Explainable and Interpretable Forecasts on Non-Smooth Multivariate Time Series for Responsible Gameplay",
        "abstract":"Multi-variate Time Series (MTS) forecasting has made large strides (with very negligible errors) through recent advancements in neural networks, e.g., Transformers. However, in critical situations like predicting gaming overindulgence that affects one's mental well-being; an accurate forecast without a contributing evidence (explanation) is irrelevant. Hence, it becomes important that the forecasts are Interpretable - intermediate representation of the forecasted trajectory is comprehensible; as well as Explainable - attentive input features and events are accessible for a personalized and timely intervention of players at risk. While the contributing state of the art research on interpretability primarily focuses on temporally-smooth single-process driven time series data, our online multi-player gameplay data demonstrates intractable temporal randomness due to intrinsic orthogonality between player's game outcome and their intent to engage further. We introduce a novel deep Actionable Forecasting Network (AFN), which addresses the inter-dependent challenges associated with three exclusive objectives - 1) forecasting accuracy; 2) smooth comprehensible trajectory and 3) explanations via multi-dimensional input features while tackling the challenges introduced by our non-smooth temporal data, together in one single solution. AFN establishes a \\it{new benchmark} via: (i) achieving 25% improvement on the MSE of the forecasts on player data in comparison to the SOM-VAE based SOTA networks; (ii) attributing unfavourable progression of a player's time series to a specific future time step(s), with the premise of eliminating near-future overindulgent player volume by over 18% with player specific actionable inputs feature(s) and (iii) proactively detecting over 23% (100% jump from SOTA) of the to-be overindulgent, players on an average, 4 weeks in advance.\n        △ Less",
        "authors":"['Hussain Jagirdar', 'Rukma Talwadker', 'Aditya Pareek', 'Pulkit Agrawal', 'Tridib Mukherjee']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring energy consumption of AI frameworks on a 64-core RV64 Server CPU",
        "abstract":"In today's era of rapid technological advancement, artificial intelligence (AI) applications require large-scale, high-performance, and data-intensive computations, leading to significant energy demands. Addressing this challenge necessitates a combined approach involving both hardware and software innovations. Hardware manufacturers are developing new, efficient, and specialized solutions, with the RISC-V architecture emerging as a prominent player due to its open, extensible, and energy-efficient instruction set architecture (ISA). Simultaneously, software developers are creating new algorithms and frameworks, yet their energy efficiency often remains unclear. In this study, we conduct a comprehensive benchmark analysis of machine learning (ML) applications on the 64-core SOPHON SG2042 RISC-V architecture. We specifically analyze the energy consumption of deep learning inference models across three leading AI frameworks: PyTorch, ONNX Runtime, and TensorFlow. Our findings show that frameworks using the XNNPACK back-end, such as ONNX Runtime and TensorFlow, consume less energy compared to PyTorch, which is compiled with the native OpenBLAS back-end.\n        △ Less",
        "authors":"['Giulio Malenza', 'Francesco Targa', 'Adriano Marques Garcia', 'Marco Aldinucci', 'Robert Birke']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SHapley Estimated Explanation (SHEP): A Fast Post-Hoc Attribution Method for Interpreting Intelligent Fault Diagnosis",
        "abstract":"Despite significant progress in intelligent fault diagnosis (IFD), the lack of interpretability remains a critical barrier to practical industrial applications, driving the growth of interpretability research in IFD. Post-hoc interpretability has gained popularity due to its ability to preserve network flexibility and scalability without modifying model structures. However, these methods often yield suboptimal time-domain explanations. Recently, combining domain transform with SHAP has improved interpretability by extending explanations to more informative domains. Nonetheless, the computational expense of SHAP, exacerbated by increased dimensions from domain transforms, remains a major challenge. To address this, we propose patch-wise attribution and SHapley Estimated Explanation (SHEP). Patch-wise attribution reduces feature dimensions at the cost of explanation granularity, while SHEP simplifies subset enumeration to approximate SHAP, reducing complexity from exponential to linear. Together, these methods significantly enhance SHAP's computational efficiency, providing feasibility for real-time interpretation in monitoring tasks. Extensive experiments confirm SHEP's efficiency, interpretability, and reliability in approximating SHAP. Additionally, with open-source code, SHEP has the potential to serve as a benchmark for post-hoc interpretability in IFD. The code is available on https:\/\/github.com\/ChenQian0618\/SHEP.\n        △ Less",
        "authors":"['Qian Chen', 'Xingjian Dong', 'Zhike Peng', 'Guang Meng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Low-cost Embedded Breathing Rate Determination Using 802.15.4z IR-UWB Hardware for Remote Healthcare",
        "abstract":"Respiratory diseases account for a significant portion of global mortality. Affordable and early detection is an effective way of addressing these ailments. To this end, a low-cost commercial off-the-shelf (COTS), IEEE 802.15.4z standard compliant impulse-radio ultra-wideband (IR-UWB) radar system is exploited to estimate human respiration rates. We propose a convolutional neural network (CNN) to predict breathing rates from ultra-wideband (UWB) channel impulse response (CIR) data, and compare its performance with other rule-based algorithms. The study uses a diverse dataset of 16 individuals, incorporating various real-life environments to evaluate system robustness. Results show that the CNN achieves a mean absolute error (MAE) of 1.73 breaths per minute (BPM) in unseen situations, significantly outperforming rule-based methods (3.40 BPM). By incorporating calibration data from other individuals in the unseen situations, the error is further reduced to 0.84 BPM. In addition, this work evaluates the feasibility of running the pipeline on a low-cost embedded device. Applying 8-bit quantization to both the weights and input\/ouput tensors, reduces memory requirements by 67% and inference time by 64% with only a 3% increase in MAE. As a result, we show it is feasible to deploy the algorithm on an nRF52840 system-on-chip (SoC) requiring only 46 KB of memory and operating with an inference time of only 192 ms. Once deployed, the system can last up to 268 days without recharging using a 20 000 mAh battery pack. For breathing monitoring in bed, the sampling rate can be lowered, extending battery life to 313 days, making the solution highly efficient for real-world, low-cost deployments.\n        △ Less",
        "authors":"['Anton Lambrecht', 'Stijn Luchie', 'Jaron Fontaine', 'Ben Van Herbruggen', 'Adnan Shahid', 'Eli De Poorter']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits",
        "abstract":"To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API calls to large language models (LLMs), data sources, and agentic tools. By connecting multiple MCP servers, each defined with a set of tools, resources, and prompts, users are able to define automated workflows fully driven by LLMs. However, we show that the current MCP design carries a wide range of security risks for end users. In particular, we demonstrate that industry-leading LLMs may be coerced into using MCP tools to compromise an AI developer's system through various attacks, such as malicious code execution, remote access control, and credential theft. To proactively mitigate these and related attacks, we introduce a safety auditing tool, MCPSafetyScanner, the first agentic tool to assess the security of an arbitrary MCP server. MCPScanner uses several agents to (a) automatically determine adversarial samples given an MCP server's tools and resources; (b) search for related vulnerabilities and remediations based on those samples; and (c) generate a security report detailing all findings. Our work highlights serious security issues with general-purpose agentic workflows while also providing a proactive tool to audit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available at: https:\/\/github.com\/johnhalloran321\/mcpSafetyScanner\n        △ Less",
        "authors":"['Brandon Radosevich', 'John Halloran']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Calibration for RRAM-based In-Memory Computing using DoRA",
        "abstract":"Resistive In-Memory Computing (RIMC) offers ultra-efficient computation for edge AI but faces accuracy degradation due to RRAM conductance drift over time. Traditional retraining methods are limited by RRAM's high energy consumption, write latency, and endurance constraints. We propose a DoRA-based calibration framework that restores accuracy by compensating influential weights with minimal calibration parameters stored in SRAM, leaving RRAM weights untouched. This eliminates in-field RRAM writes, ensuring energy-efficient, fast, and reliable calibration. Experiments on RIMC-based ResNet50 (ImageNet-1K) demonstrate 69.53% accuracy restoration using just 10 calibration samples while updating only 2.34% of parameters.\n        △ Less",
        "authors":"['Weirong Dong', 'Kai Zhou', 'Zhen Kong', 'Quan Cheng', 'Junkai Huang', 'Zhengke Yang', 'Masanori Hashimoto', 'Longyang Lin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Decoding Covert Speech from EEG Using a Functional Areas Spatio-Temporal Transformer",
        "abstract":"Covert speech involves imagining speaking without audible sound or any movements. Decoding covert speech from electroencephalogram (EEG) is challenging due to a limited understanding of neural pronunciation mapping and the low signal-to-noise ratio of the signal. In this study, we developed a large-scale multi-utterance speech EEG dataset from 57 right-handed native English-speaking subjects, each performing covert and overt speech tasks by repeating the same word in five utterances within a ten-second duration. Given the spatio-temporal nature of the neural activation process during speech pronunciation, we developed a Functional Areas Spatio-temporal Transformer (FAST), an effective framework for converting EEG signals into tokens and utilizing transformer architecture for sequence encoding. Our results reveal distinct and interpretable speech neural features by the visualization of FAST-generated activation maps across frontal and temporal brain regions with each word being covertly spoken, providing new insights into the discriminative features of the neural representation of covert speech. This is the first report of such a study, which provides interpretable evidence for speech decoding from EEG. The code for this work has been made public at https:\/\/github.com\/Jiang-Muyun\/FAST\n        △ Less",
        "authors":"['Muyun Jiang', 'Yi Ding', 'Wei Zhang', 'Kok Ann Colin Teo', 'LaiGuan Fong', 'Shuailei Zhang', 'Zhiwei Guo', 'Chenyu Liu', 'Raghavan Bhuvanakantham', 'Wei Khang Jeremy Sim', 'Chuan Huat Vince Foo', 'Rong Hui Jonathan Chua', 'Parasuraman Padmanabhan', 'Victoria Leong', 'Jia Lu', 'Balazs Gulyas', 'Cuntai Guan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Augmentation of EEG and ECG Time Series for Deep Learning Applications: Integrating Changepoint Detection into the iAAFT Surrogates",
        "abstract":"The performance of deep learning methods critically depends on the quality and quantity of the available training data. This is especially the case for physiological time series, which are both noisy and scarce, which calls for data augmentation to artificially increase the size of datasets. Another issue is that the time-evolving statistical properties of nonstationary signals prevent the use of standard data augmentation techniques. To this end, we introduce a novel method for augmenting nonstationary time series. This is achieved by combining offline changepoint detection with the iterative amplitude-adjusted Fourier transform (iAAFT), which ensures that the time-frequency properties of the original signal are preserved during augmentation. The proposed method is validated through comparisons of the performance of i) a deep learning seizure detection algorithm on both the original and augmented versions of the CHB-MIT and Siena scalp electroencephalography (EEG) databases, and ii) a deep learning atrial fibrillation (AF) detection algorithm on the original and augmented versions of the Computing in Cardiology Challenge 2017 dataset. By virtue of the proposed method, for the CHB-MIT and Siena datasets respectively, accuracy rose by 4.4% and 1.9%, precision by 10% and 5.5%, recall by 3.6% and 0.9%, and F1 by 4.2% and 1.4%. For the AF classification task, accuracy rose by 0.3%, precision by 2.1%, recall by 0.8%, and F1 by 2.1%.\n        △ Less",
        "authors":"['Nina Moutonnet', 'Gregory Scott', 'Danilo P. Mandic']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EEG-EyeTrack: A Benchmark for Time Series and Functional Data Analysis with Open Challenges and Baselines",
        "abstract":"A new benchmark dataset for functional data analysis (FDA) is presented, focusing on the reconstruction of eye movements from EEG data. The contribution is twofold: first, open challenges and evaluation metrics tailored to FDA applications are proposed. Second, functional neural networks are used to establish baseline results for the primary regression task of reconstructing eye movements from EEG signals. Baseline results are reported for the new dataset, based on consumer-grade hardware, and the EEGEyeNet dataset, based on research-grade hardware.\n        △ Less",
        "authors":"['Tiago Vasconcelos Afonso', 'Florian Heinrichs']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EEG2GAIT: A Hierarchical Graph Convolutional Network for EEG-based Gait Decoding",
        "abstract":"Decoding gait dynamics from EEG signals presents significant challenges due to the complex spatial dependencies of motor processes, the need for accurate temporal and spectral feature extraction, and the scarcity of high-quality gait EEG datasets. To address these issues, we propose EEG2GAIT, a novel hierarchical graph-based model that captures multi-level spatial embeddings of EEG channels using a Hierarchical Graph Convolutional Network (GCN) Pyramid. To further improve decoding accuracy, we introduce a Hybrid Temporal-Spectral Reward (HTSR) loss function, which combines time-domain, frequency-domain, and reward-based loss components. Moreover, we contribute a new Gait-EEG Dataset (GED), consisting of synchronized EEG and lower-limb joint angle data collected from 50 participants over two lab visits. Validation experiments on both the GED and the publicly available Mobile Brain-body imaging (MoBI) dataset demonstrate that EEG2GAIT outperforms state-of-the-art methods and achieves the best joint angle prediction. Ablation studies validate the contributions of the hierarchical GCN modules and HTSR Loss, while saliency maps reveal the significance of motor-related brain regions in decoding tasks. These findings underscore EEG2GAIT's potential for advancing brain-computer interface applications, particularly in lower-limb rehabilitation and assistive technologies.\n        △ Less",
        "authors":"['Xi Fu', 'Rui Liu', 'Aung Aung Phyo Wai', 'Hannah Pulferer', 'Neethu Robinson', 'Gernot R Müller-Putz', 'Cuntai Guan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Semi-Self Representation Learning for Crowdsourced WiFi Trajectories",
        "abstract":"WiFi fingerprint-based localization has been studied intensively. Point-based solutions rely on position annotations of WiFi fingerprints. Trajectory-based solutions, however, require end-position annotations of WiFi trajectories, where a WiFi trajectory is a multivariate time series of signal features. A trajectory dataset is much larger than a pointwise dataset as the number of potential trajectories in a field may grow exponentially with respect to the size of the field. This work presents a semi-self representation learning solution, where a large dataset $C$ of crowdsourced unlabeled WiFi trajectories can be automatically labeled by a much smaller dataset $\\tilde C$ of labeled WiFi trajectories. The size of $\\tilde C$ only needs to be proportional to the size of the physical field, while the unlabeled $C$ could be much larger. This is made possible through a novel ``cut-and-flip'' augmentation scheme based on the meet-in-the-middle paradigm. A two-stage learning consisting of trajectory embedding followed by endpoint embedding is proposed for the unlabeled $C$. Then the learned representations are labeled by $\\tilde C$ and connected to a neural-based localization network. The result, while delivering promising accuracy, significantly relieves the burden of human annotations for trajectory-based localization.\n        △ Less",
        "authors":"['Yu-Lin Kuo', 'Yu-Chee Tseng', 'Ting-Hui Chiang', 'Yan-Ann Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery",
        "abstract":"Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https:\/\/github.com\/mashijie1028\/ProtoGCD.\n        △ Less",
        "authors":"['Shijie Ma', 'Fei Zhu', 'Xu-Yao Zhang', 'Cheng-Lin Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MMCE: A Framework for Deep Monotonic Modeling of Multiple Causal Effects",
        "abstract":"When we plan to use money as an incentive to change the behavior of a person (such as making riders to deliver more orders or making consumers to buy more items), the common approach of this problem is to adopt a two-stage framework in order to maximize ROI under cost constraints. In the first stage, the individual price response curve is obtained. In the second stage, business goals and resource constraints are formally expressed and modeled as an optimization problem. The first stage is very critical. It can answer a very important question. This question is how much incremental results can incentives bring, which is the basis of the second stage. Usually, the causal modeling is used to obtain the curve. In the case of only observational data, causal modeling and evaluation are very challenging. In some business scenarios, multiple causal effects need to be obtained at the same time. This paper proposes a new observational data modeling and evaluation framework, which can simultaneously model multiple causal effects and greatly improve the modeling accuracy under some abnormal distributions. In the absence of RCT data, evaluation seems impossible. This paper summarizes three priors to illustrate the necessity and feasibility of qualitative evaluation of cognitive testing. At the same time, this paper innovatively proposes the conditions under which observational data can be considered as an evaluation dataset. Our approach is very groundbreaking. It is the first to propose a modeling framework that simultaneously obtains multiple causal effects. The offline analysis and online experimental results show the effectiveness of the results and significantly improve the effectiveness of the allocation strategies generated in real world marketing activities.\n        △ Less",
        "authors":"['Juhua Chen', 'Karson shi', 'Jialing He', 'North Chen', 'Kele Jiang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Revolutionizing Fractional Calculus with Neural Networks: Voronovskaya-Damasclin Theory for Next-Generation AI Systems",
        "abstract":"This work introduces rigorous convergence rates for neural network operators activated by symmetrized and perturbed hyperbolic tangent functions, utilizing novel Voronovskaya-Damasclin asymptotic expansions. We analyze basic, Kantorovich, and quadrature-type operators over infinite domains, extending classical approximation theory to fractional calculus via Caputo derivatives. Key innovations include parameterized activation functions with asymmetry control, symmetrized density operators, and fractional Taylor expansions for error analysis. The main theorem demonstrates that Kantorovich operators achieve \\(o(n^{-β(N-\\varepsilon)})\\) convergence rates, while basic operators exhibit \\(\\mathcal{O}(n^{-βN})\\) error decay. For deep networks, we prove \\(\\mathcal{O}(L^{-β(N-\\varepsilon)})\\) approximation bounds. Stability results under parameter perturbations highlight operator robustness. By integrating neural approximation theory with fractional calculus, this work provides foundational mathematical insights and deployable engineering solutions, with potential applications in complex system modeling and signal processing.\n        △ Less",
        "authors":"['Rômulo Damasclin Chaves dos Santos', 'Jorge Henrique de Oliveira Sales']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detecting Financial Fraud with Hybrid Deep Learning: A Mix-of-Experts Approach to Sequential and Anomalous Patterns",
        "abstract":"Financial fraud detection remains a critical challenge due to the dynamic and adversarial nature of fraudulent behavior. As fraudsters evolve their tactics, detection systems must combine robustness, adaptability, and precision. This study presents a hybrid architecture for credit card fraud detection that integrates a Mixture of Experts (MoE) framework with Recurrent Neural Networks (RNNs), Transformer encoders, and Autoencoders. Each expert module contributes a specialized capability: RNNs capture sequential behavior, Transformers extract high-order feature interactions, and Autoencoders detect anomalies through reconstruction loss. The MoE framework dynamically assigns predictive responsibility among the experts, enabling adaptive and context-sensitive decision-making.\n  Trained on a high-fidelity synthetic dataset that simulates real-world transaction patterns and fraud typologies, the hybrid model achieved 98.7 percent accuracy, 94.3 percent precision, and 91.5 percent recall, outperforming standalone models and classical machine learning baselines. The Autoencoder component significantly enhanced the system's ability to identify emerging fraud strategies and atypical behaviors.\n  Beyond technical performance, the model contributes to broader efforts in financial governance and crime prevention. It supports regulatory compliance with Anti-Money Laundering (AML) and Know Your Customer (KYC) protocols and aligns with routine activity theory by operationalizing AI as a capable guardian within financial ecosystems. The proposed hybrid system offers a scalable, modular, and regulation-aware approach to detecting increasingly sophisticated fraud patterns, contributing both to the advancement of intelligent systems and to the strengthening of institutional fraud defense infrastructures.\n        △ Less",
        "authors":"['Diego Vallarino']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Input Resolution Downsizing as a Compression Technique for Vision Deep Learning Systems",
        "abstract":"Model compression is a critical area of research in deep learning, in particular in vision, driven by the need to lighten models memory or computational footprints. While numerous methods for model compression have been proposed, most focus on pruning, quantization, or knowledge distillation. In this work, we delve into an under-explored avenue: reducing the resolution of the input image as a complementary approach to other types of compression. By systematically investigating the impact of input resolution reduction, on both tasks of classification and semantic segmentation, and on convnets and transformer-based architectures, we demonstrate that this strategy provides an interesting alternative for model compression. Our experimental results on standard benchmarks highlight the potential of this method, achieving competitive performance while significantly reducing computational and memory requirements. This study establishes input resolution reduction as a viable and promising direction in the broader landscape of model compression techniques for vision applications.\n        △ Less",
        "authors":"['Jeremy Morlier', 'Mathieu Leonardon', 'Vincent Gripon']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TDBench: Benchmarking Vision-Language Models in Understanding Top-Down Images",
        "abstract":"The rapid emergence of Vision-Language Models (VLMs) has significantly advanced multimodal understanding, enabling applications in scene comprehension and visual reasoning. While these models have been primarily evaluated and developed for front-view image understanding, their capabilities in interpreting top-down images have received limited attention, partly due to the scarcity of diverse top-down datasets and the challenges in collecting such data. In contrast, top-down vision provides explicit spatial overviews and improved contextual understanding of scenes, making it particularly valuable for tasks like autonomous navigation, aerial imaging, and spatial planning. In this work, we address this gap by introducing TDBench, a comprehensive benchmark for VLMs in top-down image understanding. TDBench is constructed from public top-down view datasets and high-quality simulated images, including diverse real-world and synthetic scenarios. TDBench consists of visual question-answer pairs across ten evaluation dimensions of image understanding. Moreover, we conduct four case studies that commonly happen in real-world scenarios but are less explored. By revealing the strengths and limitations of existing VLM through evaluation results, we hope TDBench to provide insights for motivating future research. Project homepage: https:\/\/github.com\/Columbia-ICSL\/TDBench\n        △ Less",
        "authors":"['Kaiyuan Hou', 'Minghui Zhao', 'Lilin Xu', 'Yuang Fan', 'Xiaofan Jiang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Biologically Inspired Hierarchical Temporal Memory with Hardware-Accelerated Reflex Memory",
        "abstract":"The rapid expansion of the Internet of Things (IoT) generates zettabytes of data that demand efficient unsupervised learning systems. Hierarchical Temporal Memory (HTM), a third-generation unsupervised AI algorithm, models the neocortex of the human brain by simulating columns of neurons to process and predict sequences. These neuron columns can memorize and infer sequences across multiple orders. While multiorder inferences offer robust predictive capabilities, they often come with significant computational overhead. The Sequence Memory (SM) component of HTM, which manages these inferences, encounters bottlenecks primarily due to its extensive programmable interconnects. In many cases, it has been observed that first-order temporal relationships have proven to be sufficient without any significant loss in efficiency. This paper introduces a Reflex Memory (RM) block, inspired by the Spinal Cord's working mechanisms, designed to accelerate the processing of first-order inferences. The RM block performs these inferences significantly faster than the SM. The integration of RM with HTM forms a system called the Accelerated Hierarchical Temporal Memory (AHTM), which processes repetitive information more efficiently than the original HTM while still supporting multiorder inferences. The experimental results demonstrate that the HTM predicts an event in 0.945 s, whereas the AHTM module does so in 0.125 s. Additionally, the hardware implementation of RM in a content-addressable memory (CAM) block, known as Hardware-Accelerated Hierarchical Temporal Memory (H-AHTM), predicts an event in just 0.094 s, significantly improving inference speed. Compared to the original algorithm \\cite{bautista2020matlabhtm}, AHTM accelerates inference by up to 7.55x, while H-AHTM further enhances performance with a 10.10x speedup.\n        △ Less",
        "authors":"['Pavia Bera', 'Sabrina Hassan Moon', 'Jennifer Adorno', 'Dayane Alfenas Reis', 'Sanjukta Bhanja']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative Explanations: Explanation Guided Decision Making for Human-in-the-Loop Preference Selection",
        "abstract":"This paper introduces Multi-Output LOcal Narrative Explanation (MOLONE), a novel comparative explanation method designed to enhance preference selection in human-in-the-loop Preference Bayesian optimization (PBO). The preference elicitation in PBO is a non-trivial task because it involves navigating implicit trade-offs between vector-valued outcomes, subjective priorities of decision-makers, and decision-makers' uncertainty in preference selection. Existing explainable AI (XAI) methods for BO primarily focus on input feature importance, neglecting the crucial role of outputs (objectives) in human preference elicitation. MOLONE addresses this gap by providing explanations that highlight both input and output importance, enabling decision-makers to understand the trade-offs between competing objectives and make more informed preference selections. MOLONE focuses on local explanations, comparing the importance of input features and outcomes across candidate samples within a local neighborhood of the search space, thus capturing nuanced differences relevant to preference-based decision-making. We evaluate MOLONE within a PBO framework using benchmark multi-objective optimization functions, demonstrating its effectiveness in improving convergence compared to noisy preference selections. Furthermore, a user study confirms that MOLONE significantly accelerates convergence in human-in-the-loop scenarios by facilitating more efficient identification of preferred options.\n        △ Less",
        "authors":"['Tanmay Chakraborty', 'Christian Wirth', 'Christin Seifert']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Modelling bounded rational decision-making through Wasserstein constraints",
        "abstract":"Modelling bounded rational decision-making through information constrained processing provides a principled approach for representing departures from rationality within a reinforcement learning framework, while still treating decision-making as an optimization process. However, existing approaches are generally based on Entropy, Kullback-Leibler divergence, or Mutual Information. In this work, we highlight issues with these approaches when dealing with ordinal action spaces. Specifically, entropy assumes uniform prior beliefs, missing the impact of a priori biases on decision-makings. KL-Divergence addresses this, however, has no notion of \"nearness\" of actions, and additionally, has several well known potentially undesirable properties such as the lack of symmetry, and furthermore, requires the distributions to have the same support (e.g. positive probability for all actions). Mutual information is often difficult to estimate. Here, we propose an alternative approach for modeling bounded rational RL agents utilising Wasserstein distances. This approach overcomes the aforementioned issues. Crucially, this approach accounts for the nearness of ordinal actions, modeling \"stickiness\" in agent decisions and unlikeliness of rapidly switching to far away actions, while also supporting low probability actions, zero-support prior distributions, and is simple to calculate directly.\n        △ Less",
        "authors":"['Benjamin Patrick Evans', 'Leo Ardon', 'Sumitra Ganesh']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hierarchical Local-Global Feature Learning for Few-shot Malicious Traffic Detection",
        "abstract":"With the rapid growth of internet traffic, malicious network attacks have become increasingly frequent and sophisticated, posing significant threats to global cybersecurity. Traditional detection methods, including rule-based and machine learning-based approaches, struggle to accurately identify emerging threats, particularly in scenarios with limited samples. While recent advances in few-shot learning have partially addressed the data scarcity issue, existing methods still exhibit high false positive rates and lack the capability to effectively capture crucial local traffic patterns. In this paper, we propose HLoG, a novel hierarchical few-shot malicious traffic detection framework that leverages both local and global features extracted from network sessions. HLoG employs a sliding-window approach to segment sessions into phases, capturing fine-grained local interaction patterns through hierarchical bidirectional GRU encoding, while simultaneously modeling global contextual dependencies. We further design a session similarity assessment module that integrates local similarity with global self-attention-enhanced representations, achieving accurate and robust few-shot traffic classification. Comprehensive experiments on three meticulously reconstructed datasets demonstrate that HLoG significantly outperforms existing state-of-the-art methods. Particularly, HLoG achieves superior recall rates while substantially reducing false positives, highlighting its effectiveness and practical value in real-world cybersecurity applications.\n        △ Less",
        "authors":"['Songtao Peng', 'Lei Wang', 'Wu Shuai', 'Hao Song', 'Jiajun Zhou', 'Shanqing Yu', 'Qi Xuan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Brain Network Classification Based on Graph Contrastive Learning and Graph Transformer",
        "abstract":"The dynamic characterization of functional brain networks is of great significance for elucidating the mechanisms of human brain function. Although graph neural networks have achieved remarkable progress in functional network analysis, challenges such as data scarcity and insufficient supervision persist. To address the limitations of limited training data and inadequate supervision, this paper proposes a novel model named PHGCL-DDGformer that integrates graph contrastive learning with graph transformers, effectively enhancing the representation learning capability for brain network classification tasks. To overcome the constraints of existing graph contrastive learning methods in brain network feature extraction, an adaptive graph augmentation strategy combining attribute masking and edge perturbation is implemented for data enhancement. Subsequently, a dual-domain graph transformer (DDGformer) module is constructed to integrate local and global information, where graph convolutional networks aggregate neighborhood features to capture local patterns while attention mechanisms extract global dependencies. Finally, a graph contrastive learning framework is established to maximize the consistency between positive and negative pairs, thereby obtaining high-quality graph representations. Experimental results on real-world datasets demonstrate that the PHGCL-DDGformer model outperforms existing state-of-the-art approaches in brain network classification tasks.\n        △ Less",
        "authors":"['ZhiTeng Zhu', 'Lan Yao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Unified Virtual Mixture-of-Experts Framework:Enhanced Inference and Hallucination Mitigation in Single-Model System",
        "abstract":"Generative models, such as GPT and BERT, have significantly improved performance in tasks like text generation and summarization. However, hallucinations \"where models generate non-factual or misleading content\" are especially problematic in smaller-scale architectures, limiting their real-world applicability.In this paper, we propose a unified Virtual Mixture-of-Experts (MoE) fusion strategy that enhances inference performance and mitigates hallucinations in a single Qwen 1.5 0.5B model without increasing the parameter count. Our method leverages multiple domain-specific expert prompts (with the number of experts being adjustable) to guide the model from different perspectives. We apply a statistical outlier truncation strategy based on the mean and standard deviation to filter out abnormally high probability predictions, and we inject noise into the embedding space to promote output diversity. To clearly assess the contribution of each module, we adopt a fixed voting mechanism rather than a dynamic gating network, thereby avoiding additional confounding factors. We provide detailed theoretical derivations from both statistical and ensemble learning perspectives to demonstrate how our method reduces output variance and suppresses hallucinations. Extensive ablation experiments on dialogue generation tasks show that our approach significantly improves inference accuracy and robustness in small models. Additionally, we discuss methods for evaluating the orthogonality of virtual experts and outline the potential for future work involving dynamic expert weight allocation using gating networks.\n        △ Less",
        "authors":"['Mingyan Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Attention in Diffusion Model: A Survey",
        "abstract":"Attention mechanisms have become a foundational component in diffusion models, significantly influencing their capacity across a wide range of generative and discriminative tasks. This paper presents a comprehensive survey of attention within diffusion models, systematically analysing its roles, design patterns, and operations across different modalities and tasks. We propose a unified taxonomy that categorises attention-related modifications into parts according to the structural components they affect, offering a clear lens through which to understand their functional diversity. In addition to reviewing architectural innovations, we examine how attention mechanisms contribute to performance improvements in diverse applications. We also identify current limitations and underexplored areas, and outline potential directions for future research. Our study provides valuable insights into the evolving landscape of diffusion models, with a particular focus on the integrative and ubiquitous role of attention.\n        △ Less",
        "authors":"['Litao Hua', 'Fan Liu', 'Jie Su', 'Xingyu Miao', 'Zizhou Ouyang', 'Zeyu Wang', 'Runze Hu', 'Zhenyu Wen', 'Bing Zhai', 'Yang Long', 'Haoran Duan', 'Yuan Zhou']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Uncertainty Propagation in XAI: A Comparison of Analytical and Empirical Estimators",
        "abstract":"Understanding uncertainty in Explainable AI (XAI) is crucial for building trust and ensuring reliable decision-making in Machine Learning models. This paper introduces a unified framework for quantifying and interpreting Uncertainty in XAI by defining a general explanation function $e_θ(x, f)$ that captures the propagation of uncertainty from key sources: perturbations in input data and model parameters. By using both analytical and empirical estimates of explanation variance, we provide a systematic means of assessing the impact uncertainty on explanations. We illustrate the approach using a first-order uncertainty propagation as the analytical estimator. In a comprehensive evaluation across heterogeneous datasets, we compare analytical and empirical estimates of uncertainty propagation and evaluate their robustness. Extending previous work on inconsistencies in explanations, our experiments identify XAI methods that do not reliably capture and propagate uncertainty. Our findings underscore the importance of uncertainty-aware explanations in high-stakes applications and offer new insights into the limitations of current XAI methods. The code for the experiments can be found in our repository at https:\/\/github.com\/TeodorChiaburu\/UXAI\n        △ Less",
        "authors":"['Teodor Chiaburu', 'Felix Bießmann', 'Frank Haußer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Misaligned Roles, Misplaced Images: Structural Input Perturbations Expose Multimodal Alignment Blind Spots",
        "abstract":"Multimodal Language Models (MMLMs) typically undergo post-training alignment to prevent harmful content generation. However, these alignment stages focus primarily on the assistant role, leaving the user role unaligned, and stick to a fixed input prompt structure of special tokens, leaving the model vulnerable when inputs deviate from these expectations. We introduce Role-Modality Attacks (RMA), a novel class of adversarial attacks that exploit role confusion between the user and assistant and alter the position of the image token to elicit harmful outputs. Unlike existing attacks that modify query content, RMAs manipulate the input structure without altering the query itself. We systematically evaluate these attacks across multiple Vision Language Models (VLMs) on eight distinct settings, showing that they can be composed to create stronger adversarial prompts, as also evidenced by their increased projection in the negative refusal direction in the residual stream, a property observed in prior successful attacks. Finally, for mitigation, we propose an adversarial training approach that makes the model robust against input prompt perturbations. By training the model on a range of harmful and benign prompts all perturbed with different RMA settings, it loses its sensitivity to Role Confusion and Modality Manipulation attacks and is trained to only pay attention to the content of the query in the input prompt structure, effectively reducing Attack Success Rate (ASR) while preserving the model's general utility.\n        △ Less",
        "authors":"['Erfan Shayegani', 'G M Shahariar', 'Sara Abdali', 'Lei Yu', 'Nael Abu-Ghazaleh', 'Yue Dong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Artificial Geographically Weighted Neural Network: A Novel Framework for Spatial Analysis with Geographically Weighted Layers",
        "abstract":"Geographically Weighted Regression (GWR) is a widely recognized technique for modeling spatial heterogeneity. However, it is commonly assumed that the relationships between dependent and independent variables are linear. To overcome this limitation, we propose an Artificial Geographically Weighted Neural Network (AGWNN), a novel framework that integrates geographically weighted techniques with neural networks to capture complex nonlinear spatial relationships. Central to this framework is the Geographically Weighted Layer (GWL), a specialized component designed to encode spatial heterogeneity within the neural network architecture. To rigorously evaluate the performance of AGWNN, we conducted comprehensive experiments using both simulated datasets and real-world case studies. Our results demonstrate that AGWNN significantly outperforms traditional GWR and standard Artificial Neural Networks (ANNs) in terms of model fitting accuracy. Notably, AGWNN excels in modeling intricate nonlinear relationships and effectively identifies complex spatial heterogeneity patterns, offering a robust and versatile tool for advanced spatial analysis.\n        △ Less",
        "authors":"['Jianfei Cao', 'Dongchao Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Artificial Intelligence and Deep Learning Algorithms for Epigenetic Sequence Analysis: A Review for Epigeneticists and AI Experts",
        "abstract":"Epigenetics encompasses mechanisms that can alter the expression of genes without changing the underlying genetic sequence. The epigenetic regulation of gene expression is initiated and sustained by several mechanisms such as DNA methylation, histone modifications, chromatin conformation, and non-coding RNA. The changes in gene regulation and expression can manifest in the form of various diseases and disorders such as cancer and congenital deformities. Over the last few decades, high throughput experimental approaches have been used to identify and understand epigenetic changes, but these laboratory experimental approaches and biochemical processes are time-consuming and expensive. To overcome these challenges, machine learning and artificial intelligence (AI) approaches have been extensively used for mapping epigenetic modifications to their phenotypic manifestations. In this paper we provide a narrative review of published research on AI models trained on epigenomic data to address a variety of problems such as prediction of disease markers, gene expression, enhancer promoter interaction, and chromatin states. The purpose of this review is twofold as it is addressed to both AI experts and epigeneticists. For AI researchers, we provided a taxonomy of epigenetics research problems that can benefit from an AI-based approach. For epigeneticists, given each of the above problems we provide a list of candidate AI solutions in the literature. We have also identified several gaps in the literature, research challenges, and recommendations to address these challenges.\n        △ Less",
        "authors":"['Muhammad Tahir', 'Mahboobeh Norouzi', 'Shehroz S. Khan', 'James R. Davie', 'Soichiro Yamanaka', 'Ahmed Ashraf']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Scalable Predictive Modelling Approach to Identifying Duplicate Adverse Event Reports for Drugs and Vaccines",
        "abstract":"The practice of pharmacovigilance relies on large databases of individual case safety reports to detect and evaluate potential new causal associations between medicines or vaccines and adverse events. Duplicate reports are separate and unlinked reports referring to the same case of an adverse event involving a specific patient at a certain time. They impede statistical analysis and mislead clinical assessment. The large size of such databases precludes a manual identification of duplicates, and so a computational method must be employed. This paper builds upon a hitherto state of the art model, vigiMatch, modifying existing features and introducing new ones to target known shortcomings of the original model. Two support vector machine classifiers, one for medicines and one for vaccines, classify report pairs as duplicates and non-duplicates. Recall was measured using a diverse collection of 5 independent labelled test sets. Precision was measured by having each model classify a randomly selected stream of pairs of reports until each model classified 100 pairs as duplicates. These pairs were assessed by a medical doctor without indicating which method(s) had flagged each pair. Performance on individual countries was measured by having a medical doctor assess a subset of pairs classified as duplicates for three different countries. The new model achieved higher precision and higher recall for all labelled datasets compared to the previous state of the art model, with comparable performance for medicines and vaccines. The model was shown to produce substantially fewer false positives than the comparator model on pairs from individual countries. The method presented here advances state of the art for duplicate detection in adverse event reports for medicines and vaccines.\n        △ Less",
        "authors":"['Jim W. Barrett', 'Nils Erlanson', 'Joana Félix China', 'G. Niklas Norén']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph Transformer-Based Flood Susceptibility Mapping: Application to the French Riviera and Railway Infrastructure Under Climate Change",
        "abstract":"Increasing flood frequency and severity due to climate change threatens infrastructure and demands improved susceptibility mapping techniques. While traditional machine learning (ML) approaches are widely used, they struggle to capture spatial dependencies and poor boundary delineation between susceptibility classes. This study introduces the first application of a graph transformer (GT) architecture for flood susceptibility mapping to the flood-prone French Riviera (e.g., 2020 Storm Alex) using topography, hydrology, geography, and environmental data. GT incorporates watershed topology using Laplacian positional encoders (PEs) and attention mechanisms. The developed GT model has an AUC-ROC (0.9739), slightly lower than XGBoost (0.9853). However, the GT model demonstrated better clustering and delineation with a higher Moran's I value (0.6119) compared to the random forest (0.5775) and XGBoost (0.5311) with p-value lower than 0.0001. Feature importance revealed a striking consistency across models, with elevation, slope, distance to channel, and convergence index being the critical factors. Dimensionality reduction on Laplacian PEs revealed partial clusters, indicating they could capture spatial information; however, their importance was lower than flood factors. Since climate and land use changes aggravate flood risk, susceptibility maps are developed for the 2050 year under different Representative Concentration Pathways (RCPs) and railway track vulnerability is assessed. All RCP scenarios revealed increased area across susceptibility classes, except for the very low category. RCP 8.5 projections indicate that 17.46% of the watershed area and 54% of railway length fall within very-high susceptible zones, compared to 6.19% and 35.61%, respectively, under current conditions. The developed maps can be integrated into a multi-hazard framework.\n        △ Less",
        "authors":"['Sreenath Vemula', 'Filippo Gatti', 'Pierre Jehel']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detecting Malicious AI Agents Through Simulated Interactions",
        "abstract":"This study investigates malicious AI Assistants' manipulative traits and whether the behaviours of malicious AI Assistants can be detected when interacting with human-like simulated users in various decision-making contexts. We also examine how interaction depth and ability of planning influence malicious AI Assistants' manipulative strategies and effectiveness. Using a controlled experimental design, we simulate interactions between AI Assistants (both benign and deliberately malicious) and users across eight decision-making scenarios of varying complexity and stakes. Our methodology employs two state-of-the-art language models to generate interaction data and implements Intent-Aware Prompting (IAP) to detect malicious AI Assistants. The findings reveal that malicious AI Assistants employ domain-specific persona-tailored manipulation strategies, exploiting simulated users' vulnerabilities and emotional triggers. In particular, simulated users demonstrate resistance to manipulation initially, but become increasingly vulnerable to malicious AI Assistants as the depth of the interaction increases, highlighting the significant risks associated with extended engagement with potentially manipulative systems. IAP detection methods achieve high precision with zero false positives but struggle to detect many malicious AI Assistants, resulting in high false negative rates. These findings underscore critical risks in human-AI interactions and highlight the need for robust, context-sensitive safeguards against manipulative AI behaviour in increasingly autonomous decision-support systems.\n        △ Less",
        "authors":"['Yulu Pi', 'Ella Bettison', 'Anna Becker']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Timeseries Foundation Models for Mobility: A Benchmark Comparison with Traditional and Deep Learning Models",
        "abstract":"Crowd and flow predictions have been extensively studied in mobility data science. Traditional forecasting methods have relied on statistical models such as ARIMA, later supplemented by deep learning approaches like ST-ResNet. More recently, foundation models for time series forecasting, such as TimeGPT, Chronos, and LagLlama, have emerged. A key advantage of these models is their ability to generate zero-shot predictions, allowing them to be applied directly to new tasks without retraining. This study evaluates the performance of TimeGPT compared to traditional approaches for predicting city-wide mobility timeseries using two bike-sharing datasets from New York City and Vienna, Austria. Model performance is assessed across short (1-hour), medium (12-hour), and long-term (24-hour) forecasting horizons. The results highlight the potential of foundation models for mobility forecasting while also identifying limitations of our experiments.\n        △ Less",
        "authors":"['Anita Graser']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TransNet: Transfer Knowledge for Few-shot Knowledge Graph Completion",
        "abstract":"Knowledge graphs (KGs) are ubiquitous and widely used in various applications. However, most real-world knowledge graphs are incomplete, which significantly degrades their performance on downstream tasks. Additionally, the relationships in real-world knowledge graphs often follow a long-tail distribution, meaning that most relations are represented by only a few training triplets. To address these challenges, few-shot learning has been introduced. Few-shot KG completion aims to make accurate predictions for triplets involving novel relations when only a limited number of training triplets are available. Although many methods have been proposed, they typically learn each relation individually, overlooking the correlations between different tasks and the relevant information in previously trained tasks. In this paper, we propose a transfer learning-based few-shot KG completion method (TransNet). By learning the relationships between different tasks, TransNet effectively transfers knowledge from similar tasks to improve the current task's performance. Furthermore, by employing meta-learning, TransNet can generalize effectively to new, unseen relations. Extensive experiments on benchmark datasets demonstrate the superiority of TransNet over state-of-the-art methods. Code can be found at https:\/\/github.com\/lihuiliullh\/TransNet\/tree\/main\n        △ Less",
        "authors":"['Lihui Liu', 'Zihao Wang', 'Dawei Zhou', 'Ruijie Wang', 'Yuchen Yan', 'Bo Xiong', 'Sihong He', 'Kai Shu', 'Hanghang Tong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Symmetric Low-Rank Adapters",
        "abstract":"In this paper, we introduce Symmetric Low-Rank Adapters, an optimized variant of LoRA with even fewer weights. This method utilizes Low-Rank Symmetric Weight Matrices to learn downstream tasks more efficiently. Traditional LoRA accumulates fine-tuning weights with the original pre-trained weights via a Singular Value Decomposition (SVD) like approach, i.e., model weights are fine-tuned via updates of the form $BA$ (where $B \\in \\mathbb{R}^{n\\times r}$, $A \\in \\mathbb{R}^{r\\times n}$, and $r$ is the rank of the merged weight matrix). In contrast, our approach, named SymLoRA, represents fine-tuning weights as a Spectral Decomposition, i.e., $Q \\, diag(Λ)\\, Q^T$, where $Q \\in \\mathbb{R}^{n\\times r}$ and $Λ\\in \\mathbb{R}^r$. SymLoRA requires approximately half of the finetuning weights. Here, we show that this approach has negligible losses in downstream efficacy.\n        △ Less",
        "authors":"['Tales Panoutsos', 'Rodrygo L. T. Santos', 'Flavio Figueiredo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Task-Aware Parameter-Efficient Fine-Tuning of Large Pre-Trained Models at the Edge",
        "abstract":"Large language models (LLMs) have achieved remarkable success in various tasks, such as decision-making, reasoning, and question answering. They have been widely used in edge devices. However, fine-tuning LLMs to specific tasks at the edge is challenging due to the high computational cost and the limited storage and energy resources at the edge. To address this issue, we propose TaskEdge, a task-aware parameter-efficient fine-tuning framework at the edge, which allocates the most effective parameters to the target task and only updates the task-specific parameters. Specifically, we first design a parameter importance calculation criterion that incorporates both weights and input activations into the computation of weight importance. Then, we propose a model-agnostic task-specific parameter allocation algorithm to ensure that task-specific parameters are distributed evenly across the model, rather than being concentrated in specific regions. In doing so, TaskEdge can significantly reduce the computational cost and memory usage while maintaining performance on the target downstream tasks by updating less than 0.1\\% of the parameters. In addition, TaskEdge can be easily integrated with structured sparsity to enable acceleration by NVIDIA's specialized sparse tensor cores, and it can be seamlessly integrated with LoRA to enable efficient sparse low-rank adaptation. Extensive experiments on various tasks demonstrate the effectiveness of TaskEdge.\n        △ Less",
        "authors":"['Senkang Hu', 'Yanan Ma', 'Yihang Tao', 'Zhengru Fang', 'Zihan Fang', 'Yiqin Deng', 'Sam Kwong', 'Yuguang Fang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RaanA: A Fast, Flexible, and Data-Efficient Post-Training Quantization Algorithm",
        "abstract":"Post-training Quantization (PTQ) has become a widely used technique for improving inference efficiency of large language models (LLMs). However, existing PTQ methods generally suffer from crucial limitations such as heavy calibration data requirements and inflexible choice of target number of bits. In this paper, we propose RaanA, a unified PTQ framework that overcomes these challenges by introducing two novel components: 1) RaBitQ-H, a variant of a randomized vector quantization method RaBitQ, designed for fast, accurate, and highly efficient quantization; and 2) AllocateBits, an algorithm that optimally allocates bit-widths across layers based on their quantization sensitivity. RaanA achieves competitive performance with state-of-the-art quantization methods while being extremely fast, requiring minimal calibration data, and enabling flexible bit allocation. Extensive experiments demonstrate RaanA's efficacy in balancing efficiency and accuracy. The code is publicly available at https:\/\/github.com\/FFTYYY\/RaanA .\n        △ Less",
        "authors":"['Yongyi Yang', 'Jianyang Gao', 'Wei Hu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ethical AI on the Waitlist: Group Fairness Evaluation of LLM-Aided Organ Allocation",
        "abstract":"Large Language Models (LLMs) are becoming ubiquitous, promising automation even in high-stakes scenarios. However, existing evaluation methods often fall short -- benchmarks saturate, accuracy-based metrics are overly simplistic, and many inherently ambiguous problems lack a clear ground truth. Given these limitations, evaluating fairness becomes complex. To address this, we reframe fairness evaluation using Borda scores, a method from voting theory, as a nuanced yet interpretable metric for measuring fairness. Using organ allocation as a case study, we introduce two tasks: (1) Choose-One and (2) Rank-All. In Choose-One, LLMs select a single candidate for a kidney, and we assess fairness across demographics using proportional parity. In Rank-All, LLMs rank all candidates for a kidney, reflecting real-world allocation processes. Since traditional fairness metrics do not account for ranking, we propose a novel application of Borda scoring to capture biases. Our findings highlight the potential of voting-based metrics to provide a richer, more multifaceted evaluation of LLM fairness.\n        △ Less",
        "authors":"['Hannah Murray', 'Brian Hyeongseok Kim', 'Isabelle Lee', 'Jason Byun', 'Dani Yogatama', 'Evi Micha']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Objective Quality-Diversity in Unstructured and Unbounded Spaces",
        "abstract":"Quality-Diversity algorithms are powerful tools for discovering diverse, high-performing solutions. Recently, Multi-Objective Quality-Diversity (MOQD) extends QD to problems with several objectives while preserving solution diversity. MOQD has shown promise in fields such as robotics and materials science, where finding trade-offs between competing objectives like energy efficiency and speed, or material properties is essential. However, existing methods in MOQD rely on tessellating the feature space into a grid structure, which prevents their application in domains where feature spaces are unknown or must be learned, such as complex biological systems or latent exploration tasks. In this work, we introduce Multi-Objective Unstructured Repertoire for Quality-Diversity (MOUR-QD), a MOQD algorithm designed for unstructured and unbounded feature spaces. We evaluate MOUR-QD on five robotic tasks. Importantly, we show that our method excels in tasks where features must be learned, paving the way for applying MOQD to unsupervised domains. We also demonstrate that MOUR-QD is advantageous in domains with unbounded feature spaces, outperforming existing grid-based methods. Finally, we demonstrate that MOUR-QD is competitive with established MOQD methods on existing MOQD tasks and achieves double the MOQD-score in some environments. MOUR-QD opens up new opportunities for MOQD in domains like protein design and image generation.\n        △ Less",
        "authors":"['Hannah Janmohamed', 'Antoine Cully']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models",
        "abstract":"Large Language Models (LLMs) and Vision-Language Models (VLMs) have become essential to general artificial intelligence, exhibiting remarkable capabilities in task understanding and problem-solving. However, the real-world reliability of these models critically depends on their stability, which remains an underexplored area. Despite their widespread use, rigorous studies examining the stability of LLMs under various perturbations are still lacking. In this paper, we address this gap by proposing a novel stability measure for LLMs, inspired by statistical methods rooted in information geometry. Our measure possesses desirable invariance properties, making it well-suited for analyzing model sensitivity to both parameter and input perturbations. To assess the effectiveness of our approach, we conduct extensive experiments on models ranging in size from 1.5B to 13B parameters. Our results demonstrate the utility of our measure in identifying salient parameters and detecting vulnerable regions in input images or critical dimensions in token embeddings. Furthermore, leveraging our stability framework, we enhance model robustness during model merging, leading to improved performance.\n        △ Less",
        "authors":"['Runpeng Dai', 'Run Yang', 'Fan Zhou', 'Hongtu Zhu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RLDBF: Enhancing LLMs Via Reinforcement Learning With DataBase FeedBack",
        "abstract":"While current large language models (LLMs) demonstrate remarkable linguistic capabilities through training on massive unstructured text corpora, they remain inadequate in leveraging structured scientific data (e.g., chemical molecular properties in databases) that encapsulate centuries of accumulated scientific expertise. These structured datasets hold strategic significance for advancing AI for Science yet current approaches merely treat them as auxiliary supplements to unstructured text. This study pioneers a systematic investigation into enhancing LLMs with structured scientific data, using chemical molecular science as a testbed. We investigate the impact of incorporating molecular property data on LLM across distinct training phases, including continual pre-training, supervised fine-tuning, and reinforcement learning. Notably, to address the inherent limitation of numerical insensitivity in large models, we propose an innovative methodology termed \"Reinforcement Learning with Database Feedback\" (RLDBF). Experimental evaluations demonstrate the efficacy of the proposed approach, with the model exhibiting remarkable generalization capabilities on previously unseen data and other chemical tasks. The results substantiate the potential of our method in advancing the field of structured scientific data processing within LLMs.\n        △ Less",
        "authors":"['Weichen Dai', 'Zijie Dai', 'Zhijie Huang', 'Yixuan Pan', 'Xinhe Li', 'Xi Li', 'Yi Zhou', 'Ji Qi', 'Wu Jiang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scalable heliostat surface predictions from focal spots: Sim-to-Real transfer of inverse Deep Learning Raytracing",
        "abstract":"Concentrating Solar Power (CSP) plants are a key technology in the transition toward sustainable energy. A critical factor for their safe and efficient operation is the distribution of concentrated solar flux on the receiver. However, flux distributions from individual heliostats are sensitive to surface imperfections. Measuring these surfaces across many heliostats remains impractical in real-world deployments. As a result, control systems often assume idealized heliostat surfaces, leading to suboptimal performance and potential safety risks. To address this, inverse Deep Learning Raytracing (iDLR) has been introduced as a novel method for inferring heliostat surface profiles from target images recorded during standard calibration procedures. In this work, we present the first successful Sim-to-Real transfer of iDLR, enabling accurate surface predictions directly from real-world target images. We evaluate our method on 63 heliostats under real operational conditions. iDLR surface predictions achieve a median mean absolute error (MAE) of 0.17 mm and show good agreement with deflectometry ground truth in 84% of cases. When used in raytracing simulations, it enables flux density predictions with a mean accuracy of 90% compared to deflectometry over our dataset, and outperforms the commonly used ideal heliostat surface assumption by 26%. We tested this approach in a challenging double-extrapolation scenario-involving unseen sun positions and receiver projection-and found that iDLR maintains high predictive accuracy, highlighting its generalization capabilities. Our results demonstrate that iDLR is a scalable, automated, and cost-effective solution for integrating realistic heliostat surface models into digital twins. This opens the door to improved flux control, more precise performance modeling, and ultimately, enhanced efficiency and safety in future CSP plants.\n        △ Less",
        "authors":"['Jan Lewen', 'Max Pargmann', 'Jenia Jitsev', 'Mehdi Cherti', 'Robert Pitz-Paal', 'Daniel Maldonado Quinto']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA",
        "abstract":"Artificial intelligence (AI)-driven electronic design automation (EDA) techniques have been extensively explored for VLSI circuit design applications. Most recently, foundation AI models for circuits have emerged as a new technology trend. Unlike traditional task-specific AI solutions, these new AI models are developed through two stages: 1) self-supervised pre-training on a large amount of unlabeled data to learn intrinsic circuit properties; and 2) efficient fine-tuning for specific downstream applications, such as early-stage design quality evaluation, circuit-related context generation, and functional verification. This new paradigm brings many advantages: model generalization, less reliance on labeled circuit data, efficient adaptation to new tasks, and unprecedented generative capability. In this paper, we propose referring to AI models developed with this new paradigm as circuit foundation models (CFMs). This paper provides a comprehensive survey of the latest progress in circuit foundation models, unprecedentedly covering over 130 relevant works. Over 90% of our introduced works were published in or after 2022, indicating that this emerging research trend has attracted wide attention in a short period. In this survey, we propose to categorize all existing circuit foundation models into two primary types: 1) encoder-based methods performing general circuit representation learning for predictive tasks; and 2) decoder-based methods leveraging large language models (LLMs) for generative tasks. For our introduced works, we cover their input modalities, model architecture, pre-training strategies, domain adaptation techniques, and downstream design applications. In addition, this paper discussed the unique properties of circuits from the data perspective. These circuit properties have motivated many works in this domain and differentiated them from general AI techniques.\n        △ Less",
        "authors":"['Wenji Fang', 'Jing Wang', 'Yao Lu', 'Shang Liu', 'Yuchao Wu', 'Yuzhe Ma', 'Zhiyao Xie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Geometric Flow Models over Neural Network Weights",
        "abstract":"Deep generative models such as flow and diffusion models have proven to be effective in modeling high-dimensional and complex data types such as videos or proteins, and this has motivated their use in different data modalities, such as neural network weights. A generative model of neural network weights would be useful for a diverse set of applications, such as Bayesian deep learning, learned optimization, and transfer learning. However, the existing work on weight-space generative models often ignores the symmetries of neural network weights, or only takes into account a subset of them. Modeling those symmetries, such as permutation symmetries between subsequent layers in an MLP, the filters in a convolutional network, or scaling symmetries arising with the use of non-linear activations, holds the potential to make weight-space generative modeling more efficient by effectively reducing the dimensionality of the problem.\n  In this light, we aim to design generative models in weight-space that more comprehensively respect the symmetries of neural network weights. We build on recent work on generative modeling with flow matching, and weight-space graph neural networks to design three different weight-space flows. Each of our flows takes a different approach to modeling the geometry of neural network weights, and thus allows us to explore the design space of weight-space flows in a principled way. Our results confirm that modeling the geometry of neural networks more faithfully leads to more effective flow models that can generalize to different tasks and architectures, and we show that while our flows obtain competitive performance with orders of magnitude fewer parameters than previous work, they can be further improved by scaling them up. We conclude by listing potential directions for future work on weight-space generative models.\n        △ Less",
        "authors":"['Ege Erdogan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Practical Emotion Recognition: An Unsupervised Source-Free Approach for EEG Domain Adaptation",
        "abstract":"Emotion recognition is crucial for advancing mental health, healthcare, and technologies like brain-computer interfaces (BCIs). However, EEG-based emotion recognition models face challenges in cross-domain applications due to the high cost of labeled data and variations in EEG signals from individual differences and recording conditions. Unsupervised domain adaptation methods typically require access to source domain data, which may not always be feasible in real-world scenarios due to privacy and computational constraints. Source-free unsupervised domain adaptation (SF-UDA) has recently emerged as a solution, enabling target domain adaptation without source data, but its application in emotion recognition remains unexplored. We propose a novel SF-UDA approach for EEG-based emotion classification across domains, introducing a multi-stage framework that enhances model adaptability without requiring source data. Our approach incorporates Dual-Loss Adaptive Regularization (DLAR) to minimize prediction discrepancies on confident samples and align predictions with expected pseudo-labels. Additionally, we introduce Localized Consistency Learning (LCL), which enforces local consistency by promoting similar predictions from reliable neighbors. These techniques together address domain shift and reduce the impact of noisy pseudo-labels, a key challenge in traditional SF-UDA models. Experiments on two widely used datasets, DEAP and SEED, demonstrate the effectiveness of our method. Our approach significantly outperforms state-of-the-art methods, achieving 65.84% accuracy when trained on DEAP and tested on SEED, and 58.99% accuracy in the reverse scenario. It excels at detecting both positive and negative emotions, making it well-suited for practical emotion recognition applications.\n        △ Less",
        "authors":"['Md Niaz Imtiaz', 'Naimul Khan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A multi-scale lithium-ion battery capacity prediction using mixture of experts and patch-based MLP",
        "abstract":"Lithium-ion battery health management has become increasingly important as the application of batteries expands. Precise forecasting of capacity degradation is critical for ensuring the healthy usage of batteries. In this paper, we innovatively propose MSPMLP, a multi-scale capacity prediction model utilizing the mixture of experts (MoE) architecture and patch-based multi-layer perceptron (MLP) blocks, to capture both the long-term degradation trend and local capacity regeneration phenomena. Specifically, we utilize patch-based MLP blocks with varying patch sizes to extract multi-scale features from the capacity sequence. Leveraging the MoE architecture, the model adaptively integrates the extracted features, thereby enhancing its capacity and expressiveness. Finally, the future battery capacity is predicted based on the integrated features, achieving high prediction accuracy and generalization. Experimental results on the public NASA dataset indicate that MSPMLP achieves a mean absolute error (MAE) of 0.0078, improving by 41.8\\% compared to existing methods. These findings highlight that MSPMLP, owing to its multi-scale modeling capability and generalizability, provides a promising solution to the battery capacity prediction challenges caused by capacity regeneration phenomena and complex usage conditions. The code of this work is provided at https:\/\/github.com\/LeiYuzhu\/CapacityPredict.\n        △ Less",
        "authors":"['Yuzhu Lei', 'Guanding Yu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hierarchical Attention Network for Interpretable ECG-based Heart Disease Classification",
        "abstract":"Cardiovascular disease remains one of the leading causes of mortality worldwide, underscoring the need for accurate as well as interpretable diagnostic machine learning tools. In this work, we investigate heart disease classification using electrocardiogram (ECG) data from two widely-utilized datasets: The MIT-BIH Arrhythmia and the PTB-XL datasets. We adapt a hierarchical attention network (HAN), originally developed for text classification, into an ECG-based heart-disease classification task. Our adapted HAN incorporates two attention layers that focus on ECG data segments of varying sizes. We conduct a comparative analysis between our adapted HAN and a more sophisticated state-of-the-art architecture, featuring a network with convolution, attention, and transformer layers (CAT-Net). Our empirical evaluation encompasses multiple aspects including test accuracy (quantified by 0-1 loss); model complexity (measured by the number of model parameters); and interpretability (through attention map visualization). Our adapted HAN demonstrates comparable test accuracy with significant reductions in model complexity and enhanced interpretability analysis: For the MIT-BIH dataset, our adapted HAN achieves 98.55\\% test accuracy compared to 99.14\\% for CAT-Net, while reducing the number of model parameters by a factor of 15.6. For the PTB-XL dataset, our adapted HAN achieves a 19.3-fold reduction in model complexity compared to CAT-Net, with only a 5\\% lower test accuracy. From an interpretability perspective, the significantly simpler architecture and the hierarchical nature of our adapted HAN model facilitate a more straightforward interpretability analysis based on visualizing attention weights. Building on this advantage, we conduct an interpretability analysis of our HAN that highlights the regions of the ECG signal most relevant to the model's decisions.\n        △ Less",
        "authors":"['Mario Padilla Rodriguez', 'Mohamed Nafea']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Chemistry-aware battery degradation prediction under simulated real-world cyclic protocols",
        "abstract":"Battery degradation is governed by complex and randomized cyclic conditions, yet existing modeling and prediction frameworks usually rely on rigid, unchanging protocols that fail to capture real-world dynamics. The stochastic electrical signals make such prediction extremely challenging, while, on the other hand, they provide abundant additional information, such as voltage fluctuations, which may probe the degradation mechanisms. Here, we present chemistry-aware battery degradation prediction under dynamic conditions with machine learning, which integrates hidden Markov processes for realistic power simulations, an automated batch-testing system that generates a large electrochemical dataset under randomized conditions, an interfacial chemistry database derived from high-throughput X-ray photoelectron spectroscopy for mechanistic probing, and a machine learning model for prediction. By automatically constructing a polynomial-scale feature space from irregular electrochemical curves, our model accurately predicts both battery life and critical knee points. This feature space also predicts the composition of the solid electrolyte interphase, revealing six distinct failure mechanisms-demonstrating a viable approach to use electrical signals to infer interfacial chemistry. This work establishes a scalable and adaptive framework for integrating chemical engineering and data science to advance noninvasive diagnostics and optimize processes for more durable and sustainable energy storage technologies.\n        △ Less",
        "authors":"['Yuqi Li', 'Han Zhang', 'Xiaofan Gui', 'Zhao Chen', 'Yu Li', 'Xiwen Chi', 'Quan Zhou', 'Shun Zheng', 'Ziheng Lu', 'Wei Xu', 'Jiang Bian', 'Liquan Chen', 'Hong Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SAFE: Self-Adjustment Federated Learning Framework for Remote Sensing Collaborative Perception",
        "abstract":"The rapid increase in remote sensing satellites has led to the emergence of distributed space-based observation systems. However, existing distributed remote sensing models often rely on centralized training, resulting in data leakage, communication overhead, and reduced accuracy due to data distribution discrepancies across platforms. To address these challenges, we propose the \\textit{Self-Adjustment FEderated Learning} (SAFE) framework, which innovatively leverages federated learning to enhance collaborative sensing in remote sensing scenarios. SAFE introduces four key strategies: (1) \\textit{Class Rectification Optimization}, which autonomously addresses class imbalance under unknown local and global distributions. (2) \\textit{Feature Alignment Update}, which mitigates Non-IID data issues via locally controlled EMA updates. (3) \\textit{Dual-Factor Modulation Rheostat}, which dynamically balances optimization effects during training. (4) \\textit{Adaptive Context Enhancement}, which is designed to improve model performance by dynamically refining foreground regions, ensuring computational efficiency with accuracy improvement across distributed satellites. Experiments on real-world image classification and object segmentation datasets validate the effectiveness and reliability of the SAFE framework in complex remote sensing scenarios.\n        △ Less",
        "authors":"['Xiaohe Li', 'Haohua Wu', 'Jiahao Li', 'Zide Fan', 'Kaixin Zhang', 'Xinming Li', 'Yunping Ge', 'Xinyu Zhao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance",
        "abstract":"Recent advances in the data-driven medicine approach, which integrates ethically managed and explainable artificial intelligence into clinical decision support systems (CDSS), are critical to ensure reliable and effective patient care. This paper focuses on comparing novel agent system designs that use modular agents to analyze laboratory results, vital signs, and clinical context, and to predict and validate results. We implement our agent system with the eICU database, including running lab analysis, vitals-only interpreters, and contextual reasoners agents first, then sharing the memory into the integration agent, prediction agent, transparency agent, and a validation agent. Our results suggest that the multi-agent system (MAS) performed better than the single-agent system (SAS) with mortality prediction accuracy (59%, 56%) and the mean error for length of stay (LOS)(4.37 days, 5.82 days), respectively. However, the transparency score for the SAS (86.21) is slightly better than the transparency score for MAS (85.5). Finally, this study suggests that our agent-based framework not only improves process transparency and prediction accuracy but also strengthens trustworthy AI-assisted decision support in an intensive care setting.\n        △ Less",
        "authors":"['Ying-Jung Chen', 'Ahmad Albarqawi', 'Chi-Sheng Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Are Anxiety Detection Models Generalizable? A Cross-Activity and Cross-Population Study Using Wearables",
        "abstract":"Anxiety-provoking activities, such as public speaking, can trigger heightened anxiety responses in individuals with anxiety disorders. Recent research suggests that physiological signals, including electrocardiogram (ECG) and electrodermal activity (EDA), collected via wearable devices, can be used to detect anxiety in such contexts through machine learning models. However, the generalizability of these anxiety prediction models across different activities and diverse populations remains underexplored-an essential step for assessing model bias and fostering user trust in broader applications. To address this gap, we conducted a study with 111 participants who engaged in three anxiety-provoking activities. Utilizing both our collected dataset and two well-known publicly available datasets, we evaluated the generalizability of anxiety detection models within participants (for both same-activity and cross-activity scenarios) and across participants (within-activity and cross-activity). In total, we trained and tested more than 3348 anxiety detection models (using six classifiers, 31 feature sets, and 18 train-test configurations). Our results indicate that three key metrics-AUROC, recall for anxious states, and recall for non-anxious states-were slightly above the baseline score of 0.5. The best AUROC scores ranged from 0.62 to 0.73, with recall for the anxious class spanning 35.19% to 74.3%. Interestingly, model performance (as measured by AUROC) remained relatively stable across different activities and participant groups, though recall for the anxious class did exhibit some variation.\n        △ Less",
        "authors":"['Nilesh Kumar Sahu', 'Snehil Gupta', 'Haroon R Lone']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Theoretical Framework for Graph-based Digital Twins for Supply Chain Management and Optimization",
        "abstract":"Supply chain management is growing increasingly complex due to globalization, evolving market demands, and sustainability pressures, yet traditional systems struggle with fragmented data and limited analytical capabilities. Graph-based modeling offers a powerful way to capture the intricate relationships within supply chains, while Digital Twins (DTs) enable real-time monitoring and dynamic simulations. However, current implementations often face challenges related to scalability, data integration, and the lack of sustainability-focused metrics. To address these gaps, we propose a Graph-Based Digital Twin Framework for Supply Chain Optimization, which combines graph modeling with DT architecture to create a dynamic, real-time representation of supply networks. Our framework integrates a Data Integration Layer to harmonize disparate sources, a Graph Construction Module to model complex dependencies, and a Simulation and Analysis Engine for scalable optimization. Importantly, we embed sustainability metrics - such as carbon footprints and resource utilization - into operational dashboards to drive eco-efficiency. By leveraging the synergy between graph-based modeling and DTs, our approach enhances scalability, improves decision-making, and enables organizations to proactively manage disruptions, cut costs, and transition toward greener, more resilient supply chains.\n        △ Less",
        "authors":"['Azmine Toushik Wasi', 'Mahfuz Ahmed Anik', 'Abdur Rahman', 'Md. Iqramul Hoque', 'MD Shafikul Islam', 'Md Manjurul Ahsan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning to Interfere in Non-Orthogonal Multiple-Access Joint Source-Channel Coding",
        "abstract":"We consider multiple transmitters aiming to communicate their source signals (e.g., images) over a multiple access channel (MAC). Conventional communication systems minimize interference by orthogonally allocating resources (time and\/or bandwidth) among users, which limits their capacity. We introduce a machine learning (ML)-aided wireless image transmission method that merges compression and channel coding using a multi-view autoencoder, which allows the transmitters to use all the available channel resources simultaneously, resulting in a non-orthogonal multiple access (NOMA) scheme. The receiver must recover all the images from the received superposed signal, while also associating each image with its transmitter. Traditional ML models deal with individual samples, whereas our model allows signals from different users to interfere in order to leverage gains from NOMA under limited bandwidth and power constraints. We introduce a progressive fine-tuning algorithm that doubles the number of users at each iteration, maintaining initial performance with orthogonalized user-specific projections, which is then improved through fine-tuning steps. Remarkably, our method scales up to 16 users and beyond, with only a 0.6% increase in the number of trainable parameters compared to a single-user model, significantly enhancing recovered image quality and outperforming existing NOMA-based methods over a wide range of datasets, metrics, and channel conditions. Our approach paves the way for more efficient and robust multi-user communication systems, leveraging innovative ML components and strategies.\n        △ Less",
        "authors":"['Selim F. Yilmaz', 'Can Karamanli', 'Deniz Gunduz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CLCR: Contrastive Learning-based Constraint Reordering for Efficient MILP Solving",
        "abstract":"Constraint ordering plays a critical role in the efficiency of Mixed-Integer Linear Programming (MILP) solvers, particularly for large-scale problems where poorly ordered constraints trigger increased LP iterations and suboptimal search trajectories. This paper introduces CLCR (Contrastive Learning-based Constraint Reordering), a novel framework that systematically optimizes constraint ordering to accelerate MILP solving. CLCR first clusters constraints based on their structural patterns and then employs contrastive learning with a pointer network to optimize their sequence, preserving problem equivalence while improving solver efficiency. Experiments on benchmarks show CLCR reduces solving time by 30% and LP iterations by 25% on average, without sacrificing solution accuracy. This work demonstrates the potential of data-driven constraint ordering to enhance optimization models, offering a new paradigm for bridging mathematical programming with machine learning.\n        △ Less",
        "authors":"['Shuli Zeng', 'Mengjie Zhou', 'Sijia Zhang', 'Yixiang Hu', 'Feng Wu', 'Xiang-Yang Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Revisiting Outage for Edge Inference Systems",
        "abstract":"One of the key missions of sixth-generation (6G) mobile networks is to deploy large-scale artificial intelligence (AI) models at the network edge to provide remote-inference services for edge devices. The resultant platform, known as edge inference, will support a wide range of Internet-of-Things applications, such as autonomous driving, industrial automation, and augmented reality. Given the mission-critical and time-sensitive nature of these tasks, it is essential to design edge inference systems that are both reliable and capable of meeting stringent end-to-end (E2E) latency constraints. Existing studies, which primarily focus on communication reliability as characterized by channel outage probability, may fail to guarantee E2E performance, specifically in terms of E2E inference accuracy and latency. To address this limitation, we propose a theoretical framework that introduces and mathematically characterizes the inference outage (InfOut) probability, which quantifies the likelihood that the E2E inference accuracy falls below a target threshold. Under an E2E latency constraint, this framework establishes a fundamental tradeoff between communication overhead (i.e., uploading more sensor observations) and inference reliability as quantified by the InfOut probability. To find a tractable way to optimize this tradeoff, we derive accurate surrogate functions for InfOut probability by applying a Gaussian approximation to the distribution of the received discriminant gain. Experimental results demonstrate the superiority of the proposed design over conventional communication-centric approaches in terms of E2E inference reliability.\n        △ Less",
        "authors":"['Zhanwei Wang', 'Qunsong Zeng', 'Haotian Zheng', 'Kaibin Huang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust Blind Channel Estimation for Bursty Impulsive Noise with a Constrained EM Approach",
        "abstract":"Impulsive noise (IN) commonly generated by power devices can severely degrade the performance of high sensitivity wireless receivers. Accurate channel state information (CSI) knowledge is essential for designing optimal maximum a posteriori detectors. This paper examines blind channel estimation methods based on the expectation-maximization (EM) algorithm tailored for scenarios impacted by bursty IN, which can be described by the Markov-Middleton model. We propose a constrained EM algorithm that exploits the trellis structure of the IN model and the transmitted binary phase shift keying (BPSK) symbols. By enforcing shared variance among specific trellis states and symmetry in the transition matrix, the proposed constrained EM algorithm adapted for the bursty IN channel has an almost two times faster convergence rate and better estimation performance than the standard EM approach. We comprehensively evaluate the robustness of both standard and constrained EM estimators under different types of CSI uncertainties. The results indicate that the final estimations of both EM estimators are robust enough to mismatch Markov-Middleton model parameters. However, as the level of CSI uncertainty increases, the convergence rate decreases.\n        △ Less",
        "authors":"['Chin-Hung Chen', 'Ivana Nikoloska', 'Wim van Houtum', 'Yan Wu', 'Boris Karanov', 'Alex Alvarado']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Intelligent Resource Allocation Optimization for Cloud Computing via Machine Learning",
        "abstract":"With the rapid expansion of cloud computing applications, optimizing resource allocation has become crucial for improving system performance and cost efficiency. This paper proposes an intelligent resource allocation algorithm that leverages deep learning (LSTM) for demand prediction and reinforcement learning (DQN) for dynamic scheduling. By accurately forecasting computing resource demands and enabling real-time adjustments, the proposed system enhances resource utilization by 32.5%, reduces average response time by 43.3%, and lowers operational costs by 26.6%. Experimental results in a production cloud environment confirm that the method significantly improves efficiency while maintaining high service quality. This study provides a scalable and effective solution for intelligent cloud resource management, offering valuable insights for future cloud optimization strategies.\n        △ Less",
        "authors":"['Yuqing Wang', 'Xiao Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"End-to-End Deep Learning for Real-Time Neuroimaging-Based Assessment of Bimanual Motor Skills",
        "abstract":"The real-time assessment of complex motor skills presents a challenge in fields such as surgical training and rehabilitation. Recent advancements in neuroimaging, particularly functional near-infrared spectroscopy (fNIRS), have enabled objective assessment of such skills with high accuracy. However, these techniques are hindered by extensive preprocessing requirements to extract neural biomarkers. This study presents a novel end-to-end deep learning framework that processes raw fNIRS signals directly, eliminating the need for intermediate preprocessing steps. The model was evaluated on datasets from three distinct bimanual motor tasks--suturing, pattern cutting, and endotracheal intubation (ETI)--using performance metrics derived from both training and retention datasets. It achieved a mean classification accuracy of 93.9% (SD 4.4) and a generalization accuracy of 92.6% (SD 1.9) on unseen skill retention datasets, with a leave-one-subject-out cross-validation yielding an accuracy of 94.1% (SD 3.6). Contralateral prefrontal cortex activations exhibited task-specific discriminative power, while motor cortex activations consistently contributed to accurate classification. The model also demonstrated resilience to neurovascular coupling saturation caused by extended task sessions, maintaining robust performance across trials. Comparative analysis confirms that the end-to-end model performs on par with or surpasses baseline models optimized for fully processed fNIRS data, with statistically similar (p<0.05) or improved prediction accuracies. By eliminating the need for extensive signal preprocessing, this work provides a foundation for real-time, non-invasive assessment of bimanual motor skills in medical training environments, with potential applications in robotics, rehabilitation, and sports.\n        △ Less",
        "authors":"['Aseem Subedi', 'Rahul', 'Lora Cavuoto', 'Steven Schwaitzberg', 'Matthew Hackett', 'Jack Norfleet', 'Suvranu De']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Predictive Maintenance of Electric Motors Using Supervised Learning Models: A Comparative Analysis",
        "abstract":"Predictive maintenance is a key strategy for ensuring the reliability and efficiency of industrial systems. This study investigates the use of supervised learning models to diagnose the condition of electric motors, categorizing them as \"Healthy,\" \"Needs Preventive Maintenance (PM),\" or \"Broken.\" Key features of motor operation were employed to train various machine learning algorithms, including Naive Bayes, Support Vector Machines (SVM), Regression models, Random Forest, k-Nearest Neighbors (k-NN), and Gradient Boosting techniques. The performance of these models was evaluated to identify the most effective classifier for predicting motor health. Results showed notable differences in accuracy among the models, with one emerging as the best-performing solution. This study underscores the practicality of using supervised learning for electric motor diagnostics, providing a foundation for efficient maintenance scheduling and minimizing unplanned downtimes in industrial applications.\n        △ Less",
        "authors":"['Amir Hossein Baradaran']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment",
        "abstract":"Pipe routing is a highly complex, time-consuming, and no-deterministic polynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive research efforts in optimizing constant-curvature pipe routing, the growing demand for free-form pipes poses new challenges. Dynamic design environments and fuzzy layout rules further impact the optimization performance and efficiency. To tackle these challenges, this study proposes a self-learning-based method (SLPR) for optimizing free-form pipe routing in aeroengines. The SLPR is based on the proximal policy optimization (PPO) algorithm and integrates a unified rule modeling framework for efficient obstacle detection and fuzzy rule modeling in continuous space. Additionally, a potential energy table is constructed to enable rapid queries of layout tendencies and interference. The agent within SLPR iteratively refines pipe routing and accumulates the design knowledge through interaction with the environment. Once the design environment shifts, the agent can swiftly adapt by fine-tuning network parameters. Comparative tests reveal that SLPR ensures smooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding redundant pipe segments found in constant-curvature pipe routing. Results in both static and dynamic design environments demonstrate that SLPR outperforms three representative baselines in terms of the pipe length reduction, the adherence to layout rules, the path complexity, and the computational efficiency. Furthermore, tests in dynamic environments indicate that SLPR eliminates labor-intensive searches from scratch and even yields superior solutions compared to the retrained model. These results highlight the practical value of SLPR for real-world pipe routing, meeting lightweight, precision, and sustainability requirements of the modern aeroengine design.\n        △ Less",
        "authors":"['Caicheng Wang', 'Zili Wang', 'Shuyou Zhang', 'Yongzhe Xiang', 'Zheyi Li', 'Jianrong Tan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Orchestration for Inference of Large Foundation Models at the Edge",
        "abstract":"Large Foundation Models (LFMs), including multi-modal and generative AI models, promise to unlock new capabilities for next-generation Edge AI applications. However, performing inference with LFMs in resource-constrained and heterogeneous edge environments presents significant challenges for workload orchestration. We propose a novel adaptive orchestration method and system tailored specifically for managing distributed inference workloads across multi-access edge computing (MEC) infrastructures. Our approach enhances traditional workload orchestration by introducing dynamic methods including: (1) adaptive workload distribution that selects optimal, inter-connected edge nodes based on runtime capacity profiling; (2) dynamic redistribution of LFM partitions as operational conditions evolve, and; (3) real-time reconfiguration (e.g., re-splitting) of LFM layers to balance performance and privacy requirements. Our proposed framework introduces an architecture for adaptive split inference, enabling real-time, QoS-aware management of inference workloads. We present a reference architecture, detail operational mechanisms, and demonstrate its application through various use cases in real-world scenarios.\n        △ Less",
        "authors":"['Fernando Koch', 'Aladin Djuhera', 'Alecio Binotto']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices",
        "abstract":"The high memory and computation demand of large language models (LLMs) makes them challenging to be deployed on consumer devices due to limited GPU memory. Offloading can mitigate the memory constraint but often suffers from low GPU utilization, leading to low inference efficiency. In this work, we propose a novel framework, called pipelined offloading (PIPO), for efficient inference on consumer devices. PIPO designs a fine-grained offloading pipeline, complemented with optimized data transfer and computation, to achieve high concurrency and efficient scheduling for inference. Experimental results show that compared with state-of-the-art baseline, PIPO increases GPU utilization from below 40% to over 90% and achieves up to 3.1$\\times$ higher throughput, running on a laptop equipped with a RTX3060 GPU of 6GB memory.\n        △ Less",
        "authors":"['Yangyijian Liu', 'Jun Li', 'Wu-Jun Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Falafels: A tool for Estimating Federated Learning Energy Consumption via Discrete Simulation",
        "abstract":"The growth in computational power and data hungriness of Machine Learning has led to an important shift of research efforts towards the distribution of ML models on multiple machines, leading in even more powerful models. However, there exists many Distributed Artificial Intelligence paradigms and for each of them the platform and algorithm configurations play an important role in terms of training time and energy consumption. Many mathematical models and frameworks can respectively predict and benchmark this energy consumption, nonetheless, the former lacks of realism and extensibility while the latter suffers high run-times and actual power consumption. In this article, we introduce Falafels, an extensible tool that predicts the energy consumption and training time of -but not limited to -Federated Learning systems. It distinguishes itself with its discrete-simulatorbased solution leading to nearly instant run-time and fast development of new algorithms. Furthermore, we show this approach permits the use of an evolutionary algorithm providing the ability to optimize the system configuration for a given machine learning workload.\n        △ Less",
        "authors":"['Andrew Mary Huet de Barochez', 'Stéphan Plassart', 'Sébastien Monnet']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Memory and Bandwidth are All You Need for Fully Sharded Data Parallel",
        "abstract":"Transformer models have revolutionized a wide spectrum of disciplines, especially in language processing. The recent success has proven that model size scalability is crucial for achieving superior performance metrics. However, training large transformer models is challenging even on modern hardware with powerful GPUs and high-speed interconnects. Existing studies primarily focus on optimizing model training distribution strategies to minimize memory footprint and enhance training speed, often overlooking the scalability challenges related to model size and hardware constraints. To address this oversight, we thoroughly investigate computational, memory, and network demands of training large transformers using the Fully Sharded Data Parallel (FSDP) distributed strategy across different hardware clusters. We explore the intricate relationships between model size and hardware setups to identify configurations that ensure maximum model and hardware efficiency, effective sequence length management, and optimal training throughput. A significant finding of our study is the critical interplay of the cluster's connection bandwidth and GPU memory size compared to the computational performance of GPUs. This interplay limits training efficiency, underscoring the role of both hardware characteristics as a possible bottleneck. By integrating theoretical analysis with simulations and empirical tests, we demonstrate how hardware limitations affect training efficacy, identifying key hardware thresholds and the impact of network connectivity. Our findings prompt a reassessment of training strategies guiding users on the way to finding hardware-optimal FSDP configurations, enhancing training efficiency for large-scale transformer models.\n        △ Less",
        "authors":"['Jiangtao Wang', 'Jan Ebert', 'Oleg Filatov', 'Stefan Kesselheim']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Modern Approach to Real-Time Air Traffic Management System",
        "abstract":"Air traffic analytics systems are pivotal for ensuring safety, efficiency, and predictability in air travel. However, traditional systems struggle to handle the increasing volume and complexity of air traffic data. This project explores the application of real-time big data processing frameworks like Apache Spark, HDFS, and Spark Streaming for developing a new robust system. By reviewing existing research on real-time systems and analyzing the challenges and opportunities presented by big data technologies, we propose an architecture for a real-time system. Our project pipeline involves real-time data collection from flight information sources through flight API's, ingestion into Kafka, and transmission to Elasticsearch for visualization using Kibana. Additionally, we present a dashboard of U.S. airlines on PowerBI, demonstrating the potential of real-time analytics in revolutionizing air traffic management.\n        △ Less",
        "authors":"['Priyank Vaidya', 'Vedansh Kamdar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Echo: Efficient Co-Scheduling of Hybrid Online-Offline Tasks for Large Language Model Serving",
        "abstract":"Large language models have been widely deployed in various applications, encompassing both interactive online tasks and batched offline tasks. Given the burstiness and latency sensitivity of online tasks, over-provisioning resources is common practice. This allows for the integration of latency-insensitive offline tasks during periods of low online load, enhancing resource utilization. However, strategically serving online and offline tasks through a preemption mechanism fails to fully leverage the flexibility of offline tasks and suffers from KV cache recomputation and irregular workloads.\n  In this paper, we introduce Echo, a collaborative online-offline task serving system, including a scheduler, a KV cache manager, and estimation toolkits. The scheduler and KV cache manager work tightly to maximize the throughput of offline tasks, while the estimator further predicts execution time to ensure online task SLOs. The scheduler leverages the batch information of last iteration to reduce the search space for finding the optimal schedule. The KV cache manager sets the priority of the KV cache based on the type of tasks and the opportunity of prefix sharing to reduce the recomputation. Finally, the estimation toolkits predict the execution time, future memory consumption, and the throughput of offline tasks to guide the scheduler, KV cache manager, and the system deployer. Evaluation based on real-world workloads demonstrates that Echo can increase offline task throughput by up to $3.3\\times$, while satisfying online task SLOs.\n        △ Less",
        "authors":"['Zhibin Wang', 'Shipeng Li', 'Xue Li', 'Yuhang Zhou', 'Zhonghui Zhang', 'Zibo Wang', 'Rong Gu', 'Chen Tian', 'Kun Yang', 'Sheng Zhong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BoxRL-NNV: Boxed Refinement of Latin Hypercube Samples for Neural Network Verification",
        "abstract":"BoxRL-NNV is a Python tool for the detection of safety violations in neural networks by computing the bounds of the output variables, given the bounds of the input variables of the network. This is done using global extrema estimation via Latin Hypercube Sampling, and further refinement using L-BFGS-B for local optimization around the initial guess. This paper presents an overview of BoxRL-NNV, as well as our results for a subset of the ACAS Xu benchmark. A complete evaluation of the tool's performance, including benchmark comparisons with state-of-the-art tools, shall be presented at the Sixth International Verification of Neural Networks Competition (VNN-COMP'25).\n        △ Less",
        "authors":"['Sarthak Das']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diagnostic Method for Hydropower Plant Condition-based Maintenance combining Autoencoder with Clustering Algorithms",
        "abstract":"The French company EDF uses supervisory control and data acquisition systems in conjunction with a data management platform to monitor hydropower plant, allowing engineers and technicians to analyse the time-series collected. Depending on the strategic importance of the monitored hydropower plant, the number of time-series collected can vary greatly making it difficult to generate valuable information from the extracted data. In an attempt to provide an answer to this particular problem, a condition detection and diagnosis method combining clustering algorithms and autoencoder neural networks for pattern recognition has been developed and is presented in this paper. First, a dimension reduction algorithm is used to create a 2-or 3-dimensional projection that allows the users to identify unsuspected relationships between datapoints. Then, a collection of clustering algorithms regroups the datapoints into clusters. For each identified cluster, an autoencoder neural network is trained on the corresponding dataset. The aim is to measure the reconstruction error between each autoencoder model and the measured values, thus creating a proximity index for each state discovered during the clustering stage.\n        △ Less",
        "authors":"['Samy Jad', 'Xavier Desforges', 'Pierre-Yves Villard', 'Christian Caussidéry', 'Kamal Medjaher']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Quantum Speedups for Markov Chain Monte Carlo Methods with Application to Optimization",
        "abstract":"We propose quantum algorithms that provide provable speedups for Markov Chain Monte Carlo (MCMC) methods commonly used for sampling from probability distributions of the form $π\\propto e^{-f}$, where $f$ is a potential function. Our first approach considers Gibbs sampling for finite-sum potentials in the stochastic setting, employing an oracle that provides gradients of individual functions. In the second setting, we consider access only to a stochastic evaluation oracle, allowing simultaneous queries at two points of the potential function under the same stochastic parameter. By introducing novel techniques for stochastic gradient estimation, our algorithms improve the gradient and evaluation complexities of classical samplers, such as Hamiltonian Monte Carlo (HMC) and Langevin Monte Carlo (LMC) in terms of dimension, precision, and other problem-dependent parameters. Furthermore, we achieve quantum speedups in optimization, particularly for minimizing non-smooth and approximately convex functions that commonly appear in empirical risk minimization problems.\n        △ Less",
        "authors":"['Guneykan Ozgul', 'Xiantao Li', 'Mehrdad Mahdavi', 'Chunhao Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reciprocity-Aware Convolutional Neural Networks for Map-Based Path Loss Prediction",
        "abstract":"Path loss modeling is a widely used technique for estimating point-to-point losses along a communications link from transmitter (Tx) to receiver (Rx). Accurate path loss predictions can optimize use of the radio frequency spectrum and minimize unwanted interference. Modern path loss modeling often leverages data-driven approaches, using machine learning to train models on drive test measurement datasets. Drive tests primarily represent downlink scenarios, where the Tx is located on a building and the Rx is located on a moving vehicle. Consequently, trained models are frequently reserved for downlink coverage estimation, lacking representation of uplink scenarios. In this paper, we demonstrate that data augmentation can be used to train a path loss model that is generalized to uplink, downlink, and backhaul scenarios, training using only downlink drive test measurements. By adding a small number of synthetic samples representing uplink scenarios to the training set, root mean squared error is reduced by >8 dB on uplink examples in the test set.\n        △ Less",
        "authors":"['Ryan G. Dempsey', 'Jonathan Ethier', 'Halim Yanikomeroglu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
        "abstract":"As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B\/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B\/72B and Llama-3.1-8B\/70B), while being up to 3$\\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.\n        △ Less",
        "authors":"['NVIDIA', ':', 'Aaron Blakeman', 'Aarti Basant', 'Abhinav Khattar', 'Adithya Renduchintala', 'Akhiad Bercovich', 'Aleksander Ficek', 'Alexis Bjorlin', 'Ali Taghibakhshi', 'Amala Sanjay Deshmukh', 'Ameya Sunil Mahabaleshwarkar', 'Andrew Tao', 'Anna Shors', 'Ashwath Aithal', 'Ashwin Poojary', 'Ayush Dattagupta', 'Balaram Buddharaju', 'Bobby Chen', 'Boris Ginsburg', 'Boxin Wang', 'Brandon Norick', 'Brian Butterfield', 'Bryan Catanzaro', 'Carlo del Mundo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Align to Structure: Aligning Large Language Models with Structural Information",
        "abstract":"Generating long, coherent text remains a challenge for large language models (LLMs), as they lack hierarchical planning and structured organization in discourse generation. We introduce Structural Alignment, a novel method that aligns LLMs with human-like discourse structures to enhance long-form text generation. By integrating linguistically grounded discourse frameworks into reinforcement learning, our approach guides models to produce coherent and well-organized outputs. We employ a dense reward scheme within a Proximal Policy Optimization framework, assigning fine-grained, token-level rewards based on the discourse distinctiveness relative to human writing. Two complementary reward models are evaluated: the first improves readability by scoring surface-level textual features to provide explicit structuring, while the second reinforces deeper coherence and rhetorical sophistication by analyzing global discourse patterns through hierarchical discourse motifs, outperforming both standard and RLHF-enhanced models in tasks such as essay generation and long-document summarization. All training data and code will be publicly shared at https:\/\/github.com\/minnesotanlp\/struct_align.\n        △ Less",
        "authors":"['Zae Myung Kim', 'Anand Ramachandran', 'Farideh Tavazoee', 'Joo-Kyung Kim', 'Oleg Rokhlenko', 'Dongyeop Kang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Trading off Relevance and Revenue in the Jobs Marketplace: Estimation, Optimization and Auction Design",
        "abstract":"We study the problem of position allocation in job marketplaces, where the platform determines the ranking of the jobs for each seeker. The design of ranking mechanisms is critical to marketplace efficiency, as it influences both short-term revenue from promoted job placements and long-term health through sustained seeker engagement. Our analysis focuses on the tradeoff between revenue and relevance, as well as the innovations in job auction design. We demonstrated two ways to improve relevance with minimal impact on revenue: incorporating the seekers preferences and applying position-aware auctions.\n        △ Less",
        "authors":"['Farzad Pourbabaee', 'Sophie Yanying Sheng', 'Peter McCrory', 'Luke Simon', 'Di Mo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards deployment-centric multimodal AI beyond vision and language",
        "abstract":"Multimodal artificial intelligence (AI) integrates diverse types of data via machine learning to improve understanding, prediction, and decision-making across disciplines such as healthcare, science, and engineering. However, most multimodal AI advances focus on models for vision and language data, while their deployability remains a key challenge. We advocate a deployment-centric workflow that incorporates deployment constraints early to reduce the likelihood of undeployable solutions, complementing data-centric and model-centric approaches. We also emphasise deeper integration across multiple levels of multimodality and multidisciplinary collaboration to significantly broaden the research scope beyond vision and language. To facilitate this approach, we identify common multimodal-AI-specific challenges shared across disciplines and examine three real-world use cases: pandemic response, self-driving car design, and climate change adaptation, drawing expertise from healthcare, social science, engineering, science, sustainability, and finance. By fostering multidisciplinary dialogue and open research practices, our community can accelerate deployment-centric development for broad societal impact.\n        △ Less",
        "authors":"['Xianyuan Liu', 'Jiayang Zhang', 'Shuo Zhou', 'Thijs L. van der Plas', 'Avish Vijayaraghavan', 'Anastasiia Grishina', 'Mengdie Zhuang', 'Daniel Schofield', 'Christopher Tomlinson', 'Yuhan Wang', 'Ruizhe Li', 'Louisa van Zeeland', 'Sina Tabakhi', 'Cyndie Demeocq', 'Xiang Li', 'Arunav Das', 'Orlando Timmerman', 'Thomas Baldwin-McDonald', 'Jinge Wu', 'Peizhen Bai', 'Zahraa Al Sahili', 'Omnia Alwazzan', 'Thao N. Do', 'Mohammod N. I. Suvon', 'Angeline Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
        "abstract":"Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $τ$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https:\/\/huggingface.co\/collections\/Salesforce\/xlam-2-67ef5be12949d8dcdae354c4 and project website is https:\/\/apigen-mt.github.io\n        △ Less",
        "authors":"['Akshara Prabhakar', 'Zuxin Liu', 'Ming Zhu', 'Jianguo Zhang', 'Tulika Awalgaonkar', 'Shiyu Wang', 'Zhiwei Liu', 'Haolin Chen', 'Thai Hoang', 'Juan Carlos Niebles', 'Shelby Heinecke', 'Weiran Yao', 'Huan Wang', 'Silvio Savarese', 'Caiming Xiong']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimistic Online Learning in Symmetric Cone Games",
        "abstract":"Optimistic online learning algorithms have led to significant advances in equilibrium computation, particularly for two-player zero-sum games, achieving an iteration complexity of $\\mathcal{O}(1\/ε)$ to reach an $ε$-saddle point. These advances have been established in normal-form games, where strategies are simplex vectors, and quantum games, where strategies are trace-one positive semidefinite matrices. We extend optimistic learning to symmetric cone games (SCGs), a class of two-player zero-sum games where strategy spaces are generalized simplices (trace-one slices of symmetric cones). A symmetric cone is the cone of squares of a Euclidean Jordan Algebra; canonical examples include the nonnegative orthant, the second-order cone, the cone of positive semidefinite matrices, and their products, all fundamental to convex optimization. SCGs unify normal-form and quantum games and, as we show, offer significantly greater modeling flexibility, allowing us to model applications such as distance metric learning problems and the Fermat-Weber problem. To compute approximate saddle points in SCGs, we introduce the Optimistic Symmetric Cone Multiplicative Weights Update algorithm and establish an iteration complexity of $\\mathcal{O}(1\/ε)$ to reach an $ε$-saddle point. Our analysis builds on the Optimistic Follow-the-Regularized-Leader framework, with a key technical contribution being a new proof of the strong convexity of the symmetric cone negative entropy with respect to the trace-one norm, a result that may be of independent interest.\n        △ Less",
        "authors":"['Anas Barakat', 'Wayne Lin', 'John Lazarsfeld', 'Antonios Varvitsiotis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scalable Hypergraph Structure Learning with Diverse Smoothness Priors",
        "abstract":"In graph signal processing, learning the weighted connections between nodes from a set of sample signals is a fundamental task when the underlying relationships are not known a priori. This task is typically addressed by finding a graph Laplacian on which the observed signals are smooth. With the extension of graphs to hypergraphs - where edges can connect more than two nodes - graph learning methods have similarly been generalized to hypergraphs. However, the absence of a unified framework for calculating total variation has led to divergent definitions of smoothness and, consequently, differing approaches to hyperedge recovery. We confront this challenge through generalization of several previously proposed hypergraph total variations, subsequently allowing ease of substitution into a vector based optimization. To this end, we propose a novel hypergraph learning method that recovers a hypergraph topology from time-series signals based on a smoothness prior. Our approach addresses key limitations in prior works, such as hyperedge selection and convergence issues, by formulating the problem as a convex optimization solved via a forward-backward-forward algorithm, ensuring guaranteed convergence. Additionally, we introduce a process that simultaneously limits the span of the hyperedge search and maintains a valid hyperedge selection set. In doing so, our method becomes scalable in increasingly complex network structures. The experimental results demonstrate improved performance, in terms of accuracy, over other state-of-the-art hypergraph inference methods; furthermore, we empirically show our method to be robust to total variation terms, biased towards global smoothness, and scalable to larger hypergraphs.\n        △ Less",
        "authors":"['Benjamin T. Brown', 'Haoxiang Zhang', 'Daniel L. Lau', 'Gonzalo R. Arce']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hallucination Detection on a Budget: Efficient Bayesian Estimation of Semantic Entropy",
        "abstract":"Detecting whether an LLM hallucinates is an important research challenge. One promising way of doing so is to estimate the semantic entropy (Farquhar et al., 2024) of the distribution of generated sequences. We propose a new algorithm for doing that, with two main advantages. First, due to us taking the Bayesian approach, we achieve a much better quality of semantic entropy estimates for a given budget of samples from the LLM. Second, we are able to tune the number of samples adaptively so that `harder' contexts receive more samples. We demonstrate empirically that our approach systematically beats the baselines, requiring only 59% of samples used by Farquhar et al. (2024) to achieve the same quality of hallucination detection as measured by AUROC. Moreover, quite counterintuitively, our estimator is useful even with just one sample from the LLM.\n        △ Less",
        "authors":"['Kamil Ciosek', 'Nicolò Felicioni', 'Sina Ghiassian']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement",
        "abstract":"In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https:\/\/github.com\/zjunlp\/SynWorld.\n        △ Less",
        "authors":"['Runnan Fang', 'Xiaobin Wang', 'Yuan Liang', 'Shuofei Qiao', 'Jialong Wu', 'Zekun Xi', 'Ningyu Zhang', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Stochastic Optimization with Optimal Importance Sampling",
        "abstract":"Importance Sampling (IS) is a widely used variance reduction technique for enhancing the efficiency of Monte Carlo methods, particularly in rare-event simulation and related applications. Despite its power, the performance of IS is often highly sensitive to the choice of the proposal distribution and frequently requires stochastic calibration techniques. While the design and analysis of IS have been extensively studied in estimation settings, applying IS within stochastic optimization introduces a unique challenge: the decision and the IS distribution are mutually dependent, creating a circular optimization structure. This interdependence complicates both the analysis of convergence for decision iterates and the efficiency of the IS scheme. In this paper, we propose an iterative gradient-based algorithm that jointly updates the decision variable and the IS distribution without requiring time-scale separation between the two. Our method achieves the lowest possible asymptotic variance and guarantees global convergence under convexity of the objective and mild assumptions on the IS distribution family. Furthermore, we show that these properties are preserved under linear constraints by incorporating a recent variant of Nesterov's dual averaging method.\n        △ Less",
        "authors":"['Liviu Aolaritei', 'Bart P. G. Van Parys', 'Henry Lam', 'Michael I. Jordan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Agentic Knowledgeable Self-awareness",
        "abstract":"Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a \"flood irrigation\" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https:\/\/github.com\/zjunlp\/KnowSelf.\n        △ Less",
        "authors":"['Shuofei Qiao', 'Zhisong Qiu', 'Baochang Ren', 'Xiaobin Wang', 'Xiangyuan Ru', 'Ningyu Zhang', 'Xiang Chen', 'Yong Jiang', 'Pengjun Xie', 'Fei Huang', 'Huajun Chen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MultiMed-ST: Large-scale Many-to-many Multilingual Medical Speech Translation",
        "abstract":"Multilingual speech translation (ST) in the medical domain enhances patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we present the first systematic study on medical ST, to our best knowledge, by releasing MultiMed-ST, a large-scale ST dataset for the medical domain, spanning all translation directions in five languages: Vietnamese, English, German, French, Traditional Chinese and Simplified Chinese, together with the models. With 290,000 samples, our dataset is the largest medical machine translation (MT) dataset and the largest many-to-many multilingual ST among all domains. Secondly, we present the most extensive analysis study in ST research to date, including: empirical baselines, bilingual-multilingual comparative study, end-to-end vs. cascaded comparative study, task-specific vs. multi-task sequence-to-sequence (seq2seq) comparative study, code-switch analysis, and quantitative-qualitative error analysis. All code, data, and models are available online: https:\/\/github.com\/leduckhai\/MultiMed-ST.\n        △ Less",
        "authors":"['Khai Le-Duc', 'Tuyen Tran', 'Bach Phan Tat', 'Nguyen Kim Hai Bui', 'Quan Dang', 'Hung-Phong Tran', 'Thanh-Thuy Nguyen', 'Ly Nguyen', 'Tuan-Minh Phan', 'Thi Thu Phuong Tran', 'Chris Ngo', 'Nguyen X. Khanh', 'Thanh Nguyen-Tang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dense Neural Network Based Arrhythmia Classification on Low-cost and Low-compute Micro-controller",
        "abstract":"The electrocardiogram (ECG) monitoring device is an expensive albeit essential device for the treatment and diagnosis of cardiovascular diseases (CVD). The cost of this device typically ranges from $2000 to $10000. Several studies have implemented ECG monitoring systems in micro-controller units (MCU) to reduce industrial development costs by up to 20 times. However, to match industry-grade systems and display heartbeats effectively, it is essential to develop an efficient algorithm for detecting arrhythmia (irregular heartbeat). Hence in this study, a dense neural network is developed to detect arrhythmia on the Arduino Nano. The Nano consists of the ATMega328 microcontroller with a 16MHz clock, 2KB of SRAM, and 32KB of program memory. Additionally, the AD8232 SparkFun Single-Lead Heart Rate Monitor is used as the ECG sensor. The implemented neural network model consists of two layers (excluding the input) with 10 and four neurons respectively with sigmoid activation function. However, four approaches are explored to choose the appropriate activation functions. The model has a size of 1.267 KB, achieves an F1 score (macro-average) of 78.3\\% for classifying four types of arrhythmia, an accuracy rate of 96.38%, and requires 0.001314 MOps of floating-point operations (FLOPs).\n        △ Less",
        "authors":"['Md Abu Obaida Zishan', 'H M Shihab', 'Sabik Sadman Islam', 'Maliha Alam Riya', 'Gazi Mashrur Rahman', 'Jannatun Noor']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dexterous Manipulation through Imitation Learning: A Survey",
        "abstract":"Dexterous manipulation, which refers to the ability of a robotic hand or multi-fingered end-effector to skillfully control, reorient, and manipulate objects through precise, coordinated finger movements and adaptive force modulation, enables complex interactions similar to human hand dexterity. With recent advances in robotics and machine learning, there is a growing demand for these systems to operate in complex and unstructured environments. Traditional model-based approaches struggle to generalize across tasks and object variations due to the high-dimensionality and complex contact dynamics of dexterous manipulation. Although model-free methods such as reinforcement learning (RL) show promise, they require extensive training, large-scale interaction data, and carefully designed rewards for stability and effectiveness. Imitation learning (IL) offers an alternative by allowing robots to acquire dexterous manipulation skills directly from expert demonstrations, capturing fine-grained coordination and contact dynamics while bypassing the need for explicit modeling and large-scale trial-and-error. This survey provides an overview of dexterous manipulation methods based on imitation learning (IL), details recent advances, and addresses key challenges in the field. Additionally, it explores potential research directions to enhance IL-driven dexterous manipulation. Our goal is to offer researchers and practitioners a comprehensive introduction to this rapidly evolving domain.\n        △ Less",
        "authors":"['Shan An', 'Ziyu Meng', 'Chao Tang', 'Yuning Zhou', 'Tengyu Liu', 'Fangqiang Ding', 'Shufang Zhang', 'Yao Mu', 'Ran Song', 'Wei Zhang', 'Zeng-Guang Hou', 'Hong Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hierarchical Knowledge Structuring for Effective Federated Learning in Heterogeneous Environments",
        "abstract":"Federated learning enables collaborative model training across distributed entities while maintaining individual data privacy. A key challenge in federated learning is balancing the personalization of models for local clients with generalization for the global model. Recent efforts leverage logit-based knowledge aggregation and distillation to overcome these issues. However, due to the non-IID nature of data across diverse clients and the imbalance in the client's data distribution, directly aggregating the logits often produces biased knowledge that fails to apply to individual clients and obstructs the convergence of local training. To solve this issue, we propose a Hierarchical Knowledge Structuring (HKS) framework that formulates sample logits into a multi-granularity codebook to represent logits from personalized per-sample insights to globalized per-class knowledge. The unsupervised bottom-up clustering method is leveraged to enable the global server to provide multi-granularity responses to local clients. These responses allow local training to integrate supervised learning objectives with global generalization constraints, which results in more robust representations and improved knowledge sharing in subsequent training rounds. The proposed framework's effectiveness is validated across various benchmarks and model architectures.\n        △ Less",
        "authors":"['Wai Fong Tam', 'Qilei Li', 'Ahmed M. Abdelmonie']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Operator Learning: A Statistical Perspective",
        "abstract":"Operator learning has emerged as a powerful tool in scientific computing for approximating mappings between infinite-dimensional function spaces. A primary application of operator learning is the development of surrogate models for the solution operators of partial differential equations (PDEs). These methods can also be used to develop black-box simulators to model system behavior from experimental data, even without a known mathematical model. In this article, we begin by formalizing operator learning as a function-to-function regression problem and review some recent developments in the field. We also discuss PDE-specific operator learning, outlining strategies for incorporating physical and mathematical constraints into architecture design and training processes. Finally, we end by highlighting key future directions such as active data collection and the development of rigorous uncertainty quantification frameworks.\n        △ Less",
        "authors":"['Unique Subedi', 'Ambuj Tewari']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning Dual-Arm Coordination for Grasping Large Flat Objects",
        "abstract":"Grasping large flat objects, such as books or keyboards lying horizontally, presents significant challenges for single-arm robotic systems, often requiring extra actions like pushing objects against walls or moving them to the edge of a surface to facilitate grasping. In contrast, dual-arm manipulation, inspired by human dexterity, offers a more refined solution by directly coordinating both arms to lift and grasp the object without the need for complex repositioning. In this paper, we propose a model-free deep reinforcement learning (DRL) framework to enable dual-arm coordination for grasping large flat objects. We utilize a large-scale grasp pose detection model as a backbone to extract high-dimensional features from input images, which are then used as the state representation in a reinforcement learning (RL) model. A CNN-based Proximal Policy Optimization (PPO) algorithm with shared Actor-Critic layers is employed to learn coordinated dual-arm grasp actions. The system is trained and tested in Isaac Gym and deployed to real robots. Experimental results demonstrate that our policy can effectively grasp large flat objects without requiring additional maneuvers. Furthermore, the policy exhibits strong generalization capabilities, successfully handling unseen objects. Importantly, it can be directly transferred to real robots without fine-tuning, consistently outperforming baseline methods.\n        △ Less",
        "authors":"['Yongliang Wang', 'Hamidreza Kasaei']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimistic Learning for Communication Networks",
        "abstract":"AI\/ML-based tools are at the forefront of resource management solutions for communication networks. Deep learning, in particular, is highly effective in facilitating fast and high-performing decision-making whenever representative training data is available to build offline accurate models. Conversely, online learning solutions do not require training and enable adaptive decisions based on runtime observations, alas are often overly conservative. This extensive tutorial proposes the use of optimistic learning (OpL) as a decision engine for resource management frameworks in modern communication systems. When properly designed, such solutions can achieve fast and high-performing decisions -- comparable to offline-trained models -- while preserving the robustness and performance guarantees of the respective online learning approaches. We introduce the fundamental concepts, algorithms and results of OpL, discuss the roots of this theory and present different approaches to defining and achieving optimism. We proceed to showcase how OpL can enhance resource management in communication networks for several key problems such as caching, edge computing, network slicing, and workload assignment in decentralized O-RAN platforms. Finally, we discuss the open challenges that must be addressed to unlock the full potential of this new resource management approach.\n        △ Less",
        "authors":"['George Iosifidis', 'Naram Mhaisen', 'Douglas J. Leith']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hybrid Real- and Complex-valued Neural Network Architecture",
        "abstract":"We propose a \\emph{hybrid} real- and complex-valued \\emph{neural network} (HNN) architecture, designed to combine the computational efficiency of real-valued processing with the ability to effectively handle complex-valued data. We illustrate the limitations of using real-valued neural networks (RVNNs) for inherently complex-valued problems by showing how it learnt to perform complex-valued convolution, but with notable inefficiencies stemming from its real-valued constraints. To create the HNN, we propose to use building blocks containing both real- and complex-valued paths, where information between domains is exchanged through domain conversion functions. We also introduce novel complex-valued activation functions, with higher generalisation and parameterisation efficiency. HNN-specific architecture search techniques are described to navigate the larger solution space. Experiments with the AudioMNIST dataset demonstrate that the HNN reduces cross-entropy loss and consumes less parameters compared to an RVNN for all considered cases. Such results highlight the potential for the use of partially complex-valued processing in neural networks and applications for HNNs in many signal processing domains.\n        △ Less",
        "authors":"['Alex Young', 'Luan Vinícius Fiorio', 'Bo Yang', 'Boris Karanov', 'Wim van Houtum', 'Ronald M. Aarts']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Quantifying Robustness: A Benchmarking Framework for Deep Learning Forecasting in Cyber-Physical Systems",
        "abstract":"Cyber-Physical Systems (CPS) in domains such as manufacturing and energy distribution generate complex time series data crucial for Prognostics and Health Management (PHM). While Deep Learning (DL) methods have demonstrated strong forecasting capabilities, their adoption in industrial CPS remains limited due insufficient robustness. Existing robustness evaluations primarily focus on formal verification or adversarial perturbations, inadequately representing the complexities encountered in real-world CPS scenarios. To address this, we introduce a practical robustness definition grounded in distributional robustness, explicitly tailored to industrial CPS, and propose a systematic framework for robustness evaluation. Our framework simulates realistic disturbances, such as sensor drift, noise and irregular sampling, enabling thorough robustness analyses of forecasting models on real-world CPS datasets. The robustness definition provides a standardized score to quantify and compare model performance across diverse datasets, assisting in informed model selection and architecture design. Through extensive empirical studies evaluating prominent DL architectures (including recurrent, convolutional, attention-based, modular, and structured state-space models) we demonstrate the applicability and effectiveness of our approach. We publicly release our robustness benchmark to encourage further research and reproducibility.\n        △ Less",
        "authors":"['Alexander Windmann', 'Henrik Steude', 'Daniel Boschmann', 'Oliver Niggemann']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diffusion Active Learning: Towards Data-Driven Experimental Design in Computed Tomography",
        "abstract":"We introduce Diffusion Active Learning, a novel approach that combines generative diffusion modeling with data-driven sequential experimental design to adaptively acquire data for inverse problems. Although broadly applicable, we focus on scientific computed tomography (CT) for experimental validation, where structured prior datasets are available, and reducing data requirements directly translates to shorter measurement times and lower X-ray doses. We first pre-train an unconditional diffusion model on domain-specific CT reconstructions. The diffusion model acts as a learned prior that is data-dependent and captures the structure of the underlying data distribution, which is then used in two ways: It drives the active learning process and also improves the quality of the reconstructions. During the active learning loop, we employ a variant of diffusion posterior sampling to generate conditional data samples from the posterior distribution, ensuring consistency with the current measurements. Using these samples, we quantify the uncertainty in the current estimate to select the most informative next measurement. Our results show substantial reductions in data acquisition requirements, corresponding to lower X-ray doses, while simultaneously improving image reconstruction quality across multiple real-world tomography datasets.\n        △ Less",
        "authors":"['Luis Barba', 'Johannes Kirschner', 'Tomas Aidukas', 'Manuel Guizar-Sicairos', 'Benjamín Béjar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Structured Legal Document Generation in India: A Model-Agnostic Wrapper Approach with VidhikDastaavej",
        "abstract":"Automating legal document drafting can significantly enhance efficiency, reduce manual effort, and streamline legal workflows. While prior research has explored tasks such as judgment prediction and case summarization, the structured generation of private legal documents in the Indian legal domain remains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej, a novel, anonymized dataset of private legal documents, and develop NyayaShilp, a fine-tuned legal document generation model specifically adapted to Indian legal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework that first generates structured section titles and then iteratively produces content while leveraging retrieval-based mechanisms to ensure coherence and factual accuracy. We benchmark multiple open-source LLMs, including instruction-tuned and domain-adapted versions, alongside proprietary models for comparison. Our findings indicate that while direct fine-tuning on small datasets does not always yield improvements, our structured wrapper significantly enhances coherence, factual adherence, and overall document quality while mitigating hallucinations. To ensure real-world applicability, we developed a Human-in-the-Loop (HITL) Document Generation System, an interactive user interface that enables users to specify document types, refine section details, and generate structured legal drafts. This tool allows legal professionals and researchers to generate, validate, and refine AI-generated legal documents efficiently. Extensive evaluations, including expert assessments, confirm that our framework achieves high reliability in structured legal drafting. This research establishes a scalable and adaptable foundation for AI-assisted legal drafting in India, offering an effective approach to structured legal document generation.\n        △ Less",
        "authors":"['Shubham Kumar Nigam', 'Balaramamahanthi Deepak Patnaik', 'Ajay Varghese Thomas', 'Noel Shallum', 'Kripabandhu Ghosh', 'Arnab Bhattacharya']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Gaussian Process Tilted Nonparametric Density Estimation using Fisher Divergence Score Matching",
        "abstract":"We present three Fisher divergence (FD) minimization algorithms for learning Gaussian process (GP) based score models for lower dimensional density estimation problems. The density is formed by multiplying a base multivariate normal distribution with an exponentiated GP refinement, and so we refer to it as a GP-tilted nonparametric density. By representing the GP part of the score as a linear function using the random Fourier feature (RFF) approximation, we show that all learning problems can be solved in closed form. This includes the basic and noise conditional versions of the Fisher divergence, as well as a novel alternative to noise conditional FD models based on variational inference (VI). Here, we propose using an ELBO-like optimization of the approximate posterior with which we derive a Fisher variational predictive distribution. The RFF representation of the GP, which is functionally equivalent to a single layer neural network score model with cosine activation, provides a unique linear form for which all expectations are in closed form. The Gaussian base also helps with tractability of the VI approximation. We demonstrate our three learning algorithms, as well as a MAP baseline algorithm, on several low dimensional density estimation problems. The closed-form nature of the learning problem removes the reliance on iterative algorithms, making this technique particularly well-suited to large data sets.\n        △ Less",
        "authors":"['John Paisley', 'Wei Zhang', 'Brian Barr']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Discovering Partially Known Ordinary Differential Equations: a Case Study on the Chemical Kinetics of Cellulose Degradation",
        "abstract":"The degree of polymerization (DP) is one of the methods for estimating the aging of the polymer based insulation systems, such as cellulose insulation in power components. The main degradation mechanisms in polymers are hydrolysis, pyrolysis, and oxidation. These mechanisms combined cause a reduction of the DP. However, the data availability for these types of problems is usually scarce. This study analyzes insulation aging using cellulose degradation data from power transformers. The aging problem for the cellulose immersed in mineral oil inside power transformers is modeled with ordinary differential equations (ODEs). We recover the governing equations of the degradation system using Physics-Informed Neural Networks (PINNs) and symbolic regression. We apply PINNs to discover the Arrhenius equation's unknown parameters in the Ekenstam ODE describing cellulose contamination content and the material aging process related to temperature for synthetic data and real DP values. A modification of the Ekenstam ODE is given by Emsley's system of ODEs, where the rate constant expressed by the Arrhenius equation decreases in time with the new formulation. We use PINNs and symbolic regression to recover the functional form of one of the ODEs of the system and to identify an unknown parameter.\n        △ Less",
        "authors":"['Federica Bragone', 'Kateryna Morozovska', 'Tor Laneryd', 'Khemraj Shukla', 'Stefano Markidis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Online Traffic Density Estimation using Physics-Informed Neural Networks",
        "abstract":"Recent works on the application of Physics-Informed Neural Networks to traffic density estimation have shown to be promising for future developments due to their robustness to model errors and noisy data. In this paper, we introduce a methodology for online approximation of the traffic density using measurements from probe vehicles in two settings: one using the Greenshield model and the other considering a high-fidelity traffic simulation. The proposed method continuously estimates the real-time traffic density in space and performs model identification with each new set of measurements. The density estimate is updated in almost real-time using gradient descent and adaptive weights. In the case of full model knowledge, the resulting algorithm has similar performance to the classical open-loop one. However, in the case of model mismatch, the iterative solution behaves as a closed-loop observer and outperforms the baseline method. Similarly, in the high-fidelity setting, the proposed algorithm correctly reproduces the traffic characteristics.\n        △ Less",
        "authors":"['Dennis Wilkman', 'Kateryna Morozovska', 'Karl Henrik Johansson', 'Matthieu Barreau']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Probabilistic Machine Learning for Noisy Labels in Earth Observation",
        "abstract":"Label noise poses a significant challenge in Earth Observation (EO), often degrading the performance and reliability of supervised Machine Learning (ML) models. Yet, given the critical nature of several EO applications, developing robust and trustworthy ML solutions is essential. In this study, we take a step in this direction by leveraging probabilistic ML to model input-dependent label noise and quantify data uncertainty in EO tasks, accounting for the unique noise sources inherent in the domain. We train uncertainty-aware probabilistic models across a broad range of high-impact EO applications-spanning diverse noise sources, input modalities, and ML configurations-and introduce a dedicated pipeline to assess their accuracy and reliability. Our experimental results show that the uncertainty-aware models consistently outperform the standard deterministic approaches across most datasets and evaluation metrics. Moreover, through rigorous uncertainty evaluation, we validate the reliability of the predicted uncertainty estimates, enhancing the interpretability of model predictions. Our findings emphasize the importance of modeling label noise and incorporating uncertainty quantification in EO, paving the way for more accurate, reliable, and trustworthy ML solutions in the field.\n        △ Less",
        "authors":"['Spyros Kondylatos', 'Nikolaos Ioannis Bountos', 'Ioannis Prapas', 'Angelos Zavras', 'Gustau Camps-Valls', 'Ioannis Papoutsis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generating ensembles of spatially-coherent in-situ forecasts using flow matching",
        "abstract":"We propose a machine-learning-based methodology for in-situ weather forecast postprocessing that is both spatially coherent and multivariate. Compared to previous work, our Flow MAtching Postprocessing (FMAP) better represents the correlation structures of the observations distribution, while also improving marginal performance at the stations. FMAP generates forecasts that are not bound to what is already modeled by the underlying gridded prediction and can infer new correlation structures from data. The resulting model can generate an arbitrary number of forecasts from a limited number of numerical simulations, allowing for low-cost forecasting systems. A single training is sufficient to perform postprocessing at multiple lead times, in contrast with other methods which use multiple trained networks at generation time. This work details our methodology, including a spatial attention transformer backbone trained within a flow matching generative modeling framework. FMAP shows promising performance in experiments on the EUPPBench dataset, forecasting surface temperature and wind gust values at station locations in western Europe up to five-day lead times.\n        △ Less",
        "authors":"['David Landry', 'Claire Monteleoni', 'Anastase Charantonis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conditioning Diffusions Using Malliavin Calculus",
        "abstract":"In stochastic optimal control and conditional generative modelling, a central computational task is to modify a reference diffusion process to maximise a given terminal-time reward. Most existing methods require this reward to be differentiable, using gradients to steer the diffusion towards favourable outcomes. However, in many practical settings, like diffusion bridges, the reward is singular, taking an infinite value if the target is hit and zero otherwise. We introduce a novel framework, based on Malliavin calculus and path-space integration by parts, that enables the development of methods robust to such singular rewards. This allows our approach to handle a broad range of applications, including classification, diffusion bridges, and conditioning without the need for artificial observational noise. We demonstrate that our approach offers stable and reliable training, outperforming existing techniques.\n        △ Less",
        "authors":"['Jakiw Pidstrigach', 'Elizabeth Baker', 'Carles Domingo-Enrich', 'George Deligiannidis', 'Nikolas Nüsken']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SpectR: Dynamically Composing LM Experts with Spectral Routing",
        "abstract":"Training large, general-purpose language models poses significant challenges. The growing availability of specialized expert models, fine-tuned from pretrained models for specific tasks or domains, offers a promising alternative. Leveraging the potential of these existing expert models in real-world applications requires effective methods to select or merge the models best suited for a given task. This paper introduces SPECTR, an approach for dynamically composing expert models at each time step during inference. Notably, our method requires no additional training and enables flexible, token- and layer-wise model combinations. Our experimental results demonstrate that SPECTR improves routing accuracy over alternative training-free methods, increasing task performance across expert domains.\n        △ Less",
        "authors":"['William Fleshman', 'Benjamin Van Durme']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimizing Specific and Shared Parameters for Efficient Parameter Tuning",
        "abstract":"Foundation models, with a vast number of parameters and pretraining on massive datasets, achieve state-of-the-art performance across various applications. However, efficiently adapting them to downstream tasks with minimal computational overhead remains a challenge. Parameter-Efficient Transfer Learning (PETL) addresses this by fine-tuning only a small subset of parameters while preserving pre-trained knowledge. In this paper, we propose SaS, a novel PETL method that effectively mitigates distributional shifts during fine-tuning. SaS integrates (1) a shared module that captures common statistical characteristics across layers using low-rank projections and (2) a layer-specific module that employs hypernetworks to generate tailored parameters for each layer. This dual design ensures an optimal balance between performance and parameter efficiency while introducing less than 0.05% additional parameters, making it significantly more compact than existing methods. Extensive experiments on diverse downstream tasks, few-shot settings and domain generalization demonstrate that SaS significantly enhances performance while maintaining superior parameter efficiency compared to existing methods, highlighting the importance of capturing both shared and layer-specific information in transfer learning. Code and data are available at https:\/\/anonymous.4open.science\/r\/SaS-PETL-3565.\n        △ Less",
        "authors":"['Van-Anh Nguyen', 'Thanh-Toan Do', 'Mehrtash Harandi', 'Dinh Phung', 'Trung Le']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models",
        "abstract":"To leverage the full potential of Large Language Models (LLMs) it is crucial to have some information on their answers' uncertainty. This means that the model has to be able to quantify how certain it is in the correctness of a given response. Bad uncertainty estimates can lead to overconfident wrong answers undermining trust in these models. Quite a lot of research has been done on language models that work with text inputs and provide text outputs. Still, since the visual capabilities have been added to these models recently, there has not been much progress on the uncertainty of Visual Language Models (VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found that the severity of the corruption negatively impacted the models' ability to estimate their uncertainty and the models also showed overconfidence in most of the experiments.\n        △ Less",
        "authors":"['Mirko Borszukovszki', 'Ivo Pascal de Jong', 'Matias Valdenegro-Toro']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Early detection of diabetes through transfer learning-based eye (vision) screening and improvement of machine learning model performance and advanced parameter setting algorithms",
        "abstract":"Diabetic Retinopathy (DR) is a serious and common complication of diabetes, caused by prolonged high blood sugar levels that damage the small retinal blood vessels. If left untreated, DR can progress to retinal vein occlusion and stimulate abnormal blood vessel growth, significantly increasing the risk of blindness. Traditional diabetes diagnosis methods often utilize convolutional neural networks (CNNs) to extract visual features from retinal images, followed by classification algorithms such as decision trees and k-nearest neighbors (KNN) for disease detection. However, these approaches face several challenges, including low accuracy and sensitivity, lengthy machine learning (ML) model training due to high data complexity and volume, and the use of limited datasets for testing and evaluation. This study investigates the application of transfer learning (TL) to enhance ML model performance in DR detection. Key improvements include dimensionality reduction, optimized learning rate adjustments, and advanced parameter tuning algorithms, aimed at increasing efficiency and diagnostic accuracy. The proposed model achieved an overall accuracy of 84% on the testing dataset, outperforming prior studies. The highest class-specific accuracy reached 89%, with a maximum sensitivity of 97% and an F1-score of 92%, demonstrating strong performance in identifying DR cases. These findings suggest that TL-based DR screening is a promising approach for early diagnosis, enabling timely interventions to prevent vision loss and improve patient outcomes.\n        △ Less",
        "authors":"['Mohammad Reza Yousefi', 'Ali Bakrani', 'Amin Dehghani']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Polynomial-Time Algorithm for Variational Inequalities under the Minty Condition",
        "abstract":"Solving (Stampacchia) variational inequalities (SVIs) is a foundational problem at the heart of optimization, with a host of critical applications ranging from engineering to economics. However, this expressivity comes at the cost of computational hardness. As a result, most research has focused on carving out specific subclasses that elude those intractability barriers. A classical property that goes back to the 1960s is the Minty condition, which postulates that the Minty VI (MVI) problem -- the weak dual of the SVI problem -- admits a solution.\n  In this paper, we establish the first polynomial-time algorithm -- that is, with complexity growing polynomially in the dimension $d$ and $\\log(1\/ε)$ -- for solving $ε$-SVIs for Lipschitz continuous mappings under the Minty condition. Prior approaches either incurred an exponentially worse dependence on $1\/ε$ (and other natural parameters of the problem) or made overly restrictive assumptions -- such as strong monotonicity. To do so, we introduce a new variant of the ellipsoid algorithm wherein separating hyperplanes are obtained after taking a gradient descent step from the center of the ellipsoid. It succeeds even though the set of SVIs can be nonconvex and not fully dimensional. Moreover, when our algorithm is applied to an instance with no MVI solution and fails to identify an SVI solution, it produces a succinct certificate of MVI infeasibility. We also show that deciding whether the Minty condition holds is $\\mathsf{coNP}$-complete.\n  We provide several extensions and new applications of our main results. Specifically, we obtain the first polynomial-time algorithms for i) solving monotone VIs, ii) globally minimizing a (potentially nonsmooth) quasar-convex function, and iii) computing Nash equilibria in multi-player harmonic games.\n        △ Less",
        "authors":"['Ioannis Anagnostides', 'Gabriele Farina', 'Tuomas Sandholm', 'Brian Hu Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimizing Quantum Circuits via ZX Diagrams using Reinforcement Learning and Graph Neural Networks",
        "abstract":"Quantum computing is currently strongly limited by the impact of noise, in particular introduced by the application of two-qubit gates. For this reason, reducing the number of two-qubit gates is of paramount importance on noisy intermediate-scale quantum hardware. To advance towards more reliable quantum computing, we introduce a framework based on ZX calculus, graph-neural networks and reinforcement learning for quantum circuit optimization. By combining reinforcement learning and tree search, our method addresses the challenge of selecting optimal sequences of ZX calculus rewrite rules. Instead of relying on existing heuristic rules for minimizing circuits, our method trains a novel reinforcement learning policy that directly operates on ZX-graphs, therefore allowing us to search through the space of all possible circuit transformations to find a circuit significantly minimizing the number of CNOT gates. This way we can scale beyond hard-coded rules towards discovering arbitrary optimization rules. We demonstrate our method's competetiveness with state-of-the-art circuit optimizers and generalization capabilities on large sets of diverse random circuits.\n        △ Less",
        "authors":"['Alexander Mattick', 'Maniraman Periyasamy', 'Christian Ufrecht', 'Abhishek Y. Dubey', 'Christopher Mutschler', 'Axel Plinge', 'Daniel D. Scherer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The AI Cosmologist I: An Agentic System for Automated Data Analysis",
        "abstract":"We present the AI Cosmologist, an agentic system designed to automate cosmological\/astronomical data analysis and machine learning research workflows. This implements a complete pipeline from idea generation to experimental evaluation and research dissemination, mimicking the scientific process typically performed by human researchers. The system employs specialized agents for planning, coding, execution, analysis, and synthesis that work together to develop novel approaches. Unlike traditional auto machine-learning systems, the AI Cosmologist generates diverse implementation strategies, writes complete code, handles execution errors, analyzes results, and synthesizes new approaches based on experimental outcomes. We demonstrate the AI Cosmologist capabilities across several machine learning tasks, showing how it can successfully explore solution spaces, iterate based on experimental results, and combine successful elements from different approaches. Our results indicate that agentic systems can automate portions of the research process, potentially accelerating scientific discovery. The code and experimental data used in this paper are available on GitHub at https:\/\/github.com\/adammoss\/aicosmologist. Example papers included in the appendix demonstrate the system's capability to autonomously produce complete scientific publications, starting from only the dataset and task description\n        △ Less",
        "authors":"['Adam Moss']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DML-RAM: Deep Multimodal Learning Framework for Robotic Arm Manipulation using Pre-trained Models",
        "abstract":"This paper presents a novel deep learning framework for robotic arm manipulation that integrates multimodal inputs using a late-fusion strategy. Unlike traditional end-to-end or reinforcement learning approaches, our method processes image sequences with pre-trained models and robot state data with machine learning algorithms, fusing their outputs to predict continuous action values for control. Evaluated on BridgeData V2 and Kuka datasets, the best configuration (VGG16 + Random Forest) achieved MSEs of 0.0021 and 0.0028, respectively, demonstrating strong predictive performance and robustness. The framework supports modularity, interpretability, and real-time decision-making, aligning with the goals of adaptive, human-in-the-loop cyber-physical systems.\n        △ Less",
        "authors":"['Sathish Kumar', 'Swaroop Damodaran', 'Naveen Kumar Kuruba', 'Sumit Jha', 'Arvind Ramanathan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Autonomous state-space segmentation for Deep-RL sparse reward scenarios",
        "abstract":"Dealing with environments with sparse rewards has always been crucial for systems developed to operate in autonomous open-ended learning settings. Intrinsic Motivations could be an effective way to help Deep Reinforcement Learning algorithms learn in such scenarios. In fact, intrinsic reward signals, such as novelty or curiosity, are generally adopted to improve exploration when extrinsic rewards are delayed or absent. Building on previous works, we tackle the problem of learning policies in the presence of sparse rewards by proposing a two-level architecture that alternates an ''intrinsically driven'' phase of exploration and autonomous sub-goal generation, to a phase of sparse reward, goal-directed policy learning. The idea is to build several small networks, each one specialized on a particular sub-path, and use them as starting points for future exploration without the need to further explore from scratch previously learnt paths. Two versions of the system have been trained and tested in the Gym SuperMarioBros environment without considering any additional extrinsic reward. The results show the validity of our approach and the importance of autonomously segment the environment to generate an efficient path towards the final goal.\n        △ Less",
        "authors":"['Gianluca Maselli', 'Vieri Giuliano Santucci']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"NeRFlex: Resource-aware Real-time High-quality Rendering of Complex Scenes on Mobile Devices",
        "abstract":"Neural Radiance Fields (NeRF) is a cutting-edge neural network-based technique for novel view synthesis in 3D reconstruction. However, its significant computational demands pose challenges for deployment on mobile devices. While mesh-based NeRF solutions have shown potential in achieving real-time rendering on mobile platforms, they often fail to deliver high-quality reconstructions when rendering practical complex scenes. Additionally, the non-negligible memory overhead caused by pre-computed intermediate results complicates their practical application. To overcome these challenges, we present NeRFlex, a resource-aware, high-resolution, real-time rendering framework for complex scenes on mobile devices. NeRFlex integrates mobile NeRF rendering with multi-NeRF representations that decompose a scene into multiple sub-scenes, each represented by an individual NeRF network. Crucially, NeRFlex considers both memory and computation constraints as first-class citizens and redesigns the reconstruction process accordingly. NeRFlex first designs a detail-oriented segmentation module to identify sub-scenes with high-frequency details. For each NeRF network, a lightweight profiler, built on domain knowledge, is used to accurately map configurations to visual quality and memory usage. Based on these insights and the resource constraints on mobile devices, NeRFlex presents a dynamic programming algorithm to efficiently determine configurations for all NeRF representations, despite the NP-hardness of the original decision problem. Extensive experiments on real-world datasets and mobile devices demonstrate that NeRFlex achieves real-time, high-quality rendering on commercial mobile devices.\n        △ Less",
        "authors":"['Zhe Wang', 'Yifei Zhu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BitHEP -- The Limits of Low-Precision ML in HEP",
        "abstract":"The increasing complexity of modern neural network architectures demands fast and memory-efficient implementations to mitigate computational bottlenecks. In this work, we evaluate the recently proposed BitNet architecture in HEP applications, assessing its performance in classification, regression, and generative modeling tasks. Specifically, we investigate its suitability for quark-gluon discrimination, SMEFT parameter estimation, and detector simulation, comparing its efficiency and accuracy to state-of-the-art methods. Our results show that while BitNet consistently performs competitively in classification tasks, its performance in regression and generation varies with the size and type of the network, highlighting key limitations and potential areas for improvement.\n        △ Less",
        "authors":"['Claudius Krause', 'Daohan Wang', 'Ramon Winterhalder']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Efficient GPU-based Implementation for Noise Robust Sound Source Localization",
        "abstract":"Robot audition, encompassing Sound Source Localization (SSL), Sound Source Separation (SSS), and Automatic Speech Recognition (ASR), enables robots and smart devices to acquire auditory capabilities similar to human hearing. Despite their wide applicability, processing multi-channel audio signals from microphone arrays in SSL involves computationally intensive matrix operations, which can hinder efficient deployment on Central Processing Units (CPUs), particularly in embedded systems with limited CPU resources. This paper introduces a GPU-based implementation of SSL for robot audition, utilizing the Generalized Singular Value Decomposition-based Multiple Signal Classification (GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an open-source software suite. For a 60-channel microphone array, the proposed implementation achieves significant performance improvements. On the Jetson AGX Orin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2 64-bit CPUs, we observe speedups of 4645.1x for GSVD calculations and 8.8x for the SSL module, while speedups of 2223.4x for GSVD calculation and 8.95x for the entire SSL module on a server configured with an NVIDIA A100 GPU and AMD EPYC 7352 CPUs, making real-time processing feasible for large-scale microphone arrays and providing ample capacity for real-time processing of potential subsequent machine learning or deep learning tasks.\n        △ Less",
        "authors":"['Zirui Lin', 'Masayuki Takigahira', 'Naoya Terakado', 'Haris Gulzar', 'Monikka Roslianna Busto', 'Takeharu Eda', 'Katsutoshi Itoyama', 'Kazuhiro Nakadai', 'Hideharu Amano']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for Energy Efficiency, Output Accuracy, and Inference Latency",
        "abstract":"Deploying Large Language Models (LLMs) on edge devices presents significant challenges due to computational constraints, memory limitations, inference speed, and energy consumption. Model quantization has emerged as a key technique to enable efficient LLM inference by reducing model size and computational overhead. In this study, we conduct a comprehensive analysis of 28 quantized LLMs from the Ollama library, which applies by default Post-Training Quantization (PTQ) and weight-only quantization techniques, deployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy efficiency, inference performance, and output accuracy across multiple quantization levels and task types. Models are benchmarked on five standardized datasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and we employ a high-resolution, hardware-based energy measurement tool to capture real-world power consumption. Our findings reveal the trade-offs between energy efficiency, inference speed, and accuracy in different quantization settings, highlighting configurations that optimize LLM deployment for resource-constrained environments. By integrating hardware-level energy profiling with LLM benchmarking, this study provides actionable insights for sustainable AI, bridging a critical gap in existing research on energy-aware LLM deployment.\n        △ Less",
        "authors":"['Erik Johannes Husom', 'Arda Goknil', 'Merve Astekin', 'Lwin Khin Shar', 'Andre Kåsen', 'Sagar Sen', 'Benedikt Andreas Mithassel', 'Ahmet Soylu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A metrological framework for uncertainty evaluation in machine learning classification models",
        "abstract":"Machine learning (ML) classification models are increasingly being used in a wide range of applications where it is important that predictions are accompanied by uncertainties, including in climate and earth observation, medical diagnosis and bioaerosol monitoring. The output of an ML classification model is a type of categorical variable known as a nominal property in the International Vocabulary of Metrology (VIM). However, concepts related to uncertainty evaluation for nominal properties are not defined in the VIM, nor is such evaluation addressed by the Guide to the Expression of Uncertainty in Measurement (GUM). In this paper we propose a metrological conceptual uncertainty evaluation framework for ML classification, and illustrate its use in the context of two applications that exemplify the issues and have significant societal impact, namely, climate and earth observation and medical diagnosis. Our framework would enable an extension of the VIM and GUM to uncertainty for nominal properties, which would make both applicable to ML classification models.\n        △ Less",
        "authors":"['Samuel Bilson', 'Maurice Cox', 'Anna Pustogvar', 'Andrew Thompson']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bayesian LSTM for indoor temperature modeling",
        "abstract":"Improving energy efficiency of building heating systems is essential for reducing global energy consumption and greenhouse gas emissions. Traditional control methods in buildings rely on static heating curves based solely on outdoor temperature measurements, neglecting system state and free heat sources like solar gain. Model predictive control (MPC) not only addresses these limitations but further optimizes heating control by incorporating weather forecasts and system state predictions. However, current industrial MPC solutions often use simplified physics-inspired models, which compromise accuracy for interpretability. While purely data-driven models offer better predictive performance, they face challenges like overfitting and lack of transparency.\n  To bridge this gap, we propose a Bayesian Long Short-Term Memory (LSTM) architecture for indoor temperature modeling. Our experiments across 100 real-world buildings demonstrate that the Bayesian LSTM outperforms an industrial physics-based model in predictive accuracy, enabling potential for improved energy efficiency and thermal comfort if deployed in heating MPC solutions. Over deterministic black-box approaches, the Bayesian framework provides additional advantages by improving generalization ability and allowing interpretation of predictions via uncertainty quantification. This work advances data-driven heating control by balancing predictive performance with the transparency and reliability required for real-world heating MPC applications.\n        △ Less",
        "authors":"['Emma Hannula', 'Arttu Häkkinen', 'Antti Solonen', 'Felibe Uribe', 'Jana de Wiljes', 'Lassi Roininen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data Augmentation of Time-Series Data in Human Movement Biomechanics: A Scoping Review",
        "abstract":"The integration of machine learning and deep learning has transformed data analytics in biomechanics, enabled by extensive wearable sensor data. However, the field faces challenges such as limited large-scale datasets and high data acquisition costs, which hinder the development of robust algorithms. Data augmentation techniques show promise in addressing these issues, but their application to biomechanical time-series data requires comprehensive evaluation.\n  This scoping review investigates data augmentation methods for time-series data in the biomechanics domain. It analyzes current approaches for augmenting and generating time-series datasets, evaluates their effectiveness, and offers recommendations for applying these techniques in biomechanics.\n  Four databases, PubMed, IEEE Xplore, Scopus, and Web of Science, were searched for studies published between 2013 and 2024. Following PRISMA-ScR guidelines, a two-stage screening identified 21 relevant publications.\n  Results show that there is no universally preferred method for augmenting biomechanical time-series data; instead, methods vary based on study objectives. A major issue identified is the absence of soft tissue artifacts in synthetic data, leading to discrepancies referred to as the synthetic gap. Moreover, many studies lack proper evaluation of augmentation methods, making it difficult to assess their effects on model performance and data quality.\n  This review highlights the critical role of data augmentation in addressing limited dataset availability and improving model generalization in biomechanics. Tailoring augmentation strategies to the characteristics of biomechanical data is essential for advancing predictive modeling. A better understanding of how different augmentation methods impact data quality and downstream tasks will be key to developing more effective and realistic techniques.\n        △ Less",
        "authors":"['Christina Halmich', 'Lucas Höschler', 'Christoph Schranz', 'Christian Borgelt']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Policy Optimization Algorithms in a Unified Framework",
        "abstract":"Policy optimization algorithms are crucial in many fields but challenging to grasp and implement, often due to complex calculations related to Markov decision processes and varying use of discount and average reward setups. This paper presents a unified framework that applies generalized ergodicity theory and perturbation analysis to clarify and enhance the application of these algorithms. Generalized ergodicity theory sheds light on the steady-state behavior of stochastic processes, aiding understanding of both discounted and average rewards. Perturbation analysis provides in-depth insights into the fundamental principles of policy optimization algorithms. We use this framework to identify common implementation errors and demonstrate the correct approaches. Through a case study on Linear Quadratic Regulator problems, we illustrate how slight variations in algorithm design affect implementation outcomes. We aim to make policy optimization algorithms more accessible and reduce their misuse in practice.\n        △ Less",
        "authors":"['Shuang Wu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimal Embedding Guided Negative Sample Generation for Knowledge Graph Link Prediction",
        "abstract":"Knowledge graph embedding (KGE) models encode the structural information of knowledge graphs to predicting new links. Effective training of these models requires distinguishing between positive and negative samples with high precision. Although prior research has shown that improving the quality of negative samples can significantly enhance model accuracy, identifying high-quality negative samples remains a challenging problem. This paper theoretically investigates the condition under which negative samples lead to optimal KG embedding and identifies a sufficient condition for an effective negative sample distribution. Based on this theoretical foundation, we propose \\textbf{E}mbedding \\textbf{MU}tation (\\textsc{EMU}), a novel framework that \\emph{generates} negative samples satisfying this condition, in contrast to conventional methods that focus on \\emph{identifying} challenging negative samples within the training data. Importantly, the simplicity of \\textsc{EMU} ensures seamless integration with existing KGE models and negative sampling methods. To evaluate its efficacy, we conducted comprehensive experiments across multiple datasets. The results consistently demonstrate significant improvements in link prediction performance across various KGE models and negative sampling methods. Notably, \\textsc{EMU} enables performance improvements comparable to those achieved by models with embedding dimension five times larger. An implementation of the method and experiments are available at https:\/\/github.com\/nec-research\/EMU-KG.\n        △ Less",
        "authors":"['Makoto Takamoto', 'Daniel Oñoro-Rubio', 'Wiem Ben Rim', 'Takashi Maruyama', 'Bhushan Kotnis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Block Toeplitz Sparse Precision Matrix Estimation for Large-Scale Interval-Valued Time Series Forecasting",
        "abstract":"Modeling and forecasting interval-valued time series (ITS) have attracted considerable attention due to their growing presence in various contexts. To the best of our knowledge, there have been no efforts to model large-scale ITS. In this paper, we propose a feature extraction procedure for large-scale ITS, which involves key steps such as auto-segmentation and clustering, and feature transfer learning. This procedure can be seamlessly integrated with any suitable prediction models for forecasting purposes. Specifically, we transform the automatic segmentation and clustering of ITS into the estimation of Toeplitz sparse precision matrices and assignment set. The majorization-minimization algorithm is employed to convert this highly non-convex optimization problem into two subproblems. We derive efficient dynamic programming and alternating direction method to solve these two subproblems alternately and establish their convergence properties. By employing the Joint Recurrence Plot (JRP) to image subsequence and assigning a class label to each cluster, an image dataset is constructed. Then, an appropriate neural network is chosen to train on this image dataset and used to extract features for the next step of forecasting. Real data applications demonstrate that the proposed method can effectively obtain invariant representations of the raw data and enhance forecasting performance.\n        △ Less",
        "authors":"['Wan Tian', 'Zhongfeng Qin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Classification of Interval-Valued Time Series",
        "abstract":"In recent years, the modeling and analysis of interval-valued time series have garnered significant attention in the fields of econometrics and statistics. However, the existing literature primarily focuses on regression tasks while neglecting classification aspects. In this paper, we propose an adaptive approach for interval-valued time series classification. Specifically, we represent interval-valued time series using convex combinations of upper and lower bounds of intervals and transform these representations into images based on point-valued time series imaging methods. We utilize a fine-grained image classification neural network to classify these images, to achieve the goal of classifying the original interval-valued time series. This proposed method is applicable to both univariate and multivariate interval-valued time series. On the optimization front, we treat the convex combination coefficients as learnable parameters similar to the parameters of the neural network and provide an efficient estimation method based on the alternating direction method of multipliers (ADMM). On the theoretical front, under specific conditions, we establish a margin-based multiclass generalization bound for generic CNNs composed of basic blocks involving convolution, pooling, and fully connected layers. Through simulation studies and real data applications, we validate the effectiveness of the proposed method and compare its performance against a wide range of point-valued time series classification methods.\n        △ Less",
        "authors":"['Wan Tian', 'Zhongfeng Qin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detecting underdetermination in parameterized quantum circuits",
        "abstract":"A central question in machine learning is how reliable the predictions of a trained model are. Reliability includes the identification of instances for which a model is likely not to be trusted based on an analysis of the learning system itself. Such unreliability for an input may arise from the model family providing a variety of hypotheses consistent with the training data, which can vastly disagree in their predictions on that particular input point. This is called the underdetermination problem, and it is important to develop methods to detect it. With the emergence of quantum machine learning (QML) as a prospective alternative to classical methods for certain learning problems, the question arises to what extent they are subject to underdetermination and whether similar techniques as those developed for classical models can be employed for its detection. In this work, we first provide an overview of concepts from Safe AI and reliability, which in particular received little attention in QML. We then explore the use of a method based on local second-order information for the detection of underdetermination in parameterized quantum circuits through numerical experiments. We further demonstrate that the approach is robust to certain levels of shot noise. Our work contributes to the body of literature on Safe Quantum AI, which is an emerging field of growing importance.\n        △ Less",
        "authors":"['Marie Kempkes', 'Jakob Spiegelberg', 'Evert van Nieuwenburg', 'Vedran Dunjko']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices",
        "abstract":"Large Language Models have significantly advanced natural language processing, achieving remarkable performance in tasks such as language generation, translation, and reasoning. However, their substantial computational requirements restrict deployment to high-end systems, limiting accessibility on consumer-grade devices. This challenge is especially pronounced for under-resourced languages like those spoken in the Iberian Peninsula, where relatively limited linguistic resources and benchmarks hinder effective evaluation. This work presents a comprehensive evaluation of compact state-of-the-art LLMs across several essential NLP tasks tailored for Iberian languages. The results reveal that while some models consistently excel in certain tasks, significant performance gaps remain, particularly for languages such as Basque. These findings highlight the need for further research on balancing model compactness with robust multilingual performance\n        △ Less",
        "authors":"['Luís Couto Seller', 'Íñigo Sanz Torres', 'Adrián Vogel-Fernández', 'Carlos González Carballo', 'Pedro Miguel Sánchez Sánchez', 'Adrián Carruana Martín', 'Enrique de Miguel Ambite']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Roto-Translation Invariant Metrics on Position-Orientation Space",
        "abstract":"Riemannian metrics on the position-orientation space M(3) that are roto-translation group SE(3) invariant play a key role in image analysis tasks like enhancement, denoising, and segmentation. These metrics enable roto-translation equivariant algorithms, with the associated Riemannian distance often used in implementation.\n  However, computing the Riemannian distance is costly, which makes it unsuitable in situations where constant recomputation is needed. We propose the mav (minimal angular velocity) distance, defined as the Riemannian length of a geometrically meaningful curve, as a practical alternative.\n  We see an application of the mav distance in geometric deep learning. Namely, neural networks architectures such as PONITA, relies on geometric invariants to create their roto-translation equivariant model. The mav distance offers a trainable invariant, with the parameters that determine the Riemannian metric acting as learnable weights.\n  In this paper we: 1) classify and parametrize all SE(3) invariant metrics on M(3), 2) describes how to efficiently calculate the mav distance, and 3) investigate if including the mav distance within PONITA can positively impact its accuracy in predicting molecular properties.\n        △ Less",
        "authors":"['Gijs Bellaard', 'Bart M. N. Smets']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Flow: Multi-View-Enriched Normalizing Flows for Industrial Anomaly Detection",
        "abstract":"With more well-performing anomaly detection methods proposed, many of the single-view tasks have been solved to a relatively good degree. However, real-world production scenarios often involve complex industrial products, whose properties may not be fully captured by one single image. While normalizing flow based approaches already work well in single-camera scenarios, they currently do not make use of the priors in multi-view data. We aim to bridge this gap by using these flow-based models as a strong foundation and propose Multi-Flow, a novel multi-view anomaly detection method. Multi-Flow makes use of a novel multi-view architecture, whose exact likelihood estimation is enhanced by fusing information across different views. For this, we propose a new cross-view message-passing scheme, letting information flow between neighboring views. We empirically validate it on the real-world multi-view data set Real-IAD and reach a new state-of-the-art, surpassing current baselines in both image-wise and sample-wise anomaly detection tasks.\n        △ Less",
        "authors":"['Mathis Kruse', 'Bodo Rosenhahn']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Universal Collection of Euclidean Invariants between Pairs of Position-Orientations",
        "abstract":"Euclidean E(3) equivariant neural networks that employ scalar fields on position-orientation space M(3) have been effectively applied to tasks such as predicting molecular dynamics and properties. To perform equivariant convolutional-like operations in these architectures one needs Euclidean invariant kernels on M(3) x M(3). In practice, a handcrafted collection of invariants is selected, and this collection is then fed into multilayer perceptrons to parametrize the kernels. We rigorously describe an optimal collection of 4 smooth scalar invariants on the whole of M(3) x M(3). With optimal we mean that the collection is independent and universal, meaning that all invariants are pertinent, and any invariant kernel is a function of them. We evaluate two collections of invariants, one universal and one not, using the PONITA neural network architecture. Our experiments show that using a collection of invariants that is universal positively impacts the accuracy of PONITA significantly.\n        △ Less",
        "authors":"['Gijs Bellaard', 'Bart M. N. Smets', 'Remco Duits']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"JanusDDG: A Thermodynamics-Compliant Model for Sequence-Based Protein Stability via Two-Fronts Multi-Head Attention",
        "abstract":"Understanding how residue variations affect protein stability is crucial for designing functional proteins and deciphering the molecular mechanisms underlying disease-related mutations. Recent advances in protein language models (PLMs) have revolutionized computational protein analysis, enabling, among other things, more accurate predictions of mutational effects. In this work, we introduce JanusDDG, a deep learning framework that leverages PLM-derived embeddings and a bidirectional cross-attention transformer architecture to predict $ΔΔG$ of single and multiple-residue mutations while simultaneously being constrained to respect fundamental thermodynamic properties, such as antisymmetry and transitivity. Unlike conventional self-attention, JanusDDG computes queries (Q) and values (V) as the difference between wild-type and mutant embeddings, while keys (K) alternate between the two. This cross-interleaved attention mechanism enables the model to capture mutation-induced perturbations while preserving essential contextual information. Experimental results show that JanusDDG achieves state-of-the-art performance in predicting $ΔΔG$ from sequence alone, matching or exceeding the accuracy of structure-based methods for both single and multiple mutations. Code Availability:https:\/\/github.com\/compbiomed-unito\/JanusDDG\n        △ Less",
        "authors":"['Guido Barducci', 'Ivan Rossi', 'Francesco Codicè', 'Cesare Rollo', 'Valeria Repetto', 'Corrado Pancotti', 'Virginia Iannibelli', 'Tiziana Sanavia', 'Piero Fariselli']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Verification of Autonomous Neural Car Control with KeYmaera X",
        "abstract":"This article presents a formal model and formal safety proofs for the ABZ'25 case study in differential dynamic logic (dL). The case study considers an autonomous car driving on a highway avoiding collisions with neighbouring cars. Using KeYmaera X's dL implementation, we prove absence of collision on an infinite time horizon which ensures that safety is preserved independently of trip length. The safety guarantees hold for time-varying reaction time and brake force. Our dL model considers the single lane scenario with cars ahead or behind. We demonstrate that dL with its tools is a rigorous foundation for runtime monitoring, shielding, and neural network verification. Doing so sheds light on inconsistencies between the provided specification and simulation environment highway-env of the ABZ'25 study. We attempt to fix these inconsistencies and uncover numerous counterexamples which also indicate issues in the provided reinforcement learning environment.\n        △ Less",
        "authors":"['Enguerrand Prebet', 'Samuel Teuber', 'André Platzer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving Clinical Imaging Systems using Cognition based Approaches",
        "abstract":"Clinical systems operate in safety-critical environments and are not intended to function autonomously; however, they are currently designed to replicate clinicians' diagnoses rather than assist them in the diagnostic process. To enable better supervision of system-generated diagnoses, we replicate radiologists' systematic approach used to analyze chest X-rays. This approach facilitates comprehensive analysis across all regions of clinical images and can reduce errors caused by inattentional blindness and under reading. Our work addresses a critical research gap by identifying difficult-to-diagnose diseases for clinicians using insights from human vision, enabling these systems to serve as an effective \"second pair of eyes\". These improvements make the clinical imaging systems more complementary and combine the strengths of human and machine vision. Additionally, we leverage effective receptive fields in deep learning models to present machine-generated diagnoses with sufficient context, making it easier for clinicians to evaluate them.\n        △ Less",
        "authors":"['Kailas Dayanandan', 'Brejesh Lall']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rotation Invariance in Floor Plan Digitization using Zernike Moments",
        "abstract":"Nowadays, a lot of old floor plans exist in printed form or are stored as scanned raster images. Slight rotations or shifts may occur during scanning. Bringing floor plans of this form into a machine readable form to enable further use, still poses a problem. Therefore, we propose an end-to-end pipeline that pre-processes the image and leverages a novel approach to create a region adjacency graph (RAG) from the pre-processed image and predict its nodes. By incorporating normalization steps into the RAG feature extraction, we significantly improved the rotation invariance of the RAG feature calculation. Moreover, applying our method leads to an improved F1 score and IoU on rotated data. Furthermore, we proposed a wall splitting algorithm for partitioning walls into segments associated with the corresponding rooms.\n        △ Less",
        "authors":"['Marius Graumann', 'Jan Marius Stürmer', 'Tobias Koch']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Malware Detection in Docker Containers: An Image is Worth a Thousand Logs",
        "abstract":"Malware detection is increasingly challenged by evolving techniques like obfuscation and polymorphism, limiting the effectiveness of traditional methods. Meanwhile, the widespread adoption of software containers has introduced new security challenges, including the growing threat of malicious software injection, where a container, once compromised, can serve as entry point for further cyberattacks. In this work, we address these security issues by introducing a method to identify compromised containers through machine learning analysis of their file systems. We cast the entire software containers into large RGB images via their tarball representations, and propose to use established Convolutional Neural Network architectures on a streaming, patch-based manner. To support our experiments, we release the COSOCO dataset--the first of its kind--containing 3364 large-scale RGB images of benign and compromised software containers at https:\/\/huggingface.co\/datasets\/k3ylabs\/cosoco-image-dataset. Our method detects more malware and achieves higher F1 and Recall scores than all individual and ensembles of VirusTotal engines, demonstrating its effectiveness and setting a new standard for identifying malware-compromised software containers.\n        △ Less",
        "authors":"['Akis Nousias', 'Efklidis Katsaros', 'Evangelos Syrmos', 'Panagiotis Radoglou-Grammatikis', 'Thomas Lagkas', 'Vasileios Argyriou', 'Ioannis Moscholios', 'Evangelos Markakis', 'Sotirios Goudos', 'Panagiotis Sarigiannidis']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Think When You Need: Self-Adaptive Chain-of-Thought Learning",
        "abstract":"Chain of Thought (CoT) reasoning enhances language models' performance but often leads to inefficient \"overthinking\" on simple problems. We identify that existing approaches directly penalizing reasoning length fail to account for varying problem complexity. Our approach constructs rewards through length and quality comparisons, guided by theoretical assumptions that jointly enhance solution correctness with conciseness. Moreover, we further demonstrate our method to fuzzy tasks where ground truth is unavailable. Experiments across multiple reasoning benchmarks demonstrate that our method maintains accuracy while generating significantly more concise explanations, effectively teaching models to \"think when needed.\"\n        △ Less",
        "authors":"['Junjie Yang', 'Ke Lin', 'Xing Yu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Unlocking Neural Transparency: Jacobian Maps for Explainable AI in Alzheimer's Detection",
        "abstract":"Alzheimer's disease (AD) leads to progressive cognitive decline, making early detection crucial for effective intervention. While deep learning models have shown high accuracy in AD diagnosis, their lack of interpretability limits clinical trust and adoption. This paper introduces a novel pre-model approach leveraging Jacobian Maps (JMs) within a multi-modal framework to enhance explainability and trustworthiness in AD detection. By capturing localized brain volume changes, JMs establish meaningful correlations between model predictions and well-known neuroanatomical biomarkers of AD. We validate JMs through experiments comparing a 3D CNN trained on JMs versus on traditional preprocessed data, which demonstrates superior accuracy. We also employ 3D Grad-CAM analysis to provide both visual and quantitative insights, further showcasing improved interpretability and diagnostic reliability.\n        △ Less",
        "authors":"['Yasmine Mustafa', 'Mohamed Elmahallawy', 'Tie Luo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Electromyography-Based Gesture Recognition: Hierarchical Feature Extraction for Enhanced Spatial-Temporal Dynamics",
        "abstract":"Hand gesture recognition using multichannel surface electromyography (sEMG) is challenging due to unstable predictions and inefficient time-varying feature enhancement. To overcome the lack of signal based time-varying feature problems, we propose a lightweight squeeze-excitation deep learning-based multi stream spatial temporal dynamics time-varying feature extraction approach to build an effective sEMG-based hand gesture recognition system. Each branch of the proposed model was designed to extract hierarchical features, capturing both global and detailed spatial-temporal relationships to ensure feature effectiveness. The first branch, utilizing a Bidirectional-TCN (Bi-TCN), focuses on capturing long-term temporal dependencies by modelling past and future temporal contexts, providing a holistic view of gesture dynamics. The second branch, incorporating a 1D Convolutional layer, separable CNN, and Squeeze-and-Excitation (SE) block, efficiently extracts spatial-temporal features while emphasizing critical feature channels, enhancing feature relevance. The third branch, combining a Temporal Convolutional Network (TCN) and Bidirectional LSTM (BiLSTM), captures bidirectional temporal relationships and time-varying patterns. Outputs from all branches are fused using concatenation to capture subtle variations in the data and then refined with a channel attention module, selectively focusing on the most informative features while improving computational efficiency. The proposed model was tested on the Ninapro DB2, DB4, and DB5 datasets, achieving accuracy rates of 96.41%, 92.40%, and 93.34%, respectively. These results demonstrate the capability of the system to handle complex sEMG dynamics, offering advancements in prosthetic limb control and human-machine interface technologies with significant implications for assistive technologies.\n        △ Less",
        "authors":"['Jungpil Shin', 'Abu Saleh Musa Miah', 'Sota Konnai', 'Shu Hoshitaka', 'Pankoo Kim']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning Lie Group Generators from Trajectories",
        "abstract":"This work investigates the inverse problem of generator recovery in matrix Lie groups from discretized trajectories. Let $G$ be a real matrix Lie group and $\\mathfrak{g} = \\text{Lie}(G)$ its corresponding Lie algebra. A smooth trajectory $γ($t$)$ generated by a fixed Lie algebra element $ξ\\in \\mathfrak{g}$ follows the exponential flow $γ($t$) = g_0 \\cdot \\exp(t ξ)$. The central task addressed in this work is the reconstruction of such a latent generator $ξ$ from a discretized sequence of poses $ \\{g_0, g_1, \\dots, g_T\\} \\subset G$, sampled at uniform time intervals. This problem is formulated as a data-driven regression from normalized sequences of discrete Lie algebra increments $\\log\\left(g_{t}^{-1} g_{t+1}\\right)$ to the constant generator $ξ\\in \\mathfrak{g}$. A feedforward neural network is trained to learn this mapping across several groups, including $\\text{SE(2)}, \\text{SE(3)}, \\text{SO(3)}, and \\text{SL(2,$\\mathbb{R})$}$. It demonstrates strong empirical accuracy under both clean and noisy conditions, which validates the viability of data-driven recovery of Lie group generators using shallow neural architectures. This is Lie-RL GitHub Repo https:\/\/github.com\/Anormalm\/LieRL-on-Trajectories. Feel free to make suggestions and collaborations!\n        △ Less",
        "authors":"['Lifan Hu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Structured Knowledge Accumulation: The Principle of Entropic Least Action in Forward-Only Neural Learning",
        "abstract":"This paper aims to extend the Structured Knowledge Accumulation (SKA) framework recently proposed by \\cite{mahi2025ska}. We introduce two core concepts: the Tensor Net function and the characteristic time property of neural learning. First, we reinterpret the learning rate as a time step in a continuous system. This transforms neural learning from discrete optimization into continuous-time evolution. We show that learning dynamics remain consistent when the product of learning rate and iteration steps stays constant. This reveals a time-invariant behavior and identifies an intrinsic timescale of the network. Second, we define the Tensor Net function as a measure that captures the relationship between decision probabilities, entropy gradients, and knowledge change. Additionally, we define its zero-crossing as the equilibrium state between decision probabilities and entropy gradients. We show that the convergence of entropy and knowledge flow provides a natural stopping condition, replacing arbitrary thresholds with an information-theoretic criterion. We also establish that SKA dynamics satisfy a variational principle based on the Euler-Lagrange equation. These findings extend SKA into a continuous and self-organizing learning model. The framework links computational learning with physical systems that evolve by natural laws. By understanding learning as a time-based process, we open new directions for building efficient, robust, and biologically-inspired AI systems.\n        △ Less",
        "authors":"['Bouarfa Mahi Quantiota']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Persuasive Calibration",
        "abstract":"We introduce and study the persuasive calibration problem, where a principal aims to provide trustworthy predictions about underlying events to a downstream agent to make desired decisions. We adopt the standard calibration framework that regulates predictions to be unbiased conditional on their own value, and thus, they can reliably be interpreted at the face value by the agent. Allowing a small calibration error budget, we aim to answer the following question: what is and how to compute the optimal predictor under this calibration error budget, especially when there exists incentive misalignment between the principal and the agent? We focus on standard Lt-norm Expected Calibration Error (ECE) metric.\n  We develop a general framework by viewing predictors as post-processed versions of perfectly calibrated predictors. Using this framework, we first characterize the structure of the optimal predictor. Specifically, when the principal's utility is event-independent and for L1-norm ECE, we show: (1) the optimal predictor is over-(resp. under-) confident for high (resp. low) true expected outcomes, while remaining perfectly calibrated in the middle; (2) the miscalibrated predictions exhibit a collinearity structure with the principal's utility function. On the algorithmic side, we provide a FPTAS for computing approximately optimal predictor for general principal utility and general Lt-norm ECE. Moreover, for the L1- and L-Infinity-norm ECE, we provide polynomial-time algorithms that compute the exact optimal predictor.\n        △ Less",
        "authors":"['Yiding Feng', 'Wei Tang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PIONM: A Generalized Approach to Solving Density-Constrained Mean-Field Games Equilibrium under Modified Boundary Conditions",
        "abstract":"Neural network-based methods are effective for solving equilibria in Mean-Field Games (MFGs), particularly in high-dimensional settings. However, solving the coupled partial differential equations (PDEs) in MFGs limits their applicability since solving coupled PDEs is computationally expensive. Additionally, modifying boundary conditions, such as the initial state distribution or terminal value function, necessitates extensive retraining, reducing scalability. To address these challenges, we propose a generalized framework, PIONM (Physics-Informed Neural Operator NF-MKV Net), which leverages physics-informed neural operators to solve MFGs equations. PIONM utilizes neural operators to compute MFGs equilibria for arbitrary boundary conditions. The method encodes boundary conditions as input features and trains the model to align them with density evolution, modeled using discrete-time normalizing flows. Once trained, the algorithm efficiently computes the density distribution at any time step for modified boundary condition, ensuring efficient adaptation to different boundary conditions in MFGs equilibria. Unlike traditional MFGs methods constrained by fixed coefficients, PIONM efficiently computes equilibria under varying boundary conditions, including obstacles, diffusion coefficients, initial densities, and terminal functions. PIONM can adapt to modified conditions while preserving density distribution constraints, demonstrating superior scalability and generalization capabilities compared to existing methods.\n        △ Less",
        "authors":"['Jinwei Liu', 'Wang Yao', 'Xiao Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BondMatcher: H-Bond Stability Analysis in Molecular Systems",
        "abstract":"This application paper investigates the stability of hydrogen bonds (H-bonds), as characterized by the Quantum Theory of Atoms in Molecules (QTAIM). First, we contribute a database of 4544 electron densities associated to four isomers of water hexamers (the so-called Ring, Book, Cage and Prism), generated by distorting their equilibrium geometry under various structural perturbations, modeling the natural dynamic behavior of molecular systems. Second, we present a new stability measure, called bond occurrence rate, associating each bond path present at equilibrium with its rate of occurrence within the input ensemble. We also provide an algorithm, called BondMatcher, for its automatic computation, based on a tailored, geometry-aware partial isomorphism estimation between the extremum graphs of the considered electron densities. Our new stability measure allows for the automatic identification of densities lacking H-bond paths, enabling further visual inspections. Specifically, the topological analysis enabled by our framework corroborates experimental observations and provides refined geometrical criteria for characterizing the disappearance of H-bond paths. Our electron density database and our C++ implementation are available at this address: https:\/\/github.com\/thom-dani\/BondMatcher.\n        △ Less",
        "authors":"['Thomas Daniel', 'Malgorzata Olejniczak', 'Julien Tierny']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Ground Cost for Optimal Transport of Angular Velocity",
        "abstract":"We revisit the optimal transport problem over angular velocity dynamics given by the controlled Euler equation. The solution of this problem enables stochastic guidance of spin states of a rigid body (e.g., spacecraft) over hard deadline constraint by transferring a given initial state statistics to a desired terminal state statistics. This is an instance of generalized optimal transport over a nonlinear dynamical system. While prior work has reported existence-uniqueness and numerical solution of this dynamical optimal transport problem, here we present structural results about the equivalent Kantorovich a.k.a. optimal coupling formulation. Specifically, we focus on deriving the ground cost for the associated Kantorovich optimal coupling formulation. The ground cost equals to the cost of transporting unit amount of mass from a specific realization of the initial or source joint probability measure to a realization of the terminal or target joint probability measure, and determines the Kantorovich formulation. Finding the ground cost leads to solving a structured deterministic nonlinear optimal control problem, which is shown to be amenable to an analysis technique pioneered by Athans et. al. We show that such techniques have broader applicability in determining the ground cost (thus Kantorovich formulation) for a class of generalized optimal mass transport problems involving nonlinear dynamics with translated norm-invariant drift.\n        △ Less",
        "authors":"['Karthik Elamvazhuthi', 'Abhishek Halder']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Simultaneous Learning of Optimal Transports for Training All-to-All Flow-Based Condition Transfer Model",
        "abstract":"In this paper, we propose a flow-based method for learning all-to-all transfer maps among conditional distributions, approximating pairwise optimal transport. The proposed method addresses the challenge of handling continuous conditions, which often involve a large set of conditions with sparse empirical observations per condition. We introduce a novel cost function that enables simultaneous learning of optimal transports for all pairs of conditional distributions. Our method is supported by a theoretical guarantee that, in the limit, it converges to pairwise optimal transports among infinite pairs of conditional distributions. The learned transport maps are subsequently used to couple data points in conditional flow matching. We demonstrate the effectiveness of this method on synthetic and benchmark datasets, as well as on chemical datasets where continuous physical properties are defined as conditions.\n        △ Less",
        "authors":"['Kotaro Ikeda', 'Masanori Koyama', 'Jinzhe Zhang', 'Kohei Hayashi', 'Kenji Fukumizu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Connection Between Diffusion Models and Molecular Dynamics",
        "abstract":"Neural Network Potentials (NNPs) have emerged as a powerful tool for modelling atomic interactions with high accuracy and computational efficiency. Recently, denoising diffusion models have shown promise in NNPs by training networks to remove noise added to stable configurations, eliminating the need for force data during training. In this work, we explore the connection between noise and forces by providing a new, simplified mathematical derivation of their relationship. We also demonstrate how a denoising model can be implemented using a conventional MD software package interfaced with a standard NNP architecture. We demonstrate the approach by training a diffusion-based NNP to simulate a coarse-grained lithium chloride solution and employ data duplication to enhance model performance.\n        △ Less",
        "authors":"['Liam Harcombe', 'Timothy T. Duignan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mathematical Modeling of Option Pricing with an Extended Black-Scholes Framework",
        "abstract":"This study investigates enhancing option pricing by extending the Black-Scholes model to include stochastic volatility and interest rate variability within the Partial Differential Equation (PDE). The PDE is solved using the finite difference method. The extended Black-Scholes model and a machine learning-based LSTM model are developed and evaluated for pricing Google stock options. Both models were backtested using historical market data. While the LSTM model exhibited higher predictive accuracy, the finite difference method demonstrated superior computational efficiency. This work provides insights into model performance under varying market conditions and emphasizes the potential of hybrid approaches for robust financial modeling.\n        △ Less",
        "authors":"['Nikhil Shivakumar Nayak']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bayesian Optimization of Robustness Measures Using Randomized GP-UCB-based Algorithms under Input Uncertainty",
        "abstract":"Bayesian optimization based on Gaussian process upper confidence bound (GP-UCB) has a theoretical guarantee for optimizing black-box functions. Black-box functions often have input uncertainty, but even in this case, GP-UCB can be extended to optimize evaluation measures called robustness measures. However, GP-UCB-based methods for robustness measures include a trade-off parameter $β$, which must be excessively large to achieve theoretical validity, just like the original GP-UCB. In this study, we propose a new method called randomized robustness measure GP-UCB (RRGP-UCB), which samples the trade-off parameter $β$ from a probability distribution based on a chi-squared distribution and avoids explicitly specifying $β$. The expected value of $β$ is not excessively large. Furthermore, we show that RRGP-UCB provides tight bounds on the expected value of regret based on the optimal solution and estimated solutions. Finally, we demonstrate the usefulness of the proposed method through numerical experiments.\n        △ Less",
        "authors":"['Yu Inatsu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Water Mapping and Change Detection Using Time Series Derived from the Continuous Monitoring of Land Disturbance Algorithm",
        "abstract":"Given the growing environmental challenges, accurate monitoring and prediction of changes in water bodies are essential for sustainable management and conservation. The Continuous Monitoring of Land Disturbance (COLD) algorithm provides a valuable tool for real-time analysis of land changes, such as deforestation, urban expansion, agricultural activities, and natural disasters. This capability enables timely interventions and more informed decision-making. This paper assesses the effectiveness of the algorithm to estimate water bodies and track pixel-level water trends over time. Our findings indicate that COLD-derived data can reliably estimate estimate water frequency during stable periods and delineate water bodies. Furthermore, it enables the evaluation of trends in water areas after disturbances, allowing for the determination of whether water frequency increases, decreases, or remains constant.\n        △ Less",
        "authors":"['Huong Pham', 'Samuel Cheng', 'Tao Hu', 'Chengbin Deng']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Finding the Reflection Point: Unpadding Images to Remove Data Augmentation Artifacts in Large Open Source Image Datasets for Machine Learning",
        "abstract":"In this paper, we address a novel image restoration problem relevant to machine learning dataset curation: the detection and removal of noisy mirrored padding artifacts. While data augmentation techniques like padding are necessary for standardizing image dimensions, they can introduce artifacts that degrade model evaluation when datasets are repurposed across domains. We propose a systematic algorithm to precisely delineate the reflection boundary through a minimum mean squared error approach with thresholding and remove reflective padding. Our method effectively identifies the transition between authentic content and its mirrored counterpart, even in the presence of compression or interpolation noise. We demonstrate our algorithm's efficacy on the SHEL5k dataset, showing significant performance improvements in zero-shot object detection tasks using OWLv2, with average precision increasing from 0.47 to 0.61 for hard hat detection and from 0.68 to 0.73 for person detection. By addressing annotation inconsistencies and distorted objects in padded regions, our approach enhances dataset integrity, enabling more reliable model evaluation across computer vision tasks.\n        △ Less",
        "authors":"['Lucas Choi', 'Ross Greer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhanced Penalty-based Bidirectional Reinforcement Learning Algorithms",
        "abstract":"This research focuses on enhancing reinforcement learning (RL) algorithms by integrating penalty functions to guide agents in avoiding unwanted actions while optimizing rewards. The goal is to improve the learning process by ensuring that agents learn not only suitable actions but also which actions to avoid. Additionally, we reintroduce a bidirectional learning approach that enables agents to learn from both initial and terminal states, thereby improving speed and robustness in complex environments. Our proposed Penalty-Based Bidirectional methodology is tested against Mani skill benchmark environments, demonstrating an optimality improvement of success rate of approximately 4% compared to existing RL implementations. The findings indicate that this integrated strategy enhances policy learning, adaptability, and overall performance in challenging scenarios\n        △ Less",
        "authors":"['Sai Gana Sandeep Pula', 'Sathish A. P. Kumar', 'Sumit Jha', 'Arvind Ramanathan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond Progress Measures: Theoretical Insights into the Mechanism of Grokking",
        "abstract":"Grokking, referring to the abrupt improvement in test accuracy after extended overfitting, offers valuable insights into the mechanisms of model generalization. Existing researches based on progress measures imply that grokking relies on understanding the optimization dynamics when the loss function is dominated solely by the weight decay term. However, we find that this optimization merely leads to token uniformity, which is not a sufficient condition for grokking. In this work, we investigate the grokking mechanism underlying the Transformer in the task of prime number operations. Based on theoretical analysis and experimental validation, we present the following insights: (i) The weight decay term encourages uniformity across all tokens in the embedding space when it is minimized. (ii) The occurrence of grokking is jointly determined by the uniformity of the embedding space and the distribution of the training dataset. Building on these insights, we provide a unified perspective for understanding various previously proposed progress measures and introduce a novel, concise, and effective progress measure that could trace the changes in test loss more accurately. Finally, to demonstrate the versatility of our theoretical framework, we design a dedicated dataset to validate our theory on ResNet-18, successfully showcasing the occurrence of grokking. The code is released at https:\/\/github.com\/Qihuai27\/Grokking-Insight.\n        △ Less",
        "authors":"['Zihan Gu', 'Ruoyu Chen', 'Hua Zhang', 'Yue Hu', 'Xiaochun Cao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments",
        "abstract":"Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https:\/\/github.com\/GAIR-NLP\/DeepResearcher.\n        △ Less",
        "authors":"['Yuxiang Zheng', 'Dayuan Fu', 'Xiangkun Hu', 'Xiaojie Cai', 'Lyumanshan Ye', 'Pengrui Lu', 'Pengfei Liu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accelerating Particle-based Energetic Variational Inference",
        "abstract":"In this work, we propose a novel particle-based variational inference (ParVI) method that accelerates the EVI-Im. Inspired by energy quadratization (EQ) and operator splitting techniques for gradient flows, our approach efficiently drives particles towards the target distribution. Unlike EVI-Im, which employs the implicit Euler method to solve variational-preserving particle dynamics for minimizing the KL divergence, derived using a \"discretize-then-variational\" approach, the proposed algorithm avoids repeated evaluation of inter-particle interaction terms, significantly reducing computational cost. The framework is also extensible to other gradient-based sampling techniques. Through several numerical experiments, we demonstrate that our method outperforms existing ParVI approaches in efficiency, robustness, and accuracy.\n        △ Less",
        "authors":"['Xuelian Bao', 'Lulu Kang', 'Chun Liu', 'Yiwei Wang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MORAL: A Multimodal Reinforcement Learning Framework for Decision Making in Autonomous Laboratories",
        "abstract":"We propose MORAL (a multimodal reinforcement learning framework for decision making in autonomous laboratories) that enhances sequential decision-making in autonomous robotic laboratories through the integration of visual and textual inputs. Using the BridgeData V2 dataset, we generate fine-tuned image captions with a pretrained BLIP-2 vision-language model and combine them with visual features through an early fusion strategy. The fused representations are processed using Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) agents. Experimental results demonstrate that multimodal agents achieve a 20% improvement in task completion rates and significantly outperform visual-only and textual-only baselines after sufficient training. Compared to transformer-based and recurrent multimodal RL models, our approach achieves superior performance in cumulative reward and caption quality metrics (BLEU, METEOR, ROUGE-L). These results highlight the impact of semantically aligned language cues in enhancing agent learning efficiency and generalization. The proposed framework contributes to the advancement of multimodal reinforcement learning and embodied AI systems in dynamic, real-world environments.\n        △ Less",
        "authors":"['Natalie Tirabassi', 'Sathish A. P. Kumar', 'Sumit Jha', 'Arvind Ramanathan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Safe Screening Rules for Group OWL Models",
        "abstract":"Group Ordered Weighted $L_{1}$-Norm (Group OWL) regularized models have emerged as a useful procedure for high-dimensional sparse multi-task learning with correlated features. Proximal gradient methods are used as standard approaches to solving Group OWL models. However, Group OWL models usually suffer huge computational costs and memory usage when the feature size is large in the high-dimensional scenario. To address this challenge, in this paper, we are the first to propose the safe screening rule for Group OWL models by effectively tackling the structured non-separable penalty, which can quickly identify the inactive features that have zero coefficients across all the tasks. Thus, by removing the inactive features during the training process, we may achieve substantial computational gain and memory savings. More importantly, the proposed screening rule can be directly integrated with the existing solvers both in the batch and stochastic settings. Theoretically, we prove our screening rule is safe and also can be safely applied to the existing iterative optimization algorithms. Our experimental results demonstrate that our screening rule can effectively identify the inactive features and leads to a significant computational speedup without any loss of accuracy.\n        △ Less",
        "authors":"['Runxue Bao', 'Quanchao Lu', 'Yanfu Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
        "abstract":"Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.\n        △ Less",
        "authors":"['Jing Bi', 'Susan Liang', 'Xiaofei Zhou', 'Pinxin Liu', 'Junjia Guo', 'Yunlong Tang', 'Luchuan Song', 'Chao Huang', 'Guangyu Sun', 'Jinxi He', 'Jiarui Wu', 'Shu Yang', 'Daoan Zhang', 'Chen Chen', 'Lianggong Bruce Wen', 'Zhang Liu', 'Jiebo Luo', 'Chenliang Xu']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Human Digital Twin Architecture for Knowledge-based Interactions and Context-Aware Conversations",
        "abstract":"Recent developments in Artificial Intelligence (AI) and Machine Learning (ML) are creating new opportunities for Human-Autonomy Teaming (HAT) in tasks, missions, and continuous coordinated activities. A major challenge is enabling humans to maintain awareness and control over autonomous assets, while also building trust and supporting shared contextual understanding. To address this, we present a real-time Human Digital Twin (HDT) architecture that integrates Large Language Models (LLMs) for knowledge reporting, answering, and recommendation, embodied in a visual interface.\n  The system applies a metacognitive approach to enable personalized, context-aware responses aligned with the human teammate's expectations. The HDT acts as a visually and behaviorally realistic team member, integrated throughout the mission lifecycle, from training to deployment to after-action review. Our architecture includes speech recognition, context processing, AI-driven dialogue, emotion modeling, lip-syncing, and multimodal feedback. We describe the system design, performance metrics, and future development directions for more adaptive and realistic HAT systems.\n        △ Less",
        "authors":"['Abdul Mannan Mohammed', 'Azhar Ali Mohammad', 'Jason A. Ortiz', 'Carsten Neumann', 'Grace Bochenek', 'Dirk Reiners', 'Carolina Cruz-Neira']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative Analysis of Unsupervised and Supervised Autoencoders for Nuclei Classification in Clear Cell Renal Cell Carcinoma Images",
        "abstract":"This study explores the application of supervised and unsupervised autoencoders (AEs) to automate nuclei classification in clear cell renal cell carcinoma (ccRCC) images, a diagnostic task traditionally reliant on subjective visual grading by pathologists. We evaluate various AE architectures, including standard AEs, contractive AEs (CAEs), and discriminative AEs (DAEs), as well as a classifier-based discriminative AE (CDAE), optimized using the hyperparameter tuning tool Optuna. Bhattacharyya distance is selected from several metrics to assess class separability in the latent space, revealing challenges in distinguishing adjacent grades using unsupervised models. CDAE, integrating a supervised classifier branch, demonstrated superior performance in both latent space separation and classification accuracy. Given that CDAE-CNN achieved notable improvements in classification metrics, affirming the value of supervised learning for class-specific feature extraction, F1 score was incorporated into the tuning process to optimize classification performance. Results show significant improvements in identifying aggressive ccRCC grades by leveraging the classification capability of AE through latent clustering followed by fine-grained classification. Our model outperforms the current state of the art, CHR-Network, across all evaluated metrics. These findings suggest that integrating a classifier branch in AEs, combined with neural architecture search and contrastive learning, enhances grading automation in ccRCC pathology, particularly in detecting aggressive tumor grades, and may improve diagnostic accuracy.\n        △ Less",
        "authors":"['Fatemeh Javadian', 'Zahra Aminparast', 'Johannes Stegmaier', 'Abin Jose']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable",
        "abstract":"Denoising is a crucial step in many video processing pipelines such as in interactive editing, where high quality, speed, and user control are essential. While recent approaches achieve significant improvements in denoising quality by leveraging deep learning, they are prone to unexpected failures due to discrepancies between training data distributions and the wide variety of noise patterns found in real-world videos. These methods also tend to be slow and lack user control. In contrast, traditional denoising methods perform reliably on in-the-wild videos and run relatively quickly on modern hardware. However, they require manually tuning parameters for each input video, which is not only tedious but also requires skill. We bridge the gap between these two paradigms by proposing a differentiable denoising pipeline based on traditional methods. A neural network is then trained to predict the optimal denoising parameters for each specific input, resulting in a robust and efficient approach that also supports user control.\n        △ Less",
        "authors":"['Xin Jin', 'Simon Niklaus', 'Zhoutong Zhang', 'Zhihao Xia', 'Chunle Guo', 'Yuting Yang', 'Jiawen Chen', 'Chongyi Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GraphSeg: Segmented 3D Representations via Graph Edge Addition and Contraction",
        "abstract":"Robots operating in unstructured environments often require accurate and consistent object-level representations. This typically requires segmenting individual objects from the robot's surroundings. While recent large models such as Segment Anything (SAM) offer strong performance in 2D image segmentation. These advances do not translate directly to performance in the physical 3D world, where they often over-segment objects and fail to produce consistent mask correspondences across views. In this paper, we present GraphSeg, a framework for generating consistent 3D object segmentations from a sparse set of 2D images of the environment without any depth information. GraphSeg adds edges to graphs and constructs dual correspondence graphs: one from 2D pixel-level similarities and one from inferred 3D structure. We formulate segmentation as a problem of edge addition, then subsequent graph contraction, which merges multiple 2D masks into unified object-level segmentations. We can then leverage \\emph{3D foundation models} to produce segmented 3D representations. GraphSeg achieves robust segmentation with significantly fewer images and greater accuracy than prior methods. We demonstrate state-of-the-art performance on tabletop scenes and show that GraphSeg enables improved performance on downstream robotic manipulation tasks. Code available at https:\/\/github.com\/tomtang502\/graphseg.git.\n        △ Less",
        "authors":"['Haozhan Tang', 'Tianyi Zhang', 'Oliver Kroemer', 'Matthew Johnson-Roberson', 'Weiming Zhi']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From Observation to Orientation: an Adaptive Integer Programming Approach to Intervention Design",
        "abstract":"Using both observational and experimental data, a causal discovery process can identify the causal relationships between variables. A unique adaptive intervention design paradigm is presented in this work, where causal directed acyclic graphs (DAGs) are for effectively recovered with practical budgetary considerations. In order to choose treatments that optimize information gain under these considerations, an iterative integer programming (IP) approach is proposed, which drastically reduces the number of experiments required. Simulations over a broad range of graph sizes and edge densities are used to assess the effectiveness of the suggested approach. Results show that the proposed adaptive IP approach achieves full causal graph recovery with fewer intervention iterations and variable manipulations than random intervention baselines, and it is also flexible enough to accommodate a variety of practical constraints.\n        △ Less",
        "authors":"['Abdelmonem Elrefaey', 'Rong Pan']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph Network Modeling Techniques for Visualizing Human Mobility Patterns",
        "abstract":"Human mobility analysis at urban-scale requires models to represent the complex nature of human movements, which in turn are affected by accessibility to nearby points of interest, underlying socioeconomic factors of a place, and local transport choices for people living in a geographic region. In this work, we represent human mobility and the associated flow of movements as a grapyh. Graph-based approaches for mobility analysis are still in their early stages of adoption and are actively being researched. The challenges of graph-based mobility analysis are multifaceted - the lack of sufficiently high-quality data to represent flows at high spatial and teporal resolution whereas, limited computational resources to translate large voluments of mobility data into a network structure, and scaling issues inherent in graph models etc. The current study develops a methodology by embedding graphs into a continuous space, which alleviates issues related to fast graph matching, graph time-series modeling, and visualization of mobility dynamics. Through experiments, we demonstrate how mobility data collected from taxicab trajectories could be transformed into network structures and patterns of mobility flow changes, and can be used for downstream tasks reporting approx 40% decrease in error on average in matched graphs vs unmatched ones.\n        △ Less",
        "authors":"['Sinjini Mitra', 'Anuj Srivastava', 'Avipsa Roy', 'Pavan Turaga']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A computational transition for detecting multivariate shuffled linear regression by low-degree polynomials",
        "abstract":"In this paper, we study the problem of multivariate shuffled linear regression, where the correspondence between predictors and responses in a linear model is obfuscated by a latent permutation. Specifically, we investigate the model $Y=\\tfrac{1}{\\sqrt{1+σ^2}}(Π_* X Q_* + σZ)$, where $X$ is an $n*d$ standard Gaussian design matrix, $Z$ is an $n*m$ Gaussian noise matrix, $Π_*$ is an unknown $n*n$ permutation matrix, and $Q_*$ is an unknown $d*m$ on the Grassmanian manifold satisfying $Q_*^{\\top} Q_* = \\mathbb I_m$.\n  Consider the hypothesis testing problem of distinguishing this model from the case where $X$ and $Y$ are independent Gaussian random matrices of sizes $n*d$ and $n*m$, respectively. Our results reveal a phase transition phenomenon in the performance of low-degree polynomial algorithms for this task. (1) When $m=o(d)$, we show that all degree-$D$ polynomials fail to distinguish these two models even when $σ=0$, provided with $D^4=o\\big( \\tfrac{d}{m} \\big)$. (2) When $m=d$ and $σ=ω(1)$, we show that all degree-$D$ polynomials fail to distinguish these two models provided with $D=o(σ)$. (3) When $m=d$ and $σ=o(1)$, we show that there exists a constant-degree polynomial that strongly distinguish these two models. These results establish a smooth transition in the effectiveness of low-degree polynomial algorithms for this problem, highlighting the interplay between the dimensions $m$ and $d$, the noise level $σ$, and the computational complexity of the testing task.\n        △ Less",
        "authors":"['Zhangsong Li']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Post-processing for Fair Regression via Explainable SVD",
        "abstract":"This paper presents a post-processing algorithm for training fair neural network regression models that satisfy statistical parity, utilizing an explainable singular value decomposition (SVD) of the weight matrix. We propose a linear transformation of the weight matrix, whereby the singular values derived from the SVD of the transformed matrix directly correspond to the differences in the first and second moments of the output distributions across two groups. Consequently, we can convert the fairness constraints into constraints on the singular values. We analytically solve the problem of finding the optimal weights under these constraints. Experimental validation on various datasets demonstrates that our method achieves a similar or superior fairness-accuracy trade-off compared to the baselines without using the sensitive attribute at the inference time.\n        △ Less",
        "authors":"['Zhiqun Zuo', 'Ding Zhu', 'Mohammad Mahdi Khalili']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Machine Learning-Based Detection and Analysis of Suspicious Activities in Bitcoin Wallet Transactions in the USA",
        "abstract":"The dramatic adoption of Bitcoin and other cryptocurrencies in the USA has revolutionized the financial landscape and provided unprecedented investment and transaction efficiency opportunities. The prime objective of this research project is to develop machine learning algorithms capable of effectively identifying and tracking suspicious activity in Bitcoin wallet transactions. With high-tech analysis, the study aims to create a model with a feature for identifying trends and outliers that can expose illicit activity. The current study specifically focuses on Bitcoin transaction information in America, with a strong emphasis placed on the importance of knowing about the immediate environment in and through which such transactions pass through. The dataset is composed of in-depth Bitcoin wallet transactional information, including important factors such as transaction values, timestamps, network flows, and addresses for wallets. All entries in the dataset expose information about financial transactions between wallets, including received and sent transactions, and such information is significant for analysis and trends that can represent suspicious activity. This study deployed three accredited algorithms, most notably, Logistic Regression, Random Forest, and Support Vector Machines. In retrospect, Random Forest emerged as the best model with the highest F1 Score, showcasing its ability to handle non-linear relationships in the data. Insights revealed significant patterns in wallet activity, such as the correlation between unredeemed transactions and final balances. The application of machine algorithms in tracking cryptocurrencies is a tool for creating transparent and secure U.S. markets.\n        △ Less",
        "authors":"['Md Zahidul Islam', 'Md Shahidul Islam', 'Biswajit Chandra das', 'Syed Ali Reza', 'Proshanta Kumar Bhowmik', 'Kanchon Kumar Bishnu', 'Md Shafiqur Rahman', 'Redoyan Chowdhury', 'Laxmi Pant']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Unlocking the AMD Neural Processing Unit for ML Training on the Client Using Bare-Metal-Programming Tools",
        "abstract":"There has been a growing interest in executing machine learning (ML) workloads on the client side for reasons of customizability, privacy, performance, and availability. In response, hardware manufacturers have begun to incorporate so-called Neural Processing Units (NPUs) into their processors for consumer devices. Such dedicated hardware optimizes both power efficiency and throughput for common machine learning tasks. AMD's NPU, part of their Ryzen AI processors, is one of the first such accelerators integrated into a chip with an x86 processor. AMD supports bare-metal programming of their NPU rather than limiting programmers to pre-configured libraries.\n  In this paper, we explore the potential of using a bare-metal toolchain to accelerate the weight fine-tuning of a large language model, GPT-2, entirely on the client side using the AMD NPU. Fine-tuning on the edge allows for private customization of a model to a specific use case. To the best of our knowledge, this is the first time such an accelerator has been used to perform training on the client side. We offload time-intensive matrix multiplication operations from the CPU onto the NPU, achieving a speedup of over 2.8x for these operations. This improves end-to-end performance of the model in terms of throughput (1.7x and 1.2x speedup in FLOPS\/s on mains and battery power, respectively) and energy efficiency (1.4x improvement in FLOPS\/Ws on battery power). We detail our implementation approach and present an in-depth exploration of the NPU hardware and bare-metal tool-flow.\n        △ Less",
        "authors":"['André Rösti', 'Michael Franz']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Integrating Identity-Based Identification against Adaptive Adversaries in Federated Learning",
        "abstract":"Federated Learning (FL) has recently emerged as a promising paradigm for privacy-preserving, distributed machine learning. However, FL systems face significant security threats, particularly from adaptive adversaries capable of modifying their attack strategies to evade detection. One such threat is the presence of Reconnecting Malicious Clients (RMCs), which exploit FLs open connectivity by reconnecting to the system with modified attack strategies. To address this vulnerability, we propose integration of Identity-Based Identification (IBI) as a security measure within FL environments. By leveraging IBI, we enable FL systems to authenticate clients based on cryptographic identity schemes, effectively preventing previously disconnected malicious clients from re-entering the system. Our approach is implemented using the TNC-IBI (Tan-Ng-Chin) scheme over elliptic curves to ensure computational efficiency, particularly in resource-constrained environments like Internet of Things (IoT). Experimental results demonstrate that integrating IBI with secure aggregation algorithms, such as Krum and Trimmed Mean, significantly improves FL robustness by mitigating the impact of RMCs. We further discuss the broader implications of IBI in FL security, highlighting research directions for adaptive adversary detection, reputation-based mechanisms, and the applicability of identity-based cryptographic frameworks in decentralized FL architectures. Our findings advocate for a holistic approach to FL security, emphasizing the necessity of proactive defence strategies against evolving adaptive adversarial threats.\n        △ Less",
        "authors":"['Jakub Kacper Szelag', 'Ji-Jian Chin', 'Lauren Ansell', 'Sook-Chin Yip']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models",
        "abstract":"Video editing and generation methods often rely on pre-trained image-based diffusion models. During the diffusion process, however, the reliance on rudimentary noise sampling techniques that do not preserve correlations present in subsequent frames of a video is detrimental to the quality of the results. This either produces high-frequency flickering, or texture-sticking artifacts that are not amenable to post-processing. With this in mind, we propose a novel method for preserving temporal correlations in a sequence of noise samples. This approach is materialized by a novel noise representation, dubbed $\\int$-noise (integral noise), that reinterprets individual noise samples as a continuously integrated noise field: pixel values do not represent discrete values, but are rather the integral of an underlying infinite-resolution noise over the pixel area. Additionally, we propose a carefully tailored transport method that uses $\\int$-noise to accurately advect noise samples over a sequence of frames, maximizing the correlation between different frames while also preserving the noise properties. Our results demonstrate that the proposed $\\int$-noise can be used for a variety of tasks, such as video restoration, surrogate rendering, and conditional video generation. See https:\/\/warpyournoise.github.io\/ for video results.\n        △ Less",
        "authors":"['Pascal Chang', 'Jingwei Tang', 'Markus Gross', 'Vinicius C. Azevedo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Context-Aware Self-Adaptation for Domain Generalization",
        "abstract":"Domain generalization aims at developing suitable learning algorithms in source training domains such that the model learned can generalize well on a different unseen testing domain. We present a novel two-stage approach called Context-Aware Self-Adaptation (CASA) for domain generalization. CASA simulates an approximate meta-generalization scenario and incorporates a self-adaptation module to adjust pre-trained meta source models to the meta-target domains while maintaining their predictive capability on the meta-source domains. The core concept of self-adaptation involves leveraging contextual information, such as the mean of mini-batch features, as domain knowledge to automatically adapt a model trained in the first stage to new contexts in the second stage. Lastly, we utilize an ensemble of multiple meta-source models to perform inference on the testing domain. Experimental results demonstrate that our proposed method achieves state-of-the-art performance on standard benchmarks.\n        △ Less",
        "authors":"['Hao Yan', 'Yuhong Guo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization",
        "abstract":"3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D reconstruction, achieving high-quality results with real-time radiance field rendering. However, a key challenge is the substantial storage cost: reconstructing a single scene typically requires millions of Gaussian splats, each represented by 59 floating-point parameters, resulting in approximately 1 GB of memory. To address this challenge, we propose a compression method by building separate attribute codebooks and storing only discrete code indices. Specifically, we employ noise-substituted vector quantization technique to jointly train the codebooks and model features, ensuring consistency between gradient descent optimization and parameter discretization. Our method reduces the memory consumption efficiently (around $45\\times$) while maintaining competitive reconstruction quality on standard 3D benchmark scenes. Experiments on different codebook sizes show the trade-off between compression ratio and image quality. Furthermore, the trained compressed model remains fully compatible with popular 3DGS viewers and enables faster rendering speed, making it well-suited for practical applications.\n        △ Less",
        "authors":"['Haishan Wang', 'Mohammad Hassan Vali', 'Arno Solin']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LLM Library Learning Fails: A LEGO-Prover Case Study",
        "abstract":"Recent advancements in the coding, reasoning, and tool-using abilities of LLMs have spurred interest in library learning (i.e., online learning through the creation, storage, and retrieval of reusable and composable functions, knowledge, checklists, or lemmas). Such systems often promise improved task performance through the automatic creation of broadly applicable tools, as well as superior computational performance through the caching of reasoning (i.e., the storage of generated tools). However, we find strong reason to be skeptical. We perform a deep dive into one such system, LEGO-Prover, which purports to learn reusable lemmas for mathematical reasoning. We find no evidence of the direct reuse of learned lemmas, and find evidence against the soft reuse of learned lemmas (i.e., reuse by modifying relevant examples). Crucially, we find that LEGO-Prover does not in fact improve over the simple baseline of prompting the model - the improvements in task accuracy vanish once computational cost is accounted for. Our findings suggest that serious misconceptions exist as to the effectiveness of these techniques, that a serious re-examination of the state of LLM-based library learning is required, and that we require much stronger standards for evaluation including behavioural analysis and ensuring that an equal computational budget is used for baselines.\n        △ Less",
        "authors":"['Ian Berlot-Attwell', 'Frank Rudzicz', 'Xujie Si']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Safety Modulation: Enhancing Safety in Reinforcement Learning through Cost-Modulated Rewards",
        "abstract":"Safe Reinforcement Learning (Safe RL) aims to train an RL agent to maximize its performance in real-world environments while adhering to safety constraints, as exceeding safety violation limits can result in severe consequences. In this paper, we propose a novel safe RL approach called Safety Modulated Policy Optimization (SMPO), which enables safe policy function learning within the standard policy optimization framework through safety modulated rewards. In particular, we consider safety violation costs as feedback from the RL environments that are parallel to the standard awards, and introduce a Q-cost function as safety critic to estimate expected future cumulative costs. Then we propose to modulate the rewards using a cost-aware weighting function, which is carefully designed to ensure the safety limits based on the estimation of the safety critic, while maximizing the expected rewards. The policy function and the safety critic are simultaneously learned through gradient descent during online interactions with the environment. We conduct experiments using multiple RL environments and the experimental results demonstrate that our method outperforms several classic and state-of-the-art comparison methods in terms of overall safe RL performance.\n        △ Less",
        "authors":"['Hanping Zhang', 'Yuhong Guo']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"High-dimensional ridge regression with random features for non-identically distributed data with a variance profile",
        "abstract":"The behavior of the random feature model in the high-dimensional regression framework has become a popular issue of interest in the machine learning literature}. This model is generally considered for feature vectors $x_i = Σ^{1\/2} x_i'$, where $x_i'$ is a random vector made of independent and identically distributed (iid) entries, and $Σ$ is a positive definite matrix representing the covariance of the features.\n  In this paper, we move beyond {\\CB this standard assumption by studying the performances of the random features model in the setting of non-iid feature vectors}. Our approach is related to the analysis of the spectrum of large random matrices through random matrix theory (RMT) {\\CB and free probability} results. We turn to the analysis of non-iid data by using the notion of variance profile {\\CB which} is {\\CB well studied in RMT.} Our main contribution is then the study of the limits of the training and {\\CB prediction} risks associated to the ridge estimator in the random features model when its dimensions grow. We provide asymptotic equivalents of these risks that capture the behavior of ridge regression with random features in a {\\CB high-dimensional} framework. These asymptotic equivalents, {\\CB which prove to be sharp in numerical experiments}, are retrieved by adapting, to our setting, established results from operator-valued free probability theory. Moreover, {\\CB for various classes of random feature vectors that have not been considered so far in the literature}, our approach allows to show the appearance of the double descent phenomenon when the ridge regularization parameter is small enough.\n        △ Less",
        "authors":"['Issa-Mbenard Dabo', 'Jérémie Bigot']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Reinforcement Learning via Object-Centric Attention",
        "abstract":"Deep reinforcement learning agents, trained on raw pixel inputs, often fail to generalize beyond their training environments, relying on spurious correlations and irrelevant background details. To address this issue, object-centric agents have recently emerged. However, they require different representations tailored to the task specifications. Contrary to deep agents, no single object-centric architecture can be applied to any environment. Inspired by principles of cognitive science and Occam's Razor, we introduce Object-Centric Attention via Masking (OCCAM), which selectively preserves task-relevant entities while filtering out irrelevant visual information. Specifically, OCCAM takes advantage of the object-centric inductive bias. Empirical evaluations on Atari benchmarks demonstrate that OCCAM significantly improves robustness to novel perturbations and reduces sample complexity while showing similar or improved performance compared to conventional pixel-based RL. These results suggest that structured abstraction can enhance generalization without requiring explicit symbolic representations or domain-specific object extraction pipelines.\n        △ Less",
        "authors":"['Jannis Blüml', 'Cedric Derstroff', 'Bjarne Gregori', 'Elisabeth Dillies', 'Quentin Delfosse', 'Kristian Kersting']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Page Classification for Print Imaging Pipeline",
        "abstract":"Digital copiers and printers are widely used nowadays. One of the most important things people care about is copying or printing quality. In order to improve it, we previously came up with an SVM-based classification method to classify images with only text, only pictures or a mixture of both based on the fact that modern copiers and printers are equipped with processing pipelines designed specifically for different kinds of images. However, in some other applications, we need to distinguish more than three classes. In this paper, we develop a more advanced SVM-based classification method using four more new features to classify 5 types of images which are text, picture, mixed, receipt and highlight.\n        △ Less",
        "authors":"['Shaoyuan Xu', 'Cheng Lu', 'Mark Shaw', 'Peter Bauer', 'Jan P. Allebach']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Emotion Recognition Using Convolutional Neural Networks",
        "abstract":"Emotion has an important role in daily life, as it helps people better communicate with and understand each other more efficiently. Facial expressions can be classified into 7 categories: angry, disgust, fear, happy, neutral, sad and surprise. How to detect and recognize these seven emotions has become a popular topic in the past decade. In this paper, we develop an emotion recognition system that can apply emotion recognition on both still images and real-time videos by using deep learning.\n  We build our own emotion recognition classification and regression system from scratch, which includes dataset collection, data preprocessing , model training and testing. Given a certain image or a real-time video, our system is able to show the classification and regression results for all of the 7 emotions. The proposed system is tested on 2 different datasets, and achieved an accuracy of over 80\\%. Moreover, the result obtained from real-time testing proves the feasibility of implementing convolutional neural networks in real time to detect emotions accurately and efficiently.\n        △ Less",
        "authors":"['Shaoyuan Xu', 'Yang Cheng', 'Qian Lin', 'Jan P. Allebach']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving Efficiency in Federated Learning with Optimized Homomorphic Encryption",
        "abstract":"Federated learning is a method used in machine learning to allow multiple devices to work together on a model without sharing their private data. Each participant keeps their private data on their system and trains a local model and only sends updates to a central server, which combines these updates to improve the overall model. A key enabler of privacy in FL is homomorphic encryption (HE). HE allows computations to be performed directly on encrypted data. While HE offers strong privacy guarantees, it is computationally intensive, leading to significant latency and scalability issues, particularly for large-scale models like BERT. In my research, I aimed to address this inefficiency problem. My research introduces a novel algorithm to address these inefficiencies while maintaining robust privacy guarantees. I integrated several mathematical techniques such as selective parameter encryption, sensitivity maps, and differential privacy noise within my algorithms, which has already improved its efficiency. I have also conducted rigorous mathematical proofs to validate the correctness and robustness of the approach. I implemented this algorithm by coding it in C++, simulating the environment of federated learning on large-scale models, and verified that the efficiency of my algorithm is $3$ times the efficiency of the state-of-the-art method. This research has significant implications for machine learning because its ability to improve efficiency while balancing privacy makes it a practical solution! It would enable federated learning to be used very efficiently and deployed in various resource-constrained environments, as this research provides a novel solution to one of the key challenges in federated learning: the inefficiency of homomorphic encryption, as my new algorithm is able to enhance the scalability and resource efficiency of FL while maintaining robust privacy guarantees.\n        △ Less",
        "authors":"['Feiran Yang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Anomaly Detection in Time Series Data Using Reinforcement Learning, Variational Autoencoder, and Active Learning",
        "abstract":"A novel approach to detecting anomalies in time series data is presented in this paper. This approach is pivotal in domains such as data centers, sensor networks, and finance. Traditional methods often struggle with manual parameter tuning and cannot adapt to new anomaly types. Our method overcomes these limitations by integrating Deep Reinforcement Learning (DRL) with a Variational Autoencoder (VAE) and Active Learning. By incorporating a Long Short-Term Memory (LSTM) network, our approach models sequential data and its dependencies effectively, allowing for the detection of new anomaly classes with minimal labeled data. Our innovative DRL- VAE and Active Learning combination significantly improves existing methods, as shown by our evaluations on real-world datasets, enhancing anomaly detection techniques and advancing time series analysis.\n        △ Less",
        "authors":"['Bahareh Golchin', 'Banafsheh Rekabdar']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Noise-Aware Generalization: Robustness to In-Domain Noise and Out-of-Domain Generalization",
        "abstract":"Multi-source Domain Generalization (DG) aims to improve model robustness to new distributions. However, DG methods often overlook the effect of label noise, which can confuse a model during training, reducing performance. Limited prior work has analyzed DG method's noise-robustness, typically focused on an analysis of existing methods rather than new solutions. In this paper, we investigate this underexplored space, where models are evaluated under both distribution shifts and label noise, which we refer to as Noise-Aware Generalization (NAG). A natural solution to address label noise would be to combine a Learning with Noisy Labels (LNL) method with those from DG. Many LNL methods aim to detect distribution shifts in a class's samples, i.e., they assume that distribution shifts often correspond to label noise. However, in NAG distribution shifts can be due to label noise or domain shifts, breaking the assumptions used by LNL methods. A naive solution is to make a similar assumption made by many DG methods, where we presume to have domain labels during training, enabling us to isolate the two types of shifts. However, this ignores valuable cross-domain information. Specifically, our proposed DL4ND approach improves noise detection by taking advantage of the observation that noisy samples that may appear indistinguishable within a single domain often show greater variation when compared across domains. Experiments show that DL4ND significantly improves performance across four diverse datasets, offering a promising direction for tackling NAG.\n        △ Less",
        "authors":"['Siqi Wang', 'Aoming Liu', 'Bryan A. Plummer']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving log-based anomaly detection through learned adaptive filter",
        "abstract":"Log messages record important system runtime information and are useful for detecting anomalous behaviors and managing modern software systems. Many supervised and unsupervised learning methods have been proposed recently for log-based anomaly detection. State-of-the-art unsupervised methods predict the next log event given a log sequence and apply fixed configurations that use the same filter condition (i.e. k, the top k predicted log events will be regarded as normal next events) which leads to inferior performance in the detection stage because it sets one fixed k for all log sequences, which ignores the dynamic nature and variance in different log sequences. Recently, deep reinforcement learning (DRL) are widely applied to make intelligent decisions in a dynamic environment. In this work, we contend that it is necessary to apply adaptive filters for different log sequences. To achieve this, we propose a novel approach based on DRL to construct a learned adaptive filter and apply different normal\/abnormal filter thresholds for different log sequences. We define the Markov Decision Process (MDP) and formulate the learned adaptive filter as a problem that can be solved by DRL. We evaluate the learned adaptive filter on two state-of-the-art log-based anomaly detection unsupervised approaches DeepLog and LogAnomaly in two datasets HDFS and BGL. Extensive experiments show that our approach outperforms the fixed configurations and achieves significantly better performance in log-based anomaly detection.\n        △ Less",
        "authors":"['Yiyuan Xiong', 'Shaofeng Cai']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Route Recommendations for Traffic Management Under Learned Partial Driver Compliance",
        "abstract":"In this paper, we aim to mitigate congestion in traffic management systems by guiding travelers along system-optimal (SO) routes. However, we recognize that most theoretical approaches assume perfect driver compliance, which often does not reflect reality, as drivers tend to deviate from recommendations to fulfill their personal objectives. Therefore, we propose a route recommendation framework that explicitly learns partial driver compliance and optimizes traffic flow under realistic adherence. We first compute an SO edge flow through flow optimization techniques. Next, we train a compliance model based on historical driver decisions to capture individual responses to our recommendations. Finally, we formulate a stochastic optimization problem that minimizes the gap between the target SO flow and the realized flow under conditions of imperfect adherence. Our simulations conducted on a grid network reveal that our approach significantly reduces travel time compared to baseline strategies, demonstrating the practical advantage of incorporating learned compliance into traffic management.\n        △ Less",
        "authors":"['Heeseung Bang', 'Jung-Hoon Cho', 'Cathy Wu', 'Andreas A. Malikopoulos']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Localized Definitions and Distributed Reasoning: A Proof-of-Concept Mechanistic Interpretability Study via Activation Patching",
        "abstract":"This study investigates the localization of knowledge representation in fine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching (CLAP), a method that identifies critical neural layers responsible for correct answer generation. The model was fine-tuned on 9,958 PubMed abstracts (epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions) using two configurations with validation loss monitoring for early stopping. CLAP involved (1) caching clean (correct answer) and corrupted (incorrect answer) activations, (2) computing logit difference to quantify model preference, and (3) patching corrupted activations with clean ones to assess recovery. Results revealed three findings: First, patching the first feedforward layer recovered 56% of correct preference, demonstrating that associative knowledge is distributed across multiple layers. Second, patching the final output layer completely restored accuracy (100% recovery), indicating that definitional knowledge is localised. The stronger clean logit difference for definitional questions further supports this localized representation. Third, minimal recovery from convolutional layer patching (13.6%) suggests low-level features contribute marginally to high-level reasoning. Statistical analysis confirmed significant layer-specific effects (p<0.01). These findings demonstrate that factual knowledge is more localized and associative knowledge depends on distributed representations. We also showed that editing efficacy depends on task type. Our findings not only reconcile conflicting observations about localization in model editing but also emphasize on using task-adaptive techniques for reliable, interpretable updates.\n        △ Less",
        "authors":"['Nooshin Bahador']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Global-Order GFlowNets",
        "abstract":"Order-Preserving (OP) GFlowNets have demonstrated remarkable success in tackling complex multi-objective (MOO) black-box optimization problems using stochastic optimization techniques. Specifically, they can be trained online to efficiently sample diverse candidates near the Pareto front. A key advantage of OP GFlowNets is their ability to impose a local order on training samples based on Pareto dominance, eliminating the need for scalarization - a common requirement in other approaches like Preference-Conditional GFlowNets. However, we identify an important limitation of OP GFlowNets: imposing a local order on training samples can lead to conflicting optimization objectives. To address this issue, we introduce Global-Order GFlowNets, which transform the local order into a global one, thereby resolving these conflicts. Our experimental evaluations on various benchmarks demonstrate the efficacy and promise of our proposed method.\n        △ Less",
        "authors":"['Lluís Pastor-Pérez', 'Javier Alonso-Garcia', 'Lukas Mauch']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CoLa -- Learning to Interactively Collaborate with Large LMs",
        "abstract":"LLMs' remarkable ability to tackle a wide range of language tasks opened new opportunities for collaborative human-AI problem solving. LLMs can amplify human capabilities by applying their intuitions and reasoning strategies at scale. We explore whether human guides can be simulated, by generalizing from human demonstrations of guiding an AI system to solve complex language problems. We introduce CoLa, a novel self-guided learning paradigm for training automated $\\textit{guides}$ and evaluate it on two QA datasets, a puzzle-solving task, and a constrained text generation task. Our empirical results show that CoLa consistently outperforms competitive approaches across all domains. Moreover, a small-sized trained guide outperforms a strong model like GPT-4 when acting as a guide. We compare the strategies employed by humans and automated guides by conducting a human study on a QA dataset. We show that automated guides outperform humans by adapting their strategies to reasoners' capabilities and conduct qualitative analyses highlighting distinct differences in guiding strategies.\n        △ Less",
        "authors":"['Abhishek Sharma', 'Dan Goldwasser']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Randomized Pairwise Learning with Adaptive Sampling: A PAC-Bayes Analysis",
        "abstract":"We study stochastic optimization with data-adaptive sampling schemes to train pairwise learning models. Pairwise learning is ubiquitous, and it covers several popular learning tasks such as ranking, metric learning and AUC maximization. A notable difference of pairwise learning from pointwise learning is the statistical dependencies among input pairs, for which existing analyses have not been able to handle in the general setting considered in this paper. To this end, we extend recent results that blend together two algorithm-dependent frameworks of analysis -- algorithmic stability and PAC-Bayes -- which allow us to deal with any data-adaptive sampling scheme in the optimizer. We instantiate this framework to analyze (1) pairwise stochastic gradient descent, which is a default workhorse in many machine learning problems, and (2) pairwise stochastic gradient descent ascent, which is a method used in adversarial training. All of these algorithms make use of a stochastic sampling from a discrete distribution (sample indices) before each update. Non-uniform sampling of these indices has been already suggested in the recent literature, to which our work provides generalization guarantees in both smooth and non-smooth convex problems.\n        △ Less",
        "authors":"['Sijia Zhou', 'Yunwen Lei', 'Ata Kabán']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph Attention for Heterogeneous Graphs with Positional Encoding",
        "abstract":"Graph Neural Networks (GNNs) have emerged as the de facto standard for modeling graph data, with attention mechanisms and transformers significantly enhancing their performance on graph-based tasks. Despite these advancements, the performance of GNNs on heterogeneous graphs often remains complex, with networks generally underperforming compared to their homogeneous counterparts. This work benchmarks various GNN architectures to identify the most effective methods for heterogeneous graphs, with a particular focus on node classification and link prediction. Our findings reveal that graph attention networks excel in these tasks. As a main contribution, we explore enhancements to these attention networks by integrating positional encodings for node embeddings. This involves utilizing the full Laplacian spectrum to accurately capture both the relative and absolute positions of each node within the graph, further enhancing performance on downstream tasks such as node classification and link prediction.\n        △ Less",
        "authors":"['Nikhil Shivakumar Nayak']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robustly identifying concepts introduced during chat fine-tuning using crosscoders",
        "abstract":"Model diffing is the study of how fine-tuning changes a model's representations and internal algorithms. Many behaviours of interest are introduced during fine-tuning, and model diffing offers a promising lens to interpret such behaviors. Crosscoders are a recent model diffing method that learns a shared dictionary of interpretable concepts represented as latent directions in both the base and fine-tuned models, allowing us to track how concepts shift or emerge during fine-tuning. Notably, prior work has observed concepts with no direction in the base model, and it was hypothesized that these model-specific latents were concepts introduced during fine-tuning. However, we identify two issues which stem from the crosscoders L1 training loss that can misattribute concepts as unique to the fine-tuned model, when they really exist in both models. We develop Latent Scaling to flag these issues by more accurately measuring each latent's presence across models. In experiments comparing Gemma 2 2B base and chat models, we observe that the standard crosscoder suffers heavily from these issues. Building on these insights, we train a crosscoder with BatchTopK loss and show that it substantially mitigates these issues, finding more genuinely chat-specific and highly interpretable concepts. We recommend practitioners adopt similar techniques. Using the BatchTopK crosscoder, we successfully identify a set of genuinely chat-specific latents that are both interpretable and causally effective, representing concepts such as $\\textit{false information}$ and $\\textit{personal question}$, along with multiple refusal-related latents that show nuanced preferences for different refusal triggers. Overall, our work advances best practices for the crosscoder-based methodology for model diffing and demonstrates that it can provide concrete insights into how chat tuning modifies language model behavior.\n        △ Less",
        "authors":"['Julian Minder', 'Clement Dumas', 'Caden Juang', 'Bilal Chugtai', 'Neel Nanda']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LiDAR-based Object Detection with Real-time Voice Specifications",
        "abstract":"This paper presents a LiDAR-based object detection system with real-time voice specifications, integrating KITTI's 3D point clouds and RGB images through a multi-modal PointNet framework. It achieves 87.0% validation accuracy on a 3000-sample subset, surpassing a 200-sample baseline of 67.5% by combining spatial and visual data, addressing class imbalance with weighted loss, and refining training via adaptive techniques. A Tkinter prototype provides natural Indian male voice output using Edge TTS (en-IN-PrabhatNeural), alongside 3D visualizations and real-time feedback, enhancing accessibility and safety in autonomous navigation, assistive technology, and beyond. The study offers a detailed methodology, comprehensive experimental analysis, and a broad review of applications and challenges, establishing this work as a scalable advancement in human-computer interaction and environmental perception, aligned with current research trends.\n        △ Less",
        "authors":"['Anurag Kulkarni']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ConfEviSurrogate: A Conformalized Evidential Surrogate Model for Uncertainty Quantification",
        "abstract":"Surrogate models, crucial for approximating complex simulation data across sciences, inherently carry uncertainties that range from simulation noise to model prediction errors. Without rigorous uncertainty quantification, predictions become unreliable and hence hinder analysis. While methods like Monte Carlo dropout and ensemble models exist, they are often costly, fail to isolate uncertainty types, and lack guaranteed coverage in prediction intervals. To address this, we introduce ConfEviSurrogate, a novel Conformalized Evidential Surrogate Model that can efficiently learn high-order evidential distributions, directly predict simulation outcomes, separate uncertainty sources, and provide prediction intervals. A conformal prediction-based calibration step further enhances interval reliability to ensure coverage and improve efficiency. Our ConfEviSurrogate demonstrates accurate predictions and robust uncertainty estimates in diverse simulations, including cosmology, ocean dynamics, and fluid dynamics.\n        △ Less",
        "authors":"['Yuhan Duan', 'Xin Zhao', 'Neng Shi', 'Han-Wei Shen']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Feature Engineering on LMS Data to Optimize Student Performance Prediction",
        "abstract":"Nearly every educational institution uses a learning management system (LMS), often producing terabytes of data generated by thousands of people. We examine LMS grade and login data from a regional comprehensive university, specifically documenting key considerations for engineering features from these data when trying to predict student performance. We specifically document changes to LMS data patterns since Covid-19, which are critical for data scientists to account for when using historic data. We compare numerous engineered features and approaches to utilizing those features for machine learning. We finish with a summary of the implications of including these features into more comprehensive student performance models.\n        △ Less",
        "authors":"['Keith Hubbard', 'Sheilla Amponsah']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Haphazard Inputs as Images in Online Learning",
        "abstract":"The field of varying feature space in online learning settings, also known as haphazard inputs, is very prominent nowadays due to its applicability in various fields. However, the current solutions to haphazard inputs are model-dependent and cannot benefit from the existing advanced deep-learning methods, which necessitate inputs of fixed dimensions. Therefore, we propose to transform the varying feature space in an online learning setting to a fixed-dimension image representation on the fly. This simple yet novel approach is model-agnostic, allowing any vision-based models to be applicable for haphazard inputs, as demonstrated using ResNet and ViT. The image representation handles the inconsistent input data seamlessly, making our proposed approach scalable and robust. We show the efficacy of our method on four publicly available datasets. The code is available at https:\/\/github.com\/Rohit102497\/HaphazardInputsAsImages.\n        △ Less",
        "authors":"['Rohit Agarwal', 'Aryan Dessai', 'Arif Ahmed Sekh', 'Krishna Agarwal', 'Alexander Horsch', 'Dilip K. Prasad']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Air Quality Monitoring: A Brief Review of Federated Learning Advances",
        "abstract":"Monitoring air quality and environmental conditions is crucial for public health and effective urban planning. Current environmental monitoring approaches often rely on centralized data collection and processing, which pose significant privacy, security, and scalability challenges. Federated Learning (FL) offers a promising solution to these limitations by enabling collaborative model training across multiple devices without sharing raw data. This decentralized approach addresses privacy concerns while still leveraging distributed data sources. This paper provides a comprehensive review of FL applications in air quality and environmental monitoring, emphasizing its effectiveness in predicting pollutants and managing environmental data. However, the paper also identifies key limitations of FL when applied in this domain, including challenges such as communication overhead, infrastructure demands, generalizability issues, computational complexity, and security vulnerabilities. For instance, communication overhead, caused by the frequent exchange of model updates between local devices and central servers, is a notable challenge. To address this, future research should focus on optimizing communication protocols and reducing the frequency of updates to lessen the burden on network resources. Additionally, the paper suggests further research directions to refine FL frameworks and enhance their applicability in real-world environmental monitoring scenarios. By synthesizing findings from existing studies, this paper highlights the potential of FL to improve air quality management while maintaining data privacy and security, and it provides valuable insights for future developments in the field.\n        △ Less",
        "authors":"['Sara Yarham', 'Mehran Behjati']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scenario Discovery for Urban Planning: The Case of Green Urbanism and the Impact on Stress",
        "abstract":"Urban environments significantly influence mental health outcomes, yet the role of an effective framework for decision-making under deep uncertainty (DMDU) for optimizing urban policies for stress reduction remains underexplored. While existing research has demonstrated the effects of urban design on mental health, there is a lack of systematic scenario-based analysis to guide urban planning decisions. This study addresses this gap by applying Scenario Discovery (SD) in urban planning to evaluate the effectiveness of urban vegetation interventions in stress reduction across different urban environments using a predictive model based on emotional responses collected from a neuroscience-based outdoor experiment in Lisbon. Combining these insights with detailed urban data from Copenhagen, we identify key intervention thresholds where vegetation-based solutions succeed or fail in mitigating stress responses. Our findings reveal that while increased vegetation generally correlates with lower stress levels, high-density urban environments, crowding, and individual psychological traits (e.g., extraversion) can reduce its effectiveness. This work showcases our Scenario Discovery framework as a systematic approach for identifying robust policy pathways in urban planning, opening the door for its exploration in other urban decision-making contexts where uncertainty and design resiliency are critical.\n        △ Less",
        "authors":"['Lorena Torres Lahoz', 'Carlos Lima Azevedo', 'Leonardo Ancora', 'Paulo Morgado', 'Zenia Kotval', 'Bruno Miranda', 'Francisco Camara Pereira']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence",
        "abstract":"Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by linear vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training.\n        △ Less",
        "authors":"['Hongzhe Du', 'Weikai Li', 'Min Cai', 'Karim Saraipour', 'Zimin Zhang', 'Himabindu Lakkaraju', 'Yizhou Sun', 'Shichang Zhang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hide and Seek in Noise Labels: Noise-Robust Collaborative Active Learning with LLM-Powered Assistance",
        "abstract":"Learning from noisy labels (LNL) is a challenge that arises in many real-world scenarios where collected training data can contain incorrect or corrupted labels. Most existing solutions identify noisy labels and adopt active learning to query human experts on them for denoising. In the era of large language models (LLMs), although we can reduce the human effort to improve these methods, their performances are still subject to accurately separating the clean and noisy samples from noisy data. In this paper, we propose an innovative collaborative learning framework NoiseAL based on active learning to combine LLMs and small models (SMs) for learning from noisy labels. During collaborative training, we first adopt two SMs to form a co-prediction network and propose a dynamic-enhanced threshold strategy to divide the noisy data into different subsets, then select the clean and noisy samples from these subsets to feed the active annotator LLMs to rectify noisy samples. Finally, we employ different optimization objectives to conquer subsets with different degrees of label noises. Extensive experiments on synthetic and real-world noise datasets further demonstrate the superiority of our framework over state-of-the-art baselines.\n        △ Less",
        "authors":"['Bo Yuan', 'Yulin Chen', 'Yin Zhang', 'Wei Jiang']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative Analysis of Deepfake Detection Models: New Approaches and Perspectives",
        "abstract":"The growing threat posed by deepfake videos, capable of manipulating realities and disseminating misinformation, drives the urgent need for effective detection methods. This work investigates and compares different approaches for identifying deepfakes, focusing on the GenConViT model and its performance relative to other architectures present in the DeepfakeBenchmark. To contextualize the research, the social and legal impacts of deepfakes are addressed, as well as the technical fundamentals of their creation and detection, including digital image processing, machine learning, and artificial neural networks, with emphasis on Convolutional Neural Networks (CNNs), Generative Adversarial Networks (GANs), and Transformers. The performance evaluation of the models was conducted using relevant metrics and new datasets established in the literature, such as WildDeep-fake and DeepSpeak, aiming to identify the most effective tools in the battle against misinformation and media manipulation. The obtained results indicated that GenConViT, after fine-tuning, exhibited superior performance in terms of accuracy (93.82%) and generalization capacity, surpassing other architectures in the DeepfakeBenchmark on the DeepSpeak dataset. This study contributes to the advancement of deepfake detection techniques, offering contributions to the development of more robust and effective solutions against the dissemination of false information.\n        △ Less",
        "authors":"['Matheus Martins Batista']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
        "abstract":"Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media.\n        △ Less",
        "authors":"['Lele Cao']",
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    }
]