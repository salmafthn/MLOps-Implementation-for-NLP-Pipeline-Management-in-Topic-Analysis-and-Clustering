[
    {
        "title":"Adapting a World Model for Trajectory Following in a 3D Game",
        "abstract":"Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it. In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay. In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge. Additionally, we investigate several future alignment strategies that address the distribution shift caused by the aleatoric uncertainty and imperfections of the agent. We measure both the trajectory deviation distance and the first significant deviation point between the reference and the agent's trajectory and show that the optimal configuration depends on the chosen setting. Our results show that in a diverse data setting, a GPT-style policy head with an encoder trained from scratch performs the best, DINOv2 encoder with the GPT-style policy head gives the best results in the low data regime, and both GPT-style and MLP-style policy heads had comparable results when pre-trained on a diverse setting and fine-tuned for a specific behaviour setting.\n        △ Less",
        "authors":[
            "Marko Tot",
            "Shu Ishida",
            "Abdelhak Lemkhenter",
            "David Bignell",
            "Pallavi Choudhury",
            "Chris Lovett",
            "Luis França",
            "Matheus Ribeiro Furtado de Mendonça",
            "Tarun Gupta",
            "Darren Gehring",
            "Sam Devlin",
            "Sergio Valcarcel Macua",
            "Raluca Georgescu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians",
        "abstract":"Accurate, real-time 3D reconstruction of human heads from monocular images and videos underlies numerous visual applications. As 3D ground truth data is hard to come by at scale, previous methods have sought to learn from abundant 2D videos in a self-supervised manner. Typically, this involves the use of differentiable mesh rendering, which is effective but faces limitations. To improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a set of Gaussians that are rigged to this mesh. We then reanimate this rigged head avatar to match a target frame, and backpropagate photometric losses to both the 3DMM and Gaussian prediction networks. We find that using Gaussians for rendering substantially improves the effectiveness of this self-supervised approach. Training solely on 2D data, our method surpasses existing self-supervised approaches in geometric evaluations on the NoW benchmark for neutral faces and a new benchmark for non-neutral expressions. Our method also produces highly expressive meshes, outperforming state-of-the-art in emotion classification.\n        △ Less",
        "authors":[
            "Liam Schoneveld",
            "Zhe Chen",
            "Davide Davoli",
            "Jiapeng Tang",
            "Saimon Terazawa",
            "Ko Nishino",
            "Matthias Nießner"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BitNet b1.58 2B4T Technical Report",
        "abstract":"We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.\n        △ Less",
        "authors":[
            "Shuming Ma",
            "Hongyu Wang",
            "Shaohan Huang",
            "Xingxing Zhang",
            "Ying Hu",
            "Ting Song",
            "Yan Xia",
            "Furu Wei"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions",
        "abstract":"We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input. Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Predictor: a transformer-decoder module to predict the interaction trajectory from test time inputs by using an indexer module to retrieve a latent affordance from the learned codebook. To train our model, we develop a data engine that extracts 3D hand poses and contact trajectories from the diverse HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger than existing works, in terms of diversity of objects and interactions observed, and test for generalization of the model across object categories, action categories, tasks, and scenes. Experimental results show the effectiveness of our approach over transformer & diffusion baselines across all settings.\n        △ Less",
        "authors":[
            "Aditya Prakash",
            "Benjamin Lundell",
            "Dmitry Andreychuk",
            "David Forsyth",
            "Saurabh Gupta",
            "Harpreet Sawhney"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dysarthria Normalization via Local Lie Group Transformations for Robust ASR",
        "abstract":"We present a geometry-driven method for normalizing dysarthric speech using local Lie group transformations of spectrograms. Time, frequency, and amplitude distortions are modeled as smooth, invertible deformations, parameterized by scalar fields and applied via exponential maps. A neural network is trained to infer these fields from synthetic distortions of typical speech-without using any pathological data. At test time, the model applies an approximate inverse to real dysarthric inputs. Despite zero-shot generalization, we observe substantial ASR gains, including up to 16 percentage points WER reduction on challenging TORGO samples, with no degradation on clean speech. This work introduces a principled, interpretable approach for robust speech recognition under motor speech disorders\n        △ Less",
        "authors":[
            "Mikhail Osipov"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Edge Intelligence for Wildlife Conservation: Real-Time Hornbill Call Classification Using TinyML",
        "abstract":"Hornbills, an iconic species of Malaysia's biodiversity, face threats from habi-tat loss, poaching, and environmental changes, necessitating accurate and real-time population monitoring that is traditionally challenging and re-source intensive. The emergence of Tiny Machine Learning (TinyML) offers a chance to transform wildlife monitoring by enabling efficient, real-time da-ta analysis directly on edge devices. Addressing the challenge of wildlife conservation, this research paper explores the pivotal role of machine learn-ing, specifically TinyML, in the classification and monitoring of hornbill calls in Malaysia. Leveraging audio data from the Xeno-canto database, the study aims to develop a speech recognition system capable of identifying and classifying hornbill vocalizations. The proposed methodology involves pre-processing the audio data, extracting features using Mel-Frequency Energy (MFE), and deploying the model on an Arduino Nano 33 BLE, which is adept at edge computing. The research encompasses foundational work, in-cluding a comprehensive introduction, literature review, and methodology. The model is trained using Edge Impulse and validated through real-world tests, achieving high accuracy in hornbill species identification. The project underscores the potential of TinyML for environmental monitoring and its broader application in ecological conservation efforts, contributing to both the field of TinyML and wildlife conservation.\n        △ Less",
        "authors":[
            "Kong Ka Hing",
            "Mehran Behjati"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative analysis of unsupervised clustering techniques using validation metrics: Study on cognitive features from the Canadian Longitudinal Study on Aging (CLSA)",
        "abstract":"Purpose: The primary goal of this study is to explore the application of evaluation metrics to different clustering algorithms using the data provided from the Canadian Longitudinal Study (CLSA), focusing on cognitive features. The objective of our work is to discover potential clinically relevant clusters that contribute to the development of dementia over time-based on cognitive changes. Method: The CLSA dataset includes 18,891 participants with data available at both baseline and follow-up assessments, to which clustering algorithms were applied. The clustering methodologies employed in this analysis are K-means (KM) clustering, Hierarchical Clustering (HC) and Partitioning Around Medoids (PAM). We use multiple evaluation metrics to assess our analysis. For internal evaluation metrics, we use: Average silhouette Width, Within and Between the sum of square Ratio (WB.Ratio), Entropy, Calinski-Harabasz Index (CH Index), and Separation Index. For clustering comparison metrics, we used: Homogeneity, Completeness, Adjusted Rand Index (ARI), Rand Index (RI), and Variation Information. Results: Using evaluation metrics to compare the results of the three clustering techniques, K-means and Partitioning Around Medoids (PAM) produced similar results. In contrast, there are significant differences between K-means clustering and Hierarchical Clustering. Our study highlights the importance of the two internal evaluation metrics: entropy and separation index. In between clustering comparison metrics, the Adjusted Rand Index is a key tool. Conclusion: The study results have the potential to contribute to understanding dementia. Researchers can also benefit by applying the suggested evaluation metrics to other areas of healthcare research. Overall, our study improves the understanding of using clustering techniques and evaluation metrics to reveal complex patterns in medical data.\n        △ Less",
        "authors":[
            "ChenNingZhi Sheng",
            "Rafal Kustra",
            "Davide Chicco"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields",
        "abstract":"Spatiotemporal learning is challenging due to the intricate interplay between spatial and temporal dependencies, the high dimensionality of the data, and scalability constraints. These challenges are further amplified in scientific domains, where data is often irregularly distributed (e.g., missing values from sensor failures) and high-volume (e.g., high-fidelity simulations), posing additional computational and modeling difficulties. In this paper, we present SCENT, a novel framework for scalable and continuity-informed spatiotemporal representation learning. SCENT unifies interpolation, reconstruction, and forecasting within a single architecture. Built on a transformer-based encoder-processor-decoder backbone, SCENT introduces learnable queries to enhance generalization and a query-wise cross-attention mechanism to effectively capture multi-scale dependencies. To ensure scalability in both data size and model complexity, we incorporate a sparse attention mechanism, enabling flexible output representations and efficient evaluation at arbitrary resolutions. We validate SCENT through extensive simulations and real-world experiments, demonstrating state-of-the-art performance across multiple challenging tasks while achieving superior scalability.\n        △ Less",
        "authors":[
            "David Keetae Park",
            "Xihaier Luo",
            "Guang Zhao",
            "Seungjun Lee",
            "Miruna Oprescu",
            "Shinjae Yoo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FLIP Reasoning Challenge",
        "abstract":"Over the past years, advances in artificial intelligence (AI) have demonstrated how AI can solve many perception and generation tasks, such as image classification and text writing, yet reasoning remains a challenge. This paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning capabilities based on human verification tasks on the Idena blockchain. FLIP challenges present users with two orderings of 4 images, requiring them to identify the logically coherent one. By emphasizing sequential reasoning, visual storytelling, and common sense, FLIP provides a unique testbed for multimodal AI systems. Our experiments evaluate state-of-the-art models, leveraging both vision-language models (VLMs) and large language models (LLMs). Results reveal that even the best open-sourced and closed-sourced models achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot settings, compared to human performance of 95.3%. Captioning models aid reasoning models by providing text descriptions of images, yielding better results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5 Pro. Combining the predictions from 15 models in an ensemble increases the accuracy to 85.2%. These findings highlight the limitations of existing reasoning models and the need for robust multimodal benchmarks like FLIP. The full codebase and dataset will be available at https:\/\/github.com\/aplesner\/FLIP-Reasoning-Challenge.\n        △ Less",
        "authors":[
            "Andreas Plesner",
            "Turlan Kuzhagaliyev",
            "Roger Wattenhofer"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Human Aligned Compression for Robust Models",
        "abstract":"Adversarial attacks on image models threaten system robustness by introducing imperceptible perturbations that cause incorrect predictions. We investigate human-aligned learned lossy compression as a defense mechanism, comparing two learned models (HiFiC and ELIC) against traditional JPEG across various quality levels. Our experiments on ImageNet subsets demonstrate that learned compression methods outperform JPEG, particularly for Vision Transformer architectures, by preserving semantically meaningful content while removing adversarial noise. Even in white-box settings where attackers can access the defense, these methods maintain substantial effectiveness. We also show that sequential compression--applying rounds of compression\/decompression--significantly enhances defense efficacy while maintaining classification performance. Our findings reveal that human-aligned compression provides an effective, computationally efficient defense that protects the image features most relevant to human and machine understanding. It offers a practical approach to improving model robustness against adversarial threats.\n        △ Less",
        "authors":[
            "Samuel Räber",
            "Andreas Plesner",
            "Till Aczel",
            "Roger Wattenhofer"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning",
        "abstract":"Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach attains state-of-the-art (SOTA) performance, exceeding all previous efforts in the field of Arabic ASR on the standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.\n        △ Less",
        "authors":[
            "Mahmoud Salhab",
            "Marwan Elghitany",
            "Shameed Sait",
            "Syed Sibghat Ullah",
            "Mohammad Abusheikh",
            "Hasan Abusheikh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative Evaluation of Radiomics and Deep Learning Models for Disease Detection in Chest Radiography",
        "abstract":"The application of artificial intelligence (AI) in medical imaging has revolutionized diagnostic practices, enabling advanced analysis and interpretation of radiological data. This study presents a comprehensive evaluation of radiomics-based and deep learning-based approaches for disease detection in chest radiography, focusing on COVID-19, lung opacity, and viral pneumonia. While deep learning models, particularly convolutional neural networks (CNNs) and vision transformers (ViTs), learn directly from image data, radiomics-based models extract and analyze quantitative features, potentially providing advantages in data-limited scenarios. This study systematically compares the diagnostic accuracy and robustness of various AI models, including Decision Trees, Gradient Boosting, Random Forests, Support Vector Machines (SVM), and Multi-Layer Perceptrons (MLP) for radiomics, against state-of-the-art computer vision deep learning architectures. Performance metrics across varying sample sizes reveal insights into each model's efficacy, highlighting the contexts in which specific AI approaches may offer enhanced diagnostic capabilities. The results aim to inform the integration of AI-driven diagnostic tools in clinical practice, particularly in automated and high-throughput environments where timely, reliable diagnosis is critical. This comparative study addresses an essential gap, establishing guidance for the selection of AI models based on clinical and operational needs.\n        △ Less",
        "authors":[
            "Zhijin He",
            "Alan B. McMillan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Human-Centered Early Prediction Models for Academic Performance in Real-World Contexts",
        "abstract":"Supporting student success requires collaboration among multiple stakeholders. Researchers have explored machine learning models for academic performance prediction; yet key challenges remain in ensuring these models are interpretable, equitable, and actionable within real-world educational support systems. First, many models prioritize predictive accuracy but overlook human-centered considerations, limiting trust among students and reducing their usefulness for educators and institutional decision-makers. Second, most models require at least a month of data before making reliable predictions, delaying opportunities for early intervention. Third, current models primarily rely on sporadically collected, classroom-derived data, missing broader behavioral patterns that could provide more continuous and actionable insights. To address these gaps, we present three modeling approaches-LR, 1D-CNN, and MTL-1D-CNN-to classify students as low or high academic performers. We evaluate them based on explainability, fairness, and generalizability to assess their alignment with key social values. Using behavioral and self-reported data collected within the first week of two Spring terms, we demonstrate that these models can identify at-risk students as early as week one. However, trade-offs across human-centered considerations highlight the complexity of designing predictive models that effectively support multi-stakeholder decision-making and intervention strategies. We discuss these trade-offs and their implications for different stakeholders, outlining how predictive models can be integrated into student support systems. Finally, we examine broader socio-technical challenges in deploying these models and propose future directions for advancing human-centered, collaborative academic prediction systems.\n        △ Less",
        "authors":[
            "Han Zhang",
            "Yiyi Ren",
            "Paula S. Nurius",
            "Jennifer Mankoff",
            "Anind K. Dey"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Watermarking Needs Input Repetition Masking",
        "abstract":"Recent advancements in Large Language Models (LLMs) raised concerns over potential misuse, such as for spreading misinformation. In response two counter measures emerged: machine learning-based detectors that predict if text is synthetic, and LLM watermarking, which subtly marks generated text for identification and attribution. Meanwhile, humans are known to adjust language to their conversational partners both syntactically and lexically. By implication, it is possible that humans or unwatermarked LLMs could unintentionally mimic properties of LLM generated text, making counter measures unreliable. In this work we investigate the extent to which such conversational adaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that both humans and LLMs end up mimicking, including the watermarking signal even in seemingly improbable settings. This challenges current academic assumptions and suggests that for long-term watermarking to be reliable, the likelihood of false positives needs to be significantly lower, while longer word sequences should be used for seeding watermarking mechanisms.\n        △ Less",
        "authors":[
            "David Khachaturov",
            "Robert Mullins",
            "Ilia Shumailov",
            "Sumanth Dathathri"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"zkVC: Fast Zero-Knowledge Proof for Private and Verifiable Computing",
        "abstract":"In the context of cloud computing, services are held on cloud servers, where the clients send their data to the server and obtain the results returned by server. However, the computation, data and results are prone to tampering due to the vulnerabilities on the server side. Thus, verifying the integrity of computation is important in the client-server setting. The cryptographic method known as Zero-Knowledge Proof (ZKP) is renowned for facilitating private and verifiable computing. ZKP allows the client to validate that the results from the server are computed correctly without violating the privacy of the server's intellectual property. Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zkSNARKs), in particular, has been widely applied in various applications like blockchain and verifiable machine learning. Despite their popularity, existing zkSNARKs approaches remain highly computationally intensive. For instance, even basic operations like matrix multiplication require an extensive number of constraints, resulting in significant overhead. In addressing this challenge, we introduce \\textit{zkVC}, which optimizes the ZKP computation for matrix multiplication, enabling rapid proof generation on the server side and efficient verification on the client side. zkVC integrates optimized ZKP modules, such as Constraint-reduced Polynomial Circuit (CRPC) and Prefix-Sum Query (PSQ), collectively yielding a more than 12-fold increase in proof speed over prior methods. The code is available at https:\/\/github.com\/UCF-Lou-Lab-PET\/zkformer\n        △ Less",
        "authors":[
            "Yancheng Zhang",
            "Mengxin Zheng",
            "Xun Chen",
            "Jingtong Hu",
            "Weidong Shi",
            "Lei Ju",
            "Yan Solihin",
            "Qian Lou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning",
        "abstract":"Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.\n        △ Less",
        "authors":[
            "Siyan Zhao",
            "Devaansh Gupta",
            "Qinqing Zheng",
            "Aditya Grover"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks",
        "abstract":"Decentralized federated learning (DFL) is a promising machine learning paradigm for bringing artificial intelligence (AI) capabilities to the network edge. Running DFL on top of edge networks, however, faces severe performance challenges due to the extensive parameter exchanges between agents. Most existing solutions for these challenges were based on simplistic communication models, which cannot capture the case of learning over a multi-hop bandwidth-limited network. In this work, we address this problem by jointly designing the communication scheme for the overlay network formed by the agents and the mixing matrix that controls the communication demands between the agents. By carefully analyzing the properties of our problem, we cast each design problem into a tractable optimization and develop an efficient algorithm with guaranteed performance. Our evaluations based on real topology and data show that the proposed algorithm can reduce the total training time by over $80\\%$ compared to the baseline without sacrificing accuracy, while significantly improving the computational efficiency over the state of the art.\n        △ Less",
        "authors":[
            "Tingyang Sun",
            "Tuan Nguyen",
            "Ting He"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leave-One-Out Stable Conformal Prediction",
        "abstract":"Conformal prediction (CP) is an important tool for distribution-free predictive uncertainty quantification. Yet, a major challenge is to balance computational efficiency and prediction accuracy, particularly for multiple predictions. We propose Leave-One-Out Stable Conformal Prediction (LOO-StabCP), a novel method to speed up full conformal using algorithmic stability without sample splitting. By leveraging leave-one-out stability, our method is much faster in handling a large number of prediction requests compared to existing method RO-StabCP based on replace-one stability. We derived stability bounds for several popular machine learning tools: regularized loss minimization (RLM) and stochastic gradient descent (SGD), as well as kernel method, neural networks and bagging. Our method is theoretically justified and demonstrates superior numerical performance on synthetic and real-world data. We applied our method to a screening problem, where its effective exploitation of training data led to improved test power compared to state-of-the-art method based on split conformal.\n        △ Less",
        "authors":[
            "Kiljae Lee",
            "Yuan Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure",
        "abstract":"It is sometimes assumed that Large Language Models (LLMs) know language, or for example that they know that Paris is the capital of France. But what -- if anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior.\n        △ Less",
        "authors":[
            "Céline Budding"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CoMotion: Concurrent Multi-person 3D Motion",
        "abstract":"We introduce an approach for detecting and tracking detailed 3D poses of multiple people from a single monocular camera stream. Our system maintains temporally coherent predictions in crowded scenes filled with difficult poses and occlusions. Our model performs both strong per-frame detection and a learned pose update to track people from frame to frame. Rather than match detections across time, poses are updated directly from a new input image, which enables online tracking through occlusion. We train on numerous image and video datasets leveraging pseudo-labeled annotations to produce a model that matches state-of-the-art systems in 3D pose estimation accuracy while being faster and more accurate in tracking multiple people through time. Code and weights are provided at https:\/\/github.com\/apple\/ml-comotion\n        △ Less",
        "authors":[
            "Alejandro Newell",
            "Peiyun Hu",
            "Lahav Lipson",
            "Stephan R. Richter",
            "Vladlen Koltun"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning",
        "abstract":"Federated Learning (FL) has emerged as a promising framework for distributed learning, but its growing complexity has led to significant energy consumption, particularly from computations on the client side. This challenge is especially critical in energy-harvesting FL (EHFL) systems, where device availability fluctuates due to limited and time-varying energy resources. We propose FedBacys, a battery-aware FL framework that introduces cyclic client participation based on users' battery levels to cope with these issues. FedBacys enables clients to save energy and strategically perform local training just before their designated transmission time by clustering clients and scheduling their involvement sequentially. This design minimizes redundant computation, reduces system-wide energy usage, and improves learning stability. Our experiments demonstrate that FedBacys outperforms existing approaches in terms of energy efficiency and performance consistency, exhibiting robustness even under non-i.i.d. training data distributions and with very infrequent battery charging. This work presents the first comprehensive evaluation of cyclic client participation in EHFL, incorporating both communication and computation costs into a unified, resource-aware scheduling strategy.\n        △ Less",
        "authors":[
            "Eunjeong Jeong",
            "Nikolaos Pappas"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Approximation Bounds for Transformer Networks with Application to Regression",
        "abstract":"We explore the approximation capabilities of Transformer networks for Hölder and Sobolev functions, and apply these results to address nonparametric regression estimation with dependent observations. First, we establish novel upper bounds for standard Transformer networks approximating sequence-to-sequence mappings whose component functions are Hölder continuous with smoothness index $γ\\in (0,1]$. To achieve an approximation error $\\varepsilon$ under the $L^p$-norm for $p \\in [1, \\infty]$, it suffices to use a fixed-depth Transformer network whose total number of parameters scales as $\\varepsilon^{-d_x n \/ γ}$. This result not only extends existing findings to include the case $p = \\infty$, but also matches the best known upper bounds on number of parameters previously obtained for fixed-depth FNNs and RNNs. Similar bounds are also derived for Sobolev functions. Second, we derive explicit convergence rates for the nonparametric regression problem under various $β$-mixing data assumptions, which allow the dependence between observations to weaken over time. Our bounds on the sample complexity impose no constraints on weight magnitudes. Lastly, we propose a novel proof strategy to establish approximation bounds, inspired by the Kolmogorov-Arnold representation theorem. We show that if the self-attention layer in a Transformer can perform column averaging, the network can approximate sequence-to-sequence Hölder functions, offering new insights into the interpretability of self-attention mechanisms.\n        △ Less",
        "authors":[
            "Yuling Jiao",
            "Yanming Lai",
            "Defeng Sun",
            "Yang Wang",
            "Bokai Yan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning",
        "abstract":"Semantic 3D city models are worldwide easy-accessible, providing accurate, object-oriented, and semantic-rich 3D priors. To date, their potential to mitigate the noise impact on radar object detection remains under-explored. In this paper, we first introduce a unique dataset, RadarCity, comprising 54K synchronized radar-image pairs and semantic 3D city models. Moreover, we propose a novel neural network, RADLER, leveraging the effectiveness of contrastive self-supervised learning (SSL) and semantic 3D city models to enhance radar object detection of pedestrians, cyclists, and cars. Specifically, we first obtain the robust radar features via a SSL network in the radar-image pretext task. We then use a simple yet effective feature fusion strategy to incorporate semantic-depth features from semantic 3D city models. Having prior 3D information as guidance, RADLER obtains more fine-grained details to enhance radar object detection. We extensively evaluate RADLER on the collected RadarCity dataset and demonstrate average improvements of 5.46% in mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over previous radar object detection methods. We believe this work will foster further research on semantic-guided and map-supported radar object detection. Our project page is publicly available athttps:\/\/gpp-communication.github.io\/RADLER .\n        △ Less",
        "authors":[
            "Yuan Luo",
            "Rudolf Hoffmann",
            "Yan Xia",
            "Olaf Wysocki",
            "Benedikt Schwab",
            "Thomas H. Kolbe",
            "Daniel Cremers"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Material Network: Overview, applications and current directions",
        "abstract":"Deep Material Network (DMN) has emerged as a powerful framework for multiscale material modeling, enabling efficient and accurate predictions of material behavior across different length scales. Unlike traditional machine learning approaches, the trainable parameters in DMN have direct physical interpretations, capturing the geometric characteristics of the microstructure rather than serving as purely statistical fitting parameters. Its hierarchical tree structure effectively encodes microstructural interactions and deformation mechanisms, allowing DMN to achieve a balance between accuracy and computational efficiency. This physics-informed architecture significantly reduces computational costs compared to direct numerical simulations while preserving essential microstructural physics. Furthermore, DMN can be trained solely on a linear elastic dataset while effectively extrapolating nonlinear responses during online prediction, making it a highly efficient and scalable approach for multiscale material modeling. This article provides a comprehensive review of DMN, detailing its motivation, underlying methodology, and recent advancements. We discuss key modeling aspects, including its hierarchical structure, training process, and the role of physics-based constraints in enhancing predictive accuracy. Furthermore, we highlight its applications in component-scale multiscale analysis and inverse parameter identification, demonstrating its capability to bridge microscale material behavior with macroscale engineering predictions. Finally, we discuss challenges and future directions in improving DMN's generalization capabilities and its potential extensions for broader applications in multiscale modeling.\n        △ Less",
        "authors":[
            "Ting-Ju Wei",
            "Wen-Ning Wan",
            "Chuin-Shan Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications",
        "abstract":"In many applications, especially those involving prediction, models may yield near-optimal performance yet significantly disagree on individual-level outcomes. This phenomenon, known as predictive multiplicity, has been formally defined in binary, probabilistic, and multi-target classification, and undermines the reliability of predictive systems. However, its implications remain unexplored in the context of survival analysis, which involves estimating the time until a failure or similar event while properly handling censored data. We frame predictive multiplicity as a critical concern in survival-based models and introduce formal measures -- ambiguity, discrepancy, and obscurity -- to quantify it. This is particularly relevant for downstream tasks such as maintenance scheduling, where precise individual risk estimates are essential. Understanding and reporting predictive multiplicity helps build trust in models deployed in high-stakes environments. We apply our methodology to benchmark datasets from predictive maintenance, extending the notion of multiplicity to survival models. Our findings show that ambiguity steadily increases, reaching up to 40-45% of observations; discrepancy is lower but exhibits a similar trend; and obscurity remains mild and concentrated in a few models. These results demonstrate that multiple accurate survival models may yield conflicting estimations of failure risk and degradation progression for the same equipment. This highlights the need to explicitly measure and communicate predictive multiplicity to ensure reliable decision-making in process health management.\n        △ Less",
        "authors":[
            "Mustafa Cavus"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis",
        "abstract":"Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack of interpretability in the decision logic of multimodal fusion and modality imbalance caused by disparities in inter-modal information density. To address these issues, we propose KAN-MCP, a novel framework that integrates the interpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the Multimodal Clean Pareto (MCPareto) framework. First, KAN leverages its univariate function decomposition to achieve transparent analysis of cross-modal interactions. This structural design allows direct inspection of feature transformations without relying on external interpretation tools, thereby ensuring both high expressiveness and interpretability. Second, the proposed MCPareto enhances robustness by addressing modality imbalance and noise interference. Specifically, we introduce the Dimensionality Reduction and Denoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises and reduces feature dimensionality. This approach provides KAN with discriminative low-dimensional inputs to reduce the modeling complexity of KAN while preserving critical sentiment-related information. Furthermore, MCPareto dynamically balances gradient contributions across modalities using the purified features output by DRD-MIB, ensuring lossless transmission of auxiliary signals and effectively alleviating modality imbalance. This synergy of interpretability and robustness not only achieves superior performance on benchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers an intuitive visualization interface through KAN's interpretable architecture.\n        △ Less",
        "authors":[
            "Miaosen Luo",
            "Yuncheng Jiang",
            "Sijie Mai"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -",
        "abstract":"Despite recent advances in Large Vision Language Models (LVLMs), these models still suffer from generating hallucinatory responses that do not align with the visual input provided. To mitigate such hallucinations, we introduce Efficient Contrastive Decoding (ECD), a simple method that leverages probabilistic hallucination detection to shift the output distribution towards contextually accurate answers at inference time. By contrasting token probabilities and hallucination scores, ECD subtracts hallucinated concepts from the original distribution, effectively suppressing hallucinations. Notably, our proposed method can be applied to any open-source LVLM and does not require additional LVLM training. We evaluate our method on several benchmark datasets and across different LVLMs. Our experiments show that ECD effectively mitigates hallucinations, outperforming state-of-the-art methods with respect to performance on LVLM benchmarks and computation time.\n        △ Less",
        "authors":[
            "Laura Fieback",
            "Nishilkumar Balar",
            "Jakob Spiegelberg",
            "Hanno Gottschalk"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals",
        "abstract":"Detection of spatial areas where biodiversity is at risk is of paramount importance for the conservation and monitoring of ecosystems. Large terrestrial mammalian herbivores are keystone species as their activity not only has deep effects on soils, plants, and animals, but also shapes landscapes, as large herbivores act as allogenic ecosystem engineers. One key landscape feature that indicates intense herbivore activity and potentially impacts biodiversity is the formation of grazing trails. Grazing trails are formed by the continuous trampling activity of large herbivores that can produce complex networks of tracks of bare soil. Here, we evaluated different algorithms based on machine learning techniques to identify grazing trails. Our goal is to automatically detect potential areas with intense herbivory activity, which might be beneficial for conservation and management plans.\n  We have applied five semantic segmentation methods combined with fourteen encoders aimed at mapping grazing trails on aerial images. Our results indicate that in most cases the chosen methodology successfully mapped the trails, although there were a few instances where the actual trail structure was underestimated. The UNet architecture with the MambaOut encoder was the best architecture for mapping trails. The proposed approach could be applied to develop tools for mapping and monitoring temporal changes in these landscape structures to support habitat conservation and land management programs. This is the first time, to the best of our knowledge, that competitive image segmentation results are obtained for the detection and delineation of trails of large herbivorous mammals.\n        △ Less",
        "authors":[
            "Jose Francisco Diez-Pastor",
            "Francisco Javier Gonzalez-Moya",
            "Pedro Latorre-Carmona",
            "Francisco Javier Perez-Barbería",
            "Ludmila I. Kuncheva",
            "Antonio Canepa-Oneto",
            "Alvar Arnaiz-González",
            "Cesar Garcia-Osorio"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AttentionDrop: A Novel Regularization Method for Transformer Models",
        "abstract":"Transformer-based architectures achieve state-of-the-art performance across a wide range of tasks in natural language processing, computer vision, and speech. However, their immense capacity often leads to overfitting, especially when training data is limited or noisy. We propose AttentionDrop, a unified family of stochastic regularization techniques that operate directly on the self-attention distributions. We introduces three variants: 1. Hard Attention Masking: randomly zeroes out top-k attention logits per query to encourage diverse context utilization. 2. Blurred Attention Smoothing: applies a dynamic Gaussian convolution over attention logits to diffuse overly peaked distributions. 3. Consistency-Regularized AttentionDrop: enforces output stability under multiple independent AttentionDrop perturbations via a KL-based consistency loss.\n        △ Less",
        "authors":[
            "Mirza Samad Ahmed Baig",
            "Syeda Anshrah Gillani",
            "Abdul Akbar Khan",
            "Shahid Munir Shah"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Contextual Bandits Under Delayed Feedback Constraints",
        "abstract":"This paper presents a new algorithm for neural contextual bandits (CBs) that addresses the challenge of delayed reward feedback, where the reward for a chosen action is revealed after a random, unknown delay. This scenario is common in applications such as online recommendation systems and clinical trials, where reward feedback is delayed because the outcomes or results of a user's actions (such as recommendations or treatment responses) take time to manifest and be measured. The proposed algorithm, called Delayed NeuralUCB, uses an upper confidence bound (UCB)-based exploration strategy. Under the assumption of independent and identically distributed sub-exponential reward delays, we derive an upper bound on the cumulative regret over a T-length horizon. We further consider a variant of the algorithm, called Delayed NeuralTS, that uses Thompson Sampling-based exploration. Numerical experiments on real-world datasets, such as MNIST and Mushroom, along with comparisons to benchmark approaches, demonstrate that the proposed algorithms effectively manage varying delays and are well-suited for complex real-world scenarios.\n        △ Less",
        "authors":[
            "Mohammadali Moghimi",
            "Sharu Theresa Jose",
            "Shana Moothedath"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generative Deep Learning Framework for Inverse Design of Fuels",
        "abstract":"In the present work, a generative deep learning framework combining a Co-optimized Variational Autoencoder (Co-VAE) architecture with quantitative structure-property relationship (QSPR) techniques is developed to enable accelerated inverse design of fuels. The Co-VAE integrates a property prediction component coupled with the VAE latent space, enhancing molecular reconstruction and accurate estimation of Research Octane Number (RON) (chosen as the fuel property of interest). A subset of the GDB-13 database, enriched with a curated RON database, is used for model training. Hyperparameter tuning is further utilized to optimize the balance among reconstruction fidelity, chemical validity, and RON prediction. An independent regression model is then used to refine RON prediction, while a differential evolution algorithm is employed to efficiently navigate the VAE latent space and identify promising fuel molecule candidates with high RON. This methodology addresses the limitations of traditional fuel screening approaches by capturing complex structure-property relationships within a comprehensive latent representation. The generative model provides a flexible tool for systematically exploring vast chemical spaces, paving the way for discovering fuels with superior anti-knock properties. The demonstrated approach can be readily extended to incorporate additional fuel properties and synthesizability criteria to enhance applicability and reliability for de novo design of new fuels.\n        △ Less",
        "authors":[
            "Kiran K. Yalamanchi",
            "Pinaki Pal",
            "Balaji Mohan",
            "Abdullah S. AlRamadan",
            "Jihad A. Badra",
            "Yuanjiang Pei"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimizing Compound Retrieval Systems",
        "abstract":"Modern retrieval systems do not rely on a single ranking model to construct their rankings. Instead, they generally take a cascading approach where a sequence of ranking models are applied in multiple re-ranking stages. Thereby, they balance the quality of the top-K ranking with computational costs by limiting the number of documents each model re-ranks. However, the cascading approach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of retrieval systems that apply multiple prediction models. This encapsulates cascading models but also allows other types of interactions than top-K re-ranking. In particular, we enable interactions with large language models (LLMs) which can provide relative relevance comparisons. We focus on the optimization of compound retrieval system design which uniquely involves learning where to apply the component models and how to aggregate their predictions into a final ranking. This work shows how our compound approach can combine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM relevance predictions, while optimizing a given ranking metric and efficiency target. Our experimental results show optimized compound retrieval systems provide better trade-offs between effectiveness and efficiency than cascading approaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the information retrieval field to more out-of-the-box thinking on how prediction models can interact to form rankings.\n        △ Less",
        "authors":[
            "Harrie Oosterhuis",
            "Rolf Jagerman",
            "Zhen Qin",
            "Xuanhui Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Static to Dynamic Correlation Clustering",
        "abstract":"Correlation clustering is a well-studied problem, first proposed by Bansal, Blum, and Chawla [BBC04]. The input is an unweighted, undirected graph. The problem is to cluster the vertices so as to minimizing the number of edges between vertices in different clusters and missing edges between vertices inside the same cluster. This problem has a wide application in data mining and machine learning. We introduce a general framework that transforms existing static correlation clustering algorithms into fully-dynamic ones that work against an adaptive adversary.\n  We show how to apply our framework to known efficient correlation clustering algorithms, starting from the classic 3-approximate Pivot algorithm from [ACN08]. Applied to the most recent near-linear 1.437-approximation algorithm from [CCL+25], we get a 1.437-approximation fully-dynamic algorithm that works with worst-case constant update time. The original static algorithm gets its approximation factor with constant probability, and we get the same against an adaptive adversary in the sense that for any given update step not known to our algorithm, our solution is a 1.437-approximation with constant probability when we reach this update.\n  Previous dynamic algorithms had approximation factors around 3 in expectation, and they could only handle an oblivious adversary.\n        △ Less",
        "authors":[
            "Nairen Cao",
            "Vincent Cohen-Addad",
            "Euiwoong Lee",
            "Shi Li",
            "David Rasmussen Lolck",
            "Alantha Newman",
            "Mikkel Thorup",
            "Lukas Vogl",
            "Shuyi Yan",
            "Hanwen Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the calibration of Just-in-time Defect Prediction",
        "abstract":"Just in time defect prediction (JIT DP) leverages ML to identify defect-prone code commits, enabling quality assurance (QA) teams to allocate resources more efficiently by focusing on commits that are most likely to contain defects. Although JIT DP techniques have introduced improvements in terms of predictive accuracy, they are still susceptible to misclassification errors such as false positives and negatives. This can lead to wasted resources or undetected defects, a particularly critical concern when QA resources are limited. To mitigate these challenges and preserve the practical utility of JIT DP tools, it becomes essential to estimate the reliability of the predictions, i.e., computing confidence scores. Such scores can help practitioners determine the trustworthiness of predictions and thus prioritize them efficiently. A simple approach to computing confidence scores is to extract, alongside each prediction, the corresponding prediction probabilities and use them as indicators of confidence. However, for these probabilities to reliably serve as confidence scores, the predictive model must be well-calibrated. This means that the prediction probabilities must accurately represent the true likelihood of each prediction being correct. Miscalibration, common in modern ML models, distorts probability scores such that they do not align with the actual correctness probability. In this study, we evaluate the calibration of three JIT DP techniques to determine whether and to what extent they exhibit poor calibration. Furthermore, we assess whether post-calibration methods can improve the calibration of existing JIT defect prediction models. Our results reveal that all evaluated JIT DP models exhibit some level of miscalibration, with ECE ranging from 2-35%. Furthermore, post-calibration methods do not consistently improve the calibration.\n        △ Less",
        "authors":[
            "Xhulja Shahini",
            "Jone Bartel",
            "Klaus Pohl"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"pix2pockets: Shot Suggestions in 8-Ball Pool from a Single Image in the Wild",
        "abstract":"Computer vision models have seen increased usage in sports, and reinforcement learning (RL) is famous for beating humans in strategic games such as Chess and Go. In this paper, we are interested in building upon these advances and examining the game of classic 8-ball pool. We introduce pix2pockets, a foundation for an RL-assisted pool coach. Given a single image of a pool table, we first aim to detect the table and the balls and then propose the optimal shot suggestion. For the first task, we build a dataset with 195 diverse images where we manually annotate all balls and table dots, leading to 5748 object segmentation masks. For the second task, we build a standardized RL environment that allows easy development and benchmarking of any RL algorithm. Our object detection model yields an AP50 of 91.2 while our ball location pipeline obtains an error of only 0.4 cm. Furthermore, we compare standard RL algorithms to set a baseline for the shot suggestion task and we show that all of them fail to pocket all balls without making a foul move. We also present a simple baseline that achieves a per-shot success rate of 94.7% and clears a full game in a single turn 30% of the time.\n        △ Less",
        "authors":[
            "Jonas Myhre Schiøtt",
            "Viktor Sebastian Petersen",
            "Dimitrios P. Papadopoulos"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RadMamba: Efficient Human Activity Recognition through Radar-based Micro-Doppler-Oriented Mamba State-Space Model",
        "abstract":"Radar-based HAR has emerged as a promising alternative to conventional monitoring approaches, such as wearable devices and camera-based systems, due to its unique privacy preservation and robustness advantages. However, existing solutions based on convolutional and recurrent neural networks, although effective, are computationally demanding during deployment. This limits their applicability in scenarios with constrained resources or those requiring multiple sensors. Advanced architectures, such as ViT and SSM architectures, offer improved modeling capabilities and have made efforts toward lightweight designs. However, their computational complexity remains relatively high. To leverage the strengths of transformer architectures while simultaneously enhancing accuracy and reducing computational complexity, this paper introduces RadMamba, a parameter-efficient, radar micro-Doppler-oriented Mamba SSM specifically tailored for radar-based HAR. Across three diverse datasets, RadMamba matches the top-performing previous model's 99.8% classification accuracy on Dataset DIAT with only 1\/400 of its parameters and equals the leading models' 92.0% accuracy on Dataset CI4R with merely 1\/10 of their parameters. In scenarios with continuous sequences of actions evaluated on Dataset UoG2020, RadMamba surpasses other models with significantly higher parameter counts by at least 3%, achieving this with only 6.7k parameters. Our code is available at: https:\/\/github.com\/lab-emi\/AIRHAR.\n        △ Less",
        "authors":[
            "Yizhuo Wu",
            "Francesco Fioranelli",
            "Chang Gao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FedEPA: Enhancing Personalization and Modality Alignment in Multimodal Federated Learning",
        "abstract":"Federated Learning (FL) enables decentralized model training across multiple parties while preserving privacy. However, most FL systems assume clients hold only unimodal data, limiting their real-world applicability, as institutions often possess multimodal data. Moreover, the lack of labeled data further constrains the performance of most FL methods. In this work, we propose FedEPA, a novel FL framework for multimodal learning. FedEPA employs a personalized local model aggregation strategy that leverages labeled data on clients to learn personalized aggregation weights, thereby alleviating the impact of data heterogeneity. We also propose an unsupervised modality alignment strategy that works effectively with limited labeled data. Specifically, we decompose multimodal features into aligned features and context features. We then employ contrastive learning to align the aligned features across modalities, ensure the independence between aligned features and context features within each modality, and promote the diversity of context features. A multimodal feature fusion strategy is introduced to obtain a joint embedding. The experimental results show that FedEPA significantly outperforms existing FL methods in multimodal classification tasks under limited labeled data conditions.\n        △ Less",
        "authors":[
            "Yu Zhang",
            "Qingfeng Du",
            "Jiaqi Lv"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Active Human Feedback Collection via Neural Contextual Dueling Bandits",
        "abstract":"Collecting human preference feedback is often expensive, leading recent works to develop principled algorithms to select them more efficiently. However, these works assume that the underlying reward function is linear, an assumption that does not hold in many real-life applications, such as online recommendation and LLM alignment. To address this limitation, we propose Neural-ADB, an algorithm based on the neural contextual dueling bandit framework that provides a principled and practical method for collecting human preference feedback when the underlying latent reward function is non-linear. We theoretically show that when preference feedback follows the Bradley-Terry-Luce model, the worst sub-optimality gap of the policy learned by Neural-ADB decreases at a sub-linear rate as the preference dataset increases. Our experimental results on problem instances derived from synthetic preference datasets further validate the effectiveness of Neural-ADB.\n        △ Less",
        "authors":[
            "Arun Verma",
            "Xiaoqiang Lin",
            "Zhongxiang Dai",
            "Daniela Rus",
            "Bryan Kian Hsiang Low"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Balancing Graph Embedding Smoothness in Self-Supervised Learning via Information-Theoretic Decomposition",
        "abstract":"Self-supervised learning (SSL) in graphs has garnered significant attention, particularly in employing Graph Neural Networks (GNNs) with pretext tasks initially designed for other domains, such as contrastive learning and feature reconstruction. However, it remains uncertain whether these methods effectively reflect essential graph properties, precisely representation similarity with its neighbors. We observe that existing methods position opposite ends of a spectrum driven by the graph embedding smoothness, with each end corresponding to outperformance on specific downstream tasks. Decomposing the SSL objective into three terms via an information-theoretic framework with a neighbor representation variable reveals that this polarization stems from an imbalance among the terms, which existing methods may not effectively maintain. Further insights suggest that balancing between the extremes can lead to improved performance across a wider range of downstream tasks. A framework, BSG (Balancing Smoothness in Graph SSL), introduces novel loss functions designed to supplement the representation quality in graph-based SSL by balancing the derived three terms: neighbor loss, minimal loss, and divergence loss. We present a theoretical analysis of the effects of these loss functions, highlighting their significance from both the SSL and graph smoothness perspectives. Extensive experiments on multiple real-world datasets across node classification and link prediction consistently demonstrate that BSG achieves state-of-the-art performance, outperforming existing methods. Our implementation code is available at https:\/\/github.com\/steve30572\/BSG.\n        △ Less",
        "authors":[
            "Heesoo Jung",
            "Hogun Park"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder",
        "abstract":"Voice conversion is a task of synthesizing an utterance with target speaker's voice while maintaining linguistic information of the source utterance. While a speaker can produce varying utterances from a single script with different intonations, conventional voice conversion models were limited to producing only one result per source input. To overcome this limitation, we propose a novel approach for voice conversion with diverse intonations using conditional variational autoencoder (CVAE). Experiments have shown that the speaker's style feature can be mapped into a latent space with Gaussian distribution. We have also been able to convert voices with more diverse intonation by making the posterior of the latent space more complex with inverse autoregressive flow (IAF). As a result, the converted voice not only has a diversity of intonations, but also has better sound quality than the model without CVAE.\n        △ Less",
        "authors":[
            "Soobin Suh",
            "Dabi Ahn",
            "Heewoong Park",
            "Jonghun Park"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Control of Rayleigh-Bénard Convection: Effectiveness of Reinforcement Learning in the Turbulent Regime",
        "abstract":"Data-driven flow control has significant potential for industry, energy systems, and climate science. In this work, we study the effectiveness of Reinforcement Learning (RL) for reducing convective heat transfer in the 2D Rayleigh-Bénard Convection (RBC) system under increasing turbulence. We investigate the generalizability of control across varying initial conditions and turbulence levels and introduce a reward shaping technique to accelerate the training. RL agents trained via single-agent Proximal Policy Optimization (PPO) are compared to linear proportional derivative (PD) controllers from classical control theory. The RL agents reduced convection, measured by the Nusselt Number, by up to 33% in moderately turbulent systems and 10% in highly turbulent settings, clearly outperforming PD control in all settings. The agents showed strong generalization performance across different initial conditions and to a significant extent, generalized to higher degrees of turbulence. The reward shaping improved sample efficiency and consistently stabilized the Nusselt Number to higher turbulence levels.\n        △ Less",
        "authors":[
            "Thorben Markmann",
            "Michiel Straat",
            "Sebastian Peitz",
            "Barbara Hammer"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Computationally Efficient Algorithm for Infinite-Horizon Average-Reward Linear MDPs",
        "abstract":"We study reinforcement learning in infinite-horizon average-reward settings with linear MDPs. Previous work addresses this problem by approximating the average-reward setting by discounted setting and employing a value iteration-based algorithm that uses clipping to constrain the span of the value function for improved statistical efficiency. However, the clipping procedure requires computing the minimum of the value function over the entire state space, which is prohibitive since the state space in linear MDP setting can be large or even infinite. In this paper, we introduce a value iteration method with efficient clipping operation that only requires computing the minimum of value functions over the set of states visited by the algorithm. Our algorithm enjoys the same regret bound as the previous work while being computationally efficient, with computational complexity that is independent of the size of the state space.\n        △ Less",
        "authors":[
            "Kihyuk Hong",
            "Ambuj Tewari"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation",
        "abstract":"A domain (distribution) shift between training and test data often hinders the real-world performance of deep neural networks, necessitating unsupervised domain adaptation (UDA) to bridge this gap. Online source-free UDA has emerged as a solution for practical scenarios where access to source data is restricted and target data is received as a continuous stream. However, the open-world nature of many real-world applications additionally introduces category shifts meaning that the source and target label spaces may differ. Online source-free universal domain adaptation (SF-UniDA) addresses this challenge. Existing methods mainly rely on self-training with pseudo-labels, yet the relationship between pseudo-labeling and adaptation outcomes has not been studied yet. To bridge this gap, we conduct a systematic analysis through controlled experiments with simulated pseudo-labeling, offering valuable insights into pseudo-labeling for online SF-UniDA. Our findings reveal a substantial gap between the current state-of-the-art and the upper bound of adaptation achieved with perfect pseudo-labeling. Moreover, we show that a contrastive loss enables effective adaptation even with moderate pseudo-label accuracy, while a cross-entropy loss, though less robust to pseudo-label errors, achieves superior results when pseudo-labeling approaches perfection. Lastly, our findings indicate that pseudo-label accuracy is in general more crucial than quantity, suggesting that prioritizing fewer but high-confidence pseudo-labels is beneficial. Overall, our study highlights the critical role of pseudo-labeling in (online) SF-UniDA and provides actionable insights to drive future advancements in the field. Our code is available at https:\/\/github.com\/pascalschlachter\/PLAnalysis.\n        △ Less",
        "authors":[
            "Pascal Schlachter",
            "Jonathan Fuss",
            "Bin Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Secure Transfer Learning: Training Clean Models Against Backdoor in (Both) Pre-trained Encoders and Downstream Datasets",
        "abstract":"Transfer learning from pre-trained encoders has become essential in modern machine learning, enabling efficient model adaptation across diverse tasks. However, this combination of pre-training and downstream adaptation creates an expanded attack surface, exposing models to sophisticated backdoor embeddings at both the encoder and dataset levels--an area often overlooked in prior research. Additionally, the limited computational resources typically available to users of pre-trained encoders constrain the effectiveness of generic backdoor defenses compared to end-to-end training from scratch. In this work, we investigate how to mitigate potential backdoor risks in resource-constrained transfer learning scenarios. Specifically, we conduct an exhaustive analysis of existing defense strategies, revealing that many follow a reactive workflow based on assumptions that do not scale to unknown threats, novel attack types, or different training paradigms. In response, we introduce a proactive mindset focused on identifying clean elements and propose the Trusted Core (T-Core) Bootstrapping framework, which emphasizes the importance of pinpointing trustworthy data and neurons to enhance model security. Our empirical evaluations demonstrate the effectiveness and superiority of T-Core, specifically assessing 5 encoder poisoning attacks, 7 dataset poisoning attacks, and 14 baseline defenses across five benchmark datasets, addressing four scenarios of 3 potential backdoor threats.\n        △ Less",
        "authors":[
            "Yechao Zhang",
            "Yuxuan Zhou",
            "Tianyu Li",
            "Minghui Li",
            "Shengshan Hu",
            "Wei Luo",
            "Leo Yu Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Evolution of Zero Trust Architecture (ZTA) from Concept to Implementation",
        "abstract":"Zero Trust Architecture (ZTA) is one of the paradigm changes in cybersecurity, from the traditional perimeter-based model to perimeterless. This article studies the core concepts of ZTA, its beginning, a few use cases and future trends. Emphasising the always verify and least privilege access, some key tenets of ZTA have grown to be integration technologies like Identity Management, Multi-Factor Authentication (MFA) and real-time analytics. ZTA is expected to strengthen cloud environments, education, work environments (including from home) while controlling other risks like lateral movement and insider threats. Despite ZTA's benefits, it comes with challenges in the form of complexity, performance overhead and vulnerabilities in the control plane. These require phased implementation and continuous refinement to keep up with evolving organisational needs and threat landscapes. Emerging technologies, such as Artificial Intelligence (AI) and Machine Learning (ML) will further automate policy enforcement and threat detection in keeping up with dynamic cyber threats.\n        △ Less",
        "authors":[
            "Md Nasiruzzaman",
            "Maaruf Ali",
            "Iftekhar Salam",
            "Mahdi H. Miraz"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient identification of linear, parameter-varying, and nonlinear systems with noise models",
        "abstract":"We present a general system identification procedure capable of estimating of a broad spectrum of state-space dynamical models, including linear time-invariant (LTI), linear parameter-varying} (LPV), and nonlinear (NL) dynamics, along with rather general classes of noise models. Similar to the LTI case, we show that for this general class of model structures, including the NL case, the model dynamics can be separated into a deterministic process and a stochastic noise part, allowing to seamlessly tune the complexity of the combined model both in terms of nonlinearity and noise modeling. We parameterize the involved nonlinear functional relations by means of artificial neural-networks (ANNs), although alternative parametric nonlinear mappings can also be used. To estimate the resulting model structures, we optimize a prediction-error-based criterion using an efficient combination of a constrained quasi-Newton approach and automatic differentiation, achieving training times in the order of seconds compared to existing state-of-the-art ANN methods which may require hours for models of similar complexity. We formally establish the consistency guarantees for the proposed approach and demonstrate its superior estimation accuracy and computational efficiency on several benchmark LTI, LPV, and NL system identification problems.\n        △ Less",
        "authors":[
            "Alberto Bemporad",
            "Roland Tóth"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hardware-Friendly Delayed-Feedback Reservoir for Multivariate Time-Series Classification",
        "abstract":"Reservoir computing (RC) is attracting attention as a machine-learning technique for edge computing. In time-series classification tasks, the number of features obtained using a reservoir depends on the length of the input series. Therefore, the features must be converted to a constant-length intermediate representation (IR), such that they can be processed by an output layer. Existing conversion methods involve computationally expensive matrix inversion that significantly increases the circuit size and requires processing power when implemented in hardware. In this article, we propose a simple but effective IR, namely, dot-product-based reservoir representation (DPRR), for RC based on the dot product of data features. Additionally, we propose a hardware-friendly delayed-feedback reservoir (DFR) consisting of a nonlinear element and delayed feedback loop with DPRR. The proposed DFR successfully classified multivariate time series data that has been considered particularly difficult to implement efficiently in hardware. In contrast to conventional DFR models that require analog circuits, the proposed model can be implemented in a fully digital manner suitable for high-level syntheses. A comparison with existing machine-learning methods via field-programmable gate array implementation using 12 multivariate time-series classification tasks confirmed the superior accuracy and small circuit size of the proposed method.\n        △ Less",
        "authors":[
            "Sosei Ikeda",
            "Hiromitsu Awano",
            "Takashi Sato"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Machine Learning Models to Predict the Outcome of Digital Medical Triage Interviews",
        "abstract":"Many existing digital triage systems are questionnaire-based, guiding patients to appropriate care levels based on information (e.g., symptoms, medical history, and urgency) provided by the patients answering questionnaires. Such a system often uses a deterministic model with predefined rules to determine care levels. It faces challenges with incomplete triage interviews since it can only assist patients who finish the process. In this study, we explore the use of machine learning (ML) to predict outcomes of unfinished interviews, aiming to enhance patient care and service quality. Predicting triage outcomes from incomplete data is crucial for patient safety and healthcare efficiency. Our findings show that decision-tree models, particularly LGBMClassifier and CatBoostClassifier, achieve over 80\\% accuracy in predicting outcomes from complete interviews while having a linear correlation between the prediction accuracy and interview completeness degree. For example, LGBMClassifier achieves 88,2\\% prediction accuracy for interviews with 100\\% completeness, 79,6\\% accuracy for interviews with 80\\% completeness, 58,9\\% accuracy for 60\\% completeness, and 45,7\\% accuracy for 40\\% completeness. The TabTransformer model demonstrated exceptional accuracy of over 80\\% for all degrees of completeness but required extensive training time, indicating a need for more powerful computational resources. The study highlights the linear correlation between interview completeness and predictive power of the decision-tree models.\n        △ Less",
        "authors":[
            "Sofia Krylova",
            "Fabian Schmidt",
            "Vladimir Vlassov"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring Video-Based Driver Activity Recognition under Noisy Labels",
        "abstract":"As an open research topic in the field of deep learning, learning with noisy labels has attracted much attention and grown rapidly over the past ten years. Learning with label noise is crucial for driver distraction behavior recognition, as real-world video data often contains mislabeled samples, impacting model reliability and performance. However, label noise learning is barely explored in the driver activity recognition field. In this paper, we propose the first label noise learning approach for the driver activity recognition task. Based on the cluster assumption, we initially enable the model to learn clustering-friendly low-dimensional representations from given videos and assign the resultant embeddings into clusters. We subsequently perform co-refinement within each cluster to smooth the classifier outputs. Furthermore, we propose a flexible sample selection strategy that combines two selection criteria without relying on any hyperparameters to filter clean samples from the training dataset. We also incorporate a self-adaptive parameter into the sample selection process to enforce balancing across classes. A comprehensive variety of experiments on the public Drive&Act dataset for all granularity levels demonstrates the superior performance of our method in comparison with other label-denoising methods derived from the image classification field. The source code is available at https:\/\/github.com\/ilonafan\/DAR-noisy-labels.\n        △ Less",
        "authors":[
            "Linjuan Fan",
            "Di Wen",
            "Kunyu Peng",
            "Kailun Yang",
            "Jiaming Zhang",
            "Ruiping Liu",
            "Yufan Chen",
            "Junwei Zheng",
            "Jiamin Wu",
            "Xudong Han",
            "Rainer Stiefelhagen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust and Fine-Grained Detection of AI Generated Texts",
        "abstract":"An ideal detection system for machine generated content is supposed to work well on any generator as many more advanced LLMs come into existence day by day. Existing systems often struggle with accurately identifying AI-generated content over shorter texts. Further, not all texts might be entirely authored by a human or LLM, hence we focused more over partial cases i.e human-LLM co-authored texts. Our paper introduces a set of models built for the task of token classification which are trained on an extensive collection of human-machine co-authored texts, which performed well over texts of unseen domains, unseen generators, texts by non-native speakers and those with adversarial inputs. We also introduce a new dataset of over 2.4M such texts mostly co-authored by several popular proprietary LLMs over 23 languages. We also present findings of our models' performance over each texts of each domain and generator. Additional findings include comparison of performance against each adversarial method, length of input texts and characteristics of generated texts compared to the original human authored texts.\n        △ Less",
        "authors":[
            "Ram Mohan Rao Kadiyala",
            "Siddartha Pullakhandam",
            "Kanwal Mehreen",
            "Drishti Sharma",
            "Siddhant Gupta",
            "Jebish Purbey",
            "Ashay Srivastava",
            "Subhasya TippaReddy",
            "Arvind Reddy Bobbili",
            "Suraj Telugara Chandrashekhar",
            "Modabbir Adeeb",
            "Srinadh Vura",
            "Hamza Farooq"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"VIPO: Value Function Inconsistency Penalized Offline Reinforcement Learning",
        "abstract":"Offline reinforcement learning (RL) learns effective policies from pre-collected datasets, offering a practical solution for applications where online interactions are risky or costly. Model-based approaches are particularly advantageous for offline RL, owing to their data efficiency and generalizability. However, due to inherent model errors, model-based methods often artificially introduce conservatism guided by heuristic uncertainty estimation, which can be unreliable. In this paper, we introduce VIPO, a novel model-based offline RL algorithm that incorporates self-supervised feedback from value estimation to enhance model training. Specifically, the model is learned by additionally minimizing the inconsistency between the value learned directly from the offline data and the one estimated from the model. We perform comprehensive evaluations from multiple perspectives to show that VIPO can learn a highly accurate model efficiently and consistently outperform existing methods. It offers a general framework that can be readily integrated into existing model-based offline RL algorithms to systematically enhance model accuracy. As a result, VIPO achieves state-of-the-art performance on almost all tasks in both D4RL and NeoRL benchmarks.\n        △ Less",
        "authors":[
            "Xuyang Chen",
            "Guojian Wang",
            "Keyu Yan",
            "Lin Zhao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SemDiff: Generating Natural Unrestricted Adversarial Examples via Semantic Attributes Optimization in Diffusion Models",
        "abstract":"Unrestricted adversarial examples (UAEs), allow the attacker to create non-constrained adversarial examples without given clean samples, posing a severe threat to the safety of deep learning models. Recent works utilize diffusion models to generate UAEs. However, these UAEs often lack naturalness and imperceptibility due to simply optimizing in intermediate latent noises. In light of this, we propose SemDiff, a novel unrestricted adversarial attack that explores the semantic latent space of diffusion models for meaningful attributes, and devises a multi-attributes optimization approach to ensure attack success while maintaining the naturalness and imperceptibility of generated UAEs. We perform extensive experiments on four tasks on three high-resolution datasets, including CelebA-HQ, AFHQ and ImageNet. The results demonstrate that SemDiff outperforms state-of-the-art methods in terms of attack success rate and imperceptibility. The generated UAEs are natural and exhibit semantically meaningful changes, in accord with the attributes' weights. In addition, SemDiff is found capable of evading different defenses, which further validates its effectiveness and threatening.\n        △ Less",
        "authors":[
            "Zeyu Dai",
            "Shengcai Liu",
            "Rui He",
            "Jiahao Wu",
            "Ning Lu",
            "Wenqi Fan",
            "Qing Li",
            "Ke Tang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FedCanon: Non-Convex Composite Federated Learning with Efficient Proximal Operation on Heterogeneous Data",
        "abstract":"Composite federated learning offers a general framework for solving machine learning problems with additional regularization terms. However, many existing methods require clients to perform multiple proximal operations to handle non-smooth terms and their performance are often susceptible to data heterogeneity. To overcome these limitations, we propose a novel composite federated learning algorithm called \\textbf{FedCanon}, designed to solve the optimization problems comprising a possibly non-convex loss function and a weakly convex, potentially non-smooth regularization term. By decoupling proximal mappings from local updates, FedCanon requires only a single proximal evaluation on the server per iteration, thereby reducing the overall proximal computation cost. It also introduces control variables that incorporate global gradient information into client updates, which helps mitigate the effects of data heterogeneity. Theoretical analysis demonstrates that FedCanon achieves sublinear convergence rates under general non-convex settings and linear convergence under the Polyak-Łojasiewicz condition, without relying on bounded heterogeneity assumptions. Experiments demonstrate that FedCanon outperforms the state-of-the-art methods in terms of both accuracy and computational efficiency, particularly under heterogeneous data distributions.\n        △ Less",
        "authors":[
            "Yuan Zhou",
            "Jiachen Zhong",
            "Xinli Shi",
            "Guanghui Wen",
            "Xinghuo Yu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HyperSAT: Unsupervised Hypergraph Neural Networks for Weighted MaxSAT Problems",
        "abstract":"Graph neural networks (GNNs) have shown promising performance in solving both Boolean satisfiability (SAT) and Maximum Satisfiability (MaxSAT) problems due to their ability to efficiently model and capture the structural dependencies between literals and clauses. However, GNN methods for solving Weighted MaxSAT problems remain underdeveloped. The challenges arise from the non-linear dependency and sensitive objective function, which are caused by the non-uniform distribution of weights across clauses. In this paper, we present HyperSAT, a novel neural approach that employs an unsupervised hypergraph neural network model to solve Weighted MaxSAT problems. We propose a hypergraph representation for Weighted MaxSAT instances and design a cross-attention mechanism along with a shared representation constraint loss function to capture the logical interactions between positive and negative literal nodes in the hypergraph. Extensive experiments on various Weighted MaxSAT datasets demonstrate that HyperSAT achieves better performance than state-of-the-art competitors.\n        △ Less",
        "authors":[
            "Qiyue Chen",
            "Shaolin Tan",
            "Suixiang Gao",
            "Jinhu Lü"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Benchmarking Mutual Information-based Loss Functions in Federated Learning",
        "abstract":"Federated Learning (FL) has attracted considerable interest due to growing privacy concerns and regulations like the General Data Protection Regulation (GDPR), which stresses the importance of privacy-preserving and fair machine learning approaches. In FL, model training takes place on decentralized data, so as to allow clients to upload a locally trained model and receive a globally aggregated model without exposing sensitive information. However, challenges related to fairness-such as biases, uneven performance among clients, and the \"free rider\" issue complicates its adoption. In this paper, we examine the use of Mutual Information (MI)-based loss functions to address these concerns. MI has proven to be a powerful method for measuring dependencies between variables and optimizing deep learning models. By leveraging MI to extract essential features and minimize biases, we aim to improve both the fairness and effectiveness of FL systems. Through extensive benchmarking, we assess the impact of MI-based losses in reducing disparities among clients while enhancing the overall performance of FL.\n        △ Less",
        "authors":[
            "Sarang S",
            "Harsh D. Chothani",
            "Qilei Li",
            "Ahmed M. Abdelmoniem",
            "Arnab K. Paul"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Factor-MCLS: Multi-agent learning system with reward factor matrix and multi-critic framework for dynamic portfolio optimization",
        "abstract":"Typical deep reinforcement learning (DRL) agents for dynamic portfolio optimization learn the factors influencing portfolio return and risk by analyzing the output values of the reward function while adjusting portfolio weights within the training environment. However, it faces a major limitation where it is difficult for investors to intervene in the training based on different levels of risk aversion towards each portfolio asset. This difficulty arises from another limitation: existing DRL agents may not develop a thorough understanding of the factors responsible for the portfolio return and risk by only learning from the output of the reward function. As a result, the strategy for determining the target portfolio weights is entirely dependent on the DRL agents themselves. To address these limitations, we propose a reward factor matrix for elucidating the return and risk of each asset in the portfolio. Additionally, we propose a novel learning system named Factor-MCLS using a multi-critic framework that facilitates learning of the reward factor matrix. In this way, our DRL-based learning system can effectively learn the factors influencing portfolio return and risk. Moreover, based on the critic networks within the multi-critic framework, we develop a risk constraint term in the training objective function of the policy function. This risk constraint term allows investors to intervene in the training of the DRL agent according to their individual levels of risk aversion towards the portfolio assets.\n        △ Less",
        "authors":[
            "Ruoyu Sun",
            "Angelos Stefanidis",
            "Zhengyong Jiang",
            "Jionglong Su"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transferable Deployment of Semantic Edge Inference Systems via Unsupervised Domain Adaption",
        "abstract":"This paper investigates deploying semantic edge inference systems for performing a common image clarification task. In particular, each system consists of multiple Internet of Things (IoT) devices that first locally encode the sensing data into semantic features and then transmit them to an edge server for subsequent data fusion and task inference. The inference accuracy is determined by efficient training of the feature encoder\/decoder using labeled data samples. Due to the difference in sensing data and communication channel distributions, deploying the system in a new environment may induce high costs in annotating data labels and re-training the encoder\/decoder models. To achieve cost-effective transferable system deployment, we propose an efficient Domain Adaptation method for Semantic Edge INference systems (DASEIN) that can maintain high inference accuracy in a new environment without the need for labeled samples. Specifically, DASEIN exploits the task-relevant data correlation between different deployment scenarios by leveraging the techniques of unsupervised domain adaptation and knowledge distillation. It devises an efficient two-step adaptation procedure that sequentially aligns the data distributions and adapts to the channel variations. Numerical results show that, under a substantial change in sensing data distributions, the proposed DASEIN outperforms the best-performing benchmark method by 7.09% and 21.33% in inference accuracy when the new environment has similar or 25 dB lower channel signal to noise power ratios (SNRs), respectively. This verifies the effectiveness of the proposed method in adapting both data and channel distributions in practical transfer deployment applications.\n        △ Less",
        "authors":[
            "Weiqiang Jiao",
            "Suzhi Bi",
            "Xian Li",
            "Cheng Guo",
            "Hao Chen",
            "Zhi Quan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Problem of Best Arm Retention",
        "abstract":"This paper presents a comprehensive study on the problem of Best Arm Retention (BAR), which has recently found applications in streaming algorithms for multi-armed bandits. In the BAR problem, the goal is to retain $m$ arms with the best arm included from $n$ after some trials, in stochastic multi-armed bandit settings. We first investigate pure exploration for the BAR problem under different criteria, and then minimize the regret with specific constraints, in the context of further exploration in streaming algorithms.\n  - We begin by revisiting the lower bound for the $(\\varepsilon,δ)$-PAC algorithm for Best Arm Identification (BAI) and adapt the classical KL-divergence argument to derive optimal bounds for $(\\varepsilon,δ)$-PAC algorithms for BAR.\n  - We further study another variant of the problem, called $r$-BAR, which requires the expected gap between the best arm and the optimal arm retained is less than $r$. We prove tight sample complexity for the problem.\n  - We explore the regret minimization problem for $r$-BAR and develop algorithm beyond pure exploration. We conclude with a conjecture on the optimal regret in this setting.\n        △ Less",
        "authors":[
            "Houshuang Chen",
            "Yuchen He",
            "Chihao Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evaluating the Goal-Directedness of Large Language Models",
        "abstract":"To what extent do LLMs use their capabilities towards their given goal? We take this as a measure of their goal-directedness. We evaluate goal-directedness on tasks that require information gathering, cognitive effort, and plan execution, where we use subtasks to infer each model's relevant capabilities. Our evaluations of LLMs from Google DeepMind, OpenAI, and Anthropic show that goal-directedness is relatively consistent across tasks, differs from task performance, and is only moderately sensitive to motivational prompts. Notably, most models are not fully goal-directed. We hope our goal-directedness evaluations will enable better monitoring of LLM progress, and enable more deliberate design choices of agentic properties in LLMs.\n        △ Less",
        "authors":[
            "Tom Everitt",
            "Cristina Garbacea",
            "Alexis Bellot",
            "Jonathan Richens",
            "Henry Papadatos",
            "Siméon Campos",
            "Rohin Shah"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GT-SVQ: A Linear-Time Graph Transformer for Node Classification Using Spiking Vector Quantization",
        "abstract":"Graph Transformers (GTs), which simultaneously integrate message-passing and self-attention mechanisms, have achieved promising empirical results in some graph prediction tasks. Although these approaches show the potential of Transformers in capturing long-range graph topology information, issues concerning the quadratic complexity and high computing energy consumption severely limit the scalability of GTs on large-scale graphs. Recently, as brain-inspired neural networks, Spiking Neural Networks (SNNs), facilitate the development of graph representation learning methods with lower computational and storage overhead through the unique event-driven spiking neurons. Inspired by these characteristics, we propose a linear-time Graph Transformer using Spiking Vector Quantization (GT-SVQ) for node classification. GT-SVQ reconstructs codebooks based on rate coding outputs from spiking neurons, and injects the codebooks into self-attention blocks to aggregate global information in linear complexity. Besides, spiking vector quantization effectively alleviates codebook collapse and the reliance on complex machinery (distance measure, auxiliary loss, etc.) present in previous vector quantization-based graph learning methods. In experiments, we compare GT-SVQ with other state-of-the-art baselines on node classification datasets ranging from small to large. Experimental results show that GT-SVQ has achieved competitive performances on most datasets while maintaining up to 130x faster inference speed compared to other GTs.\n        △ Less",
        "authors":[
            "Huizhe Zhang",
            "Jintang Li",
            "Yuchang Zhu",
            "Liang Chen",
            "Zibin Zheng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Support is All You Need for Certified VAE Training",
        "abstract":"Variational Autoencoders (VAEs) have become increasingly popular and deployed in safety-critical applications. In such applications, we want to give certified probabilistic guarantees on performance under adversarial attacks. We propose a novel method, CIVET, for certified training of VAEs. CIVET depends on the key insight that we can bound worst-case VAE error by bounding the error on carefully chosen support sets at the latent layer. We show this point mathematically and present a novel training algorithm utilizing this insight. We show in an extensive evaluation across different datasets (in both the wireless and vision application areas), architectures, and perturbation magnitudes that our method outperforms SOTA methods achieving good standard performance with strong robustness guarantees.\n        △ Less",
        "authors":[
            "Changming Xu",
            "Debangshu Banerjee",
            "Deepak Vasisht",
            "Gagandeep Singh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Emergence of Computational Structure in a Neural Network Physics Simulator",
        "abstract":"Neural networks often have identifiable computational structures - components of the network which perform an interpretable algorithm or task - but the mechanisms by which these emerge and the best methods for detecting these structures are not well understood. In this paper we investigate the emergence of computational structure in a transformer-like model trained to simulate the physics of a particle system, where the transformer's attention mechanism is used to transfer information between particles. We show that (a) structures emerge in the attention heads of the transformer which learn to detect particle collisions, (b) the emergence of these structures is associated to degenerate geometry in the loss landscape, and (c) the dynamics of this emergence follows a power law. This suggests that these components are governed by a degenerate \"effective potential\". These results have implications for the convergence time of computational structure within neural networks and suggest that the emergence of computational structure can be detected by studying the dynamics of network components.\n        △ Less",
        "authors":[
            "Rohan Hitchcock",
            "Gary W. Delaney",
            "Jonathan H. Manton",
            "Richard Scalzo",
            "Jingge Zhu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "abstract":"LLM inference is essential for applications like text summarization, translation, and data analysis, but the high cost of GPU instances from Cloud Service Providers (CSPs) like AWS is a major burden. This paper proposes InferSave, a cost-efficient VM selection framework for cloud based LLM inference. InferSave optimizes KV cache offloading based on Service Level Objectives (SLOs) and workload charac teristics, estimating GPU memory needs, and recommending cost-effective VM instances. Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. Experiments on AWS GPU instances show that selecting lower-cost instances without KV cache offloading improves cost efficiency by up to 73.7% for online workloads, while KV cache offloading saves up to 20.19% for offline workloads.\n        △ Less",
        "authors":[
            "Kihyun Kim",
            "Jinwoo Kim",
            "Hyunsun Chung",
            "Myung-Hoon Cha",
            "Hong-Yeon Kim",
            "Youngjae Kim"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Manifold meta-learning for reduced-complexity neural system identification",
        "abstract":"System identification has greatly benefited from deep learning techniques, particularly for modeling complex, nonlinear dynamical systems with partially unknown physics where traditional approaches may not be feasible. However, deep learning models often require large datasets and significant computational resources at training and inference due to their high-dimensional parameterizations. To address this challenge, we propose a meta-learning framework that discovers a low-dimensional manifold within the parameter space of an over-parameterized neural network architecture. This manifold is learned from a meta-dataset of input-output sequences generated by a class of related dynamical systems, enabling efficient model training while preserving the network's expressive power for the considered system class. Unlike bilevel meta-learning approaches, our method employs an auxiliary neural network to map datasets directly onto the learned manifold, eliminating the need for costly second-order gradient computations during meta-training and reducing the number of first-order updates required in inference, which could be expensive for large models. We validate our approach on a family of Bouc-Wen oscillators, which is a well-studied nonlinear system identification benchmark. We demonstrate that we are able to learn accurate models even in small-data scenarios.\n        △ Less",
        "authors":[
            "Marco Forgione",
            "Ankush Chakrabarty",
            "Dario Piga",
            "Matteo Rufolo",
            "Alberto Bemporad"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Spectral Graph Transformers Meet Neural Ordinary Differential Equations for Non-IID Graphs",
        "abstract":"Graph Neural Network (GNN) research is rapidly advancing due to GNNs' capacity to learn distributed representations from graph-structured data. However, centralizing large volumes of real-world graph data for GNN training is often impractical due to privacy concerns, regulatory restrictions, and commercial competition. Federated learning (FL), a distributed learning paradigm, offers a solution by preserving data privacy with collaborative model training. Despite progress in training huge vision and language models, federated learning for GNNs remains underexplored. To address this challenge, we present a novel method for federated learning on GNNs based on spectral GNNs equipped with neural ordinary differential equations (ODE) for better information capture, showing promising results across both homophilic and heterophilic graphs. Our approach effectively handles non-Independent and Identically Distributed (non-IID) data, while also achieving performance comparable to existing methods that only operate on IID data. It is designed to be privacy-preserving and bandwidth-optimized, making it suitable for real-world applications such as social network analysis, recommendation systems, and fraud detection, which often involve complex, non-IID, and heterophilic graph structures. Our results in the area of federated learning on non-IID heterophilic graphs demonstrate significant improvements, while also achieving better performance on homophilic graphs. This work highlights the potential of federated learning in diverse and challenging graph settings. Open-source code available on GitHub (https:\/\/github.com\/SpringWiz11\/Fed-GNODEFormer).\n        △ Less",
        "authors":[
            "Kishan Gurumurthy",
            "Himanshu Pal",
            "Charu Sharma"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Résumé abstractif à partir d'une transcription audio",
        "abstract":"Currently, large language models are gaining popularity, their achievements are used in many areas, ranging from text translation to generating answers to queries. However, the main problem with these new machine learning algorithms is that training such models requires large computing resources that only large IT companies have. To avoid this problem, a number of methods (LoRA, quantization) have been proposed so that existing models can be effectively fine-tuned for specific tasks. In this paper, we propose an E2E (end to end) audio summarization model using these techniques. In addition, this paper examines the effectiveness of these approaches to the problem under consideration and draws conclusions about the applicability of these methods.\n        △ Less",
        "authors":[
            "Ilia Derkach"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Large Language Models for Drug Overdose Prediction from Longitudinal Medical Records",
        "abstract":"The ability to predict drug overdose risk from a patient's medical records is crucial for timely intervention and prevention. Traditional machine learning models have shown promise in analyzing longitudinal medical records for this task. However, recent advancements in large language models (LLMs) offer an opportunity to enhance prediction performance by leveraging their ability to process long textual data and their inherent prior knowledge across diverse tasks. In this study, we assess the effectiveness of Open AI's GPT-4o LLM in predicting drug overdose events using patients' longitudinal insurance claims records. We evaluate its performance in both fine-tuned and zero-shot settings, comparing them to strong traditional machine learning methods as baselines. Our results show that LLMs not only outperform traditional models in certain settings but can also predict overdose risk in a zero-shot setting without task-specific training. These findings highlight the potential of LLMs in clinical decision support, particularly for drug overdose risk prediction.\n        △ Less",
        "authors":[
            "Md Sultan Al Nahian",
            "Chris Delcher",
            "Daniel Harris",
            "Peter Akpunonu",
            "Ramakanth Kavuluru"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ACMamba: Fast Unsupervised Anomaly Detection via An Asymmetrical Consensus State Space Model",
        "abstract":"Unsupervised anomaly detection in hyperspectral images (HSI), aiming to detect unknown targets from backgrounds, is challenging for earth surface monitoring. However, current studies are hindered by steep computational costs due to the high-dimensional property of HSI and dense sampling-based training paradigm, constraining their rapid deployment. Our key observation is that, during training, not all samples within the same homogeneous area are indispensable, whereas ingenious sampling can provide a powerful substitute for reducing costs. Motivated by this, we propose an Asymmetrical Consensus State Space Model (ACMamba) to significantly reduce computational costs without compromising accuracy. Specifically, we design an asymmetrical anomaly detection paradigm that utilizes region-level instances as an efficient alternative to dense pixel-level samples. In this paradigm, a low-cost Mamba-based module is introduced to discover global contextual attributes of regions that are essential for HSI reconstruction. Additionally, we develop a consensus learning strategy from the optimization perspective to simultaneously facilitate background reconstruction and anomaly compression, further alleviating the negative impact of anomaly reconstruction. Theoretical analysis and extensive experiments across eight benchmarks verify the superiority of ACMamba, demonstrating a faster speed and stronger performance over the state-of-the-art.\n        △ Less",
        "authors":[
            "Guanchun Wang",
            "Xiangrong Zhang",
            "Yifei Zhang",
            "Zelin Peng",
            "Tianyang Zhang",
            "Xu Tang",
            "Licheng Jiao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bridging the Semantic Gaps: Improving Medical VQA Consistency with LLM-Augmented Question Sets",
        "abstract":"Medical Visual Question Answering (MVQA) systems can interpret medical images in response to natural language queries. However, linguistic variability in question phrasing often undermines the consistency of these systems. To address this challenge, we propose a Semantically Equivalent Question Augmentation (SEQA) framework, which leverages large language models (LLMs) to generate diverse yet semantically equivalent rephrasings of questions. Specifically, this approach enriches linguistic diversity while preserving semantic meaning. We further introduce an evaluation metric, Total Agreement Rate with Semantically Equivalent Input and Correct Answer (TAR-SC), which assesses a model's capability to generate consistent and correct responses to semantically equivalent linguistic variations. In addition, we also propose three other diversity metrics - average number of QA items per image (ANQI), average number of questions per image with the same answer (ANQA), and average number of open-ended questions per image with the same semantics (ANQS). Using the SEQA framework, we augmented the benchmarked MVQA public datasets of SLAKE, VQA-RAD, and PathVQA. As a result, all three datasets achieved significant improvements by incorporating more semantically equivalent questions: ANQI increased by an average of 86.1, ANQA by 85.1, and ANQS by 46. Subsequent experiments evaluate three MVQA models (M2I2, MUMC, and BiomedGPT) under both zero-shot and fine-tuning settings on the enhanced datasets. Experimental results in MVQA datasets show that fine-tuned models achieve an average accuracy improvement of 19.35%, while our proposed TAR-SC metric shows an average improvement of 11. 61%, indicating a substantial enhancement in model consistency.\n        △ Less",
        "authors":[
            "Yongpei Ma",
            "Pengyu Wang",
            "Adam Dunn",
            "Usman Naseem",
            "Jinman Kim"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Discrimination-free Insurance Pricing with Privatized Sensitive Attributes",
        "abstract":"Fairness has emerged as a critical consideration in the landscape of machine learning algorithms, particularly as AI continues to transform decision-making across societal domains. To ensure that these algorithms are free from bias and do not discriminate against individuals based on sensitive attributes such as gender and race, the field of algorithmic bias has introduced various fairness concepts, along with methodologies to achieve these notions in different contexts. Despite the rapid advancement, not all sectors have embraced these fairness principles to the same extent. One specific sector that merits attention in this regard is insurance. Within the realm of insurance pricing, fairness is defined through a distinct and specialized framework. Consequently, achieving fairness according to established notions does not automatically ensure fair pricing in insurance. In particular, regulators are increasingly emphasizing transparency in pricing algorithms and imposing constraints on insurance companies on the collection and utilization of sensitive consumer attributes. These factors present additional challenges in the implementation of fairness in pricing algorithms. To address these complexities and comply with regulatory demands, we propose an efficient method for constructing fair models that are tailored to the insurance domain, using only privatized sensitive attributes. Notably, our approach ensures statistical guarantees, does not require direct access to sensitive attributes, and adapts to varying transparency requirements, addressing regulatory demands while ensuring fairness in insurance pricing.\n        △ Less",
        "authors":[
            "Tianhe Zhang",
            "Suhan Liu",
            "Peng Shi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamics and Computational Principles of Echo State Networks: A Mathematical Perspective",
        "abstract":"Reservoir computing (RC) represents a class of state-space models (SSMs) characterized by a fixed state transition mechanism (the reservoir) and a flexible readout layer that maps from the state space. It is a paradigm of computational dynamical systems that harnesses the transient dynamics of high-dimensional state spaces for efficient processing of temporal data. Rooted in concepts from recurrent neural networks, RC achieves exceptional computational power by decoupling the training of the dynamic reservoir from the linear readout layer, thereby circumventing the complexities of gradient-based optimization. This work presents a systematic exploration of RC, addressing its foundational properties such as the echo state property, fading memory, and reservoir capacity through the lens of dynamical systems theory. We formalize the interplay between input signals and reservoir states, demonstrating the conditions under which reservoirs exhibit stability and expressive power. Further, we delve into the computational trade-offs and robustness characteristics of RC architectures, extending the discussion to their applications in signal processing, time-series prediction, and control systems. The analysis is complemented by theoretical insights into optimization, training methodologies, and scalability, highlighting open challenges and potential directions for advancing the theoretical underpinnings of RC.\n        △ Less",
        "authors":[
            "Pradeep Singh",
            "Ashutosh Kumar",
            "Sutirtha Ghosh",
            "Hrishit B P",
            "Balasubramanian Raman"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision",
        "abstract":"We study the hard problem of 3D object segmentation in complex point clouds without requiring human labels of 3D scenes for supervision. By relying on the similarity of pretrained 2D features or external signals such as motion to group 3D points as objects, existing unsupervised methods are usually limited to identifying simple objects like cars or their segmented objects are often inferior due to the lack of objectness in pretrained features. In this paper, we propose a new two-stage pipeline called GrabS. The core concept of our method is to learn generative and discriminative object-centric priors as a foundation from object datasets in the first stage, and then design an embodied agent to learn to discover multiple objects by querying against the pretrained generative priors in the second stage. We extensively evaluate our method on two real-world datasets and a newly created synthetic dataset, demonstrating remarkable segmentation performance, clearly surpassing all existing unsupervised methods.\n        △ Less",
        "authors":[
            "Zihui Zhang",
            "Yafei Yang",
            "Hongtao Wen",
            "Bo Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?",
        "abstract":"Recent supervised fine-tuning (SFT) approaches have significantly improved language models' performance on mathematical reasoning tasks, even when models are trained at a small scale. However, the specific capabilities enhanced through such fine-tuning remain poorly understood. In this paper, we conduct a detailed analysis of model performance on the AIME24 dataset to understand how reasoning capabilities evolve. We discover a ladder-like structure in problem difficulty, categorize questions into four tiers (Easy, Medium, Hard, and Extremely Hard (Exh)), and identify the specific requirements for advancing between tiers. We find that progression from Easy to Medium tier requires adopting an R1 reasoning style with minimal SFT (500-1K instances), while Hard-level questions suffer from frequent model's errors at each step of the reasoning chain, with accuracy plateauing at around 65% despite logarithmic scaling. Exh-level questions present a fundamentally different challenge; they require unconventional problem-solving skills that current models uniformly struggle with. Additional findings reveal that carefully curated small-scale datasets offer limited advantage-scaling dataset size proves far more effective. Our analysis provides a clearer roadmap for advancing language model capabilities in mathematical reasoning.\n        △ Less",
        "authors":[
            "Yiyou Sun",
            "Georgia Zhou",
            "Hao Wang",
            "Dacheng Li",
            "Nouha Dziri",
            "Dawn Song"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Saga: Capturing Multi-granularity Semantics from Massive Unlabelled IMU Data for User Perception",
        "abstract":"Inertial measurement units (IMUs), have been prevalently used in a wide range of mobile perception applications such as activity recognition and user authentication, where a large amount of labelled data are normally required to train a satisfactory model. However, it is difficult to label micro-activities in massive IMU data due to the hardness of understanding raw IMU data and the lack of ground truth. In this paper, we propose a novel fine-grained user perception approach, called Saga, which only needs a small amount of labelled IMU data to achieve stunning user perception accuracy. The core idea of Saga is to first pre-train a backbone feature extraction model, utilizing the rich semantic information of different levels embedded in the massive unlabelled IMU data. Meanwhile, for a specific downstream user perception application, Bayesian Optimization is employed to determine the optimal weights for pre-training tasks involving different semantic levels. We implement Saga on five typical mobile phones and evaluate Saga on three typical tasks on three IMU datasets. Results show that when only using about 100 training samples per class, Saga can achieve over 90% accuracy of the full-fledged model trained on over ten thousands training samples with no additional system overhead.\n        △ Less",
        "authors":[
            "Yunzhe Li",
            "Facheng Hu",
            "Hongzi Zhu",
            "Shifan Zhang",
            "Liang Zhang",
            "Shan Chang",
            "Minyi Guo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Unravelling Technical debt topics through Time, Programming Languages and Repository",
        "abstract":"This study explores the dynamic landscape of Technical Debt (TD) topics in software engineering by examining its evolution across time, programming languages, and repositories. Despite the extensive research on identifying and quantifying TD, there remains a significant gap in understanding the diversity of TD topics and their temporal development. To address this, we have conducted an explorative analysis of TD data extracted from GitHub issues spanning from 2015 to September 2023. We employed BERTopic for sophisticated topic modelling. This study categorises the TD topics and tracks their progression over time. Furthermore, we have incorporated sentiment analysis for each identified topic, providing a deeper insight into the perceptions and attitudes associated with these topics. This offers a more nuanced understanding of the trends and shifts in TD topics through time, programming language, and repository.\n        △ Less",
        "authors":[
            "Karthik Shivashankar",
            "Antonio Martini"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching",
        "abstract":"We introduce Adjoint Sampling, a highly scalable and efficient algorithm for learning diffusion processes that sample from unnormalized densities, or energy functions. It is the first on-policy approach that allows significantly more gradient updates than the number of energy evaluations and model samples, allowing us to scale to much larger problem settings than previously explored by similar methods. Our framework is theoretically grounded in stochastic optimal control and shares the same theoretical guarantees as Adjoint Matching, being able to train without the need for corrective measures that push samples towards the target distribution. We show how to incorporate key symmetries, as well as periodic boundary conditions, for modeling molecules in both cartesian and torsional coordinates. We demonstrate the effectiveness of our approach through extensive experiments on classical energy functions, and further scale up to neural network-based energy models where we perform amortized conformer generation across many molecular systems. To encourage further research in developing highly scalable sampling methods, we plan to open source these challenging benchmarks, where successful methods can directly impact progress in computational chemistry.\n        △ Less",
        "authors":[
            "Aaron Havens",
            "Benjamin Kurt Miller",
            "Bing Yan",
            "Carles Domingo-Enrich",
            "Anuroop Sriram",
            "Brandon Wood",
            "Daniel Levine",
            "Bin Hu",
            "Brandon Amos",
            "Brian Karrer",
            "Xiang Fu",
            "Guan-Horng Liu",
            "Ricky T. Q. Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Clustering and analysis of user behaviour in blockchain: A case study of Planet IX",
        "abstract":"Decentralised applications (dApps) that run on public blockchains have the benefit of trustworthiness and transparency as every activity that happens on the blockchain can be publicly traced through the transaction data. However, this introduces a potential privacy problem as this data can be tracked and analysed, which can reveal user-behaviour information. A user behaviour analysis pipeline was proposed to present how this type of information can be extracted and analysed to identify separate behavioural clusters that can describe how users behave in the game. The pipeline starts with the collection of transaction data, involving smart contracts, that is collected from a blockchain-based game called Planet IX. Both the raw transaction information and the transaction events are considered in the data collection. From this data, separate game actions can be formed and those are leveraged to present how and when the users conducted their in-game activities in the form of user flows. An extended version of these user flows also presents how the Non-Fungible Tokens (NFTs) are being leveraged in the user actions. The latter is given as input for a Graph Neural Network (GNN) model to provide graph embeddings for these flows which then can be leveraged by clustering algorithms to cluster user behaviours into separate behavioural clusters. We benchmark and compare well-known clustering algorithms as a part of the proposed method. The user behaviour clusters were analysed and visualised in a graph format. It was found that behavioural information can be extracted regarding the users that belong to these clusters. Such information can be exploited by malicious users to their advantage. To demonstrate this, a privacy threat model was also presented based on the results that correspond to multiple potentially affected areas.\n        △ Less",
        "authors":[
            "Dorottya Zelenyanszki",
            "Zhe Hou",
            "Kamanashis Biswas",
            "Vallipuram Muthukkumarasamy"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"H$^3$GNNs: Harmonizing Heterophily and Homophily in GNNs via Joint Structural Node Encoding and Self-Supervised Learning",
        "abstract":"Graph Neural Networks (GNNs) struggle to balance heterophily and homophily in representation learning, a challenge further amplified in self-supervised settings. We propose H$^3$GNNs, an end-to-end self-supervised learning framework that harmonizes both structural properties through two key innovations: (i) Joint Structural Node Encoding. We embed nodes into a unified space combining linear and non-linear feature projections with K-hop structural representations via a Weighted Graph Convolution Network(WGCN). A cross-attention mechanism enhances awareness and adaptability to heterophily and homophily. (ii) Self-Supervised Learning Using Teacher-Student Predictive Architectures with Node-Difficulty Driven Dynamic Masking Strategies. We use a teacher-student model, the student sees the masked input graph and predicts node features inferred by the teacher that sees the full input graph in the joint encoding space. To enhance learning difficulty, we introduce two novel node-predictive-difficulty-based masking strategies. Experiments on seven benchmarks (four heterophily datasets and three homophily datasets) confirm the effectiveness and efficiency of H$^3$GNNs across diverse graph types. Our H$^3$GNNs achieves overall state-of-the-art performance on the four heterophily datasets, while retaining on-par performance to previous state-of-the-art methods on the three homophily datasets.\n        △ Less",
        "authors":[
            "Rui Xue",
            "Tianfu Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Steering Prosocial AI Agents: Computational Basis of LLM's Decision Making in Social Simulation",
        "abstract":"Large language models (LLMs) increasingly serve as human-like decision-making agents in social science and applied settings. These LLM-agents are typically assigned human-like characters and placed in real-life contexts. However, how these characters and contexts shape an LLM's behavior remains underexplored. This study proposes and tests methods for probing, quantifying, and modifying an LLM's internal representations in a Dictator Game -- a classic behavioral experiment on fairness and prosocial behavior. We extract ``vectors of variable variations'' (e.g., ``male'' to ``female'') from the LLM's internal state. Manipulating these vectors during the model's inference can substantially alter how those variables relate to the model's decision-making. This approach offers a principled way to study and regulate how social concepts can be encoded and engineered within transformer-based models, with implications for alignment, debiasing, and designing AI agents for social simulations in both academic and commercial applications.\n        △ Less",
        "authors":[
            "Ji Ma"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transformer-Driven Neural Beamforming with Imperfect CSI in Urban Macro Wireless Channels",
        "abstract":"The literature is abundant with methodologies focusing on using transformer architectures due to their prominence in wireless signal processing and their capability to capture long-range dependencies via attention mechanisms. In particular, depthwise separable convolutions enhance parameter efficiency for the process of high-dimensional data characteristics of MIMO systems. In this work, we introduce a novel unsupervised deep learning framework that integrates depthwise separable convolutions and transformers to generate beamforming weights under imperfect channel state information (CSI) for a multi-user single-input multiple-output (MU-SIMO) system in dense urban environments. The primary goal is to enhance throughput by maximizing sum-rate while ensuring reliable communication. Spectral efficiency and block error rate (BLER) are considered as performance metrics. Experiments are carried out under various conditions to compare the performance of the proposed NNBF framework against baseline methods zero-forcing beamforming (ZFBF) and minimum mean square error (MMSE) beamforming. Experimental results demonstrate the superiority of the proposed framework over the baseline techniques.\n        △ Less",
        "authors":[
            "Cemil Vahapoglu",
            "Timothy J. O'Shea",
            "Wan Liu",
            "Tamoghna Roy",
            "Sennur Ulukus"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Real-time Object and Event Detection Service through Computer Vision and Edge Computing",
        "abstract":"The World Health Organization suggests that road traffic crashes cost approximately 518 billion dollars globally each year, which accounts for 3% of the gross domestic product for most countries. Most fatal road accidents in urban areas involve Vulnerable Road Users (VRUs). Smart cities environments present innovative approaches to combat accidents involving cutting-edge technologies, that include advanced sensors, extensive datasets, Machine Learning (ML) models, communication systems, and edge computing. This paper proposes a strategy and an implementation of a system for road monitoring and safety for smart cities, based on Computer Vision (CV) and edge computing. Promising results were obtained by implementing vision algorithms and tracking using surveillance cameras, that are part of a Smart City testbed, the Aveiro Tech City Living Lab (ATCLL). The algorithm accurately detects and tracks cars, pedestrians, and bicycles, while predicting the road state, the distance between moving objects, and inferring on collision events to prevent collisions, in near real-time.\n        △ Less",
        "authors":[
            "Marcos Mendes",
            "Gonçalo Perna",
            "Pedro Rito",
            "Duarte Raposo",
            "Susana Sargento"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float",
        "abstract":"Large Language Models (LLMs) have grown rapidly in size, creating significant challenges for efficient deployment on resource-constrained hardware. In this paper, we introduce Dynamic-Length Float (DFloat11), a lossless compression framework that reduces LLM size by 30% while preserving outputs that are bit-for-bit identical to the original model. DFloat11 is motivated by the low entropy in the BFloat16 weight representation of LLMs, which reveals significant inefficiency in existing storage format. By applying entropy coding, DFloat11 assigns dynamic-length encodings to weights based on frequency, achieving near information-optimal compression without any loss of precision. To facilitate efficient inference with dynamic-length encodings, we develop a custom GPU kernel for fast online decompression. Our design incorporates the following: (i) decomposition of memory-intensive lookup tables (LUTs) into compact LUTs that fit in GPU SRAM, (ii) a two-phase kernel for coordinating thread read\/write positions using lightweight auxiliary variables, and (iii) transformer-block-level decompression to minimize latency. Experiments on recent models, including Llama-3.1, Qwen-2.5, and Gemma-3, validates our hypothesis that DFloat11 achieves around 30% model size reduction while preserving bit-for-bit exact outputs. Compared to a potential alternative of offloading parts of an uncompressed model to the CPU to meet memory constraints, DFloat11 achieves 1.9-38.8x higher throughput in token generation. With a fixed GPU memory budget, DFloat11 enables 5.3-13.17x longer context lengths than uncompressed models. Notably, our method enables lossless inference of Llama-3.1-405B, an 810GB model, on a single node equipped with 8x80GB GPUs. Our code and models are available at https:\/\/github.com\/LeanModels\/DFloat11.\n        △ Less",
        "authors":[
            "Tianyi Zhang",
            "Yang Sui",
            "Shaochen Zhong",
            "Vipin Chaudhary",
            "Xia Hu",
            "Anshumali Shrivastava"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data driven approach towards more efficient Newton-Raphson power flow calculation for distribution grids",
        "abstract":"Power flow (PF) calculations are fundamental to power system analysis to ensure stable and reliable grid operation. The Newton-Raphson (NR) method is commonly used for PF analysis due to its rapid convergence when initialized properly. However, as power grids operate closer to their capacity limits, ill-conditioned cases and convergence issues pose significant challenges. This work, therefore, addresses these challenges by proposing strategies to improve NR initialization, hence minimizing iterations and avoiding divergence. We explore three approaches: (i) an analytical method that estimates the basin of attraction using mathematical bounds on voltages, (ii) Two data-driven models leveraging supervised learning or physics-informed neural networks (PINNs) to predict optimal initial guesses, and (iii) a reinforcement learning (RL) approach that incrementally adjusts voltages to accelerate convergence. These methods are tested on benchmark systems. This research is particularly relevant for modern power systems, where high penetration of renewables and decentralized generation require robust and scalable PF solutions. In experiments, all three proposed methods demonstrate a strong ability to provide an initial guess for Newton-Raphson method to converge with fewer steps. The findings provide a pathway for more efficient real-time grid operations, which, in turn, support the transition toward smarter and more resilient electricity networks.\n        △ Less",
        "authors":[
            "Shengyuan Yan",
            "Farzad Vazinram",
            "Zeynab Kaseb",
            "Lindsay Spoor",
            "Jochen Stiasny",
            "Betul Mamudi",
            "Amirhossein Heydarian Ardakani",
            "Ugochukwu Orji",
            "Pedro P. Vergara",
            "Yu Xiang",
            "Jerry Guo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Achieving Tighter Finite-Time Rates for Heterogeneous Federated Stochastic Approximation under Markovian Sampling",
        "abstract":"Motivated by collaborative reinforcement learning (RL) and optimization with time-correlated data, we study a generic federated stochastic approximation problem involving $M$ agents, where each agent is characterized by an agent-specific (potentially nonlinear) local operator. The goal is for the agents to communicate intermittently via a server to find the root of the average of the agents' local operators. The generality of our setting stems from allowing for (i) Markovian data at each agent and (ii) heterogeneity in the roots of the agents' local operators. The limited recent work that has accounted for both these features in a federated setting fails to guarantee convergence to the desired point or to show any benefit of collaboration; furthermore, they rely on projection steps in their algorithms to guarantee bounded iterates. Our work overcomes each of these limitations. We develop a novel algorithm titled \\texttt{FedHSA}, and prove that it guarantees convergence to the correct point, while enjoying an $M$-fold linear speedup in sample-complexity due to collaboration. To our knowledge, \\emph{this is the first finite-time result of its kind}, and establishing it (without relying on a projection step) entails a fairly intricate argument that accounts for the interplay between complex temporal correlations due to Markovian sampling, multiple local steps to save communication, and the drift-effects induced by heterogeneous local operators. Our results have implications for a broad class of heterogeneous federated RL problems (e.g., policy evaluation and control) with function approximation, where the agents' Markov decision processes can differ in their probability transition kernels and reward functions.\n        △ Less",
        "authors":[
            "Feng Zhu",
            "Aritra Mitra",
            "Robert W. Heath"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Possibility for Proactive Anomaly Detection",
        "abstract":"Time-series anomaly detection, which detects errors and failures in a workflow, is one of the most important topics in real-world applications. The purpose of time-series anomaly detection is to reduce potential damages or losses. However, existing anomaly detection models detect anomalies through the error between the model output and the ground truth (observed) value, which makes them impractical. In this work, we present a \\textit{proactive} approach for time-series anomaly detection based on a time-series forecasting model specialized for anomaly detection and a data-driven anomaly detection model. Our proactive approach establishes an anomaly threshold from training data with a data-driven anomaly detection model, and anomalies are subsequently detected by identifying predicted values that exceed the anomaly threshold. In addition, we extensively evaluated the model using four anomaly detection benchmarks and analyzed both predictable and unpredictable anomalies. We attached the source code as supplementary material.\n        △ Less",
        "authors":[
            "Jinsung Jeon",
            "Jaehyeon Park",
            "Sewon Park",
            "Jeongwhan Choi",
            "Minjung Kim",
            "Noseong Park"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Making Acoustic Side-Channel Attacks on Noisy Keyboards Viable with LLM-Assisted Spectrograms' \"Typo\" Correction",
        "abstract":"The large integration of microphones into devices increases the opportunities for Acoustic Side-Channel Attacks (ASCAs), as these can be used to capture keystrokes' audio signals that might reveal sensitive information. However, the current State-Of-The-Art (SOTA) models for ASCAs, including Convolutional Neural Networks (CNNs) and hybrid models, such as CoAtNet, still exhibit limited robustness under realistic noisy conditions. Solving this problem requires either: (i) an increased model's capacity to infer contextual information from longer sequences, allowing the model to learn that an initially noisily typed word is the same as a futurely collected non-noisy word, or (ii) an approach to fix misidentified information from the contexts, as one does not type random words, but the ones that best fit the conversation context. In this paper, we demonstrate that both strategies are viable and complementary solutions for making ASCAs practical. We observed that no existing solution leverages advanced transformer architectures' power for these tasks and propose that: (i) Visual Transformers (VTs) are the candidate solutions for capturing long-term contextual information and (ii) transformer-powered Large Language Models (LLMs) are the candidate solutions to fix the ``typos'' (mispredictions) the model might make. Thus, we here present the first-of-its-kind approach that integrates VTs and LLMs for ASCAs.\n  We first show that VTs achieve SOTA performance in classifying keystrokes when compared to the previous CNN benchmark. Second, we demonstrate that LLMs can mitigate the impact of real-world noise. Evaluations on the natural sentences revealed that: (i) incorporating LLMs (e.g., GPT-4o) in our ASCA pipeline boosts the performance of error-correction tasks; and (ii) the comparable performance can be attained by a lightweight, fine-tuned smaller LLM (67 times smaller than GPT-4o), using...\n        △ Less",
        "authors":[
            "Seyyed Ali Ayati",
            "Jin Hyun Park",
            "Yichen Cai",
            "Marcus Botacin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust Markov stability for community detection at a scale learned based on the structure",
        "abstract":"Community detection, the unsupervised task of clustering nodes of a graph, finds applications across various fields. The common approaches for community detection involve optimizing an objective function to partition the nodes into communities at a single scale of granularity. However, the single-scale approaches often fall short of producing partitions that are robust and at a suitable scale. The existing algorithm, PyGenStability, returns multiple robust partitions for a network by optimizing the multi-scale Markov stability function. However, in cases where the suitable scale is not known or assumed by the user, there is no principled method to select a single robust partition at a suitable scale from the multiple partitions that PyGenStability produces. Our proposed method combines the Markov stability framework with a pre-trained machine learning model for scale selection to obtain one robust partition at a scale that is learned based on the graph structure. This automatic scale selection involves using a gradient boosting model pre-trained on hand-crafted and embedding-based network features from a labeled dataset of 10k benchmark networks. This model was trained to predicts the scale value that maximizes the similarity of the output partition to the planted partition of the benchmark network. Combining our scale selection algorithm with the PyGenStability algorithm results in PyGenStabilityOne (PO): a hyperparameter-free multi-scale community detection algorithm that returns one robust partition at a suitable scale without the need for any assumptions, input, or tweaking from the user. We compare the performance of PO against 29 algorithms and show that it outperforms 25 other algorithms by statistically meaningful margins. Our results facilitate choosing between community detection algorithms, among which PO stands out as the accurate, robust, and hyperparameter-free method.\n        △ Less",
        "authors":[
            "Samin Aref",
            "Sanchaai Mathiyarasan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generalized probabilistic canonical correlation analysis for multi-modal data integration with full or partial observations",
        "abstract":"Background: The integration and analysis of multi-modal data are increasingly essential across various domains including bioinformatics. As the volume and complexity of such data grow, there is a pressing need for computational models that not only integrate diverse modalities but also leverage their complementary information to improve clustering accuracy and insights, especially when dealing with partial observations with missing data. Results: We propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an unsupervised method for the integration and joint dimensionality reduction of multi-modal data. GPCCA addresses key challenges in multi-modal data analysis by handling missing values within the model, enabling the integration of more than two modalities, and identifying informative features while accounting for correlations within individual modalities. The model demonstrates robustness to various missing data patterns and provides low-dimensional embeddings that facilitate downstream clustering and analysis. In a range of simulation settings, GPCCA outperforms existing methods in capturing essential patterns across modalities. Additionally, we demonstrate its applicability to multi-omics data from TCGA cancer datasets and a multi-view image dataset. Conclusion: GPCCA offers a useful framework for multi-modal data integration, effectively handling missing data and providing informative low-dimensional embeddings. Its performance across cancer genomics and multi-view image data highlights its robustness and potential for broad application. To make the method accessible to the wider research community, we have released an R package, GPCCA, which is available at https:\/\/github.com\/Kaversoniano\/GPCCA.\n        △ Less",
        "authors":[
            "Tianjian Yang",
            "Wei Vivian Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Interpretable Deep Generative Models via Causal Representation Learning",
        "abstract":"Recent developments in generative artificial intelligence (AI) rely on machine learning techniques such as deep learning and generative modeling to achieve state-of-the-art performance across wide-ranging domains. These methods' surprising performance is due in part to their ability to learn implicit \"representations'' of complex, multi-modal data. Unfortunately, deep neural networks are notoriously black boxes that obscure these representations, making them difficult to interpret or analyze. To resolve these difficulties, one approach is to build new interpretable neural network models from the ground up. This is the goal of the emerging field of causal representation learning (CRL) that uses causality as a vector for building flexible, interpretable, and transferable generative AI. CRL can be seen as a culmination of three intrinsically statistical problems: (i) latent variable models such as factor analysis; (ii) causal graphical models with latent variables; and (iii) nonparametric statistics and deep learning. This paper reviews recent progress in CRL from a statistical perspective, focusing on connections to classical models and statistical and causal identifiablity results. This review also highlights key application areas, implementation strategies, and open statistical questions in CRL.\n        △ Less",
        "authors":[
            "Gemma E. Moran",
            "Bryon Aragam"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Measuring Computational Universality of Fully Homomorphic Encryption",
        "abstract":"Many real-world applications, such as machine learning and graph analytics, involve combinations of linear and non-linear operations. As these applications increasingly handle sensitive data, there is a significant demand for privacy-preserving computation techniques capable of efficiently supporting both types of operations-a property we define as \"computational universality.\" Fully Homomorphic Encryption (FHE) has emerged as a powerful approach to perform computations directly on encrypted data. In this paper, we systematically evaluate and measure whether existing FHE methods achieve computational universality or primarily favor either linear or non-linear operations, especially in non-interactive settings. We evaluate FHE universality in three stages. First, we categorize existing FHE methods into ten distinct approaches and analyze their theoretical complexities, selecting the three most promising universal candidates. Next, we perform measurements on representative workloads that combine linear and non-linear operations in various sequences, assessing performance across different bit lengths and with SIMD parallelization enabled or disabled. Finally, we empirically evaluate these candidates on five real-world, privacy-sensitive applications, where each involving arithmetic (linear) and comparison-like (non-linear) operations. Our findings indicate significant overheads in current universal FHE solutions, with efficiency strongly influenced by SIMD parallelism, word-wise versus bit-wise operations, and the trade-off between approximate and exact computations. Additionally, our analysis provides practical guidance to help practitioners select the most suitable universal FHE schemes and algorithms based on specific application requirements.\n        △ Less",
        "authors":[
            "Jiaqi Xue",
            "Xin Xin",
            "Wei Zhang",
            "Mengxin Zheng",
            "Qianqian Song",
            "Minxuan Zhou",
            "Yushun Dong",
            "Dongjie Wang",
            "Xun Chen",
            "Jiafeng Xie",
            "Liqiang Wang",
            "David Mohaisen",
            "Hongyi Wu",
            "Qian Lou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dueling Deep Reinforcement Learning for Financial Time Series",
        "abstract":"Reinforcement learning (RL) has emerged as a powerful paradigm for solving decision-making problems in dynamic environments. In this research, we explore the application of Double DQN (DDQN) and Dueling Network Architectures, to financial trading tasks using historical SP500 index data. Our focus is training agents capable of optimizing trading strategies while accounting for practical constraints such as transaction costs. The study evaluates the model performance across scenarios with and without commissions, highlighting the impact of cost-sensitive environments on reward dynamics. Despite computational limitations and the inherent complexity of financial time series data, the agent successfully learned meaningful trading policies. The findings confirm that RL agents, even when trained on limited datasets, can outperform random strategies by leveraging advanced architectures such as DDQN and Dueling Networks. However, significant challenges persist, particularly with a sub-optimal policy due to the complexity of data source.\n        △ Less",
        "authors":[
            "Bruno Giorgio"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards a Universal Vibration Analysis Dataset: A Framework for Transfer Learning in Predictive Maintenance and Structural Health Monitoring",
        "abstract":"ImageNet has become a reputable resource for transfer learning, allowing the development of efficient ML models with reduced training time and data requirements. However, vibration analysis in predictive maintenance, structural health monitoring, and fault diagnosis, lacks a comparable large-scale, annotated dataset to facilitate similar advancements. To address this, a dataset framework is proposed that begins with bearing vibration data as an initial step towards creating a universal dataset for vibration-based spectrogram analysis for all machinery. The initial framework includes a collection of bearing vibration signals from various publicly available datasets. To demonstrate the advantages of this framework, experiments were conducted using a deep learning architecture, showing improvements in model performance when pre-trained on bearing vibration data and fine-tuned on a smaller, domain-specific dataset. These findings highlight the potential to parallel the success of ImageNet in visual computing but for vibration analysis. For future work, this research will include a broader range of vibration signals from multiple types of machinery, emphasizing spectrogram-based representations of the data. Each sample will be labeled according to machinery type, operational status, and the presence or type of faults, ensuring its utility for supervised and unsupervised learning tasks. Additionally, a framework for data preprocessing, feature extraction, and model training specific to vibration data will be developed. This framework will standardize methodologies across the research community, allowing for collaboration and accelerating progress in predictive maintenance, structural health monitoring, and related fields. By mirroring the success of ImageNet in visual computing, this dataset has the potential to improve the development of intelligent systems in industrial applications.\n        △ Less",
        "authors":[
            "Mert Sehri",
            "Igor Varejão",
            "Zehui Hua",
            "Vitor Bonella",
            "Adriano Santos",
            "Francisco de Assis Boldt",
            "Patrick Dumond",
            "Flavio Miguel Varejão"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MULTI-LF: A Unified Continuous Learning Framework for Real-Time DDoS Detection in Multi-Environment Networks",
        "abstract":"Detecting Distributed Denial of Service (DDoS) attacks in Multi-Environment (M-En) networks presents significant challenges due to diverse malicious traffic patterns and the evolving nature of cyber threats. Existing AI-based detection systems struggle to adapt to new attack strategies and lack real-time attack detection capabilities with high accuracy and efficiency. This study proposes an online, continuous learning methodology for DDoS detection in M-En networks, enabling continuous model updates and real-time adaptation to emerging threats, including zero-day attacks. First, we develop a unique M-En network dataset by setting up a realistic, real-time simulation using the NS-3 tool, incorporating both victim and bot devices. DDoS attacks with varying packet sizes are simulated using the DDoSim application across IoT and traditional IP-based environments under M-En network criteria. Our approach employs a multi-level framework (MULTI-LF) featuring two machine learning models: a lightweight Model 1 (M1) trained on a selective, critical packet dataset for fast and efficient initial detection, and a more complex, highly accurate Model 2 (M2) trained on extensive data. When M1 exhibits low confidence in its predictions, the decision is escalated to M2 for verification and potential fine-tuning of M1 using insights from M2. If both models demonstrate low confidence, the system flags the incident for human intervention, facilitating model updates with human-verified categories to enhance adaptability to unseen attack patterns. We validate the MULTI-LF through real-world simulations, demonstrating superior classification accuracy of 0.999 and low prediction latency of 0.866 seconds compared to established baselines. Furthermore, we evaluate performance in terms of memory usage (3.632 MB) and CPU utilization (10.05%) in real-time scenarios.\n        △ Less",
        "authors":[
            "Furqan Rustam",
            "Islam Obaidat",
            "Anca Delia Jurcut"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Traffic Adaptive Moving-window Service Patrolling for Real-time Incident Management during High-impact Events",
        "abstract":"This paper presents the Traffic Adaptive Moving-window Patrolling Algorithm (TAMPA), designed to improve real-time incident management during major events like sports tournaments and concerts. Such events significantly stress transportation networks, requiring efficient and adaptive patrol solutions. TAMPA integrates predictive traffic modeling and real-time complaint estimation, dynamically optimizing patrol deployment. Using dynamic programming, the algorithm continuously adjusts patrol strategies within short planning windows, effectively balancing immediate response and efficient routing. Leveraging the Dvoretzky-Kiefer-Wolfowitz inequality, TAMPA detects significant shifts in complaint patterns, triggering proactive adjustments in patrol routes. Theoretical analyses ensure performance remains closely aligned with optimal solutions. Simulation results from an urban traffic network demonstrate TAMPA's superior performance, showing improvements of approximately 87.5\\% over stationary methods and 114.2\\% over random strategies. Future work includes enhancing adaptability and incorporating digital twin technology for improved predictive accuracy, particularly relevant for events like the 2026 FIFA World Cup at MetLife Stadium.\n        △ Less",
        "authors":[
            "Haozhe Lei",
            "Ya-Ting Yang",
            "Tao Li",
            "Zilin Bian",
            "Fan Zuo",
            "Sundeep Rangan",
            "Kaan Ozbay"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism",
        "abstract":"We introduce the Error Broadcast and Decorrelation (EBD) algorithm, a novel learning framework that addresses the credit assignment problem in neural networks by directly broadcasting output error to individual layers. Leveraging the stochastic orthogonality property of the optimal minimum mean square error (MMSE) estimator, EBD defines layerwise loss functions to penalize correlations between layer activations and output errors, offering a principled approach to error broadcasting without the need for weight transport. The optimization framework naturally leads to the experimentally observed three-factor learning rule and integrates with biologically plausible frameworks to enhance performance and plausibility. Numerical experiments demonstrate that EBD achieves performance comparable to or better than known error-broadcast methods on benchmark datasets. While the scalability of EBD to very large or complex datasets remains to be further explored, our findings suggest it provides a biologically plausible, efficient, and adaptable alternative for neural network training. This approach could inform future advancements in artificial and natural learning paradigms.\n        △ Less",
        "authors":[
            "Mete Erdogan",
            "Cengiz Pehlevan",
            "Alper T. Erdogan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sub-optimality of the Separation Principle for Quadratic Control from Bilinear Observations",
        "abstract":"We consider the problem of controlling a linear dynamical system from bilinear observations with minimal quadratic cost. Despite the similarity of this problem to standard linear quadratic Gaussian (LQG) control, we show that when the observation model is bilinear, neither does the Separation Principle hold, nor is the optimal controller affine in the estimated state. Moreover, the cost-to-go is non-convex in the control input. Hence, finding an analytical expression for the optimal feedback controller is difficult in general. Under certain settings, we show that the standard LQG controller locally maximizes the cost instead of minimizing it. Furthermore, the optimal controllers (derived analytically) are not unique and are nonlinear in the estimated state. We also introduce a notion of input-dependent observability and derive conditions under which the Kalman filter covariance remains bounded. We illustrate our theoretical results through numerical experiments in multiple synthetic settings.\n        △ Less",
        "authors":[
            "Yahya Sattar",
            "Sunmook Choi",
            "Yassir Jedra",
            "Maryam Fazel",
            "Sarah Dean"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Normalizing Flow Regression for Bayesian Inference with Offline Likelihood Evaluations",
        "abstract":"Bayesian inference with computationally expensive likelihood evaluations remains a significant challenge in many scientific domains. We propose normalizing flow regression (NFR), a novel offline inference method for approximating posterior distributions. Unlike traditional surrogate approaches that require additional sampling or inference steps, NFR directly yields a tractable posterior approximation through regression on existing log-density evaluations. We introduce training techniques specifically for flow regression, such as tailored priors and likelihood functions, to achieve robust posterior and model evidence estimation. We demonstrate NFR's effectiveness on synthetic benchmarks and real-world applications from neuroscience and biology, showing superior or comparable performance to existing methods. NFR represents a promising approach for Bayesian inference when standard methods are computationally prohibitive or existing model evaluations can be recycled.\n        △ Less",
        "authors":[
            "Chengkun Li",
            "Bobby Huggins",
            "Petrus Mikkola",
            "Luigi Acerbi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation",
        "abstract":"There is growing interest in hypothesis generation with large language models (LLMs). However, fundamental questions remain: what makes a good hypothesis, and how can we systematically evaluate methods for hypothesis generation? To address this, we introduce HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate. HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets. We evaluate four state-of-the-art LLMs combined with six existing hypothesis-generation methods. Overall, our results suggest that existing methods are capable of discovering valid and novel patterns in the data. However, the results from synthetic datasets indicate that there is still significant room for improvement, as current hypothesis generation methods do not fully uncover all relevant or meaningful patterns. Specifically, in synthetic settings, as task difficulty increases, performance significantly drops, with best models and methods only recovering 38.8% of the ground-truth hypotheses. These findings highlight challenges in hypothesis generation and demonstrate that HypoBench serves as a valuable resource for improving AI systems designed to assist scientific discovery.\n        △ Less",
        "authors":[
            "Haokun Liu",
            "Sicong Huang",
            "Jingyu Hu",
            "Yangqiaoyu Zhou",
            "Chenhao Tan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LANGTRAJ: Diffusion Model and Dataset for Language-Conditioned Trajectory Simulation",
        "abstract":"Evaluating autonomous vehicles with controllability enables scalable testing in counterfactual or structured settings, enhancing both efficiency and safety. We introduce LangTraj, a language-conditioned scene-diffusion model that simulates the joint behavior of all agents in traffic scenarios. By conditioning on natural language inputs, LangTraj provides flexible and intuitive control over interactive behaviors, generating nuanced and realistic scenarios. Unlike prior approaches that depend on domain-specific guidance functions, LangTraj incorporates language conditioning during training, facilitating more intuitive traffic simulation control. We propose a novel closed-loop training strategy for diffusion models, explicitly tailored to enhance stability and realism during closed-loop simulation. To support language-conditioned simulation, we develop Inter-Drive, a large-scale dataset with diverse and interactive labels for training language-conditioned diffusion models. Our dataset is built upon a scalable pipeline for annotating agent-agent interactions and single-agent behaviors, ensuring rich and varied supervision. Validated on the Waymo Motion Dataset, LangTraj demonstrates strong performance in realism, language controllability, and language-conditioned safety-critical simulation, establishing a new paradigm for flexible and scalable autonomous vehicle testing.\n        △ Less",
        "authors":[
            "Wei-Jer Chang",
            "Wei Zhan",
            "Masayoshi Tomizuka",
            "Manmohan Chandraker",
            "Francesco Pittaluga"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Strengthening Anomaly Awareness",
        "abstract":"We present a refined version of the Anomaly Awareness framework for enhancing unsupervised anomaly detection. Our approach introduces minimal supervision into Variational Autoencoders (VAEs) through a two-stage training strategy: the model is first trained in an unsupervised manner on background data, and then fine-tuned using a small sample of labeled anomalies to encourage larger reconstruction errors for anomalous samples.\n  We validate the method across diverse domains, including the MNIST dataset with synthetic anomalies, network intrusion data from the CICIDS benchmark, collider physics data from the LHCO2020 dataset, and simulated events from the Standard Model Effective Field Theory (SMEFT). The latter provides a realistic example of subtle kinematic deviations in Higgs boson production. In all cases, the model demonstrates improved sensitivity to unseen anomalies, achieving better separation between normal and anomalous samples. These results indicate that even limited anomaly information, when incorporated through targeted fine-tuning, can substantially improve the generalization and performance of unsupervised models for anomaly detection.\n        △ Less",
        "authors":[
            "Adam Banda",
            "Charanjit K. Khosa",
            "Veronica Sanz"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FACT: Foundation Model for Assessing Cancer Tissue Margins with Mass Spectrometry",
        "abstract":"Purpose: Accurately classifying tissue margins during cancer surgeries is crucial for ensuring complete tumor removal. Rapid Evaporative Ionization Mass Spectrometry (REIMS), a tool for real-time intraoperative margin assessment, generates spectra that require machine learning models to support clinical decision-making. However, the scarcity of labeled data in surgical contexts presents a significant challenge. This study is the first to develop a foundation model tailored specifically for REIMS data, addressing this limitation and advancing real-time surgical margin assessment. Methods: We propose FACT, a Foundation model for Assessing Cancer Tissue margins. FACT is an adaptation of a foundation model originally designed for text-audio association, pretrained using our proposed supervised contrastive approach based on triplet loss. An ablation study is performed to compare our proposed model against other models and pretraining methods. Results: Our proposed model significantly improves the classification performance, achieving state-of-the-art performance with an AUROC of $82.4\\% \\pm 0.8$. The results demonstrate the advantage of our proposed pretraining method and selected backbone over the self-supervised and semi-supervised baselines and alternative models. Conclusion: Our findings demonstrate that foundation models, adapted and pretrained using our novel approach, can effectively classify REIMS data even with limited labeled examples. This highlights the viability of foundation models for enhancing real-time surgical margin assessment, particularly in data-scarce clinical environments.\n        △ Less",
        "authors":[
            "Mohammad Farahmand",
            "Amoon Jamzad",
            "Fahimeh Fooladgar",
            "Laura Connolly",
            "Martin Kaufmann",
            "Kevin Yi Mi Ren",
            "John Rudan",
            "Doug McKay",
            "Gabor Fichtinger",
            "Parvin Mousavi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FEAT: Free energy Estimators with Adaptive Transport",
        "abstract":"We present Free energy Estimators with Adaptive Transport (FEAT), a novel framework for free energy estimation -- a critical challenge across scientific domains. FEAT leverages learned transports implemented via stochastic interpolants and provides consistent, minimum-variance estimators based on escorted Jarzynski equality and controlled Crooks theorem, alongside variational upper and lower bounds on free energy differences. Unifying equilibrium and non-equilibrium methods under a single theoretical framework, FEAT establishes a principled foundation for neural free energy calculations. Experimental validation on toy examples, molecular simulations, and quantum field theory demonstrates improvements over existing learning-based methods.\n        △ Less",
        "authors":[
            "Jiajun He",
            "Yuanqi Du",
            "Francisco Vargas",
            "Yuanqing Wang",
            "Carla P. Gomes",
            "José Miguel Hernández-Lobato",
            "Eric Vanden-Eijnden"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Autonomous Driving Systems with On-Board Deployed Large Language Models",
        "abstract":"Neural Networks (NNs) trained through supervised learning struggle with managing edge-case scenarios common in real-world driving due to the intractability of exhaustive datasets covering all edge-cases, making knowledge-driven approaches, akin to how humans intuitively detect unexpected driving behavior, a suitable complement to data-driven methods. This work proposes a hybrid architecture combining low-level Model Predictive Controller (MPC) with locally deployed Large Language Models (LLMs) to enhance decision-making and Human Machine Interaction (HMI). The DecisionxLLM module evaluates robotic state information against natural language instructions to ensure adherence to desired driving behavior. The MPCxLLM module then adjusts MPC parameters based on LLM-generated insights, achieving control adaptability while preserving the safety and constraint guarantees of traditional MPC systems. Further, to enable efficient on-board deployment and to eliminate dependency on cloud connectivity, we shift processing to the on-board computing platform: We propose an approach that exploits Retrieval Augmented Generation (RAG), Low Rank Adaptation (LoRA) fine-tuning, and quantization. Experimental results demonstrate that these enhancements yield significant improvements in reasoning accuracy by up to 10.45%, control adaptability by as much as 52.2%, and up to 10.5x increase in computational efficiency (tokens\/s), validating the proposed framework's practicality for real-time deployment even on down-scaled robotic platforms. This work bridges high-level decision-making with low-level control adaptability, offering a synergistic framework for knowledge-driven and adaptive Autonomous Driving Systems (ADS).\n        △ Less",
        "authors":[
            "Nicolas Baumann",
            "Cheng Hu",
            "Paviththiren Sivasothilingam",
            "Haotong Qin",
            "Lei Xie",
            "Michele Magno",
            "Luca Benini"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-output Classification Framework and Frequency Layer Normalization for Compound Fault Diagnosis in Motor",
        "abstract":"This work introduces a multi-output classification (MOC) framework designed for domain adaptation in fault diagnosis, particularly under partially labeled (PL) target domain scenarios and compound fault conditions in rotating machinery. Unlike traditional multi-class classification (MCC) methods that treat each fault combination as a distinct class, the proposed approach independently estimates the severity of each fault type, improving both interpretability and diagnostic accuracy. The model incorporates multi-kernel maximum mean discrepancy (MK-MMD) and entropy minimization (EM) losses to facilitate feature transfer from the source to the target domain. In addition, frequency layer normalization (FLN) is applied to preserve structural properties in the frequency domain, which are strongly influenced by system dynamics and are often stationary with respect to changes in rpm. Evaluations across six domain adaptation cases with PL data demonstrate that MOC outperforms baseline models in macro F1 score. Moreover, MOC consistently achieves better classification performance for individual fault types, and FLN shows superior adaptability compared to other normalization techniques.\n        △ Less",
        "authors":[
            "Wonjun Yi",
            "Yong-Hwa Park"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learned enclosure method for experimental EIT data",
        "abstract":"Electrical impedance tomography (EIT) is a non-invasive imaging method with diverse applications, including medical imaging and non-destructive testing. The inverse problem of reconstructing internal electrical conductivity from boundary measurements is nonlinear and highly ill-posed, making it difficult to solve accurately. In recent years, there has been growing interest in combining analytical methods with machine learning to solve inverse problems. In this paper, we propose a method for estimating the convex hull of inclusions from boundary measurements by combining the enclosure method proposed by Ikehata with neural networks. We demonstrate its performance using experimental data. Compared to the classical enclosure method with least squares fitting, the learned convex hull achieves superior performance on both simulated and experimental data.\n        △ Less",
        "authors":[
            "Sara Sippola",
            "Siiri Rautio",
            "Andreas Hauptmann",
            "Takanori Ide",
            "Samuli Siltanen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Position Paper: Rethinking Privacy in RL for Sequential Decision-making in the Age of LLMs",
        "abstract":"The rise of reinforcement learning (RL) in critical real-world applications demands a fundamental rethinking of privacy in AI systems. Traditional privacy frameworks, designed to protect isolated data points, fall short for sequential decision-making systems where sensitive information emerges from temporal patterns, behavioral strategies, and collaborative dynamics. Modern RL paradigms, such as federated RL (FedRL) and RL with human feedback (RLHF) in large language models (LLMs), exacerbate these challenges by introducing complex, interactive, and context-dependent learning environments that traditional methods do not address. In this position paper, we argue for a new privacy paradigm built on four core principles: multi-scale protection, behavioral pattern protection, collaborative privacy preservation, and context-aware adaptation. These principles expose inherent tensions between privacy, utility, and interpretability that must be navigated as RL systems become more pervasive in high-stakes domains like healthcare, autonomous vehicles, and decision support systems powered by LLMs. To tackle these challenges, we call for the development of new theoretical frameworks, practical mechanisms, and rigorous evaluation methodologies that collectively enable effective privacy protection in sequential decision-making systems.\n        △ Less",
        "authors":[
            "Flint Xiaofeng Fan",
            "Cheston Tan",
            "Roger Wattenhofer",
            "Yew-Soon Ong"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RAID: An In-Training Defense against Attribute Inference Attacks in Recommender Systems",
        "abstract":"In various networks and mobile applications, users are highly susceptible to attribute inference attacks, with particularly prevalent occurrences in recommender systems. Attackers exploit partially exposed user profiles in recommendation models, such as user embeddings, to infer private attributes of target users, such as gender and political views. The goal of defenders is to mitigate the effectiveness of these attacks while maintaining recommendation performance. Most existing defense methods, such as differential privacy and attribute unlearning, focus on post-training settings, which limits their capability of utilizing training data to preserve recommendation performance. Although adversarial training extends defenses to in-training settings, it often struggles with convergence due to unstable training processes. In this paper, we propose RAID, an in-training defense method against attribute inference attacks in recommender systems. In addition to the recommendation objective, we define a defensive objective to ensure that the distribution of protected attributes becomes independent of class labels, making users indistinguishable from attribute inference attacks. Specifically, this defensive objective aims to solve a constrained Wasserstein barycenter problem to identify the centroid distribution that makes the attribute indistinguishable while complying with recommendation performance constraints. To optimize our proposed objective, we use optimal transport to align users with the centroid distribution. We conduct extensive experiments on four real-world datasets to evaluate RAID. The experimental results validate the effectiveness of RAID and demonstrate its significant superiority over existing methods in multiple aspects.\n        △ Less",
        "authors":[
            "Xiaohua Feng",
            "Yuyuan Li",
            "Fengyuan Yu",
            "Ke Xiong",
            "Junjie Fang",
            "Li Zhang",
            "Tianyu Du",
            "Chaochao Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reward Distance Comparisons Under Transition Sparsity",
        "abstract":"Reward comparisons are vital for evaluating differences in agent behaviors induced by a set of reward functions. Most conventional techniques utilize the input reward functions to learn optimized policies, which are then used to compare agent behaviors. However, learning these policies can be computationally expensive and can also raise safety concerns. Direct reward comparison techniques obviate policy learning but suffer from transition sparsity, where only a small subset of transitions are sampled due to data collection challenges and feasibility constraints. Existing state-of-the-art direct reward comparison methods are ill-suited for these sparse conditions since they require high transition coverage, where the majority of transitions from a given coverage distribution are sampled. When this requirement is not satisfied, a distribution mismatch between sampled and expected transitions can occur, leading to significant errors. This paper introduces the Sparsity Resilient Reward Distance (SRRD) pseudometric, designed to eliminate the need for high transition coverage by accommodating diverse sample distributions, which are common under transition sparsity. We provide theoretical justification for SRRD's robustness and conduct experiments to demonstrate its practical efficacy across multiple domains.\n        △ Less",
        "authors":[
            "Clement Nyanhongo",
            "Bruno Miranda Henrique",
            "Eugene Santos"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cross-cultural Deployment of Autonomous Vehicles Using Data-light Inverse Reinforcement Learning",
        "abstract":"More than the adherence to specific traffic regulations, driving culture touches upon a more implicit part - an informal, conventional, collective behavioral pattern followed by drivers - that varies across countries, regions, and even cities. Such cultural divergence has become one of the biggest challenges in deploying autonomous vehicles (AVs) across diverse regions today. The current emergence of data-driven methods has shown a potential solution to enable culture-compatible driving through learning from data, but what if some underdeveloped regions cannot provide sufficient local data to inform driving culture? This issue is particularly significant for a broader global AV market. Here, we propose a cross-cultural deployment scheme for AVs, called data-light inverse reinforcement learning, designed to re-calibrate culture-specific AVs and assimilate them into other cultures. First, we report the divergence in driving cultures through a comprehensive comparative analysis of naturalistic driving datasets on highways from three countries: Germany, China, and the USA. Then, we demonstrate the effectiveness of our scheme by testing the expeditious cross-cultural deployment across these three countries, with cumulative testing mileage of over 56084 km. The performance is particularly advantageous when cross-cultural deployment is carried out without affluent local data. Results show that we can reduce the dependence on local data by a margin of 98.67% at best. This study is expected to bring a broader, fairer AV global market, particularly in those regions that lack enough local data to develop culture-compatible AVs.\n        △ Less",
        "authors":[
            "Hongliang Lu",
            "Shuqi Shen",
            "Junjie Yang",
            "Chao Lu",
            "Xinhu Zheng",
            "Hai Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Counterfactual Fairness Evaluation of Machine Learning Models on Educational Datasets",
        "abstract":"As machine learning models are increasingly used in educational settings, from detecting at-risk students to predicting student performance, algorithmic bias and its potential impacts on students raise critical concerns about algorithmic fairness. Although group fairness is widely explored in education, works on individual fairness in a causal context are understudied, especially on counterfactual fairness. This paper explores the notion of counterfactual fairness for educational data by conducting counterfactual fairness analysis of machine learning models on benchmark educational datasets. We demonstrate that counterfactual fairness provides meaningful insight into the causality of sensitive attributes and causal-based individual fairness in education.\n        △ Less",
        "authors":[
            "Woojin Kim",
            "Hyeoncheol Kim"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Timing Analysis Agent: Autonomous Multi-Corner Multi-Mode (MCMM) Timing Debugging with Timing Debug Relation Graph",
        "abstract":"Timing analysis is an essential and demanding verification method for Very Large Scale Integrated (VLSI) circuit design and optimization. In addition, it also serves as the cornerstone of the final sign-off, determining whether the chip is ready to be sent to the semiconductor foundry for fabrication. Recently, as the technology advance relentlessly, smaller metal pitches and the increasing number of devices have led to greater challenges and longer turn-around-time for experienced human designers to debug timing issues from the Multi-Corner Multi-Mode (MCMM) timing reports. As a result, an efficient and intelligent methodology is highly necessary and essential for debugging timing issues and reduce the turnaround times. Recently, Large Language Models (LLMs) have shown great promise across various tasks in language understanding and interactive decision-making, incorporating reasoning and actions. In this work, we propose a timing analysis agent, that is empowered by multi-LLMs task solving, and incorporates a novel hierarchical planning and solving flow to automate the analysis of timing reports from commercial tool. In addition, we build a Timing Debug Relation Graph (TDRG) that connects the reports with the relationships of debug traces from experienced timing engineers. The timing analysis agent employs the novel Agentic Retrieval Augmented Generation (RAG) approach, that includes agent and coding to retrieve data accurately, on the developed TDRG. In our studies, the proposed timing analysis agent achieves an average 98% pass-rate on a single-report benchmark and a 90% pass-rate for multi-report benchmark from industrial designs, demonstrating its effectiveness and adaptability.\n        △ Less",
        "authors":[
            "Jatin Nainani",
            "Chia-Tung Ho",
            "Anirudh Dhurka",
            "Haoxing Ren"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LLM-based AI Agent for Sizing of Analog and Mixed Signal Circuit",
        "abstract":"The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose an LLM-based AI agent for AMS circuit design to assist in the sizing process. By integrating LLMs with external circuit simulation tools and data analysis functions and employing prompt engineering strategies, the agent successfully optimized multiple circuits to achieve target performance metrics. We evaluated the performance of different LLMs to assess their applicability and optimization effectiveness across seven basic circuits, and selected the best-performing model Claude 3.5 Sonnet for further exploration on an operational amplifier, with complementary input stage and class AB output stage. This circuit was evaluated against nine performance metrics, and we conducted experiments under three distinct performance requirement groups. A success rate of up to 60% was achieved for reaching the target requirements. Overall, this work demonstrates the potential of LLMs to improve AMS circuit design.\n        △ Less",
        "authors":[
            "Chang Liu",
            "Emmanuel A. Olowe",
            "Danial Chitnis"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Probabilistic Task Parameterization of Tool-Tissue Interaction via Sparse Landmarks Tracking in Robotic Surgery",
        "abstract":"Accurate modeling of tool-tissue interactions in robotic surgery requires precise tracking of deformable tissues and integration of surgical domain knowledge. Traditional methods rely on labor-intensive annotations or rigid assumptions, limiting flexibility. We propose a framework combining sparse keypoint tracking and probabilistic modeling that propagates expert-annotated landmarks across endoscopic frames, even with large tissue deformations. Clustered tissue keypoints enable dynamic local transformation construction via PCA, and tool poses, tracked similarly, are expressed relative to these frames. Embedding these into a Task-Parameterized Gaussian Mixture Model (TP-GMM) integrates data-driven observations with labeled clinical expertise, effectively predicting relative tool-tissue poses and enhancing visual understanding of robotic surgical motions directly from video data.\n        △ Less",
        "authors":[
            "Yiting Wang",
            "Yunxin Fan",
            "Fei Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Attention GhostUNet++: Enhanced Segmentation of Adipose Tissue and Liver in CT Images",
        "abstract":"Accurate segmentation of abdominal adipose tissue, including subcutaneous (SAT) and visceral adipose tissue (VAT), along with liver segmentation, is essential for understanding body composition and associated health risks such as type 2 diabetes and cardiovascular disease. This study proposes Attention GhostUNet++, a novel deep learning model incorporating Channel, Spatial, and Depth Attention mechanisms into the Ghost UNet++ bottleneck for automated, precise segmentation. Evaluated on the AATTCT-IDS and LiTS datasets, the model achieved Dice coefficients of 0.9430 for VAT, 0.9639 for SAT, and 0.9652 for liver segmentation, surpassing baseline models. Despite minor limitations in boundary detail segmentation, the proposed model significantly enhances feature refinement, contextual understanding, and computational efficiency, offering a robust solution for body composition analysis. The implementation of the proposed Attention GhostUNet++ model is available at:https:\/\/github.com\/MansoorHayat777\/Attention-GhostUNetPlusPlus.\n        △ Less",
        "authors":[
            "Mansoor Hayat",
            "Supavadee Aramvith",
            "Subrata Bhattacharjee",
            "Nouman Ahmad"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CI-RKM: A Class-Informed Approach to Robust Restricted Kernel Machines",
        "abstract":"Restricted kernel machines (RKMs) represent a versatile and powerful framework within the kernel machine family, leveraging conjugate feature duality to address a wide range of machine learning tasks, including classification, regression, and feature learning. However, their performance can degrade significantly in the presence of noise and outliers, which compromises robustness and predictive accuracy. In this paper, we propose a novel enhancement to the RKM framework by integrating a class-informed weighted function. This weighting mechanism dynamically adjusts the contribution of individual training points based on their proximity to class centers and class-specific characteristics, thereby mitigating the adverse effects of noisy and outlier data. By incorporating weighted conjugate feature duality and leveraging the Schur complement theorem, we introduce the class-informed restricted kernel machine (CI-RKM), a robust extension of the RKM designed to improve generalization and resilience to data imperfections. Experimental evaluations on benchmark datasets demonstrate that the proposed CI-RKM consistently outperforms existing baselines, achieving superior classification accuracy and enhanced robustness against noise and outliers. Our proposed method establishes a significant advancement in the development of kernel-based learning models, addressing a core challenge in the field.\n        △ Less",
        "authors":[
            "Ritik Mishra",
            "Mushir Akhtar",
            "M. Tanveer"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MultiCore+TPU Accelerated Multi-Modal TinyML for Livestock Behaviour Recognition",
        "abstract":"The advancement of technology has revolutionised the agricultural industry, transitioning it from labour-intensive farming practices to automated, AI-powered management systems. In recent years, more intelligent livestock monitoring solutions have been proposed to enhance farming efficiency and productivity. This work presents a novel approach to animal activity recognition and movement tracking, leveraging tiny machine learning (TinyML) techniques, wireless communication framework, and microcontroller platforms to develop an efficient, cost-effective livestock sensing system. It collects and fuses accelerometer data and vision inputs to build a multi-modal network for three tasks: image classification, object detection, and behaviour recognition. The system is deployed and evaluated on commercial microcontrollers for real-time inference using embedded applications, demonstrating up to 270$\\times$ model size reduction, less than 80ms response latency, and on-par performance comparable to existing methods. The incorporation of the TinyML technique allows for seamless data transmission between devices, benefiting use cases in remote locations with poor Internet connectivity. This work delivers a robust, scalable IoT-edge livestock monitoring solution adaptable to diverse farming needs, offering flexibility for future extensions.\n        △ Less",
        "authors":[
            "Qianxue Zhang",
            "Eiman Kanjo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Elucidating the Design Space of Multimodal Protein Language Models",
        "abstract":"Multimodal protein language models (PLMs) integrate sequence and token-based structural information, serving as a powerful foundation for protein modeling, generation, and design. However, the reliance on tokenizing 3D structures into discrete tokens causes substantial loss of fidelity about fine-grained structural details and correlations. In this paper, we systematically elucidate the design space of multimodal PLMs to overcome their limitations. We identify tokenization loss and inaccurate structure token predictions by the PLMs as major bottlenecks. To address these, our proposed design space covers improved generative modeling, structure-aware architectures and representation learning, and data exploration. Our advancements approach finer-grained supervision, demonstrating that token-based multimodal PLMs can achieve robust structural modeling. The effective design methods dramatically improve the structure generation diversity, and notably, folding abilities of our 650M model by reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B baselines and on par with the specialized folding models.\n        △ Less",
        "authors":[
            "Cheng-Yen Hsieh",
            "Xinyou Wang",
            "Daiheng Zhang",
            "Dongyu Xue",
            "Fei Ye",
            "Shujian Huang",
            "Zaixiang Zheng",
            "Quanquan Gu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Clean Slate for Offline Reinforcement Learning",
        "abstract":"Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches within a single, comprehensive hyperparameter space, enabling algorithm development in a shared hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https:\/\/github.com\/EmptyJackson\/unifloral.\n        △ Less",
        "authors":[
            "Matthew Thomas Jackson",
            "Uljad Berdica",
            "Jarek Liesen",
            "Shimon Whiteson",
            "Jakob Nicolaus Foerster"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Early Impacts of M365 Copilot",
        "abstract":"Advances in generative AI have rapidly expanded the potential of computers to perform or assist in a wide array of tasks traditionally performed by humans. We analyze a large, real-world randomized experiment of over 6,000 workers at 56 firms to present some of the earliest evidence on how these technologies are changing the way knowledge workers do their jobs. We find substantial time savings on common core tasks across a wide range of industries and occupations: workers who make use of this technology spent half an hour less reading email each week and completed documents 12% faster. Despite the newness of the technology, nearly 40% of workers who were given access to the tool used it regularly in their work throughout the 6-month study.\n        △ Less",
        "authors":[
            "Eleanor Wiske Dillon",
            "Sonia Jaffe",
            "Sida Peng",
            "Alexia Cambon"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TextArena",
        "abstract":"TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https:\/\/github.com\/LeonGuertler\/TextArena and https:\/\/www.textarena.ai\/.\n        △ Less",
        "authors":[
            "Leon Guertler",
            "Bobby Cheng",
            "Simon Yu",
            "Bo Liu",
            "Leshem Choshen",
            "Cheston Tan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mamba-Based Ensemble learning for White Blood Cell Classification",
        "abstract":"White blood cell (WBC) classification assists in assessing immune health and diagnosing various diseases, yet manual classification is labor-intensive and prone to inconsistencies. Recent advancements in deep learning have shown promise over traditional methods; however, challenges such as data imbalance and the computational demands of modern technologies, such as Transformer-based models which do not scale well with input size, limit their practical application. This paper introduces a novel framework that leverages Mamba models integrated with ensemble learning to improve WBC classification. Mamba models, known for their linear complexity, provide a scalable alternative to Transformer-based approaches, making them suitable for deployment in resource-constrained environments. Additionally, we introduce a new WBC dataset, Chula-WBC-8, for benchmarking. Our approach not only validates the effectiveness of Mamba models in this domain but also demonstrates their potential to significantly enhance classification efficiency without compromising accuracy. The source code can be found at https:\/\/github.com\/LewisClifton\/Mamba-WBC-Classification.\n        △ Less",
        "authors":[
            "Lewis Clifton",
            "Xin Tian",
            "Duangdao Palasuwan",
            "Phandee Watanaboonyongcharoen",
            "Ponlapat Rojnuckarin",
            "Nantheera Anantrasirichai"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Shifting Work Patterns with Generative AI",
        "abstract":"We present evidence on how generative AI changes the work patterns of knowledge workers using data from a 6-month-long, cross-industry, randomized field experiment. Half of the 6,000 workers in the study received access to a generative AI tool integrated into the applications they already used for emails, document creation, and meetings. We find that access to the AI tool during the first year of its release primarily impacted behaviors that could be changed independently and not behaviors that required coordination to change: workers who used the tool spent 3 fewer hours, or 25% less time on email each week (intent to treat estimate is 1.4 hours) and seemed to complete documents moderately faster, but did not significantly change time spent in meetings.\n        △ Less",
        "authors":[
            "Eleanor Wiske Dillon",
            "Sonia Jaffe",
            "Nicole Immorlica",
            "Christopher T. Stanton"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Out-of-Distribution Detection with Extended Logit Normalization",
        "abstract":"Out-of-distribution (OOD) detection is essential for the safe deployment of machine learning models. Recent advances have explored improved classification losses and representation learning strategies to enhance OOD detection. However, these methods are often tailored to specific post-hoc detection techniques, limiting their generalizability. In this work, we identify a critical issue in Logit Normalization (LogitNorm), which inhibits its effectiveness in improving certain post-hoc OOD detection methods. To address this, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel hyperparameter-free formulation that significantly benefits a wide range of post-hoc detection methods. By incorporating feature distance-awareness to LogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and in-distribution (ID) confidence calibration than its predecessor. Extensive experiments across standard benchmarks demonstrate that our approach outperforms state-of-the-art training-time methods in OOD detection while maintaining strong ID classification accuracy.\n        △ Less",
        "authors":[
            "Yifan Ding",
            "Xixi Liu",
            "Jonas Unger",
            "Gabriel Eilertsen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Predicting Wave Dynamics using Deep Learning with Multistep Integration Inspired Attention and Physics-Based Loss Decomposition",
        "abstract":"In this paper, we present a physics-based deep learning framework for data-driven prediction of wave propagation in fluid media. The proposed approach, termed Multistep Integration-Inspired Attention (MI2A), combines a denoising-based convolutional autoencoder for reduced latent representation with an attention-based recurrent neural network with long-short-term memory cells for time evolution of reduced coordinates. This proposed architecture draws inspiration from classical linear multistep methods to enhance stability and long-horizon accuracy in latent-time integration. Despite the efficiency of hybrid neural architectures in modeling wave dynamics, autoregressive predictions are often prone to accumulating phase and amplitude errors over time. To mitigate this issue within the MI2A framework, we introduce a novel loss decomposition strategy that explicitly separates the training loss function into distinct phase and amplitude components. We assess the performance of MI2A against two baseline reduced-order models trained with standard mean-squared error loss: a sequence-to-sequence recurrent neural network and a variant using Luong-style attention. To demonstrate the effectiveness of the MI2A model, we consider three benchmark wave propagation problems of increasing complexity, namely one-dimensional linear convection, the nonlinear viscous Burgers equation, and the two-dimensional Saint-Venant shallow water system. Our results demonstrate that the MI2A framework significantly improves the accuracy and stability of long-term predictions, accurately preserving wave amplitude and phase characteristics. Compared to the standard long-short term memory and attention-based models, MI2A-based deep learning exhibits superior generalization and temporal accuracy, making it a promising tool for real-time wave modeling.\n        △ Less",
        "authors":[
            "Indu Kant Deo",
            "Rajeev K. Jaiman"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models",
        "abstract":"Masculine defaults are widely recognized as a significant type of gender bias, but they are often unseen as they are under-researched. Masculine defaults involve three key parts: (i) the cultural context, (ii) the masculine characteristics or behaviors, and (iii) the reward for, or simply acceptance of, those masculine characteristics or behaviors. In this work, we study discourse-based masculine defaults, and propose a twofold framework for (i) the large-scale discovery and analysis of gendered discourse words in spoken content via our Gendered Discourse Correlation Framework (GDCF); and (ii) the measurement of the gender bias associated with these gendered discourse words in LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus our study on podcasts, a popular and growing form of social media, analyzing 15,117 podcast episodes. We analyze correlations between gender and discourse words -- discovered via LDA and BERTopic -- to automatically form gendered discourse word lists. We then study the prevalence of these gendered discourse words in domain-specific contexts, and find that gendered discourse-based masculine defaults exist in the domains of business, technology\/politics, and video games. Next, we study the representation of these gendered discourse words from a state-of-the-art LLM embedding model from OpenAI, and find that the masculine discourse words have a more stable and robust representation than the feminine discourse words, which may result in better system performance on downstream tasks for men. Hence, men are rewarded for their discourse patterns with better system performance by one of the state-of-the-art language models -- and this embedding disparity is a representational harm and a masculine default.\n        △ Less",
        "authors":[
            "Maria Teleki",
            "Xiangjue Dong",
            "Haoran Liu",
            "James Caverlee"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Dual-Space Framework for General Knowledge Distillation of Large Language Models",
        "abstract":"Knowledge distillation (KD) is a promising solution to compress large language models (LLMs) by transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output distributions of the teacher model and the student model to transfer more information. However, we reveal that the current white-box KD framework exhibits two limitations: a) bridging probability distributions from different output spaces will limit the similarity between the teacher model and the student model; b) this framework cannot be applied to LLMs with different vocabularies. One of the root causes for these limitations is that the distributions from the teacher and the student for KD are output by different prediction heads, which yield distributions in different output spaces and dimensions. Therefore, in this paper, we propose a dual-space knowledge distillation (DSKD) framework that unifies the prediction heads of the teacher and the student models for KD. Specifically, we first introduce two projectors with ideal initialization to project the teacher\/student hidden states into the student\/teacher representation spaces. After this, the hidden states from different models can share the same head and unify the output spaces of the distributions. Furthermore, we develop an exact token alignment (ETA) algorithm to align the same tokens in two differently-tokenized sequences. Based on the above, our DSKD framework is a general KD framework that supports both off-policy and on-policy KD, and KD between any two LLMs regardless of their vocabularies. Extensive experiments on instruction-following, mathematical reasoning, and code generation benchmarks show that DSKD significantly outperforms existing methods based on the current white-box KD framework and surpasses other cross-tokenizer KD methods for LLMs with different vocabularies.\n        △ Less",
        "authors":[
            "Xue Zhang",
            "Songming Zhang",
            "Yunlong Liang",
            "Fandong Meng",
            "Yufeng Chen",
            "Jinan Xu",
            "Jie Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HeatSense: Intelligent Thermal Anomaly Detection for Securing NoC-Enabled MPSoCs",
        "abstract":"Multi-Processor System-on-Chips (MPSoCs) are highly vulnerable to thermal attacks that manipulate dynamic thermal management systems. To counter this, we propose an adaptive real-time monitoring mechanism that detects abnormal thermal patterns in chip tiles. Our design space exploration helped identify key thermal features for an efficient anomaly detection module to be implemented at routers of network-enabled MPSoCs. To minimize hardware overhead, we employ weighted moving average (WMA) calculations and bit-shift operations, ensuring a lightweight yet effective implementation. By defining a spectrum of abnormal behaviors, our system successfully detects and mitigates malicious temperature fluctuations, reducing severe cases from 3.00°C to 1.9°C. The anomaly detection module achieves up to 82% of accuracy in detecting thermal attacks, which is only 10-15% less than top-performing machine learning (ML) models like Random Forest. However, our approach reduces hardware usage by up to 75% for logic resources and 100% for specialized resources, making it significantly more efficient than ML-based solutions. This method provides a practical, low-cost solution for resource-constrained environments, ensuring resilience against thermal attacks while maintaining system performance.\n        △ Less",
        "authors":[
            "Mahdi Hasanzadeh",
            "Kasem Khalil",
            "Cynthia Sturton",
            "Ahmad Patooghy"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Embodied World Models Emerge from Navigational Task in Open-Ended Environments",
        "abstract":"Understanding how artificial systems can develop spatial awareness and reasoning has long been a challenge in AI research. Traditional models often rely on passive observation, but embodied cognition theory suggests that deeper understanding emerges from active interaction with the environment. This study investigates whether neural networks can autonomously internalize spatial concepts through interaction, focusing on planar navigation tasks. Using Gated Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we show that agents can learn to encode spatial properties like direction, distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS) to model the agent-environment interaction as a closed dynamical system, revealing stable limit cycles that correspond to optimal navigation strategies. Ridge Representation allows us to map navigation paths into a fixed-dimensional behavioral space, enabling comparison with neural states. Canonical Correlation Analysis (CCA) confirms strong alignment between these representations, suggesting that the agent's neural states actively encode spatial knowledge. Intervention experiments further show that specific neural dimensions are causally linked to navigation performance. This work provides an approach to bridging the gap between action and perception in AI, offering new insights into building adaptive, interpretable models that can generalize across complex environments. The causal validation of neural representations also opens new avenues for understanding and controlling the internal mechanisms of AI systems, pushing the boundaries of how machines learn and reason in dynamic, real-world scenarios.\n        △ Less",
        "authors":[
            "Li Jin",
            "Liu Jia"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robustness and sex differences in skin cancer detection: logistic regression vs CNNs",
        "abstract":"Deep learning has been reported to achieve high performances in the detection of skin cancer, yet many challenges regarding the reproducibility of results and biases remain. This study is a replication (different data, same analysis) of a study on Alzheimer's disease [28] which studied robustness of logistic regression (LR) and convolutional neural networks (CNN) across patient sexes. We explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset with LR trained on handcrafted features reflecting dermatological guidelines (ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We evaluate these models in alignment with [28]: across multiple training datasets with varied sex composition to determine their robustness. Our results show that both the LR and the CNN were robust to the sex distributions, but the results also revealed that the CNN had a significantly higher accuracy (ACC) and area under the receiver operating characteristics (AUROC) for male patients than for female patients. We hope these findings to contribute to the growing field of investigating potential bias in popular medical machine learning methods. The data and relevant scripts to reproduce our results can be found in our Github.\n        △ Less",
        "authors":[
            "Nikolette Pedersen",
            "Regitze Sydendal",
            "Andreas Wulff",
            "Ralf Raumanns",
            "Eike Petersen",
            "Veronika Cheplygina"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Measures of Variability for Risk-averse Policy Gradient",
        "abstract":"Risk-averse reinforcement learning (RARL) is critical for decision-making under uncertainty, which is especially valuable in high-stake applications. However, most existing works focus on risk measures, e.g., conditional value-at-risk (CVaR), while measures of variability remain underexplored. In this paper, we comprehensively study nine common measures of variability, namely Variance, Gini Deviation, Mean Deviation, Mean-Median Deviation, Standard Deviation, Inter-Quantile Range, CVaR Deviation, Semi_Variance, and Semi_Standard Deviation. Among them, four metrics have not been previously studied in RARL. We derive policy gradient formulas for these unstudied metrics, improve gradient estimation for Gini Deviation, analyze their gradient properties, and incorporate them with the REINFORCE and PPO frameworks to penalize the dispersion of returns.\n  Our empirical study reveals that variance-based metrics lead to unstable policy updates. In contrast, CVaR Deviation and Gini Deviation show consistent performance across different randomness and evaluation domains, achieving high returns while effectively learning risk-averse policies. Mean Deviation and Semi_Standard Deviation are also competitive across different scenarios. This work provides a comprehensive overview of variability measures in RARL, offering practical insights for risk-aware decision-making and guiding future research on risk metrics and RARL algorithms.\n        △ Less",
        "authors":[
            "Yudong Luo",
            "Yangchen Pan",
            "Jiaqi Tan",
            "Pascal Poupart"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MLPs and KANs for data-driven learning in physical problems: A performance comparison",
        "abstract":"There is increasing interest in solving partial differential equations (PDEs) by casting them as machine learning problems. Recently, there has been a spike in exploring Kolmogorov-Arnold Networks (KANs) as an alternative to traditional neural networks represented by Multi-Layer Perceptrons (MLPs). While showing promise, their performance advantages in physics-based problems remain largely unexplored. Several critical questions persist: Can KANs capture complex physical dynamics and under what conditions might they outperform traditional architectures? In this work, we present a comparative study of KANs and MLPs for learning physical systems governed by PDEs. We assess their performance when applied in deep operator networks (DeepONet) and graph network-based simulators (GNS), and test them on physical problems that vary significantly in scale and complexity. Drawing inspiration from the Kolmogorov Representation Theorem, we examine the behavior of KANs and MLPs across shallow and deep network architectures. Our results reveal that although KANs do not consistently outperform MLPs when configured as deep neural networks, they demonstrate superior expressiveness in shallow network settings, significantly outpacing MLPs in accuracy over our test cases. This suggests that KANs are a promising choice, offering a balance of efficiency and accuracy in applications involving physical systems.\n        △ Less",
        "authors":[
            "Raghav Pant",
            "Sikan Li",
            "Xingjian Li",
            "Hassan Iqbal",
            "Krishna Kumar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Property Inheritance for Subtensors in Tensor Train Decompositions",
        "abstract":"Tensor dimensionality reduction is one of the fundamental tools for modern data science. To address the high computational overhead, fiber-wise sampled subtensors that preserve the original tensor rank are often used in designing efficient and scalable tensor dimensionality reduction. However, the theory of property inheritance for subtensors is still underdevelopment, that is, how the essential properties of the original tensor will be passed to its subtensors. This paper theoretically studies the property inheritance of the two key tensor properties, namely incoherence and condition number, under the tensor train setting. We also show how tensor train rank is preserved through fiber-wise sampling. The key parameters introduced in theorems are numerically evaluated under various settings. The results show that the properties of interest can be well preserved to the subtensors formed via fiber-wise sampling. Overall, this paper provides several handy analytic tools for developing efficient tensor analysis\n        △ Less",
        "authors":[
            "HanQin Cai",
            "Longxiu Huang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DataDecide: How to Predict Best Pretraining Data with Small Experiments",
        "abstract":"Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.\n        △ Less",
        "authors":[
            "Ian Magnusson",
            "Nguyen Tai",
            "Ben Bogin",
            "David Heineman",
            "Jena D. Hwang",
            "Luca Soldaini",
            "Akshita Bhagia",
            "Jiacheng Liu",
            "Dirk Groeneveld",
            "Oyvind Tafjord",
            "Noah A. Smith",
            "Pang Wei Koh",
            "Jesse Dodge"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Trajectory Encoding Temporal Graph Networks",
        "abstract":"Temporal Graph Networks (TGNs) have demonstrated significant success in dynamic graph tasks such as link prediction and node classification. Both tasks comprise transductive settings, where the model predicts links among known nodes, and in inductive settings, where it generalises learned patterns to previously unseen nodes. Existing TGN designs face a dilemma under these dual scenarios. Anonymous TGNs, which rely solely on temporal and structural information, offer strong inductive generalisation but struggle to distinguish known nodes. In contrast, non-anonymous TGNs leverage node features to excel in transductive tasks yet fail to adapt to new nodes. To address this challenge, we propose Trajectory Encoding TGN (TETGN). Our approach introduces automatically expandable node identifiers (IDs) as learnable temporal positional features and performs message passing over these IDs to capture each node's historical context. By integrating this trajectory-aware module with a standard TGN using multi-head attention, TETGN effectively balances transductive accuracy with inductive generalisation. Experimental results on three real-world datasets show that TETGN significantly outperforms strong baselines on both link prediction and node classification tasks, demonstrating its ability to unify the advantages of anonymous and non-anonymous models for dynamic graph learning.\n        △ Less",
        "authors":[
            "Jiafeng Xiong",
            "Rizos Sakellariou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and Neural Operators with Domain Decomposition",
        "abstract":"Numerical solvers for partial differential equations (PDEs) face challenges balancing computational cost and accuracy, especially in multiscale and dynamic systems. Neural operators can significantly speed up simulations; however, they often face challenges such as error accumulation and limited generalization in multiphysics problems. This work introduces a novel hybrid framework that integrates physics-informed DeepONet with FEM through domain decomposition. The core innovation lies in adaptively coupling FEM and DeepONet subdomains via a Schwarz alternating method. This methodology strategically allocates computationally demanding regions to a pre-trained Deep Operator Network, while the remaining computational domain is solved through FEM. To address dynamic systems, we integrate the Newmark time-stepping scheme directly into the DeepONet, significantly mitigating error accumulation in long-term simulations. Furthermore, an adaptive subdomain evolution enables the ML-resolved region to expand dynamically, capturing emerging fine-scale features without remeshing. The framework's efficacy has been validated across a range of solid mechanics problems, including static, quasi-static, and dynamic regimes, demonstrating accelerated convergence rates (up to 20% improvement compared to FE-FE approaches), while preserving solution fidelity with error < 1%. Our case studies show that our proposed hybrid solver: (1) maintains solution continuity across subdomain interfaces, (2) reduces computational costs by eliminating fine mesh requirements, (3) mitigates error accumulation in time-dependent simulations, and (4) enables automatic adaptation to evolving physical phenomena. This work bridges the gap between numerical methods and AI-driven surrogates, offering a scalable pathway for high-fidelity simulations in engineering and scientific applications.\n        △ Less",
        "authors":[
            "Wei Wang",
            "Maryam Hakimzadeh",
            "Haihui Ruan",
            "Somdatta Goswami"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution",
        "abstract":"Open Large Language Models (OLLMs) are increasingly leveraged in generative AI applications, posing new challenges for detecting their outputs. We propose OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. OpenTuringBench focuses on a representative set of OLLMs, and features a number of challenging evaluation tasks, including human\/machine-manipulated texts, out-of-domain texts, and texts from previously unseen models. We also provide OTBDetector, a contrastive learning framework to detect and attribute OLLM-based machine-generated texts. Results highlight the relevance and varying degrees of difficulty of the OpenTuringBench tasks, with our detector achieving remarkable capabilities across the various tasks and outperforming most existing detectors. Resources are available on the OpenTuringBench Hugging Face repository at https:\/\/huggingface.co\/datasets\/MLNTeam-Unical\/OpenTuringBench\n        △ Less",
        "authors":[
            "Lucio La Cava",
            "Andrea Tagarelli"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Teaching Large Language Models to Reason through Learning and Forgetting",
        "abstract":"Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it using both successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. While fine-tuning the model with these data might seem straightforward, we identify a critical issue: the model's search capability tends to degrade rapidly if fine-tuning is performed naively. We show that this degradation can be substantially mitigated by employing a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown mathematical reasoning benchmarks show that our approach not only outperforms both standard fine-tuning and inference-time search baselines but also significantly reduces inference time by 180$\\times$.\n        △ Less",
        "authors":[
            "Tianwei Ni",
            "Allen Nie",
            "Sapana Chaudhary",
            "Yao Liu",
            "Huzefa Rangwala",
            "Rasool Fakoor"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Adaptive Dropout Approach for High-Dimensional Bayesian Optimization",
        "abstract":"Bayesian optimization (BO) is a widely used algorithm for solving expensive black-box optimization problems. However, its performance decreases significantly on high-dimensional problems due to the inherent high-dimensionality of the acquisition function. In the proposed algorithm, we adaptively dropout the variables of the acquisition function along the iterations. By gradually reducing the dimension of the acquisition function, the proposed approach has less and less difficulty to optimize the acquisition function. Numerical experiments demonstrate that AdaDropout effectively tackle high-dimensional challenges and improve solution quality where standard Bayesian optimization methods often struggle. Moreover, it achieves superior results when compared with state-of-the-art high-dimensional Bayesian optimization approaches. This work provides a simple yet efficient solution for high-dimensional expensive optimization.\n        △ Less",
        "authors":[
            "Jundi Huang",
            "Dawei Zhan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Erzeugunsgrad, VC-Dimension and Neural Networks with rational activation function",
        "abstract":"The notion of Erzeugungsgrad was introduced by Joos Heintz in 1983 to bound the number of non-empty cells occurring after a process of quantifier elimination. We extend this notion and the combinatorial bounds of Theorem 2 in Heintz (1983) using the degree for constructible sets defined in Pardo-Sebastián (2022). We show that the Erzeugungsgrad is the key ingredient to connect affine Intersection Theory over algebraically closed fields and the VC-Theory of Computational Learning Theory for families of classifiers given by parameterized families of constructible sets. In particular, we prove that the VC-dimension and the Krull dimension are linearly related up to logarithmic factors based on Intersection Theory. Using this relation, we study the density of correct test sequences in evasive varieties. We apply these ideas to analyze parameterized families of neural networks with rational activation function.\n        △ Less",
        "authors":[
            "Luis Miguel Pardo",
            "Daniel Sebastián"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Interpretable Hybrid-Rule Temporal Point Processes",
        "abstract":"Temporal Point Processes (TPPs) are widely used for modeling event sequences in various medical domains, such as disease onset prediction, progression analysis, and clinical decision support. Although TPPs effectively capture temporal dynamics, their lack of interpretability remains a critical challenge. Recent advancements have introduced interpretable TPPs. However, these methods fail to incorporate numerical features, thereby limiting their ability to generate precise predictions. To address this issue, we propose Hybrid-Rule Temporal Point Processes (HRTPP), a novel framework that integrates temporal logic rules with numerical features, improving both interpretability and predictive accuracy in event modeling. HRTPP comprises three key components: basic intensity for intrinsic event likelihood, rule-based intensity for structured temporal dependencies, and numerical feature intensity for dynamic probability modulation. To effectively discover valid rules, we introduce a two-phase rule mining strategy with Bayesian optimization. To evaluate our method, we establish a multi-criteria assessment framework, incorporating rule validity, model fitting, and temporal predictive accuracy. Experimental results on real-world medical datasets demonstrate that HRTPP outperforms state-of-the-art interpretable TPPs in terms of predictive performance and clinical interpretability. In case studies, the rules extracted by HRTPP explain the disease progression, offering valuable contributions to medical diagnosis.\n        △ Less",
        "authors":[
            "Yunyang Cao",
            "Juekai Lin",
            "Hongye Wang",
            "Wenhao Li",
            "Bo Jin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce",
        "abstract":"Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.\n        △ Less",
        "authors":[
            "Wei Xiong",
            "Jiarui Yao",
            "Yuhui Xu",
            "Bo Pang",
            "Lei Wang",
            "Doyen Sahoo",
            "Junnan Li",
            "Nan Jiang",
            "Tong Zhang",
            "Caiming Xiong",
            "Hanze Dong"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Looking beyond the next token",
        "abstract":"The structure of causal language model training assumes that each token can be accurately predicted from the previous context. This contrasts with humans' natural writing and reasoning process, where goals are typically known before the exact argument or phrasings. While this mismatch has been well studied in the literature, the working assumption has been that architectural changes are needed to address this mismatch. We argue that rearranging and processing the training data sequences can allow models to more accurately imitate the true data-generating process, and does not require any other changes to the architecture or training infrastructure. We demonstrate that this technique, Trelawney, and the inference algorithms derived from it allow us to improve performance on several key benchmarks that span planning, algorithmic reasoning, and story generation tasks. Finally, our method naturally enables the generation of long-term goals at no additional cost. We investigate how using the model's goal-generation capability can further improve planning and reasoning. Additionally, we believe Trelawney could potentially open doors to new capabilities beyond the current language modeling paradigm.\n        △ Less",
        "authors":[
            "Abitha Thankaraj",
            "Yiding Jiang",
            "J. Zico Kolter",
            "Yonatan Bisk"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Code Reborn AI-Driven Legacy Systems Modernization from COBOL to Java",
        "abstract":"This study investigates AI-driven modernization of legacy COBOL code into Java, addressing a critical challenge in aging software systems. Leveraging the Legacy COBOL 2024 Corpus -- 50,000 COBOL files from public and enterprise sources -- Java parses the code, AI suggests upgrades, and React visualizes gains. Achieving 93% accuracy, complexity drops 35% (from 18 to 11.7) and coupling 33% (from 8 to 5.4), surpassing manual efforts (75%) and rule-based tools (82%). The approach offers a scalable path to rejuvenate COBOL systems, vital for industries like banking and insurance.\n        △ Less",
        "authors":[
            "Gopichand Bandarupalli"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Mathematical Framework of Semantic Communication based on Category Theory",
        "abstract":"While semantic communication (SemCom) has recently demonstrated great potential to enhance transmission efficiency and reliability by leveraging machine learning (ML) and knowledge base (KB), there is a lack of mathematical modeling to rigorously characterize SemCom system and quantify the performance gain obtained from ML and KB. In this paper, we develop a mathematical framework for SemCom based on category theory, rigorously model the concepts of semantic entities and semantic probability space. Within this framework, semantic entropy is introduced to quantify the uncertainty of semantic entities. We theoretically prove that semantic entropy can be effectively reduced by exploiting KB, which capture semantic dependencies. Specifically, semantic entities can be combined based on semantic ambiguity, and are encoded based on contextual relationships among them. Then we refine semantic channel capacity modeling, which considers the mutual information contained in KB to better reflect SemCom efficiency. Numerical simulations validate the effectiveness of the proposed framework, showing that SemCom with KB integration outperforms traditional communication in both entropy reduction and coding efficiency.\n        △ Less",
        "authors":[
            "Shuheng Hua",
            "Yao Sun",
            "Kairong Ma",
            "Muhammad Ali Imran"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Subset-Contrastive Multi-Omics Network Embedding",
        "abstract":"Motivation: Network-based analyses of omics data are widely used, and while many of these methods have been adapted to single-cell scenarios, they often remain memory- and space-intensive. As a result, they are better suited to batch data or smaller datasets. Furthermore, the application of network-based methods in multi-omics often relies on similarity-based networks, which lack structurally-discrete topologies. This limitation may reduce the effectiveness of graph-based methods that were initially designed for topologies with better defined structures. Results: We propose Subset-Contrastive multi-Omics Network Embedding (SCONE), a method that employs contrastive learning techniques on large datasets through a scalable subgraph contrastive approach. By exploiting the pairwise similarity basis of many network-based omics methods, we transformed this characteristic into a strength, developing an approach that aims to achieve scalable and effective analysis. Our method demonstrates synergistic omics integration for cell type clustering in single-cell data. Additionally, we evaluate its performance in a bulk multi-omics integration scenario, where SCONE performs comparable to the state-of-the-art despite utilising limited views of the original data. We anticipate that our findings will motivate further research into the use of subset contrastive methods for omics data.\n        △ Less",
        "authors":[
            "Pedro Henrique da Costa Avelar",
            "Min Wu",
            "Sophia Tsoka"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints",
        "abstract":"Large Language Models (LLMs) are indispensable in today's applications, but their inference procedure -- generating responses by processing text in segments and using a memory-heavy Key-Value (KV) cache -- demands significant computational resources, particularly under memory constraints. This paper formulates LLM inference optimization as a multi-stage online scheduling problem where sequential prompt arrivals and KV cache growth render conventional scheduling ineffective. We develop a fluid dynamics approximation to provide a tractable benchmark that guides algorithm design. Building on this, we propose the Waiting for Accumulated Inference Threshold (WAIT) algorithm, which uses multiple thresholds to schedule incoming prompts optimally when output lengths are known, and extend it to Nested WAIT for cases with unknown output lengths. Theoretical analysis shows that both algorithms achieve near-optimal performance against the fluid benchmark in heavy traffic conditions, balancing throughput, latency, and Time to First Token (TTFT). Experiments with the Llama-7B model on an A100 GPU using both synthetic and real-world datasets demonstrate improved throughput and reduced latency relative to established baselines like vLLM and Sarathi. This work bridges operations research and machine learning, offering a rigorous framework for the efficient deployment of LLMs under memory constraints.\n        △ Less",
        "authors":[
            "Ruicheng Ao",
            "Gan Luo",
            "David Simchi-Levi",
            "Xinshang Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mildly-Interacting Fermionic Unitaries are Efficiently Learnable",
        "abstract":"Recent work has shown that one can efficiently learn fermionic Gaussian unitaries, also commonly known as nearest-neighbor matchcircuits or non-interacting fermionic unitaries. However, one could ask a similar question about unitaries that are near Gaussian: for example, unitaries prepared with a small number of non-Gaussian circuit elements. These operators find significance in quantum chemistry and many-body physics, yet no algorithm exists to learn them.\n  We give the first such result by devising an algorithm which makes queries to a $n$-mode fermionic unitary $U$ prepared by at most $O(t)$ non-Gaussian gates and returns a circuit approximating $U$ to diamond distance $\\varepsilon$ in time $\\textrm{poly}(n,2^t,1\/\\varepsilon)$. This resolves a central open question of Mele and Herasymenko under the strongest distance metric. In fact, our algorithm is much more general: we define a property of unitary Gaussianity known as unitary Gaussian dimension and show that our algorithm can learn $n$-mode unitaries of Gaussian dimension at least $2n - O(t)$ in time $\\textrm{poly}(n,2^t,1\/\\varepsilon)$. Indeed, this class subsumes unitaries prepared by at most $O(t)$ non-Gaussian gates but also includes several unitaries that require up to $2^{O(t)}$ non-Gaussian gates to construct.\n  In addition, we give a $\\textrm{poly}(n,1\/\\varepsilon)$-time algorithm to distinguish whether an $n$-mode unitary is of Gaussian dimension at least $k$ or $\\varepsilon$-far from all such unitaries in Frobenius distance, promised that one is the case. Along the way, we prove structural results about near-Gaussian fermionic unitaries that are likely to be of independent interest.\n        △ Less",
        "authors":[
            "Vishnu Iyer"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Differentially Private Geodesic and Linear Regression",
        "abstract":"In statistical applications it has become increasingly common to encounter data structures that live on non-linear spaces such as manifolds. Classical linear regression, one of the most fundamental methodologies of statistical learning, captures the relationship between an independent variable and a response variable which both are assumed to live in Euclidean space. Thus, geodesic regression emerged as an extension where the response variable lives on a Riemannian manifold. The parameters of geodesic regression, as with linear regression, capture the relationship of sensitive data and hence one should consider the privacy protection practices of said parameters. We consider releasing Differentially Private (DP) parameters of geodesic regression via the K-Norm Gradient (KNG) mechanism for Riemannian manifolds. We derive theoretical bounds for the sensitivity of the parameters showing they are tied to their respective Jacobi fields and hence the curvature of the space. This corroborates recent findings of differential privacy for the Fréchet mean. We demonstrate the efficacy of our methodology on the sphere, $\\mbS^2\\subset\\mbR^3$ and, since it is general to Riemannian manifolds, the manifold of Euclidean space which simplifies geodesic regression to a case of linear regression. Our methodology is general to any Riemannian manifold and thus it is suitable for data in domains such as medical imaging and computer vision.\n        △ Less",
        "authors":[
            "Aditya Kulkarni",
            "Carlos Soto"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Limits of Discrete Energy of Families of Increasing Sets",
        "abstract":"The Hausdorff dimension of a set can be detected using the Riesz energy. Here, we consider situations where a sequence of points, $\\{x_n\\}$, ``fills in'' a set $E \\subset \\mathbb{R}^d$ in an appropriate sense and investigate the degree to which the discrete analog to the Riesz energy of these sets can be used to bound the Hausdorff dimension of $E$. We also discuss applications to data science and Erdős\/Falconer type problems.\n        △ Less",
        "authors":[
            "Hari Nathan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning to Be A Doctor: Searching for Effective Medical Agent Architectures",
        "abstract":"Large Language Model (LLM)-based agents have demonstrated strong capabilities across a wide range of tasks, and their application in the medical domain holds particular promise due to the demand for high generalizability and reliance on interdisciplinary knowledge. However, existing medical agent systems often rely on static, manually crafted workflows that lack the flexibility to accommodate diverse diagnostic requirements and adapt to emerging clinical scenarios. Motivated by the success of automated machine learning (AutoML), this paper introduces a novel framework for the automated design of medical agent architectures. Specifically, we define a hierarchical and expressive agent search space that enables dynamic workflow adaptation through structured modifications at the node, structural, and framework levels. Our framework conceptualizes medical agents as graph-based architectures composed of diverse, functional node types and supports iterative self-improvement guided by diagnostic feedback. Experimental results on skin disease diagnosis tasks demonstrate that the proposed method effectively evolves workflow structures and significantly enhances diagnostic accuracy over time. This work represents the first fully automated framework for medical agent architecture design and offers a scalable, adaptable foundation for deploying intelligent agents in real-world clinical environments.\n        △ Less",
        "authors":[
            "Yangyang Zhuang",
            "Wenjia Jiang",
            "Jiayu Zhang",
            "Ze Yang",
            "Joey Tianyi Zhou",
            "Chi Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient and Stable Multi-Dimensional Kolmogorov-Smirnov Distance",
        "abstract":"We revisit extending the Kolmogorov-Smirnov distance between probability distributions to the multidimensional setting and make new arguments about the proper way to approach this generalization. Our proposed formulation maximizes the difference over orthogonal dominating rectangular ranges (d-sided rectangles in R^d), and is an integral probability metric. We also prove that the distance between a distribution and a sample from the distribution converges to 0 as the sample size grows, and bound this rate. Moreover, we show that one can, up to this same approximation error, compute the distance efficiently in 4 or fewer dimensions; specifically the runtime is near-linear in the size of the sample needed for that error. With this, we derive a delta-precision two-sample hypothesis test using this distance. Finally, we show these metric and approximation properties do not hold for other popular variants.\n        △ Less",
        "authors":[
            "Peter Matthew Jacobs",
            "Foad Namjoo",
            "Jeff M. Phillips"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation",
        "abstract":"Bipartite ranking is a fundamental supervised learning problem, with the goal of learning a ranking over instances with maximal area under the ROC curve (AUC) against a single binary target label. However, one may often observe multiple binary target labels, e.g., from distinct human annotators. How can one synthesize such labels into a single coherent ranking? In this work, we formally analyze two approaches to this problem -- loss aggregation and label aggregation -- by characterizing their Bayes-optimal solutions. Based on this, we show that while both methods can yield Pareto-optimal solutions, loss aggregation can exhibit label dictatorship: one can inadvertently (and undesirably) favor one label over others. This suggests that label aggregation can be preferable to loss aggregation, which we empirically verify.\n        △ Less",
        "authors":[
            "Michal Lukasik",
            "Lin Chen",
            "Harikrishna Narasimhan",
            "Aditya Krishna Menon",
            "Wittawat Jitkrittum",
            "Felix X. Yu",
            "Sashank J. Reddi",
            "Gang Fu",
            "Mohammadhossein Bateni",
            "Sanjiv Kumar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DeepSelective: Feature Gating and Representation Matching for Interpretable Clinical Prediction",
        "abstract":"The rapid accumulation of Electronic Health Records (EHRs) has transformed healthcare by providing valuable data that enhance clinical predictions and diagnoses. While conventional machine learning models have proven effective, they often lack robust representation learning and depend heavily on expert-crafted features. Although deep learning offers powerful solutions, it is often criticized for its lack of interpretability. To address these challenges, we propose DeepSelective, a novel end to end deep learning framework for predicting patient prognosis using EHR data, with a strong emphasis on enhancing model interpretability. DeepSelective combines data compression techniques with an innovative feature selection approach, integrating custom-designed modules that work together to improve both accuracy and interpretability. Our experiments demonstrate that DeepSelective not only enhances predictive accuracy but also significantly improves interpretability, making it a valuable tool for clinical decision-making. The source code is freely available at http:\/\/www.healthinformaticslab.org\/supp\/resources.php .\n        △ Less",
        "authors":[
            "Ruochi Zhang",
            "Qian Yang",
            "Xiaoyang Wang",
            "Haoran Wu",
            "Qiong Zhou",
            "Yu Wang",
            "Kewei Li",
            "Yueying Wang",
            "Yusi Fan",
            "Jiale Zhang",
            "Lan Huang",
            "Chang Liu",
            "Fengfeng Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets",
        "abstract":"Climate change is a major threat to the future of humanity, and its impacts are being intensified by excess man-made greenhouse gas emissions. One method governments can employ to control these emissions is to provide firms with emission limits and penalize any excess emissions above the limit. Excess emissions may also be offset by firms who choose to invest in carbon reducing and capturing projects. These projects generate offset credits which can be submitted to a regulating agency to offset a firm's excess emissions, or they can be traded with other firms. In this work, we characterize the finite-agent Nash equilibrium for offset credit markets. As computing Nash equilibria is an NP-hard problem, we utilize the modern reinforcement learning technique Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate not only the validity of employing reinforcement learning methods applied to climate themed financial markets, but also the significant financial savings emitting firms may achieve when abiding by the Nash equilibria through numerical experiments.\n        △ Less",
        "authors":[
            "Liam Welsh",
            "Udit Grover",
            "Sebastian Jaimungal"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reconstructing Fine-Grained Network Data using Autoencoder Architectures with Domain Knowledge Penalties",
        "abstract":"The ability to reconstruct fine-grained network session data, including individual packets, from coarse-grained feature vectors is crucial for improving network security models. However, the large-scale collection and storage of raw network traffic pose significant challenges, particularly for capturing rare cyberattack samples. These challenges hinder the ability to retain comprehensive datasets for model training and future threat detection. To address this, we propose a machine learning approach guided by formal methods to encode and reconstruct network data. Our method employs autoencoder models with domain-informed penalties to impute PCAP session headers from structured feature representations. Experimental results demonstrate that incorporating domain knowledge through constraint-based loss terms significantly improves reconstruction accuracy, particularly for categorical features with session-level encodings. By enabling efficient reconstruction of detailed network sessions, our approach facilitates data-efficient model training while preserving privacy and storage efficiency.\n        △ Less",
        "authors":[
            "Mark Cheung",
            "Sridhar Venkatesan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Rollout-Based Algorithm and Reward Function for Efficient Resource Allocation in Business Processes",
        "abstract":"Resource allocation plays a critical role in minimizing cycle time and improving the efficiency of business processes. Recently, Deep Reinforcement Learning (DRL) has emerged as a powerful tool to optimize resource allocation policies in business processes. In the DRL framework, an agent learns a policy through interaction with the environment, guided solely by reward signals that indicate the quality of its decisions. However, existing algorithms are not suitable for dynamic environments such as business processes. Furthermore, existing DRL-based methods rely on engineered reward functions that approximate the desired objective, but a misalignment between reward and objective can lead to undesired decisions or suboptimal policies. To address these issues, we propose a rollout-based DRL algorithm and a reward function to optimize the objective directly. Our algorithm iteratively improves the policy by evaluating execution trajectories following different actions. Our reward function directly decomposes the objective function of minimizing the mean cycle time. Maximizing our reward function guarantees that the objective function is minimized without requiring extensive reward engineering. The results show that our method consistently learns the optimal policy in all six evaluated business processes, outperforming the state-of-the-art algorithm that can only learn the optimal policy in two of the evaluated processes.\n        △ Less",
        "authors":[
            "Jeroen Middelhuis",
            "Zaharah Bukhsh",
            "Ivo Adan",
            "Remco Dijkman"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cryo-em images are intrinsically low dimensional",
        "abstract":"Simulation-based inference provides a powerful framework for cryo-electron microscopy, employing neural networks in methods like CryoSBI to infer biomolecular conformations via learned latent representations. This latent space represents a rich opportunity, encoding valuable information about the physical system and the inference process. Harnessing this potential hinges on understanding the underlying geometric structure of these representations. We investigate this structure by applying manifold learning techniques to CryoSBI representations of hemagglutinin (simulated and experimental). We reveal that these high-dimensional data inherently populate low-dimensional, smooth manifolds, with simulated data effectively covering the experimental counterpart. By characterizing the manifold's geometry using Diffusion Maps and identifying its principal axes of variation via coordinate interpretation methods, we establish a direct link between the latent structure and key physical parameters. Discovering this intrinsic low-dimensionality and interpretable geometric organization not only validates the CryoSBI approach but enables us to learn more from the data structure and provides opportunities for improving future inference strategies by exploiting this revealed manifold geometry.\n        △ Less",
        "authors":[
            "Luke Evans",
            "Octavian-Vlad Murad",
            "Lars Dingeldein",
            "Pilar Cossio",
            "Roberto Covino",
            "Marina Meila"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks",
        "abstract":"Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art algorithm for achieving sample-efficient multi-goal reinforcement learning (RL) in robotic manipulation tasks with binary rewards. HER facilitates learning from failed attempts by replaying trajectories with redefined goals. However, it relies on a heuristic-based replay method that lacks a principled framework. To address this limitation, we introduce a novel replay strategy, \"Next-Future\", which focuses on rewarding single-step transitions. This approach significantly enhances sample efficiency and accuracy in learning multi-goal Markov decision processes (MDPs), particularly under stringent accuracy requirements -- a critical aspect for performing complex and precise robotic-arm tasks. We demonstrate the efficacy of our method by highlighting how single-step learning enables improved value approximation within the multi-goal RL framework. The performance of the proposed replay strategy is evaluated across eight challenging robotic manipulation tasks, using ten random seeds for training. Our results indicate substantial improvements in sample efficiency for seven out of eight tasks and higher success rates in six tasks. Furthermore, real-world experiments validate the practical feasibility of the learned policies, demonstrating the potential of \"Next-Future\" in solving complex robotic-arm tasks.\n        △ Less",
        "authors":[
            "Fikrican Özgür",
            "René Zurbrügg",
            "Suryansh Kumar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Respiratory Inhaler Sound Event Classification Using Self-Supervised Learning",
        "abstract":"Asthma is a chronic respiratory condition that affects millions of people worldwide. While this condition can be managed by administering controller medications through handheld inhalers, clinical studies have shown low adherence to the correct inhaler usage technique. Consequently, many patients may not receive the full benefit of their medication. Automated classification of inhaler sounds has recently been studied to assess medication adherence. However, the existing classification models were typically trained using data from specific inhaler types, and their ability to generalize to sounds from different inhalers remains unexplored. In this study, we adapted the wav2vec 2.0 self-supervised learning model for inhaler sound classification by pre-training and fine-tuning this model on inhaler sounds. The proposed model shows a balanced accuracy of 98% on a dataset collected using a dry powder inhaler and smartwatch device. The results also demonstrate that re-finetuning this model on minimal data from a target inhaler is a promising approach to adapting a generic inhaler sound classification model to a different inhaler device and audio capture hardware. This is the first study in the field to demonstrate the potential of smartwatches as assistive technologies for the personalized monitoring of inhaler adherence using machine learning models.\n        △ Less",
        "authors":[
            "Davoud Shariat Panah",
            "Alessandro N Franciosi",
            "Cormac McCarthy",
            "Andrew Hines"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Automated Safety Requirements Derivation Using Agent-based RAG",
        "abstract":"We study the automated derivation of safety requirements in a self-driving vehicle use case, leveraging LLMs in combination with agent-based retrieval-augmented generation. Conventional approaches that utilise pre-trained LLMs to assist in safety analyses typically lack domain-specific knowledge. Existing RAG approaches address this issue, yet their performance deteriorates when handling complex queries and it becomes increasingly harder to retrieve the most relevant information. This is particularly relevant for safety-relevant applications. In this paper, we propose the use of agent-based RAG to derive safety requirements and show that the retrieved information is more relevant to the queries. We implement an agent-based approach on a document pool of automotive standards and the Apollo case study, as a representative example of an automated driving perception system. Our solution is tested on a data set of safety requirement questions and answers, extracted from the Apollo data. Evaluating a set of selected RAG metrics, we present and discuss advantages of a agent-based approach compared to default RAG methods.\n        △ Less",
        "authors":[
            "Balahari Vignesh Balu",
            "Florian Geissler",
            "Francesco Carella",
            "Joao-Vitor Zacchi",
            "Josef Jiru",
            "Nuria Mata",
            "Reinhard Stolle"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Forward-Forward Algorithm: Characterizing Training Behavior",
        "abstract":"The Forward-Forward algorithm is an alternative learning method which consists of two forward passes rather than a forward and backward pass employed by backpropagation. Forward-Forward networks employ layer local loss functions which are optimized based on the layer activation for each forward pass rather than a single global objective function. This work explores the dynamics of model and layer accuracy changes in Forward-Forward networks as training progresses in pursuit of a mechanistic understanding of their internal behavior. Treatments to various system characteristics are applied to investigate changes in layer and overall model accuracy as training progresses, how accuracy is impacted by layer depth, and how strongly individual layer accuracy is correlated with overall model accuracy. The empirical results presented suggest that layers deeper within Forward-Forward networks experience a delay in accuracy improvement relative to shallower layers and that shallower layer accuracy is strongly correlated with overall model accuracy.\n        △ Less",
        "authors":[
            "Reece Adamson"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax Computation in Transformers",
        "abstract":"While Transformers are dominated by Floating-Point (FP) Matrix-Multiplications, their aggressive acceleration through dedicated hardware or many-core programmable systems has shifted the performance bottleneck to non-linear functions like Softmax. Accelerating Softmax is challenging due to its non-pointwise, non-linear nature, with exponentiation as the most demanding step. To address this, we design a custom arithmetic block for Bfloat16 exponentiation leveraging a novel approximation algorithm based on Schraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of the RISC-V cores of a compute cluster, through custom Instruction Set Architecture (ISA) extensions, with a negligible area overhead of 1\\%. By optimizing the software kernels to leverage the extension, we execute Softmax with 162.7$\\times$ less latency and 74.3$\\times$ less energy compared to the baseline cluster, achieving an 8.2$\\times$ performance improvement and 4.1$\\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2 configuration. Moreover, the proposed approach enables a multi-cluster system to efficiently execute end-to-end inference of pre-trained Transformer models, such as GPT-2, GPT-3 and ViT, achieving up to 5.8$\\times$ and 3.6$\\times$ reduction in latency and energy consumption, respectively, without requiring re-training and with negligible accuracy loss.\n        △ Less",
        "authors":[
            "Run Wang",
            "Gamze Islamoglu",
            "Andrea Belano",
            "Viviane Potocnik",
            "Francesco Conti",
            "Angelo Garofalo",
            "Luca Benini"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diversity-Driven Learning: Tackling Spurious Correlations and Data Heterogeneity in Federated Models",
        "abstract":"Federated Learning (FL) enables decentralized training of machine learning models on distributed data while preserving privacy. However, in real-world FL settings, client data is often non-identically distributed and imbalanced, resulting in statistical data heterogeneity which impacts the generalization capabilities of the server's model across clients, slows convergence and reduces performance. In this paper, we address this challenge by first proposing a characterization of statistical data heterogeneity by means of 6 metrics of global and client attribute imbalance, class imbalance, and spurious correlations. Next, we create and share 7 computer vision datasets for binary and multiclass image classification tasks in Federated Learning that cover a broad range of statistical data heterogeneity and hence simulate real-world situations. Finally, we propose FedDiverse, a novel client selection algorithm in FL which is designed to manage and leverage data heterogeneity across clients by promoting collaboration between clients with complementary data distributions. Experiments on the seven proposed FL datasets demonstrate FedDiverse's effectiveness in enhancing the performance and robustness of a variety of FL methods while having low communication and computational overhead.\n        △ Less",
        "authors":[
            "Gergely D. Németh",
            "Eros Fanì",
            "Yeat Jeng Ng",
            "Barbara Caputo",
            "Miguel Ángel Lozano",
            "Nuria Oliver",
            "Novi Quadrianto"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SDFs from Unoriented Point Clouds using Neural Variational Heat Distances",
        "abstract":"We propose a novel variational approach for computing neural Signed Distance Fields (SDF) from unoriented point clouds. To this end, we replace the commonly used eikonal equation with the heat method, carrying over to the neural domain what has long been standard practice for computing distances on discrete surfaces. This yields two convex optimization problems for whose solution we employ neural networks: We first compute a neural approximation of the gradients of the unsigned distance field through a small time step of heat flow with weighted point cloud densities as initial data. Then we use it to compute a neural approximation of the SDF. We prove that the underlying variational problems are well-posed. Through numerical experiments, we demonstrate that our method provides state-of-the-art surface reconstruction and consistent SDF gradients. Furthermore, we show in a proof-of-concept that it is accurate enough for solving a PDE on the zero-level set.\n        △ Less",
        "authors":[
            "Samuel Weidemaier",
            "Florine Hartwig",
            "Josua Sassen",
            "Sergio Conti",
            "Mirela Ben-Chen",
            "Martin Rumpf"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Distributed Retrieval-Augmented Generation for Enhancing Language Model Performance",
        "abstract":"Small language models (SLMs) support efficient deployments on resource-constrained edge devices, but their limited capacity compromises inference performance. Retrieval-augmented generation (RAG) is a promising solution to enhance model performance by integrating external databases, without requiring intensive on-device model retraining. However, large-scale public databases and user-specific private contextual documents are typically located on the cloud and the device separately, while existing RAG implementations are primarily centralized. To bridge this gap, we propose DRAGON, a distributed RAG framework to enhance on-device SLMs through both general and personal knowledge without the risk of leaking document privacy. Specifically, DRAGON decomposes multi-document RAG into multiple parallel token generation processes performed independently and locally on the cloud and the device, and employs a newly designed Speculative Aggregation, a dual-side speculative algorithm to avoid frequent output synchronization between the cloud and device. A new scheduling algorithm is further introduced to identify the optimal aggregation side based on real-time network conditions. Evaluations on real-world hardware testbed demonstrate a significant performance improvement of DRAGON-up to 1.9x greater gains over standalone SLM compared to the centralized RAG, substantial reduction in per-token latency, and negligible Time to First Token (TTFT) overhead.\n        △ Less",
        "authors":[
            "Shangyu Liu",
            "Zhenzhe Zheng",
            "Xiaoyao Huang",
            "Fan Wu",
            "Guihai Chen",
            "Jie Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning",
        "abstract":"Vision-language models (VLMs), such as CLIP, have gained significant popularity as foundation models, with numerous fine-tuning methods developed to enhance performance on downstream tasks. However, due to their inherent vulnerability and the common practice of selecting from a limited set of open-source models, VLMs suffer from a higher risk of adversarial attacks than traditional vision models. Existing defense techniques typically rely on adversarial fine-tuning during training, which requires labeled data and lacks of flexibility for downstream tasks. To address these limitations, we propose robust test-time prompt tuning (R-TPT), which mitigates the impact of adversarial attacks during the inference stage. We first reformulate the classic marginal entropy objective by eliminating the term that introduces conflicts under adversarial conditions, retaining only the pointwise entropy minimization. Furthermore, we introduce a plug-and-play reliability-based weighted ensembling strategy, which aggregates useful information from reliable augmented views to strengthen the defense. R-TPT enhances defense against adversarial attacks without requiring labeled training data while offering high flexibility for inference tasks. Extensive experiments on widely used benchmarks with various attacks demonstrate the effectiveness of R-TPT. The code is available in https:\/\/github.com\/TomSheng21\/R-TPT.\n        △ Less",
        "authors":[
            "Lijun Sheng",
            "Jian Liang",
            "Zilei Wang",
            "Ran He"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space",
        "abstract":"The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code will be made publicly available after acceptance.\n        △ Less",
        "authors":[
            "Taewook Kang",
            "Bum-Jae You",
            "Juyoun Park",
            "Yisoo Lee"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails",
        "abstract":"Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems.\n        △ Less",
        "authors":[
            "William Hackett",
            "Lewis Birch",
            "Stefan Trawicki",
            "Neeraj Suri",
            "Peter Garraghan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention",
        "abstract":"Predicting future trajectories of surrounding vehicles heavily relies on what contextual information is given to a motion prediction model. The context itself can be static (lanes, regulatory elements, etc) or dynamic (traffic participants). This paper presents a lane graph-based motion prediction model that first predicts graph-based goal proposals and later fuses them with cross attention over multiple contextual elements. We follow the famous encoder-interactor-decoder architecture where the encoder encodes scene context using lightweight Gated Recurrent Units, the interactor applies cross-context attention over encoded scene features and graph goal proposals, and the decoder regresses multimodal trajectories via Laplacian Mixture Density Network from the aggregated encodings. Using cross-attention over graph-based goal proposals gives robust trajectory estimates since the model learns to attend to future goal-relevant scene elements for the intended agent. We evaluate our work on nuScenes motion prediction dataset, achieving state-of-the-art results.\n        △ Less",
        "authors":[
            "Mahir Gulzar",
            "Yar Muhammad",
            "Naveed Muhammad"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Divergence of Empirical Neural Tangent Kernel in Classification Problems",
        "abstract":"This paper demonstrates that in classification problems, fully connected neural networks (FCNs) and residual neural networks (ResNets) cannot be approximated by kernel logistic regression based on the Neural Tangent Kernel (NTK) under overtraining (i.e., when training time approaches infinity). Specifically, when using the cross-entropy loss, regardless of how large the network width is (as long as it is finite), the empirical NTK diverges from the NTK on the training samples as training time increases. To establish this result, we first demonstrate the strictly positive definiteness of the NTKs for multi-layer FCNs and ResNets. Then, we prove that during training, % with the cross-entropy loss, the neural network parameters diverge if the smallest eigenvalue of the empirical NTK matrix (Gram matrix) with respect to training samples is bounded below by a positive constant. This behavior contrasts sharply with the lazy training regime commonly observed in regression problems. Consequently, using a proof by contradiction, we show that the empirical NTK does not uniformly converge to the NTK across all times on the training samples as the network width increases. We validate our theoretical results through experiments on both synthetic data and the MNIST classification task. This finding implies that NTK theory is not applicable in this context, with significant theoretical implications for understanding neural networks in classification problems.\n        △ Less",
        "authors":[
            "Zixiong Yu",
            "Songtao Tian",
            "Guhan Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"K-means Enhanced Density Gradient Analysis for Urban and Transport Metrics Using Multi-Modal Satellite Imagery",
        "abstract":"This paper presents a novel computational approach for evaluating urban metrics through density gradient analysis using multi-modal satellite imagery, with applications including public transport and other urban systems. By combining optical and Synthetic Aperture Radar (SAR) data, we develop a method to segment urban areas, identify urban centers, and quantify density gradients. Our approach calculates two key metrics: the density gradient coefficient ($α$) and the minimum effective distance (LD) at which density reaches a target threshold. We further employ machine learning techniques, specifically K-means clustering, to objectively identify uniform and high-variability regions within density gradient plots. We demonstrate that these metrics provide an effective screening tool for public transport analyses by revealing the underlying urban structure. Through comparative analysis of two representative cities with contrasting urban morphologies (monocentric vs polycentric), we establish relationships between density gradient characteristics and public transport network topologies. Cities with clear density peaks in their gradient plots indicate distinct urban centers requiring different transport strategies than those with more uniform density distributions. This methodology offers urban planners a cost-effective, globally applicable approach to preliminary public transport assessment using freely available satellite data. The complete implementation, with additional examples and documentation, is available in an open-source repository under the MIT license at https:\/\/github.com\/nexri\/Satellite-Imagery-Urban-Analysis.\n        △ Less",
        "authors":[
            "P. Tomkiewicz",
            "J. Jaworski",
            "P. Zielonka",
            "A. Wilinski"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay",
        "abstract":"This study introduces a novel method for revealing human covert attention patterns using gameplay data alone, utilizing offline attention techniques from reinforcement learning (RL). We propose the contextualized, task-relevant (CTR) attention network, which generates attention maps from both human and RL agent gameplay in Atari environments. These maps are sparse yet retain the necessary information for the current player's decision making. We compare the CTR-derived attention maps with a temporally integrated overt attention (TIOA) model based on eye-tracking data, serving as a point of comparison and discussion. Visual inspection reveals distinct attention patterns: human CTR maps focus on the player and rather nearby opponents, occasionally shifting between stronger focus and broader views - sometimes even attending to empty space ahead. In contrast, agent maps maintain a consistent broad focus on most objects, including distant ones and the player. Quantitative analysis further demonstrates that human CTR maps align more closely with TIOA than agent maps do. Our findings indicate that the CTR attention network can effectively reveal human covert attention patterns from gameplay alone, without the need for additional data like brain activity recordings. This work contributes to understanding human-agent attention differences and enables the development of RL agents augmented with human covert attention.\n        △ Less",
        "authors":[
            "Henrik Krauss",
            "Takehisa Yairi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fine-Tuning Large Language Models on Quantum Optimization Problems for Circuit Generation",
        "abstract":"Large language models (LLM) have achieved remarkable outcomes in addressing complex problems, including math, coding, and analyzing large amounts of scientific reports. Yet few works have explored the potential of LLM in quantum computing. The most challenging problem is how to leverage LLMs to automatically generate quantum circuits at a large scale. In this paper, we address such a challenge by fine-tuning LLMs and injecting the domain-specific knowledge of quantum computing. In particular, we investigate the mechanisms to generate training data sets and construct the end-to-end pipeline to fine-tune pre-trained LLMs that produce parameterized quantum circuits for optimization problems. We have prepared 14,000 quantum circuits covering a substantial part of the quantum optimization landscape: 12 optimization problem instances and their optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can construct syntactically correct parametrized quantum circuits in the most recent OpenQASM 3.0. We have evaluated the quality of the parameters by comparing them to the optimized expectation values and distributions. Our evaluation shows that the fine-tuned LLM outperforms state-of-the-art models and that the parameters are better than random. The LLM-generated parametrized circuits and initial parameters can be used as a starting point for further optimization, \\emph{e.g.,} templates in quantum machine learning and the benchmark for compilers and hardware.\n        △ Less",
        "authors":[
            "Linus Jern",
            "Valter Uotila",
            "Cong Yu",
            "Bo Zhao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification",
        "abstract":"Antibiotic resistance presents a growing global health crisis, demanding new therapeutic strategies that target novel bacterial mechanisms. Recent advances in protein structure prediction and machine learning-driven molecule generation offer a promising opportunity to accelerate drug discovery. However, practical guidance on selecting and integrating these models into real-world pipelines remains limited. In this study, we develop an end-to-end, artificial intelligence-guided antibiotic discovery pipeline that spans target identification to compound realization. We leverage structure-based clustering across predicted proteomes of multiple pathogens to identify conserved, essential, and non-human-homologous targets. We then systematically evaluate six leading 3D-structure-aware generative models$\\unicode{x2014}$spanning diffusion, autoregressive, graph neural network, and language model architectures$\\unicode{x2014}$on their usability, chemical validity, and biological relevance. Rigorous post-processing filters and commercial analogue searches reduce over 100 000 generated compounds to a focused, synthesizable set. Our results highlight DeepBlock and TamGen as top performers across diverse criteria, while also revealing critical trade-offs between model complexity, usability, and output quality. This work provides a comparative benchmark and blueprint for deploying artificial intelligence in early-stage antibiotic development.\n        △ Less",
        "authors":[
            "Maximilian G. Schuh",
            "Joshua Hesse",
            "Stephan A. Sieber"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InfoClus: Informative Clustering of High-dimensional Data Embeddings",
        "abstract":"Developing an understanding of high-dimensional data can be facilitated by visualizing that data using dimensionality reduction. However, the low-dimensional embeddings are often difficult to interpret. To facilitate the exploration and interpretation of low-dimensional embeddings, we introduce a new concept named partitioning with explanations. The idea is to partition the data shown through the embedding into groups, each of which is given a sparse explanation using the original high-dimensional attributes. We introduce an objective function that quantifies how much we can learn through observing the explanations of the data partitioning, using information theory, and also how complex the explanations are. Through parameterization of the complexity, we can tune the solutions towards the desired granularity. We propose InfoClus, which optimizes the partitioning and explanations jointly, through greedy search constrained over a hierarchical clustering. We conduct a qualitative and quantitative analysis of InfoClus on three data sets. We contrast the results on the Cytometry data with published manual analysis results, and compare with two other recent methods for explaining embeddings (RVX and VERA). These comparisons highlight that InfoClus has distinct advantages over existing procedures and methods. We find that InfoClus can automatically create good starting points for the analysis of dimensionality-reduction-based scatter plots.\n        △ Less",
        "authors":[
            "Fuyin Lai",
            "Edith Heiter",
            "Guillaume Bied",
            "Jefrey Lijffijt"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TD-Suite: All Batteries Included Framework for Technical Debt Classification",
        "abstract":"Recognizing that technical debt is a persistent and significant challenge requiring sophisticated management tools, TD-Suite offers a comprehensive software framework specifically engineered to automate the complex task of its classification within software projects. It leverages the advanced natural language understanding of state-of-the-art transformer models to analyze textual artifacts, such as developer discussions in issue reports, where subtle indicators of debt often lie hidden.\n  TD-Suite provides a seamless end-to-end pipeline, managing everything from initial data ingestion and rigorous preprocessing to model training, thorough evaluation, and final inference. This allows it to support both straightforward binary classification (debt or no debt) and more valuable, identifying specific categories like code, design, or documentation debt, thus enabling more targeted management strategies.\n  To ensure the generated models are robust and perform reliably on real-world, often imbalanced, datasets, TD-Suite incorporates critical training methodologies: k-fold cross-validation assesses generalization capability, early stopping mechanisms prevent overfitting to the training data, and class weighting strategies effectively address skewed data distributions. Beyond core functionality, and acknowledging the growing importance of sustainability, the framework integrates tracking and reporting of carbon emissions associated with the computationally intensive model training process.\n  It also features a user-friendly Gradio web interface in a Docker container setup, simplifying model interaction, evaluation, and inference.\n        △ Less",
        "authors":[
            "Karthik Shivashankar",
            "Antonio Martini"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"QAMA: Quantum annealing multi-head attention operator with classical deep learning framework",
        "abstract":"As large language models scale up, the conventional attention mechanism faces critical challenges of exponential growth in memory consumption and energy costs. Quantum annealing computing, with its inherent advantages in computational efficiency and low energy consumption, offers an innovative direction for constructing novel deep learning architectures. This study proposes the first Quantum Annealing-based Multi-head Attention (QAMA) mechanism, achieving seamless compatibility with classical attention architectures through quadratic unconstrained binary optimization (QUBO) modeling of forward propagation and energy-based backpropagation. The method innovatively leverages the quantum bit interaction characteristics of Ising models to optimize the conventional $O(n^2)$ spatiotemporal complexity into linear resource consumption. Integrated with the optical computing advantages of coherent Ising machines (CIM), the system maintains millisecond-level real-time responsiveness while significantly reducing energy consumption. Our key contributions include: Theoretical proofs establish QAMA mathematical equivalence to classical attention mechanisms; Dual optimization of multi-head specificity and long-range information capture via QUBO constraints; Explicit gradient proofs for the Ising energy equation are utilized to implement gradient conduction as the only path in the computational graph as a layer; Proposed soft selection mechanism overcoming traditional binary attention limitations to approximate continuous weights. Experiments on QBoson CPQC quantum computer show QAMA achieves comparable accuracy to classical operators while reducing inference time to millisecond level and improving solution quality. This work pioneers architectural-level integration of quantum computing and deep learning, applicable to any attention-based model, driving paradigm innovation in AI foundational computing.\n        △ Less",
        "authors":[
            "Peng Du",
            "Shuolei Wang",
            "Shicheng Li",
            "Jinjing Shi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DPS: Design Pattern Summarisation Using Code Features",
        "abstract":"Automatic summarisation has been used efficiently in recent years to condense texts, conversations, audio, code, and various other artefacts. A range of methods, from simple template-based summaries to complex machine learning techniques -- and more recently, large language models -- have been employed to generate these summaries. Summarising software design patterns is important because it helps developers quickly understand and reuse complex design concepts, thereby improving software maintainability and development efficiency. However, the generation of summaries for software design patterns has not yet been explored. Our approach utilises code features and JavaParser to parse the code and create a JSON representation. Using an NLG library on this JSON representation, we convert it into natural language text that acts as a summary of the code, capturing the contextual information of the design pattern. Our empirical results indicate that the summaries generated by our approach capture the context in which patterns are applied in the codebase. Statistical evaluations demonstrate that our summaries closely align with human-written summaries, as evident from high values in the ROUGE-L, BLEU-4, NIST, and FrugalScore metrics. A follow-up survey further shows that DPS summaries were rated as capturing context better than human-generated summaries.\n        △ Less",
        "authors":[
            "Najam Nazar",
            "Sameer Sikka",
            "Christoph Treude"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scalability and Maintainability Challenges and Solutions in Machine Learning: Systematic Literature Review",
        "abstract":"This systematic literature review examines the critical challenges and solutions related to scalability and maintainability in Machine Learning (ML) systems. As ML applications become increasingly complex and widespread across industries, the need to balance system scalability with long-term maintainability has emerged as a significant concern. This review synthesizes current research and practices addressing these dual challenges across the entire ML life-cycle, from data engineering to model deployment in production. We analyzed 124 papers to identify and categorize 41 maintainability challenges and 13 scalability challenges, along with their corresponding solutions. Our findings reveal intricate inter dependencies between scalability and maintainability, where improvements in one often impact the other.\n  The review is structured around six primary research questions, examining maintainability and scalability challenges in data engineering, model engineering, and ML system development. We explore how these challenges manifest differently across various stages of the ML life-cycle.\n  This comprehensive overview offers valuable insights for both researchers and practitioners in the field of ML systems. It aims to guide future research directions, inform best practices, and contribute to the development of more robust, efficient, and sustainable ML applications across various domains.\n        △ Less",
        "authors":[
            "Karthik Shivashankar",
            "Ghadi S. Al Hajj",
            "Antonio Martini"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using Time Structure to Estimate Causal Effects",
        "abstract":"There exist several approaches for estimating causal effects in time series when latent confounding is present. Many of these approaches rely on additional auxiliary observed variables or time series such as instruments, negative controls or time series that satisfy the front- or backdoor criterion in certain graphs. In this paper, we present a novel approach for estimating direct (and via Wright's path rule total) causal effects in a time series setup which does not rely on additional auxiliary observed variables or time series. This approach assumes that the underlying time series is a Structural Vector Autoregressive (SVAR) process and estimates direct causal effects by solving certain linear equation systems made up of different covariances and model parameters. We state sufficient graphical criteria in terms of the so-called full time graph under which these linear equations systems are uniquely solvable and under which their solutions contain the to-be-identified direct causal effects as components. We also state sufficient lag-based criteria under which the previously mentioned graphical conditions are satisfied and, thus, under which direct causal effects are identifiable. Several numerical experiments underline the correctness and applicability of our results.\n        △ Less",
        "authors":[
            "Tom Hochsprung",
            "Jakob Runge",
            "Andreas Gerhardus"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamical errors in machine learning forecasts",
        "abstract":"In machine learning forecasting, standard error metrics such as mean absolute error (MAE) and mean squared error (MSE) quantify discrepancies between predictions and target values. However, these metrics do not directly evaluate the physical and\/or dynamical consistency of forecasts, an increasingly critical concern in scientific and engineering applications.\n  Indeed, a fundamental yet often overlooked question is whether machine learning forecasts preserve the dynamical behavior of the underlying system. Addressing this issue is essential for assessing the fidelity of machine learning models and identifying potential failure modes, particularly in applications where maintaining correct dynamical behavior is crucial.\n  In this work, we investigate the relationship between standard forecasting error metrics, such as MAE and MSE, and the dynamical properties of the underlying system. To achieve this goal, we use two recently developed dynamical indices: the instantaneous dimension ($d$), and the inverse persistence ($θ$). Our results indicate that larger forecast errors -- e.g., higher MSE -- tend to occur in states with higher $d$ (higher complexity) and higher $θ$ (lower persistence). To further assess dynamical consistency, we propose error metrics based on the dynamical indices that measure the discrepancy of the forecasted $d$ and $θ$ versus their correct values. Leveraging these dynamical indices-based metrics, we analyze direct and recursive forecasting strategies for three canonical datasets -- Lorenz, Kuramoto-Sivashinsky equation, and Kolmogorov flow -- as well as a real-world weather forecasting task. Our findings reveal substantial distortions in dynamical properties in ML forecasts, especially for long forecast lead times or long recursive simulations, providing complementary information on ML forecast fidelity that can be used to improve ML models.\n        △ Less",
        "authors":[
            "Zhou Fang",
            "Gianmarco Mengaldo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Morphing-based Compression for Data-centric ML Pipelines",
        "abstract":"Data-centric ML pipelines extend traditional machine learning (ML) pipelines -- of feature transformations and ML model training -- by outer loops for data cleaning, augmentation, and feature engineering to create high-quality input data. Existing lossless matrix compression applies lightweight compression schemes to numeric matrices and performs linear algebra operations such as matrix-vector multiplications directly on the compressed representation but struggles to efficiently rediscover structural data redundancy. Compressed operations are effective at fitting data in available memory, reducing I\/O across the storage-memory-cache hierarchy, and improving instruction parallelism. The applied data cleaning, augmentation, and feature transformations provide a rich source of information about data characteristics such as distinct items, column sparsity, and column correlations. In this paper, we introduce BWARE -- an extension of AWARE for workload-aware lossless matrix compression -- that pushes compression through feature transformations and engineering to leverage information about structural transformations. Besides compressed feature transformations, we introduce a novel technique for lightweight morphing of a compressed representation into workload-optimized compressed representations without decompression. BWARE shows substantial end-to-end runtime improvements, reducing the execution time for training data-centric ML pipelines from days to hours.\n        △ Less",
        "authors":[
            "Sebastian Baunsgaard",
            "Matthias Boehm"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models",
        "abstract":"Unsupervised reinforcement learning (RL) aims at pre-training agents that can solve a wide range of downstream tasks in complex environments. Despite recent advancements, existing approaches suffer from several limitations: they may require running an RL process on each downstream task to achieve a satisfactory performance, they may need access to datasets with good coverage or well-curated task-specific samples, or they may pre-train policies with unsupervised losses that are poorly correlated with the downstream tasks of interest. In this paper, we introduce a novel algorithm regularizing unsupervised RL towards imitating trajectories from unlabeled behavior datasets. The key technical novelty of our method, called Forward-Backward Representations with Conditional-Policy Regularization, is to train forward-backward representations to embed the unlabeled trajectories to the same latent space used to represent states, rewards, and policies, and use a latent-conditional discriminator to encourage policies to ``cover'' the states in the unlabeled behavior dataset. As a result, we can learn policies that are well aligned with the behaviors in the dataset, while retaining zero-shot generalization capabilities for reward-based and imitation tasks. We demonstrate the effectiveness of this new approach in a challenging humanoid control problem: leveraging observation-only motion capture datasets, we train Meta Motivo, the first humanoid behavioral foundation model that can be prompted to solve a variety of whole-body tasks, including motion tracking, goal reaching, and reward optimization. The resulting model is capable of expressing human-like behaviors and it achieves competitive performance with task-specific methods while outperforming state-of-the-art unsupervised RL and model-based baselines.\n        △ Less",
        "authors":[
            "Andrea Tirinzoni",
            "Ahmed Touati",
            "Jesse Farebrother",
            "Mateusz Guzek",
            "Anssi Kanervisto",
            "Yingchen Xu",
            "Alessandro Lazaric",
            "Matteo Pirotta"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"QualiTagger: Automating software quality detection in issue trackers",
        "abstract":"A systems quality is a major concern for development teams when it evolve. Understanding the effects of a loss of quality in the codebase is crucial to avoid side effects like the appearance of technical debt. Although the identification of these qualities in software requirements described in natural language has been investigated, most of the results are often not applicable in practice, and rely on having been validated on small datasets and limited amount of projects. For many years, machine learning (ML) techniques have been proved as a valid technique to identify and tag terms described in natural language. In order to advance previous works, in this research we use cutting edge models like Transformers, together with a vast dataset mined and curated from GitHub, to identify what text is usually associated with different quality properties. We also study the distribution of such qualities in issue trackers from openly accessible software repositories, and we evaluate our approach both with students from a software engineering course and with its application to recognize security labels in industry.\n        △ Less",
        "authors":[
            "Karthik Shivashankar",
            "Rafael Capilla",
            "Maren Maritsdatter Kruke",
            "Mili Orucevic",
            "Antonio Martini"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging LLMs and attention-mechanism for automatic annotation of historical maps",
        "abstract":"Historical maps are essential resources that provide insights into the geographical landscapes of the past. They serve as valuable tools for researchers across disciplines such as history, geography, and urban studies, facilitating the reconstruction of historical environments and the analysis of spatial transformations over time. However, when constrained to analogue or scanned formats, their interpretation is limited to humans and therefore not scalable. Recent advancements in machine learning, particularly in computer vision and large language models (LLMs), have opened new avenues for automating the recognition and classification of features and objects in historical maps. In this paper, we propose a novel distillation method that leverages LLMs and attention mechanisms for the automatic annotation of historical maps. LLMs are employed to generate coarse classification labels for low-resolution historical image patches, while attention mechanisms are utilized to refine these labels to higher resolutions. Experimental results demonstrate that the refined labels achieve a high recall of more than 90%. Additionally, the intersection over union (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with precision scores of 87.1% and 79.5%, respectively, indicate that most labels are well-aligned with ground-truth annotations. Notably, these results were achieved without the use of fine-grained manual labels during training, underscoring the potential of our approach for efficient and scalable historical map analysis.\n        △ Less",
        "authors":[
            "Yunshuang Yuan",
            "Monika Sester"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Defending Against Frequency-Based Attacks with Diffusion Models",
        "abstract":"Adversarial training is a common strategy for enhancing model robustness against adversarial attacks. However, it is typically tailored to the specific attack types it is trained on, limiting its ability to generalize to unseen threat models. Adversarial purification offers an alternative by leveraging a generative model to remove perturbations before classification. Since the purifier is trained independently of both the classifier and the threat models, it is better equipped to handle previously unseen attack scenarios. Diffusion models have proven highly effective for noise purification, not only in countering pixel-wise adversarial perturbations but also in addressing non-adversarial data shifts. In this study, we broaden the focus beyond pixel-wise robustness to explore the extent to which purification can mitigate both spectral and spatial adversarial attacks. Our findings highlight its effectiveness in handling diverse distortion patterns across low- to high-frequency regions.\n        △ Less",
        "authors":[
            "Fatemeh Amerehi",
            "Patrick Healy"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A PyTorch-Compatible Spike Encoding Framework for Energy-Efficient Neuromorphic Applications",
        "abstract":"Spiking Neural Networks (SNNs) offer promising energy efficiency advantages, particularly when processing sparse spike trains. However, their incompatibility with traditional datasets, which consist of batches of input vectors rather than spike trains, necessitates the development of efficient encoding methods. This paper introduces a novel, open-source PyTorch-compatible Python framework for spike encoding, designed for neuromorphic applications in machine learning and reinforcement learning. The framework supports a range of encoding algorithms, including Leaky Integrate-and-Fire (LIF), Step Forward (SF), Pulse Width Modulation (PWM), and Ben's Spiker Algorithm (BSA), as well as specialized encoding strategies covering population coding and reinforcement learning scenarios. Furthermore, we investigate the performance trade-offs of each method on embedded hardware using C\/C++ implementations, considering energy consumption, computation time, spike sparsity, and reconstruction accuracy. Our findings indicate that SF typically achieves the lowest reconstruction error and offers the highest energy efficiency and fastest encoding speed, achieving the second-best spike sparsity. At the same time, other methods demonstrate particular strengths depending on the signal characteristics. This framework and the accompanying empirical analysis provide valuable resources for selecting optimal encoding strategies for energy-efficient SNN applications.\n        △ Less",
        "authors":[
            "Alexandru Vasilache",
            "Jona Scholz",
            "Vincent Schilling",
            "Sven Nitzsche",
            "Florian Kaelber",
            "Johannes Korsch",
            "Juergen Becker"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Meta-learning For Few-Shot Time Series Crop Type Classification: A Benchmark On The EuroCropsML Dataset",
        "abstract":"Spatial imbalances in crop type data pose significant challenges for accurate classification in remote sensing applications. Algorithms aiming at transferring knowledge from data-rich to data-scarce tasks have thus surged in popularity. However, despite their effectiveness in previous evaluations, their performance in challenging real-world applications is unclear and needs to be evaluated. This study benchmarks transfer learning and several meta-learning algorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML), Almost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the real-world EuroCropsML time series dataset, which combines farmer-reported crop data with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal. Our findings indicate that MAML-based meta-learning algorithms achieve slightly higher accuracy compared to simpler transfer learning methods when applied to crop type classification tasks in Estonia after pre-training on data from Latvia. However, this improvement comes at the cost of increased computational demands and training time. Moreover, we find that the transfer of knowledge between geographically disparate regions, such as Estonia and Portugal, poses significant challenges to all investigated algorithms. These insights underscore the trade-offs between accuracy and computational resource requirements in selecting machine learning methods for real-world crop type classification tasks and highlight the difficulties of transferring knowledge between different regions of the Earth. To facilitate future research in this domain, we present the first comprehensive benchmark for evaluating transfer and meta-learning methods for crop type classification under real-world conditions. The corresponding code is publicly available at https:\/\/github.com\/dida-do\/eurocrops-meta-learning.\n        △ Less",
        "authors":[
            "Joana Reuss",
            "Jan Macdonald",
            "Simon Becker",
            "Konrad Schultka",
            "Lorenz Richter",
            "Marco Körner"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Vertical Public-Private Split for Improved Synthetic Data Generation",
        "abstract":"Differentially Private Synthetic Data Generation (DP-SDG) is a key enabler of private and secure tabular-data sharing, producing artificial data that carries through the underlying statistical properties of the input data. This typically involves adding carefully calibrated statistical noise to guarantee individual privacy, at the cost of synthetic data quality. Recent literature has explored scenarios where a small amount of public data is used to help enhance the quality of synthetic data. These methods study a horizontal public-private partitioning which assumes access to a small number of public rows that can be used for model initialization, providing a small utility gain. However, realistic datasets often naturally consist of public and private attributes, making a vertical public-private partitioning relevant for practical synthetic data deployments. We propose a novel framework that adapts horizontal public-assisted methods into the vertical setting. We compare this framework against our alternative approach that uses conditional generation, highlighting initial limitations of public-data assisted methods and proposing future research directions to address these challenges.\n        △ Less",
        "authors":[
            "Samuel Maddock",
            "Shripad Gade",
            "Graham Cormode",
            "Will Bullock"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ProtFlow: Fast Protein Sequence Design via Flow Matching on Compressed Protein Language Model Embeddings",
        "abstract":"The design of protein sequences with desired functionalities is a fundamental task in protein engineering. Deep generative methods, such as autoregressive models and diffusion models, have greatly accelerated the discovery of novel protein sequences. However, these methods mainly focus on local or shallow residual semantics and suffer from low inference efficiency, large modeling space and high training cost. To address these challenges, we introduce ProtFlow, a fast flow matching-based protein sequence design framework that operates on embeddings derived from semantically meaningful latent space of protein language models. By compressing and smoothing the latent space, ProtFlow enhances performance while training on limited computational resources. Leveraging reflow techniques, ProtFlow enables high-quality single-step sequence generation. Additionally, we develop a joint design pipeline for the design scene of multichain proteins. We evaluate ProtFlow across diverse protein design tasks, including general peptides and long-chain proteins, antimicrobial peptides, and antibodies. Experimental results demonstrate that ProtFlow outperforms task-specific methods in these applications, underscoring its potential and broad applicability in computational protein sequence design and analysis.\n        △ Less",
        "authors":[
            "Zitai Kong",
            "Yiheng Zhu",
            "Yinlong Xu",
            "Hanjing Zhou",
            "Mingzhe Yin",
            "Jialu Wu",
            "Hongxia Xu",
            "Chang-Yu Hsieh",
            "Tingjun Hou",
            "Jian Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Early Detection of Cognitive Impairment in Elderly using a Passive FPVS-EEG BCI and Machine Learning -- Extended Version",
        "abstract":"Early dementia diagnosis requires biomarkers sensitive to both structural and functional brain changes. While structural neuroimaging biomarkers have progressed significantly, objective functional biomarkers of early cognitive decline remain a critical unmet need. Current cognitive assessments often rely on behavioral responses, making them susceptible to factors like effort, practice effects, and educational background, thereby hindering early and accurate detection. This work introduces a novel approach, leveraging a lightweight convolutional neural network (CNN) to infer cognitive impairment levels directly from electroencephalography (EEG) data. Critically, this method employs a passive fast periodic visual stimulation (FPVS) paradigm, eliminating the need for explicit behavioral responses or task comprehension from the participant. This passive approach provides an objective measure of working memory function, independent of confounding factors inherent in active cognitive tasks, and offers a promising new avenue for early and unbiased detection of cognitive decline.\n        △ Less",
        "authors":[
            "Tomasz M. Rutkowski",
            "Stanisław Narębski",
            "Mihoko Otake-Matsuura",
            "Tomasz Komendziński"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning-Based User Association for MmWave Vehicular Networks With Kernelized Contextual Bandits",
        "abstract":"Vehicles require timely channel conditions to determine the base station (BS) to communicate with, but it is costly to estimate the fast-fading mmWave channels frequently. Without additional channel estimations, the proposed Distributed Kernelized Upper Confidence Bound (DK-UCB) algorithm estimates the current instantaneous transmission rates utilizing past contexts, such as the vehicle's location and velocity, along with past instantaneous transmission rates. To capture the nonlinear mapping from a context to the instantaneous transmission rate, DK-UCB maps a context into the reproducing kernel Hilbert space (RKHS) where a linear mapping becomes observable. To improve estimation accuracy, we propose a novel kernel function in RKHS which incorporates the propagation characteristics of the mmWave signals. Moreover, DK-UCB encourages a vehicle to share necessary information when it has conducted significant explorations, which speeds up the learning process while maintaining affordable communication costs.\n        △ Less",
        "authors":[
            "Xiaoyang He",
            "Xiaoxia Huang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Recognition of Geometrical Shapes by Dictionary Learning",
        "abstract":"Dictionary learning is a versatile method to produce an overcomplete set of vectors, called atoms, to represent a given input with only a few atoms. In the literature, it has been used primarily for tasks that explore its powerful representation capabilities, such as for image reconstruction. In this work, we present a first approach to make dictionary learning work for shape recognition, considering specifically geometrical shapes. As we demonstrate, the choice of the underlying optimization method has a significant impact on recognition quality. Experimental results confirm that dictionary learning may be an interesting method for shape recognition tasks.\n        △ Less",
        "authors":[
            "Alexander Köhler",
            "Michael Breuß"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers",
        "abstract":"Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).\n        △ Less",
        "authors":[
            "Hongkang Li",
            "Yihua Zhang",
            "Shuai Zhang",
            "Meng Wang",
            "Sijia Liu",
            "Pin-Yu Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BEACON: A Benchmark for Efficient and Accurate Counting of Subgraphs",
        "abstract":"Subgraph counting the task of determining the number of instances of a query pattern within a large graph lies at the heart of many critical applications, from analyzing financial networks and transportation systems to understanding biological interactions. Despite decades of work yielding efficient algorithmic (AL) solutions and, more recently, machine learning (ML) approaches, a clear comparative understanding is elusive. This gap stems from the absence of a unified evaluation framework, standardized datasets, and accessible ground truths, all of which hinder systematic analysis and fair benchmarking. To overcome these barriers, we introduce BEACON: a comprehensive benchmark designed to rigorously evaluate both AL and ML-based subgraph counting methods. BEACON provides a standardized dataset with verified ground truths, an integrated evaluation environment, and a public leaderboard, enabling reproducible and transparent comparisons across diverse approaches. Our extensive experiments reveal that while AL methods excel in efficiently counting subgraphs on very large graphs, they struggle with complex patterns (e.g., those exceeding six nodes). In contrast, ML methods are capable of handling larger patterns but demand massive graph data inputs and often yield suboptimal accuracy on small, dense graphs. These insights not only highlight the unique strengths and limitations of each approach but also pave the way for future advancements in subgraph counting techniques. Overall, BEACON represents a significant step towards unifying and accelerating research in subgraph counting, encouraging innovative solutions and fostering a deeper understanding of the trade-offs between algorithmic and machine learning paradigms.\n        △ Less",
        "authors":[
            "Mohammad Matin Najafi",
            "Xianju Zhu",
            "Chrysanthi Kosyfaki",
            "Laks V. S. Lakshmanan",
            "Reynold Cheng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can LLMs Leverage Observational Data? Towards Data-Driven Causal Discovery with LLMs",
        "abstract":"Causal discovery traditionally relies on statistical methods applied to observational data, often requiring large datasets and assumptions about underlying causal structures. Recent advancements in Large Language Models (LLMs) have introduced new possibilities for causal discovery by providing domain expert knowledge. However, it remains unclear whether LLMs can effectively process observational data for causal discovery. In this work, we explore the potential of LLMs for data-driven causal discovery by integrating observational data for LLM-based reasoning. Specifically, we examine whether LLMs can effectively utilize observational data through two prompting strategies: pairwise prompting and breadth first search (BFS)-based prompting. In both approaches, we incorporate the observational data directly into the prompt to assess LLMs' ability to infer causal relationships from such data. Experiments on benchmark datasets show that incorporating observational data enhances causal discovery, boosting F1 scores by up to 0.11 point using both pairwise and BFS LLM-based prompting, while outperforming traditional statistical causal discovery baseline by up to 0.52 points. Our findings highlight the potential and limitations of LLMs for data-driven causal discovery, demonstrating their ability to move beyond textual metadata and effectively interpret and utilize observational data for more informed causal reasoning. Our studies lays the groundwork for future advancements toward fully LLM-driven causal discovery.\n        △ Less",
        "authors":[
            "Yuni Susanti",
            "Michael Färber"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-scale DeepOnet (Mscale-DeepOnet) for Mitigating Spectral Bias in Learning High Frequency Operators of Oscillatory Functions",
        "abstract":"In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce the spectral bias of the DeepOnet in learning high-frequency mapping between highly oscillatory functions, with an application to the nonlinear mapping between the coefficient of the Helmholtz equation and its solution. The Mscale-DeepOnet introduces the multiscale neural network in the branch and trunk networks of the original DeepOnet, the resulting Mscale-DeepOnet is shown to be able to capture various high-frequency components of the mapping itself and its image. Numerical results demonstrate the substantial improvement of the Mscale-DeepOnet for the problem of wave scattering in the high-frequency regime over the normal DeepOnet with a similar number of network parameters.\n        △ Less",
        "authors":[
            "Bo Wang",
            "Lizuo Liu",
            "Wei Cai"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transfer Learning for Temporal Link Prediction",
        "abstract":"Link prediction on graphs has applications spanning from recommender systems to drug discovery. Temporal link prediction (TLP) refers to predicting future links in a temporally evolving graph and adds additional complexity related to the dynamic nature of graphs. State-of-the-art TLP models incorporate memory modules alongside graph neural networks to learn both the temporal mechanisms of incoming nodes and the evolving graph topology. However, memory modules only store information about nodes seen at train time, and hence such models cannot be directly transferred to entirely new graphs at test time and deployment. In this work, we study a new transfer learning task for temporal link prediction, and develop transfer-effective methods for memory-laden models. Specifically, motivated by work showing the informativeness of structural signals for the TLP task, we augment a structural mapping module to the existing TLP model architectures, which learns a mapping from graph structural (topological) features to memory embeddings. Our work paves the way for a memory-free foundation model for TLP.\n        △ Less",
        "authors":[
            "Ayan Chatterjee",
            "Barbara Ikica",
            "Babak Ravandi",
            "John Palowitch"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fast-Powerformer: A Memory-Efficient Transformer for Accurate Mid-Term Wind Power Forecasting",
        "abstract":"Wind power forecasting (WPF), as a significant research topic within renewable energy, plays a crucial role in enhancing the security, stability, and economic operation of power grids. However, due to the high stochasticity of meteorological factors (e.g., wind speed) and significant fluctuations in wind power output, mid-term wind power forecasting faces a dual challenge of maintaining high accuracy and computational efficiency. To address these issues, this paper proposes an efficient and lightweight mid-term wind power forecasting model, termed Fast-Powerformer. The proposed model is built upon the Reformer architecture, incorporating structural enhancements such as a lightweight Long Short-Term Memory (LSTM) embedding module, an input transposition mechanism, and a Frequency Enhanced Channel Attention Mechanism (FECAM). These improvements enable the model to strengthen temporal feature extraction, optimize dependency modeling across variables, significantly reduce computational complexity, and enhance sensitivity to periodic patterns and dominant frequency components. Experimental results conducted on multiple real-world wind farm datasets demonstrate that the proposed Fast-Powerformer achieves superior prediction accuracy and operational efficiency compared to mainstream forecasting approaches. Furthermore, the model exhibits fast inference speed and low memory consumption, highlighting its considerable practical value for real-world deployment scenarios.\n        △ Less",
        "authors":[
            "Mingyi Zhu",
            "Zhaoxin Li",
            "Qiao Lin",
            "Li Ding"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards A Universal Graph Structural Encoder",
        "abstract":"Recent advancements in large-scale pre-training have shown the potential to learn generalizable representations for downstream tasks. In the graph domain, however, capturing and transferring structural information across different graph domains remains challenging, primarily due to the inherent differences in topological patterns across various contexts. Additionally, most existing models struggle to capture the complexity of rich graph structures, leading to inadequate exploration of the embedding space. To address these challenges, we propose GFSE, a universal graph structural encoder designed to capture transferable structural patterns across diverse domains such as molecular graphs, social networks, and citation networks. GFSE is the first cross-domain graph structural encoder pre-trained with multiple self-supervised learning objectives. Built on a Graph Transformer, GFSE incorporates attention mechanisms informed by graph inductive bias, enabling it to encode intricate multi-level and fine-grained topological features. The pre-trained GFSE produces generic and theoretically expressive positional and structural encoding for graphs, which can be seamlessly integrated with various downstream graph feature encoders, including graph neural networks for vectorized features and Large Language Models for text-attributed graphs. Comprehensive experiments on synthetic and real-world datasets demonstrate GFSE's capability to significantly enhance the model's performance while requiring substantially less task-specific fine-tuning. Notably, GFSE achieves state-of-the-art performance in 81.6% evaluated cases, spanning diverse graph models and datasets, highlighting its potential as a powerful and versatile encoder for graph-structured data.\n        △ Less",
        "authors":[
            "Jialin Chen",
            "Haolan Zuo",
            "Haoyu Peter Wang",
            "Siqi Miao",
            "Pan Li",
            "Rex Ying"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs",
        "abstract":"Task arithmetic is a straightforward yet highly effective strategy for model merging, enabling the resultant model to exhibit multi-task capabilities. Recent research indicates that models demonstrating linearity enhance the performance of task arithmetic. In contrast to existing methods that rely on the global linearization of the model, we argue that this linearity already exists within the model's submodules. In particular, we present a statistical analysis and show that submodules (e.g., layers, self-attentions, and MLPs) exhibit significantly higher linearity than the overall model. Based on these findings, we propose an innovative model merging strategy that independently merges these submodules. Especially, we derive a closed-form solution for optimal merging weights grounded in the linear properties of these submodules. Experimental results demonstrate that our method consistently outperforms the standard task arithmetic approach and other established baselines across different model scales and various tasks. This result highlights the benefits of leveraging the linearity of submodules and provides a new perspective for exploring solutions for effective and practical multi-task model merging.\n        △ Less",
        "authors":[
            "Rui Dai",
            "Sile Hu",
            "Xu Shen",
            "Yonggang Zhang",
            "Xinmei Tian",
            "Jieping Ye"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bridging Distribution Gaps in Time Series Foundation Model Pretraining with Prototype-Guided Normalization",
        "abstract":"Foundation models have achieved remarkable success across diverse machine-learning domains through large-scale pretraining on large, diverse datasets. However, pretraining on such datasets introduces significant challenges due to substantial mismatches in data distributions, a problem particularly pronounced with time series data. In this paper, we tackle this issue by proposing a domain-aware adaptive normalization strategy within the Transformer architecture. Specifically, we replace the traditional LayerNorm with a prototype-guided dynamic normalization mechanism (ProtoNorm), where learned prototypes encapsulate distinct data distributions, and sample-to-prototype affinity determines the appropriate normalization layer. This mechanism effectively captures the heterogeneity of time series characteristics, aligning pretrained representations with downstream tasks. Through comprehensive empirical evaluation, we demonstrate that our method significantly outperforms conventional pretraining techniques across both classification and forecasting tasks, while effectively mitigating the adverse effects of distribution shifts during pretraining. Incorporating ProtoNorm is as simple as replacing a single line of code. Extensive experiments on diverse real-world time series benchmarks validate the robustness and generalizability of our approach, advancing the development of more versatile time series foundation models.\n        △ Less",
        "authors":[
            "Peiliang Gong",
            "Emadeldeen Eldele",
            "Min Wu",
            "Zhenghua Chen",
            "Xiaoli Li",
            "Daoqiang Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation of Credibility Perceptions of Visual Content",
        "abstract":"In today's visually dominated social media landscape, predicting the perceived credibility of visual content and understanding what drives human judgment are crucial for countering misinformation. However, these tasks are challenging due to the diversity and richness of visual features. We introduce a Large Language Model (LLM)-informed feature discovery framework that leverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and explain its reasoning. We extract and quantify interpretable features using targeted prompts and integrate them into machine learning models to improve credibility predictions. We tested this approach on 4,191 visual social media posts across eight topics in science, health, and politics, using credibility ratings from 5,355 crowdsourced workers. Our method outperformed zero-shot GPT-based predictions by 13 percent in R2, and revealed key features like information concreteness and image format. We discuss the implications for misinformation mitigation, visual credibility, and the role of LLMs in social science.\n        △ Less",
        "authors":[
            "Yilang Peng",
            "Sijia Qian",
            "Yingdan Lu",
            "Cuihua Shen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration",
        "abstract":"In this paper, we study the theoretical properties of the projected Bellman equation (PBE) and two algorithms to solve this equation: linear Q-learning and approximate value iteration (AVI). We consider two sufficient conditions for the existence of a solution to PBE : strictly negatively row dominating diagonal (SNRDD) assumption and a condition motivated by the convergence of AVI. The SNRDD assumption also ensures the convergence of linear Q-learning, and its relationship with the convergence of AVI is examined. Lastly, several interesting observations on the solution of PBE are provided when using $ε$-greedy policy.\n        △ Less",
        "authors":[
            "Han-Dong Lim",
            "Donghwan Lee"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ICAFS: Inter-Client-Aware Feature Selection for Vertical Federated Learning",
        "abstract":"Vertical federated learning (VFL) enables a paradigm for vertically partitioned data across clients to collaboratively train machine learning models. Feature selection (FS) plays a crucial role in Vertical Federated Learning (VFL) due to the unique nature that data are distributed across multiple clients. In VFL, different clients possess distinct subsets of features for overlapping data samples, making the process of identifying and selecting the most relevant features a complex yet essential task. Previous FS efforts have primarily revolved around intra-client feature selection, overlooking vital feature interaction across clients, leading to subpar model outcomes. We introduce ICAFS, a novel multi-stage ensemble approach for effective FS in VFL by considering inter-client interactions. By employing conditional feature synthesis alongside multiple learnable feature selectors, ICAFS facilitates ensemble FS over these selectors using synthetic embeddings. This method bypasses the limitations of private gradient sharing and allows for model training using real data with refined embeddings. Experiments on multiple real-world datasets demonstrate that ICAFS surpasses current state-of-the-art methods in prediction accuracy.\n        △ Less",
        "authors":[
            "Ruochen Jin",
            "Boning Tong",
            "Shu Yang",
            "Bojian Hou",
            "Li Shen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?",
        "abstract":"With the rise of powerful foundation models, a pre-training-fine-tuning paradigm becomes increasingly popular these days: A foundation model is pre-trained using a huge amount of data from various sources, and then the downstream users only need to fine-tune and adapt it to specific downstream tasks. However, due to the high computation complexity of adversarial training, it is not feasible to fine-tune the foundation model to improve its robustness on the downstream task. Observing the above challenge, we want to improve the downstream robustness without updating\/accessing the weights in the foundation model. Inspired from existing literature in robustness inheritance (Kim et al., 2020), through theoretical investigation, we identify a close relationship between robust contrastive learning with the adversarial robustness of supervised learning. To further validate and utilize this theoretical insight, we design a simple-yet-effective robust auto-encoder as a data pre-processing method before feeding the data into the foundation model. The proposed approach has zero access to the foundation model when training the robust auto-encoder. Extensive experiments demonstrate the effectiveness of the proposed method in improving the robustness of downstream tasks, verifying the connection between the feature robustness (implied by small adversarial contrastive loss) and the robustness of the downstream task.\n        △ Less",
        "authors":[
            "Meiqi Liu",
            "Zhuoqun Huang",
            "Yue Xing"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations",
        "abstract":"Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a promising tool for generating semantic explanations of the decision-making processes in deep neural networks, having applications in both model improvement and understanding. It is vital that the explanation is accurate, or faithful, to the model, yet we identify several limitations of prior faithfulness metrics that inhibit an accurate evaluation; most notably, prior metrics involve only the set of concepts present, ignoring how they may be spatially distributed. We address these limitations with Surrogate Faithfulness (SF), an evaluation method that introduces a spatially-aware surrogate and two novel faithfulness metrics. Using SF, we produce Optimally Faithful (OF) explanations, where concepts are found that maximize faithfulness. Our experiments show that (1) adding spatial-awareness to prior U-CBEMs increases faithfulness in all cases; (2) OF produces significantly more faithful explanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's learned concepts generalize well to out-of-domain data and are more robust to adversarial examples, where prior U-CBEMs struggle.\n        △ Less",
        "authors":[
            "Shubham Kumar",
            "Dwip Dalal",
            "Narendra Ahuja"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Progressive Rock Music Classification",
        "abstract":"This study investigates the classification of progressive rock music, a genre characterized by complex compositions and diverse instrumentation, distinct from other musical styles. Addressing this Music Information Retrieval (MIR) task, we extracted comprehensive audio features, including spectrograms, Mel-Frequency Cepstral Coefficients (MFCCs), chromagrams, and beat positions from song snippets using the Librosa library. A winner-take-all voting strategy was employed to aggregate snippet-level predictions into final song classifications. We conducted a comparative analysis of various machine learning techniques. Ensemble methods, encompassing Bagging (Random Forest, ExtraTrees, Bagging Classifier) and Boosting (XGBoost, Gradient Boosting), were explored, utilizing Principal Component Analysis (PCA) for dimensionality reduction to manage computational constraints with high-dimensional feature sets. Additionally, deep learning approaches were investigated, including the development of custom 1D Convolutional Neural Network (1D CNN) architectures (named \"Zuck\" and \"Satya\") featuring specific layer configurations, normalization, and activation functions. Furthermore, we fine-tuned a state-of-the-art Audio Spectrogram Transformer (AST) model, leveraging its attention-based mechanisms for audio classification. Performance evaluation on validation and test sets revealed varying effectiveness across models, with ensemble methods like Extra Trees achieving test accuracies up to 76.38%. This research provides insights into the application and relative performance of diverse machine learning paradigms for the nuanced task of progressive rock genre classification.\n        △ Less",
        "authors":[
            "Arpan Nagar",
            "Joseph Bensabat",
            "Jokent Gaza",
            "Moinak Dey"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FHBench: Towards Efficient and Personalized Federated Learning for Multimodal Healthcare",
        "abstract":"Federated Learning (FL) has emerged as an effective solution for multi-institutional collaborations without sharing patient data, offering a range of methods tailored for diverse applications. However, real-world medical datasets are often multimodal, and computational resources are limited, posing significant challenges for existing FL approaches. Recognizing these limitations, we developed the Federated Healthcare Benchmark(FHBench), a benchmark specifically designed from datasets derived from real-world healthcare applications. FHBench encompasses critical diagnostic tasks across domains such as the nervous, cardiovascular, and respiratory systems and general pathology, providing comprehensive support for multimodal healthcare evaluations and filling a significant gap in existing benchmarks. Building on FHBench, we introduced Efficient Personalized Federated Learning with Adaptive LoRA(EPFL), a personalized FL framework that demonstrates superior efficiency and effectiveness across various healthcare modalities. Our results highlight the robustness of FHBench as a benchmarking tool and the potential of EPFL as an innovative approach to advancing healthcare-focused FL, addressing key limitations of existing methods.\n        △ Less",
        "authors":[
            "Penghao Wang",
            "Qian Chen",
            "Teng Zhang",
            "Yingwei Zhang",
            "Wang Lu",
            "Yiqiang Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Tabular foundation model to detect empathy from visual cues",
        "abstract":"Detecting empathy from video interactions is an emerging area of research. Video datasets, however, are often released as extracted features (i.e., tabular data) rather than raw footage due to privacy and ethical concerns. Prior research on such tabular datasets established tree-based classical machine learning approaches as the best-performing models. Motivated by the recent success of textual foundation models (i.e., large language models), we explore the use of tabular foundation models in empathy detection from tabular visual features. We experiment with two recent tabular foundation models $-$ TabPFN v2 and TabICL $-$ through in-context learning and fine-tuning setups. Our experiments on a public human-robot interaction benchmark demonstrate a significant boost in cross-subject empathy detection accuracy over several strong baselines (accuracy: $0.590 \\rightarrow 0.730$; AUC: $0.564 \\rightarrow 0.669$). In addition to performance improvement, we contribute novel insights and an evaluation setup to ensure generalisation on unseen subjects in this public benchmark. As the practice of releasing video features as tabular datasets is likely to persist due to privacy constraints, our findings will be widely applicable to future empathy detection video datasets as well.\n        △ Less",
        "authors":[
            "Md Rakibul Hasan",
            "Shafin Rahman",
            "Md Zakir Hossain",
            "Aneesh Krishna",
            "Tom Gedeon"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Power-scaled Bayesian Inference with Score-based Generative mModels",
        "abstract":"We propose a score-based generative algorithm for sampling from power-scaled priors and likelihoods within the Bayesian inference framework. Our algorithm enables flexible control over prior-likelihood influence without requiring retraining for different power-scaling configurations. Specifically, we focus on synthesizing seismic velocity models conditioned on imaged seismic. Our method enables sensitivity analysis by sampling from intermediate power posteriors, allowing us to assess the relative influence of the prior and likelihood on samples of the posterior distribution. Through a comprehensive set of experiments, we evaluate the effects of varying the power parameter in different settings: applying it solely to the prior, to the likelihood of a Bayesian formulation, and to both simultaneously. The results show that increasing the power of the likelihood up to a certain threshold improves the fidelity of posterior samples to the conditioning data (e.g., seismic images), while decreasing the prior power promotes greater structural diversity among samples. Moreover, we find that moderate scaling of the likelihood leads to a reduced shot data residual, confirming its utility in posterior refinement.\n        △ Less",
        "authors":[
            "Huseyin Tuna Erdinc",
            "Yunlin Zeng",
            "Abhinav Prakash Gahlot",
            "Felix J. Herrmann"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Wasserstein Distributionally Robust Regret Optimization",
        "abstract":"Distributionally Robust Optimization (DRO) is a popular framework for decision-making under uncertainty, but its adversarial nature can lead to overly conservative solutions. To address this, we study ex-ante Distributionally Robust Regret Optimization (DRRO), focusing on Wasserstein-based ambiguity sets which are popular due to their links to regularization and machine learning. We provide a systematic analysis of Wasserstein DRRO, paralleling known results for Wasserstein DRO. Under smoothness and regularity conditions, we show that Wasserstein DRRO coincides with Empirical Risk Minimization (ERM) up to first-order terms, and exactly so in convex quadratic settings. We revisit the Wasserstein DRRO newsvendor problem, where the loss is the maximum of two linear functions of demand and decision. Extending [25], we show that the regret can be computed by maximizing two one-dimensional concave functions. For more general loss functions involving the maximum of multiple linear terms in multivariate random variables and decision vectors, we prove that computing the regret and thus also the DRRO policy is NP-hard. We then propose a convex relaxation for these more general Wasserstein DRRO problems and demonstrate its strong empirical performance. Finally, we provide an upper bound on the optimality gap of our relaxation and show it improves over recent alternatives.\n        △ Less",
        "authors":[
            "Lukas-Benedikt Fiechtner",
            "Jose Blanchet"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SonicSieve: Bringing Directional Speech Extraction to Smartphones Using Acoustic Microstructures",
        "abstract":"Imagine placing your smartphone on a table in a noisy restaurant and clearly capturing the voices of friends seated around you, or recording a lecturer's voice with clarity in a reverberant auditorium. We introduce SonicSieve, the first intelligent directional speech extraction system for smartphones using a bio-inspired acoustic microstructure. Our passive design embeds directional cues onto incoming speech without any additional electronics. It attaches to the in-line mic of low-cost wired earphones which can be attached to smartphones. We present an end-to-end neural network that processes the raw audio mixtures in real-time on mobile devices. Our results show that SonicSieve achieves a signal quality improvement of 5.0 dB when focusing on a 30° angular region. Additionally, the performance of our system based on only two microphones exceeds that of conventional 5-microphone arrays.\n        △ Less",
        "authors":[
            "Kuang Yuan",
            "Yifeng Wang",
            "Xiyuxing Zhang",
            "Chengyi Shen",
            "Swarun Kumar",
            "Justin Chan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Visual Language Models show widespread visual deficits on neuropsychological tests",
        "abstract":"Visual Language Models (VLMs) show remarkable performance in visual reasoning tasks, successfully tackling college-level challenges that require high-level understanding of images. However, some recent reports of VLMs struggling to reason about elemental visual concepts like orientation, position, continuity, and occlusion suggest a potential gulf between human and VLM vision. Here we use the toolkit of neuropsychology to systematically assess the capabilities of three state-of-the-art VLMs across visual domains. Using 51 tests drawn from six clinical and experimental batteries, we characterise the visual abilities of leading VLMs relative to normative performance in healthy adults. While the models excel in straightforward object recognition tasks, we find widespread deficits in low- and mid-level visual abilities that would be considered clinically significant in humans. These selective deficits, profiled through validated test batteries, suggest that an artificial system can achieve complex object recognition without developing foundational visual concepts that in humans require no explicit training.\n        △ Less",
        "authors":[
            "Gene Tangtartharakul",
            "Katherine R. Storrs"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Network Emulation of the Classical Limit in Quantum Systems via Learned Observable Mappings",
        "abstract":"The classical limit of quantum mechanics, formally investigated through frameworks like strict deformation quantization, remains a profound area of inquiry in the philosophy of physics. This paper explores a computational approach employing a neural network to emulate the emergence of classical behavior from the quantum harmonic oscillator as Planck's constant $\\hbar$ approaches zero. We develop and train a neural network architecture to learn the mapping from initial expectation values and $\\hbar$ to the time evolution of the expectation value of position. By analyzing the network's predictions across different regimes of hbar, we aim to provide computational insights into the nature of the quantum-classical transition. This work demonstrates the potential of machine learning as a complementary tool for exploring foundational questions in quantum mechanics and its classical limit.\n        △ Less",
        "authors":[
            "Kamran Majid"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AtlasD: Automatic Local Symmetry Discovery",
        "abstract":"Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they fail to consider the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we formalize the notion of local symmetry as atlas equivariance. Our proposed pipeline, automatic local symmetry discovery (AtlasD), recovers the local symmetries of a function by training local predictor networks and then learning a Lie group basis to which the predictors are equivariant. We demonstrate AtlasD is capable of discovering local symmetry groups with multiple connected components in top-quark tagging and partial differential equation experiments. The discovered local symmetry is shown to be a useful inductive bias that improves the performance of downstream tasks in climate segmentation and vision tasks.\n        △ Less",
        "authors":[
            "Manu Bhat",
            "Jonghyun Park",
            "Jianke Yang",
            "Nima Dehmamy",
            "Robin Walters",
            "Rose Yu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Collaborative Bayesian Optimization via Wasserstein Barycenters",
        "abstract":"Motivated by the growing need for black-box optimization and data privacy, we introduce a collaborative Bayesian optimization (BO) framework that addresses both of these challenges. In this framework agents work collaboratively to optimize a function they only have oracle access to. In order to mitigate against communication and privacy constraints, agents are not allowed to share their data but can share their Gaussian process (GP) surrogate models. To enable collaboration under these constraints, we construct a central model to approximate the objective function by leveraging the concept of Wasserstein barycenters of GPs. This central model integrates the shared models without accessing the underlying data. A key aspect of our approach is a collaborative acquisition function that balances exploration and exploitation, allowing for the optimization of decision variables collaboratively in each iteration. We prove that our proposed algorithm is asymptotically consistent and that its implementation via Monte Carlo methods is numerically accurate. Through numerical experiments, we demonstrate that our approach outperforms other baseline collaborative frameworks and is competitive with centralized approaches that do not consider data privacy.\n        △ Less",
        "authors":[
            "Donglin Zhan",
            "Haoting Zhang",
            "Rhonda Righter",
            "Zeyu Zheng",
            "James Anderson"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients",
        "abstract":"As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low\/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.\n        △ Less",
        "authors":[
            "Ming Li",
            "Yanhong Li",
            "Ziyue Li",
            "Tianyi Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Auto-Test: Learning Semantic-Domain Constraints for Unsupervised Error Detection in Tables",
        "abstract":"Data cleaning is a long-standing challenge in data management. While powerful logic and statistical algorithms have been developed to detect and repair data errors in tables, existing algorithms predominantly rely on domain-experts to first manually specify data-quality constraints specific to a given table, before data cleaning algorithms can be applied.\n  In this work, we propose a new class of data-quality constraints that we call Semantic-Domain Constraints, which can be reliably inferred and automatically applied to any tables, without requiring domain-experts to manually specify on a per-table basis. We develop a principled framework to systematically learn such constraints from table corpora using large-scale statistical tests, which can further be distilled into a core set of constraints using our optimization framework, with provable quality guarantees. Extensive evaluations show that this new class of constraints can be used to both (1) directly detect errors on real tables in the wild, and (2) augment existing expert-driven data-cleaning techniques as a new class of complementary constraints.\n  Our extensively labeled benchmark dataset with 2400 real data columns, as well as our code are available at https:\/\/github.com\/qixuchen\/AutoTest to facilitate future research.\n        △ Less",
        "authors":[
            "Qixu Chen",
            "Yeye He",
            "Raymond Chi-Wing Wong",
            "Weiwei Cui",
            "Song Ge",
            "Haidong Zhang",
            "Dongmei Zhang",
            "Surajit Chaudhuri"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models",
        "abstract":"Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. We compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions. These findings highlight the importance of transparent decision processes in safety-critical domains and offer a promising direction for developing more interpretable autonomous driving systems.\n        △ Less",
        "authors":[
            "Amirhosein Chahe",
            "Lifeng Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"auto-fpt: Automating Free Probability Theory Calculations for Machine Learning Theory",
        "abstract":"A large part of modern machine learning theory often involves computing the high-dimensional expected trace of a rational expression of large rectangular random matrices. To symbolically compute such quantities using free probability theory, we introduce auto-fpt, a lightweight Python and SymPy-based tool that can automatically produce a reduced system of fixed-point equations which can be solved for the quantities of interest, and effectively constitutes a theory. We overview the algorithmic ideas underlying auto-fpt and its applications to various interesting problems, such as the high-dimensional error of linearized feed-forward neural networks, recovering well-known results. We hope that auto-fpt streamlines the majority of calculations involved in high-dimensional analysis, while helping the machine learning community reproduce known and uncover new phenomena.\n        △ Less",
        "authors":[
            "Arjun Subramonian",
            "Elvis Dohmatob"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Epistemic Uncertainty-aware Recommendation Systems via Bayesian Deep Ensemble Learning",
        "abstract":"Recommending items to users has long been a fundamental task, and studies have tried to improve it ever since. Most well-known models commonly employ representation learning to map users and items into a unified embedding space for matching assessment. These approaches have primary limitations, especially when dealing with explicit feedback and sparse data contexts. Two primary limitations are their proneness to overfitting and failure to incorporate epistemic uncertainty in predictions. To address these problems, we propose a novel Bayesian Deep Ensemble Collaborative Filtering method named BDECF. To improve model generalization and quality, we utilize Bayesian Neural Networks, which incorporate uncertainty within their weight parameters. In addition, we introduce a new interpretable non-linear matching approach for the user and item embeddings, leveraging the advantages of the attention mechanism. Furthermore, we endorse the implementation of an ensemble-based supermodel to generate more robust and reliable predictions, resulting in a more complete model. Empirical evaluation through extensive experiments and ablation studies across a range of publicly accessible real-world datasets with differing sparsity characteristics confirms our proposed method's effectiveness and the importance of its components.\n        △ Less",
        "authors":[
            "Radin Cheraghi",
            "Amir Mohammad Mahfoozi",
            "Sepehr Zolfaghari",
            "Mohammadshayan Shabani",
            "Maryam Ramezani",
            "Hamid R. Rabiee"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Time-varying EEG spectral power predicts evoked and spontaneous fMRI motor brain activity",
        "abstract":"Simultaneous EEG-fMRI recordings are increasingly used to investigate brain activity by leveraging the complementary high spatial and high temporal resolution of fMRI and EEG signals respectively. It remains unclear, however, to what degree these two imaging modalities capture shared information about neural activity. Here, we investigate whether it is possible to predict both task-evoked and spontaneous fMRI signals of motor brain networks from EEG time-varying spectral power using interpretable models trained for individual subjects with Sparse Group Lasso regularization. Critically, we test the trained models on data acquired from each subject on a different day and obtain statistical validation by comparison with appropriate null models as well as the conventional EEG sensorimotor rhythm. We find significant prediction results in most subjects, although less frequently for resting-state compared to task-based conditions. Furthermore, we interpret the model learned parameters to understand representations of EEG-fMRI coupling in terms of predictive EEG channels, frequencies, and haemodynamic delays. In conclusion, our work provides evidence of the ability to predict fMRI motor brain activity from EEG recordings alone across different days, in both task-evoked and spontaneous conditions, with statistical significance in individual subjects. These results present great potential for translation to EEG neurofeedback applications.\n        △ Less",
        "authors":[
            "Neil Mehta",
            "Ines Goncalves",
            "Alberto Montagna",
            "Mathis Fleury",
            "Gustavo Caetano",
            "Ines Esteves",
            "Athanasios Vourvopoulos",
            "Pulkit Grover",
            "Patricia Figueiredo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Real-time Seafloor Segmentation and Mapping",
        "abstract":"Posidonia oceanica meadows are a species of seagrass highly dependent on rocks for their survival and conservation. In recent years, there has been a concerning global decline in this species, emphasizing the critical need for efficient monitoring and assessment tools. While deep learning-based semantic segmentation and visual automated monitoring systems have shown promise in a variety of applications, their performance in underwater environments remains challenging due to complex water conditions and limited datasets. This paper introduces a framework that combines machine learning and computer vision techniques to enable an autonomous underwater vehicle (AUV) to inspect the boundaries of Posidonia oceanica meadows autonomously. The framework incorporates an image segmentation module using an existing Mask R-CNN model and a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a new class dedicated to rocks is introduced to enhance the existing model, aiming to contribute to a comprehensive monitoring approach and provide a deeper understanding of the intricate interactions between the meadow and its surrounding environment. The image segmentation model is validated using real underwater images, while the overall inspection framework is evaluated in a realistic simulation environment, replicating actual monitoring scenarios with real underwater images. The results demonstrate that the proposed framework enables the AUV to autonomously accomplish the main tasks of underwater inspection and segmentation of rocks. Consequently, this work holds significant potential for the conservation and protection of marine environments, providing valuable insights into the status of Posidonia oceanica meadows and supporting targeted preservation efforts\n        △ Less",
        "authors":[
            "Michele Grimaldi",
            "Nouf Alkaabi",
            "Francesco Ruscio",
            "Sebastian Realpe Rua",
            "Rafael Garcia",
            "Nuno Gracias"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hearing Anywhere in Any Environment",
        "abstract":"In mixed reality applications, a realistic acoustic experience in spatial environments is as crucial as the visual experience for achieving true immersion. Despite recent advances in neural approaches for Room Impulse Response (RIR) estimation, most existing methods are limited to the single environment on which they are trained, lacking the ability to generalize to new rooms with different geometries and surface materials. We aim to develop a unified model capable of reconstructing the spatial acoustic experience of any environment with minimum additional measurements. To this end, we present xRIR, a framework for cross-room RIR prediction. The core of our generalizable approach lies in combining a geometric feature extractor, which captures spatial context from panorama depth images, with a RIR encoder that extracts detailed acoustic features from only a few reference RIR samples. To evaluate our method, we introduce ACOUSTICROOMS, a new dataset featuring high-fidelity simulation of over 300,000 RIRs from 260 rooms. Experiments show that our method strongly outperforms a series of baselines. Furthermore, we successfully perform sim-to-real transfer by evaluating our model on four real-world environments, demonstrating the generalizability of our approach and the realism of our dataset.\n        △ Less",
        "authors":[
            "Xiulong Liu",
            "Anurag Kumar",
            "Paul Calamia",
            "Sebastia V. Amengual",
            "Calvin Murdock",
            "Ishwarya Ananthabhotla",
            "Philip Robinson",
            "Eli Shlizerman",
            "Vamsi Krishna Ithapu",
            "Ruohan Gao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates",
        "abstract":"The rapid growth of intelligent connected vehicles (ICVs) and integrated vehicle-road-cloud systems has increased the demand for accurate, real-time HD map updates. However, ensuring map reliability remains challenging due to inconsistencies in crowdsourced data, which suffer from motion blur, lighting variations, adverse weather, and lane marking degradation. This paper introduces CleanMAP, a Multimodal Large Language Model (MLLM)-based distillation framework designed to filter and refine crowdsourced data for high-confidence HD map updates. CleanMAP leverages an MLLM-driven lane visibility scoring model that systematically quantifies key visual parameters, assigning confidence scores (0-10) based on their impact on lane detection. A novel dynamic piecewise confidence-scoring function adapts scores based on lane visibility, ensuring strong alignment with human evaluations while effectively filtering unreliable data. To further optimize map accuracy, a confidence-driven local map fusion strategy ranks and selects the top-k highest-scoring local maps within an optimal confidence range (best score minus 10%), striking a balance between data quality and quantity. Experimental evaluations on a real-world autonomous vehicle dataset validate CleanMAP's effectiveness, demonstrating that fusing the top three local maps achieves the lowest mean map update error of 0.28m, outperforming the baseline (0.37m) and meeting stringent accuracy thresholds (<= 0.32m). Further validation with real-vehicle data confirms 84.88% alignment with human evaluators, reinforcing the model's robustness and reliability. This work establishes CleanMAP as a scalable and deployable solution for crowdsourced HD map updates, ensuring more precise and reliable autonomous navigation. The code will be available at https:\/\/Ankit-Zefan.github.io\/CleanMap\/\n        △ Less",
        "authors":[
            "Ankit Kumar Shaw",
            "Kun Jiang",
            "Tuopu Wen",
            "Chandan Kumar Sah",
            "Yining Shi",
            "Mengmeng Yang",
            "Diange Yang",
            "Xiaoli Lian"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization",
        "abstract":"As model sizes grow, finding efficient and cost-effective hyperparameter optimization (HPO) methods becomes increasingly crucial for deep learning pipelines. While multi-fidelity HPO (MF-HPO) trades off computational resources required for DL training with lower fidelity estimations, existing fidelity sources often fail under lower compute and memory constraints. We propose a novel fidelity source: the number of layers that are trained or frozen during training. For deep networks, this approach offers significant compute and memory savings while preserving rank correlations between hyperparameters at low fidelities compared to full model training. We demonstrate this in our empirical evaluation across ResNets and Transformers and additionally analyze the utility of frozen layers as a fidelity in using GPU resources as a fidelity in HPO, and for a combined MF-HPO with other fidelity sources. This contribution opens new applications for MF-HPO with hardware resources as a fidelity and creates opportunities for improved algorithms navigating joint fidelity spaces.\n        △ Less",
        "authors":[
            "Timur Carstensen",
            "Neeratyoy Mallik",
            "Frank Hutter",
            "Martin Rapp"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cross-Problem Parameter Transfer in Quantum Approximate Optimization Algorithm: A Machine Learning Approach",
        "abstract":"Quantum Approximate Optimization Algorithm (QAOA) is one of the most promising candidates to achieve the quantum advantage in solving combinatorial optimization problems. The process of finding a good set of variational parameters in the QAOA circuit has proven to be challenging due to multiple factors, such as barren plateaus. As a result, there is growing interest in exploiting parameter transferability, where parameter sets optimized for one problem instance are transferred to another that could be more complex either to estimate the solution or to serve as a warm start for further optimization. But can we transfer parameters from one class of problems to another? Leveraging parameter sets learned from a well-studied class of problems could help navigate the less studied one, reducing optimization overhead and mitigating performance pitfalls. In this paper, we study whether pretrained QAOA parameters of MaxCut can be used as is or to warm start the Maximum Independent Set (MIS) circuits. Specifically, we design machine learning models to find good donor candidates optimized on MaxCut and apply their parameters to MIS acceptors. Our experimental results show that such parameter transfer can significantly reduce the number of optimization iterations required while achieving comparable approximation ratios.\n        △ Less",
        "authors":[
            "Kien X. Nguyen",
            "Bao Bach",
            "Ilya Safro"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving",
        "abstract":"Deploying large language models (LLMs) presents critical challenges due to the inherent trade-offs associated with key performance metrics, such as latency, accuracy, and throughput. Typically, gains in one metric is accompanied with degradation in others. Early-Exit LLMs (EE-LLMs) efficiently navigate this trade-off space by skipping some of the later model layers when it confidently finds an output token early, thus reducing latency without impacting accuracy. However, as the early exits taken depend on the task and are unknown apriori to request processing, EE-LLMs conservatively load the entire model, limiting resource savings and throughput. Also, current frameworks statically select a model for a user task, limiting our ability to adapt to changing nature of the input queries.\n  We propose HELIOS to address these challenges. First, HELIOS shortlists a set of candidate LLMs, evaluates them using a subset of prompts, gathering telemetry data in real-time. Second, HELIOS uses the early exit data from these evaluations to greedily load the selected model only up to a limited number of layers. This approach yields memory savings which enables us to process more requests at the same time, thereby improving throughput. Third, HELIOS monitors and periodically reassesses the performance of the candidate LLMs and if needed, switches to another model that can service incoming queries more efficiently (such as using fewer layers without lowering accuracy). Our evaluations show that HELIOS achieves 1.48$\\times$ throughput, 1.10$\\times$ energy-efficiency, 1.39$\\times$ lower response time, and 3.7$\\times$ improvements in inference batch sizes compared to the baseline, when optimizing for the respective service level objectives.\n        △ Less",
        "authors":[
            "Avinash Kumar",
            "Shashank Nag",
            "Jason Clemons",
            "Lizy John",
            "Poulami Das"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Deep Operator Networks (DeepONet) for Acoustic Full Waveform Inversion (FWI)",
        "abstract":"Full Waveform Inversion (FWI) is an important geophysical technique considered in subsurface property prediction. It solves the inverse problem of predicting high-resolution Earth interior models from seismic data. Traditional FWI methods are computationally demanding. Inverse problems in geophysics often face challenges of non-uniqueness due to limited data, as data are often collected only on the surface. In this study, we introduce a novel methodology that leverages Deep Operator Networks (DeepONet) to attempt to improve both the efficiency and accuracy of FWI. The proposed DeepONet methodology inverts seismic waveforms for the subsurface velocity field. This approach is able to capture some key features of the subsurface velocity field. We have shown that the architecture can be applied to noisy seismic data with an accuracy that is better than some other machine learning methods. We also test our proposed method with out-of-distribution prediction for different velocity models. The proposed DeepONet shows comparable and better accuracy in some velocity models than some other machine learning methods. To improve the FWI workflow, we propose using the DeepONet output as a starting model for conventional FWI and that it may improve FWI performance. While we have only shown that DeepONet facilitates faster convergence than starting with a homogeneous velocity field, it may have some benefits compared to other approaches to constructing starting models. This integration of DeepONet into FWI may accelerate the inversion process and may also enhance its robustness and reliability.\n        △ Less",
        "authors":[
            "Kamaljyoti Nath",
            "Khemraj Shukla",
            "Victor C. Tsai",
            "Umair bin Waheed",
            "Christian Huber",
            "Omer Alpak",
            "Chuen-Song Chen",
            "Ligang Lu",
            "Amik St-Cyr"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Distinct hydrologic response patterns and trends worldwide revealed by physics-embedded learning",
        "abstract":"To track rapid changes within our water sector, Global Water Models (GWMs) need to realistically represent hydrologic systems' response patterns - such as baseflow fraction - but are hindered by their limited ability to learn from data. Here we introduce a high-resolution physics-embedded big-data-trained model as a breakthrough in reliably capturing characteristic hydrologic response patterns ('signatures') and their shifts. By realistically representing the long-term water balance, the model revealed widespread shifts - up to ~20% over 20 years - in fundamental green-blue-water partitioning and baseflow ratios worldwide. Shifts in these response patterns, previously considered static, contributed to increasing flood risks in northern mid-latitudes, heightening water supply stresses in southern subtropical regions, and declining freshwater inputs to many European estuaries, all with ecological implications. With more accurate simulations at monthly and daily scales than current operational systems, this next-generation model resolves large, nonlinear seasonal runoff responses to rainfall ('elasticity') and streamflow flashiness in semi-arid and arid regions. These metrics highlight regions with management challenges due to large water supply variability and high climate sensitivity, but also provide tools to forecast seasonal water availability. This capability newly enables global-scale models to deliver reliable and locally relevant insights for water management.\n        △ Less",
        "authors":[
            "Haoyu Ji",
            "Yalan Song",
            "Tadd Bindas",
            "Chaopeng Shen",
            "Yuan Yang",
            "Ming Pan",
            "Jiangtao Liu",
            "Farshid Rahmani",
            "Ather Abbas",
            "Hylke Beck",
            "Yoshihide Wada",
            "Kathryn Lawson"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PDSP-Bench: A Benchmarking System for Parallel and Distributed Stream Processing",
        "abstract":"The paper introduces PDSP-Bench, a novel benchmarking system designed for a systematic understanding of performance of parallel stream processing in a distributed environment. Such an understanding is essential for determining how Stream Processing Systems (SPS) use operator parallelism and the available resources to process massive workloads of modern applications. Existing benchmarking systems focus on analyzing SPS using queries with sequential operator pipelines within a homogeneous centralized environment. Quite differently, PDSP-Bench emphasizes the aspects of parallel stream processing in a distributed heterogeneous environment and simultaneously allows the integration of machine learning models for SPS workloads. In our results, we benchmark a well-known SPS, Apache Flink, using parallel query structures derived from real-world applications and synthetic queries to show the capabilities of PDSP-Bench towards parallel stream processing. Moreover, we compare different learned cost models using generated SPS workloads on PDSP-Bench by showcasing their evaluations on model and training efficiency. We present key observations from our experiments using PDSP-Bench that highlight interesting trends given different query workloads, such as non-linearity and paradoxical effects of parallelism on the performance.\n        △ Less",
        "authors":[
            "Pratyush Agnihotri",
            "Boris Koldehofe",
            "Roman Heinrich",
            "Carsten Binnig",
            "Manisha Luthra"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Jailbreak Tax: How Useful are Your Jailbreak Outputs?",
        "abstract":"Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs. In this paper, we ask whether the model outputs produced by existing jailbreaks are actually useful. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions? Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math). Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the jailbreak tax. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy. Overall, our work proposes the jailbreak tax as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks. We make the benchmark available at https:\/\/github.com\/ethz-spylab\/jailbreak-tax\n        △ Less",
        "authors":[
            "Kristina Nikolić",
            "Luze Sun",
            "Jie Zhang",
            "Florian Tramèr"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Keyword Extraction, and Aspect Classification in Sinhala, English, and Code-Mixed Content",
        "abstract":"Brand reputation in the banking sector is maintained through insightful analysis of customer opinion on code-mixed and multilingual content. Conventional NLP models misclassify or ignore code-mixed text, when mix with low resource languages such as Sinhala-English and fail to capture domain-specific knowledge. This study introduces a hybrid NLP method to improve keyword extraction, content filtering, and aspect-based classification of banking content. Keyword extraction in English is performed with a hybrid approach comprising a fine-tuned SpaCy NER model, FinBERT-based KeyBERT embeddings, YAKE, and EmbedRank, which results in a combined accuracy of 91.2%. Code-mixed and Sinhala keywords are extracted using a fine-tuned XLM-RoBERTa model integrated with a domain-specific Sinhala financial vocabulary, and it results in an accuracy of 87.4%. To ensure data quality, irrelevant comment filtering was performed using several models, with the BERT-base-uncased model achieving 85.2% for English and XLM-RoBERTa 88.1% for Sinhala, which was better than GPT-4o, SVM, and keyword-based filtering. Aspect classification followed the same pattern, with the BERT-base-uncased model achieving 87.4% for English and XLM-RoBERTa 85.9% for Sinhala, both exceeding GPT-4 and keyword-based approaches. These findings confirm that fine-tuned transformer models outperform traditional methods in multilingual financial text analysis. The present framework offers an accurate and scalable solution for brand reputation monitoring in code-mixed and low-resource banking environments.\n        △ Less",
        "authors":[
            "F. A. Rizvi",
            "T. Navojith",
            "A. M. N. H. Adhikari",
            "W. P. U. Senevirathna",
            "Dharshana Kasthurirathna",
            "Lakmini Abeywardhana"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning",
        "abstract":"In this paper, we present a multi-agent reinforcement learning (MARL) framework for optimizing tissue repair processes using engineered biological agents. Our approach integrates: (1) stochastic reaction-diffusion systems modeling molecular signaling, (2) neural-like electrochemical communication with Hebbian plasticity, and (3) a biologically informed reward function combining chemical gradient tracking, neural synchronization, and robust penalties. A curriculum learning scheme guides the agent through progressively complex repair scenarios. In silico experiments demonstrate emergent repair strategies, including dynamic secretion control and spatial coordination.\n        △ Less",
        "authors":[
            "Muhammad Al-Zafar Khan",
            "Jamal Al-Karaki"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Perturbed State Space Feature Encoders for Optical Flow with Event Cameras",
        "abstract":"With their motion-responsive nature, event-based cameras offer significant advantages over traditional cameras for optical flow estimation. While deep learning has improved upon traditional methods, current neural networks adopted for event-based optical flow still face temporal and spatial reasoning limitations. We propose Perturbed State Space Feature Encoders (P-SSE) for multi-frame optical flow with event cameras to address these challenges. P-SSE adaptively processes spatiotemporal features with a large receptive field akin to Transformer-based methods, while maintaining the linear computational complexity characteristic of SSMs. However, the key innovation that enables the state-of-the-art performance of our model lies in our perturbation technique applied to the state dynamics matrix governing the SSM system. This approach significantly improves the stability and performance of our model. We integrate P-SSE into a framework that leverages bi-directional flows and recurrent connections, expanding the temporal context of flow prediction. Evaluations on DSEC-Flow and MVSEC datasets showcase P-SSE's superiority, with 8.48% and 11.86% improvements in EPE performance, respectively.\n        △ Less",
        "authors":[
            "Gokul Raju Govinda Raju",
            "Nikola Zubić",
            "Marco Cannici",
            "Davide Scaramuzza"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Emotion Alignment: Discovering the Gap Between Social Media and Real-World Sentiments in Persian Tweets and Images",
        "abstract":"In contemporary society, widespread social media usage is evident in people's daily lives. Nevertheless, disparities in emotional expressions between the real world and online platforms can manifest. We comprehensively analyzed Persian community on X to explore this phenomenon. An innovative pipeline was designed to measure the similarity between emotions in the real world compared to social media. Accordingly, recent tweets and images of participants were gathered and analyzed using Transformers-based text and image sentiment analysis modules. Each participant's friends also provided insights into the their real-world emotions. A distance criterion was used to compare real-world feelings with virtual experiences. Our study encompassed N=105 participants, 393 friends who contributed their perspectives, over 8,300 collected tweets, and 2,000 media images. Results indicated a 28.67% similarity between images and real-world emotions, while tweets exhibited a 75.88% alignment with real-world feelings. Additionally, the statistical significance confirmed that the observed disparities in sentiment proportions.\n        △ Less",
        "authors":[
            "Sina Elahimanesh",
            "Mohammadali Mohammadkhani",
            "Shohreh Kasaei"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transfer Learning Assisted XgBoost For Adaptable Cyberattack Detection In Battery Packs",
        "abstract":"Optimal charging of electric vehicle (EVs) depends heavily on reliable sensor measurements from the battery pack to the cloud-controller of the smart charging station. However, an adversary could corrupt the voltage sensor data during transmission, potentially causing local to wide-scale disruptions. Therefore, it is essential to detect sensor cyberattacks in real-time to ensure secure EV charging, and the developed algorithms must be readily adaptable to variations, including pack configurations. To tackle these challenges, we propose adaptable fine-tuning of an XgBoost-based cell-level model using limited pack-level data to use for voltage prediction and residual generation. We used battery cell and pack data from high-fidelity charging experiments in PyBaMM and `liionpack' package to train and test the detection algorithm. The algorithm's performance has been evaluated for two large-format battery packs under sensor swapping and replay attacks. The simulation results also highlight the adaptability and efficacy of our proposed detection algorithm.\n        △ Less",
        "authors":[
            "Sanchita Ghosh",
            "Tanushree Roy"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MatterTune: An Integrated, User-Friendly Platform for Fine-Tuning Atomistic Foundation Models to Accelerate Materials Simulation and Discovery",
        "abstract":"Geometric machine learning models such as graph neural networks have achieved remarkable success in recent years in chemical and materials science research for applications such as high-throughput virtual screening and atomistic simulations. The success of these models can be attributed to their ability to effectively learn latent representations of atomic structures directly from the training data. Conversely, this also results in high data requirements for these models, hindering their application to problems which are data sparse which are common in this domain. To address this limitation, there is a growing development in the area of pre-trained machine learning models which have learned general, fundamental, geometric relationships in atomistic data, and which can then be fine-tuned to much smaller application-specific datasets. In particular, models which are pre-trained on diverse, large-scale atomistic datasets have shown impressive generalizability and flexibility to downstream applications, and are increasingly referred to as atomistic foundation models. To leverage the untapped potential of these foundation models, we introduce MatterTune, a modular and extensible framework that provides advanced fine-tuning capabilities and seamless integration of atomistic foundation models into downstream materials informatics and simulation workflows, thereby lowering the barriers to adoption and facilitating diverse applications in materials science. In its current state, MatterTune supports a number of state-of-the-art foundation models such as ORB, MatterSim, JMP, and EquformerV2, and hosts a wide range of features including a modular and flexible design, distributed and customizable fine-tuning, broad support for downstream informatics tasks, and more.\n        △ Less",
        "authors":[
            "Lingyu Kong",
            "Nima Shoghi",
            "Guoxiang Hu",
            "Pan Li",
            "Victor Fung"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Contractivity of Stochastic Interpolation Flow",
        "abstract":"We investigate stochastic interpolation, a recently introduced framework for high dimensional sampling which bears many similarities to diffusion modeling. Stochastic interpolation generates a data sample by first randomly initializing a particle drawn from a simple base distribution, then simulating deterministic or stochastic dynamics such that in finite time the particle's distribution converges to the target. We show that for a Gaussian base distribution and a strongly log-concave target distribution, the stochastic interpolation flow map is Lipschitz with a sharp constant which matches that of Caffarelli's theorem for optimal transport maps. We are further able to construct Lipschitz transport maps between non-Gaussian distributions, generalizing some recent constructions in the literature on transport methods for establishing functional inequalities. We discuss the practical implications of our theorem for the sampling and estimation problems required by stochastic interpolation.\n        △ Less",
        "authors":[
            "Max Daniels"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Weight-of-Thought Reasoning: Exploring Neural Network Weights for Enhanced LLM Reasoning",
        "abstract":"Large language models (LLMs) have demonstrated remarkable reasoning capabilities when prompted with strategies such as Chain-of-Thought (CoT). However, these approaches focus on token-level output without considering internal weight dynamics. We introduce Weight-of-Thought (WoT) reasoning, a novel approach that examines neural network weights before inference to identify reasoning pathways. Unlike existing methods, WoT explores the weight space through graph-based message passing, multi-step reasoning processes, and attention mechanisms. Our implementation creates an interconnected graph of reasoning nodes. Experiments on diverse reasoning tasks (syllogistic, mathematical, algebraic, combinatorial, and geometric) demonstrate that WoT achieves superior performance compared to traditional methods, particularly for complex problems. This approach leads to both improved performance and greater interpretability of the reasoning process, offering a promising direction for enhancing LLM reasoning capabilities.\n        △ Less",
        "authors":[
            "Saif Punjwani",
            "Larry Heck"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Better Estimation of the KL Divergence Between Language Models",
        "abstract":"Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to the use of sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance, and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is also unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially in practice. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient.\n        △ Less",
        "authors":[
            "Afra Amini",
            "Tim Vieira",
            "Ryan Cotterell"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SPreV",
        "abstract":"SPREV, short for hyperSphere Reduced to two-dimensional Regular Polygon for Visualisation, is a novel dimensionality reduction technique developed to address the challenges of reducing dimensions and visualizing labeled datasets that exhibit a unique combination of three characteristics: small class size, high dimensionality, and low sample size. SPREV is designed not only to uncover but also to visually represent hidden patterns within such datasets. Its distinctive integration of geometric principles, adapted for discrete computational environments, makes it an indispensable tool in the modern data science toolkit, enabling users to identify trends, extract insights, and navigate complex data efficiently and effectively.\n        △ Less",
        "authors":[
            "Srivathsan Amruth"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling",
        "abstract":"Generative models often map noise to data by matching flows or scores, but these approaches become cumbersome for incorporating partial observations or additional priors. Inspired by recent advances in Wasserstein gradient flows, we propose Energy Matching, a framework that unifies flow-based approaches with the flexibility of energy-based models (EBMs). Far from the data manifold, samples move along curl-free, optimal transport paths from noise to data. As they approach the data manifold, an entropic energy term guides the system into a Boltzmann equilibrium distribution, explicitly capturing the underlying likelihood structure of the data. We parameterize this dynamic with a single time-independent scalar field, which serves as both a powerful generator and a flexible prior for effective regularization of inverse problems. Our method substantially outperforms existing EBMs on CIFAR-10 generation (FID 3.97 compared to 8.61), while retaining the simulation-free training of transport-based approaches away from the data manifold. Additionally, we exploit the flexibility of our method and introduce an interaction energy for diverse mode exploration. Our approach focuses on learning a static scalar potential energy -- without time conditioning, auxiliary generators, or additional networks -- marking a significant departure from recent EBM methods. We believe this simplified framework significantly advances EBM capabilities and paves the way for their broader adoption in generative modeling across diverse domains.\n        △ Less",
        "authors":[
            "Michal Balcerak",
            "Tamaz Amiranashvili",
            "Suprosanna Shit",
            "Antonio Terpin",
            "Sebastian Kaltenbach",
            "Petros Koumoutsakos",
            "Bjoern Menze"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond Worst-Case Online Classification: VC-Based Regret Bounds for Relaxed Benchmarks",
        "abstract":"We revisit online binary classification by shifting the focus from competing with the best-in-class binary loss to competing against relaxed benchmarks that capture smoothed notions of optimality. Instead of measuring regret relative to the exact minimal binary error -- a standard approach that leads to worst-case bounds tied to the Littlestone dimension -- we consider comparing with predictors that are robust to small input perturbations, perform well under Gaussian smoothing, or maintain a prescribed output margin. Previous examples of this were primarily limited to the hinge loss. Our algorithms achieve regret guarantees that depend only on the VC dimension and the complexity of the instance space (e.g., metric entropy), and notably, they incur only an $O(\\log(1\/γ))$ dependence on the generalized margin $γ$. This stands in contrast to most existing regret bounds, which typically exhibit a polynomial dependence on $1\/γ$. We complement this with matching lower bounds. Our analysis connects recent ideas from adversarial robustness and smoothed online learning.\n        △ Less",
        "authors":[
            "Omar Montasser",
            "Abhishek Shetty",
            "Nikita Zhivotovskiy"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FLOWR: Flow Matching for Structure-Aware De Novo, Interaction- and Fragment-Based Ligand Generation",
        "abstract":"We introduce FLOWR, a novel structure-based framework for the generation and optimization of three-dimensional ligands. FLOWR integrates continuous and categorical flow matching with equivariant optimal transport, enhanced by an efficient protein pocket conditioning. Alongside FLOWR, we present SPINDR, a thoroughly curated dataset comprising ligand-pocket co-crystal complexes specifically designed to address existing data quality issues. Empirical evaluations demonstrate that FLOWR surpasses current state-of-the-art diffusion- and flow-based methods in terms of PoseBusters-validity, pose accuracy, and interaction recovery, while offering a significant inference speedup, achieving up to 70-fold faster performance. In addition, we introduce FLOWR.multi, a highly accurate multi-purpose model allowing for the targeted sampling of novel ligands that adhere to predefined interaction profiles and chemical substructures for fragment-based design without the need of re-training or any re-sampling strategies\n        △ Less",
        "authors":[
            "Julian Cremer",
            "Ross Irwin",
            "Alessandro Tibot",
            "Jon Paul Janet",
            "Simon Olsson",
            "Djork-Arné Clevert"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Self-Controlled Dynamic Expansion Model for Continual Learning",
        "abstract":"Continual Learning (CL) epitomizes an advanced training paradigm wherein prior data samples remain inaccessible during the acquisition of new tasks. Numerous investigations have delved into leveraging a pre-trained Vision Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless, these approaches typically utilize a singular, static backbone, which inadequately adapts to novel tasks, particularly when engaging with diverse data domains, due to a substantial number of inactive parameters. This paper addresses this limitation by introducing an innovative Self-Controlled Dynamic Expansion Model (SCDEM), which orchestrates multiple distinct trainable pre-trained ViT backbones to furnish diverse and semantically enriched representations. Specifically, by employing the multi-backbone architecture as a shared module, the proposed SCDEM dynamically generates a new expert with minimal parameters to accommodate a new task. A novel Collaborative Optimization Mechanism (COM) is introduced to synergistically optimize multiple backbones by harnessing prediction signals from historical experts, thereby facilitating new task learning without erasing previously acquired knowledge. Additionally, a novel Feature Distribution Consistency (FDC) approach is proposed to align semantic similarity between previously and currently learned representations through an optimal transport distance-based mechanism, effectively mitigating negative knowledge transfer effects. Furthermore, to alleviate over-regularization challenges, this paper presents a novel Dynamic Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the penalization intensity on each trainable representation layer. An extensive series of experiments have been conducted to evaluate the proposed methodology's efficacy, with empirical results corroborating that the approach attains state-of-the-art performance.\n        △ Less",
        "authors":[
            "Runqing Wu",
            "Kaihui Huang",
            "Hanyi Zhang",
            "Fei Ye"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Molecular Learning Dynamics",
        "abstract":"We apply the physics-learning duality to molecular systems by complementing the physical description of interacting particles with a dual learning description, where each particle is modeled as an agent minimizing a loss function. In the traditional physics framework, the equations of motion are derived from the Lagrangian function, while in the learning framework, the same equations emerge from learning dynamics driven by the agent loss function. The loss function depends on scalar quantities that describe invariant properties of all other agents or particles. To demonstrate this approach, we first infer the loss functions of oxygen and hydrogen directly from a dataset generated by the CP2K physics-based simulation of water molecules. We then employ the loss functions to develop a learning-based simulation of water molecules, which achieves comparable accuracy while being significantly more computationally efficient than standard physics-based simulations.\n        △ Less",
        "authors":[
            "Yaroslav Gusev",
            "Vitaly Vanchurin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Process Reward Model Training via Active Learning",
        "abstract":"Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.\n        △ Less",
        "authors":[
            "Keyu Duan",
            "Zichen Liu",
            "Xin Mao",
            "Tianyu Pang",
            "Changyu Chen",
            "Qiguang Chen",
            "Michael Qizhe Shieh",
            "Longxu Dou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"VAE-based Feature Disentanglement for Data Augmentation and Compression in Generalized GNSS Interference Classification",
        "abstract":"Distributed learning and Edge AI necessitate efficient data processing, low-latency communication, decentralized model training, and stringent data privacy to facilitate real-time intelligence on edge devices while reducing dependency on centralized infrastructure and ensuring high model performance. In the context of global navigation satellite system (GNSS) applications, the primary objective is to accurately monitor and classify interferences that degrade system performance in distributed environments, thereby enhancing situational awareness. To achieve this, machine learning (ML) models can be deployed on low-resource devices, ensuring minimal communication latency and preserving data privacy. The key challenge is to compress ML models while maintaining high classification accuracy. In this paper, we propose variational autoencoders (VAEs) for disentanglement to extract essential latent features that enable accurate classification of interferences. We demonstrate that the disentanglement approach can be leveraged for both data compression and data augmentation by interpolating the lower-dimensional latent representations of signal power. To validate our approach, we evaluate three VAE variants - vanilla, factorized, and conditional generative - on four distinct datasets, including two collected in controlled indoor environments and two real-world highway datasets. Additionally, we conduct extensive hyperparameter searches to optimize performance. Our proposed VAE achieves a data compression rate ranging from 512 to 8,192 and achieves an accuracy up to 99.92%.\n        △ Less",
        "authors":[
            "Lucas Heublein",
            "Simon Kocher",
            "Tobias Feigl",
            "Alexander Rügamer",
            "Christopher Mutschler",
            "Felix Ott"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond the Generative Learning Trilemma: Generative Model Assessment in Data Scarcity Domains",
        "abstract":"Data scarcity remains a critical bottleneck impeding technological advancements across various domains, including but not limited to medicine and precision agriculture. To address this challenge, we explore the potential of Deep Generative Models (DGMs) in producing synthetic data that satisfies the Generative Learning Trilemma: fidelity, diversity, and sampling efficiency. However, recognizing that these criteria alone are insufficient for practical applications, we extend the trilemma to include utility, robustness, and privacy, factors crucial for ensuring the applicability of DGMs in real-world scenarios. Evaluating these metrics becomes particularly challenging in data-scarce environments, as DGMs traditionally rely on large datasets to perform optimally. This limitation is especially pronounced in domains like medicine and precision agriculture, where ensuring acceptable model performance under data constraints is vital. To address these challenges, we assess the Generative Learning Trilemma in data-scarcity settings using state-of-the-art evaluation metrics, comparing three prominent DGMs: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models (DMs). Furthermore, we propose a comprehensive framework to assess utility, robustness, and privacy in synthetic data generated by DGMs. Our findings demonstrate varying strengths among DGMs, with each model exhibiting unique advantages based on the application context. This study broadens the scope of the Generative Learning Trilemma, aligning it with real-world demands and providing actionable guidance for selecting DGMs tailored to specific applications.\n        △ Less",
        "authors":[
            "Marco Salmè",
            "Lorenzo Tronchin",
            "Rosa Sicilia",
            "Paolo Soda",
            "Valerio Guarrasi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Inferring the Hubble Constant Using Simulated Strongly Lensed Supernovae and Neural Network Ensembles",
        "abstract":"Strongly lensed supernovae are a promising new probe to obtain independent measurements of the Hubble constant (${H_0}$). In this work, we employ simulated gravitationally lensed Type Ia supernovae (glSNe Ia) to train our machine learning (ML) pipeline to constrain $H_0$. We simulate image time-series of glSNIa, as observed with the upcoming Nancy Grace Roman Space Telescope, that we employ for training an ensemble of five convolutional neural networks (CNNs). The outputs of this ensemble network are combined with a simulation-based inference (SBI) framework to quantify the uncertainties on the network predictions and infer full posteriors for the $H_0$ estimates. We illustrate that the combination of multiple glSN systems enhances constraint precision, providing a $4.4\\%$ estimate of $H_0$ based on 100 simulated systems, which is in agreement with the ground truth. This research highlights the potential of leveraging the capabilities of ML with glSNe systems to obtain a pipeline capable of fast and automated $H_0$ measurements.\n        △ Less",
        "authors":[
            "Gonçalo Gonçalves",
            "Nikki Arendse",
            "Doogesh Kodi Ramanah",
            "Radosław Wojtak"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LEMUR Neural Network Dataset: Towards Seamless AutoML",
        "abstract":"Neural networks are fundamental in artificial intelligence, driving progress in computer vision and natural language processing. High-quality datasets are crucial for their development, and there is growing interest in datasets composed of neural networks themselves to support benchmarking, automated machine learning (AutoML), and model analysis. We introduce LEMUR, an open source dataset of neural network models with well-structured code for diverse architectures across tasks such as object detection, image classification, segmentation, and natural language processing. LEMUR is primarily designed to enable fine-tuning of large language models (LLMs) for AutoML tasks, providing a rich source of structured model representations and associated performance data. Leveraging Python and PyTorch, LEMUR enables seamless extension to new datasets and models while maintaining consistency. It integrates an Optuna-powered framework for evaluation, hyperparameter optimization, statistical analysis, and graphical insights. LEMUR provides an extension that enables models to run efficiently on edge devices, facilitating deployment in resource-constrained environments. Providing tools for model evaluation, preprocessing, and database management, LEMUR supports researchers and practitioners in developing, testing, and analyzing neural networks. Additionally, it offers an API that delivers comprehensive information about neural network models and their complete performance statistics with a single request, which can be used in experiments with code-generating large language models. The LEMUR will be released as an open source project under the MIT license upon acceptance of the paper.\n        △ Less",
        "authors":[
            "Arash Torabi Goodarzi",
            "Roman Kochnev",
            "Waleed Khalid",
            "Furui Qin",
            "Tolgay Atinc Uzun",
            "Yashkumar Sanjaybhai Dhameliya",
            "Yash Kanubhai Kathiriya",
            "Zofia Antonina Bentyn",
            "Dmitry Ignatov",
            "Radu Timofte"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers",
        "abstract":"Empirical Risk Minimization (ERM) models often rely on spurious correlations between features and labels during the learning process, leading to shortcut learning behavior that undermines robustness generalization performance. Current research mainly targets identifying or mitigating a single shortcut; however, in real-world scenarios, cues within the data are diverse and unknown. In empirical studies, we reveal that the models rely to varying extents on different shortcuts. Compared to weak shortcuts, models depend more heavily on strong shortcuts, resulting in their poor generalization ability. To address these challenges, we propose MiMu, a novel method integrated with Transformer-based ERMs designed to Mitigate Multiple shortcut learning behavior, which incorporates self-calibration strategy and self-improvement strategy. In the source model, we preliminarily propose the self-calibration strategy to prevent the model from relying on shortcuts and make overconfident predictions. Then, we further design self-improvement strategy in target model to reduce the reliance on multiple shortcuts. The random mask strategy involves randomly masking partial attention positions to diversify the focus of target model other than concentrating on a fixed region. Meanwhile, the adaptive attention alignment module facilitates the alignment of attention weights to the calibrated source model, without the need for post-hoc attention maps or supervision. Finally, extensive experiments conducted on Natural Language Processing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu in improving robustness generalization abilities.\n        △ Less",
        "authors":[
            "Lili Zhao",
            "Qi Liu",
            "Wei Chen",
            "Liyi Chen",
            "Ruijun Sun",
            "Min Hou",
            "Yang Wang",
            "Shijin Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LCDC: Bridging Science and Machine Learning for Light Curve Analysis",
        "abstract":"The characterization and analysis of light curves are vital for understanding the physical and rotational properties of artificial space objects such as satellites, rocket stages, and space debris. This paper introduces the Light Curve Dataset Creator (LCDC), a Python-based toolkit designed to facilitate the preprocessing, analysis, and machine learning applications of light curve data. LCDC enables seamless integration with publicly available datasets, such as the newly introduced Mini Mega Tortora (MMT) database. Moreover, it offers data filtering, transformation, as well as feature extraction tooling. To demonstrate the toolkit's capabilities, we created the first standardized dataset for rocket body classification, RoBo6, which was used to train and evaluate several benchmark machine learning models, addressing the lack of reproducibility and comparability in recent studies. Furthermore, the toolkit enables advanced scientific analyses, such as surface characterization of the Atlas 2AS Centaur and the rotational dynamics of the Delta 4 rocket body, by streamlining data preprocessing, feature extraction, and visualization. These use cases highlight LCDC's potential to advance space debris characterization and promote sustainable space exploration. Additionally, they highlight the toolkit's ability to enable AI-focused research within the space debris community.\n        △ Less",
        "authors":[
            "Daniel Kyselica",
            "Tomáš Hrobár",
            "Jiří Šilha",
            "Roman Ďurikovič",
            "Marek Šuppa"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Integrating Textual Embeddings from Contrastive Learning with Generative Recommender for Enhanced Personalization",
        "abstract":"Recent advances in recommender systems have highlighted the complementary strengths of generative modeling and pretrained language models. We propose a hybrid framework that augments the Hierarchical Sequential Transduction Unit (HSTU) generative recommender with BLaIR -- a contrastive text embedding model. This integration enriches item representations with semantic signals from textual metadata while preserving HSTU's powerful sequence modeling capabilities.\n  We evaluate our method on two domains from the Amazon Reviews 2023 dataset, comparing it against the original HSTU and a variant that incorporates embeddings from OpenAI's state-of-the-art text-embedding-3-large model. While the OpenAI embedding model is likely trained on a substantially larger corpus with significantly more parameters, our lightweight BLaIR-enhanced approach -- pretrained on domain-specific data -- consistently achieves better performance, highlighting the effectiveness of contrastive text embeddings in compute-efficient settings.\n        △ Less",
        "authors":[
            "Yijun Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Efficient Quantum Classifier Based on Hamiltonian Representations",
        "abstract":"Quantum machine learning (QML) is a discipline that seeks to transfer the advantages of quantum computing to data-driven tasks. However, many studies rely on toy datasets or heavy feature reduction, raising concerns about their scalability. Progress is further hindered by hardware limitations and the significant costs of encoding dense vector representations on quantum devices. To address these challenges, we propose an efficient approach called Hamiltonian classifier that circumvents the costs associated with data encoding by mapping inputs to a finite set of Pauli strings and computing predictions as their expectation values. In addition, we introduce two classifier variants with different scaling in terms of parameters and sample complexity. We evaluate our approach on text and image classification tasks, against well-established classical and quantum models. The Hamiltonian classifier delivers performance comparable to or better than these methods. Notably, our method achieves logarithmic complexity in both qubits and quantum gates, making it well-suited for large-scale, real-world applications. We make our implementation available on GitHub.\n        △ Less",
        "authors":[
            "Federico Tiblias",
            "Anna Schroeder",
            "Yue Zhang",
            "Mariami Gachechiladze",
            "Iryna Gurevych"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AB-Cache: Training-Free Acceleration of Diffusion Models via Adams-Bashforth Cached Feature Reuse",
        "abstract":"Diffusion models have demonstrated remarkable success in generative tasks, yet their iterative denoising process results in slow inference, limiting their practicality. While existing acceleration methods exploit the well-known U-shaped similarity pattern between adjacent steps through caching mechanisms, they lack theoretical foundation and rely on simplistic computation reuse, often leading to performance degradation. In this work, we provide a theoretical understanding by analyzing the denoising process through the second-order Adams-Bashforth method, revealing a linear relationship between the outputs of consecutive steps. This analysis explains why the outputs of adjacent steps exhibit a U-shaped pattern. Furthermore, extending Adams-Bashforth method to higher order, we propose a novel caching-based acceleration approach for diffusion models, instead of directly reusing cached results, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step size. Extensive validation across diverse image and video diffusion models (including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates our method's effectiveness in achieving nearly $3\\times$ speedup while maintaining original performance levels, offering a practical real-time solution without compromising generation quality.\n        △ Less",
        "authors":[
            "Zichao Yu",
            "Zhen Zou",
            "Guojiang Shao",
            "Chengwei Zhang",
            "Shengze Xu",
            "Jie Huang",
            "Feng Zhao",
            "Xiaodong Cun",
            "Wenyi Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP",
        "abstract":"Federated learning (FL) enables collaborative model training across organizations without sharing raw data, addressing crucial privacy concerns in healthcare natural language processing (NLP). However, training large language models (LLMs) in federated settings faces significant challenges, including communication overhead and data heterogeneity. We propose Layer-Skipping Federated Learning, where only selected layers of a pre-trained LLM are fine-tuned across clients while others remain frozen. Applied to LLaMA 3.2-1B, our approach reduces communication costs by approximately 70% while maintaining performance within 2% of centralized training. We evaluate our method on clinical NER and classification tasks using i2b2 and MIMIC-III datasets. Our experiments demonstrate that Layer-Skipping FL outperforms competitive baselines, handles non-IID clinical data distributions effectively, and shows robustness when combined with differential privacy. This approach represents a practical solution for privacy-preserving collaborative learning in healthcare NLP.\n        △ Less",
        "authors":[
            "Lihong Zhang",
            "Yue Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Integrating Emotion Distribution Networks and Textual Message Analysis for X User Emotional State Classification",
        "abstract":"As the popularity and reach of social networks continue to surge, a vast reservoir of opinions and sentiments across various subjects inundates these platforms. Among these, X social network (formerly Twitter) stands as a juggernaut, boasting approximately 420 million active users. Extracting users' emotional and mental states from their expressed opinions on social media has become a common pursuit. While past methodologies predominantly focused on the textual content of messages to analyze user sentiment, the interactive nature of these platforms suggests a deeper complexity. This study employs hybrid methodologies, integrating textual analysis, profile examination, follower analysis, and emotion dissemination patterns. Initially, user interactions are leveraged to refine emotion classification within messages, encompassing exchanges where users respond to each other. Introducing the concept of a communication tree, a model is extracted to map these interactions. Subsequently, users' bios and interests from this tree are juxtaposed with message text to enrich analysis. Finally, influential figures are identified among users' followers in the communication tree, categorized into different topics to gauge interests. The study highlights that traditional sentiment analysis methodologies, focusing solely on textual content, are inadequate in discerning sentiment towards significant events, notably the presidential election. Comparative analysis with conventional methods reveals a substantial improvement in accuracy with the incorporation of emotion distribution patterns and user profiles. The proposed approach yields a 12% increase in accuracy with emotion distribution patterns and a 15% increase when considering user profiles, underscoring its efficacy in capturing nuanced sentiment dynamics.\n        △ Less",
        "authors":[
            "Pardis Moradbeiki",
            "Mohammad Ali Zare Chahooki"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Toward Super Agent System with Hybrid AI Routers",
        "abstract":"AI Agents powered by Large Language Models are transforming the world through enormous applications. A super agent has the potential to fulfill diverse user needs, such as summarization, coding, and research, by accurately understanding user intent and leveraging the appropriate tools to solve tasks. However, to make such an agent viable for real-world deployment and accessible at scale, significant optimizations are required to ensure high efficiency and low cost. This paper presents a design of the Super Agent System. Upon receiving a user prompt, the system first detects the intent of the user, then routes the request to specialized task agents with the necessary tools or automatically generates agentic workflows. In practice, most applications directly serve as AI assistants on edge devices such as phones and robots. As different language models vary in capability and cloud-based models often entail high computational costs, latency, and privacy concerns, we then explore the hybrid mode where the router dynamically selects between local and cloud models based on task complexity. Finally, we introduce the blueprint of an on-device super agent enhanced with cloud. With advances in multi-modality models and edge hardware, we envision that most computations can be handled locally, with cloud collaboration only as needed. Such architecture paves the way for super agents to be seamlessly integrated into everyday life in the near future.\n        △ Less",
        "authors":[
            "Yuhang Yao",
            "Haixin Wang",
            "Yibo Chen",
            "Jiawen Wang",
            "Min Chang Jordan Ren",
            "Bosheng Ding",
            "Salman Avestimehr",
            "Chaoyang He"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness",
        "abstract":"Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.\n        △ Less",
        "authors":[
            "Yijun Liang",
            "Ming Li",
            "Chenrui Fan",
            "Ziyue Li",
            "Dang Nguyen",
            "Kwesi Cobbina",
            "Shweta Bhardwaj",
            "Jiuhai Chen",
            "Fuxiao Liu",
            "Tianyi Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems",
        "abstract":"Generative retrieval methods utilize generative sequential modeling techniques, such as transformers, to generate candidate items for recommender systems. These methods have demonstrated promising results in academic benchmarks, surpassing traditional retrieval models like two-tower architectures. However, current generative retrieval methods lack the scalability required for industrial recommender systems, and they are insufficiently flexible to satisfy the multiple metric requirements of modern systems.\n  This paper introduces PinRec, a novel generative retrieval model developed for applications at Pinterest. PinRec utilizes outcome-conditioned generation, enabling modelers to specify how to balance various outcome metrics, such as the number of saves and clicks, to effectively align with business goals and user exploration. Additionally, PinRec incorporates multi-token generation to enhance output diversity while optimizing generation. Our experiments demonstrate that PinRec can successfully balance performance, diversity, and efficiency, delivering a significant positive impact to users using generative models. This paper marks a significant milestone in generative retrieval, as it presents, to our knowledge, the first rigorous study on implementing generative retrieval at the scale of Pinterest.\n        △ Less",
        "authors":[
            "Anirudhan Badrinath",
            "Prabhat Agarwal",
            "Laksh Bhasin",
            "Jaewon Yang",
            "Jiajing Xu",
            "Charles Rosenberg"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Human-Oriented Image Retrieval System (HORSE): A Neuro-Symbolic Approach to Optimizing Retrieval of Previewed Images",
        "abstract":"Image retrieval remains a challenging task due to the complex interaction between human visual perception, memory, and computational processes. Current image search engines often struggle to efficiently retrieve images based on natural language descriptions, as they rely on time-consuming preprocessing, tagging, and machine learning pipelines. This paper introduces the Human-Oriented Retrieval Search Engine for Images (HORSE), a novel approach that leverages neuro-symbolic indexing to improve image retrieval by focusing on human-oriented indexing. By integrating cognitive science insights with advanced computational techniques, HORSE enhances the retrieval process, making it more aligned with how humans perceive, store, and recall visual information. The neuro-symbolic framework combines the strengths of neural networks and symbolic reasoning, mitigating their individual limitations. The proposed system optimizes image retrieval, offering a more intuitive and efficient solution for users. We discuss the design and implementation of HORSE, highlight its potential applications in fields such as design error detection and knowledge management, and suggest future directions for research to further refine the system's metrics and capabilities.\n        △ Less",
        "authors":[
            "Abraham Itzhak Weinberg"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging Auto-Distillation and Generative Self-Supervised Learning in Residual Graph Transformers for Enhanced Recommender Systems",
        "abstract":"This paper introduces a cutting-edge method for enhancing recommender systems through the integration of generative self-supervised learning (SSL) with a Residual Graph Transformer. Our approach emphasizes the importance of superior data enhancement through the use of pertinent pretext tasks, automated through rationale-aware SSL to distill clear ways of how users and items interact. The Residual Graph Transformer incorporates a topology-aware transformer for global context and employs residual connections to improve graph representation learning. Additionally, an auto-distillation process refines self-supervised signals to uncover consistent collaborative rationales. Experimental evaluations on multiple datasets demonstrate that our approach consistently outperforms baseline methods.\n        △ Less",
        "authors":[
            "Eya Mhedhbi",
            "Youssef Mourchid",
            "Alice Othmani"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ArxivBench: Can LLMs Assist Researchers in Conducting Research?",
        "abstract":"Large language models (LLMs) have demonstrated remarkable effectiveness in completing various tasks such as reasoning, translation, and question answering. However the issue of factual incorrect content in LLM-generated responses remains a persistent challenge. In this study, we evaluate both proprietary and open-source LLMs on their ability to respond with relevant research papers and accurate links to articles hosted on the arXiv platform, based on high level prompts. To facilitate this evaluation, we introduce arXivBench, a benchmark specifically designed to assess LLM performance across eight major subject categories on arXiv and five subfields within computer science, one of the most popular categories among them. Our findings reveal a concerning accuracy of LLM-generated responses depending on the subject, with some subjects experiencing significantly lower accuracy than others. Notably, Claude-3.5-Sonnet exhibits a substantial advantage in generating both relevant and accurate responses. And interestingly, most LLMs achieve a much higher accuracy in the Artificial Intelligence sub-field than other sub-fields. This benchmark provides a standardized tool for evaluating the reliability of LLM-generated scientific responses, promoting more dependable use of LLMs in academic and research environments. Our code is open-sourced at https:\/\/github.com\/arxivBenchLLM\/arXivBench and our dataset is available on huggingface at https:\/\/huggingface.co\/datasets\/arXivBenchLLM\/arXivBench.\n        △ Less",
        "authors":[
            "Ning Li",
            "Jingran Zhang",
            "Justin Cui"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA",
        "abstract":"We explore the potential of integrating learnable and interpretable modules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based representations--within a pre-trained GPT-2 model to enhance multi-task learning accuracy. Motivated by the recent surge in using KAN and graph attention (GAT) architectures in chain-of-thought (CoT) models and debates over their benefits compared to simpler architectures like MLPs, we begin by enhancing a standard self-attention transformer using Low-Rank Adaptation (LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This approach yields significant improvements. To further boost interpretability and richer representations, we develop two variants that attempt to improve the standard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However, systematic evaluations reveal that neither variant outperforms the optimized LoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set, 99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On sonnet generation, we get a CHRF score of 42.097. These findings highlight that efficient parameter adaptation via LoRA remains the most effective strategy for our tasks: sentiment analysis, paraphrase detection, and sonnet generation.\n        △ Less",
        "authors":[
            "Gabriel Bo",
            "Marc Bernardino",
            "Justin Gu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning",
        "abstract":"In this paper, we present Roamify, an Artificial Intelligence powered travel assistant that aims to ease the process of travel planning. We have tested and used multiple Large Language Models like Llama and T5 to generate personalised itineraries per user preferences. Results from user surveys highlight the preference for AI powered mediums over existing methods to help in travel planning across all user age groups. These results firmly validate the potential need of such a travel assistant. We highlight the two primary design considerations for travel assistance: D1) incorporating a web-scraping method to gather up-to-date news articles about destinations from various blog sources, which significantly improves our itinerary suggestions, and D2) utilising user preferences to create customised travel experiences along with a recommendation system which changes the itinerary according to the user needs. Our findings suggest that Roamify has the potential to improve and simplify how users across multiple age groups plan their travel experiences.\n        △ Less",
        "authors":[
            "Vikranth Udandarao",
            "Noel Abraham Tiju",
            "Muthuraj Vairamuthu",
            "Harsh Mistry",
            "Dhruv Kumar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FLOSS: Free Lunch in Open-vocabulary Semantic Segmentation",
        "abstract":"Recent Open-Vocabulary Semantic Segmentation (OVSS) models extend the CLIP model to segmentation while maintaining the use of multiple templates (e.g., a photo of <class>, a sketch of a <class>, etc.) for constructing class-wise averaged text embeddings, acting as a classifier. In this paper, we challenge this status quo and investigate the impact of templates for OVSS. Empirically, we observe that for each class, there exist single-template classifiers significantly outperforming the conventional averaged classifier. We refer to them as class-experts. Given access to unlabeled images and without any training involved, we estimate these experts by leveraging the class-wise prediction entropy of single-template classifiers, selecting as class-wise experts those which yield the lowest entropy. All experts, each specializing in a specific class, collaborate in a newly proposed fusion method to generate more accurate OVSS predictions. Our plug-and-play method, coined FLOSS, is orthogonal and complementary to existing OVSS methods, offering a ''free lunch'' to systematically improve OVSS without labels and additional training. Extensive experiments demonstrate that FLOSS consistently boosts state-of-the-art methods on various OVSS benchmarks. Moreover, the selected expert templates can generalize well from one dataset to others sharing the same semantic categories, yet exhibiting distribution shifts. Additionally, we obtain satisfactory improvements under a low-data regime, where only a few unlabeled images are available. Our code is available at https:\/\/github.com\/yasserben\/FLOSS .\n        △ Less",
        "authors":[
            "Yasser Benigmim",
            "Mohammad Fahes",
            "Tuan-Hung Vu",
            "Andrei Bursuc",
            "Raoul de Charette"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"REPA-E: Unlocking VAE for End-to-End Tuning with Latent Diffusion Transformers",
        "abstract":"In this paper we tackle a fundamental question: \"Can we train latent diffusion models together with the variational auto-encoder (VAE) tokenizer in an end-to-end manner?\" Traditional deep-learning wisdom dictates that end-to-end training is often preferable when possible. However, for latent diffusion transformers, it is observed that end-to-end training both VAE and diffusion-model using standard diffusion-loss is ineffective, even causing a degradation in final performance. We show that while diffusion loss is ineffective, end-to-end training can be unlocked through the representation-alignment (REPA) loss -- allowing both VAE and diffusion model to be jointly tuned during the training process. Despite its simplicity, the proposed training recipe (REPA-E) shows remarkable performance; speeding up diffusion model training by over 17x and 45x over REPA and vanilla training recipes, respectively. Interestingly, we observe that end-to-end tuning with REPA-E also improves the VAE itself; leading to improved latent space structure and downstream generation performance. In terms of final performance, our approach sets a new state-of-the-art; achieving FID of 1.26 and 1.83 with and without classifier-free guidance on ImageNet 256 x 256. Code is available at https:\/\/end2end-diffusion.github.io.\n        △ Less",
        "authors":[
            "Xingjian Leng",
            "Jaskirat Singh",
            "Yunzhong Hou",
            "Zhenchang Xing",
            "Saining Xie",
            "Liang Zheng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Weight Ensembling Improves Reasoning in Language Models",
        "abstract":"We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades-off between bias and variance.\n        △ Less",
        "authors":[
            "Xingyu Dang",
            "Christina Baek",
            "Kaiyue Wen",
            "Zico Kolter",
            "Aditi Raghunathan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Privacy-Preserving Distributed Link Predictions Among Peers in Online Classrooms Using Federated Learning",
        "abstract":"Social interactions among classroom peers, represented as social learning networks (SLNs), play a crucial role in enhancing learning outcomes. While SLN analysis has recently garnered attention, most existing approaches rely on centralized training, where data is aggregated and processed on a local\/cloud server with direct access to raw data. However, in real-world educational settings, such direct access across multiple classrooms is often restricted due to privacy concerns. Furthermore, training models on isolated classroom data prevents the identification of common interaction patterns that exist across multiple classrooms, thereby limiting model performance. To address these challenges, we propose one of the first frameworks that integrates Federated Learning (FL), a distributed and collaborative machine learning (ML) paradigm, with SLNs derived from students' interactions in multiple classrooms' online forums to predict future link formations (i.e., interactions) among students. By leveraging FL, our approach enables collaborative model training across multiple classrooms while preserving data privacy, as it eliminates the need for raw data centralization. Recognizing that each classroom may exhibit unique student interaction dynamics, we further employ model personalization techniques to adapt the FL model to individual classroom characteristics. Our results demonstrate the effectiveness of our approach in capturing both shared and classroom-specific representations of student interactions in SLNs. Additionally, we utilize explainable AI (XAI) techniques to interpret model predictions, identifying key factors that influence link formation across different classrooms. These insights unveil the drivers of social learning interactions within a privacy-preserving, collaborative, and distributed ML framework -- an aspect that has not been explored before.\n        △ Less",
        "authors":[
            "Anurata Prabha Hridi",
            "Muntasir Hoq",
            "Zhikai Gao",
            "Collin Lynch",
            "Rajeev Sahay",
            "Seyyedali Hosseinalipour",
            "Bita Akram"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Anchors no more: Using peculiar velocities to constrain $H_0$ and the primordial Universe without calibrators",
        "abstract":"We develop a novel approach to constrain the Hubble parameter $H_0$ and the primordial power spectrum amplitude $A_\\mathrm{s}$ using supernovae type Ia (SNIa) data. By considering SNIa as tracers of the peculiar velocity field, we can model their distance and their covariance as a function of cosmological parameters without the need of calibrators like Cepheids; this yields a new independent probe of the large-scale structure based on SNIa data without distance anchors. Crucially, we implement a differentiable pipeline in JAX, including efficient emulators and affine sampling, reducing inference time from years to hours on a single GPU. We first validate our method on mock datasets, demonstrating that we can constrain $H_0$ and $\\log 10^{10}A_\\mathrm{s}$ within $\\sim10\\%$ using $\\sim10^3$ SNIa. We then test our pipeline with SNIa from an $N$-body simulation, obtaining $7\\%$-level unbiased constraints on $H_0$ with a moderate noise level. We finally apply our method to Pantheon+ data, constraining $H_0$ at the $10\\%$ level without Cepheids when fixing $A_\\mathrm{s}$ to its $\\it{Planck}$ value. On the other hand, we obtain $15\\%$-level constraints on $\\log 10^{10}A_\\mathrm{s}$ in agreement with $\\it{Planck}$ when including Cepheids in the analysis. In light of upcoming observations of low redshift SNIa from the Zwicky Transient Facility and the Vera Rubin Legacy Survey of Space and Time, surveys for which our method will develop its full potential, we make our code publicly available.\n        △ Less",
        "authors":[
            "Davide Piras",
            "Francesco Sorrenti",
            "Ruth Durrer",
            "Martin Kunz"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Integrating Vision and Location with Transformers: A Multimodal Deep Learning Framework for Medical Wound Analysis",
        "abstract":"Effective recognition of acute and difficult-to-heal wounds is a necessary step in wound diagnosis. An efficient classification model can help wound specialists classify wound types with less financial and time costs and also help in deciding on the optimal treatment method. Traditional machine learning models suffer from feature selection and are usually cumbersome models for accurate recognition. Recently, deep learning (DL) has emerged as a powerful tool in wound diagnosis. Although DL seems promising for wound type recognition, there is still a large scope for improving the efficiency and accuracy of the model. In this study, a DL-based multimodal classifier was developed using wound images and their corresponding locations to classify them into multiple classes, including diabetic, pressure, surgical, and venous ulcers. A body map was also created to provide location data, which can help wound specialists label wound locations more effectively. The model uses a Vision Transformer to extract hierarchical features from input images, a Discrete Wavelet Transform (DWT) layer to capture low and high frequency components, and a Transformer to extract spatial features. The number of neurons and weight vector optimization were performed using three swarm-based optimization techniques (Monster Gorilla Toner (MGTO), Improved Gray Wolf Optimization (IGWO), and Fox Optimization Algorithm). The evaluation results show that weight vector optimization using optimization algorithms can increase diagnostic accuracy and make it a very effective approach for wound detection. In the classification using the original body map, the proposed model was able to achieve an accuracy of 0.8123 using image data and an accuracy of 0.8007 using a combination of image data and wound location. Also, the accuracy of the model in combination with the optimization models varied from 0.7801 to 0.8342.\n        △ Less",
        "authors":[
            "Ramin Mousa",
            "Hadis Taherinia",
            "Khabiba Abdiyeva",
            "Amir Ali Bengari",
            "Mohammadmahdi Vahediahmar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
        "abstract":"Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning.\n        △ Less",
        "authors":[
            "Junxiong Wang",
            "Wen-Ding Li",
            "Daniele Paliotta",
            "Daniel Ritter",
            "Alexander M. Rush",
            "Tri Dao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RealWebAssist: A Benchmark for Long-Horizon Web Assistance with Real-World Users",
        "abstract":"To achieve successful assistance with long-horizon web-based tasks, AI agents must be able to sequentially follow real-world user instructions over a long period. Unlike existing web-based agent benchmarks, sequential instruction following in the real world poses significant challenges beyond performing a single, clearly defined task. For instance, real-world human instructions can be ambiguous, require different levels of AI assistance, and may evolve over time, reflecting changes in the user's mental state. To address this gap, we introduce RealWebAssist, a novel benchmark designed to evaluate sequential instruction-following in realistic scenarios involving long-horizon interactions with the web, visual GUI grounding, and understanding ambiguous real-world user instructions. RealWebAssist includes a dataset of sequential instructions collected from real-world human users. Each user instructs a web-based assistant to perform a series of tasks on multiple websites. A successful agent must reason about the true intent behind each instruction, keep track of the mental state of the user, understand user-specific routines, and ground the intended tasks to actions on the correct GUI elements. Our experimental results show that state-of-the-art models struggle to understand and ground user instructions, posing critical challenges in following real-world user instructions for long-horizon web assistance.\n        △ Less",
        "authors":[
            "Suyu Ye",
            "Haojun Shi",
            "Darren Shih",
            "Hyokun Yun",
            "Tanya Roosta",
            "Tianmin Shu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multimodal Long Video Modeling Based on Temporal Dynamic Context",
        "abstract":"Recent advances in Large Language Models (LLMs) have led to significant breakthroughs in video understanding. However, existing models still struggle with long video processing due to the context length constraint of LLMs and the vast amount of information within the video. Although some recent methods are designed for long video understanding, they often lose crucial information during token compression and struggle with additional modality like audio. In this work, we propose a dynamic long video encoding method utilizing the temporal relationship between frames, named Temporal Dynamic Context (TDC). Firstly, we segment the video into semantically consistent scenes based on inter-frame similarities, then encode each frame into tokens using visual-audio encoders. Secondly, we propose a novel temporal context compressor to reduce the number of tokens within each segment. Specifically, we employ a query-based Transformer to aggregate video, audio, and instruction text tokens into a limited set of temporal context tokens. Finally, we feed the static frame tokens and the temporal context tokens into the LLM for video understanding. Furthermore, to handle extremely long videos, we propose a training-free chain-of-thought strategy that progressively extracts answers from multiple video segments. These intermediate answers serve as part of the reasoning process and contribute to the final answer. We conduct extensive experiments on general video understanding and audio-video understanding benchmarks, where our method demonstrates strong performance. The code and models are available at https:\/\/github.com\/Hoar012\/TDC-Video.\n        △ Less",
        "authors":[
            "Haoran Hao",
            "Jiaming Han",
            "Yiyuan Zhang",
            "Xiangyu Yue"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning with Positive and Imperfect Unlabeled Data",
        "abstract":"We study the problem of learning binary classifiers from positive and unlabeled data when the unlabeled data distribution is shifted, which we call Positive and Imperfect Unlabeled (PIU) Learning. In the absence of covariate shifts, i.e., with perfect unlabeled data, Denis (1998) reduced this problem to learning under Massart noise; however, that reduction fails under even slight shifts.\n  Our main results on PIU learning are the characterizations of the sample complexity of PIU learning and a computationally and sample-efficient algorithm achieving a misclassification error $\\varepsilon$. We further show that our results lead to new algorithms for several related problems.\n  1. Learning from smooth distributions: We give algorithms that learn interesting concept classes from only positive samples under smooth feature distributions, bypassing known existing impossibility results and contributing to recent advances in smoothened learning (Haghtalab et al, J.ACM'24) (Chandrasekaran et al., COLT'24).\n  2. Learning with a list of unlabeled distributions: We design new algorithms that apply to a broad class of concept classes under the assumption that we are given a list of unlabeled distributions, one of which--unknown to the learner--is $O(1)$-close to the true feature distribution.\n  3. Estimation in the presence of unknown truncation: We give the first polynomial sample and time algorithm for estimating the parameters of an exponential family distribution from samples truncated to an unknown set approximable by polynomials in $L_1$-norm. This improves the algorithm by Lee et al. (FOCS'24) that requires approximation in $L_2$-norm.\n  4. Detecting truncation: We present new algorithms for detecting whether given samples have been truncated (or not) for a broad class of non-product distributions, including non-product distributions, improving the algorithm by De et al. (STOC'24).\n        △ Less",
        "authors":[
            "Jane H. Lee",
            "Anay Mehrotra",
            "Manolis Zampetakis"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Foundation models for electronic health records: representation dynamics and transferability",
        "abstract":"Foundation models (FMs) trained on electronic health records (EHRs) have shown strong performance on a range of clinical prediction tasks. However, adapting these models to local health systems remains challenging due to limited data availability and resource constraints. In this study, we investigated what these models learn and evaluated the transferability of an FM trained on MIMIC-IV to an institutional EHR dataset at the University of Chicago Medical Center. We assessed their ability to identify outlier patients and examined representation-space patient trajectories in relation to future clinical outcomes. We also evaluated the performance of supervised fine-tuned classifiers on both source and target datasets. Our findings offer insights into the adaptability of FMs across different healthcare systems, highlight considerations for their effective implementation, and provide an empirical analysis of the underlying factors that contribute to their predictive performance.\n        △ Less",
        "authors":[
            "Michael C. Burkhart",
            "Bashar Ramadan",
            "Zewei Liao",
            "Kaveri Chhikara",
            "Juan C. Rojas",
            "William F. Parker",
            "Brett K. Beaulieu-Jones"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models",
        "abstract":"Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.\n        △ Less",
        "authors":[
            "Parshin Shojaee",
            "Ngoc-Hieu Nguyen",
            "Kazem Meidani",
            "Amir Barati Farimani",
            "Khoa D Doan",
            "Chandan K Reddy"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-Driven Code Refactoring: Using Graph Neural Networks to Enhance Software Maintainability",
        "abstract":"This study explores Graph Neural Networks (GNNs) as a transformative tool for code refactoring, using abstract syntax trees (ASTs) to boost software maintainability. It analyzes a dataset of 2 million snippets from CodeSearchNet and a custom 75000-file GitHub Python corpus, comparing GNNs against rule-based SonarQube and decision trees. Metrics include cyclomatic complexity (target below 10), coupling (target below 5), and refactoring precision. GNNs achieve 92% accuracy, reducing complexity by 35% and coupling by 33%, outperforming SonarQube (78%, 16%) and decision trees (85%, 25%). Preprocessing fixed 60% of syntax errors. Bar graphs, tables, and AST visuals clarify results. This offers a scalable AI-driven path to cleaner codebases, which is crucial for software engineering.\n        △ Less",
        "authors":[
            "Gopichand Bandarupalli"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks",
        "abstract":"Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited bandwidth. Satellite federated learning (FL) offers a solution by enabling model fine-tuning directly on-board satellites and aggregating model updates without data downloading. Nevertheless, for large foundation models, the computational capacity of satellites is insufficient to support effective on-board fine-tuning in traditional satellite FL frameworks. To address these challenges, we propose a satellite-ground collaborative federated fine-tuning framework. The key of the framework lies in how to reasonably decompose and allocate model components to alleviate insufficient on-board computation capabilities. During fine-tuning, satellites exchange intermediate results with ground stations or other satellites for forward propagation and back propagation, which brings communication challenges due to the special communication topology of space transmission networks, such as intermittent satellite-ground communication, short duration of satellite-ground communication windows, and unstable inter-orbit inter-satellite links (ISLs). To reduce transmission delays, we further introduce tailored communication strategies that integrate both communication and computing resources. Specifically, we propose a parallel intra-orbit communication strategy, a topology-aware satellite-ground communication strategy, and a latency-minimalization inter-orbit communication strategy to reduce space communication costs. Simulation results demonstrate significant reductions in training time with improvements of approximately 33%.\n        △ Less",
        "authors":[
            "Yan zhu",
            "Jingyang zhu",
            "Ting Wang",
            "Yuanming Shi",
            "Chunxiao Jiang",
            "Khaled Ben Letaief"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can LLMs Assist Expert Elicitation for Probabilistic Causal Modeling?",
        "abstract":"Objective: This study investigates the potential of Large Language Models (LLMs) as an alternative to human expert elicitation for extracting structured causal knowledge and facilitating causal modeling in biometric and healthcare applications.\n  Material and Methods: LLM-generated causal structures, specifically Bayesian networks (BNs), were benchmarked against traditional statistical methods (e.g., Bayesian Information Criterion) using healthcare datasets. Validation techniques included structural equation modeling (SEM) to verifying relationships, and measures such as entropy, predictive accuracy, and robustness to compare network structures.\n  Results and Discussion: LLM-generated BNs demonstrated lower entropy than expert-elicited and statistically generated BNs, suggesting higher confidence and precision in predictions. However, limitations such as contextual constraints, hallucinated dependencies, and potential biases inherited from training data require further investigation.\n  Conclusion: LLMs represent a novel frontier in expert elicitation for probabilistic causal modeling, promising to improve transparency and reduce uncertainty in the decision-making using such models.\n        △ Less",
        "authors":[
            "Olha Shaposhnyk",
            "Daria Zahorska",
            "Svetlana Yanushkevich"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Inferring genotype-phenotype maps using attention models",
        "abstract":"Predicting phenotype from genotype is a central challenge in genetics. Traditional approaches in quantitative genetics typically analyze this problem using methods based on linear regression. These methods generally assume that the genetic architecture of complex traits can be parameterized in terms of an additive model, where the effects of loci are independent, plus (in some cases) pairwise epistatic interactions between loci. However, these models struggle to analyze more complex patterns of epistasis or subtle gene-environment interactions. Recent advances in machine learning, particularly attention-based models, offer a promising alternative. Initially developed for natural language processing, attention-based models excel at capturing context-dependent interactions and have shown exceptional performance in predicting protein structure and function. Here, we apply attention-based models to quantitative genetics. We analyze the performance of this attention-based approach in predicting phenotype from genotype using simulated data across a range of models with increasing epistatic complexity, and using experimental data from a recent quantitative trait locus mapping study in budding yeast. We find that our model demonstrates superior out-of-sample predictions in epistatic regimes compared to standard methods. We also explore a more general multi-environment attention-based model to jointly analyze genotype-phenotype maps across multiple environments and show that such architectures can be used for \"transfer learning\" - predicting phenotypes in novel environments with limited training data.\n        △ Less",
        "authors":[
            "Krishna Rijal",
            "Caroline M. Holmes",
            "Samantha Petti",
            "Gautam Reddy",
            "Michael M. Desai",
            "Pankaj Mehta"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PG-DPIR: An efficient plug-and-play method for high-count Poisson-Gaussian inverse problems",
        "abstract":"Poisson-Gaussian noise describes the noise of various imaging systems thus the need of efficient algorithms for Poisson-Gaussian image restoration. Deep learning methods offer state-of-the-art performance but often require sensor-specific training when used in a supervised setting. A promising alternative is given by plug-and-play (PnP) methods, which consist in learning only a regularization through a denoiser, allowing to restore images from several sources with the same network. This paper introduces PG-DPIR, an efficient PnP method for high-count Poisson-Gaussian inverse problems, adapted from DPIR. While DPIR is designed for white Gaussian noise, a naive adaptation to Poisson-Gaussian noise leads to prohibitively slow algorithms due to the absence of a closed-form proximal operator. To address this, we adapt DPIR for the specificities of Poisson-Gaussian noise and propose in particular an efficient initialization of the gradient descent required for the proximal step that accelerates convergence by several orders of magnitude. Experiments are conducted on satellite image restoration and super-resolution problems. High-resolution realistic Pleiades images are simulated for the experiments, which demonstrate that PG-DPIR achieves state-of-the-art performance with improved efficiency, which seems promising for on-ground satellite processing chains.\n        △ Less",
        "authors":[
            "Maud Biquard",
            "Marie Chabert",
            "Florence Genin",
            "Christophe Latry",
            "Thomas Oberlin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ctrl-Z: Controlling AI Agents via Resampling",
        "abstract":"Control evaluations measure whether monitoring and security protocols for AI systems prevent intentionally subversive AI models from causing harm. Our work presents the first control evaluation performed in an agent environment. We construct BashBench, a dataset of 257 challenging multi-step system administration tasks, and evaluate whether various safety measures can prevent an adversarially constructed AI agent from covertly downloading and executing malicious code in this environment. This multi-step setting introduces new attack and defense dynamics, which we investigate in order to design novel control protocols that prevent safety failures without hindering the ability of non-malicious agents to perform useful work. We introduce a class of control protocols called resample protocols that dynamically take additional samples of certain actions. We find these protocols significantly improve on existing techniques by selectively blocking the AI agent from executing suspicious code and incriminating the agent by generating additional examples of dangerous behavior. We measure the tradeoff between attack prevention and usefulness; our best protocol combines resampling with analysis of previous steps, reducing the success rate of attacks from 58% to 7% at a 5% cost to the performance of a non-malicious agent.\n        △ Less",
        "authors":[
            "Aryan Bhatt",
            "Cody Rushing",
            "Adam Kaufman",
            "Tyler Tracy",
            "Vasil Georgiev",
            "David Matolcsi",
            "Akbir Khan",
            "Buck Shlegeris"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DUE: A Deep Learning Framework and Library for Modeling Unknown Equations",
        "abstract":"Equations, particularly differential equations, are fundamental for understanding natural phenomena and predicting complex dynamics across various scientific and engineering disciplines. However, the governing equations for many complex systems remain unknown due to intricate underlying mechanisms. Recent advancements in machine learning and data science offer a new paradigm for modeling unknown equations from measurement or simulation data. This paradigm shift, known as data-driven discovery or modeling, stands at the forefront of AI for science, with significant progress made in recent years. In this paper, we introduce a systematic framework for data-driven modeling of unknown equations using deep learning. This versatile framework is capable of learning unknown ODEs, PDEs, DAEs, IDEs, SDEs, reduced or partially observed systems, and non-autonomous differential equations. Based on this framework, we have developed Deep Unknown Equations (DUE), an open-source software package designed to facilitate the data-driven modeling of unknown equations using modern deep learning techniques. DUE serves as an educational tool for classroom instruction, enabling students and newcomers to gain hands-on experience with differential equations, data-driven modeling, and contemporary deep learning approaches such as FNN, ResNet, generalized ResNet, operator semigroup networks (OSG-Net), and Transformers. Additionally, DUE is a versatile and accessible toolkit for researchers across various scientific and engineering fields. It is applicable not only for learning unknown equations from data but also for surrogate modeling of known, yet complex, equations that are costly to solve using traditional numerical methods. We provide detailed descriptions of DUE and demonstrate its capabilities through diverse examples, which serve as templates that can be easily adapted for other applications.\n        △ Less",
        "authors":[
            "Junfeng Chen",
            "Kailiang Wu",
            "Dongbin Xiu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning",
        "abstract":"Optimizing Register Transfer Level (RTL) code is crucial for improving the power, performance, and area (PPA) of digital circuits in the early stages of synthesis. Manual rewriting, guided by synthesis feedback, can yield high-quality results but is time-consuming and error-prone. Most existing compiler-based approaches have difficulty handling complex design constraints. Large Language Model (LLM)-based methods have emerged as a promising alternative to address these challenges. However, LLM-based approaches often face difficulties in ensuring alignment between the generated code and the provided prompts. This paper presents SymRTLO, a novel neuron-symbolic RTL optimization framework that seamlessly integrates LLM-based code rewriting with symbolic reasoning techniques. Our method incorporates a retrieval-augmented generation (RAG) system of optimization rules and Abstract Syntax Tree (AST)-based templates, enabling LLM-based rewriting that maintains syntactic correctness while minimizing undesired circuit behaviors. A symbolic module is proposed for analyzing and optimizing finite state machine (FSM) logic, allowing fine-grained state merging and partial specification handling beyond the scope of pattern-based compilers. Furthermore, a fast verification pipeline, combining formal equivalence checks with test-driven validation, further reduces the complexity of verification. Experiments on the RTL-Rewriter benchmark with Synopsys Design Compiler and Yosys show that SymRTLO improves power, performance, and area (PPA) by up to 43.9%, 62.5%, and 51.1%, respectively, compared to the state-of-the-art methods.\n        △ Less",
        "authors":[
            "Yiting Wang",
            "Wanghao Ye",
            "Ping Guo",
            "Yexiao He",
            "Ziyao Wang",
            "Yexiao He",
            "Bowei Tian",
            "Shwai He",
            "Guoheng Sun",
            "Zheyu Shen",
            "Sihan Chen",
            "Ankur Srivastava",
            "Qingfu Zhang",
            "Gang Qu",
            "Ang Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Domain-Adversarial Neural Network and Explainable AI for Reducing Tissue-of-Origin Signal in Pan-cancer Mortality Classification",
        "abstract":"Tissue-of-origin signals dominate pan-cancer gene expression, often obscuring molecular features linked to patient survival. This hampers the discovery of generalizable biomarkers, as models tend to overfit tissue-specific patterns rather than capture survival-relevant signals. To address this, we propose a Domain-Adversarial Neural Network (DANN) trained on TCGA RNA-seq data to learn representations less biased by tissue and more focused on survival. Identifying tissue-independent genetic profiles is key to revealing core cancer programs. We assess the DANN using: (1) Standard SHAP, based on the original input space and DANN's mortality classifier; (2) A layer-aware strategy applied to hidden activations, including an unsupervised manifold from raw activations and a supervised manifold from mortality-specific SHAP values. Standard SHAP remains confounded by tissue signals due to biases inherent in its computation. The raw activation manifold was dominated by high-magnitude activations, which masked subtle tissue and mortality-related signals. In contrast, the layer-aware SHAP manifold offers improved low-dimensional representations of both tissue and mortality signals, independent of activation strength, enabling subpopulation stratification and pan-cancer identification of survival-associated genes.\n        △ Less",
        "authors":[
            "Cristian Padron-Manrique",
            "Juan José Oropeza Valdez",
            "Osbaldo Resendis-Antonio"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families",
        "abstract":"Clinical case reports encode rich, temporal patient trajectories that are often underexploited by traditional machine learning methods relying on structured data. In this work, we introduce the forecasting problem from textual time series, where timestamped clinical findings--extracted via an LLM-assisted annotation pipeline--serve as the primary input for prediction. We systematically evaluate a diverse suite of models, including fine-tuned decoder-based large language models and encoder-based transformers, on tasks of event occurrence prediction, temporal ordering, and survival analysis. Our experiments reveal that encoder-based models consistently achieve higher F1 scores and superior temporal concordance for short- and long-horizon event forecasting, while fine-tuned masking approaches enhance ranking performance. In contrast, instruction-tuned decoder models demonstrate a relative advantage in survival analysis, especially in early prognosis settings. Our sensitivity analyses further demonstrate the importance of time ordering, which requires clinical time series construction, as compared to text ordering, the format of the text inputs that LLMs are classically trained on. This highlights the additional benefit that can be ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM use.\n        △ Less",
        "authors":[
            "Shahriar Noroozizadeh",
            "Sayantan Kumar",
            "Jeremy C. Weiss"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Siamese Network with Dual Attention for EEG-Driven Social Learning: Bridging the Human-Robot Gap in Long-Tail Autonomous Driving",
        "abstract":"Robots with wheeled, quadrupedal, or humanoid forms are increasingly integrated into built environments. However, unlike human social learning, they lack a critical pathway for intrinsic cognitive development, namely, learning from human feedback during interaction. To understand human ubiquitous observation, supervision, and shared control in dynamic and uncertain environments, this study presents a brain-computer interface (BCI) framework that enables classification of Electroencephalogram (EEG) signals to detect cognitively demanding and safety-critical events. As a timely and motivating co-robotic engineering application, we simulate a human-in-the-loop scenario to flag risky events in semi-autonomous robotic driving-representative of long-tail cases that pose persistent bottlenecks to the safety performance of smart mobility systems and robotic vehicles. Drawing on recent advances in few-shot learning, we propose a dual-attention Siamese convolutional network paired with Dynamic Time Warping Barycenter Averaging approach to generate robust EEG-encoded signal representations. Inverse source localization reveals activation in Broadman areas 4 and 9, indicating perception-action coupling during task-relevant mental imagery. The model achieves 80% classification accuracy under data-scarce conditions and exhibits a nearly 100% increase in the utility of salient features compared to state-of-the-art methods, as measured through integrated gradient attribution. Beyond performance, this study contributes to our understanding of the cognitive architecture required for BCI agents-particularly the role of attention and memory mechanisms-in categorizing diverse mental states and supporting both inter- and intra-subject adaptation. Overall, this research advances the development of cognitive robotics and socially guided learning for service robots in complex built environments.\n        △ Less",
        "authors":[
            "Xiaoshan Zhou",
            "Carol C. Menassa",
            "Vineet R. Kamat"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Noise2Ghost: Self-supervised deep convolutional reconstruction for ghost imaging",
        "abstract":"We present a new self-supervised deep-learning-based Ghost Imaging (GI) reconstruction method, which provides unparalleled reconstruction performance for noisy acquisitions among unsupervised methods. We present the supporting mathematical framework and results from theoretical and real data use cases. Self-supervision removes the need for clean reference data while offering strong noise reduction. This provides the necessary tools for addressing signal-to-noise ratio concerns for GI acquisitions in emerging and cutting-edge low-light GI scenarios. Notable examples include micro- and nano-scale x-ray emission imaging, e.g., x-ray fluorescence imaging of dose-sensitive samples. Their applications include in-vivo and in-operando case studies for biological samples and batteries.\n        △ Less",
        "authors":[
            "Mathieu Manni",
            "Dmitry Karpov",
            "K. Joost Batenburg",
            "Sharon Shwartz",
            "Nicola Viganò"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"$α$-Flow: A Unified Framework for Continuous-State Discrete Flow Matching Models",
        "abstract":"Recent efforts have extended the flow-matching framework to discrete generative modeling. One strand of models directly works with the continuous probabilities instead of discrete tokens, which we colloquially refer to as Continuous-State Discrete Flow Matching (CS-DFM). Existing CS-DFM models differ significantly in their representations and geometric assumptions. This work presents a unified framework for CS-DFM models, under which the existing variants can be understood as operating on different $α$-representations of probabilities. Building upon the theory of information geometry, we introduce $α$-Flow, a family of CS-DFM models that adheres to the canonical $α$-geometry of the statistical manifold, and demonstrate its optimality in minimizing the generalized kinetic energy. Theoretically, we show that the flow matching loss for $α$-flow establishes a unified variational bound for the discrete negative log-likelihood. We comprehensively evaluate different instantiations of $α$-flow on various discrete generation domains to demonstrate their effectiveness in discrete generative modeling, including intermediate values whose geometries have never been explored before. $α$-flow significantly outperforms its discrete-state counterpart in image and protein sequence generation and better captures the entropy in language modeling.\n        △ Less",
        "authors":[
            "Chaoran Cheng",
            "Jiahan Li",
            "Jiajun Fan",
            "Ge Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Zero-shot Autonomous Microscopy for Scalable and Intelligent Characterization of 2D Materials",
        "abstract":"Characterization of atomic-scale materials traditionally requires human experts with months to years of specialized training. Even for trained human operators, accurate and reliable characterization remains challenging when examining newly discovered materials such as two-dimensional (2D) structures. This bottleneck drives demand for fully autonomous experimentation systems capable of comprehending research objectives without requiring large training datasets. In this work, we present ATOMIC (Autonomous Technology for Optical Microscopy & Intelligent Characterization), an end-to-end framework that integrates foundation models to enable fully autonomous, zero-shot characterization of 2D materials. Our system integrates the vision foundation model (i.e., Segment Anything Model), large language models (i.e., ChatGPT), unsupervised clustering, and topological analysis to automate microscope control, sample scanning, image segmentation, and intelligent analysis through prompt engineering, eliminating the need for additional training. When analyzing typical MoS2 samples, our approach achieves 99.7% segmentation accuracy for single layer identification, which is equivalent to that of human experts. In addition, the integrated model is able to detect grain boundary slits that are challenging to identify with human eyes. Furthermore, the system retains robust accuracy despite variable conditions including defocus, color temperature fluctuations, and exposure variations. It is applicable to a broad spectrum of common 2D materials-including graphene, MoS2, WSe2, SnSe-regardless of whether they were fabricated via chemical vapor deposition or mechanical exfoliation. This work represents the implementation of foundation models to achieve autonomous analysis, establishing a scalable and data-efficient characterization paradigm that fundamentally transforms the approach to nanoscale materials research.\n        △ Less",
        "authors":[
            "Jingyun Yang",
            "Ruoyan Avery Yin",
            "Chi Jiang",
            "Yuepeng Hu",
            "Xiaokai Zhu",
            "Xingjian Hu",
            "Sutharsika Kumar",
            "Xiao Wang",
            "Xiaohua Zhai",
            "Keran Rong",
            "Yunyue Zhu",
            "Tianyi Zhang",
            "Zongyou Yin",
            "Jing Kong",
            "Neil Zhenqiang Gong",
            "Zhichu Ren",
            "Haozhe Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LMFormer: Lane based Motion Prediction Transformer",
        "abstract":"Motion prediction plays an important role in autonomous driving. This study presents LMFormer, a lane-aware transformer network for trajectory prediction tasks. In contrast to previous studies, our work provides a simple mechanism to dynamically prioritize the lanes and shows that such a mechanism introduces explainability into the learning behavior of the network. Additionally, LMFormer uses the lane connection information at intersections, lane merges, and lane splits, in order to learn long-range dependency in lane structure. Moreover, we also address the issue of refining the predicted trajectories and propose an efficient method for iterative refinement through stacked transformer layers. For benchmarking, we evaluate LMFormer on the nuScenes dataset and demonstrate that it achieves SOTA performance across multiple metrics. Furthermore, the Deep Scenario dataset is used to not only illustrate cross-dataset network performance but also the unification capabilities of LMFormer to train on multiple datasets and achieve better performance.\n        △ Less",
        "authors":[
            "Harsh Yadav",
            "Maximilian Schaefer",
            "Kun Zhao",
            "Tobias Meisen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sidecar: A Structure-Preserving Framework for Solving Partial Differential Equations with Neural Networks",
        "abstract":"Solving partial differential equations (PDEs) with neural networks (NNs) has shown great potential in various scientific and engineering fields. However, most existing NN solvers mainly focus on satisfying the given PDEs, without explicitly considering intrinsic physical properties such as mass conservation or energy dissipation. This limitation can result in unstable or nonphysical solutions, particularly in long-term simulations. To address this issue, we propose Sidecar, a novel framework that enhances the accuracy and physical consistency of existing NN solvers by incorporating structure-preserving knowledge. Inspired by the Time-Dependent Spectral Renormalization (TDSR) approach, our Sidecar framework introduces a small copilot network, which is trained to guide the existing NN solver in preserving physical structure. This framework is designed to be highly flexible, enabling the incorporation of structure-preserving principles from diverse PDEs into a wide range of NN solvers. Our experimental results on benchmark PDEs demonstrate the improvement of the existing neural network solvers in terms of accuracy and consistency with structure-preserving properties.\n        △ Less",
        "authors":[
            "Gaohang Chen",
            "Zhonghua Qiao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Vision based driving agent for race car simulation environments",
        "abstract":"In recent years, autonomous driving has become a popular field of study. As control at tire grip limit is essential during emergency situations, algorithms developed for racecars are useful for road cars too. This paper examines the use of Deep Reinforcement Learning (DRL) to solve the problem of grip limit driving in a simulated environment. Proximal Policy Optimization (PPO) method is used to train an agent to control the steering wheel and pedals of the vehicle, using only visual inputs to achieve professional human lap times. The paper outlines the formulation of the task of time optimal driving on a race track as a deep reinforcement learning problem, and explains the chosen observations, actions, and reward functions. The results demonstrate human-like learning and driving behavior that utilize maximum tire grip potential.\n        △ Less",
        "authors":[
            "Gergely Bári",
            "László Palkovics"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TinyverseGP: Towards a Modular Cross-domain Benchmarking Framework for Genetic Programming",
        "abstract":"Over the years, genetic programming (GP) has evolved, with many proposed variations, especially in how they represent a solution. Being essentially a program synthesis algorithm, it is capable of tackling multiple problem domains. Current benchmarking initiatives are fragmented, as the different representations are not compared with each other and their performance is not measured across the different domains. In this work, we propose a unified framework, dubbed TinyverseGP (inspired by tinyGP), which provides support to multiple representations and problem domains, including symbolic regression, logic synthesis and policy search.\n        △ Less",
        "authors":[
            "Roman Kalkreuth",
            "Fabricio Olivetti de França",
            "Julian Dierkes",
            "Marie Anastacio",
            "Anja Jankovic",
            "Zdenek Vasicek",
            "Holger Hoos"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Sensor Steering Strategy Using Deep Reinforcement Learning for Dynamic Data Acquisition in Digital Twins",
        "abstract":"This paper introduces a sensor steering methodology based on deep reinforcement learning to enhance the predictive accuracy and decision support capabilities of digital twins by optimising the data acquisition process. Traditional sensor placement techniques are often constrained by one-off optimisation strategies, which limit their applicability for online applications requiring continuous informative data assimilation. The proposed approach addresses this limitation by offering an adaptive framework for sensor placement within the digital twin paradigm. The sensor placement problem is formulated as a Markov decision process, enabling the training and deployment of an agent capable of dynamically repositioning sensors in response to the evolving conditions of the physical structure as represented by the digital twin. This ensures that the digital twin maintains a highly representative and reliable connection to its physical counterpart. The proposed framework is validated through a series of comprehensive case studies involving a cantilever plate structure subjected to diverse conditions, including healthy and damaged conditions. The results demonstrate the capability of the deep reinforcement learning agent to adaptively reposition sensors improving the quality of data acquisition and hence enhancing the overall accuracy of digital twins.\n        △ Less",
        "authors":[
            "Collins O. Ogbodo",
            "Timothy J. Rogers",
            "Mattia Dal Borgo",
            "David J. Wagg"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GNN-ACLP: Graph Neural Networks based Analog Circuit Link Prediction",
        "abstract":"Circuit link prediction identifying missing component connections from incomplete netlists is crucial in automating analog circuit design. However, existing methods face three main challenges: 1) Insufficient use of topological patterns in circuit graphs reduces prediction accuracy; 2) Data scarcity due to the complexity of annotations hinders model generalization; 3) Limited adaptability to various netlist formats. We propose GNN-ACLP, a Graph Neural Networks (GNNs) based framework featuring three innovations to tackle these challenges. First, we introduce the SEAL (Subgraphs, Embeddings, and Attributes for Link Prediction) framework and achieve port-level accuracy in circuit link prediction. Second, we propose Netlist Babel Fish, a netlist format conversion tool leveraging retrieval-augmented generation (RAG) with large language model (LLM) to enhance the compatibility of netlist formats. Finally, we construct SpiceNetlist, a comprehensive dataset that contains 775 annotated circuits across 10 different classes of components. The experimental results demonstrate an improvement of 15.05% on the SpiceNetlist dataset and 12.01% on the Image2Net dataset over the existing approach.\n        △ Less",
        "authors":[
            "Guanyuan Pan",
            "Tiansheng Zhou",
            "Bingtao Ma",
            "Yaqi Wang",
            "Jianxiang Zhao",
            "Shuai Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Model Zoo of Vision Transformers",
        "abstract":"The availability of large, structured populations of neural networks - called 'model zoos' - has led to the development of a multitude of downstream tasks ranging from model analysis, to representation learning on model weights or generative modeling of neural network parameters. However, existing model zoos are limited in size and architecture and neglect the transformer, which is among the currently most successful neural network architectures. We address this gap by introducing the first model zoo of vision transformers (ViT). To better represent recent training approaches, we develop a new blueprint for model zoo generation that encompasses both pre-training and fine-tuning steps, and publish 250 unique models. They are carefully generated with a large span of generating factors, and their diversity is validated using a thorough choice of weight-space and behavioral metrics. To further motivate the utility of our proposed dataset, we suggest multiple possible applications grounded in both extensive exploratory experiments and a number of examples from the existing literature. By extending previous lines of similar work, our model zoo allows researchers to push their model population-based methods from the small model regime to state-of-the-art architectures. We make our model zoo available at github.com\/ModelZoos\/ViTModelZoo.\n        △ Less",
        "authors":[
            "Damian Falk",
            "Léo Meynent",
            "Florence Pfammatter",
            "Konstantin Schürholt",
            "Damian Borth"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ROSFD: Robust Online Streaming Fraud Detection with Resilience to Concept Drift in Data Streams",
        "abstract":"Continuous generation of streaming data from diverse sources, such as online transactions and digital interactions, necessitates timely fraud detection. Traditional batch processing methods often struggle to capture the rapidly evolving patterns of fraudulent activities. This paper highlights the critical importance of processing streaming data for effective fraud detection. To address the inherent challenges of latency, scalability, and concept drift in streaming environments, we propose a robust online streaming fraud detection (ROSFD) framework. Our proposed framework comprises two key stages: (i) Stage One: Offline Model Initialization. In this initial stage, a model is built in offline settings using incremental learning principles to overcome the \"cold-start\" problem. (ii) Stage Two: Real-time Model Adaptation. In this dynamic stage, drift detection algorithms (viz.,, DDM, EDDM, and ADWIN) are employed to identify concept drift in the incoming data stream and incrementally train the model accordingly. This \"train-only-when-required\" strategy drastically reduces the number of retrains needed without significantly impacting the area under the receiver operating characteristic curve (AUC). Overall, ROSFD utilizing ADWIN as the drift detection method demonstrated the best performance among the employed methods. In terms of model efficacy, Adaptive Random Forest consistently outperformed other models, achieving the highest AUC in four out of five datasets.\n        △ Less",
        "authors":[
            "Vivek Yelleti"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From Prompting to Alignment: A Generative Framework for Query Recommendation",
        "abstract":"In modern search systems, search engines often suggest relevant queries to users through various panels or components, helping refine their information needs. Traditionally, these recommendations heavily rely on historical search logs to build models, which suffer from cold-start or long-tail issues. Furthermore, tasks such as query suggestion, completion or clarification are studied separately by specific design, which lacks generalizability and hinders adaptation to novel applications. Despite recent attempts to explore the use of LLMs for query recommendation, these methods mainly rely on the inherent knowledge of LLMs or external sources like few-shot examples, retrieved documents, or knowledge bases, neglecting the importance of the calibration and alignment with user feedback, thus limiting their practical utility. To address these challenges, we first propose a general Generative Query Recommendation (GQR) framework that aligns LLM-based query generation with user preference. Specifically, we unify diverse query recommendation tasks by a universal prompt framework, leveraging the instruct-following capability of LLMs for effective generation. Secondly, we align LLMs with user feedback via presenting a CTR-alignment framework, which involves training a query-wise CTR predictor as a process reward model and employing list-wise preference alignment to maximize the click probability of the generated query list. Furthermore, recognizing the inconsistency between LLM knowledge and proactive search intents arising from the separation of user-initiated queries from models, we align LLMs with user initiative via retrieving co-occurrence queries as side information when historical logs are available.\n        △ Less",
        "authors":[
            "Erxue Min",
            "Hsiu-Yuan Huang",
            "Min Yang",
            "Xihong Yang",
            "Xin Jia",
            "Yunfang Wu",
            "Hengyi Cai",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Generative Model Training via Embedded Representation Warmup",
        "abstract":"Diffusion models excel at generating high-dimensional data but fall short in training efficiency and representation quality compared to self-supervised methods. We identify a key bottleneck: the underutilization of high-quality, semantically rich representations during training notably slows down convergence. Our systematic analysis reveals a critical representation processing region -- primarily in the early layers -- where semantic and structural pattern learning takes place before generation can occur. To address this, we propose Embedded Representation Warmup (ERW), a plug-and-play framework where in the first stage we get the ERW module serves as a warmup that initializes the early layers of the diffusion model with high-quality, pretrained representations. This warmup minimizes the burden of learning representations from scratch, thereby accelerating convergence and boosting performance. Our theoretical analysis demonstrates that ERW's efficacy depends on its precise integration into specific neural network layers -- termed the representation processing region -- where the model primarily processes and transforms feature representations for later generation. We further establish that ERW not only accelerates training convergence but also enhances representation quality: empirically, our method achieves a 40$\\times$ acceleration in training speed compared to REPA, the current state-of-the-art methods. Code is available at https:\/\/github.com\/LINs-lab\/ERW.\n        △ Less",
        "authors":[
            "Deyuan Liu",
            "Peng Sun",
            "Xufeng Li",
            "Tao Lin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks",
        "abstract":"Large language model unlearning has become a critical challenge in ensuring safety and controlled model behavior by removing undesired data-model influences from the pretrained model while preserving general utility. Significant recent efforts have been dedicated to developing LLM unlearning benchmarks such as WMDP (Weapons of Mass Destruction Proxy) and MUSE (Machine Unlearning Six-way Evaluation), facilitating standardized unlearning performance assessment and method comparison. Despite their usefulness, we uncover for the first time a novel coreset effect within these benchmarks. Specifically, we find that LLM unlearning achieved with the original (full) forget set can be effectively maintained using a significantly smaller subset (functioning as a \"coreset\"), e.g., as little as 5% of the forget set, even when selected at random. This suggests that LLM unlearning in these benchmarks can be performed surprisingly easily, even in an extremely low-data regime. We demonstrate that this coreset effect remains strong, regardless of the LLM unlearning method used, such as NPO (Negative Preference Optimization) and RMU (Representation Misdirection Unlearning), the popular ones in these benchmarks. The surprisingly strong coreset effect is also robust across various data selection methods, ranging from random selection to more sophisticated heuristic approaches. We explain the coreset effect in LLM unlearning through a keyword-based perspective, showing that keywords extracted from the forget set alone contribute significantly to unlearning effectiveness and indicating that current unlearning is driven by a compact set of high-impact tokens rather than the entire dataset. We further justify the faithfulness of coreset-unlearned models along additional dimensions, such as mode connectivity and robustness to jailbreaking attacks. Codes are available at https:\/\/github.com\/OPTML-Group\/MU-Coreset.\n        △ Less",
        "authors":[
            "Soumyadeep Pal",
            "Changsheng Wang",
            "James Diffenderfer",
            "Bhavya Kailkhura",
            "Sijia Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Challenges in interpretability of additive models",
        "abstract":"We review generalized additive models as a type of ``transparent'' model that has recently seen renewed interest in the deep learning community as neural additive models. We highlight multiple types of nonidentifiability in this model class and discuss challenges in interpretability, arguing for restraint when claiming ``interpretability'' or ``suitability for safety-critical applications'' of such models.\n        △ Less",
        "authors":[
            "Xinyu Zhang",
            "Julien Martinelli",
            "ST John"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning",
        "abstract":"Large-scale reinforcement learning (RL) methods have proven highly effective in enhancing the reasoning abilities of large language models (LLMs), particularly for tasks with verifiable solutions such as mathematics and coding. However, applying this idea to machine translation (MT), where outputs are flexibly formatted and difficult to automatically evaluate with explicit rules, remains underexplored. In this work, we introduce MT-R1-Zero, the first open-source adaptation of the R1-Zero RL framework for MT without supervised fine-tuning or cold-start. We propose a rule-metric mixed reward mechanism to guide LLMs towards improved translation quality via emergent reasoning. On the WMT 24 English-Chinese benchmark, our MT-R1-Zero-3B-Mix achieves competitive performance, surpassing TowerInstruct-7B-v0.2 by an average of 1.26 points. Meanwhile, our MT-R1-Zero-7B-Mix attains a high average score of 62.25 across all metrics, placing it on par with advanced proprietary models such as GPT-4o and Claude-3.5-Sonnet, while the MT-R1-Zero-7B-Sem variant achieves state-of-the-art scores on semantic metrics. Moreover, our work exhibits strong generalization capabilities on out-of-distribution MT tasks, robustly supporting multilingual and low-resource settings. Extensive analysis of model behavior across different initializations and reward metrics offers pioneering insight into the critical role of reward design, LLM adaptability, training dynamics, and emergent reasoning patterns within the R1-Zero paradigm for MT. Our code is available at https:\/\/github.com\/fzp0424\/MT-R1-Zero.\n        △ Less",
        "authors":[
            "Zhaopeng Feng",
            "Shaosheng Cao",
            "Jiahan Ren",
            "Jiayuan Su",
            "Ruizhe Chen",
            "Yan Zhang",
            "Zhe Xu",
            "Yao Hu",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Continual learning for rotating machinery fault diagnosis with cross-domain environmental and operational variations",
        "abstract":"Although numerous machine learning models exist to detect issues like rolling bearing strain and deformation, typically caused by improper mounting, overloading, or poor lubrication, these models often struggle to isolate faults from the noise of real-world operational and environmental variability. Conditions such as variable loads, high temperatures, stress, and rotational speeds can mask early signs of failure, making reliable detection challenging. To address these limitations, this work proposes a continual deep learning approach capable of learning across domains that share underlying structure over time. This approach goes beyond traditional accuracy metrics by addressing four second-order challenges: catastrophic forgetting (where new learning overwrites past knowledge), lack of plasticity (where models fail to adapt to new data), forward transfer (using past knowledge to improve future learning), and backward transfer (refining past knowledge with insights from new domains). The method comprises a feature generator and domain-specific classifiers, allowing capacity to grow as new domains emerge with minimal interference, while an experience replay mechanism selectively revisits prior domains to mitigate forgetting. Moreover, nonlinear dependencies across domains are exploited by prioritizing replay from those with the highest prior errors, refining models based on most informative past experiences. Experiments show high average domain accuracy (up to 88.96%), with forgetting measures as low as .0027 across non-stationary class-incremental environments.\n        △ Less",
        "authors":[
            "Diogo Risca",
            "Afonso Lourenço",
            "Goreti Marreiros"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BoTTA: Benchmarking on-device Test Time Adaptation",
        "abstract":"The performance of deep learning models depends heavily on test samples at runtime, and shifts from the training data distribution can significantly reduce accuracy. Test-time adaptation (TTA) addresses this by adapting models during inference without requiring labeled test data or access to the original training set. While research has explored TTA from various perspectives like algorithmic complexity, data and class distribution shifts, model architectures, and offline versus continuous learning, constraints specific to mobile and edge devices remain underexplored. We propose BoTTA, a benchmark designed to evaluate TTA methods under practical constraints on mobile and edge devices. Our evaluation targets four key challenges caused by limited resources and usage conditions: (i) limited test samples, (ii) limited exposure to categories, (iii) diverse distribution shifts, and (iv) overlapping shifts within a sample. We assess state-of-the-art TTA methods under these scenarios using benchmark datasets and report system-level metrics on a real testbed. Furthermore, unlike prior work, we align with on-device requirements by advocating periodic adaptation instead of continuous inference-time adaptation. Experiments reveal key insights: many recent TTA algorithms struggle with small datasets, fail to generalize to unseen categories, and depend on the diversity and complexity of distribution shifts. BoTTA also reports device-specific resource use. For example, while SHOT improves accuracy by $2.25\\times$ with $512$ adaptation samples, it uses $1.08\\times$ peak memory on Raspberry Pi versus the base model. BoTTA offers actionable guidance for TTA in real-world, resource-constrained deployments.\n        △ Less",
        "authors":[
            "Michal Danilowski",
            "Soumyajit Chatterjee",
            "Abhirup Ghosh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Survey of Personalization: From RAG to Agent",
        "abstract":"Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more advanced agent-based architectures within personalized settings to enhance user satisfaction. Building on this foundation, this survey systematically examines personalization across the three core stages of RAG: pre-retrieval, retrieval, and generation. Beyond RAG, we further extend its capabilities into the realm of Personalized LLM-based Agents, which enhance traditional RAG systems with agentic functionalities, including user understanding, personalized planning and execution, and dynamic generation. For both personalization in RAG and agent-based personalization, we provide formal definitions, conduct a comprehensive review of recent literature, and summarize key datasets and evaluation metrics. Additionally, we discuss fundamental challenges, limitations, and promising research directions in this evolving field. Relevant papers and resources are continuously updated at https:\/\/github.com\/Applied-Machine-Learning-Lab\/Awesome-Personalized-RAG-Agent.\n        △ Less",
        "authors":[
            "Xiaopeng Li",
            "Pengyue Jia",
            "Derong Xu",
            "Yi Wen",
            "Yingyi Zhang",
            "Wenlin Zhang",
            "Wanyu Wang",
            "Yichao Wang",
            "Zhaocheng Du",
            "Xiangyang Li",
            "Yong Liu",
            "Huifeng Guo",
            "Ruiming Tang",
            "Xiangyu Zhao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GeoUni: A Unified Model for Generating Geometry Diagrams, Problems and Problem Solutions",
        "abstract":"We propose GeoUni, the first unified geometry expert model capable of generating problem solutions and diagrams within a single framework in a way that enables the creation of unique and individualized geometry problems. Traditionally, solving geometry problems and generating diagrams have been treated as separate tasks in machine learning, with no models successfully integrating both to support problem creation. However, we believe that mastery in geometry requires frictionless integration of all of these skills, from solving problems to visualizing geometric relationships, and finally, crafting tailored problems. Our extensive experiments demonstrate that GeoUni, with only 1.5B parameters, achieves performance comparable to larger models such as DeepSeek-R1 with 671B parameters in geometric reasoning tasks. GeoUni also excels in generating precise geometric diagrams, surpassing both text-to-image models and unified models, including the GPT-4o image generation. Most importantly, GeoUni is the only model capable of successfully generating textual problems with matching diagrams based on specific knowledge points, thus offering a wider range of capabilities that extend beyond current models.\n        △ Less",
        "authors":[
            "Jo-Ku Cheng",
            "Zeren Zhang",
            "Ran Chen",
            "Jingyang Deng",
            "Ziran Qin",
            "Jinwen Ma"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning",
        "abstract":"Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize misalignment by introducing two specific mechanisms: selection bias, where some semantic variables are missing, and perturbation bias, where semantic variables are distorted -- both affecting latent variables shared across modalities. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings through extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of misalignment on multimodal representation learning.\n        △ Less",
        "authors":[
            "Yichao Cai",
            "Yuhang Liu",
            "Erdun Gao",
            "Tianjiao Jiang",
            "Zhen Zhang",
            "Anton van den Hengel",
            "Javen Qinfeng Shi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Impact of Model Zoo Size and Composition on Weight Space Learning",
        "abstract":"Re-using trained neural network models is a common strategy to reduce training cost and transfer knowledge. Weight space learning - using the weights of trained models as data modality - is a promising new field to re-use populations of pre-trained models for future tasks. Approaches in this field have demonstrated high performance both on model analysis and weight generation tasks. However, until now their learning setup requires homogeneous model zoos where all models share the same exact architecture, limiting their capability to generalize beyond the population of models they saw during training. In this work, we remove this constraint and propose a modification to a common weight space learning method to accommodate training on heterogeneous populations of models. We further investigate the resulting impact of model diversity on generating unseen neural network model weights for zero-shot knowledge transfer. Our extensive experimental evaluation shows that including models with varying underlying image datasets has a high impact on performance and generalization, for both in- and out-of-distribution settings. Code is available on github.com\/HSG-AIML\/MultiZoo-SANE.\n        △ Less",
        "authors":[
            "Damian Falk",
            "Konstantin Schürholt",
            "Damian Borth"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conditional Distribution Compression via the Kernel Conditional Mean Embedding",
        "abstract":"Existing distribution compression methods, like Kernel Herding (KH), were originally developed for unlabelled data. However, no existing approach directly compresses the conditional distribution of labelled data. To address this gap, we first introduce the Average Maximum Conditional Mean Discrepancy (AMCMD), a natural metric for comparing conditional distributions. We then derive a consistent estimator for the AMCMD and establish its rate of convergence. Next, we make a key observation: in the context of distribution compression, the cost of constructing a compressed set targeting the AMCMD can be reduced from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(n)$. Building on this, we extend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a linear-time greedy algorithm that constructs a compressed set targeting the AMCMD. To better understand the advantages of directly compressing the conditional distribution rather than doing so via the joint distribution, we introduce Joint Kernel Herding (JKH), a straightforward adaptation of KH designed to compress the joint distribution of labelled data. While herding methods provide a simple and interpretable selection process, they rely on a greedy heuristic. To explore alternative optimisation strategies, we propose Joint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing Points (ACKIP), which jointly optimise the compressed set while maintaining linear complexity. Experiments show that directly preserving conditional distributions with ACKIP outperforms both joint distribution compression (via JKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP consistently outperforms JKH.\n        △ Less",
        "authors":[
            "Dominic Broadbent",
            "Nick Whiteley",
            "Robert Allison",
            "Tom Lovett"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Uncertainty Propagation in the Fast Fourier Transform",
        "abstract":"We address the problem of uncertainty propagation in the discrete Fourier transform by modeling the fast Fourier transform as a factor graph. Building on this representation, we propose an efficient framework for approximate Bayesian inference using belief propagation (BP) and expectation propagation, extending its applicability beyond Gaussian assumptions. By leveraging an appropriate BP message representation and a suitable schedule, our method achieves stable convergence with accurate mean and variance estimates. Numerical experiments in representative scenarios from communications demonstrate the practical potential of the proposed framework for uncertainty-aware inference in probabilistic systems operating across both time and frequency domain.\n        △ Less",
        "authors":[
            "Luca Schmid",
            "Charlotte Muth",
            "Laurent Schmalen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"STaRFormer: Semi-Supervised Task-Informed Representation Learning via Dynamic Attention-Based Regional Masking for Sequential Data",
        "abstract":"Accurate predictions using sequential spatiotemporal data are crucial for various applications. Utilizing real-world data, we aim to learn the intent of a smart device user within confined areas of a vehicle's surroundings. However, in real-world scenarios, environmental factors and sensor limitations result in non-stationary and irregularly sampled data, posing significant challenges. To address these issues, we developed a Transformer-based approach, STaRFormer, which serves as a universal framework for sequential modeling. STaRFormer employs a novel, dynamic attention-based regional masking scheme combined with semi-supervised contrastive learning to enhance task-specific latent representations. Comprehensive experiments on 15 datasets varying in types (including non-stationary and irregularly sampled), domains, sequence lengths, training samples, and applications, demonstrate the efficacy and practicality of STaRFormer. We achieve notable improvements over state-of-the-art approaches. Code and data will be made available.\n        △ Less",
        "authors":[
            "Maxmilian Forstenhäusler",
            "Daniel Külzer",
            "Christos Anagnostopoulos",
            "Shameem Puthiya Parambath",
            "Natascha Weber"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Quantifying Commonsense Reasoning with Mechanistic Insights",
        "abstract":"Commonsense reasoning deals with the implicit knowledge that is well understood by humans and typically acquired via interactions with the world. In recent times, commonsense reasoning and understanding of various LLMs have been evaluated using text-based tasks. In this work, we argue that a proxy of this understanding can be maintained as a graphical structure that can further help to perform a rigorous evaluation of commonsense reasoning abilities about various real-world activities. We create an annotation scheme for capturing this implicit knowledge in the form of a graphical structure for 37 daily human activities. We find that the created resource can be used to frame an enormous number of commonsense queries (~ 10^{17}), facilitating rigorous evaluation of commonsense reasoning in LLMs. Moreover, recently, the remarkable performance of LLMs has raised questions about whether these models are truly capable of reasoning in the wild and, in general, how reasoning occurs inside these models. In this resource paper, we bridge this gap by proposing design mechanisms that facilitate research in a similar direction. Our findings suggest that the reasoning components are localized in LLMs that play a prominent role in decision-making when prompted with a commonsense query.\n        △ Less",
        "authors":[
            "Abhinav Joshi",
            "Areeb Ahmad",
            "Divyaksh Shukla",
            "Ashutosh Modi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Scalable Bayesian Optimization via Gradient-Informed Bayesian Neural Networks",
        "abstract":"Bayesian optimization (BO) is a widely used method for data-driven optimization that generally relies on zeroth-order data of objective function to construct probabilistic surrogate models. These surrogates guide the exploration-exploitation process toward finding global optimum. While Gaussian processes (GPs) are commonly employed as surrogates of the unknown objective function, recent studies have highlighted the potential of Bayesian neural networks (BNNs) as scalable and flexible alternatives. Moreover, incorporating gradient observations into GPs, when available, has been shown to improve BO performance. However, the use of gradients within BNN surrogates remains unexplored. By leveraging automatic differentiation, gradient information can be seamlessly integrated into BNN training, resulting in more informative surrogates for BO. We propose a gradient-informed loss function for BNN training, effectively augmenting function observations with local gradient information. The effectiveness of this approach is demonstrated on well-known benchmarks in terms of improved BNN predictions and faster BO convergence as the number of decision variables increases.\n        △ Less",
        "authors":[
            "Georgios Makrygiorgos",
            "Joshua Hang Sai Ip",
            "Ali Mesbah"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Undermining Federated Learning Accuracy in EdgeIoT via Variational Graph Auto-Encoders",
        "abstract":"EdgeIoT represents an approach that brings together mobile edge computing with Internet of Things (IoT) devices, allowing for data processing close to the data source. Sending source data to a server is bandwidth-intensive and may compromise privacy. Instead, federated learning allows each device to upload a shared machine-learning model update with locally processed data. However, this technique, which depends on aggregating model updates from various IoT devices, is vulnerable to attacks from malicious entities that may inject harmful data into the learning process. This paper introduces a new attack method targeting federated learning in EdgeIoT, known as data-independent model manipulation attack. This attack does not rely on training data from the IoT devices but instead uses an adversarial variational graph auto-encoder (AV-GAE) to create malicious model updates by analyzing benign model updates intercepted during communication. AV-GAE identifies and exploits structural relationships between benign models and their training data features. By manipulating these structural correlations, the attack maximizes the training loss of the federated learning system, compromising its overall effectiveness.\n        △ Less",
        "authors":[
            "Kai Li",
            "Shuyan Hu",
            "Bochun Wu",
            "Sai Zou",
            "Wei Ni",
            "Falko Dressler"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CHARM: Calibrating Reward Models With Chatbot Arena Scores",
        "abstract":"Reward models (RMs) play a crucial role in Reinforcement Learning from Human Feedback by serving as proxies for human preferences in aligning large language models. In this paper, we identify a model preference bias in RMs, where they systematically assign disproportionately high scores to responses from certain policy models. This bias distorts ranking evaluations and leads to unfair judgments. To address this issue, we propose a calibration method named CHatbot Arena calibrated Reward Modeling (CHARM) that leverages Elo scores from the Chatbot Arena leaderboard to mitigate RM overvaluation. We also introduce a Mismatch Degree metric to measure this preference bias. Our approach is computationally efficient, requiring only a small preference dataset for continued training of the RM. We conduct extensive experiments on reward model benchmarks and human preference alignment. Results demonstrate that our calibrated RMs (1) achieve improved evaluation accuracy on RM-Bench and the Chat-Hard domain of RewardBench, and (2) exhibit a stronger correlation with human preferences by producing scores more closely aligned with Elo rankings. By mitigating model preference bias, our method provides a generalizable and efficient solution for building fairer and more reliable reward models.\n        △ Less",
        "authors":[
            "Xiao Zhu",
            "Chenmien Tan",
            "Pinzhen Chen",
            "Rico Sennrich",
            "Yanlin Zhang",
            "Hanxu Hu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Investigating the Role of Bilateral Symmetry for Inpainting Brain MRI",
        "abstract":"Inpainting has recently emerged as a valuable and interesting technology to employ in the analysis of medical imaging data, in particular brain MRI. A wide variety of methodologies for inpainting MRI have been proposed and demonstrated on tasks including anomaly detection. In this work we investigate the statistical relationship between inpainted brain structures and the amount of subject-specific conditioning information, i.e. the other areas of the image that are masked. In particular, we analyse the distribution of inpainting results when masking additional regions of the image, specifically the contra-lateral structure. This allows us to elucidate where in the brain the model is drawing information from, and in particular, what is the importance of hemispherical symmetry? Our experiments interrogate a diffusion inpainting model through analysing the inpainting of subcortical brain structures based on intensity and estimated area change. We demonstrate that some structures show a strong influence of symmetry in the conditioning of the inpainting process.\n        △ Less",
        "authors":[
            "Sergey Kuznetsov",
            "Sanduni Pinnawala",
            "Peter A. Wijeratne",
            "Ivor J. A. Simpson"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using Reinforcement Learning to Integrate Subjective Wellbeing into Climate Adaptation Decision Making",
        "abstract":"Subjective wellbeing is a fundamental aspect of human life, influencing life expectancy and economic productivity, among others. Mobility plays a critical role in maintaining wellbeing, yet the increasing frequency and intensity of both nuisance and high-impact floods due to climate change are expected to significantly disrupt access to activities and destinations, thereby affecting overall wellbeing. Addressing climate adaptation presents a complex challenge for policymakers, who must select and implement policies from a broad set of options with varying effects while managing resource constraints and uncertain climate projections. In this work, we propose a multi-modular framework that uses reinforcement learning as a decision-support tool for climate adaptation in Copenhagen, Denmark. Our framework integrates four interconnected components: long-term rainfall projections, flood modeling, transport accessibility, and wellbeing modeling. This approach enables decision-makers to identify spatial and temporal policy interventions that help sustain or enhance subjective wellbeing over time. By modeling climate adaptation as an open-ended system, our framework provides a structured framework for exploring and evaluating adaptation policy pathways. In doing so, it supports policymakers to make informed decisions that maximize wellbeing in the long run.\n        △ Less",
        "authors":[
            "Arthur Vandervoort",
            "Miguel Costa",
            "Morten W. Petersen",
            "Martin Drews",
            "Sonja Haustein",
            "Karyn Morrissey",
            "Francisco C. Pereira"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sequence models for by-trial decoding of cognitive strategies from neural data",
        "abstract":"Understanding the sequence of cognitive operations that underlie decision-making is a fundamental challenge in cognitive neuroscience. Traditional approaches often rely on group-level statistics, which obscure trial-by-trial variations in cognitive strategies. In this study, we introduce a novel machine learning method that combines Hidden Multivariate Pattern analysis with a Structured State Space Sequence model to decode cognitive strategies from electroencephalography data at the trial level. We apply this method to a decision-making task, where participants were instructed to prioritize either speed or accuracy in their responses. Our results reveal an additional cognitive operation, labeled Confirmation, which seems to occur predominantly in the accuracy condition but also frequently in the speed condition. The modeled probability that this operation occurs is associated with higher probability of responding correctly as well as changes of mind, as indexed by electromyography data. By successfully modeling cognitive operations at the trial level, we provide empirical evidence for dynamic variability in decision strategies, challenging the assumption of homogeneous cognitive processes within experimental conditions. Our approach shows the potential of sequence modeling in cognitive neuroscience to capture trial-level variability that is obscured by aggregate analyses. The introduced method offers a new way to detect and understand cognitive strategies in a data-driven manner, with implications for both theoretical research and practical applications in many fields.\n        △ Less",
        "authors":[
            "Rick den Otter",
            "Gabriel Weindel",
            "Sjoerd Stuit",
            "Leendert van Maanen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Masked Autoencoder Self Pre-Training for Defect Detection in Microelectronics",
        "abstract":"Whereas in general computer vision, transformer-based architectures have quickly become the gold standard, microelectronics defect detection still heavily relies on convolutional neural networks (CNNs). We hypothesize that this is due to the fact that a) transformers have an increased need for data and b) labelled image generation procedures for microelectronics are costly, and labelled data is therefore sparse. Whereas in other domains, pre-training on large natural image datasets can mitigate this problem, in microelectronics transfer learning is hindered due to the dissimilarity of domain data and natural images. Therefore, we evaluate self pre-training, where models are pre-trained on the target dataset, rather than another dataset. We propose a vision transformer (ViT) pre-training framework for defect detection in microelectronics based on masked autoencoders (MAE). In MAE, a large share of image patches is masked and reconstructed by the model during pre-training. We perform pre-training and defect detection using a dataset of less than 10.000 scanning acoustic microscopy (SAM) images labelled using transient thermal analysis (TTA). Our experimental results show that our approach leads to substantial performance gains compared to a) supervised ViT, b) ViT pre-trained on natural image datasets, and c) state-of-the-art CNN-based defect detection models used in the literature. Additionally, interpretability analysis reveals that our self pre-trained models, in comparison to ViT baselines, correctly focus on defect-relevant features such as cracks in the solder material. This demonstrates that our approach yields fault-specific feature representations, making our self pre-trained models viable for real-world defect detection in microelectronics.\n        △ Less",
        "authors":[
            "Nikolai Röhrich",
            "Alwin Hoffmann",
            "Richard Nordsieck",
            "Emilio Zarbali",
            "Alireza Javanmardi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Air Quality Prediction with A Meteorology-Guided Modality-Decoupled Spatio-Temporal Network",
        "abstract":"Air quality prediction plays a crucial role in public health and environmental protection. Accurate air quality prediction is a complex multivariate spatiotemporal problem, that involves interactions across temporal patterns, pollutant correlations, spatial station dependencies, and particularly meteorological influences that govern pollutant dispersion and chemical transformations. Existing works underestimate the critical role of atmospheric conditions in air quality prediction and neglect comprehensive meteorological data utilization, thereby impairing the modeling of dynamic interdependencies between air quality and meteorological data. To overcome this, we propose MDSTNet, an encoder-decoder framework that explicitly models air quality observations and atmospheric conditions as distinct modalities, integrating multi-pressure-level meteorological data and weather forecasts to capture atmosphere-pollution dependencies for prediction. Meantime, we construct ChinaAirNet, the first nationwide dataset combining air quality records with multi-pressure-level meteorological observations. Experimental results on ChinaAirNet demonstrate MDSTNet's superiority, substantially reducing 48-hour prediction errors by 17.54\\% compared to the state-of-the-art model. The source code and dataset will be available on github.\n        △ Less",
        "authors":[
            "Hang Yin",
            "Yan-Ming Zhang",
            "Jian Xu",
            "Jian-Long Chang",
            "Yin Li",
            "Cheng-Lin Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration",
        "abstract":"In recent years, deep neural networks (DNNs) have demonstrated state-of-the-art performance across various domains. However, despite their success, they often face calibration issues, particularly in safety-critical applications such as autonomous driving and healthcare, where unreliable predictions can have serious consequences. Recent research has started to improve model calibration from the view of the classifier. However, the exploration of designing the classifier to solve the model calibration problem is insufficient. Let alone most of the existing methods ignore the calibration errors arising from underconfidence. In this work, we propose a novel method by balancing learnable and ETF classifiers to solve the overconfidence or underconfidence problem for model Calibration named BalCAL. By introducing a confidence-tunable module and a dynamic adjustment method, we ensure better alignment between model confidence and its true accuracy. Extensive experimental validation shows that ours significantly improves model calibration performance while maintaining high predictive accuracy, outperforming existing techniques. This provides a novel solution to the calibration challenges commonly encountered in deep learning.\n        △ Less",
        "authors":[
            "Jiani Ni",
            "He Zhao",
            "Jintong Gao",
            "Dandan Guo",
            "Hongyuan Zha"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving Controller Generalization with Dimensionless Markov Decision Processes",
        "abstract":"Controllers trained with Reinforcement Learning tend to be very specialized and thus generalize poorly when their testing environment differs from their training one. We propose a Model-Based approach to increase generalization where both world model and policy are trained in a dimensionless state-action space. To do so, we introduce the Dimensionless Markov Decision Process ($Π$-MDP): an extension of Contextual-MDPs in which state and action spaces are non-dimensionalized with the Buckingham-$Π$ theorem. This procedure induces policies that are equivariant with respect to changes in the context of the underlying dynamics. We provide a generic framework for this approach and apply it to a model-based policy search algorithm using Gaussian Process models. We demonstrate the applicability of our method on simulated actuated pendulum and cartpole systems, where policies trained on a single environment are robust to shifts in the distribution of the context.\n        △ Less",
        "authors":[
            "Valentin Charvet",
            "Sebastian Stein",
            "Roderick Murray-Smith"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Session-based Recommender Systems: User Interest as a Stochastic Process in the Latent Space",
        "abstract":"This paper jointly addresses the problem of data uncertainty, popularity bias, and exposure bias in session-based recommender systems. We study the symptoms of this bias both in item embeddings and in recommendations. We propose treating user interest as a stochastic process in the latent space and providing a model-agnostic implementation of this mathematical concept. The proposed stochastic component consists of elements: debiasing item embeddings with regularization for embedding uniformity, modeling dense user interest from session prefixes, and introducing fake targets in the data to simulate extended exposure. We conducted computational experiments on two popular benchmark datasets, Diginetica and YooChoose 1\/64, as well as several modifications of the YooChoose dataset with different ratios of popular items. The results show that the proposed approach allows us to mitigate the challenges mentioned.\n        △ Less",
        "authors":[
            "Klaudia Balcer",
            "Piotr Lipinski"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FLoRA: Sample-Efficient Preference-based RL via Low-Rank Style Adaptation of Reward Functions",
        "abstract":"Preference-based reinforcement learning (PbRL) is a suitable approach for style adaptation of pre-trained robotic behavior: adapting the robot's policy to follow human user preferences while still being able to perform the original task. However, collecting preferences for the adaptation process in robotics is often challenging and time-consuming. In this work we explore the adaptation of pre-trained robots in the low-preference-data regime. We show that, in this regime, recent adaptation approaches suffer from catastrophic reward forgetting (CRF), where the updated reward model overfits to the new preferences, leading the agent to become unable to perform the original task. To mitigate CRF, we propose to enhance the original reward model with a small number of parameters (low-rank matrices) responsible for modeling the preference adaptation. Our evaluation shows that our method can efficiently and effectively adjust robotic behavior to human preferences across simulation benchmark tasks and multiple real-world robotic tasks.\n        △ Less",
        "authors":[
            "Daniel Marta",
            "Simon Holk",
            "Miguel Vasco",
            "Jens Lundell",
            "Timon Homberger",
            "Finn Busch",
            "Olov Andersson",
            "Danica Kragic",
            "Iolanda Leite"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
        "abstract":"Multi-modal large language models (MLLMs) have made significant progress, yet their safety alignment remains limited. Typically, current open-source MLLMs rely on the alignment inherited from their language module to avoid harmful generations. However, the lack of safety measures specifically designed for multi-modal inputs creates an alignment gap, leaving MLLMs vulnerable to vision-domain attacks such as typographic manipulation. Current methods utilize a carefully designed safety dataset to enhance model defense capability, while the specific knowledge or patterns acquired from the high-quality dataset remain unclear. Through comparison experiments, we find that the alignment gap primarily arises from data distribution biases, while image content, response quality, or the contrastive behavior of the dataset makes little contribution to boosting multi-modal safety. To further investigate this and identify the key factors in improving MLLM safety, we propose finetuning MLLMs on a small set of benign instruct-following data with responses replaced by simple, clear rejection sentences. Experiments show that, without the need for labor-intensive collection of high-quality malicious data, model safety can still be significantly improved, as long as a specific fraction of rejection data exists in the finetuning set, indicating the security alignment is not lost but rather obscured during multi-modal pretraining or instruction finetuning. Simply correcting the underlying data bias could narrow the safety gap in the vision domain.\n        △ Less",
        "authors":[
            "Yanbo Wang",
            "Jiyang Guan",
            "Jian Liang",
            "Ran He"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Physical Scales Matter: The Role of Receptive Fields and Advection in Satellite-Based Thunderstorm Nowcasting with Convolutional Neural Networks",
        "abstract":"The focus of nowcasting development is transitioning from physically motivated advection methods to purely data-driven Machine Learning (ML) approaches. Nevertheless, recent work indicates that incorporating advection into the ML value chain has improved skill for radar-based precipitation nowcasts. However, the generality of this approach and the underlying causes remain unexplored. This study investigates the generality by probing the approach on satellite-based thunderstorm nowcasts for the first time. Resorting to a scale argument, we then put forth an explanation when and why skill improvements can be expected. In essence, advection guarantees that thunderstorm patterns relevant for nowcasting are contained in the receptive field at long lead times. To test our hypotheses, we train ResU-Nets solving segmentation tasks with lightning observations as ground truth. The input of the Baseline Neural Network (BNN) are short time series of multispectral satellite imagery and lightning observations, whereas the Advection-Informed Neural Network (AINN) additionally receives the Lagrangian persistence nowcast of all input channels at the desired lead time. Overall, we find only a minor skill improvement of the AINN over the BNN when considering fully averaged scores. However, assessing skill conditioned on lead time and wind speed, we demonstrate that our scale argument correctly predicts the onset of skill improvement of the AINN over the BNN after 2h lead time. We confirm that generally advection becomes gradually more important with longer lead times and higher wind speeds. Our work accentuates the importance of considering and incorporating the underlying physical scales when designing ML based forecasting models.\n        △ Less",
        "authors":[
            "Christoph Metzl",
            "Kianusch Vahid Yousefnia",
            "Richard Müller",
            "Virginia Poli",
            "Miria Celano",
            "Tobias Bölle"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AimTS: Augmented Series and Image Contrastive Learning for Time Series Classification",
        "abstract":"Time series classification (TSC) is an important task in time series analysis. Existing TSC methods mainly train on each single domain separately, suffering from a degradation in accuracy when the samples for training are insufficient in certain domains. The pre-training and fine-tuning paradigm provides a promising direction for solving this problem. However, time series from different domains are substantially divergent, which challenges the effective pre-training on multi-source data and the generalization ability of pre-trained models. To handle this issue, we introduce Augmented Series and Image Contrastive Learning for Time Series Classification (AimTS), a pre-training framework that learns generalizable representations from multi-source time series data. We propose a two-level prototype-based contrastive learning method to effectively utilize various augmentations in multi-source pre-training, which learns representations for TSC that can be generalized to different domains. In addition, considering augmentations within the single time series modality are insufficient to fully address classification problems with distribution shift, we introduce the image modality to supplement structural information and establish a series-image contrastive learning to improve the generalization of the learned representations for TSC tasks. Extensive experiments show that after multi-source pre-training, AimTS achieves good generalization performance, enabling efficient learning and even few-shot learning on various downstream TSC datasets.\n        △ Less",
        "authors":[
            "Yuxuan Chen",
            "Shanshan Huang",
            "Yunyao Cheng",
            "Peng Chen",
            "Zhongwen Rao",
            "Yang Shu",
            "Bin Yang",
            "Lujia Pan",
            "Chenjuan Guo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"IsoSEL: Isometric Structural Entropy Learning for Deep Graph Clustering in Hyperbolic Space",
        "abstract":"Graph clustering is a longstanding topic in machine learning. In recent years, deep learning methods have achieved encouraging results, but they still require predefined cluster numbers K, and typically struggle with imbalanced graphs, especially in identifying minority clusters. The limitations motivate us to study a challenging yet practical problem: deep graph clustering without K considering the imbalance in reality. We approach this problem from a fresh perspective of information theory (i.e., structural information). In the literature, structural information has rarely been touched in deep clustering, and the classic definition falls short in its discrete formulation, neglecting node attributes and exhibiting prohibitive complexity. In this paper, we first establish a new Differentiable Structural Information, generalizing the discrete formalism to continuous realm, so that the optimal partitioning tree, revealing the cluster structure, can be created by the gradient backpropagation. Theoretically, we demonstrate its capability in clustering without requiring K and identifying the minority clusters in imbalanced graphs, while reducing the time complexity to O(N) w.r.t. the number of nodes. Subsequently, we present a novel IsoSEL framework for deep graph clustering, where we design a hyperbolic neural network to learn the partitioning tree in the Lorentz model of hyperbolic space, and further conduct Lorentz Tree Contrastive Learning with isometric augmentation. As a result, the partitioning tree incorporates node attributes via mutual information maximization, while the cluster assignment is refined by the proposed tree contrastive learning. Extensive experiments on five benchmark datasets show the IsoSEL outperforms 14 recent baselines by an average of +1.3% in NMI.\n        △ Less",
        "authors":[
            "Li Sun",
            "Zhenhao Huang",
            "Yujie Wang",
            "Hongbo Lv",
            "Chunyang Liu",
            "Hao Peng",
            "Philip S. Yu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Multi-task Learning Capability of Medical Generalist Foundation Model via Image-centric Multi-annotation Data",
        "abstract":"The emergence of medical generalist foundation models has revolutionized conventional task-specific model development paradigms, aiming to better handle multiple tasks through joint training on large-scale medical datasets. However, recent advances prioritize simple data scaling or architectural component enhancement, while neglecting to re-examine multi-task learning from a data-centric perspective. Critically, simply aggregating existing data resources leads to decentralized image-task alignment, which fails to cultivate comprehensive image understanding or align with clinical needs for multi-dimensional image interpretation. In this paper, we introduce the image-centric multi-annotation X-ray dataset (IMAX), the first attempt to enhance the multi-task learning capabilities of medical multi-modal large language models (MLLMs) from the data construction level. To be specific, IMAX is featured from the following attributes: 1) High-quality data curation. A comprehensive collection of more than 354K entries applicable to seven different medical tasks. 2) Image-centric dense annotation. Each X-ray image is associated with an average of 4.10 tasks and 7.46 training entries, ensuring multi-task representation richness per image. Compared to the general decentralized multi-annotation X-ray dataset (DMAX), IMAX consistently demonstrates significant multi-task average performance gains ranging from 3.20% to 21.05% across seven open-source state-of-the-art medical MLLMs. Moreover, we investigate differences in statistical patterns exhibited by IMAX and DMAX training processes, exploring potential correlations between optimization dynamics and multi-task performance. Finally, leveraging the core concept of IMAX data construction, we propose an optimized DMAX-based training strategy to alleviate the dilemma of obtaining high-quality IMAX data in practical scenarios.\n        △ Less",
        "authors":[
            "Xun Zhu",
            "Fanbin Mo",
            "Zheng Zhang",
            "Jiaxi Wang",
            "Yiming Shi",
            "Ming Wu",
            "Chuang Zhang",
            "Miao Li",
            "Ji Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Unbiased Federated Graph Learning: Label and Topology Perspectives",
        "abstract":"Federated Graph Learning (FGL) enables privacy-preserving, distributed training of graph neural networks without sharing raw data. Among its approaches, subgraph-FL has become the dominant paradigm, with most work focused on improving overall node classification accuracy. However, these methods often overlook fairness due to the complexity of node features, labels, and graph structures. In particular, they perform poorly on nodes with disadvantaged properties, such as being in the minority class within subgraphs or having heterophilous connections (neighbors with dissimilar labels or misleading features). This reveals a critical issue: high accuracy can mask degraded performance on structurally or semantically marginalized nodes. To address this, we advocate for two fairness goals: (1) improving representation of minority class nodes for class-wise fairness and (2) mitigating topological bias from heterophilous connections for topology-aware fairness. We propose FairFGL, a novel framework that enhances fairness through fine-grained graph mining and collaborative learning. On the client side, the History-Preserving Module prevents overfitting to dominant local classes, while the Majority Alignment Module refines representations of heterophilous majority-class nodes. The Gradient Modification Module transfers minority-class knowledge from structurally favorable clients to improve fairness. On the server side, FairFGL uploads only the most influenced subset of parameters to reduce communication costs and better reflect local distributions. A cluster-based aggregation strategy reconciles conflicting updates and curbs global majority dominance . Extensive evaluations on eight benchmarks show FairFGL significantly improves minority-group performance , achieving up to a 22.62 percent Macro-F1 gain while enhancing convergence over state-of-the-art baselines.\n        △ Less",
        "authors":[
            "Zhengyu Wu",
            "Boyang Pang",
            "Xunkai Li",
            "Yinlin Zhu",
            "Daohan Su",
            "Bowen Fan",
            "Rong-Hua Li",
            "Guoren Wang",
            "Chenghu Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Semantic Depth Matters: Explaining Errors of Deep Vision Networks through Perceived Class Similarities",
        "abstract":"Understanding deep neural network (DNN) behavior requires more than evaluating classification accuracy alone; analyzing errors and their predictability is equally crucial. Current evaluation methodologies lack transparency, particularly in explaining the underlying causes of network misclassifications. To address this, we introduce a novel framework that investigates the relationship between the semantic hierarchy depth perceived by a network and its real-data misclassification patterns. Central to our framework is the Similarity Depth (SD) metric, which quantifies the semantic hierarchy depth perceived by a network along with a method of evaluation of how closely the network's errors align with its internally perceived similarity structure. We also propose a graph-based visualization of model semantic relationships and misperceptions. A key advantage of our approach is that leveraging class templates -- representations derived from classifier layer weights -- is applicable to already trained networks without requiring additional data or experiments. Our approach reveals that deep vision networks encode specific semantic hierarchies and that high semantic depth improves the compliance between perceived class similarities and actual errors.\n        △ Less",
        "authors":[
            "Katarzyna Filus",
            "Michał Romaszewski",
            "Mateusz Żarski"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Weaker Variance Assumptions for Stochastic Optimization",
        "abstract":"We revisit a classical assumption for analyzing stochastic gradient algorithms where the squared norm of the stochastic subgradient (or the variance for smooth problems) is allowed to grow as fast as the squared norm of the optimization variable. We contextualize this assumption in view of its inception in the 1960s, its seemingly independent appearance in the recent literature, its relationship to weakest-known variance assumptions for analyzing stochastic gradient algorithms, and its relevance in deterministic problems for non-Lipschitz nonsmooth convex optimization. We build on and extend a connection recently made between this assumption and the Halpern iteration. For convex nonsmooth, and potentially stochastic, optimization, we analyze horizon-free, anytime algorithms with last-iterate rates. For problems beyond simple constrained optimization, such as convex problems with functional constraints or regularized convex-concave min-max problems, we obtain rates for optimality measures that do not require boundedness of the feasible set.\n        △ Less",
        "authors":[
            "Ahmet Alacaoglu",
            "Yura Malitsky",
            "Stephen J. Wright"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FedRecon: Missing Modality Reconstruction in Distributed Heterogeneous Environments",
        "abstract":"Multimodal data are often incomplete and exhibit Non-Independent and Identically Distributed (Non-IID) characteristics in real-world scenarios. These inherent limitations lead to both modality heterogeneity through partial modality absence and data heterogeneity from distribution divergence, creating fundamental challenges for effective federated learning (FL). To address these coupled challenges, we propose FedRecon, the first method targeting simultaneous missing modality reconstruction and Non-IID adaptation in multimodal FL. Our approach first employs a lightweight Multimodal Variational Autoencoder (MVAE) to reconstruct missing modalities while preserving cross-modal consistency. Distinct from conventional imputation methods, we achieve sample-level alignment through a novel distribution mapping mechanism that guarantees both data consistency and completeness. Additionally, we introduce a strategy employing global generator freezing to prevent catastrophic forgetting, which in turn mitigates Non-IID fluctuations. Extensive evaluations on multimodal datasets demonstrate FedRecon's superior performance in modality reconstruction under Non-IID conditions, surpassing state-of-the-art methods.\n        △ Less",
        "authors":[
            "Junming Liu",
            "Guosun Zeng",
            "Ding Wang",
            "Yanting Gao",
            "Yufei Jin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TianQuan-Climate: A Subseasonal-to-Seasonal Global Weather Model via Incorporate Climatology State",
        "abstract":"Subseasonal forecasting serves as an important support for Sustainable Development Goals (SDGs), such as climate challenges, agricultural yield and sustainable energy production. However, subseasonal forecasting is a complex task in meteorology due to dissipating initial conditions and delayed external forces. Although AI models are increasingly pushing the boundaries of this forecasting limit, they face two major challenges: error accumulation and Smoothness. To address these two challenges, we propose Climate Furnace Subseasonal-to-Seasonal (TianQuan-Climate), a novel machine learning model designed to provide global daily mean forecasts up to 45 days, covering five upper-air atmospheric variables at 13 pressure levels and two surface variables. Our proposed TianQuan-Climate has two advantages: 1) it utilizes a multi-model prediction strategy to reduce system error impacts in long-term subseasonal forecasts; 2) it incorporates a Content Fusion Module for climatological integration and extends ViT with uncertainty blocks (UD-ViT) to improve generalization by learning from uncertainty. We demonstrate the effectiveness of TianQuan-Climate on benchmarks for weather forecasting and climate projections within the 15 to 45-day range, where TianQuan-Climate outperforms existing numerical and AI methods.\n        △ Less",
        "authors":[
            "Guowen Li",
            "Xintong Liu",
            "Shilei Cao",
            "Haoyuan Liang",
            "Mengxuan Chen",
            "Lixian Zhang",
            "Jinxiao Zhang",
            "Jiuke Wang",
            "Meng Jin",
            "Juepeng Zheng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference",
        "abstract":"Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.\n        △ Less",
        "authors":[
            "Yuxuan Tian",
            "Zihan Wang",
            "Yebo Peng",
            "Aomufei Yuan",
            "Zhiming Wang",
            "Bairen Yi",
            "Xin Liu",
            "Yong Cui",
            "Tong Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-objective Bayesian Optimization With Mixed-categorical Design Variables for Expensive-to-evaluate Aeronautical Applications",
        "abstract":"This work aims at developing new methodologies to optimize computational costly complex systems (e.g., aeronautical engineering systems). The proposed surrogate-based method (often called Bayesian optimization) uses adaptive sampling to promote a trade-off between exploration and exploitation. Our in-house implementation, called SEGOMOE, handles a high number of design variables (continuous, discrete or categorical) and nonlinearities by combining mixtures of experts for the objective and\/or the constraints. Additionally, the method handles multi-objective optimization settings, as it allows the construction of accurate Pareto fronts with a minimal number of function evaluations. Different infill criteria have been implemented to handle multiple objectives with or without constraints. The effectiveness of the proposed method was tested on practical aeronautical applications within the context of the European Project AGILE 4.0 and demonstrated favorable results. A first example concerns a retrofitting problem where a comparison between two optimizers have been made. A second example introduces hierarchical variables to deal with architecture system in order to design an aircraft family. The third example increases drastically the number of categorical variables as it combines aircraft design, supply chain and manufacturing process. In this article, we show, on three different realistic problems, various aspects of our optimization codes thanks to the diversity of the treated aircraft problems.\n        △ Less",
        "authors":[
            "Nathalie Bartoli",
            "Thierry Lefebvre",
            "Rémi Lafage",
            "Paul Saves",
            "Youssef Diouane",
            "Joseph Morlier",
            "Jasper Bussemaker",
            "Giuseppa Donelli",
            "Joao Marcos Gomes de Mello",
            "Massimo Mandorino",
            "Pierluigi Della Vecchia"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Moderate Actor-Critic Methods: Controlling Overestimation Bias via Expectile Loss",
        "abstract":"Overestimation is a fundamental characteristic of model-free reinforcement learning (MF-RL), arising from the principles of temporal difference learning and the approximation of the Q-function. To address this challenge, we propose a novel moderate target in the Q-function update, formulated as a convex optimization of an overestimated Q-function and its lower bound. Our primary contribution lies in the efficient estimation of this lower bound through the lower expectile of the Q-value distribution conditioned on a state. Notably, our moderate target integrates seamlessly into state-of-the-art (SOTA) MF-RL algorithms, including Deep Deterministic Policy Gradient (DDPG) and Soft Actor Critic (SAC). Experimental results validate the effectiveness of our moderate target in mitigating overestimation bias in DDPG, SAC, and distributional RL algorithms.\n        △ Less",
        "authors":[
            "Ukjo Hwang",
            "Songnam Hong"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data",
        "abstract":"Large language models~(LLMs) are expected to be helpful, harmless, and honest. In various alignment scenarios, such as general human preference, safety, and confidence alignment, binary preference data collection and reward modeling are resource-intensive but necessary for human preference transferring. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function for LLM alignment. Using similarity as a reward circumvents training reward models, and collecting a single reference answer potentially costs less time than constructing binary preference pairs when multiple candidates are available. Specifically, we develop \\textit{RefAlign}, a versatile REINFORCE-style alignment algorithm, which is free of reference and reward models. Instead, RefAlign utilizes BERTScore between sampled generations and high-quality reference answers as the surrogate reward. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, {RefAlign} demonstrates comparable performance to previous alignment methods while offering high efficiency.\n        △ Less",
        "authors":[
            "Shuai Zhao",
            "Linchao Zhu",
            "Yi Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Truncated Matrix Completion - An Empirical Study",
        "abstract":"Low-rank Matrix Completion (LRMC) describes the problem where we wish to recover missing entries of partially observed low-rank matrix. Most existing matrix completion work deals with sampling procedures that are independent of the underlying data values. While this assumption allows the derivation of nice theoretical guarantees, it seldom holds in real-world applications. In this paper, we consider various settings where the sampling mask is dependent on the underlying data values, motivated by applications in sensing, sequential decision-making, and recommender systems. Through a series of experiments, we study and compare the performance of various LRMC algorithms that were originally successful for data-independent sampling patterns.\n        △ Less",
        "authors":[
            "Rishhabh Naik",
            "Nisarg Trivedi",
            "Davoud Ataee Tarzanagh",
            "Laura Balzano"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures",
        "abstract":"Irregular embedding lookups are a critical bottleneck in recommender models, sparse large language models, and graph learning models. In this paper, we first demonstrate that, by offloading these lookups to specialized access units, Decoupled Access-Execute (DAE) processors achieve 2.6$\\times$ higher performance and 6.4$\\times$ higher performance\/watt than GPUs on end-to-end models. Then, we propose the Ember compiler for automatically generating optimized DAE code from PyTorch and TensorFlow. Conversely from other DAE compilers, Ember features multiple intermediate representations specifically designed for different optimization levels. In this way, Ember can implement all optimizations to match the performance of hand-written code, unlocking the full potential of DAE architectures at scale.\n        △ Less",
        "authors":[
            "Marco Siracusa",
            "Olivia Hsu",
            "Victor Soria-Pardos",
            "Joshua Randall",
            "Arnaud Grasset",
            "Eric Biscondi",
            "Doug Joseph",
            "Randy Allen",
            "Fredrik Kjolstad",
            "Miquel Moretó Planas",
            "Adrià Armejach"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"NeRF-Based Transparent Object Grasping Enhanced by Shape Priors",
        "abstract":"Transparent object grasping remains a persistent challenge in robotics, largely due to the difficulty of acquiring precise 3D information. Conventional optical 3D sensors struggle to capture transparent objects, and machine learning methods are often hindered by their reliance on high-quality datasets. Leveraging NeRF's capability for continuous spatial opacity modeling, our proposed architecture integrates a NeRF-based approach for reconstructing the 3D information of transparent objects. Despite this, certain portions of the reconstructed 3D information may remain incomplete. To address these deficiencies, we introduce a shape-prior-driven completion mechanism, further refined by a geometric pose estimation method we have developed. This allows us to obtain a complete and reliable 3D information of transparent objects. Utilizing this refined data, we perform scene-level grasp prediction and deploy the results in real-world robotic systems. Experimental validation demonstrates the efficacy of our architecture, showcasing its capability to reliably capture 3D information of various transparent objects in cluttered scenes, and correspondingly, achieve high-quality, stables, and executable grasp predictions.\n        △ Less",
        "authors":[
            "Yi Han",
            "Zixin Lin",
            "Dongjie Li",
            "Lvping Chen",
            "Yongliang Shi",
            "Gan Ma"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence",
        "abstract":"Millimeter-wave radar provides a privacy-preserving solution for human motion analysis, yet its sparse point clouds pose significant challenges for semantic understanding. We present Radar-LLM, the first framework that leverages large language models (LLMs) for human motion understanding using millimeter-wave radar as the sensing modality. Our approach introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture that incorporates deformable body templates and masked trajectory modeling to encode spatiotemporal point clouds into compact semantic tokens, and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To address data scarcity, we introduce a physics-aware synthesis pipeline that generates realistic radar-text pairs from motion-text datasets. Extensive experiments demonstrate that Radar-LLM achieves state-of-the-art performance across both synthetic and real-world benchmarks, enabling accurate translation of millimeter-wave signals to natural language descriptions. This breakthrough facilitates comprehensive motion understanding in privacy-sensitive applications like healthcare and smart homes. We will release the full implementation to support further research on https:\/\/inowlzy.github.io\/RadarLLM\/.\n        △ Less",
        "authors":[
            "Zengyuan Lai",
            "Jiarui Yang",
            "Songpengcheng Xia",
            "Lizhou Lin",
            "Lan Sun",
            "Renwen Wang",
            "Jianran Liu",
            "Qi Wu",
            "Ling Pei"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accelerating Differentially Private Federated Learning via Adaptive Extrapolation",
        "abstract":"The federated learning (FL) framework enables multiple clients to collaboratively train machine learning models without sharing their raw data, but it remains vulnerable to privacy attacks. One promising approach is to incorporate differential privacy (DP)-a formal notion of privacy-into the FL framework. DP-FedAvg is one of the most popular algorithms for DP-FL, but it is known to suffer from the slow convergence in the presence of heterogeneity among clients' data. Most of the existing methods to accelerate DP-FL require 1) additional hyperparameters or 2) additional computational cost for clients, which is not desirable since 1) hyperparameter tuning is computationally expensive and data-dependent choice of hyperparameters raises the risk of privacy leakage, and 2) clients are often resource-constrained. To address this issue, we propose DP-FedEXP, which adaptively selects the global step size based on the diversity of the local updates without requiring any additional hyperparameters or client computational cost. We show that DP-FedEXP provably accelerates the convergence of DP-FedAvg and it empirically outperforms existing methods tailored for DP-FL.\n        △ Less",
        "authors":[
            "Shokichi Takakura",
            "Seng Pei Liew",
            "Satoshi Hasegawa"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GlyTwin: Digital Twin for Glucose Control in Type 1 Diabetes Through Optimal Behavioral Modifications Using Patient-Centric Counterfactuals",
        "abstract":"Frequent and long-term exposure to hyperglycemia (i.e., high blood glucose) increases the risk of chronic complications such as neuropathy, nephropathy, and cardiovascular disease. Current technologies like continuous subcutaneous insulin infusion (CSII) and continuous glucose monitoring (CGM) primarily model specific aspects of glycemic control-like hypoglycemia prediction or insulin delivery. Similarly, most digital twin approaches in diabetes management simulate only physiological processes. These systems lack the ability to offer alternative treatment scenarios that support proactive behavioral interventions. To address this, we propose GlyTwin, a novel digital twin framework that uses counterfactual explanations to simulate optimal treatments for glucose regulation. Our approach helps patients and caregivers modify behaviors like carbohydrate intake and insulin dosing to avoid abnormal glucose events. GlyTwin generates behavioral treatment suggestions that proactively prevent hyperglycemia by recommending small adjustments to daily choices, reducing both frequency and duration of these events. Additionally, it incorporates stakeholder preferences into the intervention design, making recommendations patient-centric and tailored. We evaluate GlyTwin on AZT1D, a newly constructed dataset with longitudinal data from 21 type 1 diabetes (T1D) patients on automated insulin delivery systems over 26 days. Results show GlyTwin outperforms state-of-the-art counterfactual methods, generating 76.6% valid and 86% effective interventions. These findings demonstrate the promise of counterfactual-driven digital twins in delivering personalized healthcare.\n        △ Less",
        "authors":[
            "Asiful Arefeen",
            "Saman Khamesian",
            "Maria Adela Grando",
            "Bithika Thompson",
            "Hassan Ghasemzadeh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SafeSpeech: Robust and Universal Voice Protection Against Malicious Speech Synthesis",
        "abstract":"Speech synthesis technology has brought great convenience, while the widespread usage of realistic deepfake audio has triggered hazards. Malicious adversaries may unauthorizedly collect victims' speeches and clone a similar voice for illegal exploitation (\\textit{e.g.}, telecom fraud). However, the existing defense methods cannot effectively prevent deepfake exploitation and are vulnerable to robust training techniques. Therefore, a more effective and robust data protection method is urgently needed. In response, we propose a defensive framework, \\textit{\\textbf{SafeSpeech}}, which protects the users' audio before uploading by embedding imperceptible perturbations on original speeches to prevent high-quality synthetic speech. In SafeSpeech, we devise a robust and universal proactive protection technique, \\textbf{S}peech \\textbf{PE}rturbative \\textbf{C}oncealment (\\textbf{SPEC}), that leverages a surrogate model to generate universally applicable perturbation for generative synthetic models. Moreover, we optimize the human perception of embedded perturbation in terms of time and frequency domains. To evaluate our method comprehensively, we conduct extensive experiments across advanced models and datasets, both subjectively and objectively. Our experimental results demonstrate that SafeSpeech achieves state-of-the-art (SOTA) voice protection effectiveness and transferability and is highly robust against advanced adaptive adversaries. Moreover, SafeSpeech has real-time capability in real-world tests. The source code is available at \\href{https:\/\/github.com\/wxzyd123\/SafeSpeech}{https:\/\/github.com\/wxzyd123\/SafeSpeech}.\n        △ Less",
        "authors":[
            "Zhisheng Zhang",
            "Derui Wang",
            "Qianyi Yang",
            "Pengyang Huang",
            "Junhan Pu",
            "Yuxin Cao",
            "Kai Ye",
            "Jie Hao",
            "Yixian Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Score Matching Diffusion Based Feedback Control and Planning of Nonlinear Systems",
        "abstract":"We propose a novel control-theoretic framework that leverages principles from generative modeling -- specifically, Denoising Diffusion Probabilistic Models (DDPMs) -- to stabilize control-affine systems with nonholonomic constraints. Unlike traditional stochastic approaches, which rely on noise-driven dynamics in both forward and reverse processes, our method crucially eliminates the need for noise in the reverse phase, making it particularly relevant for control applications. We introduce two formulations: one where noise perturbs all state dimensions during the forward phase while the control system enforces time reversal deterministically, and another where noise is restricted to the control channels, embedding system constraints directly into the forward process.\n  For controllable nonlinear drift-free systems, we prove that deterministic feedback laws can exactly reverse the forward process, ensuring that the system's probability density evolves correctly without requiring artificial diffusion in the reverse phase. Furthermore, for linear time-invariant systems, we establish a time-reversal result under the second formulation. By eliminating noise in the backward process, our approach provides a more practical alternative to machine learning-based denoising methods, which are unsuitable for control applications due to the presence of stochasticity. We validate our results through numerical simulations on benchmark systems, including a unicycle model in a domain with obstacles, a driftless five-dimensional system, and a four-dimensional linear system, demonstrating the potential for applying diffusion-inspired techniques in linear, nonlinear, and settings with state space constraints.\n        △ Less",
        "authors":[
            "Karthik Elamvazhuthi",
            "Darshan Gadginmath",
            "Fabio Pasqualetti"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand",
        "abstract":"In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering quantities, inventory levels, covariates, and censored sales levels, to estimate the optimal pricing and inventory control policy that maximizes long-term profit. While the underlying dynamic without censoring can be modeled by Markov decision process (MDP), the primary obstacle arises from the observed process where demand censoring is present, resulting in missing profit information, the failure of the Markov property, and a non-stationary optimal policy. To overcome these challenges, we first approximate the optimal policy by solving a high-order MDP characterized by the number of consecutive censoring instances, which ultimately boils down to solving a specialized Bellman equation tailored for this problem. Inspired by offline reinforcement learning and survival analysis, we propose two novel data-driven algorithms to solving these Bellman equations and, thus, estimate the optimal policy. Furthermore, we establish finite sample regret bounds to validate the effectiveness of these algorithms. Finally, we conduct numerical experiments to demonstrate the efficacy of our algorithms in estimating the optimal policy. To the best of our knowledge, this is the first data-driven approach to learning optimal pricing and inventory control policies in a sequential decision-making environment characterized by censored and dependent demand. The implementations of the proposed algorithms are available at https:\/\/github.com\/gundemkorel\/Inventory_Pricing_Control\n        △ Less",
        "authors":[
            "Korel Gundem",
            "Zhengling Qi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FATE: A Prompt-Tuning-Based Semi-Supervised Learning Framework for Extremely Limited Labeled Data",
        "abstract":"Semi-supervised learning (SSL) has achieved significant progress by leveraging both labeled data and unlabeled data. Existing SSL methods overlook a common real-world scenario when labeled data is extremely scarce, potentially as limited as a single labeled sample in the dataset. General SSL approaches struggle to train effectively from scratch under such constraints, while methods utilizing pre-trained models often fail to find an optimal balance between leveraging limited labeled data and abundant unlabeled data. To address this challenge, we propose Firstly Adapt, Then catEgorize (FATE), a novel SSL framework tailored for scenarios with extremely limited labeled data. At its core, the two-stage prompt tuning paradigm FATE exploits unlabeled data to compensate for scarce supervision signals, then transfers to downstream tasks. Concretely, FATE first adapts a pre-trained model to the feature distribution of downstream data using volumes of unlabeled samples in an unsupervised manner. It then applies an SSL method specifically designed for pre-trained models to complete the final classification task. FATE is designed to be compatible with both vision and vision-language pre-trained models. Extensive experiments demonstrate that FATE effectively mitigates challenges arising from the scarcity of labeled samples in SSL, achieving an average performance improvement of 33.74% across seven benchmarks compared to state-of-the-art SSL methods. Code is available at https:\/\/anonymous.4open.science\/r\/Semi-supervised-learning-BA72.\n        △ Less",
        "authors":[
            "Hezhao Liu",
            "Yang Lu",
            "Mengke Li",
            "Yiqun Zhang",
            "Shreyank N Gowda",
            "Chen Gong",
            "Hanzi Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Multi-Task Modeling through Automated Fusion of Trained Models",
        "abstract":"Although multi-task learning is widely applied in intelligent services, traditional multi-task modeling methods often require customized designs based on specific task combinations, resulting in a cumbersome modeling process. Inspired by the rapid development and excellent performance of single-task models, this paper proposes an efficient multi-task modeling method that can automatically fuse trained single-task models with different structures and tasks to form a multi-task model. As a general framework, this method allows modelers to simply prepare trained models for the required tasks, simplifying the modeling process while fully utilizing the knowledge contained in the trained models. This eliminates the need for excessive focus on task relationships and model structure design. To achieve this goal, we consider the structural differences among various trained models and employ model decomposition techniques to hierarchically decompose them into multiple operable model components. Furthermore, we have designed an Adaptive Knowledge Fusion (AKF) module based on Transformer, which adaptively integrates intra-task and inter-task knowledge based on model components. Through the proposed method, we achieve efficient and automated construction of multi-task models, and its effectiveness is verified through extensive experiments on three datasets.\n        △ Less",
        "authors":[
            "Jingxuan Zhou",
            "Weidong Bao",
            "Ji Wang",
            "Zhengyi Zhong",
            "Dayu Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Virtual domain extension for imposing boundary conditions in flow simulation using pre-trained local neural operator",
        "abstract":"This paper builds up a virtual domain extension (VDE) framework for imposing boundary conditions (BCs) in flow simulation using pre-trained local neural operator (LNO). It creates extended virtual domains to the input function to compensate for the corrosion nature of computational domains during LNO inference, thus turns the implementation of BC into the determination of field values on the extended domain. Several strategies to calculate the field values are proposed and validated in solving numerical examples, including padding operation, direct imposition, pressure symmetry, and optimization by backpropagation, and compared with boundary imposition in traditional solvers. It is found that the large time interval of LNO induces a relatively wide near-boundary domain to be processed, thus imposing BC on only a few nodes near the boundary following the immersed boundary conception in traditional solvers can hardly achieve high accuracy. With appropriate values assigned on the extended virtual domains, VDE can accurately impose BCs and lead to reasonable flow field predictions. This work provides a guidance for imposing BCs reliably in LNO prediction, which could facilitate the reuse of pre-trained LNO in more applications.\n        △ Less",
        "authors":[
            "Ximeng Ye",
            "Hongyu Li",
            "Zhen-Guo Yan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BO-SA-PINNs: Self-adaptive physics-informed neural networks based on Bayesian optimization for automatically designing PDE solvers",
        "abstract":"Physics-informed neural networks (PINNs) is becoming a popular alternative method for solving partial differential equations (PDEs). However, they require dedicated manual modifications to the hyperparameters of the network, the sampling methods and loss function weights for different PDEs, which reduces the efficiency of the solvers. In this paper, we pro- pose a general multi-stage framework, i.e. BO-SA-PINNs to alleviate this issue. In the first stage, Bayesian optimization (BO) is used to select hyperparameters for the training process, and based on the results of the pre-training, the network architecture, learning rate, sampling points distribution and loss function weights suitable for the PDEs are automatically determined. The proposed hyperparameters search space based on experimental results can enhance the efficiency of BO in identifying optimal hyperparameters. After selecting the appropriate hyperparameters, we incorporate a global self-adaptive (SA) mechanism the second stage. Using the pre-trained model and loss information in the second-stage training, the exponential moving average (EMA) method is employed to optimize the loss function weights, and residual-based adaptive refinement with distribution (RAR-D) is used to optimize the sampling points distribution. In the third stage, L-BFGS is used for stable training. In addition, we introduce a new activation function that enables BO-SA-PINNs to achieve higher accuracy. In numerical experiments, we conduct comparative and ablation experiments to verify the performance of the model on Helmholtz, Maxwell, Burgers and high-dimensional Poisson equations. The comparative experiment results show that our model can achieve higher accuracy and fewer iterations in test cases, and the ablation experiments demonstrate the positive impact of every improvement.\n        △ Less",
        "authors":[
            "Rui Zhang",
            "Liang Li",
            "Stéphane Lanteri",
            "Hao Kang",
            "Jiaqi Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CUT: Pruning Pre-Trained Multi-Task Models into Compact Models for Edge Devices",
        "abstract":"Multi-task learning has garnered widespread attention in the industry due to its efficient data utilization and strong generalization capabilities, making it particularly suitable for providing high-quality intelligent services to users. Edge devices, as the primary platforms directly serving users, play a crucial role in delivering multi-task services. However, current multi-task models are often large, and user task demands are increasingly diverse. Deploying such models directly on edge devices not only increases the burden on these devices but also leads to task redundancy. To address this issue, this paper innovatively proposes a pre-trained multi-task model pruning method specifically designed for edge computing. The goal is to utilize existing pre-trained multi-task models to construct a compact multi-task model that meets the needs of edge devices. The specific implementation steps are as follows: First, decompose the tasks within the pre-trained multi-task model and select tasks based on actual user needs. Next, while retaining the knowledge of the original pre-trained model, evaluate parameter importance and use a parameter fusion method to effectively integrate shared parameters among tasks. Finally, obtain a compact multi-task model suitable for edge devices. To validate the effectiveness of the proposed method, we conducted experiments on three public image datasets. The experimental results fully demonstrate the superiority and efficiency of this method, providing a new solution for multi-task learning on edge devices.\n        △ Less",
        "authors":[
            "Jingxuan Zhou",
            "Weidong Bao",
            "Ji Wang",
            "Zhengyi Zhong"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-task Federated Learning with Encoder-Decoder Structure: Enabling Collaborative Learning Across Different Tasks",
        "abstract":"Federated learning has been extensively studied and applied due to its ability to ensure data security in distributed environments while building better models. However, clients participating in federated learning still face limitations, as clients with different structures or tasks cannot participate in learning together. In view of this, constructing a federated learning framework that allows collaboration between clients with different model structures and performing different tasks, enabling them to share valuable knowledge to enhance model efficiency, holds significant practical implications for the widespread application of federated learning. To achieve this goal, we propose a multi-task federated learning with encoder-decoder structure (M-Fed). Specifically, given the widespread adoption of the encoder-decoder architecture in current models, we leverage this structure to share intra-task knowledge through traditional federated learning methods and extract general knowledge from the encoder to achieve cross-task knowledge sharing. The training process is similar to traditional federated learning, and we incorporate local decoder and global decoder information into the loss function. The local decoder iteratively updates and gradually approaches the global decoder until sufficient cross-task knowledge sharing is achieved. Our method is lightweight and modular, demonstrating innovation compared to previous research. It enables clients performing different tasks to share general knowledge while maintaining the efficiency of traditional federated learning systems. We conducted experiments on two widely used benchmark datasets to verify the feasibility of M-Fed and compared it with traditional methods. The experimental results demonstrate the effectiveness of M-Fed in multi-task federated learning.\n        △ Less",
        "authors":[
            "Jingxuan Zhou",
            "Weidong Bao",
            "Ji Wang",
            "Dayu Zhang",
            "Xiongtao Zhang",
            "Yaohong Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Advancing RFI-Detection in Radio Astronomy with Liquid State Machines",
        "abstract":"Radio Frequency Interference (RFI) from anthropogenic radio sources poses significant challenges to current and future radio telescopes. Contemporary approaches to detecting RFI treat the task as a semantic segmentation problem on radio telescope spectrograms. Typically, complex heuristic algorithms handle this task of `flagging' in combination with manual labeling (in the most difficult cases). While recent machine-learning approaches have demonstrated high accuracy, they often fail to meet the stringent operational requirements of modern radio observatories. Owing to their inherently time-varying nature, spiking neural networks (SNNs) are a promising alternative method to RFI-detection by utilizing the time-varying nature of the spectrographic source data. In this work, we apply Liquid State Machines (LSMs), a class of spiking neural networks, to RFI-detection. We employ second-order Leaky Integrate-and-Fire (LiF) neurons, marking the first use of this architecture and neuron type for RFI-detection. We test three encoding methods and three increasingly complex readout layers, including a transformer decoder head, providing a hybrid of SNN and ANN techniques. Our methods extend LSMs beyond conventional classification tasks to fine-grained spatio-temporal segmentation. We train LSMs on simulated data derived from the Hyrogen Epoch of Reionization Array (HERA), a known benchmark for RFI-detection. Our model achieves a per-pixel accuracy of 98% and an F1-score of 0.743, demonstrating competitive performance on this highly challenging task. This work expands the sophistication of SNN techniques and architectures applied to RFI-detection, and highlights the effectiveness of LSMs in handling fine-grained, complex, spatio-temporal signal-processing tasks.\n        △ Less",
        "authors":[
            "Nicholas J Pritchard",
            "Andreas Wicenec",
            "Mohammed Bennamoun",
            "Richard Dodson"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Tale of Two Learning Algorithms: Multiple Stream Random Walk and Asynchronous Gossip",
        "abstract":"Although gossip and random walk-based learning algorithms are widely known for decentralized learning, there has been limited theoretical and experimental analysis to understand their relative performance for different graph topologies and data heterogeneity. We first design and analyze a random walk-based learning algorithm with multiple streams (walks), which we name asynchronous \"Multi-Walk (MW)\". We provide a convergence analysis for MW w.r.t iteration (computation), wall-clock time, and communication. We also present a convergence analysis for \"Asynchronous Gossip\", noting the lack of a comprehensive analysis of its convergence, along with the computation and communication overhead, in the literature. Our results show that MW has better convergence in terms of iterations as compared to Asynchronous Gossip in graphs with large diameters (e.g., cycles), while its relative performance, as compared to Asynchronous Gossip, depends on the number of walks and the data heterogeneity in graphs with small diameters (e.g., complete graphs). In wall-clock time analysis, we observe a linear speed-up with the number of walks and nodes in MW and Asynchronous Gossip, respectively. Finally, we show that MW outperforms Asynchronous Gossip in communication overhead, except in small-diameter topologies with extreme data heterogeneity. These results highlight the effectiveness of each algorithm in different graph topologies and data heterogeneity. Our codes are available for reproducibility.\n        △ Less",
        "authors":[
            "Peyman Gholami",
            "Hulya Seferoglu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reasoning without Regret",
        "abstract":"Chain-of-thought reasoning enables large language models to solve multi-step tasks by framing problem solving as sequential decision problems. Outcome-based rewards, which provide feedback only on final answers, show impressive success, but face challenges with credit assignment and slow convergence. In contrast, procedure-based rewards offer efficient step-level feedback, but typically require costly human supervision. We introduce \\emph{Backwards Adaptive Reward Shaping} (BARS), a no-regret framework that converts sparse outcomes-based rewards into effective procedure-based signals. BARS uses sparse rewards generated from terminal-state priors and cover trees to scale rewards while preventing exploitation. With Bellman contraction and $(Δ, ε)$-gap rewards, our backward Euler solver achieves $ε$-accuracy in $O\\left((R_{\\max}\/Δ)\\log(1\/ε)\\right)$ iterations with $O(\\log T)$ dynamic regret over $T$ rounds. Our analysis, based on generic chaining, continuous scaling limits, and non-linear Feynman-Kac bounds, connects recent outcome-based methods' empirical successes with the benefits of intermediate supervision. Combined, this provides the first rigorous no-regret algorithm for outcome reward shaping, providing a theoretical foundation for the empirical success of DeepSeek's R1.\n        △ Less",
        "authors":[
            "Tarun Chitra"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Investigation of Large Language Models and Their Vulnerabilities in Spam Detection",
        "abstract":"Spam messages continue to present significant challenges to digital users, cluttering inboxes and posing security risks. Traditional spam detection methods, including rules-based, collaborative, and machine learning approaches, struggle to keep up with the rapidly evolving tactics employed by spammers. This project studies new spam detection systems that leverage Large Language Models (LLMs) fine-tuned with spam datasets. More importantly, we want to understand how LLM-based spam detection systems perform under adversarial attacks that purposefully modify spam emails and data poisoning attacks that exploit the differences between the training data and the massages in detection, to which traditional machine learning models are shown to be vulnerable. This experimentation employs two LLM models of GPT2 and BERT and three spam datasets of Enron, LingSpam, and SMSspamCollection for extensive training and testing tasks. The results show that, while they can function as effective spam filters, the LLM models are susceptible to the adversarial and data poisoning attacks. This research provides very useful insights for future applications of LLM models for information security.\n        △ Less",
        "authors":[
            "Qiyao Tang",
            "Xiangyang Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Understanding and Optimizing Multi-Stage AI Inference Pipelines",
        "abstract":"The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads.\n        △ Less",
        "authors":[
            "Abhimanyu Rajeshkumar Bambhaniya",
            "Hanjiang Wu",
            "Suvinay Subramanian",
            "Sudarshan Srinivasan",
            "Souvik Kundu",
            "Amir Yazdanbakhsh",
            "Midhilesh Elavazhagan",
            "Madhu Kumar",
            "Tushar Krishna"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the representation of stack operators by mathematical morphology",
        "abstract":"This paper introduces the class of grey-scale image stack operators as those that (a) map binary-images into binary-images and (b) commute in average with cross-sectioning. We show that stack operators are 1-Lipchitz extensions of set operators which can be represented by applying a characteristic set operator to the cross-sections of the image and summing. In particular, they are a generalisation of stack filters, for which the characteristic set operators are increasing. Our main result is that stack operators inherit lattice properties of the characteristic set operators. We focus on the case of translation-invariant and locally defined stack operators and show the main result by deducing the characteristic function, kernel, and basis representation of stack operators. The results of this paper have implications on the design of image operators, since imply that to solve some grey-scale image processing problems it is enough to design an operator for performing the desired transformation on binary images, and then considering its extension given by a stack operator. We leave many topics for future research regarding the machine learning of stack operators and the characterisation of the image processing problems that can be solved by them.\n        △ Less",
        "authors":[
            "Diego Marcondes"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems",
        "abstract":"Scientists often infer abstract procedures from specific instances of problems and use the abstractions to generate new, related instances. For example, programs encoding the formal rules and properties of a system have been useful in fields ranging from RL (procedural environments) to physics (simulation engines). These programs can be seen as functions which execute to different outputs based on their parameterizations (e.g., gridworld configuration or initial physical conditions). We introduce the term EFA (Executable Functional Abstraction) to denote such programs for math problems. EFA-like constructs have been shown to be useful for math reasoning as problem generators for stress-testing models. However, prior work has been limited to abstractions for grade-school math (whose simple rules are easy to encode in programs), while generating EFAs for advanced math has thus far required human engineering. We explore the automatic construction of EFAs for advanced math problems. We operationalize the task of automatically constructing EFAs as a program synthesis task, and develop EFAGen, which conditions an LLM on a seed math problem and its step-by-step solution to generate candidate EFA programs that are faithful to the generalized problem and solution class underlying the seed problem. Furthermore, we formalize properties any valid EFA must possess in terms of executable unit tests, and show how the tests can be used as verifiable rewards to train LLMs to become better writers of EFAs. We demonstrate that EFAs constructed by EFAGen behave rationally by remaining faithful to seed problems, produce learnable problem variations, and that EFAGen can infer EFAs across multiple diverse sources of competition-level math problems. Finally, we show downstream uses of model-written EFAs e.g. finding problem variations that are harder or easier for a learner to solve, as well as data generation.\n        △ Less",
        "authors":[
            "Zaid Khan",
            "Elias Stengel-Eskin",
            "Archiki Prasad",
            "Jaemin Cho",
            "Mohit Bansal"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamical symmetries in the fluctuation-driven regime: an application of Noether's theorem to noisy dynamical systems",
        "abstract":"Noether's theorem provides a powerful link between continuous symmetries and conserved quantities for systems governed by some variational principle. Perhaps unfortunately, most dynamical systems of interest in neuroscience and artificial intelligence cannot be described by any such principle. On the other hand, nonequilibrium physics provides a variational principle that describes how fairly generic noisy dynamical systems are most likely to transition between two states; in this work, we exploit this principle to apply Noether's theorem, and hence learn about how the continuous symmetries of dynamical systems constrain their most likely trajectories. We identify analogues of the conservation of energy, momentum, and angular momentum, and briefly discuss examples of each in the context of models of decision-making, recurrent neural networks, and diffusion generative models.\n        △ Less",
        "authors":[
            "John J. Vastola"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Classifier Evaluation: A Fairer Benchmarking Strategy Based on Ability and Robustness",
        "abstract":"Benchmarking is a fundamental practice in machine learning (ML) for comparing the performance of classification algorithms. However, traditional evaluation methods often overlook a critical aspect: the joint consideration of dataset complexity and an algorithm's ability to generalize. Without this dual perspective, assessments may favor models that perform well on easy instances while failing to capture their true robustness. To address this limitation, this study introduces a novel evaluation methodology that combines Item Response Theory (IRT) with the Glicko-2 rating system, originally developed to measure player strength in competitive games. IRT assesses classifier ability based on performance over difficult instances, while Glicko-2 updates performance metrics - such as rating, deviation, and volatility - via simulated tournaments between classifiers. This combined approach provides a fairer and more nuanced measure of algorithm capability. A case study using the OpenML-CC18 benchmark showed that only 15% of the datasets are truly challenging and that a reduced subset with 50% of the original datasets offers comparable evaluation power. Among the algorithms tested, Random Forest achieved the highest ability score. The results highlight the importance of improving benchmark design by focusing on dataset quality and adopting evaluation strategies that reflect both difficulty and classifier proficiency.\n        △ Less",
        "authors":[
            "Lucas Cardoso",
            "Vitor Santos",
            "José Ribeiro",
            "Regiane Kawasaki",
            "Ricardo Prudêncio",
            "Ronnie Alves"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Integrating Large Language Models for Automated Structural Analysis",
        "abstract":"Automated analysis for engineering structures offers considerable potential for boosting efficiency by minimizing repetitive tasks. Although AI-driven methods are increasingly common, no systematic framework yet leverages Large Language Models (LLMs) for automatic structural analysis. To address this gap, we propose a novel framework that integrates LLMs with structural analysis software. LLMs serve as the core engine: they parse structural descriptions from text and translate them into executable Python scripts. Moreover, the framework integrates the generative capabilities of LLMs with code-based finite element (FE) tools like OpenSeesPy. It employs domain-specific prompt design and in-context learning strategies to enhance the LLM's problem-solving capabilities and generative stability, enabling fully automated structural analysis from descriptive text to model outputs. In our experiments, we introduce a well-curated small-scale benchmark dataset of 20 structural analysis word problems (SAWPs) with ground-truth solutions and evaluate the performance of different LLMs within our framework in solving these SAWPs. The role of system instructions, crafted by structural engineers, is also investigated to understand their impact on LLM-driven structural analysis. Additionally, the generative stability of our framework is examined. Through multiple validation experiments on the benchmark, our results demonstrate that the proposed framework can substantially increase the level of automation in solving SAWPs compared to traditional methods. Quantitatively, the framework, built on GPT-4o, achieved 100% accuracy, surpassing GPT-4 (85%), Gemini 1.5 Pro (80%), and Llama-3.3 (30%) on the test examples. Furthermore, integrating domain-specific instructions enhanced performance by 30% on problems with asymmetrical structural configurations.\n        △ Less",
        "authors":[
            "Haoran Liang",
            "Mohammad Talebi Kalaleh",
            "Qipei Mei"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Stochastic generative methods for stable and accurate closure modeling of chaotic dynamical systems",
        "abstract":"Traditional deterministic subgrid-scale (SGS) models are often dissipative and unstable, especially in regions of chaotic and turbulent flow. Ongoing work in climate science and ocean modeling motivates the use of stochastic SGS models for chaotic dynamics. Further, developing stochastic generative models of underlying dynamics is a rapidly expanding field. In this work, we aim to incorporate stochastic integration toward closure modeling for chaotic dynamical systems. Further, we want to explore the potential stabilizing effect that stochastic models could have on linearized chaotic systems. We propose parametric and generative approaches for closure modeling using stochastic differential equations (SDEs). We derive and implement a quadratic diffusion model based on the fluctuations, demonstrating increased accuracy from bridging theoretical models with generative approaches. Results are demonstrated on the Lorenz-63 dynamical system.\n        △ Less",
        "authors":[
            "Emily Williams",
            "David Darmofal"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Automatic Detection of Intro and Credits in Video using CLIP and Multihead Attention",
        "abstract":"Detecting transitions between intro\/credits and main content in videos is a crucial task for content segmentation, indexing, and recommendation systems. Manual annotation of such transitions is labor-intensive and error-prone, while heuristic-based methods often fail to generalize across diverse video styles. In this work, we introduce a deep learning-based approach that formulates the problem as a sequence-to-sequence classification task, where each second of a video is labeled as either \"intro\" or \"film.\" Our method extracts frames at a fixed rate of 1 FPS, encodes them using CLIP (Contrastive Language-Image Pretraining), and processes the resulting feature representations with a multihead attention model incorporating learned positional encoding. The system achieves an F1-score of 91.0%, Precision of 89.0%, and Recall of 97.0% on the test set, and is optimized for real-time inference, achieving 11.5 FPS on CPU and 107 FPS on high-end GPUs. This approach has practical applications in automated content indexing, highlight detection, and video summarization. Future work will explore multimodal learning, incorporating audio features and subtitles to further enhance detection accuracy.\n        △ Less",
        "authors":[
            "Vasilii Korolkov",
            "Andrey Yanchenko"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025",
        "abstract":"Peer review at AI conferences is stressed by rapidly rising submission volumes, leading to deteriorating review quality and increased author dissatisfaction. To address these issues, we developed Review Feedback Agent, a system leveraging multiple large language models (LLMs) to improve review clarity and actionability by providing automated feedback on vague comments, content misunderstandings, and unprofessional remarks to reviewers. Implemented at ICLR 2025 as a large randomized control study, our system provided optional feedback to more than 20,000 randomly selected reviews. To ensure high-quality feedback for reviewers at this scale, we also developed a suite of automated reliability tests powered by LLMs that acted as guardrails to ensure feedback quality, with feedback only being sent to reviewers if it passed all the tests. The results show that 27% of reviewers who received feedback updated their reviews, and over 12,000 feedback suggestions from the agent were incorporated by those reviewers. This suggests that many reviewers found the AI-generated feedback sufficiently helpful to merit updating their reviews. Incorporating AI feedback led to significantly longer reviews (an average increase of 80 words among those who updated after receiving feedback) and more informative reviews, as evaluated by blinded researchers. Moreover, reviewers who were selected to receive AI feedback were also more engaged during paper rebuttals, as seen in longer author-reviewer discussions. This work demonstrates that carefully designed LLM-generated review feedback can enhance peer review quality by making reviews more specific and actionable while increasing engagement between reviewers and authors. The Review Feedback Agent is publicly available at https:\/\/github.com\/zou-group\/review_feedback_agent.\n        △ Less",
        "authors":[
            "Nitya Thakkar",
            "Mert Yuksekgonul",
            "Jake Silberg",
            "Animesh Garg",
            "Nanyun Peng",
            "Fei Sha",
            "Rose Yu",
            "Carl Vondrick",
            "James Zou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE) of 2D Black Box Classifier Functions",
        "abstract":"Accurately estimating decision boundaries in black box systems is critical when ensuring safety, quality, and feasibility in real-world applications. However, existing methods iteratively refine boundary estimates by sampling in regions of uncertainty, without providing guarantees on the closeness to the decision boundary and also result in unnecessary exploration that is especially disadvantageous when evaluations are costly. This paper presents the Epsilon-Neighborhood Decision-Boundary Governed Estimation (EDGE), a sample efficient and function-agnostic algorithm that leverages the intermediate value theorem to estimate the location of the decision boundary of a black box binary classifier within a user-specified epsilon-neighborhood. Evaluations are conducted on three nonlinear test functions and a case study of an electric grid stability problem with uncertain renewable power injection. The EDGE algorithm demonstrates superior sample efficiency and better boundary approximation than adaptive sampling techniques and grid-based searches.\n        △ Less",
        "authors":[
            "Mithun Goutham",
            "Riccardo DalferroNucci",
            "Stephanie Stockar",
            "Meghna Menon",
            "Sneha Nayak",
            "Harshad Zade",
            "Chetan Patel",
            "Mario Santillo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training",
        "abstract":"Recent advances in reinforcement learning (RL)-based post-training have led to notable improvements in large language models (LLMs), particularly in enhancing their reasoning capabilities to handle complex tasks. However, most existing methods treat the training data as a unified whole, overlooking the fact that modern LLM training often involves a mixture of data from diverse distributions-varying in both source and difficulty. This heterogeneity introduces a key challenge: how to adaptively schedule training across distributions to optimize learning efficiency. In this paper, we present a principled curriculum learning framework grounded in the notion of distribution-level learnability. Our core insight is that the magnitude of policy advantages reflects how much a model can still benefit from further training on a given distribution. Based on this, we propose a distribution-level curriculum learning framework for RL-based LLM post-training, which leverages the Upper Confidence Bound (UCB) principle to dynamically adjust sampling probabilities for different distrubutions. This approach prioritizes distributions with either high average advantage (exploitation) or low sample count (exploration), yielding an adaptive and theoretically grounded training schedule. We instantiate our curriculum learning framework with GRPO as the underlying RL algorithm and demonstrate its effectiveness on logic reasoning datasets with multiple difficulties and sources. Our experiments show that our framework significantly improves convergence speed and final performance, highlighting the value of distribution-aware curriculum strategies in LLM post-training. Code: https:\/\/github.com\/ZhentingWang\/DUMP.\n        △ Less",
        "authors":[
            "Zhenting Wang",
            "Guofeng Cui",
            "Kun Wan",
            "Wentian Zhao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Preconditioned Gradient Descent for Over-Parameterized Nonconvex Matrix Factorization",
        "abstract":"In practical instances of nonconvex matrix factorization, the rank of the true solution $r^{\\star}$ is often unknown, so the rank $r$ of the model can be overspecified as $r>r^{\\star}$. This over-parameterized regime of matrix factorization significantly slows down the convergence of local search algorithms, from a linear rate with $r=r^{\\star}$ to a sublinear rate when $r>r^{\\star}$. We propose an inexpensive preconditioner for the matrix sensing variant of nonconvex matrix factorization that restores the convergence rate of gradient descent back to linear, even in the over-parameterized case, while also making it agnostic to possible ill-conditioning in the ground truth. Classical gradient descent in a neighborhood of the solution slows down due to the need for the model matrix factor to become singular. Our key result is that this singularity can be corrected by $\\ell_{2}$ regularization with a specific range of values for the damping parameter. In fact, a good damping parameter can be inexpensively estimated from the current iterate. The resulting algorithm, which we call preconditioned gradient descent or PrecGD, is stable under noise, and converges linearly to an information theoretically optimal error bound. Our numerical experiments find that PrecGD works equally well in restoring the linear convergence of other variants of nonconvex matrix factorization in the over-parameterized regime.\n        △ Less",
        "authors":[
            "Gavin Zhang",
            "Salar Fattahi",
            "Richard Y. Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InfoMAE: Pair-Efficient Cross-Modal Alignment for Multimodal Time-Series Sensing Signals",
        "abstract":"Standard multimodal self-supervised learning (SSL) algorithms regard cross-modal synchronization as implicit supervisory labels during pretraining, thus posing high requirements on the scale and quality of multimodal samples. These constraints significantly limit the performance of sensing intelligence in IoT applications, as the heterogeneity and the non-interpretability of time-series signals result in abundant unimodal data but scarce high-quality multimodal pairs. This paper proposes InfoMAE, a cross-modal alignment framework that tackles the challenge of multimodal pair efficiency under the SSL setting by facilitating efficient cross-modal alignment of pretrained unimodal representations. InfoMAE achieves \\textit{efficient cross-modal alignment} with \\textit{limited data pairs} through a novel information theory-inspired formulation that simultaneously addresses distribution-level and instance-level alignment. Extensive experiments on two real-world IoT applications are performed to evaluate InfoMAE's pairing efficiency to bridge pretrained unimodal models into a cohesive joint multimodal model. InfoMAE enhances downstream multimodal tasks by over 60% with significantly improved multimodal pairing efficiency. It also improves unimodal task accuracy by an average of 22%.\n        △ Less",
        "authors":[
            "Tomoyoshi Kimura",
            "Xinlin Li",
            "Osama Hanna",
            "Yatong Chen",
            "Yizhuo Chen",
            "Denizhan Kara",
            "Tianshi Wang",
            "Jinyang Li",
            "Xiaomin Ouyang",
            "Shengzhong Liu",
            "Mani Srivastava",
            "Suhas Diggavi",
            "Tarek Abdelzaher"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transformer-Based Representation Learning for Robust Gene Expression Modeling and Cancer Prognosis",
        "abstract":"Transformer-based models have achieved remarkable success in natural language and vision tasks, but their application to gene expression analysis remains limited due to data sparsity, high dimensionality, and missing values. We present GexBERT, a transformer-based autoencoder framework for robust representation learning of gene expression data. GexBERT learns context-aware gene embeddings by pretraining on large-scale transcriptomic profiles with a masking and restoration objective that captures co-expression relationships among thousands of genes. We evaluate GexBERT across three critical tasks in cancer research: pan-cancer classification, cancer-specific survival prediction, and missing value imputation. GexBERT achieves state-of-the-art classification accuracy from limited gene subsets, improves survival prediction by restoring expression of prognostic anchor genes, and outperforms conventional imputation methods under high missingness. Furthermore, its attention-based interpretability reveals biologically meaningful gene patterns across cancer types. These findings demonstrate the utility of GexBERT as a scalable and effective tool for gene expression modeling, with translational potential in settings where gene coverage is limited or incomplete.\n        △ Less",
        "authors":[
            "Shuai Jiang",
            "Saeed Hassanpour"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MLRC-Bench: Can Language Agents Solve Machine Learning Research Challenges?",
        "abstract":"Existing evaluation of large language model (LLM) agents on scientific discovery lacks objective baselines and metrics to assess the viability of their proposed methods. To address this issue, we introduce MLRC-Bench, a benchmark designed to quantify how effectively language agents can tackle challenging Machine Learning (ML) Research Competitions. Our benchmark highlights open research problems that demand novel methodologies, in contrast to recent benchmarks such as OpenAI's MLE-Bench (Chan et al., 2024) and METR's RE-Bench (Wijk et al., 2024), which focus on well-established research tasks that are largely solvable through sufficient engineering effort. Unlike prior work, e.g., AI Scientist (Lu et al., 2024b), which evaluates the end-to-end agentic pipeline by using LLM-as-a-judge, MLRC-Bench measures the key steps of proposing and implementing novel research methods and evaluates them with newly proposed rigorous protocol and objective metrics. Our curated suite of 7 competition tasks reveals significant challenges for LLM agents. Even the best-performing tested agent (gemini-exp-1206 under MLAB (Huang et al., 2024a)) closes only 9.3% of the gap between baseline and top human participant scores. Furthermore, our analysis reveals a misalignment between the LLM-judged innovation and their actual performance on cutting-edge ML research problems. MLRC-Bench is a dynamic benchmark, which is designed to continually grow with new ML competitions to encourage rigorous and objective evaluations of AI's research capabilities.\n        △ Less",
        "authors":[
            "Yunxiang Zhang",
            "Muhammad Khalifa",
            "Shitanshu Bhushan",
            "Grant D Murphy",
            "Lajanugen Logeswaran",
            "Jaekyeom Kim",
            "Moontae Lee",
            "Honglak Lee",
            "Lu Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow",
        "abstract":"Recent prompt-based image editing models have demonstrated impressive prompt-following capability at structural editing tasks. However, existing models still fail to perform local edits, follow detailed editing prompts, or maintain global image quality beyond a single editing step. To address these challenges, we introduce SPICE, a training-free workflow that accepts arbitrary resolutions and aspect ratios, accurately follows user requirements, and improves image quality consistently during more than 100 editing steps. By synergizing the strengths of a base diffusion model and a Canny edge ControlNet model, SPICE robustly handles free-form editing instructions from the user. SPICE outperforms state-of-the-art baselines on a challenging realistic image-editing dataset consisting of semantic editing (object addition, removal, replacement, and background change), stylistic editing (texture changes), and structural editing (action change) tasks. Not only does SPICE achieve the highest quantitative performance according to standard evaluation metrics, but it is also consistently preferred by users over existing image-editing methods. We release the workflow implementation for popular diffusion model Web UIs to support further research and artistic exploration.\n        △ Less",
        "authors":[
            "Kenan Tang",
            "Yanhong Li",
            "Yao Qin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Computer-Aided Layout Generation for Building Design: A Review",
        "abstract":"Generating realistic building layouts for automatic building design has been studied in both the computer vision and architecture domains. Traditional approaches from the architecture domain, which are based on optimization techniques or heuristic design guidelines, can synthesize desirable layouts, but usually require post-processing and involve human interaction in the design pipeline, making them costly and timeconsuming. The advent of deep generative models has significantly improved the fidelity and diversity of the generated architecture layouts, reducing the workload by designers and making the process much more efficient. In this paper, we conduct a comprehensive review of three major research topics of architecture layout design and generation: floorplan layout generation, scene layout synthesis, and generation of some other formats of building layouts. For each topic, we present an overview of the leading paradigms, categorized either by research domains (architecture or machine learning) or by user input conditions or constraints. We then introduce the commonly-adopted benchmark datasets that are used to verify the effectiveness of the methods, as well as the corresponding evaluation metrics. Finally, we identify the well-solved problems and limitations of existing approaches, then propose new perspectives as promising directions for future research in this important research area. A project associated with this survey to maintain the resources is available at awesome-building-layout-generation.\n        △ Less",
        "authors":[
            "Jiachen Liu",
            "Yuan Xue",
            "Haomiao Ni",
            "Rui Yu",
            "Zihan Zhou",
            "Sharon X. Huang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety",
        "abstract":"The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https:\/\/github.com\/1akaman\/EmoAgent\n        △ Less",
        "authors":[
            "Jiahao Qiu",
            "Yinghui He",
            "Xinzhe Juan",
            "Yiming Wang",
            "Yuhan Liu",
            "Zixin Yao",
            "Yue Wu",
            "Xun Jiang",
            "Ling Yang",
            "Mengdi Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GAMA: High-Performance GEMM Acceleration on AMD Versal ML-Optimized AI Engines",
        "abstract":"General matrix-matrix multiplication (GEMM) is a fundamental operation in machine learning (ML) applications. We present the first comprehensive performance acceleration of GEMM workloads on AMD's second-generation AIE-ML (AIE2) architecture, which is specifically optimized for ML applications. Compared to AI-Engine (AIE1), AIE offers increased compute throughput and larger on-chip memory capacity. We propose a novel design that maximizes AIE memory utilization, incorporates custom buffer placement within the AIE2 and staggered kernel placement across the AIE2 array, significantly reducing performance bottlenecks such as memory stalls and routing congestion, resulting in improved performance and efficiency compared to the default compiler provided by AMD. We evaluate the performance benefits of our design at three levels: single AIE, pack of AIEs and the complete AIE array. GAMA achieves state-of-the-art performance, delivering up to 165 TOPS (85% of peak) for int8 precision and 83 TBFLOPS (86% of peak) for bfloat16 precision GEMM workloads. Our solution achieves 8.7%, 9%, 39% and 53.6% higher peak throughput efficiency compared to the state-of-the-art AIE1 frameworks AMA, MAXEVA, ARIES and CHARM, respectively.\n        △ Less",
        "authors":[
            "Kaustubh Mhatre",
            "Endri Taka",
            "Aman Arora"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Domain-Adaptive Continued Pre-Training of Small Language Models",
        "abstract":"Continued pre-training of small language models offers a promising path for domain adaptation with limited computational resources. I've investigated this approach within educational domains, evaluating it as a resource-efficient alternative to training models from scratch. Using a 125M parameter model, I demonstrate significant performance improvements through incremental training on 400 million tokens, followed by further training to reach 1 billion tokens. My approach includes comprehensive data preprocessing, memory-optimized training configurations, and benchmark-based evaluation. Results show notable gains in knowledge-intensive tasks (MMLU +8.1%) and contextual understanding (HellaSwag +7.6%), while revealing educational domain specialization trade-offs. I analyze token efficiency, catastrophic forgetting mitigation strategies, and scaling patterns. My findings suggest that thoughtful preprocessing and training methodologies enable meaningful improvements in language model capabilities even with constrained computational resources, opening pathways for domain-specific adaptation of smaller language models.\n        △ Less",
        "authors":[
            "Salman Faroz"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Can LLMs Revolutionize the Design of Explainable and Efficient TinyML Models?",
        "abstract":"This paper introduces a novel framework for designing efficient neural network architectures specifically tailored to tiny machine learning (TinyML) platforms. By leveraging large language models (LLMs) for neural architecture search (NAS), a vision transformer (ViT)-based knowledge distillation (KD) strategy, and an explainability module, the approach strikes an optimal balance between accuracy, computational efficiency, and memory usage. The LLM-guided search explores a hierarchical search space, refining candidate architectures through Pareto optimization based on accuracy, multiply-accumulate operations (MACs), and memory metrics. The best-performing architectures are further fine-tuned using logits-based KD with a pre-trained ViT-B\/16 model, which enhances generalization without increasing model size. Evaluated on the CIFAR-100 dataset and deployed on an STM32H7 microcontroller (MCU), the three proposed models, LMaNet-Elite, LMaNet-Core, and QwNet-Core, achieve accuracy scores of 74.50%, 74.20% and 73.00%, respectively. All three models surpass current state-of-the-art (SOTA) models, such as MCUNet-in3\/in4 (69.62% \/ 72.86%) and XiNet (72.27%), while maintaining a low computational cost of less than 100 million MACs and adhering to the stringent 320 KB static random-access memory (SRAM) constraint. These results demonstrate the efficiency and performance of the proposed framework for TinyML platforms, underscoring the potential of combining LLM-driven search, Pareto optimization, KD, and explainability to develop accurate, efficient, and interpretable models. This approach opens new possibilities in NAS, enabling the design of efficient architectures specifically suited for TinyML.\n        △ Less",
        "authors":[
            "Christophe El Zeinaty",
            "Wassim Hamidouche",
            "Glenn Herrou",
            "Daniel Menard",
            "Merouane Debbah"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SPOT: Spatio-Temporal Pattern Mining and Optimization for Load Consolidation in Freight Transportation Networks",
        "abstract":"Freight consolidation has significant potential to reduce transportation costs and mitigate congestion and pollution. An effective load consolidation plan relies on carefully chosen consolidation points to ensure alignment with existing transportation management processes, such as driver scheduling, personnel planning, and terminal operations. This complexity represents a significant challenge when searching for optimal consolidation strategies. Traditional optimization-based methods provide exact solutions, but their computational complexity makes them impractical for large-scale instances and they fail to leverage historical data. Machine learning-based approaches address these issues but often ignore operational constraints, leading to infeasible consolidation plans.\n  This work proposes SPOT, an end-to-end approach that integrates the benefits of machine learning (ML) and optimization for load consolidation. The ML component plays a key role in the planning phase by identifying the consolidation points through spatio-temporal clustering and constrained frequent itemset mining, while the optimization selects the most cost-effective feasible consolidation routes for a given operational day. Extensive experiments conducted on industrial load data demonstrate that SPOT significantly reduces travel distance and transportation costs (by about 50% on large terminals) compared to the existing industry-standard load planning strategy and a neighborhood-based heuristic. Moreover, the ML component provides valuable tactical-level insights by identifying frequently recurring consolidation opportunities that guide proactive planning. In addition, SPOT is computationally efficient and can be easily scaled to accommodate large transportation networks.\n        △ Less",
        "authors":[
            "Sikai Cheng",
            "Amira Hijazi",
            "Jeren Konak",
            "Alan Erera",
            "Pascal Van Hentenryck"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adapting to the Unknown: Robust Meta-Learning for Zero-Shot Financial Time Series Forecasting",
        "abstract":"Financial time series forecasting in the zero-shot setting is essential for risk management and investment decision-making, particularly during abrupt market regime shifts or in emerging markets with limited historical data. While Model-Agnostic Meta-Learning (MAML)-based approaches have shown promise in this domain, existing meta task construction strategies often lead to suboptimal performance, especially when dealing with highly turbulent financial time series. To address this challenge, we propose a novel task construction method that leverages learned embeddings for more effective meta-learning in the zero-shot setting. Specifically, we construct two complementary types of meta-tasks based on the learned embeddings: intra-cluster tasks and inter-cluster tasks. To capture diverse fine-grained patterns, we apply stochastic projection matrices to the learned embeddings and use clustering algorithm to form the tasks. Additionally, to improve generalization capabilities, we employ hard task mining strategies and leverage inter-cluster tasks to identify invariant patterns across different time series. Extensive experiments on the real world financial dataset demonstrate that our method significantly outperforms existing approaches, showing better generalization ability in the zero-shot scenario.\n        △ Less",
        "authors":[
            "Anxian Liu",
            "Junying Ma",
            "Guang Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ordinary Least Squares as an Attention Mechanism",
        "abstract":"I show that ordinary least squares (OLS) predictions can be rewritten as the output of a restricted attention module, akin to those forming the backbone of large language models. This connection offers an alternative perspective on attention beyond the conventional information retrieval framework, making it more accessible to researchers and analysts with a background in traditional statistics. It falls into place when OLS is framed as a similarity-based method in a transformed regressor space, distinct from the standard view based on partial correlations. In fact, the OLS solution can be recast as the outcome of an alternative problem: minimizing squared prediction errors by optimizing the embedding space in which training and test vectors are compared via inner products. Rather than estimating coefficients directly, we equivalently learn optimal encoding and decoding operations for predictors. From this vantage point, OLS maps naturally onto the query-key-value structure of attention mechanisms. Building on this foundation, I discuss key elements of Transformer-style attention and draw connections to classic ideas from time series econometrics.\n        △ Less",
        "authors":[
            "Philippe Goulet Coulombe"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GitBugs: Bug Reports for Duplicate Detection, Retrieval Augmented Generation, Triage, and More",
        "abstract":"Bug reports provide critical insights into software quality, yet existing datasets often suffer from limited scope, outdated content, or insufficient metadata for machine learning. To address these limitations, we present GitBugs-a comprehen- sive and up-to-date dataset comprising over 150,000 bug reports from nine actively maintained open-source projects, including Firefox, Cassandra, and VS Code. GitBugs aggregates data from Github, Bugzilla and Jira issue trackers, offering standardized categorical fields for classification tasks and predefined train\/test splits for duplicate bug detection. In addition, it includes ex- ploratory analysis notebooks and detailed project-level statistics, such as duplicate rates and resolution times. GitBugs supports various software engineering research tasks, including duplicate detection, retrieval augmented generation, resolution prediction, automated triaging, and temporal analysis. The openly licensed dataset provides a valuable cross-project resource for bench- marking and advancing automated bug report analysis. Access the data and code at https:\/\/github.com\/av9ash\/gitbugs\/.\n        △ Less",
        "authors":[
            "Avinash Patil"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions",
        "abstract":"In this paper, we study the problem of robust subspace recovery (RSR) in the presence of both strong adversarial corruptions and Gaussian noise. Specifically, given a limited number of noisy samples -- some of which are tampered by an adaptive and strong adversary -- we aim to recover a low-dimensional subspace that approximately contains a significant fraction of the uncorrupted samples, up to an error that scales with the Gaussian noise. Existing approaches to this problem often suffer from high computational costs or rely on restrictive distributional assumptions, limiting their applicability in truly adversarial settings. To address these challenges, we revisit the classical random sample consensus (RANSAC) algorithm, which offers strong robustness to adversarial outliers, but sacrifices efficiency and robustness against Gaussian noise and model misspecification in the process. We propose a two-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure modes of standard RANSAC. Our method is provably robust to both Gaussian and adversarial corruptions, achieves near-optimal sample complexity without requiring prior knowledge of the subspace dimension, and is more efficient than existing RANSAC-type methods.\n        △ Less",
        "authors":[
            "Guixian Chen",
            "Jianhao Ma",
            "Salar Fattahi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Evaluating Machine Learning-Driven Intrusion Detection Systems in IoT: Performance and Energy Consumption",
        "abstract":"In the evolving landscape of the Internet of Things (IoT), Machine Learning (ML)-based Intrusion Detection Systems (IDS) represent a significant advancement, especially when integrated with Software-Defined Networking (SDN). These systems play a critical role in enhancing security infrastructure within resource-constrained IoT systems. Despite their growing adoption, limited research has explored the impact of ML-based IDS on key performance metrics, such as CPU load, CPU usage, and energy consumption, particularly under real-time cyber threats. This study bridges that gap through an empirical evaluation of cutting-edge ML-based IDSs deployed at the edge of IoT networks under both benign and attack scenarios. Additionally, we investigate how SDN's centralized control and dynamic resource management influence IDS performance. Our experimental framework compares traditional ML-based IDS with deep learning (DL)-based counterparts, both with and without SDN integration. Results reveal that edge-deployed ML-based IDSs significantly impact system performance during cyber threats, with marked increases in resource consumption. SDN integration further influences these outcomes, emphasizing the need for optimized architectural design. Statistical analysis using ANOVA confirms the significance of our findings. This research provides critical insights into the performance and trade-offs of deploying ML-based IDSs in edge-based IoT systems.\n        △ Less",
        "authors":[
            "Saeid Jamshidi",
            "Kawser Wazed Nafi",
            "Amin Nikanjam",
            "Foutse Khomh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization",
        "abstract":"Layer-wise post-training quantization has emerged as a widely used technique for compressing large language models (LLMs) without retraining. However, recent progress in this line of research is saturating, underscoring the need to revisit its core limitation and explore further improvements. This study identifies a critical bottleneck in existing layer-wise PTQ methods: the accumulation of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this, we propose Quantization Error Propagation (QEP), a lightweight and general framework that enhances layer-wise PTQ by explicitly propagating the quantization error which enable compensating for accumulated quantization errors. Additionally, we introduce a tunable propagation mechanism that allows for control over both propagation strength and computational overhead, making the framework adaptable to various architectures and resource constraints. Empirical evaluation on LLaMA2 models (7B, 13B, 70B) demonstrate that incorporating QEP into standard layer-wise PTQ pipelines outperforms standard PTQ methods. Notably, QEP yields substantial performance improvements under extreme low-bit quantization settings.\n        △ Less",
        "authors":[
            "Yamato Arai",
            "Yuma Ichikawa"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mitigating Many-Shot Jailbreaking",
        "abstract":"Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the long context windows of modern LLMs to circumvent model safety training by including in the prompt many examples of a ``fake'' assistant responding inappropriately before the final request. With enough examples, the model's in-context learning abilities override its safety training, and it responds as if it were the ``fake'' assistant. In this work, we probe the effectiveness of different fine tuning and input sanitization approaches on mitigating MSJ attacks, alone and in combination. We find incremental mitigation effectiveness for each, and we show that the combined techniques significantly reduce the effectiveness of MSJ attacks, while retaining model performance in benign in-context learning and conversational tasks. We suggest that our approach could meaningfully ameliorate this vulnerability if incorporated into model safety post-training.\n        △ Less",
        "authors":[
            "Christopher M. Ackerman",
            "Nina Panickssery"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation",
        "abstract":"Single domain generalization (SDG) has recently attracted growing attention in medical image segmentation. One promising strategy for SDG is to leverage consistent semantic shape priors across different imaging protocols, scanner vendors, and clinical sites. However, existing dictionary learning methods that encode shape priors often suffer from limited representational power with a small set of offline computed shape elements, or overfitting when the dictionary size grows. Moreover, they are not readily compatible with large foundation models such as the Segment Anything Model (SAM). In this paper, we propose a novel Mixture-of-Shape-Experts (MoSE) framework that seamlessly integrates the idea of mixture-of-experts (MoE) training into dictionary learning to efficiently capture diverse and robust shape priors. Our method conceptualizes each dictionary atom as a shape expert, which specializes in encoding distinct semantic shape information. A gating network dynamically fuses these shape experts into a robust shape map, with sparse activation guided by SAM encoding to prevent overfitting. We further provide this shape map as a prompt to SAM, utilizing the powerful generalization capability of SAM through bidirectional integration. All modules, including the shape dictionary, are trained in an end-to-end manner. Extensive experiments on multiple public datasets demonstrate its effectiveness.\n        △ Less",
        "authors":[
            "Jia Wei",
            "Xiaoqi Zhao",
            "Jonghye Woo",
            "Jinsong Ouyang",
            "Georges El Fakhri",
            "Qingyu Chen",
            "Xiaofeng Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
        "abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet principled explanations for their underlying mechanisms and several phenomena, such as scaling laws, hallucinations, and related behaviors, remain elusive. In this work, we revisit the classical relationship between compression and prediction, grounded in Kolmogorov complexity and Shannon information theory, to provide deeper insights into LLM behaviors. By leveraging the Kolmogorov Structure Function and interpreting LLM compression as a two-part coding process, we offer a detailed view of how LLMs acquire and store information across increasing model and data scales -- from pervasive syntactic patterns to progressively rarer knowledge elements. Motivated by this theoretical perspective and natural assumptions inspired by Heap's and Zipf's laws, we introduce a simplified yet representative hierarchical data-generation framework called the Syntax-Knowledge model. Under the Bayesian setting, we show that prediction and compression within this model naturally lead to diverse learning and scaling behaviors of LLMs. In particular, our theoretical analysis offers intuitive and principled explanations for both data and model scaling laws, the dynamics of knowledge acquisition during training and fine-tuning, factual knowledge hallucinations in LLMs. The experimental results validate our theoretical predictions.\n        △ Less",
        "authors":[
            "Zhixuan Pan",
            "Shaowen Wang",
            "Jian Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ControlNET: A Firewall for RAG-based LLM System",
        "abstract":"Retrieval-Augmented Generation (RAG) has significantly enhanced the factual accuracy and domain adaptability of Large Language Models (LLMs). This advancement has enabled their widespread deployment across sensitive domains such as healthcare, finance, and enterprise applications. RAG mitigates hallucinations by integrating external knowledge, yet introduces privacy risk and security risk, notably data breaching risk and data poisoning risk. While recent studies have explored prompt injection and poisoning attacks, there remains a significant gap in comprehensive research on controlling inbound and outbound query flows to mitigate these threats. In this paper, we propose an AI firewall, ControlNET, designed to safeguard RAG-based LLM systems from these vulnerabilities. ControlNET controls query flows by leveraging activation shift phenomena to detect adversarial queries and mitigate their impact through semantic divergence. We conduct comprehensive experiments on four different benchmark datasets including Msmarco, HotpotQA, FinQA, and MedicalSys using state-of-the-art open source LLMs (Llama3, Vicuna, and Mistral). Our results demonstrate that ControlNET achieves over 0.909 AUROC in detecting and mitigating security threats while preserving system harmlessness. Overall, ControlNET offers an effective, robust, harmless defense mechanism, marking a significant advancement toward the secure deployment of RAG-based LLM systems.\n        △ Less",
        "authors":[
            "Hongwei Yao",
            "Haoran Shi",
            "Yidou Chen",
            "Yixin Jiang",
            "Cong Wang",
            "Zhan Qin",
            "Kui Ren",
            "Chun Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reduction of Supervision for Biomedical Knowledge Discovery",
        "abstract":"Knowledge discovery is hindered by the increasing volume of publications and the scarcity of extensive annotated data. To tackle the challenge of information overload, it is essential to employ automated methods for knowledge extraction and processing. Finding the right balance between the level of supervision and the effectiveness of models poses a significant challenge. While supervised techniques generally result in better performance, they have the major drawback of demanding labeled data. This requirement is labor-intensive and time-consuming and hinders scalability when exploring new domains. In this context, our study addresses the challenge of identifying semantic relationships between biomedical entities (e.g., diseases, proteins) in unstructured text while minimizing dependency on supervision. We introduce a suite of unsupervised algorithms based on dependency trees and attention mechanisms and employ a range of pointwise binary classification methods. Transitioning from weakly supervised to fully unsupervised settings, we assess the methods' ability to learn from data with noisy labels. The evaluation on biomedical benchmark datasets explores the effectiveness of the methods. Our approach tackles a central issue in knowledge discovery: balancing performance with minimal supervision. By gradually decreasing supervision, we assess the robustness of pointwise binary classification techniques in handling noisy labels, revealing their capability to shift from weakly supervised to entirely unsupervised scenarios. Comprehensive benchmarking offers insights into the effectiveness of these techniques, suggesting an encouraging direction toward adaptable knowledge discovery systems, representing progress in creating data-efficient methodologies for extracting useful insights when annotated data is limited.\n        △ Less",
        "authors":[
            "Christos Theodoropoulos",
            "Andrei Catalin Coman",
            "James Henderson",
            "Marie-Francine Moens"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline",
        "abstract":"When the complete source sentence is provided, Large Language Models (LLMs) perform excellently in offline machine translation even with a simple prompt \"Translate the following sentence from [src lang] into [tgt lang]:\". However, in many real scenarios, the source tokens arrive in a streaming manner and simultaneous machine translation (SiMT) is required, then the efficiency and performance of decoder-only LLMs are significantly limited by their auto-regressive nature. To enable LLMs to achieve high-quality SiMT as efficiently as offline translation, we propose a novel paradigm that includes constructing supervised fine-tuning (SFT) data for SiMT, along with new training and inference strategies. To replicate the token input\/output stream in SiMT, the source and target tokens are rearranged into an interleaved sequence, separated by special tokens according to varying latency requirements. This enables powerful LLMs to learn read and write operations adaptively, based on varying latency prompts, while still maintaining efficient auto-regressive decoding. Experimental results show that, even with limited SFT data, our approach achieves state-of-the-art performance across various SiMT benchmarks, and preserves the original abilities of offline translation. Moreover, our approach generalizes well to document-level SiMT setting without requiring specific fine-tuning, even beyond the offline translation model.\n        △ Less",
        "authors":[
            "Biao Fu",
            "Minpeng Liao",
            "Kai Fan",
            "Chengxi Li",
            "Liang Zhang",
            "Yidong Chen",
            "Xiaodong Shi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conditional Independence Test Based on Transport Maps",
        "abstract":"Testing conditional independence between two random vectors given a third is a fundamental and challenging problem in statistics, particularly in multivariate nonparametric settings due to the complexity of conditional structures. We propose a novel framework for testing conditional independence using transport maps. At the population level, we show that two well-defined transport maps can transform the conditional independence test into an unconditional independence test, this substantially simplifies the problem. These transport maps are estimated from data using conditional continuous normalizing flow models. Within this framework, we derive a test statistic and prove its consistency under both the null and alternative hypotheses. A permutation-based procedure is employed to evaluate the significance of the test. We validate the proposed method through extensive simulations and real-data analysis. Our numerical studies demonstrate the practical effectiveness of the proposed method for conditional independence testing.\n        △ Less",
        "authors":[
            "Chenxuan He",
            "Yuan Gao",
            "Liping Zhu",
            "Jian Huang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Causal integration of chemical structures improves representations of microscopy images for morphological profiling",
        "abstract":"Recent advances in self-supervised deep learning have improved our ability to quantify cellular morphological changes in high-throughput microscopy screens, a process known as morphological profiling. However, most current methods only learn from images, despite many screens being inherently multimodal, as they involve both a chemical or genetic perturbation as well as an image-based readout. We hypothesized that incorporating chemical compound structure during self-supervised pre-training could improve learned representations of images in high-throughput microscopy screens. We introduce a representation learning framework, MICON (Molecular-Image Contrastive Learning), that models chemical compounds as treatments that induce counterfactual transformations of cell phenotypes. MICON significantly outperforms classical hand-crafted features such as CellProfiler and existing deep-learning-based representation learning methods in challenging evaluation settings where models must identify reproducible effects of drugs across independent replicates and data-generating centers. We demonstrate that incorporating chemical compound information into the learning process provides consistent improvements in our evaluation setting and that modeling compounds specifically as treatments in a causal framework outperforms approaches that directly align images and compounds in a single representation space. Our findings point to a new direction for representation learning in morphological profiling, suggesting that methods should explicitly account for the multimodal nature of microscopy screening data.\n        △ Less",
        "authors":[
            "Yemin Yu",
            "Neil Tenenholtz",
            "Lester Mackey",
            "Ying Wei",
            "David Alvarez-Melis",
            "Ava P. Amini",
            "Alex X. Lu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Capturing Longitudinal Changes in Brain Morphology Using Temporally Parameterized Neural Displacement Fields",
        "abstract":"Longitudinal image registration enables studying temporal changes in brain morphology which is useful in applications where monitoring the growth or atrophy of specific structures is important. However this task is challenging due to; noise\/artifacts in the data and quantifying small anatomical changes between sequential scans. We propose a novel longitudinal registration method that models structural changes using temporally parameterized neural displacement fields. Specifically, we implement an implicit neural representation (INR) using a multi-layer perceptron that serves as a continuous coordinate-based approximation of the deformation field at any time point. In effect, for any N scans of a particular subject, our model takes as input a 3D spatial coordinate location x, y, z and a corresponding temporal representation t and learns to describe the continuous morphology of structures for both observed and unobserved points in time. Furthermore, we leverage the analytic derivatives of the INR to derive a new regularization function that enforces monotonic rate of change in the trajectory of the voxels, which is shown to provide more biologically plausible patterns. We demonstrate the effectiveness of our method on 4D brain MR registration.\n        △ Less",
        "authors":[
            "Aisha L. Shuaibu",
            "Kieran A. Gibb",
            "Peter A. Wijeratne",
            "Ivor J. A. Simpson"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimal sparse phase retrieval via a quasi-Bayesian approach",
        "abstract":"This paper addresses the problem of sparse phase retrieval, a fundamental inverse problem in applied mathematics, physics, and engineering, where a signal need to be reconstructed using only the magnitude of its transformation while phase information remains inaccessible. Leveraging the inherent sparsity of many real-world signals, we introduce a novel sparse quasi-Bayesian approach and provide the first theoretical guarantees for such an approach. Specifically, we employ a scaled Student distribution as a continuous shrinkage prior to enforce sparsity and analyze the method using the PAC-Bayesian inequality framework. Our results establish that the proposed Bayesian estimator achieves minimax-optimal convergence rates under sub-exponential noise, matching those of state-of-the-art frequentist methods. To ensure computational feasibility, we develop an efficient Langevin Monte Carlo sampling algorithm. Through numerical experiments, we demonstrate that our method performs comparably to existing frequentist techniques, highlighting its potential as a principled alternative for sparse phase retrieval in noisy settings.\n        △ Less",
        "authors":[
            "The Tien Mai"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PCM-SAR: Physics-Driven Contrastive Mutual Learning for SAR Classification",
        "abstract":"Existing SAR image classification methods based on Contrastive Learning often rely on sample generation strategies designed for optical images, failing to capture the distinct semantic and physical characteristics of SAR data. To address this, we propose Physics-Driven Contrastive Mutual Learning for SAR Classification (PCM-SAR), which incorporates domain-specific physical insights to improve sample generation and feature extraction. PCM-SAR utilizes the gray-level co-occurrence matrix (GLCM) to simulate realistic noise patterns and applies semantic detection for unsupervised local sampling, ensuring generated samples accurately reflect SAR imaging properties. Additionally, a multi-level feature fusion mechanism based on mutual learning enables collaborative refinement of feature representations. Notably, PCM-SAR significantly enhances smaller models by refining SAR feature representations, compensating for their limited capacity. Experimental results show that PCM-SAR consistently outperforms SOTA methods across diverse datasets and SAR classification tasks.\n        △ Less",
        "authors":[
            "Pengfei Wang",
            "Hao Zheng",
            "Zhigang Hu",
            "Aikun Xu",
            "Meiguang Zheng",
            "Liu Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Decoding the mechanisms of the Hattrick football manager game using Bayesian network structure learning for optimal decision-making",
        "abstract":"Hattrick is a free web-based probabilistic football manager game with over 200,000 users competing for titles at national and international levels. Launched in Sweden in 1997 as part of an MSc project, the game's slow-paced design has fostered a loyal community, with many users remaining active for decades. Hattrick's game-engine mechanics are partially hidden, and users have attempted to decode them with incremental success over the years. Rule-based, statistical and machine learning models have been developed to aid this effort and are widely used by the community. However, these models or tools have not been formally described or evaluated in the scientific literature. This study is the first to explore Hattrick using structure learning techniques and Bayesian networks, integrating both data and domain knowledge to develop models capable of explaining and simulating the game engine. We present a comprehensive analysis assessing the effectiveness of structure learning algorithms in relation to knowledge-based structures, and show that while structure learning may achieve a higher overall network fit, it does not result in more accurate predictions for selected variables of interest, when compared to knowledge-based networks that produce a lower overall network fit. Additionally, we introduce and publicly share a fully specified Bayesian network model that matches the performance of top models used by the Hattrick community. We further demonstrate how analysis extends beyond prediction by providing a visual representation of conditional dependencies, and using the best performing Bayesian network model for in-game decision-making. To support future research, we make all data, graphical structures, and models publicly available online.\n        △ Less",
        "authors":[
            "Anthony C. Constantinou",
            "Nicholas Higgins",
            "Neville K. Kitson"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Federated Prototype Graph Learning",
        "abstract":"In recent years, Federated Graph Learning (FGL) has gained significant attention for its distributed training capabilities in graph-based machine intelligence applications, mitigating data silos while offering a new perspective for privacy-preserve large-scale graph learning. However, multi-level FGL heterogeneity presents various client-server collaboration challenges: (1) Model-level: The variation in clients for expected performance and scalability necessitates the deployment of heterogeneous models. Unfortunately, most FGL methods rigidly demand identical client models due to the direct model weight aggregation on the server. (2) Data-level: The intricate nature of graphs, marked by the entanglement of node profiles and topology, poses an optimization dilemma. This implies that models obtained by federated training struggle to achieve superior performance. (3) Communication-level: Some FGL methods attempt to increase message sharing among clients or between clients and the server to improve training, which inevitably leads to high communication costs. In this paper, we propose FedPG as a general prototype-guided optimization method for the above multi-level FGL heterogeneity. Specifically, on the client side, we integrate multi-level topology-aware prototypes to capture local graph semantics. Subsequently, on the server side, leveraging the uploaded prototypes, we employ topology-guided contrastive learning and personalized technology to tailor global prototypes for each client, broadcasting them to improve local training. Experiments demonstrate that FedPG outperforms SOTA baselines by an average of 3.57\\% in accuracy while reducing communication costs by 168x.\n        △ Less",
        "authors":[
            "Zhengyu Wu",
            "Xunkai Li",
            "Yinlin Zhu",
            "Rong-Hua Li",
            "Guoren Wang",
            "Chenghu Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Cluster-Based Synthetic Minority Oversampling Technique for Traffic Mode Choice Prediction with Imbalanced Dataset",
        "abstract":"Urban datasets such as citizen transportation modes often contain disproportionately distributed classes, posing significant challenges to the classification of under-represented samples using data-driven models. In the literature, various resampling methods have been developed to create synthetic data for minority classes (oversampling) or remove samples from majority classes (undersampling) to alleviate class imbalance. However, oversampling approaches tend to overgeneralize minor classes that are closely clustered and neglect sparse regions which may contain crucial information. Conversely, undersampling methods potentially remove useful information on certain subgroups. Hence, a resampling approach that takes the inherent distribution of data into consideration is required to ensure appropriate synthetic data creation. This study proposes an adaptive cluster-based synthetic minority oversampling technique. Density-based spatial clustering is applied on minority classes to identify subgroups based on their input features. The classes in each of these subgroups are then oversampled according to the ratio of data points of their local cluster to the largest majority class. When used in conjunction with machine learning models such as random forest and extreme gradient boosting, this oversampling method results in significantly higher F1 scores for the minority classes compared to other resampling techniques. These improved models provide accurate classification of transportation modes.\n        △ Less",
        "authors":[
            "Guang An Ooi",
            "Shehab Ahmed"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GenEDA: Unleashing Generative Reasoning on Netlist via Multimodal Encoder-Decoder Aligned Foundation Model",
        "abstract":"The success of foundation AI has motivated the research of circuit foundation models, which are customized to assist the integrated circuit (IC) design process. However, existing pre-trained circuit models are typically limited to standalone encoders for predictive tasks or decoders for generative tasks. These two model types are developed independently, operate on different circuit modalities, and reside in separate latent spaces, which restricts their ability to complement each other for more advanced applications. In this work, we present GenEDA, the first framework that aligns circuit encoders with decoders within a shared latent space. GenEDA bridges the gap between graph-based circuit representations and text-based large language models (LLMs), enabling communication between their respective latent spaces. To achieve the alignment, we propose two paradigms that support both open-source trainable LLMs and commercial frozen LLMs. Built on this aligned architecture, GenEDA enables three unprecedented generative reasoning tasks over netlists, where the model reversely generates the high-level functionality from low-level netlists in different granularities. These tasks extend traditional gate-type prediction to direct generation of full-circuit functionality. Experiments demonstrate that GenEDA significantly boosts advanced LLMs' (e.g., GPT-4o and DeepSeek-V3) performance in all tasks.\n        △ Less",
        "authors":[
            "Wenji Fang",
            "Jing Wang",
            "Yao Lu",
            "Shang Liu",
            "Zhiyao Xie"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An overview of condensation phenomenon in deep learning",
        "abstract":"In this paper, we provide an overview of a common phenomenon, condensation, observed during the nonlinear training of neural networks: During the nonlinear training of neural networks, neurons in the same layer tend to condense into groups with similar outputs. Empirical observations suggest that the number of condensed clusters of neurons in the same layer typically increases monotonically as training progresses. Neural networks with small weight initializations or Dropout optimization can facilitate this condensation process. We also examine the underlying mechanisms of condensation from the perspectives of training dynamics and the structure of the loss landscape. The condensation phenomenon offers valuable insights into the generalization abilities of neural networks and correlates to stronger reasoning abilities in transformer-based language models.\n        △ Less",
        "authors":[
            "Zhi-Qin John Xu",
            "Yaoyu Zhang",
            "Zhangchen Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation",
        "abstract":"Drug-target binding affinity prediction is a fundamental task for drug discovery. It has been extensively explored in literature and promising results are reported. However, in this paper, we demonstrate that the results may be misleading and cannot be well generalized to real practice. The core observation is that the canonical randomized split of a test set in conventional evaluation leaves the test set dominated by samples with high similarity to the training set. The performance of models is severely degraded on samples with lower similarity to the training set but the drawback is highly overlooked in current evaluation. As a result, the performance can hardly be trusted when the model meets low-similarity samples in real practice. To address this problem, we propose a framework of similarity aware evaluation in which a novel split methodology is proposed to adapt to any desired distribution. This is achieved by a formulation of optimization problems which are approximately and efficiently solved by gradient descent. We perform extensive experiments across five representative methods in four datasets for two typical target evaluations and compare them with various counterpart methods. Results demonstrate that the proposed split methodology can significantly better fit desired distributions and guide the development of models. Code is released at https:\/\/github.com\/Amshoreline\/SAE\/tree\/main.\n        △ Less",
        "authors":[
            "Chenbin Zhang",
            "Zhiqiang Hu",
            "Chuchu Jiang",
            "Wen Chen",
            "Jie Xu",
            "Shaoting Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A highly maneuverable flying squirrel drone with controllable foldable wings",
        "abstract":"Typical drones with multi rotors are generally less maneuverable due to unidirectional thrust, which may be unfavorable to agile flight in very narrow and confined spaces. This paper suggests a new bio-inspired drone that is empowered with high maneuverability in a lightweight and easy-to-carry way. The proposed flying squirrel inspired drone has controllable foldable wings to cover a wider range of flight attitudes and provide more maneuverable flight capability with stable tracking performance. The wings of a drone are fabricated with silicone membranes and sophisticatedly controlled by reinforcement learning based on human-demonstrated data. Specially, such learning based wing control serves to capture even the complex aerodynamics that are often impossible to model mathematically. It is shown through experiment that the proposed flying squirrel drone intentionally induces aerodynamic drag and hence provides the desired additional repulsive force even under saturated mechanical thrust. This work is very meaningful in demonstrating the potential of biomimicry and machine learning for realizing an animal-like agile drone.\n        △ Less",
        "authors":[
            "Jun-Gill Kang",
            "Dohyeon Lee",
            "Soohee Han"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comorbidity-Informed Transfer Learning for Neuro-developmental Disorder Diagnosis",
        "abstract":"Neuro-developmental disorders are manifested as dysfunctions in cognition, communication, behaviour and adaptability, and deep learning-based computer-aided diagnosis (CAD) can alleviate the increasingly strained healthcare resources on neuroimaging. However, neuroimaging such as fMRI contains complex spatio-temporal features, which makes the corresponding representations susceptible to a variety of distractions, thus leading to less effective in CAD. For the first time, we present a Comorbidity-Informed Transfer Learning(CITL) framework for diagnosing neuro-developmental disorders using fMRI. In CITL, a new reinforced representation generation network is proposed, which first combines transfer learning with pseudo-labelling to remove interfering patterns from the temporal domain of fMRI and generates new representations using encoder-decoder architecture. The new representations are then trained in an architecturally simple classification network to obtain CAD model. In particular, the framework fully considers the comorbidity mechanisms of neuro-developmental disorders and effectively integrates them with semi-supervised learning and transfer learning, providing new perspectives on interdisciplinary. Experimental results demonstrate that CITL achieves competitive accuracies of 76.32% and 73.15% for detecting autism spectrum disorder and attention deficit hyperactivity disorder, respectively, which outperforms existing related transfer learning work for 7.2% and 0.5% respectively.\n        △ Less",
        "authors":[
            "Xin Wen",
            "Shijie Guo",
            "Wenbo Ning",
            "Rui Cao",
            "Jie Xiang",
            "Xiaobo Liu",
            "Jintai Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Measuring Leakage in Concept-Based Methods: An Information Theoretic Approach",
        "abstract":"Concept Bottleneck Models (CBMs) aim to enhance interpretability by structuring predictions around human-understandable concepts. However, unintended information leakage, where predictive signals bypass the concept bottleneck, compromises their transparency. This paper introduces an information-theoretic measure to quantify leakage in CBMs, capturing the extent to which concept embeddings encode additional, unintended information beyond the specified concepts. We validate the measure through controlled synthetic experiments, demonstrating its effectiveness in detecting leakage trends across various configurations. Our findings highlight that feature and concept dimensionality significantly influence leakage, and that classifier choice impacts measurement stability, with XGBoost emerging as the most reliable estimator. Additionally, preliminary investigations indicate that the measure exhibits the anticipated behavior when applied to soft joint CBMs, suggesting its reliability in leakage quantification beyond fully synthetic settings. While this study rigorously evaluates the measure in controlled synthetic experiments, future work can extend its application to real-world datasets.\n        △ Less",
        "authors":[
            "Mikael Makonnen",
            "Moritz Vandenhirtz",
            "Sonia Laguna",
            "Julia E Vogt"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"aweSOM: a CPU\/GPU-accelerated Self-organizing Map and Statistically Combined Ensemble Framework for Machine-learning Clustering Analysis",
        "abstract":"We introduce aweSOM, an open-source Python package for machine learning (ML) clustering and classification, using a Self-organizing Maps (SOM) algorithm that incorporates CPU\/GPU acceleration to accommodate large ($N > 10^6$, where $N$ is the number of data points), multidimensional datasets. aweSOM consists of two main modules, one that handles the initialization and training of the SOM, and another that stacks the results of multiple SOM realizations to obtain more statistically robust clusters. Existing Python-based SOM implementations (e.g., POPSOM, Yuan (2018); MiniSom, Vettigli (2018); sklearn-som) primarily serve as proof-of-concept demonstrations, optimized for smaller datasets, but lacking scalability for large, multidimensional data. aweSOM provides a solution for this gap in capability, with good performance scaling up to $\\sim 10^8$ individual points, and capable of utilizing multiple features per point. We compare the code performance against the legacy implementations it is based on, and find a 10-100x speed up, as well as significantly improved memory efficiency, due to several built-in optimizations.\n        △ Less",
        "authors":[
            "Trung Ha",
            "Joonas Nättilä",
            "Jordy Davelaar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InfoBound: A Provable Information-Bounds Inspired Framework for Both OoD Generalization and OoD Detection",
        "abstract":"In real-world scenarios, distribution shifts give rise to the importance of two problems: out-of-distribution (OoD) generalization, which focuses on models' generalization ability against covariate shifts (i.e., the changes of environments), and OoD detection, which aims to be aware of semantic shifts (i.e., test-time unseen classes). Real-world testing environments often involve a combination of both covariate and semantic shifts. While numerous methods have been proposed to address these critical issues, only a few works tackled them simultaneously. Moreover, prior works often improve one problem but sacrifice the other. To overcome these limitations, we delve into boosting OoD detection and OoD generalization from the perspective of information theory, which can be easily applied to existing models and different tasks. Building upon the theoretical bounds for mutual information and conditional entropy, we provide a unified approach, composed of Mutual Information Minimization (MI-Min) and Conditional Entropy Maximizing (CE-Max). Extensive experiments and comprehensive evaluations on multi-label image classification and object detection have demonstrated the superiority of our method. It successfully mitigates trade-offs between the two challenges compared to competitive baselines.\n        △ Less",
        "authors":[
            "Lin Zhu",
            "Yifeng Yang",
            "Zichao Nie",
            "Yuan Gao",
            "Jiarui Li",
            "Qinying Gu",
            "Xinbing Wang",
            "Chenghu Zhou",
            "Nanyang Ye"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Constants of motion network revisited",
        "abstract":"Discovering constants of motion is meaningful in helping understand the dynamical systems, but inevitably needs proficient mathematical skills and keen analytical capabilities. With the prevalence of deep learning, methods employing neural networks, such as Constant Of Motion nETwork (COMET), are promising in handling this scientific problem. Although the COMET method can produce better predictions on dynamics by exploiting the discovered constants of motion, there is still plenty of room to sharpen it. In this paper, we propose a novel neural network architecture, built using the singular-value-decomposition (SVD) technique, and a two-phase training algorithm to improve the performance of COMET. Extensive experiments show that our approach not only retains the advantages of COMET, such as applying to non-Hamiltonian systems and indicating the number of constants of motion, but also can be more lightweight and noise-robust than COMET.\n        △ Less",
        "authors":[
            "Wenqi Fang",
            "Chao Chen",
            "Yongkui Yang",
            "Zheng Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ensemble-Enhanced Graph Autoencoder with GAT and Transformer-Based Encoders for Robust Fault Diagnosis",
        "abstract":"Fault classification in industrial machinery is vital for enhancing reliability and reducing downtime, yet it remains challenging due to the variability of vibration patterns across diverse operating conditions. This study introduces a novel graph-based framework for fault classification, converting time-series vibration data from machinery operating at varying horsepower levels into a graph representation. We utilize Shannon's entropy to determine the optimal window size for data segmentation, ensuring each segment captures significant temporal patterns, and employ Dynamic Time Warping (DTW) to define graph edges based on segment similarity. A Graph Auto Encoder (GAE) with a deep graph transformer encoder, decoder, and ensemble classifier is developed to learn latent graph representations and classify faults across various categories. The GAE's performance is evaluated on the Case Western Reserve University (CWRU) dataset, with cross-dataset generalization assessed on the HUST dataset. Results show that GAE achieves a mean F1-score of 0.99 on the CWRU dataset, significantly outperforming baseline models-CNN, LSTM, RNN, GRU, and Bi-LSTM (F1-scores: 0.94-0.97, p < 0.05, Wilcoxon signed-rank test for Bi-LSTM: p < 0.05) -- particularly in challenging classes (e.g., Class 8: 0.99 vs. 0.71 for Bi-LSTM). Visualization of dataset characteristics reveals that datasets with amplified vibration patterns and diverse fault dynamics enhance generalization. This framework provides a robust solution for fault diagnosis under varying conditions, offering insights into dataset impacts on model performance.\n        △ Less",
        "authors":[
            "Moirangthem Tiken Singh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Spatially Directional Dual-Attention GAT for Spatial Fluoride Health Risk Modeling",
        "abstract":"Environmental exposure to fluoride is a major public health concern, particularly in regions with naturally elevated fluoride concentrations. Accurate modeling of fluoride-related health risks, such as dental fluorosis, requires spatially aware learning frameworks capable of capturing both geographic and semantic heterogeneity. In this work, we propose Spatially Directional Dual-Attention Graph Attention Network (SDD-GAT), a novel spatial graph neural network designed for fine-grained health risk prediction. SDD-GAT introduces a dual-graph architecture that disentangles geographic proximity and attribute similarity, and incorporates a directional attention mechanism that explicitly encodes spatial orientation and distance into the message passing process. To further enhance spatial coherence, we introduce a spatial smoothness regularization term that enforces consistency in predictions across neighboring locations. We evaluate SDD-GAT on a large-scale dataset covering over 50,000 fluoride monitoring samples and fluorosis records across Guizhou Province, China. Results show that SDD-GAT significantly outperforms traditional models and state-of-the-art GNNs in both regression and classification tasks, while also exhibiting improved spatial autocorrelation as measured by Moran's I. Our framework provides a generalizable foundation for spatial health risk modeling and geospatial learning under complex environmental settings.\n        △ Less",
        "authors":[
            "Da Yuan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Nash Equilibrium Between Consumer Electronic Devices and DoS Attacker for Distributed IoT-enabled RSE Systems",
        "abstract":"In electronic consumer Internet of Things (IoT), consumer electronic devices as edge devices require less computational overhead and the remote state estimation (RSE) of consumer electronic devices is always at risk of denial-of-service (DoS) attacks. Therefore, the adversarial strategy between consumer electronic devices and DoS attackers is critical. This paper focuses on the adversarial strategy between consumer electronic devices and DoS attackers in IoT-enabled RSE Systems. We first propose a remote joint estimation model for distributed measurements to effectively reduce consumer electronic device workload and minimize data leakage risks. The Kalman filter is deployed on the remote estimator, and the DoS attacks with open-loop as well as closed-loop are considered. We further introduce advanced reinforcement learning techniques, including centralized and distributed Minimax-DQN, to address high-dimensional decision-making challenges in both open-loop and closed-loop scenarios. Especially, the Q-network instead of the Q-table is used in the proposed approaches, which effectively solves the challenge of Q-learning. Moreover, the proposed distributed Minimax-DQN reduces the action space to expedite the search for Nash Equilibrium (NE). The experimental results validate that the proposed model can expeditiously restore the RSE error covariance to a stable state in the presence of DoS attacks, exhibiting notable attack robustness. The proposed centralized and distributed Minimax-DQN effectively resolves the NE in both open and closed-loop case, showcasing remarkable performance in terms of convergence. It reveals that substantial advantages in both efficiency and stability are achieved compared with the state-of-the-art methods.\n        △ Less",
        "authors":[
            "Gengcan Chen",
            "Donghong Cai",
            "Zahid Khan",
            "Jawad Ahmad",
            "Wadii Boulila"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bregman Linearized Augmented Lagrangian Method for Nonconvex Constrained Stochastic Zeroth-order Optimization",
        "abstract":"In this paper, we study nonconvex constrained stochastic zeroth-order optimization problems, for which we have access to exact information of constraints and noisy function values of the objective. We propose a Bregman linearized augmented Lagrangian method that utilizes stochastic zeroth-order gradient estimators combined with a variance reduction technique. We analyze its oracle complexity, in terms of the total number of stochastic function value evaluations required to achieve an \\(ε\\)-KKT point in \\(\\ell_p\\)-norm metrics with \\(p \\ge 2\\), where \\(p\\) is a parameter associated with the selected Bregman distance. In particular, starting from a near-feasible initial point and using Rademacher smoothing, the oracle complexity is in order \\(O(p d^{2\/p} ε^{-3})\\) for \\(p \\in [2, 2 \\ln d]\\), and \\(O(\\ln d \\cdot ε^{-3})\\) for \\(p > 2 \\ln d\\), where \\(d\\) denotes the problem dimension. Those results show that the complexity of the proposed method can achieve a dimensional dependency lower than \\(O(d)\\) without requiring additional assumptions, provided that a Bregman distance is chosen properly. This offers a significant improvement in the high-dimensional setting over existing work, and matches the lowest complexity order with respect to the tolerance \\(ε\\) reported in the literature. Numerical experiments on constrained Lasso and black-box adversarial attack problems highlight the promising performances of the proposed method.\n        △ Less",
        "authors":[
            "Qiankun Shi",
            "Xiao Wang",
            "Hao Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Tin-Tin: Towards Tiny Learning on Tiny Devices with Integer-based Neural Network Training",
        "abstract":"Recent advancements in machine learning (ML) have enabled its deployment on resource-constrained edge devices, fostering innovative applications such as intelligent environmental sensing. However, these devices, particularly microcontrollers (MCUs), face substantial challenges due to limited memory, computing capabilities, and the absence of dedicated floating-point units (FPUs). These constraints hinder the deployment of complex ML models, especially those requiring lifelong learning capabilities. To address these challenges, we propose Tin-Tin, an integer-based on-device training framework designed specifically for low-power MCUs. Tin-Tin introduces novel integer rescaling techniques to efficiently manage dynamic ranges and facilitate efficient weight updates using integer data types. Unlike existing methods optimized for devices with FPUs, GPUs, or FPGAs, Tin-Tin addresses the unique demands of tiny MCUs, prioritizing energy efficiency and optimized memory utilization. We validate the effectiveness of Tin-Tin through end-to-end application examples on real-world tiny devices, demonstrating its potential to support energy-efficient and sustainable ML applications on edge platforms.\n        △ Less",
        "authors":[
            "Yi Hu",
            "Jinhang Zuo",
            "Eddie Zhang",
            "Bob Iannucci",
            "Carlee Joe-Wong"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Insurance Reserving with CVaR-Constrained Reinforcement Learning under Macroeconomic Regimes",
        "abstract":"This paper proposes a reinforcement learning (RL) framework for insurance reserving that integrates tail-risk sensitivity, macroeconomic regime modeling, and regulatory compliance. The reserving problem is formulated as a finite-horizon Markov Decision Process (MDP), in which reserve adjustments are optimized using Proximal Policy Optimization (PPO) subject to Conditional Value-at-Risk (CVaR) constraints. To enhance policy robustness across varying economic conditions, the agent is trained using a regime-aware curriculum that progressively increases volatility exposure.\n  The reward structure penalizes reserve shortfall, capital inefficiency, and solvency floor violations, with design elements informed by Solvency II and Own Risk and Solvency Assessment (ORSA) frameworks. Empirical evaluations on two industry datasets--Workers' Compensation, and Other Liability--demonstrate that the RL-CVaR agent achieves superior performance relative to classical reserving methods across multiple criteria, including tail-risk control (CVaR$_{0.95}$), capital efficiency, and regulatory violation rate. The framework also accommodates fixed-shock stress testing and regime-stratified analysis, providing a principled and extensible approach to reserving under uncertainty.\n        △ Less",
        "authors":[
            "Stella C. Dong",
            "James R. Finlay"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Vision Transformers Exhibit Human-Like Biases: Evidence of Orientation and Color Selectivity, Categorical Perception, and Phase Transitions",
        "abstract":"This study explored whether Vision Transformers (ViTs) developed orientation and color biases similar to those observed in the human brain. Using synthetic datasets with controlled variations in noise levels, angles, lengths, widths, and colors, we analyzed the behavior of ViTs fine-tuned with LoRA. Our findings revealed four key insights: First, ViTs exhibited an oblique effect showing the lowest angle prediction errors at 180 deg (horizontal) across all conditions. Second, angle prediction errors varied by color. Errors were highest for bluish hues and lowest for yellowish ones. Additionally, clustering analysis of angle prediction errors showed that ViTs grouped colors in a way that aligned with human perceptual categories. In addition to orientation and color biases, we observed phase transition phenomena. While two phase transitions occurred consistently across all conditions, the training loss curves exhibited delayed transitions when color was incorporated as an additional data attribute. Finally, we observed that attention heads in certain layers inherently develop specialized capabilities, functioning as task-agnostic feature extractors regardless of the downstream task. These observations suggest that biases and properties arise primarily from pre-training on the original dataset which shapes the model's foundational representations and the inherent architectural constraints of the vision transformer, rather than being solely determined by downstream data statistics.\n        △ Less",
        "authors":[
            "Nooshin Bahador"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Expressivity of Quadratic Neural ODEs",
        "abstract":"This work focuses on deriving quantitative approximation error bounds for neural ordinary differential equations having at most quadratic nonlinearities in the dynamics. The simple dynamics of this model form demonstrates how expressivity can be derived primarily from iteratively composing many basic elementary operations, versus from the complexity of those elementary operations themselves. Like the analog differential analyzer and universal polynomial DAEs, the expressivity is derived instead primarily from the \"depth\" of the model. These results contribute to our understanding of what depth specifically imparts to the capabilities of deep learning architectures.\n        △ Less",
        "authors":[
            "Joshua Hanson",
            "Maxim Raginsky"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Machine Learning-Based Cyberattack Detection and Identification for Automatic Generation Control Systems Considering Nonlinearities",
        "abstract":"Automatic generation control (AGC) systems play a crucial role in maintaining system frequency across power grids. However, AGC systems' reliance on communicated measurements exposes them to false data injection attacks (FDIAs), which can compromise the overall system stability. This paper proposes a machine learning (ML)-based detection framework that identifies FDIAs and determines the compromised measurements. The approach utilizes an ML model trained offline to accurately detect attacks and classify the manipulated signals based on a comprehensive set of statistical and time-series features extracted from AGC measurements before and after disturbances. For the proposed approach, we compare the performance of several powerful ML algorithms. Our results demonstrate the efficacy of the proposed method in detecting FDIAs while maintaining a low false alarm rate, with an F1-score of up to 99.98%, outperforming existing approaches.\n        △ Less",
        "authors":[
            "Nour M. Shabar",
            "Ahmad Mohammad Saber",
            "Deepa Kundur"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"REMEMBER: Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning in Zero- and Few-shot Neurodegenerative Diagnosis",
        "abstract":"Timely and accurate diagnosis of neurodegenerative disorders, such as Alzheimer's disease, is central to disease management. Existing deep learning models require large-scale annotated datasets and often function as \"black boxes\". Additionally, datasets in clinical practice are frequently small or unlabeled, restricting the full potential of deep learning methods. Here, we introduce REMEMBER -- Retrieval-based Explainable Multimodal Evidence-guided Modeling for Brain Evaluation and Reasoning -- a new machine learning framework that facilitates zero- and few-shot Alzheimer's diagnosis using brain MRI scans through a reference-based reasoning process. Specifically, REMEMBER first trains a contrastively aligned vision-text model using expert-annotated reference data and extends pseudo-text modalities that encode abnormality types, diagnosis labels, and composite clinical descriptions. Then, at inference time, REMEMBER retrieves similar, human-validated cases from a curated dataset and integrates their contextual information through a dedicated evidence encoding module and attention-based inference head. Such an evidence-guided design enables REMEMBER to imitate real-world clinical decision-making process by grounding predictions in retrieved imaging and textual context. Specifically, REMEMBER outputs diagnostic predictions alongside an interpretable report, including reference images and explanations aligned with clinical workflows. Experimental results demonstrate that REMEMBER achieves robust zero- and few-shot performance and offers a powerful and explainable framework to neuroimaging-based diagnosis in the real world, especially under limited data.\n        △ Less",
        "authors":[
            "Duy-Cat Can",
            "Quang-Huy Tang",
            "Huong Ha",
            "Binh T. Nguyen",
            "Oliver Y. Chén"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Explorer: Robust Collection of Interactable GUI Elements",
        "abstract":"Automation of existing Graphical User Interfaces (GUIs) is important but hard to achieve. Upstream of making the GUI user-accessible or somehow scriptable, even the data-collection to understand the original interface poses significant challenges. For example, large quantities of general UI data seem helpful for training general machine learning (ML) models, but accessibility for each person can hinge on the ML's precision on a specific app. We therefore take the perspective that a given user needs confidence, that the relevant UI elements are being detected correctly throughout one app or digital environment. We mostly assume that the target application is known in advance, so that data collection and ML-training can be personalized for the test-time target domain. The proposed Explorer system focuses on detecting on-screen buttons and text-entry fields, i.e. interactables, where the training process has access to a live version of the application. The live application can run on almost any popular platform except iOS phones, and the collection is especially streamlined for Android phones or for desktop Chrome browsers. Explorer also enables the recording of interactive user sessions, and subsequent mapping of how such sessions overlap and sometimes loop back to similar states. We show how having such a map enables a kind of path planning through the GUI, letting a user issue audio commands to get to their destination. Critically, we are releasing our code for Explorer openly at https:\/\/github.com\/varnelis\/Explorer.\n        △ Less",
        "authors":[
            "Iason Chaimalas",
            "Arnas Vyšniauskas",
            "Gabriel Brostow"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph-Based Prediction Models for Data Debiasing",
        "abstract":"Bias in data collection, arising from both under-reporting and over-reporting, poses significant challenges in critical applications such as healthcare and public safety. In this work, we introduce Graph-based Over- and Under-reporting Debiasing (GROUD), a novel graph-based optimization framework that debiases reported data by jointly estimating the true incident counts and the associated reporting bias probabilities. By modeling the bias as a smooth signal over a graph constructed from geophysical or feature-based similarities, our convex formulation not only ensures a unique solution but also comes with theoretical recovery guarantees under certain assumptions. We validate GROUD on both challenging simulated experiments and real-world datasets -- including Atlanta emergency calls and COVID-19 vaccine adverse event reports -- demonstrating its robustness and superior performance in accurately recovering debiased counts. This approach paves the way for more reliable downstream decision-making in systems affected by reporting irregularities.\n        △ Less",
        "authors":[
            "Dongze Wu",
            "Hanyang Jiang",
            "Yao Xie"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Inferring Outcome Means of Exponential Family Distributions Estimated by Deep Neural Networks",
        "abstract":"While deep neural networks (DNNs) are widely used for prediction, inference on DNN-estimated subject-specific means for categorical or exponential family outcomes remains underexplored. We address this by proposing a DNN estimator under generalized nonparametric regression models (GNRMs) and developing a rigorous inference framework. Unlike existing approaches that assume independence between prediction errors and inputs to establish the error bound, a condition often violated in GNRMs, we allow for dependence and our theoretical analysis demonstrates the feasibility of drawing inference under GNRMs. To implement inference, we consider an Ensemble Subsampling Method (ESM) that leverages U-statistics and the Hoeffding decomposition to construct reliable confidence intervals for DNN estimates. We show that, under GNRM settings, ESM enables model-free variance estimation and accounts for heterogeneity among individuals in the population. Through simulations under nonparametric logistic, Poisson, and binomial regression models, we demonstrate the effectiveness and efficiency of our method. We further apply the method to the electronic Intensive Care Unit (eICU) dataset, a large-scale collection of anonymized health records from ICU patients, to predict ICU readmission risk and offer patient-centric insights for clinical decision-making.\n        △ Less",
        "authors":[
            "Xuran Meng",
            "Yi Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Context-Aware Adaptive Sampling for Intelligent Data Acquisition Systems Using DQN",
        "abstract":"Multi-sensor systems are widely used in the Internet of Things, environmental monitoring, and intelligent manufacturing. However, traditional fixed-frequency sampling strategies often lead to severe data redundancy, high energy consumption, and limited adaptability, failing to meet the dynamic sensing needs of complex environments. To address these issues, this paper proposes a DQN-based multi-sensor adaptive sampling optimization method. By leveraging a reinforcement learning framework to learn the optimal sampling strategy, the method balances data quality, energy consumption, and redundancy. We first model the multi-sensor sampling task as a Markov Decision Process (MDP), then employ a Deep Q-Network to optimize the sampling policy. Experiments on the Intel Lab Data dataset confirm that, compared with fixed-frequency sampling, threshold-triggered sampling, and other reinforcement learning approaches, DQN significantly improves data quality while lowering average energy consumption and redundancy rates. Moreover, in heterogeneous multi-sensor environments, DQN-based adaptive sampling shows enhanced robustness, maintaining superior data collection performance even in the presence of interference factors. These findings demonstrate that DQN-based adaptive sampling can enhance overall data acquisition efficiency in multi-sensor systems, providing a new solution for efficient and intelligent sensing.\n        △ Less",
        "authors":[
            "Weiqiang Huang",
            "Juecen Zhan",
            "Yumeng Sun",
            "Xu Han",
            "Tai An",
            "Nan Jiang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Minority Reports: Balancing Cost and Quality in Ground Truth Data Annotation",
        "abstract":"High-quality data annotation is an essential but laborious and costly aspect of developing machine learning-based software. We explore the inherent tradeoff between annotation accuracy and cost by detecting and removing minority reports -- instances where annotators provide incorrect responses -- that indicate unnecessary redundancy in task assignments. We propose an approach to prune potentially redundant annotation task assignments before they are executed by estimating the likelihood of an annotator disagreeing with the majority vote for a given task. Our approach is informed by an empirical analysis over computer vision datasets annotated by a professional data annotation platform, which reveals that the likelihood of a minority report event is dependent primarily on image ambiguity, worker variability, and worker fatigue. Simulations over these datasets show that we can reduce the number of annotations required by over 60% with a small compromise in label quality, saving approximately 6.6 days-equivalent of labor. Our approach provides annotation service platforms with a method to balance cost and dataset quality. Machine learning practitioners can tailor annotation accuracy levels according to specific application needs, thereby optimizing budget allocation while maintaining the data quality necessary for critical settings like autonomous driving technology.\n        △ Less",
        "authors":[
            "Hsuan Wei Liao",
            "Christopher Klugmann",
            "Daniel Kondermann",
            "Rafid Mahmood"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Building Machine Learning Limited Area Models: Kilometer-Scale Weather Forecasting in Realistic Settings",
        "abstract":"Machine learning is revolutionizing global weather forecasting, with models that efficiently produce highly accurate forecasts. Apart from global forecasting there is also a large value in high-resolution regional weather forecasts, focusing on accurate simulations of the atmosphere for a limited area. Initial attempts have been made to use machine learning for such limited area scenarios, but these experiments do not consider realistic forecasting settings and do not investigate the many design choices involved. We present a framework for building kilometer-scale machine learning limited area models with boundary conditions imposed through a flexible boundary forcing method. This enables boundary conditions defined either from reanalysis or operational forecast data. Our approach employs specialized graph constructions with rectangular and triangular meshes, along with multi-step rollout training strategies to improve temporal consistency. We perform systematic evaluation of different design choices, including the boundary width, graph construction and boundary forcing integration. Models are evaluated across both a Danish and a Swiss domain, two regions that exhibit different orographical characteristics. Verification is performed against both gridded analysis data and in-situ observations, including a case study for the storm Ciara in February 2020. Both models achieve skillful predictions across a wide range of variables, with our Swiss model outperforming the numerical weather prediction baseline for key surface variables. With their substantially lower computational cost, our findings demonstrate great potential for machine learning limited area models in the future of regional weather forecasting.\n        △ Less",
        "authors":[
            "Simon Adamov",
            "Joel Oskarsson",
            "Leif Denby",
            "Tomas Landelius",
            "Kasper Hintz",
            "Simon Christiansen",
            "Irene Schicker",
            "Carlos Osuna",
            "Fredrik Lindsten",
            "Oliver Fuhrer",
            "Sebastian Schemm"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Optimal Differentially Private Regret Bounds in Linear MDPs",
        "abstract":"We study regret minimization under privacy constraints in episodic inhomogeneous linear Markov Decision Processes (MDPs), motivated by the growing use of reinforcement learning (RL) in personalized decision-making systems that rely on sensitive user data. In this setting, both transition probabilities and reward functions are assumed to be linear in a feature mapping $φ(s, a)$, and we aim to ensure privacy through joint differential privacy (JDP), a relaxation of differential privacy suited to online learning. Prior work has established suboptimal regret bounds by privatizing the LSVI-UCB algorithm, which achieves $\\widetilde{O}(\\sqrt{d^3 H^4 K})$ regret in the non-private setting. Building on recent advances that improve this to minimax optimal regret $\\widetilde{O}(HD\\sqrt{K})$ via LSVI-UCB++ with Bernstein-style bonuses, we design a new differentially private algorithm by privatizing LSVI-UCB++ and adapting techniques for variance-aware analysis from offline RL. Our algorithm achieves a regret bound of $\\widetilde{O}(d \\sqrt{H^3 K} + H^{4.5} d^{7\/6} K^{1\/2} \/ ε)$, improving over previous private methods. Empirical results show that our algorithm retains near-optimal utility compared to non-private baselines, indicating that privacy can be achieved with minimal performance degradation in this setting.\n        △ Less",
        "authors":[
            "Sharan Sahu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient Implementation of Reinforcement Learning over Homomorphic Encryption",
        "abstract":"We investigate encrypted control policy synthesis over the cloud. While encrypted control implementations have been studied previously, we focus on the less explored paradigm of privacy-preserving control synthesis, which can involve heavier computations ideal for cloud outsourcing. We classify control policy synthesis into model-based, simulator-driven, and data-driven approaches and examine their implementation over fully homomorphic encryption (FHE) for privacy enhancements. A key challenge arises from comparison operations (min or max) in standard reinforcement learning algorithms, which are difficult to execute over encrypted data. This observation motivates our focus on Relative-Entropy-regularized reinforcement learning (RL) problems, which simplifies encrypted evaluation of synthesis algorithms due to their comparison-free structures. We demonstrate how linearly solvable value iteration, path integral control, and Z-learning can be readily implemented over FHE. We conduct a case study of our approach through numerical simulations of encrypted Z-learning in a grid world environment using the CKKS encryption scheme, showing convergence with acceptable approximation error. Our work suggests the potential for secure and efficient cloud-based reinforcement learning.\n        △ Less",
        "authors":[
            "Jihoon Suh",
            "Takashi Tanaka"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Regretful Decisions under Label Noise",
        "abstract":"Machine learning models are routinely used to support decisions that affect individuals -- be it to screen a patient for a serious illness or to gauge their response to treatment. In these tasks, we are limited to learning models from datasets with noisy labels. In this paper, we study the instance-level impact of learning under label noise. We introduce a notion of regret for this regime which measures the number of unforeseen mistakes due to noisy labels. We show that standard approaches to learning under label noise can return models that perform well at a population level while subjecting individuals to a lottery of mistakes. We present a versatile approach to estimate the likelihood of mistakes at the individual level from a noisy dataset by training models over plausible realizations of datasets without label noise. This is supported by a comprehensive empirical study of label noise in clinical prediction tasks. Our results reveal how failure to anticipate mistakes can compromise model reliability and adoption, and demonstrate how we can address these challenges by anticipating and avoiding regretful decisions.\n        △ Less",
        "authors":[
            "Sujay Nagaraj",
            "Yang Liu",
            "Flavio P. Calmon",
            "Berk Ustun"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Text To 3D Object Generation For Scalable Room Assembly",
        "abstract":"Modern machine learning models for scene understanding, such as depth estimation and object tracking, rely on large, high-quality datasets that mimic real-world deployment scenarios. To address data scarcity, we propose an end-to-end system for synthetic data generation for scalable, high-quality, and customizable 3D indoor scenes. By integrating and adapting text-to-image and multi-view diffusion models with Neural Radiance Field-based meshing, this system generates highfidelity 3D object assets from text prompts and incorporates them into pre-defined floor plans using a rendering tool. By introducing novel loss functions and training strategies into existing methods, the system supports on-demand scene generation, aiming to alleviate the scarcity of current available data, generally manually crafted by artists. This system advances the role of synthetic data in addressing machine learning training limitations, enabling more robust and generalizable models for real-world applications.\n        △ Less",
        "authors":[
            "Sonia Laguna",
            "Alberto Garcia-Garcia",
            "Marie-Julie Rakotosaona",
            "Stylianos Moschoglou",
            "Leonhard Helminger",
            "Sergio Orts-Escolano"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MedIL: Implicit Latent Spaces for Generating Heterogeneous Medical Images at Arbitrary Resolutions",
        "abstract":"In this work, we introduce MedIL, a first-of-its-kind autoencoder built for encoding medical images with heterogeneous sizes and resolutions for image generation. Medical images are often large and heterogeneous, where fine details are of vital clinical importance. Image properties change drastically when considering acquisition equipment, patient demographics, and pathology, making realistic medical image generation challenging. Recent work in latent diffusion models (LDMs) has shown success in generating images resampled to a fixed-size. However, this is a narrow subset of the resolutions native to image acquisition, and resampling discards fine anatomical details. MedIL utilizes implicit neural representations to treat images as continuous signals, where encoding and decoding can be performed at arbitrary resolutions without prior resampling. We quantitatively and qualitatively show how MedIL compresses and preserves clinically-relevant features over large multi-site, multi-resolution datasets of both T1w brain MRIs and lung CTs. We further demonstrate how MedIL can influence the quality of images generated with a diffusion model, and discuss how MedIL can enhance generative models to resemble raw clinical acquisitions.\n        △ Less",
        "authors":[
            "Tyler Spears",
            "Shen Zhu",
            "Yinzhu Jin",
            "Aman Shrivastava",
            "P. Thomas Fletcher"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conformal Calibration: Ensuring the Reliability of Black-Box AI in Wireless Systems",
        "abstract":"AI is poised to revolutionize telecommunication networks by boosting efficiency, automation, and decision-making. However, the black-box nature of most AI models introduces substantial risk, possibly deterring adoption by network operators. These risks are not addressed by the current prevailing deployment strategy, which typically follows a best-effort train-and-deploy paradigm. This paper reviews conformal calibration, a general framework that moves beyond the state of the art by adopting computationally lightweight, advanced statistical tools that offer formal reliability guarantees without requiring further training or fine-tuning. Conformal calibration encompasses pre-deployment calibration via uncertainty quantification or hyperparameter selection; online monitoring to detect and mitigate failures in real time; and counterfactual post-deployment performance analysis to address \"what if\" diagnostic questions after deployment. By weaving conformal calibration into the AI model lifecycle, network operators can establish confidence in black-box AI models as a dependable enabling technology for wireless systems.\n        △ Less",
        "authors":[
            "Osvaldo Simeone",
            "Sangwoo Park",
            "Matteo Zecchin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Contrastive Demonstration Selection with Semantic Diversity for Robust In-Context Machine Translation",
        "abstract":"In-Context Learning (ICL) empowers large language models to perform tasks by conditioning on a few input-output examples. However, the performance of ICL is highly sensitive to the selection of these demonstrations. While existing methods focus on similarity or contrastive selection, they often overlook the importance of diversity among the chosen examples. In this paper, we propose DiverseConE (Diversity-Enhanced Contrastive Example Selection), a novel approach for demonstration selection in in-context learning for machine translation. Our method builds upon contrastive selection by incorporating a diversity enhancement step based on embedding space dissimilarity. We conduct extensive experiments on the Llama2-7b model across four language pairs (English-Chinese, Chinese-English, Russian-German, German-Russian) in 1-shot and 3-shot settings, using COMET20 and COMET22 for evaluation. Our results demonstrate that DiverseConE consistently outperforms strong baseline methods, including random selection, BM25, TopK, and a state-of-the-art contrastive selection method. Further analysis, including diversity metrics and human evaluation, validates the effectiveness of our approach and highlights the benefits of considering demonstration diversity for improved translation quality.\n        △ Less",
        "authors":[
            "Owen Patterson",
            "Chee Ng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Application of Contrastive Learning on ECG Data: Evaluating Performance in Japanese and Classification with Around 100 Labels",
        "abstract":"The electrocardiogram (ECG) is a fundamental tool in cardiovascular diagnostics due to its powerful and non-invasive nature. One of the most critical usages is to determine whether more detailed examinations are necessary, with users ranging across various levels of expertise. Given this diversity in expertise, it is essential to assist users to avoid critical errors. Recent studies in machine learning have addressed this challenge by extracting valuable information from ECG data. Utilizing language models, these studies have implemented multimodal models aimed at classifying ECGs according to labeled terms. However, the number of classes was reduced, and it remains uncertain whether the technique is effective for languages other than English. To move towards practical application, we utilized ECG data from regular patients visiting hospitals in Japan, maintaining a large number of Japanese labels obtained from actual ECG readings. Using a contrastive learning framework, we found that even with 98 labels for classification, our Japanese-based language model achieves accuracy comparable to previous research. This study extends the applicability of multimodal machine learning frameworks to broader clinical studies and non-English languages.\n        △ Less",
        "authors":[
            "Junichiro Takahashi",
            "JingChuan Guan",
            "Masataka Sato",
            "Kaito Baba",
            "Kazuto Haruguchi",
            "Daichi Nagashima",
            "Satoshi Kodera",
            "Norihiko Takeda"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond Glucose-Only Assessment: Advancing Nocturnal Hypoglycemia Prediction in Children with Type 1 Diabetes",
        "abstract":"The dead-in-bed syndrome describes the sudden and unexplained death of young individuals with Type 1 Diabetes (T1D) without prior long-term complications. One leading hypothesis attributes this phenomenon to nocturnal hypoglycemia (NH), a dangerous drop in blood glucose during sleep. This study aims to improve NH prediction in children with T1D by leveraging physiological data and machine learning (ML) techniques. We analyze an in-house dataset collected from 16 children with T1D, integrating physiological metrics from wearable sensors. We explore predictive performance through feature engineering, model selection, architectures, and oversampling. To address data limitations, we apply transfer learning from a publicly available adult dataset. Our results achieve an AUROC of 0.75 +- 0.21 on the in-house dataset, further improving to 0.78 +- 0.05 with transfer learning. This research moves beyond glucose-only predictions by incorporating physiological parameters, showcasing the potential of ML to enhance NH detection and improve clinical decision-making for pediatric diabetes management.\n        △ Less",
        "authors":[
            "Marco Voegeli",
            "Sonia Laguna",
            "Heike Leutheuser",
            "Marc Pfister",
            "Marie-Anne Burckhardt",
            "Julia E Vogt"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sparse Hybrid Linear-Morphological Networks",
        "abstract":"We investigate hybrid linear-morphological networks. Recent studies highlight the inherent affinity of morphological layers to pruning, but also their difficulty in training. We propose a hybrid network structure, wherein morphological layers are inserted between the linear layers of the network, in place of activation functions. We experiment with the following morphological layers: 1) maxout pooling layers (as a special case of a morphological layer), 2) fully connected dense morphological layers, and 3) a novel, sparsely initialized variant of (2). We conduct experiments on the Magna-Tag-A-Tune (music auto-tagging) and CIFAR-10 (image classification) datasets, replacing the linear classification heads of state-of-the-art convolutional network architectures with our proposed network structure for the various morphological layers. We demonstrate that these networks induce sparsity to their linear layers, making them more prunable under L1 unstructured pruning. We also show that on MTAT our proposed sparsely initialized layer achieves slightly better performance than ReLU, maxout, and densely initialized max-plus layers, and exhibits faster initial convergence.\n        △ Less",
        "authors":[
            "Konstantinos Fotopoulos",
            "Christos Garoufis",
            "Petros Maragos"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"No-Regret Generative Modeling via Parabolic Monge-Ampère PDE",
        "abstract":"We introduce a novel generative modeling framework based on a discretized parabolic Monge-Ampère PDE, which emerges as a continuous limit of the Sinkhorn algorithm commonly used in optimal transport. Our method performs iterative refinement in the space of Brenier maps using a mirror gradient descent step. We establish theoretical guarantees for generative modeling through the lens of no-regret analysis, demonstrating that the iterates converge to the optimal Brenier map under a variety of step-size schedules. As a technical contribution, we derive a new Evolution Variational Inequality tailored to the parabolic Monge-Ampère PDE, connecting geometry, transportation cost, and regret. Our framework accommodates non-log-concave target distributions, constructs an optimal sampling process via the Brenier map, and integrates favorable learning techniques from generative adversarial networks and score-based diffusion models. As direct applications, we illustrate how our theory paves new pathways for generative modeling and variational inference.\n        △ Less",
        "authors":[
            "Nabarun Deb",
            "Tengyuan Liang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mixture of Group Experts for Learning Invariant Representations",
        "abstract":"Sparsely activated Mixture-of-Experts (MoE) models effectively increase the number of parameters while maintaining consistent computational costs per token. However, vanilla MoE models often suffer from limited diversity and specialization among experts, constraining their performance and scalability, especially as the number of experts increases. In this paper, we present a novel perspective on vanilla MoE with top-$k$ routing inspired by sparse representation. This allows us to bridge established theoretical insights from sparse representation into MoE models. Building on this foundation, we propose a group sparse regularization approach for the input of top-$k$ routing, termed Mixture of Group Experts (MoGE). MoGE indirectly regularizes experts by imposing structural constraints on the routing inputs, while preserving the original MoE architecture. Furthermore, we organize the routing input into a 2D topographic map, spatially grouping neighboring elements. This structure enables MoGE to capture representations invariant to minor transformations, thereby significantly enhancing expert diversity and specialization. Comprehensive evaluations across various Transformer models for image classification and language modeling tasks demonstrate that MoGE substantially outperforms its MoE counterpart, with minimal additional memory and computation overhead. Our approach provides a simple yet effective solution to scale the number of experts and reduce redundancy among them. The source code is included in the supplementary material and will be publicly released.\n        △ Less",
        "authors":[
            "Lei Kang",
            "Jia Li",
            "Mi Tian",
            "Hua Huang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Machine Learning-Based AP Selection in User-Centric Cell-free Multiple-Antenna Networks",
        "abstract":"User-centric cell-free (UCCF) massive multiple-input multiple-output (MIMO) systems are considered a viable solution to realize the advantages offered by cell-free (CF) networks, including reduced interference and consistent quality of service while maintaining manageable complexity. In this paper, we propose novel learning-based access point (AP) selection schemes tailored for UCCF massive MIMO systems. The learning model exploits the dataset generated from two distinct AP selection schemes, based on large-scale fading (LSF) coefficients and the sum-rate coefficients, respectively. The proposed learning-based AP selection schemes could be implemented centralized or distributed, with the aim of performing AP selection efficiently. We evaluate our model's performance against CF and two heuristic clustering schemes for UCCF networks. The results demonstrate that the learning-based approach achieves a comparable sum-rate performance to that of competing techniques for UCCF networks, while significantly reducing computational complexity.\n        △ Less",
        "authors":[
            "S. Salehi",
            "S. Mashdour",
            "O. Tamyigit",
            "S. Seyedmasoumian",
            "M. Moradikia",
            "R. C. de Lamare",
            "A. Schmeink"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"NetTAG: A Multimodal RTL-and-Layout-Aligned Netlist Foundation Model via Text-Attributed Graph",
        "abstract":"Circuit representation learning has shown promise in advancing Electronic Design Automation (EDA) by capturing structural and functional circuit properties for various tasks. Existing pre-trained solutions rely on graph learning with complex functional supervision, such as truth table simulation. However, they only handle simple and-inverter graphs (AIGs), struggling to fully encode other complex gate functionalities. While large language models (LLMs) excel at functional understanding, they lack the structural awareness for flattened netlists. To advance netlist representation learning, we present NetTAG, a netlist foundation model that fuses gate semantics with graph structure, handling diverse gate types and supporting a variety of functional and physical tasks. Moving beyond existing graph-only methods, NetTAG formulates netlists as text-attributed graphs, with gates annotated by symbolic logic expressions and physical characteristics as text attributes. Its multimodal architecture combines an LLM-based text encoder for gate semantics and a graph transformer for global structure. Pre-trained with gate and graph self-supervised objectives and aligned with RTL and layout stages, NetTAG captures comprehensive circuit intrinsics. Experimental results show that NetTAG consistently outperforms each task-specific method on four largely different functional and physical tasks and surpasses state-of-the-art AIG encoders, demonstrating its versatility.\n        △ Less",
        "authors":[
            "Wenji Fang",
            "Wenkai Li",
            "Shang Liu",
            "Yao Lu",
            "Hongce Zhang",
            "Zhiyao Xie"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MiMIC: Multi-Modal Indian Earnings Calls Dataset to Predict Stock Prices",
        "abstract":"Predicting stock market prices following corporate earnings calls remains a significant challenge for investors and researchers alike, requiring innovative approaches that can process diverse information sources. This study investigates the impact of corporate earnings calls on stock prices by introducing a multi-modal predictive model. We leverage textual data from earnings call transcripts, along with images and tables from accompanying presentations, to forecast stock price movements on the trading day immediately following these calls. To facilitate this research, we developed the MiMIC (Multi-Modal Indian Earnings Calls) dataset, encompassing companies representing the Nifty 50, Nifty MidCap 50, and Nifty Small 50 indices. The dataset includes earnings call transcripts, presentations, fundamentals, technical indicators, and subsequent stock prices. We present a multimodal analytical framework that integrates quantitative variables with predictive signals derived from textual and visual modalities, thereby enabling a holistic approach to feature representation and analysis. This multi-modal approach demonstrates the potential for integrating diverse information sources to enhance financial forecasting accuracy. To promote further research in computational economics, we have made the MiMIC dataset publicly available under the CC-NC-SA-4.0 licence. Our work contributes to the growing body of literature on market reactions to corporate communications and highlights the efficacy of multi-modal machine learning techniques in financial analysis.\n        △ Less",
        "authors":[
            "Sohom Ghosh",
            "Arnab Maji",
            "Sudip Kumar Naskar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adiabatic Encoding of Pre-trained MPS Classifiers into Quantum Circuits",
        "abstract":"Although Quantum Neural Networks (QNNs) offer powerful methods for classification tasks, the training of QNNs faces two major training obstacles: barren plateaus and local minima. A promising solution is to first train a tensor-network (TN) model classically and then embed it into a QNN.\\ However, embedding TN-classifiers into quantum circuits generally requires postselection whose success probability may decay exponentially with the system size. We propose an \\emph{adiabatic encoding} framework that encodes pre-trained MPS-classifiers into quantum MPS (qMPS) circuits with postselection, and gradually removes the postselection while retaining performance. We prove that training qMPS-classifiers from scratch on a certain artificial dataset is exponentially hard due to barren plateaus, but our adiabatic encoding circumvents this issue. Additional numerical experiments on binary MNIST also confirm its robustness.\n        △ Less",
        "authors":[
            "Keisuke Murota"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"NoTeS-Bank: Benchmarking Neural Transcription and Search for Scientific Notes Understanding",
        "abstract":"Understanding and reasoning over academic handwritten notes remains a challenge in document AI, particularly for mathematical equations, diagrams, and scientific notations. Existing visual question answering (VQA) benchmarks focus on printed or structured handwritten text, limiting generalization to real-world note-taking. To address this, we introduce NoTeS-Bank, an evaluation benchmark for Neural Transcription and Search in note-based question answering. NoTeS-Bank comprises complex notes across multiple domains, requiring models to process unstructured and multimodal content. The benchmark defines two tasks: (1) Evidence-Based VQA, where models retrieve localized answers with bounding-box evidence, and (2) Open-Domain VQA, where models classify the domain before retrieving relevant documents and answers. Unlike classical Document VQA datasets relying on optical character recognition (OCR) and structured data, NoTeS-BANK demands vision-language fusion, retrieval, and multimodal reasoning. We benchmark state-of-the-art Vision-Language Models (VLMs) and retrieval frameworks, exposing structured transcription and reasoning limitations. NoTeS-Bank provides a rigorous evaluation with NDCG@5, MRR, Recall@K, IoU, and ANLS, establishing a new standard for visual document understanding and reasoning.\n        △ Less",
        "authors":[
            "Aniket Pal",
            "Sanket Biswas",
            "Alloy Das",
            "Ayush Lodh",
            "Priyanka Banerjee",
            "Soumitri Chattopadhyay",
            "Dimosthenis Karatzas",
            "Josep Llados",
            "C. V. Jawahar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Type-Constrained Code Generation with Language Models",
        "abstract":"Large language models (LLMs) have achieved notable success in code generation. However, they still frequently produce uncompilable output because their next-token inference procedure does not model formal aspects of code. Although constrained decoding is a promising approach to alleviate this issue, it has only been applied to handle either domain-specific languages or syntactic language features. This leaves typing errors, which are beyond the domain of syntax and generally hard to adequately constrain. To address this challenge, we introduce a type-constrained decoding approach that leverages type systems to guide code generation. We develop novel prefix automata for this purpose and introduce a sound approach to enforce well-typedness based on type inference and a search over inhabitable types. We formalize our approach on a simply-typed language and extend it to TypeScript to demonstrate practicality. Our evaluation on HumanEval shows that our approach reduces compilation errors by more than half and increases functional correctness in code synthesis, translation, and repair tasks across LLMs of various sizes and model families, including SOTA open-weight models with more than 30B parameters.\n        △ Less",
        "authors":[
            "Niels Mündler",
            "Jingxuan He",
            "Hao Wang",
            "Koushik Sen",
            "Dawn Song",
            "Martin Vechev"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DL-QAT: Weight-Decomposed Low-Rank Quantization-Aware Training for Large Language Models",
        "abstract":"Improving the efficiency of inference in Large Language Models (LLMs) is a critical area of research. Post-training Quantization (PTQ) is a popular technique, but it often faces challenges at low-bit levels, particularly in downstream tasks. Quantization-aware Training (QAT) can alleviate this problem, but it requires significantly more computational resources. To tackle this, we introduced Weight-Decomposed Low-Rank Quantization-Aware Training (DL-QAT), which merges the advantages of QAT while training only less than 1% of the total parameters. Specifically, we introduce a group-specific quantization magnitude to adjust the overall scale of each quantization group. Within each quantization group, we use LoRA matrices to update the weight size and direction in the quantization space. We validated the effectiveness of our method on the LLaMA and LLaMA2 model families. The results show significant improvements over our baseline method across different quantization granularities. For instance, for LLaMA-7B, our approach outperforms the previous state-of-the-art method by 4.2% in MMLU on 3-bit LLaMA-7B model. Additionally, our quantization results on pre-trained models also surpass previous QAT methods, demonstrating the superior performance and efficiency of our approach.\n        △ Less",
        "authors":[
            "Wenjin Ke",
            "Zhe Li",
            "Dong Li",
            "Lu Tian",
            "Emad Barsoum"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CMCRD: Cross-Modal Contrastive Representation Distillation for Emotion Recognition",
        "abstract":"Emotion recognition is an important component of affective computing, and also human-machine interaction. Unimodal emotion recognition is convenient, but the accuracy may not be high enough; on the contrary, multi-modal emotion recognition may be more accurate, but it also increases the complexity and cost of the data collection system. This paper considers cross-modal emotion recognition, i.e., using both electroencephalography (EEG) and eye movement in training, but only EEG or eye movement in test. We propose cross-modal contrastive representation distillation (CMCRD), which uses a pre-trained eye movement classification model to assist the training of an EEG classification model, improving feature extraction from EEG signals, or vice versa. During test, only EEG signals (or eye movement signals) are acquired, eliminating the need for multi-modal data. CMCRD not only improves the emotion recognition accuracy, but also makes the system more simplified and practical. Experiments using three different neural network architectures on three multi-modal emotion recognition datasets demonstrated the effectiveness of CMCRD. Compared with the EEG-only model, it improved the average classification accuracy by about 6.2%.\n        △ Less",
        "authors":[
            "Siyuan Kan",
            "Huanyu Wu",
            "Zhenyao Cui",
            "Fan Huang",
            "Xiaolong Xu",
            "Dongrui Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Spiking Neural Network for Intra-cortical Brain Signal Decoding",
        "abstract":"Decoding brain signals accurately and efficiently is crucial for intra-cortical brain-computer interfaces. Traditional decoding approaches based on neural activity vector features suffer from low accuracy, whereas deep learning based approaches have high computational cost. To improve both the decoding accuracy and efficiency, this paper proposes a spiking neural network (SNN) for effective and energy-efficient intra-cortical brain signal decoding. We also propose a feature fusion approach, which integrates the manually extracted neural activity vector features with those extracted by a deep neural network, to further improve the decoding accuracy. Experiments in decoding motor-related intra-cortical brain signals of two rhesus macaques demonstrated that our SNN model achieved higher accuracy than traditional artificial neural networks; more importantly, it was tens or hundreds of times more efficient. The SNN model is very suitable for high precision and low power applications like intra-cortical brain-computer interfaces.\n        △ Less",
        "authors":[
            "Song Yang",
            "Haotian Fu",
            "Herui Zhang",
            "Peng Zhang",
            "Wei Li",
            "Dongrui Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accurate Diagnosis of Respiratory Viruses Using an Explainable Machine Learning with Mid-Infrared Biomolecular Fingerprinting of Nasopharyngeal Secretions",
        "abstract":"Accurate identification of respiratory viruses (RVs) is critical for outbreak control and public health. This study presents a diagnostic system that combines Attenuated Total Reflectance Fourier Transform Infrared Spectroscopy (ATR-FTIR) from nasopharyngeal secretions with an explainable Rotary Position Embedding-Sparse Attention Transformer (RoPE-SAT) model to accurately identify multiple RVs within 10 minutes. Spectral data (4000-00 cm-1) were collected, and the bio-fingerprint region (1800-900 cm-1) was employed for analysis. Standard normal variate (SNV) normalization and second-order derivation were applied to reduce scattering and baseline drift. Gradient-weighted class activation mapping (Grad-CAM) was employed to generate saliency maps, highlighting spectral regions most relevant to classification and enhancing the interpretability of model outputs. Two independent cohorts from Beijing Youan Hospital, processed with different viral transport media (VTMs) and drying methods, were evaluated, with one including influenza B, SARS-CoV-2, and healthy controls, and the other including mycoplasma, SARS-CoV-2, and healthy controls. The model achieved sensitivity and specificity above 94.40% across both cohorts. By correlating model-selected infrared regions with known biomolecular signatures, we verified that the system effectively recognizes virus-specific spectral fingerprints, including lipids, Amide I, Amide II, Amide III, nucleic acids, and carbohydrates, and leverages their weighted contributions for accurate classification.\n        △ Less",
        "authors":[
            "Wenwen Zhang",
            "Zhouzhuo Tang",
            "Yingmei Feng",
            "Xia Yu",
            "Qi Jie Wang",
            "Zhiping Lin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FairACE: Achieving Degree Fairness in Graph Neural Networks via Contrastive and Adversarial Group-Balanced Training",
        "abstract":"Fairness has been a significant challenge in graph neural networks (GNNs) since degree biases often result in un-equal prediction performance among nodes with varying degrees. Existing GNN models focus on prediction accuracy, frequently overlooking fairness across different degree groups. To addressthis issue, we propose a novel GNN framework, namely Fairness- Aware Asymmetric Contrastive Ensemble (FairACE), which inte-grates asymmetric contrastive learning with adversarial training to improve degree fairness. FairACE captures one-hop local neighborhood information and two-hop monophily similarity to create fairer node representations and employs a degree fairness regulator to balance performance between high-degree and low-degree nodes. During model training, a novel group-balanced fairness loss is proposed to minimize classification disparities across degree groups. In addition, we also propose a novel fairness metric, the Accuracy Distribution Gap (ADG), which can quantitatively assess and ensure equitable performance across different degree-based node groups. Experimental results on both synthetic and real-world datasets demonstrate that FairACE significantly improves degree fairness metrics while maintaining competitive accuracy in comparison to the state-of-the-art GNN models.\n        △ Less",
        "authors":[
            "Jiaxin Liu",
            "Xiaoqian Jiang",
            "Xiang Li",
            "Bohan Zhang",
            "Jing Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rethinking Remaining Useful Life Prediction with Scarce Time Series Data: Regression under Indirect Supervision",
        "abstract":"Supervised time series prediction relies on directly measured target variables, but real-world use cases such as predicting remaining useful life (RUL) involve indirect supervision, where the target variable is labeled as a function of another dependent variable. Trending temporal regression techniques rely on sequential time series inputs to capture temporal patterns, requiring interpolation when dealing with sparsely and irregularly sampled covariates along the timeline. However, interpolation can introduce significant biases, particularly with highly scarce data. In this paper, we address the RUL prediction problem with data scarcity as time series regression under indirect supervision. We introduce a unified framework called parameterized static regression, which takes single data points as inputs for regression of target values, inherently handling data scarcity without requiring interpolation. The time dependency under indirect supervision is captured via a parametrical rectification (PR) process, approximating a parametric function during inference with historical posteriori estimates, following the same underlying distribution used for labeling during training. Additionally, we propose a novel batch training technique for tasks in indirect supervision to prevent overfitting and enhance efficiency. We evaluate our model on public benchmarks for RUL prediction with simulated data scarcity. Our method demonstrates competitive performance in prediction accuracy when dealing with highly scarce time series data.\n        △ Less",
        "authors":[
            "Jiaxiang Cheng",
            "Yipeng Pang",
            "Guoqiang Hu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Query-based Knowledge Transfer for Heterogeneous Learning Environments",
        "abstract":"Decentralized collaborative learning under data heterogeneity and privacy constraints has rapidly advanced. However, existing solutions like federated learning, ensembles, and transfer learning, often fail to adequately serve the unique needs of clients, especially when local data representation is limited. To address this issue, we propose a novel framework called Query-based Knowledge Transfer (QKT) that enables tailored knowledge acquisition to fulfill specific client needs without direct data exchange. QKT employs a data-free masking strategy to facilitate communication-efficient query-focused knowledge transfer while refining task-specific parameters to mitigate knowledge interference and forgetting. Our experiments, conducted on both standard and clinical benchmarks, show that QKT significantly outperforms existing collaborative learning methods by an average of 20.91\\% points in single-class query settings and an average of 14.32\\% points in multi-class query scenarios. Further analysis and ablation studies reveal that QKT effectively balances the learning of new and existing knowledge, showing strong potential for its application in decentralized learning.\n        △ Less",
        "authors":[
            "Norah Alballa",
            "Wenxuan Zhang",
            "Ziquan Liu",
            "Ahmed M. Abdelmoniem",
            "Mohamed Elhoseiny",
            "Marco Canini"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards More Efficient, Robust, Instance-adaptive, and Generalizable Online Learning",
        "abstract":"The primary goal of my Ph.D. study is to develop provably efficient and practical algorithms for data-driven online sequential decision-making under uncertainty. My work focuses on reinforcement learning (RL), multi-armed bandits, and their applications, including recommendation systems, computer networks, video analytics, and large language models (LLMs). Online learning methods, such as bandits and RL, have demonstrated remarkable success - ranging from outperforming human players in complex games like Atari and Go to advancing robotics, recommendation systems, and fine-tuning LLMs. Despite these successes, many established algorithms rely on idealized models that can fail under model misspecifications or adversarial perturbations, particularly in settings where accurate prior knowledge of the underlying model class is unavailable or where malicious users operate within dynamic systems. These challenges are pervasive in real-world applications, where robust and adaptive solutions are critical. Furthermore, while worst-case guarantees provide theoretical reliability, they often fail to capture instance-dependent performance, which can lead to more efficient and practical solutions. Another key challenge lies in generalizing to new, unseen environments, a crucial requirement for deploying these methods in dynamic and unpredictable settings. To address these limitations, my research aims to develop more efficient, robust, instance-adaptive, and generalizable online learning algorithms for both reinforcement learning and bandits. Towards this end, I focus on developing more efficient, robust, instance-adaptive, and generalizable for both general reinforcement learning (RL) and bandits.\n        △ Less",
        "authors":[
            "Zhiyong Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RSLAQ -- A Robust SLA-driven 6G O-RAN QoS xApp using deep reinforcement learning",
        "abstract":"The evolution of 6G envisions a wide range of applications and services characterized by highly differentiated and stringent Quality of Service (QoS) requirements. Open Radio Access Network (O-RAN) technology has emerged as a transformative approach that enables intelligent software-defined management of the RAN. A cornerstone of O-RAN is the RAN Intelligent Controller (RIC), which facilitates the deployment of intelligent applications (xApps and rApps) near the radio unit. In this context, QoS management through O-RAN has been explored using network slice and machine learning (ML) techniques. Although prior studies have demonstrated the ability to optimize RAN resource allocation and prioritize slices effectively, they have not considered the critical integration of Service Level Agreements (SLAs) into the ML learning process. This omission can lead to suboptimal resource utilization and, in many cases, service outages when target Key Performance Indicators (KPIs) are not met. This work introduces RSLAQ, an innovative xApp designed to ensure robust QoS management for RAN slicing while incorporating SLAs directly into its operational framework. RSLAQ translates operator policies into actionable configurations, guiding resource distribution and scheduling for RAN slices. Using deep reinforcement learning (DRL), RSLAQ dynamically monitors RAN performance metrics and computes optimal actions, embedding SLA constraints to mitigate conflicts and prevent outages. Extensive system-level simulations validate the efficacy of the proposed solution, demonstrating its ability to optimize resource allocation, improve SLA adherence, and maintain operational reliability (>95%) in challenging scenarios.\n        △ Less",
        "authors":[
            "Noe M. Yungaicela-Naula",
            "Vishal Sharma",
            "Sandra Scott-Hayward"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction",
        "abstract":"Long sequence prediction is a key challenge in time series forecasting. While Mamba-based models have shown strong performance due to their sequence selection capabilities, they still struggle with insufficient focus on critical time steps and incomplete noise suppression, caused by limited selective abilities. To address this, we introduce Repetitive Contrastive Learning (RCL), a token-level contrastive pretraining framework aimed at enhancing Mamba's selective capabilities. RCL pretrains a single Mamba block to strengthen its selective abilities and then transfers these pretrained parameters to initialize Mamba blocks in various backbone models, improving their temporal prediction performance. RCL uses sequence augmentation with Gaussian noise and applies inter-sequence and intra-sequence contrastive learning to help the Mamba module prioritize information-rich time steps while ignoring noisy ones. Extensive experiments show that RCL consistently boosts the performance of backbone models, surpassing existing methods and achieving state-of-the-art results. Additionally, we propose two metrics to quantify Mamba's selective capabilities, providing theoretical, qualitative, and quantitative evidence for the improvements brought by RCL.\n        △ Less",
        "authors":[
            "Wenbo Yan",
            "Hanzhong Cao",
            "Ying Tan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Confounding Factors-Inhibition Adversarial Learning Framework for Multi-site fMRI Mental Disorder Identification",
        "abstract":"In open data sets of functional magnetic resonance imaging (fMRI), the heterogeneity of the data is typically attributed to a combination of factors, including differences in scanning procedures, the presence of confounding effects, and population diversities between multiple sites. These factors contribute to the diminished effectiveness of representation learning, which in turn affects the overall efficacy of subsequent classification procedures. To address these limitations, we propose a novel multi-site adversarial learning network (MSalNET) for fMRI-based mental disorder detection. Firstly, a representation learning module is introduced with a node information assembly (NIA) mechanism to better extract features from functional connectivity (FC). This mechanism aggregates edge information from both horizontal and vertical directions, effectively assembling node information. Secondly, to generalize the feature across sites, we proposed a site-level feature extraction module that can learn from individual FC data, which circumvents additional prior information. Lastly, an adversarial learning network is proposed as a means of balancing the trade-off between individual classification and site regression tasks, with the introduction of a novel loss function. The proposed method was evaluated on two multi-site fMRI datasets, i.e., Autism Brain Imaging Data Exchange (ABIDE) and ADHD-200. The results indicate that the proposed method achieves a better performance than other related algorithms with the accuracy of 75.56 and 68.92 in ABIDE and ADHD-200 datasets, respectively. Furthermore, the result of the site regression indicates that the proposed method reduces site variability from a data-driven perspective. The most discriminative brain regions revealed by NIA are consistent with statistical findings, uncovering the \"black box\" of deep learning to a certain extent.\n        △ Less",
        "authors":[
            "Xin Wen",
            "Shijie Guo",
            "Wenbo Ning",
            "Rui Cao",
            "Yan Niu",
            "Bin Wan",
            "Peng Wei",
            "Xiaobo Liu",
            "Jie Xiang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deconfounded Reasoning for Multimodal Fake News Detection via Causal Intervention",
        "abstract":"The rapid growth of social media has led to the widespread dissemination of fake news across multiple content forms, including text, images, audio, and video. Traditional unimodal detection methods fall short in addressing complex cross-modal manipulations; as a result, multimodal fake news detection has emerged as a more effective solution. However, existing multimodal approaches, especially in the context of fake news detection on social media, often overlook the confounders hidden within complex cross-modal interactions, leading models to rely on spurious statistical correlations rather than genuine causal mechanisms. In this paper, we propose the Causal Intervention-based Multimodal Deconfounded Detection (CIMDD) framework, which systematically models three types of confounders via a unified Structural Causal Model (SCM): (1) Lexical Semantic Confounder (LSC); (2) Latent Visual Confounder (LVC); (3) Dynamic Cross-Modal Coupling Confounder (DCCC). To mitigate the influence of these confounders, we specifically design three causal modules based on backdoor adjustment, frontdoor adjustment, and cross-modal joint intervention to block spurious correlations from different perspectives and achieve causal disentanglement of representations for deconfounded reasoning. Experimental results on the FakeSV and FVC datasets demonstrate that CIMDD significantly improves detection accuracy, outperforming state-of-the-art methods by 4.27% and 4.80%, respectively. Furthermore, extensive experimental results indicate that CIMDD exhibits strong generalization and robustness across diverse multimodal scenarios.\n        △ Less",
        "authors":[
            "Moyang Liu",
            "Kaiying Yan",
            "Yukun Liu",
            "Ruibo Fu",
            "Zhengqi Wen",
            "Xuefei Liu",
            "Chenxing Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dose-finding design based on level set estimation in phase I cancer clinical trials",
        "abstract":"The primary objective of phase I cancer clinical trials is to evaluate the safety of a new experimental treatment and to find the maximum tolerated dose (MTD). We show that the MTD estimation problem can be regarded as a level set estimation (LSE) problem whose objective is to determine the regions where an unknown function value is above or below a given threshold. Then, we propose a novel dose-finding design in the framework of LSE. The proposed design determines the next dose on the basis of an acquisition function incorporating uncertainty in the posterior distribution of the dose-toxicity curve as well as overdose control. Simulation experiments show that the proposed LSE design achieves a higher accuracy in estimating the MTD and involves a lower risk of overdosing allocation compared to existing designs, thereby indicating that it provides an effective methodology for phase I cancer clinical trial design.\n        △ Less",
        "authors":[
            "Keiichiro Seno",
            "Kota Matsui",
            "Shogo Iwazaki",
            "Yu Inatsu",
            "Shion Takeno",
            "Shigeyuki Matsui"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring Modality Disruption in Multimodal Fake News Detection",
        "abstract":"The rapid growth of social media has led to the widespread dissemination of fake news across multiple content forms, including text, images, audio, and video. Compared to unimodal fake news detection, multimodal fake news detection benefits from the increased availability of information across multiple modalities. However, in the context of social media, certain modalities in multimodal fake news detection tasks may contain disruptive or over-expressive information. These elements often include exaggerated or embellished content. We define this phenomenon as modality disruption and explore its impact on detection models through experiments. To address the issue of modality disruption in a targeted manner, we propose a multimodal fake news detection framework, FND-MoE. Additionally, we design a two-pass feature selection mechanism to further mitigate the impact of modality disruption. Extensive experiments on the FakeSV and FVC-2018 datasets demonstrate that FND-MoE significantly outperforms state-of-the-art methods, with accuracy improvements of 3.45% and 3.71% on the respective datasets compared to baseline models.\n        △ Less",
        "authors":[
            "Moyang Liu",
            "Kaiying Yan",
            "Yukun Liu",
            "Ruibo Fu",
            "Zhengqi Wen",
            "Xuefei Liu",
            "Chenxing Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MatWheel: Addressing Data Scarcity in Materials Science Through Synthetic Data",
        "abstract":"Data scarcity and the high cost of annotation have long been persistent challenges in the field of materials science. Inspired by its potential in other fields like computer vision, we propose the MatWheel framework, which train the material property prediction model using the synthetic data generated by the conditional generative model. We explore two scenarios: fully-supervised and semi-supervised learning. Using CGCNN for property prediction and Con-CDVAE as the conditional generative model, experiments on two data-scarce material property datasets from Matminer database are conducted. Results show that synthetic data has potential in extreme data-scarce scenarios, achieving performance close to or exceeding that of real samples in all two tasks. We also find that pseudo-labels have little impact on generated data quality. Future work will integrate advanced models and optimize generation conditions to boost the effectiveness of the materials data flywheel.\n        △ Less",
        "authors":[
            "Wentao Li",
            "Yizhe Chen",
            "Jiangjie Qiu",
            "Xiaonan Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Kernel-Based Enhanced Oversampling Method for Imbalanced Classification",
        "abstract":"This paper introduces a novel oversampling technique designed to improve classification performance on imbalanced datasets. The proposed method enhances the traditional SMOTE algorithm by incorporating convex combination and kernel-based weighting to generate synthetic samples that better represent the minority class. Through experiments on multiple real-world datasets, we demonstrate that the new technique outperforms existing methods in terms of F1-score, G-mean, and AUC, providing a robust solution for handling imbalanced datasets in classification tasks.\n        △ Less",
        "authors":[
            "Wenjie Li",
            "Sibo Zhu",
            "Zhijian Li",
            "Hanlin Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Self-Supervised Autoencoder Network for Robust Heart Rate Extraction from Noisy Photoplethysmogram: Applying Blind Source Separation to Biosignal Analysis",
        "abstract":"Biosignals can be viewed as mixtures measuring particular physiological events, and blind source separation (BSS) aims to extract underlying source signals from mixtures. This paper proposes a self-supervised multi-encoder autoencoder (MEAE) to separate heartbeat-related source signals from photoplethysmogram (PPG), enhancing heart rate (HR) detection in noisy PPG data. The MEAE is trained on PPG signals from a large open polysomnography database without any pre-processing or data selection. The trained network is then applied to a noisy PPG dataset collected during the daily activities of nine subjects. The extracted heartbeat-related source signal significantly improves HR detection as compared to the original PPG. The absence of pre-processing and the self-supervised nature of the proposed method, combined with its strong performance, highlight the potential of BSS in biosignal analysis.\n        △ Less",
        "authors":[
            "Matthew B. Webster",
            "Dongheon Lee",
            "Joonnyong Lee"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deploying Large AI Models on Resource-Limited Devices with Split Federated Learning",
        "abstract":"Large Artificial Intelligence Models (LAMs) powered by massive datasets, extensive parameter scales, and extensive computational resources, leading to significant transformations across various industries. Yet, their practical deployment on resource-limited mobile edge devices is hindered by critical challenges such as data privacy, constrained resources, and high overhead costs. Addressing this gap, this paper proposes a novel framework, named Quantized Split Federated Fine-Tuning Large AI Model (SFLAM). By partitioning the training load between edge devices and servers using a split learning paradigm, SFLAM can facilitate the operation of large models on devices and significantly lowers the memory requirements on edge devices. Additionally, SFLAM incorporates quantization management, power control, and bandwidth allocation strategies to enhance training efficiency while concurrently reducing energy consumption and communication latency. A theoretical analysis exploring the latency-energy trade-off is presented, and the framework's efficacy is validated via comprehensive simulations. The findings indicate that SFLAM achieves superior performance in terms of learning efficiency and scalability compared to conventional methods, thereby providing a valuable approach for enabling advanced AI services in resource-constrained scenarios.\n        △ Less",
        "authors":[
            "Xianke Qiang",
            "Hongda Liu",
            "Xinran Zhang",
            "Zheng Chang",
            "Ying-Chang Liang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Shrinkage Initialization for Smooth Learning of Neural Networks",
        "abstract":"The successes of intelligent systems have quite relied on the artificial learning of information, which lead to the broad applications of neural learning solutions. As a common sense, the training of neural networks can be largely improved by specifically defined initialization, neuron layers as well as the activation functions. Though there are sequential layer based initialization available, the generalized solution to initial stages is still desired. In this work, an improved approach to initialization of neural learning is presented, which adopts the shrinkage approach to initialize the transformation of each layer of networks. It can be universally adapted for the structures of any networks with random layers, while stable performance can be attained. Furthermore, the smooth learning of networks is adopted in this work, due to the diverse influence on neural learning. Experimental results on several artificial data sets demonstrate that, the proposed method is able to present robust results with the shrinkage initialization, and competent for smooth learning of neural networks.\n        △ Less",
        "authors":[
            "Miao Cheng",
            "Feiyan Zhou",
            "Hongwei Zou",
            "Limin Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE",
        "abstract":"In modern air traffic management, generating synthetic flight trajectories has emerged as a promising solution for addressing data scarcity, protecting sensitive information, and supporting large-scale analyses. In this paper, we propose a novel method for trajectory synthesis by adapting the Time-Based Vector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages time-frequency domain processing, vector quantization, and transformer-based priors to capture both global and local dynamics in flight data. By discretizing the latent space and integrating transformer priors, the model learns long-range spatiotemporal dependencies and preserves coherence across entire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite of quality, statistical, and distributional metrics, as well as a flyability assessment conducted in an open-source air traffic simulator. Results indicate that TimeVQVAE outperforms a temporal convolutional VAE baseline, generating synthetic trajectories that mirror real flight data in terms of spatial accuracy, temporal consistency, and statistical properties. Furthermore, the simulator-based assessment shows that most generated trajectories maintain operational feasibility, although occasional outliers underscore the potential need for additional domain-specific constraints. Overall, our findings underscore the importance of multi-scale representation learning for capturing complex flight behaviors and demonstrate the promise of TimeVQVAE in producing representative synthetic trajectories for downstream tasks such as model training, airspace design, and air traffic forecasting.\n        △ Less",
        "authors":[
            "Abdulmajid Murad",
            "Massimiliano Ruocco"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"High dimensional online calibration in polynomial time",
        "abstract":"In online (sequential) calibration, a forecaster predicts probability distributions over a finite outcome space $[d]$ over a sequence of $T$ days, with the goal of being calibrated. While asymptotically calibrated strategies are known to exist, they suffer from the curse of dimensionality: the best known algorithms require $\\exp(d)$ days to achieve non-trivial calibration.\n  In this work, we present the first asymptotically calibrated strategy that guarantees non-trivial calibration after a polynomial number of rounds. Specifically, for any desired accuracy $ε> 0$, our forecaster becomes $ε$-calibrated after $T = d^{O(1\/ε^2)}$ days. We complement this result with a lower bound, proving that at least $T = d^{Ω(\\log(1\/ε))}$ rounds are necessary to achieve $ε$-calibration. Our results resolve the open questions posed by [Abernethy-Mannor'11, Hazan-Kakade'12].\n  Our algorithm is inspired by recent breakthroughs in swap regret minimization [Peng-Rubinstein'24, Dagan et al.'24]. Despite its strong theoretical guarantees, the approach is remarkably simple and intuitive: it randomly selects among a set of sub-forecasters, each of which predicts the empirical outcome frequency over recent time windows.\n        △ Less",
        "authors":[
            "Binghui Peng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"crowd-hpo: Realistic Hyperparameter Optimization and Benchmarking for Learning from Crowds with Noisy Labels",
        "abstract":"Crowdworking is a cost-efficient solution to acquire class labels. Since these labels are subject to noise, various approaches to learning from crowds have been proposed. Typically, these approaches are evaluated with default hyperparameters, resulting in suboptimal performance, or with hyperparameters tuned using a validation set with ground truth class labels, representing an often unrealistic scenario. Moreover, both experimental setups can produce different rankings of approaches, complicating comparisons between studies. Therefore, we introduce crowd-hpo as a realistic benchmark and experimentation protocol including hyperparameter optimization under noisy crowd-labeled data. At its core, crowd-hpo investigates model selection criteria to identify well-performing hyperparameter configurations only with access to noisy crowd-labeled validation data. Extensive experimental evaluations with neural networks show that these criteria are effective for optimizing hyperparameters in learning from crowds approaches. Accordingly, incorporating such criteria into experimentation protocols is essential for enabling more realistic and fair benchmarking.\n        △ Less",
        "authors":[
            "Marek Herde",
            "Lukas Lührs",
            "Denis Huseljic",
            "Bernhard Sick"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Case for Kolmogorov-Arnold Networks in Prefetching: Towards Low-Latency, Generalizable ML-Based Prefetchers",
        "abstract":"The memory wall problem arises due to the disparity between fast processors and slower memory, causing significant delays in data access, even more so on edge devices. Data prefetching is a key strategy to address this, with traditional methods evolving to incorporate Machine Learning (ML) for improved accuracy. Modern prefetchers must balance high accuracy with low latency to further practicality. We explore the applicability of utilizing Kolmogorov-Arnold Networks (KAN) with learnable activation functions,a prefetcher we implemented called KANBoost, to further this aim. KANs are a novel, state-of-the-art model that work on breaking down continuous, bounded multi-variate functions into functions of their constituent variables, and use these constitutent functions as activations on each individual neuron. KANBoost predicts the next memory access by modeling deltas between consecutive addresses, offering a balance of accuracy and efficiency to mitigate the memory wall problem with minimal overhead, instead of relying on address-correlation prefetching. Initial results indicate that KAN-based prefetching reduces inference latency (18X lower than state-of-the-art ML prefetchers) while achieving moderate IPC improvements (2.5\\% over no-prefetching). While KANs still face challenges in capturing long-term dependencies, we propose that future research should explore hybrid models that combine KAN efficiency with stronger sequence modeling techniques, paving the way for practical ML-based prefetching in edge devices and beyond.\n        △ Less",
        "authors":[
            "Dhruv Kulkarni",
            "Bharat Bhammar",
            "Henil Thaker",
            "Pranav Dhobi",
            "R. P. Gohil",
            "Sai Manoj Pudukotai Dinkarrao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation",
        "abstract":"We offer a novel approach, MGS (Markov Greedy Sums), to improve the accuracy of low-bitwidth floating-point dot products in neural network computations. In conventional 32-bit floating-point summation, adding values with different exponents may lead to loss of precision in the mantissa of the smaller term, which is right-shifted to align with the larger term's exponent. Such shifting (a.k.a. 'swamping') is a significant source of numerical errors in accumulation when implementing low-bitwidth dot products (e.g., 8-bit floating point) as the mantissa has a small number of bits. We avoid most swamping errors by arranging the terms in dot product summation based on their exponents and summing the mantissas without overflowing the low-bitwidth accumulator. We design, analyze, and implement the algorithm to minimize 8-bit floating point error at inference time for several neural networks. In contrast to traditional sequential summation, our method has significantly lowered numerical errors, achieving classification accuracy on par with high-precision floating-point baselines for multiple image classification tasks. Our dMAC hardware units can reduce power consumption by up to 34.1\\% relative to conventional MAC units.\n        △ Less",
        "authors":[
            "Vikas Natesh",
            "H. T. Kung",
            "David Kong"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Incremental Non-Linear Manifold Approximation Method",
        "abstract":"Analyzing high-dimensional data presents challenges due to the \"curse of dimensionality'', making computations intensive. Dimension reduction techniques, categorized as linear or non-linear, simplify such data. Non-linear methods are particularly essential for efficiently visualizing and processing complex data structures in interactive and graphical applications. This research develops an incremental non-linear dimension reduction method using the Geometric Multi-Resolution Analysis (GMRA) framework for streaming data. The proposed method enables real-time data analysis and visualization by incrementally updating the cluster map, PCA basis vectors, and wavelet coefficients. Numerical experiments show that the incremental GMRA accurately represents non-linear manifolds even with small initial samples and aligns closely with batch GMRA, demonstrating efficient updates and maintaining the multiscale structure. The findings highlight the potential of Incremental GMRA for real-time visualization and interactive graphics applications that require adaptive high-dimensional data representations.\n        △ Less",
        "authors":[
            "Praveen T. W. Hettige",
            "Benjamin W. Ong"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PQS (Prune, Quantize, and Sort): Low-Bitwidth Accumulation of Dot Products in Neural Network Computations",
        "abstract":"We present PQS, which uses three techniques together - Prune, Quantize, and Sort - to achieve low-bitwidth accumulation of dot products in neural network computations. In conventional quantized (e.g., 8-bit) dot products, partial results are accumulated into wide (e.g., 32-bit) accumulators to avoid overflows when accumulating intermediate partial sums. However, such wide accumulators increase memory bandwidth usage and reduce energy efficiency. We show that iterative N:M pruning in floating point followed by quantization to 8 (or fewer) bits, and accumulation of partial products in a sorted order (\"small to large\") allows for accurate, compressed models with short dot product lengths that do not require wide accumulators. We design, analyze, and implement the PQS algorithm to eliminate accumulation overflows at inference time for several neural networks. Our method offers a 2.5x reduction in accumulator bitwidth while achieving model accuracy on par with floating-point baselines for multiple image classification tasks.\n        △ Less",
        "authors":[
            "Vikas Natesh",
            "H. T. Kung"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Practical Approach to using Supervised Machine Learning Models to Classify Aviation Safety Occurrences",
        "abstract":"This paper describes a practical approach of using supervised machine learning (ML) models to assist safety investigators to classify aviation occurrences into either incident or serious incident categories. Our implementation currently deployed as a ML web application is trained on a labelled dataset derived from publicly available aviation investigation reports. A selection of five supervised learning models (Support Vector Machine, Logistic Regression, Random Forest Classifier, XGBoost and K-Nearest Neighbors) were evaluated. This paper showed the best performing ML algorithm was the Random Forest Classifier with accuracy = 0.77, F1 Score = 0.78 and MCC = 0.51 (average of 100 sample runs). The study had also explored the effect of applying Synthetic Minority Over-sampling Technique (SMOTE) to the imbalanced dataset, and the overall observation ranged from no significant effect to substantial degradation in performance for some of the models after the SMOTE adjustment.\n        △ Less",
        "authors":[
            "Bryan Y. Siow"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multimodal 3D Genome Pre-training",
        "abstract":"Deep learning techniques have driven significant progress in various analytical tasks within 3D genomics in computational biology. However, a holistic understanding of 3D genomics knowledge remains underexplored. Here, we propose MIX-HIC, the first multimodal foundation model of 3D genome that integrates both 3D genome structure and epigenomic tracks, which obtains unified and comprehensive semantics. For accurate heterogeneous semantic fusion, we design the cross-modal interaction and mapping blocks for robust unified representation, yielding the accurate aggregation of 3D genome knowledge. Besides, we introduce the first large-scale dataset comprising over 1 million pairwise samples of Hi-C contact maps and epigenomic tracks for high-quality pre-training, enabling the exploration of functional implications in 3D genomics. Extensive experiments show that MIX-HIC can significantly surpass existing state-of-the-art methods in diverse downstream tasks. This work provides a valuable resource for advancing 3D genomics research.\n        △ Less",
        "authors":[
            "Minghao Yang",
            "Pengteng Li",
            "Yan Liang",
            "Qianyi Cai",
            "Zhihang Zheng",
            "Shichen Zhang",
            "Pengfei Zhang",
            "Zhi-An Huang",
            "Hui Xiong"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization",
        "abstract":"Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose \\textbf{Dynamic Mask coupled with Concept-Aware Loss}, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our \\textbf{Dynamic Mask} mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our \\textbf{Concept-Aware Loss} explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. The code will be released publicly.\n        △ Less",
        "authors":[
            "Gen Li",
            "Yang Xiao",
            "Jie Ji",
            "Kaiyuan Deng",
            "Bo Hui",
            "Linke Guo",
            "Xiaolong Ma"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InterQ: A DQN Framework for Optimal Intermittent Control",
        "abstract":"In this letter, we explore the communication-control co-design of discrete-time stochastic linear systems through reinforcement learning. Specifically, we examine a closed-loop system involving two sequential decision-makers: a scheduler and a controller. The scheduler continuously monitors the system's state but transmits it to the controller intermittently to balance the communication cost and control performance. The controller, in turn, determines the control input based on the intermittently received information. Given the partially nested information structure, we show that the optimal control policy follows a certainty-equivalence form. Subsequently, we analyze the qualitative behavior of the scheduling policy. To develop the optimal scheduling policy, we propose InterQ, a deep reinforcement learning algorithm which uses a deep neural network to approximate the Q-function. Through extensive numerical evaluations, we analyze the scheduling landscape and further compare our approach against two baseline strategies: (a) a multi-period periodic scheduling policy, and (b) an event-triggered policy. The results demonstrate that our proposed method outperforms both baselines. The open source implementation can be found at https:\/\/github.com\/AC-sh\/InterQ.\n        △ Less",
        "authors":[
            "Shubham Aggarwal",
            "Dipankar Maity",
            "Tamer Başar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Hierarchical Decomposition of Kullback-Leibler Divergence: Disentangling Marginal Mismatches from Statistical Dependencies",
        "abstract":"The Kullback-Leibler (KL) divergence is a foundational measure for comparing probability distributions. Yet in multivariate settings, its single value often obscures the underlying reasons for divergence, conflating mismatches in individual variable distributions (marginals) with effects arising from statistical dependencies. We derive an algebraically exact, additive, and hierarchical decomposition of the KL divergence between a joint distribution P(X1,...,Xn) and a standard product reference distribution Q(X1,...,Xn) = product_i q(Xi), where variables are assumed independent and identically distributed according to a common reference q. The total divergence precisely splits into two primary components: (1) the summed divergence of each marginal distribution Pi(Xi) from the common reference q(Xi), quantifying marginal deviations; and (2) the total correlation (or multi-information), capturing the total statistical dependency among variables. Leveraging Mobius inversion on the subset lattice, we further decompose this total correlation term into a hierarchy of signed contributions from distinct pairwise, triplet, and higher-order statistical interactions, expressed using standard Shannon information quantities. This decomposition provides an algebraically complete and interpretable breakdown of KL divergence using established information measures, requiring no approximations or model assumptions. Numerical validation using hypergeometric sampling confirms the decomposition's exactness to machine precision across diverse system configurations. This framework enables a precise diagnosis of divergence origins--distinguishing marginal effects from interaction effects--with potential applications across machine learning, econometrics, and complex systems analysis.\n        △ Less",
        "authors":[
            "William Cook"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards On-Device Learning and Reconfigurable Hardware Implementation for Encoded Single-Photon Signal Processing",
        "abstract":"Deep neural networks (DNNs) enhance the accuracy and efficiency of reconstructing key parameters from time-resolved photon arrival signals recorded by single-photon detectors. However, the performance of conventional backpropagation-based DNNs is highly dependent on various parameters of the optical setup and biological samples under examination, necessitating frequent network retraining, either through transfer learning or from scratch. Newly collected data must also be stored and transferred to a high-performance GPU server for retraining, introducing latency and storage overhead. To address these challenges, we propose an online training algorithm based on a One-Sided Jacobi rotation-based Online Sequential Extreme Learning Machine (OSOS-ELM). We fully exploit parallelism in executing OSOS-ELM on a heterogeneous FPGA with integrated ARM cores. Extensive evaluations of OSOS-ELM and OSELM demonstrate that both achieve comparable accuracy across different network dimensions (i.e., input, hidden, and output layers), while OSOS-ELM proves to be more hardware-efficient. By leveraging the parallelism of OSOS-ELM, we implement a holistic computing prototype on a Xilinx ZCU104 FPGA, which integrates a multi-core CPU and programmable logic fabric. We validate our approach through three case studies involving single-photon signal analysis: sensing through fog using commercial single-photon LiDAR, fluorescence lifetime estimation in FLIM, and blood flow index reconstruction in DCS, all utilizing one-dimensional data encoded from photonic signals. From a hardware perspective, we optimize the OSOS-ELM workload by employing multi-tasked processing on ARM CPU cores and pipelined execution on the FPGA's logic fabric. We also implement our OSOS-ELM on the NVIDIA Jetson Xavier NX GPU to comprehensively investigate its computing performance on another type of heterogeneous computing platform.\n        △ Less",
        "authors":[
            "Zhenya Zang",
            "Xingda Li",
            "David Day Uei Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Associating transportation planning-related measures with Mild Cognitive Impairment",
        "abstract":"Understanding the relationship between mild cognitive impairment and driving behavior is essential to improve road safety, especially among older adults. In this study, we computed certain variables that reflect daily driving habits, such as trips to specific locations (e.g., home, work, medical, social, and errands) of older drivers in Nebraska using geohashing. The computed variables were then analyzed using a two-fold approach involving data visualization and machine learning models (C5.0, Random Forest, Support Vector Machines) to investigate the efficiency of the computed variables in predicting whether a driver is cognitively impaired or unimpaired. The C5.0 model demonstrated robust and stable performance with a median recall of 74\\%, indicating that our methodology was able to identify cognitive impairment in drivers 74\\% of the time correctly. This highlights our model's effectiveness in minimizing false negatives which is an important consideration given the cost of missing impaired drivers could be potentially high. Our findings highlight the potential of life space variables in understanding and predicting cognitive decline, offering avenues for early intervention and tailored support for affected individuals.\n        △ Less",
        "authors":[
            "Souradeep Chattopadhyay",
            "Guillermo Basulto-Elias",
            "Jun Ha Chang",
            "Matthew Rizzo",
            "Shauna Hallmark",
            "Anuj Sharma",
            "Soumik Sarkar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detecting Instruction Fine-tuning Attack on Language Models with Influence Function",
        "abstract":"Instruction fine-tuning attacks pose a significant threat to large language models (LLMs) by subtly embedding poisoned data in fine-tuning datasets, which can trigger harmful or unintended responses across a range of tasks. This undermines model alignment and poses security risks in real-world deployment. In this work, we present a simple and effective approach to detect and mitigate such attacks using influence functions, a classical statistical tool adapted for machine learning interpretation. Traditionally, the high computational costs of influence functions have limited their application to large models and datasets. The recent Eigenvalue-Corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation method enables efficient influence score computation, making it feasible for large-scale analysis.\n  We are the first to apply influence functions for detecting language model instruction fine-tuning attacks on large-scale datasets, as both the instruction fine-tuning attack on language models and the influence calculation approximation technique are relatively new. Our large-scale empirical evaluation of influence functions on 50,000 fine-tuning examples and 32 tasks reveals a strong association between influence scores and sentiment. Building on this, we introduce a novel sentiment transformation combined with influence functions to detect and remove critical poisons -- poisoned data points that skew model predictions. Removing these poisons (only 1% of total data) recovers model performance to near-clean levels, demonstrating the effectiveness and efficiency of our approach. Artifact is available at https:\/\/github.com\/lijiawei20161002\/Poison-Detection.\n  WARNING: This paper contains offensive data examples.\n        △ Less",
        "authors":[
            "Jiawei Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Champion-level Vision-based Reinforcement Learning Agent for Competitive Racing in Gran Turismo 7",
        "abstract":"Deep reinforcement learning has achieved superhuman racing performance in high-fidelity simulators like Gran Turismo 7 (GT7). It typically utilizes global features that require instrumentation external to a car, such as precise localization of agents and opponents, limiting real-world applicability. To address this limitation, we introduce a vision-based autonomous racing agent that relies solely on ego-centric camera views and onboard sensor data, eliminating the need for precise localization during inference. This agent employs an asymmetric actor-critic framework: the actor uses a recurrent neural network with the sensor data local to the car to retain track layouts and opponent positions, while the critic accesses the global features during training. Evaluated in GT7, our agent consistently outperforms GT7's built-drivers. To our knowledge, this work presents the first vision-based autonomous racing agent to demonstrate champion-level performance in competitive racing scenarios.\n        △ Less",
        "authors":[
            "Hojoon Lee",
            "Takuma Seno",
            "Jun Jet Tai",
            "Kaushik Subramanian",
            "Kenta Kawamoto",
            "Peter Stone",
            "Peter R. Wurman"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learning in Structured Stackelberg Games",
        "abstract":"We study structured Stackelberg games, in which both players (the leader and the follower) observe information about the state of the world at time of play. Importantly, this information may contain information about the follower, which the leader may use when deciding her strategy. Under this setting, we show that no-regret learning is possible if and only if the set of mappings from contexts to follower types that the leader uses to learn is not ``too complex''. Specifically, we find that standard learning theoretic measures of complexity do not characterize learnability in our setting and we give a new dimension which does, which we term the Stackelberg-Littlestone dimension. In the distributional setting, we give analogous results by showing that standard complexity measures do not characterize the sample complexity of learning, but a new dimension called the Stackelberg-Natarajan dimension does. We then show that an appropriate empirical risk minimization procedure achieves the corresponding sample complexity.\n        △ Less",
        "authors":[
            "Maria-Florina Balcan",
            "Kiriaki Fragkia",
            "Keegan Harris"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ReCA: A Parametric ReLU Composite Activation Function",
        "abstract":"Activation functions have been shown to affect the performance of deep neural networks significantly. While the Rectified Linear Unit (ReLU) remains the dominant choice in practice, the optimal activation function for deep neural networks remains an open research question. In this paper, we propose a novel parametric activation function, ReCA, based on ReLU, which has been shown to outperform all baselines on state-of-the-art datasets using different complex neural network architectures.\n        △ Less",
        "authors":[
            "John Chidiac",
            "Danielle Azar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RouterKT: Mixture-of-Experts for Knowledge Tracing",
        "abstract":"Knowledge Tracing (KT) is a fundamental task in Intelligent Tutoring Systems (ITS), which aims to model the dynamic knowledge states of students based on their interaction histories. However, existing KT models often rely on a global forgetting decay mechanism for capturing learning patterns, assuming that students' performance is predominantly influenced by their most recent interactions. Such approaches fail to account for the diverse and complex learning patterns arising from individual differences and varying learning stages. To address this limitation, we propose RouterKT, a novel Mixture-of-Experts (MoE) architecture designed to capture heterogeneous learning patterns by enabling experts to specialize in different patterns without any handcrafted learning pattern bias such as forgetting decay. Specifically, RouterKT introduces a \\textbf{person-wise routing mechanism} to effectively model individual-specific learning behaviors and employs \\textbf{multi-heads as experts} to enhance the modeling of complex and diverse patterns. Comprehensive experiments on ten benchmark datasets demonstrate that RouterKT exhibits significant flexibility and improves the performance of various KT backbone models, with a maximum average AUC improvement of 3.29\\% across different backbones and datasets, outperforming other state-of-the-art models. Moreover, RouterKT demonstrates consistently superior inference efficiency compared to existing approaches based on handcrafted learning pattern bias, highlighting its usability for real-world educational applications. The source code is available at https:\/\/github.com\/derek-liao\/RouterKT.git.\n        △ Less",
        "authors":[
            "Han Liao",
            "Shuaishuai Zu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AGENT: An Aerial Vehicle Generation and Design Tool Using Large Language Models",
        "abstract":"Computer-aided design (CAD) is a promising application area for emerging artificial intelligence methods. Traditional workflows for cyberphysical systems create detailed digital models which can be evaluated by physics simulators in order to narrow the search space before creating physical prototypes. A major bottleneck of this approach is that the simulators are often computationally expensive and slow. Recent advancements in AI methods offer the possibility to accelerate these pipelines. We use the recently released AircraftVerse dataset, which is especially suited for developing and evaluating large language models for designs. AircraftVerse contains a diverse set of UAV designs represented via textual design trees together with detailed physics simulation results. Following the recent success of large language models (LLMs), we propose AGENT (Aircraft GENeraTor). AGENT is a comprehensive design tool built on the CodeT5+ LLM which learns powerful representations of aircraft textual designs directly from JSON files. We develop a curriculum of training tasks which imbues a single model with a suite of useful features. AGENT is able to generate designs conditioned on properties of flight dynamics (hover time, maximum speed, etc.). Additionally, AGENT can issue evaluations of designs allowing it to act as a surrogate model of the physics simulation that underlies the AircraftVerse dataset. We present a series of experiments which demonstrate our system's abilities. We are able to achieve strong performance using the smallest member of the CodeT5+ family (220M parameters). This allows for a flexible and powerful system which can be executed on a single GPU enabling a clear path toward future deployment.\n        △ Less",
        "authors":[
            "Colin Samplawski",
            "Adam D. Cobb",
            "Susmit Jha"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving municipal responsiveness through AI-powered image analysis in E-Government",
        "abstract":"Integration of Machine Learning (ML) techniques into public administration marks a new and transformative era for e-government systems. While traditionally e-government studies were focusing on text-based interactions, this one explores the innovative application of ML for image analysis, an approach that enables governments to address citizen petitions more efficiently. By using image classification and object detection algorithms, the model proposed in this article supports public institutions in identifying and fast responding to evidence submitted by citizens in picture format, such as infrastructure issues, environmental concerns or other urban issues that citizens might face. The research also highlights the Jevons Paradox as a critical factor, wherein increased efficiency from the citizen side (especially using mobile platforms and apps) may generate higher demand which should lead to scalable and robust solutions. Using as a case study a Romanian municipality who provided datasets of citizen-submitted images, the author analysed and proved that ML can improve accuracy and responsiveness of public institutions. The findings suggest that adopting ML for e-petition systems can not only enhance citizen participation but also speeding up administrative processes, paving the way for more transparent and effective governance. This study contributes to the discourse on e-government 3.0 by showing the potential of Artificial Intelligence (AI) to transform public service delivery, ensuring sustainable (and scalable) solutions for the growing demands of modern urban governance.\n        △ Less",
        "authors":[
            "Catalin Vrabie"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion",
        "abstract":"Knowledge graph embedding (KGE) models are extensively studied for knowledge graph completion, yet their evaluation remains constrained by unrealistic benchmarks. Commonly used datasets are either faulty or too small to reflect real-world data. Few studies examine the role of mediator nodes, which are essential for modeling n-ary relationships, or investigate model performance variation across domains. Standard evaluation metrics rely on the closed-world assumption, which penalizes models for correctly predicting missing triples, contradicting the fundamental goals of link prediction. These metrics often compress accuracy assessment into a single value, obscuring models' specific strengths and weaknesses. The prevailing evaluation protocol operates under the unrealistic assumption that an entity's properties, for which values are to be predicted, are known in advance. While alternative protocols such as property prediction, entity-pair ranking and triple classification address some of these limitations, they remain underutilized. This paper conducts a comprehensive evaluation of four representative KGE models on large-scale datasets FB-CVT-REV and FB+CVT-REV. Our analysis reveals critical insights, including substantial performance variations between small and large datasets, both in relative rankings and absolute metrics, systematic overestimation of model capabilities when n-ary relations are binarized, and fundamental limitations in current evaluation protocols and metrics.\n        △ Less",
        "authors":[
            "Nasim Shirvani-Mahdavi",
            "Farahnaz Akrami",
            "Chengkai Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bidirectional Linear Recurrent Models for Sequence-Level Multisource Fusion",
        "abstract":"Sequence modeling is a critical yet challenging task with wide-ranging applications, especially in time series forecasting for domains like weather prediction, temperature monitoring, and energy load forecasting. Transformers, with their attention mechanism, have emerged as state-of-the-art due to their efficient parallel training, but they suffer from quadratic time complexity, limiting their scalability for long sequences. In contrast, recurrent neural networks (RNNs) offer linear time complexity, spurring renewed interest in linear RNNs for more computationally efficient sequence modeling. In this work, we introduce BLUR (Bidirectional Linear Unit for Recurrent network), which uses forward and backward linear recurrent units (LRUs) to capture both past and future dependencies with high computational efficiency. BLUR maintains the linear time complexity of traditional RNNs, while enabling fast parallel training through LRUs. Furthermore, it offers provably stable training and strong approximation capabilities, making it highly effective for modeling long-term dependencies. Extensive experiments on sequential image and time series datasets reveal that BLUR not only surpasses transformers and traditional RNNs in accuracy but also significantly reduces computational costs, making it particularly suitable for real-world forecasting tasks. Our code is available here.\n        △ Less",
        "authors":[
            "Qisai Liu",
            "Zhanhong Jiang",
            "Joshua R. Waite",
            "Chao Liu",
            "Aditya Balu",
            "Soumik Sarkar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Forecasting Cryptocurrency Prices using Contextual ES-adRNN with Exogenous Variables",
        "abstract":"In this paper, we introduce a new approach to multivariate forecasting cryptocurrency prices using a hybrid contextual model combining exponential smoothing (ES) and recurrent neural network (RNN). The model consists of two tracks: the context track and the main track. The context track provides additional information to the main track, extracted from representative series. This information as well as information extracted from exogenous variables is dynamically adjusted to the individual series forecasted by the main track. The RNN stacked architecture with hierarchical dilations, incorporating recently developed attentive dilated recurrent cells, allows the model to capture short and long-term dependencies across time series and dynamically weight input information. The model generates both point daily forecasts and predictive intervals for one-day, one-week and four-week horizons. We apply our model to forecast prices of 15 cryptocurrencies based on 17 input variables and compare its performance with that of comparative models, including both statistical and ML ones.\n        △ Less",
        "authors":[
            "Slawek Smyl",
            "Grzegorz Dudek",
            "Paweł Pełka"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Investigating the Treacherous Turn in Deep Reinforcement Learning",
        "abstract":"The Treacherous Turn refers to the scenario where an artificial intelligence (AI) agent subtly, and perhaps covertly, learns to perform a behavior that benefits itself but is deemed undesirable and potentially harmful to a human supervisor. During training, the agent learns to behave as expected by the human supervisor, but when deployed to perform its task, it performs an alternate behavior without the supervisor there to prevent it. Initial experiments applying DRL to an implementation of the A Link to the Past example do not produce the treacherous turn effect naturally, despite various modifications to the environment intended to produce it. However, in this work, we find the treacherous behavior to be reproducible in a DRL agent when using other trojan injection strategies. This approach deviates from the prototypical treacherous turn behavior since the behavior is explicitly trained into the agent, rather than occurring as an emergent consequence of environmental complexity or poor objective specification. Nonetheless, these experiments provide new insights into the challenges of producing agents capable of true treacherous turn behavior.\n        △ Less",
        "authors":[
            "Chace Ashcraft",
            "Kiran Karra",
            "Josh Carney",
            "Nathan Drenkow"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
        "abstract":"Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https:\/\/agent-reward-bench.github.io\n        △ Less",
        "authors":[
            "Xing Han Lù",
            "Amirhossein Kazemnejad",
            "Nicholas Meade",
            "Arkil Patel",
            "Dongchan Shin",
            "Alejandra Zambrano",
            "Karolina Stańczak",
            "Peter Shaw",
            "Christopher J. Pal",
            "Siva Reddy"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Combining Forecasts using Meta-Learning: A Comparative Study for Complex Seasonality",
        "abstract":"In this paper, we investigate meta-learning for combining forecasts generated by models of different types. While typical approaches for combining forecasts involve simple averaging, machine learning techniques enable more sophisticated methods of combining through meta-learning, leading to improved forecasting accuracy. We use linear regression, $k$-nearest neighbors, multilayer perceptron, random forest, and long short-term memory as meta-learners. We define global and local meta-learning variants for time series with complex seasonality and compare meta-learners on multiple forecasting problems, demonstrating their superior performance compared to simple averaging.\n        △ Less",
        "authors":[
            "Grzegorz Dudek"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rethinking Few-Shot Fusion: Granular Ball Priors Enable General-Purpose Deep Image Fusion",
        "abstract":"In image fusion tasks, due to the lack of real fused images as priors, most deep learning-based fusion methods obtain global weight features from original images in large-scale data pairs to generate images that approximate real fused images. However, unlike previous studies, this paper utilizes Granular Ball adaptation to extract features in the brightness space as priors for deep networks, enabling the fusion network to converge quickly and complete the fusion task. This leads to few-shot training for a general image fusion network, and based on this, we propose the GBFF fusion method. According to the information expression division of pixel pairs in the original fused image, we classify pixel pairs with significant performance as the positive domain and non-significant pixel pairs as the boundary domain. We perform split inference in the brightness space using Granular Ball adaptation to compute weights for pixels that express information to varying degrees, generating approximate supervision images that provide priors for the neural network in the structural brightness space. Additionally, the extracted global saliency features also adaptively provide priors for setting the loss function weights of each image in the network, guiding the network to converge quickly at both global and pixel levels alongside the supervised images, thereby enhancing the expressiveness of the fused images. Each modality only used 10 pairs of images as the training set, completing the fusion task with a limited number of iterations. Experiments validate the effectiveness of the algorithm and theory, and qualitative and quantitative comparisons with SOTA methods show that this approach is highly competitive in terms of fusion time and image expressiveness.\n        △ Less",
        "authors":[
            "Minjie Deng",
            "Yan Wei",
            "Hao Zhai",
            "An Wu",
            "Yuncan Ouyang",
            "Qianyao Peng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Long Context In-Context Compression by Getting to the Gist of Gisting",
        "abstract":"Long context processing is critical for the adoption of LLMs, but existing methods often introduce architectural complexity that hinders their practical adoption. Gisting, an in-context compression method with no architectural modification to the decoder transformer, is a promising approach due to its simplicity and compatibility with existing frameworks. While effective for short instructions, we demonstrate that gisting struggles with longer contexts, with significant performance drops even at minimal compression rates. Surprisingly, a simple average pooling baseline consistently outperforms gisting. We analyze the limitations of gisting, including information flow interruptions, capacity limitations and the inability to restrict its attention to subsets of the context. Motivated by theoretical insights into the performance gap between gisting and average pooling, and supported by extensive experimentation, we propose GistPool, a new in-context compression method. GistPool preserves the simplicity of gisting, while significantly boosting its performance on long context compression tasks.\n        △ Less",
        "authors":[
            "Aleksandar Petrov",
            "Mark Sandler",
            "Andrey Zhmoginov",
            "Nolan Miller",
            "Max Vladymyrov"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Adaptive Vector Index Partitioning Scheme for Low-Latency RAG Pipeline",
        "abstract":"Retrieval Augmented Generation (RAG) systems enhance response quality by integrating Large Language Models (LLMs) with vector databases, enabling external knowledge retrieval to support language model reasoning. While RAG enables efficient question answering with smaller LLMs, existing optimizations for vector search and LLM serving have largely been developed in isolation. As a result, their integration often leads to suboptimal end-to-end performance. ... This paper introduces VectorLiteRAG, an optimized vector index partitioning mechanism designed for RAG systems that enhances the responsiveness of the system by jointly optimizing vector search and LLM serving across CPU and GPU system. A key challenge is to determine which indices and how much of the vector index should reside on the GPU and adjusting LLM batch sizes to balance the pipeline for lower Time-To-First-Token (TTFT) and meeting user-defined Service-Level Objectives (SLOs). To address this, we leverage the insight that cluster access in vector databases exhibits access skew, where a subset of clusters are queried significantly more frequently than others. VectorLiteRAG exploits this property through an optimized memory distribution strategy, dynamically allocating the minimum number of vector indices corresponding to frequently accessed clusters onto the GPU HBM to ensure a balanced pipeline with the LLM for high responsiveness. This adaptive partitioning scheme is guided by a statistical model that informs memory allocation and workload distribution. Our evaluation demonstrates that VectorLiteRAG improves vector search responsiveness by 2x, significantly reduces end-to-end TTFT in RAG systems by intelligently balancing memory resources between vector search and LLM execution.\n        △ Less",
        "authors":[
            "Junkyum Kim",
            "Divya Mahajan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Are We Merely Justifying Results ex Post Facto? Quantifying Explanatory Inversion in Post-Hoc Model Explanations",
        "abstract":"Post-hoc explanation methods provide interpretation by attributing predictions to input features. Natural explanations are expected to interpret how the inputs lead to the predictions. Thus, a fundamental question arises: Do these explanations unintentionally reverse the natural relationship between inputs and outputs? Specifically, are the explanations rationalizing predictions from the output rather than reflecting the true decision process? To investigate such explanatory inversion, we propose Inversion Quantification (IQ), a framework that quantifies the degree to which explanations rely on outputs and deviate from faithful input-output relationships. Using the framework, we demonstrate on synthetic datasets that widely used methods such as LIME and SHAP are prone to such inversion, particularly in the presence of spurious correlations, across tabular, image, and text domains. Finally, we propose Reproduce-by-Poking (RBP), a simple and model-agnostic enhancement to post-hoc explanation methods that integrates forward perturbation checks. We further show that under the IQ framework, RBP theoretically guarantees the mitigation of explanatory inversion. Empirically, for example, on the synthesized data, RBP can reduce the inversion by 1.8% on average across iconic post-hoc explanation approaches and domains.\n        △ Less",
        "authors":[
            "Zhen Tan",
            "Song Wang",
            "Yifan Li",
            "Yu Kong",
            "Jundong Li",
            "Tianlong Chen",
            "Huan Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Improving the evaluation of samplers on multi-modal targets",
        "abstract":"Addressing multi-modality constitutes one of the major challenges of sampling. In this reflection paper, we advocate for a more systematic evaluation of samplers towards two sources of difficulty that are mode separation and dimension. For this, we propose a synthetic experimental setting that we illustrate on a selection of samplers, focusing on the challenging criterion of recovery of the mode relative importance. These evaluations are crucial to diagnose the potential of samplers to handle multi-modality and therefore to drive progress in the field.\n        △ Less",
        "authors":[
            "Louis Grenioux",
            "Maxence Noble",
            "Marylou Gabrié"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"HyperCore: The Core Framework for Building Hyperbolic Foundation Models with Comprehensive Modules",
        "abstract":"Hyperbolic neural networks have emerged as a powerful tool for modeling hierarchical data across diverse modalities. Recent studies show that token distributions in foundation models exhibit scale-free properties, suggesting that hyperbolic space is a more suitable ambient space than Euclidean space for many pre-training and downstream tasks. However, existing tools lack essential components for building hyperbolic foundation models, making it difficult to leverage recent advancements. We introduce HyperCore, a comprehensive open-source framework that provides core modules for constructing hyperbolic foundation models across multiple modalities. HyperCore's modules can be effortlessly combined to develop novel hyperbolic foundation models, eliminating the need to extensively modify Euclidean modules from scratch and possible redundant research efforts. To demonstrate its versatility, we build and test the first fully hyperbolic vision transformers (LViT) with a fine-tuning pipeline, the first fully hyperbolic multimodal CLIP model (L-CLIP), and a hybrid Graph RAG with a hyperbolic graph encoder. Our experiments demonstrate that LViT outperforms its Euclidean counterpart. Additionally, we benchmark and reproduce experiments across hyperbolic GNNs, CNNs, Transformers, and vision Transformers to highlight HyperCore's advantages.\n        △ Less",
        "authors":[
            "Neil He",
            "Menglin Yang",
            "Rex Ying"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hybrid AI-Physical Modeling for Penetration Bias Correction in X-band InSAR DEMs: A Greenland Case Study",
        "abstract":"Digital elevation models derived from Interferometric Synthetic Aperture Radar (InSAR) data over glacial and snow-covered regions often exhibit systematic elevation errors, commonly termed \"penetration bias.\" We leverage existing physics-based models and propose an integrated correction framework that combines parametric physical modeling with machine learning. We evaluate the approach across three distinct training scenarios - each defined by a different set of acquisition parameters - to assess overall performance and the model's ability to generalize. Our experiments on Greenland's ice sheet using TanDEM-X data show that the proposed hybrid model corrections significantly reduce the mean and standard deviation of DEM errors compared to a purely physical modeling baseline. The hybrid framework also achieves significantly improved generalization than a pure ML approach when trained on data with limited diversity in acquisition parameters.\n        △ Less",
        "authors":[
            "Islam Mansour",
            "Georg Fischer",
            "Ronny Haensch",
            "Irena Hajnsek"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
        "abstract":"Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce Laplacian Pyramid Warping, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams (arXiv:2311.17919) to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions.\n        △ Less",
        "authors":[
            "Pascal Chang",
            "Sergio Sancho",
            "Jingwei Tang",
            "Markus Gross",
            "Vinicius C. Azevedo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Toward Spiking Neural Network Local Learning Modules Resistant to Adversarial Attacks",
        "abstract":"Recent research has shown the vulnerability of Spiking Neural Networks (SNNs) under adversarial examples that are nearly indistinguishable from clean data in the context of frame-based and event-based information. The majority of these studies are constrained in generating adversarial examples using Backpropagation Through Time (BPTT), a gradient-based method which lacks biological plausibility. In contrast, local learning methods, which relax many of BPTT's constraints, remain under-explored in the context of adversarial attacks. To address this problem, we examine adversarial robustness in SNNs through the framework of four types of training algorithms. We provide an in-depth analysis of the ineffectiveness of gradient-based adversarial attacks to generate adversarial instances in this scenario. To overcome these limitations, we introduce a hybrid adversarial attack paradigm that leverages the transferability of adversarial instances. The proposed hybrid approach demonstrates superior performance, outperforming existing adversarial attack methods. Furthermore, the generalizability of the method is assessed under multi-step adversarial attacks, adversarial attacks in black-box FGSM scenarios, and within the non-spiking domain.\n        △ Less",
        "authors":[
            "Jiaqi Lin",
            "Abhronil Sengupta"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Position: Beyond Euclidean -- Foundation Models Should Embrace Non-Euclidean Geometries",
        "abstract":"In the era of foundation models and Large Language Models (LLMs), Euclidean space has been the de facto geometric setting for machine learning architectures. However, recent literature has demonstrated that this choice comes with fundamental limitations. At a large scale, real-world data often exhibit inherently non-Euclidean structures, such as multi-way relationships, hierarchies, symmetries, and non-isotropic scaling, in a variety of domains, such as languages, vision, and the natural sciences. It is challenging to effectively capture these structures within the constraints of Euclidean spaces. This position paper argues that moving beyond Euclidean geometry is not merely an optional enhancement but a necessity to maintain the scaling law for the next-generation of foundation models. By adopting these geometries, foundation models could more efficiently leverage the aforementioned structures. Task-aware adaptability that dynamically reconfigures embeddings to match the geometry of downstream applications could further enhance efficiency and expressivity. Our position is supported by a series of theoretical and empirical investigations of prevalent foundation models.Finally, we outline a roadmap for integrating non-Euclidean geometries into foundation models, including strategies for building geometric foundation models via fine-tuning, training from scratch, and hybrid approaches.\n        △ Less",
        "authors":[
            "Neil He",
            "Jiahong Liu",
            "Buze Zhang",
            "Ngoc Bui",
            "Ali Maatouk",
            "Menglin Yang",
            "Irwin King",
            "Melanie Weber",
            "Rex Ying"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Knowledge Graph-extended Retrieval Augmented Generation for Question Answering",
        "abstract":"Large Language Models (LLMs) and Knowledge Graphs (KGs) offer a promising approach to robust and explainable Question Answering (QA). While LLMs excel at natural language understanding, they suffer from knowledge gaps and hallucinations. KGs provide structured knowledge but lack natural language interaction. Ideally, an AI system should be both robust to missing facts as well as easy to communicate with. This paper proposes such a system that integrates LLMs and KGs without requiring training, ensuring adaptability across different KGs with minimal human effort. The resulting approach can be classified as a specific form of a Retrieval Augmented Generation (RAG) with a KG, thus, it is dubbed Knowledge Graph-extended Retrieval Augmented Generation (KG-RAG). It includes a question decomposition module to enhance multi-hop information retrieval and answer explainability. Using In-Context Learning (ICL) and Chain-of-Thought (CoT) prompting, it generates explicit reasoning chains processed separately to improve truthfulness. Experiments on the MetaQA benchmark show increased accuracy for multi-hop questions, though with a slight trade-off in single-hop performance compared to LLM with KG baselines. These findings demonstrate KG-RAG's potential to improve transparency in QA by bridging unstructured language understanding with structured knowledge retrieval.\n        △ Less",
        "authors":[
            "Jasper Linders",
            "Jakub M. Tomczak"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The SERENADE project: Sensor-Based Explainable Detection of Cognitive Decline",
        "abstract":"Mild Cognitive Impairment (MCI) affects 12-18% of individuals over 60. MCI patients exhibit cognitive dysfunctions without significant daily functional loss. While MCI may progress to dementia, predicting this transition remains a clinical challenge due to limited and unreliable indicators. Behavioral changes, like in the execution of Activities of Daily Living (ADLs), can signal such progression. Sensorized smart homes and wearable devices offer an innovative solution for continuous, non-intrusive monitoring ADLs for MCI patients. However, current machine learning models for detecting behavioral changes lack transparency, hindering clinicians' trust. This paper introduces the SERENADE project, a European Union-funded initiative that aims to detect and explain behavioral changes associated with cognitive decline using explainable AI methods. SERENADE aims at collecting one year of data from 30 MCI patients living alone, leveraging AI to support clinical decision-making and offering a new approach to early dementia detection.\n        △ Less",
        "authors":[
            "Gabriele Civitarese",
            "Michele Fiori",
            "Andrea Arighi",
            "Daniela Galimberti",
            "Graziana Florio",
            "Claudio Bettini"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DataMap: A Portable Application for Visualizing High-Dimensional Data",
        "abstract":"Motivation: The visualization and analysis of high-dimensional data are essential in biomedical research. There is a need for secure, scalable, and reproducible tools to facilitate data exploration and interpretation. Results: We introduce DataMap, a browser-based application for visualization of high-dimensional data using heatmaps, principal component analysis (PCA), and t-distributed stochastic neighbor embedding (t-SNE). DataMap runs in the web browser, ensuring data privacy while eliminating the need for installation or a server. The application has an intuitive user interface for data transformation, annotation, and generation of reproducible R code. Availability and Implementation: Freely available as a GitHub page https:\/\/gexijin.github.io\/datamap\/. The source code can be found at https:\/\/github.com\/gexijin\/datamap, and can also be installed as an R package. Contact: Xijin.Ge@sdstate.ed\n        △ Less",
        "authors":[
            "Xijin Ge"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions",
        "abstract":"Machine learning and Bayesian optimization (BO) algorithms can significantly accelerate the optimization of chemical reactions. Transfer learning can bolster the effectiveness of BO algorithms in low-data regimes by leveraging pre-existing chemical information or data outside the direct optimization task (i.e., source data). Large language models (LLMs) have demonstrated that chemical information present in foundation training data can give them utility for processing chemical data. Furthermore, they can be augmented with and help synthesize potentially multiple modalities of source chemical data germane to the optimization task. In this work, we examine how chemical information from LLMs can be elicited and used for transfer learning to accelerate the BO of reaction conditions to maximize yield. Specifically, we show that a survey-like prompting scheme and preference learning can be used to infer a utility function which models prior chemical information embedded in LLMs over a chemical parameter space; we find that the utility function shows modest correlation to true experimental measurements (yield) over the parameter space despite operating in a zero-shot setting. Furthermore, we show that the utility function can be leveraged to focus BO efforts in promising regions of the parameter space, improving the yield of the initial BO query and enhancing optimization in 4 of the 6 datasets studied. Overall, we view this work as a step towards bridging the gap between the chemistry knowledge embedded in LLMs and the capabilities of principled BO methods to accelerate reaction optimization.\n        △ Less",
        "authors":[
            "Roshan Patel",
            "Saeed Moayedpour",
            "Louis De Lescure",
            "Lorenzo Kogler-Anele",
            "Alan Cherney",
            "Sven Jager",
            "Yasser Jangjou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Personalizing Federated Learning for Hierarchical Edge Networks with Non-IID Data",
        "abstract":"Accommodating edge networks between IoT devices and the cloud server in Hierarchical Federated Learning (HFL) enhances communication efficiency without compromising data privacy. However, devices connected to the same edge often share geographic or contextual similarities, leading to varying edge-level data heterogeneity with different subsets of labels per edge, on top of device-level heterogeneity. This hierarchical non-Independent and Identically Distributed (non-IID) nature, which implies that each edge has its own optimization goal, has been overlooked in HFL research. Therefore, existing edge-accommodated HFL demonstrates inconsistent performance across edges in various hierarchical non-IID scenarios. To ensure robust performance with diverse edge-level non-IID data, we propose a Personalized Hierarchical Edge-enabled Federated Learning (PHE-FL), which personalizes each edge model to perform well on the unique class distributions specific to each edge. We evaluated PHE-FL across 4 scenarios with varying levels of edge-level non-IIDness, with extreme IoT device level non-IIDness. To accurately assess the effectiveness of our personalization approach, we deployed test sets on each edge server instead of the cloud server, and used both balanced and imbalanced test sets. Extensive experiments show that PHE-FL achieves up to 83 percent higher accuracy compared to existing federated learning approaches that incorporate edge networks, given the same number of training rounds. Moreover, PHE-FL exhibits improved stability, as evidenced by reduced accuracy fluctuations relative to the state-of-the-art FedAvg with two-level (edge and cloud) aggregation.\n        △ Less",
        "authors":[
            "Seunghyun Lee",
            "Omid Tavallaie",
            "Shuaijun Chen",
            "Kanchana Thilakarathna",
            "Suranga Seneviratne",
            "Adel Nadjaran Toosi",
            "Albert Y. Zomaya"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"In almost all shallow analytic neural network optimization landscapes, efficient minimizers have strongly convex neighborhoods",
        "abstract":"Whether or not a local minimum of a cost function has a strongly convex neighborhood greatly influences the asymptotic convergence rate of optimizers. In this article, we rigorously analyze the prevalence of this property for the mean squared error induced by shallow, 1-hidden layer neural networks with analytic activation functions when applied to regression problems. The parameter space is divided into two domains: the 'efficient domain' (all parameters for which the respective realization function cannot be generated by a network having a smaller number of neurons) and the 'redundant domain' (the remaining parameters). In almost all regression problems on the efficient domain the optimization landscape only features local minima that are strongly convex. Formally, we will show that for certain randomly picked regression problems the optimization landscape is almost surely a Morse function on the efficient domain. The redundant domain has significantly smaller dimension than the efficient domain and on this domain, potential local minima are never isolated.\n        △ Less",
        "authors":[
            "Felix Benning",
            "Steffen Dereich"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On Transfer-based Universal Attacks in Pure Black-box Setting",
        "abstract":"Despite their impressive performance, deep visual models are susceptible to transferable black-box adversarial attacks. Principally, these attacks craft perturbations in a target model-agnostic manner. However, surprisingly, we find that existing methods in this domain inadvertently take help from various priors that violate the black-box assumption such as the availability of the dataset used to train the target model, and the knowledge of the number of classes in the target model. Consequently, the literature fails to articulate the true potency of transferable black-box attacks. We provide an empirical study of these biases and propose a framework that aids in a prior-free transparent study of this paradigm. Using our framework, we analyze the role of prior knowledge of the target model data and number of classes in attack performance. We also provide several interesting insights based on our analysis, and demonstrate that priors cause overestimation in transferability scores. Finally, we extend our framework to query-based attacks. This extension inspires a novel image-blending technique to prepare data for effective surrogate model training.\n        △ Less",
        "authors":[
            "Mohammad A. A. K. Jalwana",
            "Naveed Akhtar",
            "Ajmal Mian",
            "Nazanin Rahnavard",
            "Mubarak Shah"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diachronic and synchronic variation in the performance of adaptive machine learning systems: The ethical challenges",
        "abstract":"Objectives: Machine learning (ML) has the potential to facilitate \"continual learning\" in medicine, in which an ML system continues to evolve in response to exposure to new data over time, even after being deployed in a clinical setting. In this paper, we provide a tutorial on the range of ethical issues raised by the use of such \"adaptive\" ML systems in medicine that have, thus far, been neglected in the literature.\n  Target audience: The target audiences for this tutorial are the developers of machine learning AI systems, healthcare regulators, the broader medical informatics community, and practicing clinicians.\n  Scope: Discussions of adaptive ML systems to date have overlooked the distinction between two sorts of variance that such systems may exhibit -- diachronic evolution (change over time) and synchronic variation (difference between cotemporaneous instantiations of the algorithm at different sites) -- and under-estimated the significance of the latter. We highlight the challenges that diachronic evolution and synchronic variation present for the quality of patient care, informed consent, and equity, and discuss the complex ethical trade-offs involved in the design of such systems.\n        △ Less",
        "authors":[
            "Joshua Hatherley",
            "Robert Sparrow"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Examining GPT's Capability to Generate and Map Course Concepts and Their Relationship",
        "abstract":"Extracting key concepts and their relationships from course information and materials facilitates the provision of visualizations and recommendations for learners who need to select the right courses to take from a large number of courses. However, identifying and extracting themes manually is labor-intensive and time-consuming. Previous machine learning-based methods to extract relevant concepts from courses heavily rely on detailed course materials, which necessitates labor-intensive preparation of course materials. This paper investigates the potential of LLMs such as GPT in automatically generating course concepts and their relations. Specifically, we design a suite of prompts and provide GPT with the course information with different levels of detail, thereby generating high-quality course concepts and identifying their relations. Furthermore, we comprehensively evaluate the quality of the generated concepts and relationships through extensive experiments. Our results demonstrate the viability of LLMs as a tool for supporting educational content selection and delivery.\n        △ Less",
        "authors":[
            "Tianyuan Yang",
            "Ren Baofeng",
            "Chenghao Gu",
            "Tianjia He",
            "Boxuan Ma",
            "Shinichi Konomi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ML For Hardware Design Interpretability: Challenges and Opportunities",
        "abstract":"The increasing size and complexity of machine learning (ML) models have driven the growing need for custom hardware accelerators capable of efficiently supporting ML workloads. However, the design of such accelerators remains a time-consuming process, heavily relying on engineers to manually ensure design interpretability through clear documentation and effective communication. Recent advances in large language models (LLMs) offer a promising opportunity to automate these design interpretability tasks, particularly the generation of natural language descriptions for register-transfer level (RTL) code, what we refer to as \"RTL-to-NL tasks.\" In this paper, we examine how design interpretability, particularly in RTL-to-NL tasks, influences the efficiency of the hardware design process. We review existing work adapting LLMs for these tasks, highlight key challenges that remain unaddressed, including those related to data, computation, and model development, and identify opportunities to address them. By doing so, we aim to guide future research in leveraging ML to automate RTL-to-NL tasks and improve hardware design interpretability, thereby accelerating the hardware design process and meeting the increasing demand for custom hardware accelerators in machine learning and beyond.\n        △ Less",
        "authors":[
            "Raymond Baartmans",
            "Andrew Ensinger",
            "Victor Agostinelli",
            "Lizhong Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mimic In-Context Learning for Multimodal Tasks",
        "abstract":"Recently, In-context Learning (ICL) has become a significant inference paradigm in Large Multimodal Models (LMMs), utilizing a few in-context demonstrations (ICDs) to prompt LMMs for new tasks. However, the synergistic effects in multimodal data increase the sensitivity of ICL performance to the configurations of ICDs, stimulating the need for a more stable and general mapping function. Mathematically, in Transformer-based models, ICDs act as ``shift vectors'' added to the hidden states of query tokens. Inspired by this, we introduce Mimic In-Context Learning (MimIC) to learn stable and generalizable shift effects from ICDs. Specifically, compared with some previous shift vector-based methods, MimIC more strictly approximates the shift effects by integrating lightweight learnable modules into LMMs with four key enhancements: 1) inserting shift vectors after attention layers, 2) assigning a shift vector to each attention head, 3) making shift magnitude query-dependent, and 4) employing a layer-wise alignment loss. Extensive experiments on two LMMs (Idefics-9b and Idefics2-8b-base) across three multimodal tasks (VQAv2, OK-VQA, Captioning) demonstrate that MimIC outperforms existing shift vector-based methods. The code is available at https:\/\/github.com\/Kamichanw\/MimIC.\n        △ Less",
        "authors":[
            "Yuchu Jiang",
            "Jiale Fu",
            "Chenduo Hao",
            "Xinting Hu",
            "Yingzhe Peng",
            "Xin Geng",
            "Xu Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-University: An LLM-based platform for instructional alignment to scientific classrooms",
        "abstract":"We introduce AI University (AI-U), a flexible framework for AI-driven course content delivery that adapts to instructors' teaching styles. At its core, AI-U fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to generate instructor-aligned responses from lecture videos, notes, and textbooks. Using a graduate-level finite-element-method (FEM) course as a case study, we present a scalable pipeline to systematically construct training data, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and optimize its responses through RAG-based synthesis. Our evaluation - combining cosine similarity, LLM-based assessment, and expert review - demonstrates strong alignment with course materials. We also have developed a prototype web application, available at https:\/\/my-ai-university.com, that enhances traceability by linking AI-generated responses to specific sections of the relevant course material and time-stamped instances of the open-access video lectures. Our expert model is found to have greater cosine similarity with a reference on 86% of test cases. An LLM judge also found our expert model to outperform the base Llama 3.2 model approximately four times out of five. AI-U offers a scalable approach to AI-assisted education, paving the way for broader adoption in higher education. Here, our framework has been presented in the setting of a class on FEM - a subject that is central to training PhD and Master students in engineering science. However, this setting is a particular instance of a broader context: fine-tuning LLMs to research content in science.\n        △ Less",
        "authors":[
            "Mostafa Faghih Shojaei",
            "Rahul Gulati",
            "Benjamin A. Jasperson",
            "Shangshang Wang",
            "Simone Cimolato",
            "Dangli Cao",
            "Willie Neiswanger",
            "Krishna Garikipati"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Combinatorial Interpretability of Neural Computation",
        "abstract":"We introduce combinatorial interpretability, a methodology for understanding neural computation by analyzing the combinatorial structures in the sign-based categorization of a network's weights and biases. We demonstrate its power through feature channel coding, a theory that explains how neural networks compute Boolean expressions and potentially underlies other categories of neural network computation. According to this theory, features are computed via feature channels: unique cross-neuron encodings shared among the inputs the feature operates on. Because different feature channels share neurons, the neurons are polysemantic and the channels interfere with one another, making the computation appear inscrutable.\n  We show how to decipher these computations by analyzing a network's feature channel coding, offering complete mechanistic interpretations of several small neural networks that were trained with gradient descent. Crucially, this is achieved via static combinatorial analysis of the weight matrices, without examining activations or training new autoencoding networks. Feature channel coding reframes the superposition hypothesis, shifting the focus from neuron activation directionality in high-dimensional space to the combinatorial structure of codes. It also allows us for the first time to exactly quantify and explain the relationship between a network's parameter size and its computational capacity (i.e. the set of features it can compute with low error), a relationship that is implicitly at the core of many modern scaling laws.\n  Though our initial studies of feature channel coding are restricted to Boolean functions, we believe they provide a rich, controlled, and informative research space, and that the path we propose for combinatorial interpretation of neural computation can provide a basis for understanding both artificial and biological neural circuits.\n        △ Less",
        "authors":[
            "Micah Adler",
            "Dan Alistarh",
            "Nir Shavit"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Shrinkage Estimation For Personalized Deep Kernel Regression In Modeling Brain Trajectories",
        "abstract":"Longitudinal biomedical studies monitor individuals over time to capture dynamics in brain development, disease progression, and treatment effects. However, estimating trajectories of brain biomarkers is challenging due to biological variability, inconsistencies in measurement protocols (e.g., differences in MRI scanners), scarcity, and irregularity in longitudinal measurements. Herein, we introduce a novel personalized deep kernel regression framework for forecasting brain biomarkers, with application to regional volumetric measurements. Our approach integrates two key components: a population model that captures brain trajectories from a large and diverse cohort, and a subject-specific model that captures individual trajectories. To optimally combine these, we propose Adaptive Shrinkage Estimation, which effectively balances population and subject-specific models. We assess our model's performance through predictive accuracy metrics, uncertainty quantification, and validation against external clinical studies. Benchmarking against state-of-the-art statistical and machine learning models -- including linear mixed effects models, generalized additive models, and deep learning methods -- demonstrates the superior predictive performance of our approach. Additionally, we apply our method to predict trajectories of composite neuroimaging biomarkers, which highlights the versatility of our approach in modeling the progression of longitudinal neuroimaging biomarkers. Furthermore, validation on three external neuroimaging studies confirms the robustness of our method across different clinical contexts. We make the code available at https:\/\/github.com\/vatass\/AdaptiveShrinkageDKGP.\n        △ Less",
        "authors":[
            "Vasiliki Tassopoulou",
            "Haochang Shou",
            "Christos Davatzikos"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning",
        "abstract":"Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a textual rethinking trigger to the end of initial rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse, and MathVision to achieve 80.3%, 61.8%, and 43.9% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with GPT-o1.\n        △ Less",
        "authors":[
            "Haozhe Wang",
            "Chao Qu",
            "Zuming Huang",
            "Wei Chu",
            "Fangzhen Lin",
            "Wenhu Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Double Machine Learning for Causal Inference under Shared-State Interference",
        "abstract":"Researchers and practitioners often wish to measure treatment effects in settings where units interact via markets and recommendation systems. In these settings, units are affected by certain shared states, like prices, algorithmic recommendations or social signals. We formalize this structure, calling it shared-state interference, and argue that our formulation captures many relevant applied settings. Our key modeling assumption is that individuals' potential outcomes are independent conditional on the shared state. We then prove an extension of a double machine learning (DML) theorem providing conditions for achieving efficient inference under shared-state interference. We also instantiate our general theorem in several models of interest where it is possible to efficiently estimate the average direct effect (ADE) or global average treatment effect (GATE).\n        △ Less",
        "authors":[
            "Chris Hays",
            "Manish Raghavan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Datum-wise Transformer for Synthetic Tabular Data Detection in the Wild",
        "abstract":"The growing power of generative models raises major concerns about the authenticity of published content. To address this problem, several synthetic content detection methods have been proposed for uniformly structured media such as image or text. However, little work has been done on the detection of synthetic tabular data, despite its importance in industry and government. This form of data is complex to handle due to the diversity of its structures: the number and types of the columns may vary wildly from one table to another. We tackle the tough problem of detecting synthetic tabular data ''in the wild'', i.e. when the model is deployed on table structures it has never seen before. We introduce a novel datum-wise transformer architecture and show that it outperforms existing models. Furthermore, we investigate the application of domain adaptation techniques to enhance the effectiveness of our model, thereby providing a more robust data-forgery detection solution.\n        △ Less",
        "authors":[
            "G. Charbel N. Kindji",
            "Elisa Fromont",
            "Lina Maria Rojas-Barahona",
            "Tanguy Urvoy"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PatchTrAD: A Patch-Based Transformer focusing on Patch-Wise Reconstruction Error for Time Series Anomaly Detection",
        "abstract":"Time series anomaly detection (TSAD) focuses on identifying whether observations in streaming data deviate significantly from normal patterns. With the prevalence of connected devices, anomaly detection on time series has become paramount, as it enables real-time monitoring and early detection of irregular behaviors across various application domains. In this work, we introduce PatchTrAD, a Patch-based Transformer model for time series anomaly detection. Our approach leverages a Transformer encoder along with the use of patches under a reconstructionbased framework for anomaly detection. Empirical evaluations on multiple benchmark datasets show that PatchTrAD is on par, in terms of detection performance, with state-of-the-art deep learning models for anomaly detection while being time efficient during inference.\n        △ Less",
        "authors":[
            "Samy-Melwan Vilhes",
            "Gilles Gasso",
            "Mokhtar Z Alaya"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ColonScopeX: Leveraging Explainable Expert Systems with Multimodal Data for Improved Early Diagnosis of Colorectal Cancer",
        "abstract":"Colorectal cancer (CRC) ranks as the second leading cause of cancer-related deaths and the third most prevalent malignant tumour worldwide. Early detection of CRC remains problematic due to its non-specific and often embarrassing symptoms, which patients frequently overlook or hesitate to report to clinicians. Crucially, the stage at which CRC is diagnosed significantly impacts survivability, with a survival rate of 80-95\\% for Stage I and a stark decline to 10\\% for Stage IV. Unfortunately, in the UK, only 14.4\\% of cases are diagnosed at the earliest stage (Stage I).\n  In this study, we propose ColonScopeX, a machine learning framework utilizing explainable AI (XAI) methodologies to enhance the early detection of CRC and pre-cancerous lesions. Our approach employs a multimodal model that integrates signals from blood sample measurements, processed using the Savitzky-Golay algorithm for fingerprint smoothing, alongside comprehensive patient metadata, including medication history, comorbidities, age, weight, and BMI. By leveraging XAI techniques, we aim to render the model's decision-making process transparent and interpretable, thereby fostering greater trust and understanding in its predictions. The proposed framework could be utilised as a triage tool or a screening tool of the general population.\n  This research highlights the potential of combining diverse patient data sources and explainable machine learning to tackle critical challenges in medical diagnostics.\n        △ Less",
        "authors":[
            "Natalia Sikora",
            "Robert L. Manschke",
            "Alethea M. Tang",
            "Peter Dunstan",
            "Dean A. Harris",
            "Su Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning",
        "abstract":"How to adapt a pre-trained model continuously for sequential tasks with different prediction class labels and domains and finally learn a generalizable model across diverse tasks is a long-lasting challenge. Continual learning (CL) has emerged as a promising approach to leverage pre-trained models (e.g., Transformers) for sequential tasks. While many existing CL methods incrementally store additional learned structures, such as Low-Rank Adaptation (LoRA) adapters or prompts and sometimes even preserve features from previous samples to maintain performance. This leads to unsustainable parameter growth and escalating storage costs as the number of tasks increases. Moreover, current approaches often lack task similarity awareness, which further hinders the models ability to effectively adapt to new tasks without interfering with previously acquired knowledge. To address these challenges, we propose FM-LoRA, a novel and efficient low-rank adaptation method that integrates both a dynamic rank selector (DRS) and dynamic meta-prompting (DMP). This framework allocates model capacity more effectively across tasks by leveraging a shared low-rank subspace critical for preserving knowledge, thereby avoiding continual parameter expansion. Extensive experiments on various CL benchmarks, including ImageNet-R, CIFAR100, and CUB200 for class-incremental learning (CIL), and DomainNet for domain-incremental learning (DIL), with Transformers backbone demonstrate that FM-LoRA effectively mitigates catastrophic forgetting while delivering robust performance across a diverse range of tasks and domains.\n        △ Less",
        "authors":[
            "Xiaobing Yu",
            "Jin Yang",
            "Xiao Wu",
            "Peijie Qiu",
            "Xiaofeng Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Probabilistic QoS Metric Forecasting in Delay-Tolerant Networks Using Conditional Diffusion Models on Latent Dynamics",
        "abstract":"Active QoS metric prediction, commonly employed in the maintenance and operation of DTN, could enhance network performance regarding latency, throughput, energy consumption, and dependability. Naturally formulated as a multivariate time series forecasting problem, it attracts substantial research efforts. Traditional mean regression methods for time series forecasting cannot capture the data complexity adequately, resulting in deteriorated performance in operational tasks in DTNs such as routing. This paper formulates the prediction of QoS metrics in DTN as a probabilistic forecasting problem on multivariate time series, where one could quantify the uncertainty of forecasts by characterizing the distribution of these samples. The proposed approach hires diffusion models and incorporates the latent temporal dynamics of non-stationary and multi-mode data into them. Extensive experiments demonstrate the efficacy of the proposed approach by showing that it outperforms the popular probabilistic time series forecasting methods.\n        △ Less",
        "authors":[
            "Enming Zhang",
            "Zheng Liu",
            "Yu Xiang",
            "Yanwen Qu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"From Text to Time? Rethinking the Effectiveness of the Large Language Model for Time Series Forecasting",
        "abstract":"Using pre-trained large language models (LLMs) as the backbone for time series prediction has recently gained significant research interest. However, the effectiveness of LLM backbones in this domain remains a topic of debate. Based on thorough empirical analyses, we observe that training and testing LLM-based models on small datasets often leads to the Encoder and Decoder becoming overly adapted to the dataset, thereby obscuring the true predictive capabilities of the LLM backbone. To investigate the genuine potential of LLMs in time series prediction, we introduce three pre-training models with identical architectures but different pre-training strategies. Thereby, large-scale pre-training allows us to create unbiased Encoder and Decoder components tailored to the LLM backbone. Through controlled experiments, we evaluate the zero-shot and few-shot prediction performance of the LLM, offering insights into its capabilities. Extensive experiments reveal that although the LLM backbone demonstrates some promise, its forecasting performance is limited. Our source code is publicly available in the anonymous repository: https:\/\/anonymous.4open.science\/r\/LLM4TS-0B5C.\n        △ Less",
        "authors":[
            "Xinyu Zhang",
            "Shanshan Feng",
            "Xutao Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring utilization of generative AI for research and education in data-driven materials science",
        "abstract":"Generative AI has recently had a profound impact on various fields, including daily life, research, and education. To explore its efficient utilization in data-driven materials science, we organized a hackathon -- AIMHack2024 -- in July 2024. In this hackathon, researchers from fields such as materials science, information science, bioinformatics, and condensed matter physics worked together to explore how generative AI can facilitate research and education. Based on the results of the hackathon, this paper presents topics related to (1) conducting AI-assisted software trials, (2) building AI tutors for software, and (3) developing GUI applications for software. While generative AI continues to evolve rapidly, this paper provides an early record of its application in data-driven materials science and highlights strategies for integrating AI into research and education.\n        △ Less",
        "authors":[
            "Takahiro Misawa",
            "Ai Koizumi",
            "Ryo Tamura",
            "Kazuyoshi Yoshimi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Graph-Enhanced DeepONet Approach for Real-Time Estimating Hydrogen-Enriched Natural Gas Flow under Variable Operations",
        "abstract":"Blending green hydrogen into natural gas presents a promising approach for renewable energy integration and fuel decarbonization. Accurate estimation of hydrogen fraction in hydrogen-enriched natural gas (HENG) pipeline networks is crucial for operational safety and efficiency, yet it remains challenging due to complex dynamics. While existing data-driven approaches adopt end-to-end architectures for HENG flow state estimation, their limited adaptability to varying operational conditions hinders practical applications. To this end, this study proposes a graph-enhanced DeepONet framework for the real-time estimation of HENG flow, especially hydrogen fractions. First, a dual-network architecture, called branch network and trunk network, is employed to characterize operational conditions and sparse sensor measurements to estimate the HENG state at targeted locations and time points. Second, a graph-enhance branch network is proposed to incorporate pipeline topology, improving the estimation accuracy in large-scale pipeline networks. Experimental results demonstrate that the proposed method achieves superior estimation accuracy for HCNG flow under varying operational conditions compared to conventional approaches.\n        △ Less",
        "authors":[
            "Sicheng Liu",
            "Hongchang Huang",
            "Bo Yang",
            "Mingxuan Cai",
            "Xu Yang",
            "Xinping Guan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"When Federated Learning Meets Quantum Computing: Survey and Research Opportunities",
        "abstract":"Quantum Federated Learning (QFL) is an emerging field that harnesses advances in Quantum Computing (QC) to improve the scalability and efficiency of decentralized Federated Learning (FL) models. This paper provides a systematic and comprehensive survey of the emerging problems and solutions when FL meets QC, from research protocol to a novel taxonomy, particularly focusing on both quantum and federated limitations, such as their architectures, Noisy Intermediate Scale Quantum (NISQ) devices, and privacy preservation, so on. This work explores key developments and integration strategies, along with the impact of quantum computing on FL, keeping a sharp focus on hybrid quantum-classical approaches. The paper offers an in-depth understanding of how the strengths of QC, such as gradient hiding, state entanglement, quantum key distribution, quantum security, and quantum-enhanced differential privacy, have been integrated into FL to ensure the privacy of participants in an enhanced, fast, and secure framework. Finally, this study proposes potential future directions to address the identified research gaps and challenges, aiming to inspire faster and more secure QFL models for practical use.\n        △ Less",
        "authors":[
            "Aakar Mathur",
            "Ashish Gupta",
            "Sajal K. Das"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SafeMLRM: Demystifying Safety in Multi-modal Large Reasoning Models",
        "abstract":"The rapid advancement of multi-modal large reasoning models (MLRMs) -- enhanced versions of multimodal language models (MLLMs) equipped with reasoning capabilities -- has revolutionized diverse applications. However, their safety implications remain underexplored. While prior work has exposed critical vulnerabilities in unimodal reasoning models, MLRMs introduce distinct risks from cross-modal reasoning pathways. This work presents the first systematic safety analysis of MLRMs through large-scale empirical studies comparing MLRMs with their base MLLMs. Our experiments reveal three critical findings: (1) The Reasoning Tax: Acquiring reasoning capabilities catastrophically degrades inherited safety alignment. MLRMs exhibit 37.44% higher jailbreaking success rates than base MLLMs under adversarial attacks. (2) Safety Blind Spots: While safety degradation is pervasive, certain scenarios (e.g., Illegal Activity) suffer 25 times higher attack rates -- far exceeding the average 3.4 times increase, revealing scenario-specific vulnerabilities with alarming cross-model and datasets consistency. (3) Emergent Self-Correction: Despite tight reasoning-answer safety coupling, MLRMs demonstrate nascent self-correction -- 16.9% of jailbroken reasoning steps are overridden by safe answers, hinting at intrinsic safeguards. These findings underscore the urgency of scenario-aware safety auditing and mechanisms to amplify MLRMs' self-correction potential. To catalyze research, we open-source OpenSafeMLRM, the first toolkit for MLRM safety evaluation, providing unified interface for mainstream models, datasets, and jailbreaking methods. Our work calls for immediate efforts to harden reasoning-augmented AI, ensuring its transformative potential aligns with ethical safeguards.\n        △ Less",
        "authors":[
            "Junfeng Fang",
            "Yukai Wang",
            "Ruipeng Wang",
            "Zijun Yao",
            "Kun Wang",
            "An Zhang",
            "Xiang Wang",
            "Tat-Seng Chua"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Mechanistic Anomaly Detection for \"Quirky\" Language Models",
        "abstract":"As LLMs grow in capability, the task of supervising LLMs becomes more challenging. Supervision failures can occur if LLMs are sensitive to factors that supervisors are unaware of. We investigate Mechanistic Anomaly Detection (MAD) as a technique to augment supervision of capable models; we use internal model features to identify anomalous training signals so they can be investigated or discarded. We train detectors to flag points from the test environment that differ substantially from the training environment, and experiment with a large variety of detector features and scoring rules to detect anomalies in a set of ``quirky'' language models. We find that detectors can achieve high discrimination on some tasks, but no detector is effective across all models and tasks. MAD techniques may be effective in low-stakes applications, but advances in both detection and evaluation are likely needed if they are to be used in high stakes settings.\n        △ Less",
        "authors":[
            "David O. Johnston",
            "Arkajyoti Chakraborty",
            "Nora Belrose"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Analogical Learning for Cross-Scenario Generalization: Framework and Application to Intelligent Localization",
        "abstract":"Existing learning models often exhibit poor generalization when deployed across diverse scenarios. It is mainly due to that the underlying reference frame of the data varies with the deployment environment and settings. However, despite the data of each scenario has its distinct reference frame, its generation generally follows the same underlying physical rule. Based on these findings, this article proposes a brand-new universal deep learning framework named analogical learning (AL), which provides a highly efficient way to implicitly retrieve the reference frame information associated with a scenario and then to make accurate prediction by relative analogy across scenarios. Specifically, an elegant bipartite neural network architecture called Mateformer is designed, the first part of which calculates the relativity within multiple feature spaces between the input data and a small amount of embedded data from the current scenario, while the second part uses these relativity to guide the nonlinear analogy. We apply AL to the typical multi-scenario learning problem of intelligent wireless localization in cellular networks. Extensive experiments show that AL achieves state-of-the-art accuracy, stable transferability and robust adaptation to new scenarios without any tuning, and outperforming conventional methods with a precision improvement of nearly two orders of magnitude. All data and code are available at https:\/\/github.com\/ziruichen-research\/ALLoc.\n        △ Less",
        "authors":[
            "Zirui Chen",
            "Zhaoyang Zhang",
            "Ziqing Xing",
            "Ridong Li",
            "Zhaohui Yang",
            "Richeng Jin",
            "Chongwen Huang",
            "Yuzhi Yang",
            "Mérouane Debbah"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PriM: Principle-Inspired Material Discovery through Multi-Agent Collaboration",
        "abstract":"Complex chemical space and limited knowledge scope with biases holds immense challenge for human scientists, yet in automated materials discovery. Existing intelligent methods relies more on numerical computation, leading to inefficient exploration and results with hard-interpretability. To bridge this gap, we introduce a principles-guided material discovery system powered by language inferential multi-agent system (MAS), namely PriM. Our framework integrates automated hypothesis generation with experimental validation in a roundtable system of MAS, enabling systematic exploration while maintaining scientific rigor. Based on our framework, the case study of nano helix demonstrates higher materials exploration rate and property value while providing transparent reasoning pathways. This approach develops an automated-and-transparent paradigm for material discovery, with broad implications for rational design of functional materials. Code is publicly available at our \\href{https:\/\/github.com\/amair-lab\/PriM}{GitHub}.\n        △ Less",
        "authors":[
            "Zheyuan Lai",
            "Yingming Pu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models",
        "abstract":"Although multimodal large language models (MLLMs) exhibit remarkable reasoning capabilities on complex multimodal understanding tasks, they still suffer from the notorious hallucination issue: generating outputs misaligned with obvious visual or factual evidence. Currently, training-based solutions, like direct preference optimization (DPO), leverage paired preference data to suppress hallucinations. However, they risk sacrificing general reasoning capabilities due to the likelihood displacement. Meanwhile, training-free solutions, like contrastive decoding, achieve this goal by subtracting the estimated hallucination pattern from a distorted input. Yet, these handcrafted perturbations (e.g., add noise to images) may poorly capture authentic hallucination patterns. To avoid these weaknesses of existing methods, and realize robust hallucination mitigation (i.e., maintaining general reasoning performance), we propose a novel framework: Decoupling Contrastive Decoding (DCD). Specifically, DCD decouples the learning of positive and negative samples in preference datasets, and trains separate positive and negative image projections within the MLLM. The negative projection implicitly models real hallucination patterns, which enables vision-aware negative images in the contrastive decoding inference stage. Our DCD alleviates likelihood displacement by avoiding pairwise optimization and generalizes robustly without handcrafted degradation. Extensive ablations across hallucination benchmarks and general reasoning tasks demonstrate the effectiveness of DCD, i.e., it matches DPO's hallucination suppression while preserving general capabilities and outperforms the handcrafted contrastive decoding methods.\n        △ Less",
        "authors":[
            "Wei Chen",
            "Xin Yan",
            "Bin Wen",
            "Fan Yang",
            "Tingting Gao",
            "Di Zhang",
            "Long Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generative AI in Live Operations: Evidence of Productivity Gains in Cybersecurity and Endpoint Management",
        "abstract":"We measure the association between generative AI (GAI) tool adoption and four metrics spanning security operations, information protection, and endpoint management: 1) number of security alerts per incident, 2) probability of security incident reopenings, 3) time to classify a data loss prevention alert, and 4) time to resolve device policy conflicts. We find that GAI is associated with robust and statistically and practically significant improvements in the four metrics. Although unobserved confounders inhibit causal identification, these results are among the first to use observational data from live operations to investigate the relationship between GAI adoption and security operations, data loss prevention, and device policy management.\n        △ Less",
        "authors":[
            "James Bono",
            "Justin Grana",
            "Kleanthis Karakolios",
            "Pruthvi Hanumanthapura Ramakrishna",
            "Ankit Srivastava"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Estimating Item Difficulty Using Large Language Models and Tree-Based Machine Learning Algorithms",
        "abstract":"Estimating item difficulty through field-testing is often resource-intensive and time-consuming. As such, there is strong motivation to develop methods that can predict item difficulty at scale using only the item content. Large Language Models (LLMs) represent a new frontier for this goal. The present research examines the feasibility of using an LLM to predict item difficulty for K-5 mathematics and reading assessment items (N = 5170). Two estimation approaches were implemented: (a) a direct estimation method that prompted the LLM to assign a single difficulty rating to each item, and (b) a feature-based strategy where the LLM extracted multiple cognitive and linguistic features, which were then used in ensemble tree-based models (random forests and gradient boosting) to predict difficulty. Overall, direct LLM estimates showed moderate to strong correlations with true item difficulties. However, their accuracy varied by grade level, often performing worse for early grades. In contrast, the feature-based method yielded stronger predictive accuracy, with correlations as high as r = 0.87 and lower error estimates compared to both direct LLM predictions and baseline regressors. These findings highlight the promise of LLMs in streamlining item development and reducing reliance on extensive field testing and underscore the importance of structured feature extraction. We provide a seven-step workflow for testing professionals who would want to implement a similar item difficulty estimation approach with their item pool.\n        △ Less",
        "authors":[
            "Pooya Razavi",
            "Sonya J. Powers"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A temporal scale transformer framework for precise remaining useful life prediction in fuel cells",
        "abstract":"In exploring Predictive Health Management (PHM) strategies for Proton Exchange Membrane Fuel Cells (PEMFC), the Transformer model, widely used in data-driven approaches, excels in many fields but struggles with time series analysis due to its self-attention mechanism, which yields a complexity of the input sequence squared and low computational efficiency. It also faces challenges in capturing both global long-term dependencies and local details effectively. To tackle this, we propose the Temporal Scale Transformer (TSTransformer), an enhanced version of the inverted Transformer (iTransformer). Unlike traditional Transformers that treat each timestep as an input token, TSTransformer maps sequences of varying lengths into tokens at different stages for inter-sequence modeling, using attention to capture multivariate correlations and feed-forward networks (FFN) to encode sequence representations. By integrating a one-dimensional convolutional layer into the multivariate attention for multi-level scaling of K and V matrices, it improves local feature extraction, captures temporal scale characteristics, and reduces token count and computational costs. Experiments comparing TSTransformer with models like Long Short-Term Memory, iTransformer, and Transformer demonstrate its potential as a powerful tool for advancing PHM in renewable energy, effectively addressing the limitations of pure Transformer models in data-driven time series tasks.\n        △ Less",
        "authors":[
            "Zezhi Tang",
            "Xiaoyu Chen",
            "Xin Jin",
            "Benyuan Zhang",
            "Wenyu Liang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"InfoGain Wavelets: Furthering the Design of Diffusion Wavelets for Graph-Structured Data",
        "abstract":"Diffusion wavelets extract information from graph signals at different scales of resolution by utilizing graph diffusion operators raised to various powers, known as diffusion scales. Traditionally, the diffusion scales are chosen to be dyadic integers, $\\mathbf{2^j}$. Here, we propose a novel, unsupervised method for selecting the diffusion scales based on ideas from information theory. We then show that our method can be incorporated into wavelet-based GNNs via graph classification experiments.\n        △ Less",
        "authors":[
            "David R. Johnson",
            "Smita Krishnaswamy",
            "Michael Perlmutter"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Learnable Multi-Scale Wavelet Transformer: A Novel Alternative to Self-Attention",
        "abstract":"Transformer architectures, underpinned by the self-attention mechanism, have achieved state-of-the-art results across numerous natural language processing (NLP) tasks by effectively modeling long-range dependencies. However, the computational complexity of self-attention, scaling quadratically with input sequence length, presents significant challenges for processing very long sequences or operating under resource constraints. This paper introduces the Learnable Multi-Scale Wavelet Transformer (LMWT), a novel architecture that replaces the standard dot-product self-attention with a learnable multi-scale Haar wavelet transform module. Leveraging the intrinsic multi-resolution properties of wavelets, the LMWT efficiently captures both local details and global context. Crucially, the parameters of the wavelet transform, including scale-specific coefficients, are learned end-to-end during training, allowing the model to adapt its decomposition strategy to the data and task. We present the detailed mathematical formulation of the learnable Haar wavelet module and its integration into the transformer framework, supplemented by an architectural diagram. We conduct a comprehensive experimental evaluation on a standard machine translation benchmark (WMT16 En-De), comparing the LMWT against a baseline self-attention transformer using metrics like BLEU score, perplexity, and token accuracy. Furthermore, we analyze the computational complexity, highlighting the linear scaling of our approach, discuss its novelty in the context of related work, and explore the interpretability offered by visualizing the learned Haar coefficients. Our results indicate that the LMWT achieves competitive performance while offering substantial computational advantages, positioning it as a promising and novel alternative for efficient sequence modeling.\n        △ Less",
        "authors":[
            "Andrew Kiruluta",
            "Priscilla Burity",
            "Samantha Williams"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Exploring Gradient-Guided Masked Language Model to Detect Textual Adversarial Attacks",
        "abstract":"Textual adversarial examples pose serious threats to the reliability of natural language processing systems. Recent studies suggest that adversarial examples tend to deviate from the underlying manifold of normal texts, whereas pre-trained masked language models can approximate the manifold of normal data. These findings inspire the exploration of masked language models for detecting textual adversarial attacks. We first introduce Masked Language Model-based Detection (MLMD), leveraging the mask and unmask operations of the masked language modeling (MLM) objective to induce the difference in manifold changes between normal and adversarial texts. Although MLMD achieves competitive detection performance, its exhaustive one-by-one masking strategy introduces significant computational overhead. Our posterior analysis reveals that a significant number of non-keywords in the input are not important for detection but consume resources. Building on this, we introduce Gradient-guided MLMD (GradMLMD), which leverages gradient information to identify and skip non-keywords during detection, significantly reducing resource consumption without compromising detection performance.\n        △ Less",
        "authors":[
            "Xiaomei Zhang",
            "Zhaoxi Zhang",
            "Yanjun Zhang",
            "Xufei Zheng",
            "Leo Yu Zhang",
            "Shengshan Hu",
            "Shirui Pan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SLOs-Serve: Optimized Serving of Multi-SLO LLMs",
        "abstract":"This paper introduces SLOs-Serve, a system designed for serving multi-stage large language model (LLM) requests with application- and stage-specific service level objectives (SLOs). The key idea behind SLOs-Serve is to customize the allocation of tokens to meet these SLO requirements. SLOs-Serve uses a multi-SLO dynamic programming-based algorithm to continuously optimize token allocations under SLO constraints by exploring the full design space of chunked prefill and (optional) speculative decoding. Leveraging this resource planning algorithm, SLOs-Serve effectively supports multi-SLOs and multi-replica serving with dynamic request routing while being resilient to bursty arrivals. Our evaluation across 6 LLM application scenarios (including summarization, coding, chatbot, tool calling, and reasoning) demonstrates that SLOs-Serve improves per-GPU serving capacity by 2.2x on average compared to prior state-of-the-art systems.\n        △ Less",
        "authors":[
            "Siyuan Chen",
            "Zhipeng Jia",
            "Samira Khan",
            "Arvind Krishnamurthy",
            "Phillip B. Gibbons"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Embedding Hidden Adversarial Capabilities in Pre-Trained Diffusion Models",
        "abstract":"We introduce a new attack paradigm that embeds hidden adversarial capabilities directly into diffusion models via fine-tuning, without altering their observable behavior or requiring modifications during inference. Unlike prior approaches that target specific images or adjust the generation process to produce adversarial outputs, our method integrates adversarial functionality into the model itself. The resulting tampered model generates high-quality images indistinguishable from those of the original, yet these images cause misclassification in downstream classifiers at a high rate. The misclassification can be targeted to specific output classes. Users can employ this compromised model unaware of its embedded adversarial nature, as it functions identically to a standard diffusion model. We demonstrate the effectiveness and stealthiness of our approach, uncovering a covert attack vector that raises new security concerns. These findings expose a risk arising from the use of externally-supplied models and highlight the urgent need for robust model verification and defense mechanisms against hidden threats in generative models. The code is available at https:\/\/github.com\/LucasBeerens\/CRAFTed-Diffusion .\n        △ Less",
        "authors":[
            "Lucas Beerens",
            "Desmond J. Higham"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How Relevance Emerges: Interpreting LoRA Fine-Tuning in Reranking LLMs",
        "abstract":"We conduct a behavioral exploration of LoRA fine-tuned LLMs for Passage Reranking to understand how relevance signals are learned and deployed by Large Language Models. By fine-tuning Mistral-7B, LLaMA3.1-8B, and Pythia-6.9B on MS MARCO under diverse LoRA configurations, we investigate how relevance modeling evolves across checkpoints, the impact of LoRA rank (1, 2, 8, 32), and the relative importance of updated MHA vs. MLP components. Our ablations reveal which layers and projections within LoRA transformations are most critical for reranking accuracy. These findings offer fresh explanations into LoRA's adaptation mechanisms, setting the stage for deeper mechanistic studies in Information Retrieval. All models used in this study have been shared.\n        △ Less",
        "authors":[
            "Atharva Nijasure",
            "Tanya Chowdhury",
            "James Allan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics",
        "abstract":"With the shift from traditional to digital media, the online landscape now hosts not only reliable news articles but also a significant amount of unreliable content. Digital media has faster reachability by significantly influencing public opinion and advancing political agendas. While newspaper readers may be familiar with their preferred outlets political leanings or credibility, determining unreliable news articles is much more challenging. The credibility of many online sources is often opaque, with AI generated content being easily disseminated at minimal cost. Unreliable news articles, particularly those that followed the Russian invasion of Ukraine in 2022, closely mimic the topics and writing styles of credible sources, making them difficult to distinguish. To address this, we introduce SemCAFE, a system designed to detect news reliability by incorporating entity relatedness into its assessment. SemCAFE employs standard Natural Language Processing techniques, such as boilerplate removal and tokenization, alongside entity level semantic analysis using the YAGO knowledge base. By creating a semantic fingerprint for each news article, SemCAFE could assess the credibility of 46,020 reliable and 3,407 unreliable articles on the 2022 Russian invasion of Ukraine. Our approach improved the macro F1 score by 12% over state of the art methods. The sample data and code are available on GitHub\n        △ Less",
        "authors":[
            "Gautam Kishore Shahi",
            "Oshani Seneviratne",
            "Marc Spaniol"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Counterfactual Inference under Thompson Sampling",
        "abstract":"Recommender systems exemplify sequential decision-making under uncertainty, strategically deciding what content to serve to users, to optimise a range of potential objectives. To balance the explore-exploit trade-off successfully, Thompson sampling provides a natural and widespread paradigm to probabilistically select which action to take. Questions of causal and counterfactual inference, which underpin use-cases like offline evaluation, are not straightforward to answer in these contexts. Specifically, whilst most existing estimators rely on action propensities, these are not readily available under Thompson sampling procedures.\n  We derive exact and efficiently computable expressions for action propensities under a variety of parameter and outcome distributions, enabling the use of off-policy estimators in Thompson sampling scenarios. This opens up a range of practical use-cases where counterfactual inference is crucial, including unbiased offline evaluation of recommender systems, as well as general applications of causal inference in online advertising, personalisation, and beyond.\n        △ Less",
        "authors":[
            "Olivier Jeunen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Reward Generation via Large Vision-Language Model in Offline Reinforcement Learning",
        "abstract":"In offline reinforcement learning (RL), learning from fixed datasets presents a promising solution for domains where real-time interaction with the environment is expensive or risky. However, designing dense reward signals for offline dataset requires significant human effort and domain expertise. Reinforcement learning with human feedback (RLHF) has emerged as an alternative, but it remains costly due to the human-in-the-loop process, prompting interest in automated reward generation models. To address this, we propose Reward Generation via Large Vision-Language Models (RG-VLM), which leverages the reasoning capabilities of LVLMs to generate rewards from offline data without human involvement. RG-VLM improves generalization in long-horizon tasks and can be seamlessly integrated with the sparse reward signals to enhance task performance, demonstrating its potential as an auxiliary reward signal.\n        △ Less",
        "authors":[
            "Younghwan Lee",
            "Tung M. Luu",
            "Donghoon Lee",
            "Chang D. Yoo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"High-order expansion of Neural Ordinary Differential Equations flows",
        "abstract":"Artificial neural networks, widely recognised for their role in machine learning, are now transforming the study of ordinary differential equations (ODEs), bridging data-driven modelling with classical dynamical systems and enabling the development of infinitely deep neural models. However, the practical applicability of these models remains constrained by the opacity of their learned dynamics, which operate as black-box systems with limited explainability, thereby hindering trust in their deployment. Existing approaches for the analysis of these dynamical systems are predominantly restricted to first-order gradient information due to computational constraints, thereby limiting the depth of achievable insight. Here, we introduce Event Transition Tensors, a framework based on high-order differentials that provides a rigorous mathematical description of neural ODE dynamics on event manifolds. We demonstrate its versatility across diverse applications: characterising uncertainties in a data-driven prey-predator control model, analysing neural optimal feedback dynamics, and mapping landing trajectories in a three-body neural Hamiltonian system. In all cases, our method enhances the interpretability and rigour of neural ODEs by expressing their behaviour through explicit mathematical structures. Our findings contribute to a deeper theoretical foundation for event-triggered neural differential equations and provide a mathematical construct for explaining complex system dynamics.\n        △ Less",
        "authors":[
            "Dario Izzo",
            "Sebastien Origer",
            "Giacomo Acciarini",
            "Francesco Biscani"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards scientific machine learning for granular material simulations -- challenges and opportunities",
        "abstract":"Micro-scale mechanisms, such as inter-particle and particle-fluid interactions, govern the behaviour of granular systems. While particle-scale simulations provide detailed insights into these interactions, their computational cost is often prohibitive. Attended by researchers from both the granular materials (GM) and machine learning (ML) communities, a recent Lorentz Center Workshop on \"Machine Learning for Discrete Granular Media\" brought the ML community up to date with GM challenges.\n  This position paper emerged from the workshop discussions. We define granular materials and identify seven key challenges that characterise their distinctive behaviour across various scales and regimes, ranging from gas-like to fluid-like and solid-like. Addressing these challenges is essential for developing robust and efficient digital twins for granular systems in various industrial applications. To showcase the potential of ML to the GM community, we present classical and emerging machine\/deep learning techniques that have been, or could be, applied to granular materials. We reviewed sequence-based learning models for path-dependent constitutive behaviour, followed by encoder-decoder type models for representing high-dimensional data. We then explore graph neural networks and recent advances in neural operator learning. Lastly, we discuss model-order reduction and probabilistic learning techniques for high-dimensional parameterised systems, which are crucial for quantifying uncertainties arising from physics-based and data-driven models.\n  We present a workflow aimed at unifying data structures and modelling pipelines and guiding readers through the selection, training, and deployment of ML surrogates for granular material simulations. Finally, we illustrate the workflow's practical use with two representative examples, focusing on granular materials in solid-like and fluid-like regimes.\n        △ Less",
        "authors":[
            "Marc Fransen",
            "Andreas Fürst",
            "Deepak Tunuguntla",
            "Daniel N. Wilke",
            "Benedikt Alkin",
            "Daniel Barreto",
            "Johannes Brandstetter",
            "Miguel Angel Cabrera",
            "Xinyan Fan",
            "Mengwu Guo",
            "Bram Kieskamp",
            "Krishna Kumar",
            "John Morrissey",
            "Jonathan Nuttall",
            "Jin Ooi",
            "Luisa Orozco",
            "Stefanos-Aldo Papanicolopulos",
            "Tongming Qu",
            "Dingena Schott",
            "Takayuki Shuku",
            "WaiChing Sun",
            "Thomas Weinhart",
            "Dongwei Ye",
            "Hongyang Cheng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"WebMap -- Large Language Model-assisted Semantic Link Induction in the Web",
        "abstract":"Carrying out research tasks is only inadequately supported, if not hindered, by current web search engines. This paper therefore proposes functional extensions of WebMap, a semantically induced overlay linking structure on the web to inherently facilitate research activities. These add-ons support the dynamic determination and regrouping of document clusters, the creation of a semantic signpost in the web, and the interactive tracing of topics back to their origins.\n        △ Less",
        "authors":[
            "Shiraj Pokharel",
            "Georg P. Roßrucker",
            "Mario M. Kubek"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Survey of Multimodal Retrieval-Augmented Generation",
        "abstract":"Multimodal Retrieval-Augmented Generation (MRAG) enhances large language models (LLMs) by integrating multimodal data (text, images, videos) into retrieval and generation processes, overcoming the limitations of text-only Retrieval-Augmented Generation (RAG). While RAG improves response accuracy by incorporating external textual knowledge, MRAG extends this framework to include multimodal retrieval and generation, leveraging contextual information from diverse data types. This approach reduces hallucinations and enhances question-answering systems by grounding responses in factual, multimodal knowledge. Recent studies show MRAG outperforms traditional RAG, especially in scenarios requiring both visual and textual understanding. This survey reviews MRAG's essential components, datasets, evaluation methods, and limitations, providing insights into its construction and improvement. It also identifies challenges and future research directions, highlighting MRAG's potential to revolutionize multimodal information retrieval and generation. By offering a comprehensive perspective, this work encourages further exploration into this promising paradigm.\n        △ Less",
        "authors":[
            "Lang Mei",
            "Siyu Mo",
            "Zhihan Yang",
            "Chong Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamic Topic Analysis in Academic Journals using Convex Non-negative Matrix Factorization Method",
        "abstract":"With the rapid advancement of large language models, academic topic identification and topic evolution analysis are crucial for enhancing AI's understanding capabilities. Dynamic topic analysis provides a powerful approach to capturing and understanding the temporal evolution of topics in large-scale datasets. This paper presents a two-stage dynamic topic analysis framework that incorporates convex optimization to improve topic consistency, sparsity, and interpretability. In Stage 1, a two-layer non-negative matrix factorization (NMF) model is employed to extract annual topics and identify key terms. In Stage 2, a convex optimization algorithm refines the dynamic topic structure using the convex NMF (cNMF) model, further enhancing topic integration and stability. Applying the proposed method to IEEE journal abstracts from 2004 to 2022 effectively identifies and quantifies emerging research topics, such as COVID-19 and digital twins. By optimizing sparsity differences in the clustering feature space between traditional and emerging research topics, the framework provides deeper insights into topic evolution and ranking analysis. Moreover, the NMF-cNMF model demonstrates superior stability in topic consistency. At sparsity levels of 0.4, 0.6, and 0.9, the proposed approach improves topic ranking stability by 24.51%, 56.60%, and 36.93%, respectively. The source code (to be open after publication) is available at https:\/\/github.com\/meetyangyang\/CDNMF.\n        △ Less",
        "authors":[
            "Yang Yang",
            "Tong Zhang",
            "Jian Wu",
            "Lijie Su"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DEEP: Edge-based Dataflow Processing with Hybrid Docker Hub and Regional Registries",
        "abstract":"Reducing energy consumption is essential to lessen greenhouse gas emissions, conserve natural resources, and help mitigate the impacts of climate change. In this direction, edge computing, a complementary technology to cloud computing, extends computational capabilities closer to the data producers, enabling energy-efficient and latency-sensitive service delivery for end users. To properly manage data and microservice storage, expanding the Docker Hub registry to the edge using an AWS S3-compatible MinIO-based object storage service can reduce completion time and energy consumption. To address this, we introduce Docker rEgistry-based Edge dataflow Processing (DEEP) to optimize the energy consumption of microservice-based application deployments by focusing on deployments from Docker Hub and MinIO-based regional registries and their processing on edge devices. After applying nash equilibrium and benchmarking the execution of two compute-intensive machine learning (ML) applications of video and text processing, we compare energy consumption across three deployment scenarios: exclusively from Docker Hub, exclusively from the regional registry, and a hybrid method utilizing both. Experimental results show that deploying 83% of text processing microservices from the regional registry improves the energy consumption by 0.34% (18J) compared to microservice deployments exclusively from Docker Hub.\n        △ Less",
        "authors":[
            "Narges Mehran",
            "Zahra Najafabadi Samani",
            "Reza Farahani",
            "Josef Hammer",
            "Dragi Kimovski"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Recommendation System in Advertising and Streaming Media: Unsupervised Data Enhancement Sequence Suggestions",
        "abstract":"Sequential recommendation is an extensively explored approach to capturing users' evolving preferences based on past interactions, aimed at predicting their next likely choice. Despite significant advancements in this domain, including methods based on RNNs and self-attention, challenges like limited supervised signals and noisy data caused by unintentional clicks persist. To address these challenges, some studies have incorporated unsupervised learning by leveraging local item contexts within individual sequences. However, these methods often overlook the intricate associations between items across multiple sequences and are susceptible to noise in item co-occurrence patterns. In this context, we introduce a novel framework, Global Unsupervised Data-Augmentation (UDA4SR), which adopts a graph contrastive learning perspective to generate more robust item embeddings for sequential recommendation. Our approach begins by integrating Generative Adversarial Networks (GANs) for data augmentation, which serves as the first step to enhance the diversity and richness of the training data. Then, we build a Global Item Relationship Graph (GIG) based on all user interaction sequences. Subsequently, we employ graph contrastive learning on the refined graph to enhance item embeddings by capturing complex global associations. To model users' dynamic and diverse interests more effectively, we enhance the CapsNet module with a novel target-attention mechanism. Extensive experiments show that UDA4SR significantly outperforms state-of-the-art approaches.\n        △ Less",
        "authors":[
            "Kowei Shih",
            "Yi Han",
            "Li Tan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-Driven Sentiment Analytics: Unlocking Business Value in the E-Commerce Landscape_v1",
        "abstract":"The rapid growth of e-commerce has led to an overwhelming volume of customer feedback, from product reviews to service interactions. Extracting meaningful insights from this data is crucial for businesses aiming to improve customer satisfaction and optimize decision-making. This paper presents an AI-driven sentiment analysis system designed specifically for e-commerce applications, balancing accuracy with interpretability. Our approach integrates traditional machine learning techniques with modern deep learning models, allowing for a more nuanced understanding of customer sentiment while ensuring transparency in decision-making. Experimental results show that our system outperforms standard sentiment analysis methods, achieving an accuracy of 89.7% on diverse, large-scale datasets. Beyond technical performance, real-world implementation across multiple e-commerce platforms demonstrates tangible improvements in customer engagement and operational efficiency. This study highlights both the potential and the challenges of applying AI to sentiment analysis in a commercial setting, offering insights into practical deployment strategies and areas for future refinement.\n        △ Less",
        "authors":[
            "Qianye Wu",
            "Chengxuan Xia",
            "Sixuan Tian"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dimension reduction for derivative-informed operator learning: An analysis of approximation errors",
        "abstract":"We study the derivative-informed learning of nonlinear operators between infinite-dimensional separable Hilbert spaces by neural networks. Such operators can arise from the solution of partial differential equations (PDEs), and are used in many simulation-based outer-loop tasks in science and engineering, such as PDE-constrained optimization, Bayesian inverse problems, and optimal experimental design. In these settings, the neural network approximations can be used as surrogate models to accelerate the solution of the outer-loop tasks. However, since outer-loop tasks in infinite dimensions often require knowledge of the underlying geometry, the approximation accuracy of the operator's derivatives can also significantly impact the performance of the surrogate model. Motivated by this, we analyze the approximation errors of neural operators in Sobolev norms over infinite-dimensional Gaussian input measures. We focus on the reduced basis neural operator (RBNO), which uses linear encoders and decoders defined on dominant input\/output subspaces spanned by reduced sets of orthonormal bases. To this end, we study two methods for generating the bases; principal component analysis (PCA) and derivative-informed subspaces (DIS), which use the dominant eigenvectors of the covariance of the data or the derivatives as the reduced bases, respectively. We then derive bounds for errors arising from both the dimension reduction and the latent neural network approximation, including the sampling errors associated with the empirical estimation of the PCA\/DIS. Our analysis is validated on numerical experiments with elliptic PDEs, where our results show that bases informed by the map (i.e., DIS or output PCA) yield accurate reconstructions and generalization errors for both the operator and its derivatives, while input PCA may underperform unless ranks and training sample sizes are sufficiently large.\n        △ Less",
        "authors":[
            "Dingcheng Luo",
            "Thomas O'Leary-Roseberry",
            "Peng Chen",
            "Omar Ghattas"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Steering CLIP's vision transformer with sparse autoencoders",
        "abstract":"While vision models are highly capable, their internal mechanisms remain poorly understood -- a challenge which sparse autoencoders (SAEs) have helped address in language, but which remains underexplored in vision. We address this gap by training SAEs on CLIP's vision transformer and uncover key differences between vision and language processing, including distinct sparsity patterns for SAEs trained across layers and token types. We then provide the first systematic analysis on the steerability of CLIP's vision transformer by introducing metrics to quantify how precisely SAE features can be steered to affect the model's output. We find that 10-15\\% of neurons and features are steerable, with SAEs providing thousands more steerable features than the base model. Through targeted suppression of SAE features, we then demonstrate improved performance on three vision disentanglement tasks (CelebA, Waterbirds, and typographic attacks), finding optimal disentanglement in middle model layers, and achieving state-of-the-art performance on defense against typographic attacks.\n        △ Less",
        "authors":[
            "Sonia Joseph",
            "Praneet Suresh",
            "Ethan Goldfarb",
            "Lorenz Hufe",
            "Yossi Gandelsman",
            "Robert Graham",
            "Danilo Bzdok",
            "Wojciech Samek",
            "Blake Aaron Richards"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DocAgent: A Multi-Agent System for Automated Code Documentation Generation",
        "abstract":"High-quality code documentation is crucial for software development especially in the era of AI. However, generating it automatically using Large Language Models (LLMs) remains challenging, as existing approaches often produce incomplete, unhelpful, or factually incorrect outputs. We introduce DocAgent, a novel multi-agent collaborative system using topological code processing for incremental context building. Specialized agents (Reader, Searcher, Writer, Verifier, Orchestrator) then collaboratively generate documentation. We also propose a multi-faceted evaluation framework assessing Completeness, Helpfulness, and Truthfulness. Comprehensive experiments show DocAgent significantly outperforms baselines consistently. Our ablation study confirms the vital role of the topological processing order. DocAgent offers a robust approach for reliable code documentation generation in complex and proprietary repositories.\n        △ Less",
        "authors":[
            "Dayu Yang",
            "Antoine Simoulin",
            "Xin Qian",
            "Xiaoyi Liu",
            "Yuwei Cao",
            "Zhaopu Teng",
            "Grey Yang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Surrogate-based optimization of system architectures subject to hidden constraints",
        "abstract":"The exploration of novel architectures requires physics-based simulation due to a lack of prior experience to start from, which introduces two specific challenges for optimization algorithms: evaluations become more expensive (in time) and evaluations might fail. The former challenge is addressed by Surrogate-Based Optimization (SBO) algorithms, in particular Bayesian Optimization (BO) using Gaussian Process (GP) models. An overview is provided of how BO can deal with challenges specific to architecture optimization, such as design variable hierarchy and multiple objectives: specific measures include ensemble infills and a hierarchical sampling algorithm. Evaluations might fail due to non-convergence of underlying solvers or infeasible geometry in certain areas of the design space. Such failed evaluations, also known as hidden constraints, pose a particular challenge to SBO\/BO, as the surrogate model cannot be trained on empty results. This work investigates various strategies for satisfying hidden constraints in BO algorithms. Three high-level strategies are identified: rejection of failed points from the training set, replacing failed points based on viable (non-failed) points, and predicting the failure region. Through investigations on a set of test problems including a jet engine architecture optimization problem, it is shown that best performance is achieved with a mixed-discrete GP to predict the Probability of Viability (PoV), and by ensuring selected infill points satisfy some minimum PoV threshold. This strategy is demonstrated by solving a jet engine architecture problem that features at 50% failure rate and could not previously be solved by a BO algorithm. The developed BO algorithm and used test problems are available in the open-source Python library SBArchOpt.\n        △ Less",
        "authors":[
            "Jasper Bussemaker",
            "Paul Saves",
            "Nathalie Bartoli",
            "Thierry Lefebvre",
            "Björn Nagel"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generating Fine Details of Entity Interactions",
        "abstract":"Images not only depict objects but also encapsulate rich interactions between them. However, generating faithful and high-fidelity images involving multiple entities interacting with each other, is a long-standing challenge. While pre-trained text-to-image models are trained on large-scale datasets to follow diverse text instructions, they struggle to generate accurate interactions, likely due to the scarcity of training data for uncommon object interactions. This paper introduces InterActing, an interaction-focused dataset with 1000 fine-grained prompts covering three key scenarios: (1) functional and action-based interactions, (2) compositional spatial relationships, and (3) multi-subject interactions. To address interaction generation challenges, we propose a decomposition-augmented refinement procedure. Our approach, DetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose interactions into finer-grained concepts, uses a VLM to critique generated images, and applies targeted interventions within the diffusion process in refinement. Automatic and human evaluations show significantly improved image quality, demonstrating the potential of enhanced inference strategies. Our dataset and code are available at https:\/\/concepts-ai.com\/p\/detailscribe\/ to facilitate future exploration of interaction-rich image generation.\n        △ Less",
        "authors":[
            "Xinyi Gu",
            "Jiayuan Mao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning",
        "abstract":"Deep learning-based electrocardiogram (ECG) classification has shown impressive performance but clinical adoption has been slowed by the lack of transparent and faithful explanations. Post hoc methods such as saliency maps may fail to reflect a model's true decision process. Prototype-based reasoning offers a more transparent alternative by grounding decisions in similarity to learned representations of real ECG segments, enabling faithful, case-based explanations. We introduce ProtoECGNet, a prototype-based deep learning model for interpretable, multi-label ECG classification. ProtoECGNet employs a structured, multi-branch architecture that reflects clinical interpretation workflows: it integrates a 1D CNN with global prototypes for rhythm classification, a 2D CNN with time-localized prototypes for morphology-based reasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each branch is trained with a prototype loss designed for multi-label learning, combining clustering, separation, diversity, and a novel contrastive loss that encourages appropriate separation between prototypes of unrelated classes while allowing clustering for frequently co-occurring diagnoses. We evaluate ProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating competitive performance relative to state-of-the-art black-box models while providing structured, case-based explanations. To assess prototype quality, we conduct a structured clinician review of the final model's projected prototypes, finding that they are rated as representative and clear. ProtoECGNet shows that prototype learning can be effectively scaled to complex, multi-label time-series classification, offering a practical path toward transparent and trustworthy deep learning models for clinical decision support.\n        △ Less",
        "authors":[
            "Sahil Sethi",
            "David Chen",
            "Thomas Statchen",
            "Michael C. Burkhart",
            "Nipun Bhandari",
            "Bashar Ramadan",
            "Brett Beaulieu-Jones"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond Black-Box Predictions: Identifying Marginal Feature Effects in Tabular Transformer Networks",
        "abstract":"In recent years, deep neural networks have showcased their predictive power across a variety of tasks. Beyond natural language processing, the transformer architecture has proven efficient in addressing tabular data problems and challenges the previously dominant gradient-based decision trees in these areas. However, this predictive power comes at the cost of intelligibility: Marginal feature effects are almost completely lost in the black-box nature of deep tabular transformer networks. Alternative architectures that use the additivity constraints of classical statistical regression models can maintain intelligible marginal feature effects, but often fall short in predictive power compared to their more complex counterparts. To bridge the gap between intelligibility and performance, we propose an adaptation of tabular transformer networks designed to identify marginal feature effects. We provide theoretical justifications that marginal feature effects can be accurately identified, and our ablation study demonstrates that the proposed model efficiently detects these effects, even amidst complex feature interactions. To demonstrate the model's predictive capabilities, we compare it to several interpretable as well as black-box models and find that it can match black-box performances while maintaining intelligibility. The source code is available at https:\/\/github.com\/OpenTabular\/NAMpy.\n        △ Less",
        "authors":[
            "Anton Thielmann",
            "Arik Reuter",
            "Benjamin Saefken"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Offline Reinforcement Learning using Human-Aligned Reward Labeling for Autonomous Emergency Braking in Occluded Pedestrian Crossing",
        "abstract":"Effective leveraging of real-world driving datasets is crucial for enhancing the training of autonomous driving systems. While Offline Reinforcement Learning enables the training of autonomous vehicles using such data, most available datasets lack meaningful reward labels. Reward labeling is essential as it provides feedback for the learning algorithm to distinguish between desirable and undesirable behaviors, thereby improving policy performance. This paper presents a novel pipeline for generating human-aligned reward labels. The proposed approach addresses the challenge of absent reward signals in real-world datasets by generating labels that reflect human judgment and safety considerations. The pipeline incorporates an adaptive safety component, activated by analyzing semantic segmentation maps, allowing the autonomous vehicle to prioritize safety over efficiency in potential collision scenarios. The proposed pipeline is applied to an occluded pedestrian crossing scenario with varying levels of pedestrian traffic, using synthetic and simulation data. The results indicate that the generated reward labels closely match the simulation reward labels. When used to train the driving policy using Behavior Proximal Policy Optimisation, the results are competitive with other baselines. This demonstrates the effectiveness of our method in producing reliable and human-aligned reward signals, facilitating the training of autonomous driving systems through Reinforcement Learning outside of simulation environments and in alignment with human values.\n        △ Less",
        "authors":[
            "Vinal Asodia",
            "Zhenhua Feng",
            "Saber Fallah"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow",
        "abstract":"Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE agents, have made tremendous progress (>60% on SWE-Bench Verified) on real-world coding challenges including GitHub issue resolution. SWE agents use a combination of reasoning, environment interaction and self-reflection to resolve issues thereby generating \"trajectories\". Analysis of SWE agent trajectories is difficult, not only as they exceed LLM sequence length (sometimes, greater than 128k) but also because it involves a relatively prolonged interaction between an LLM and the environment managed by the agent. In case of an agent error, it can be hard to decipher, locate and understand its scope. Similarly, it can be hard to track improvements or regression over multiple runs or experiments. While a lot of research has gone into making these SWE agents reach state-of-the-art, much less focus has been put into creating tools to help analyze and visualize agent output. We propose a novel tool called SeaView: Software Engineering Agent Visual Interface for Enhanced Workflow, with a vision to assist SWE-agent researchers to visualize and inspect their experiments. SeaView's novel mechanisms help compare experimental runs with varying hyper-parameters or LLMs, and quickly get an understanding of LLM or environment related problems. Based on our user study, experienced researchers spend between 10 and 30 minutes to gather the information provided by SeaView, while researchers with little experience can spend between 30 minutes to 1 hour to diagnose their experiment.\n        △ Less",
        "authors":[
            "Timothy Bula",
            "Saurabh Pujar",
            "Luca Buratti",
            "Mihaela Bornea",
            "Avirup Sil"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design",
        "abstract":"Multidisciplinary design optimization methods aim at adapting numerical optimization techniques to the design of engineering systems involving multiple disciplines. In this context, a large number of mixed continuous, integer and categorical variables might arise during the optimization process and practical applications involve a large number of design variables. Recently, there has been a growing interest in mixed variables constrained Bayesian optimization but most existing approaches severely increase the number of the hyperparameters related to the surrogate model. In this paper, we address this issue by constructing surrogate models using less hyperparameters. The reduction process is based on the partial least squares method. An adaptive procedure for choosing the number of hyperparameters is proposed. The performance of the proposed approach is confirmed on analytical tests as well as two real applications related to aircraft design. A significant improvement is obtained compared to genetic algorithms.\n        △ Less",
        "authors":[
            "Paul Saves",
            "Nathalie Bartoli",
            "Youssef Diouane",
            "Thierry Lefebvre",
            "Joseph Morlier",
            "Christophe David",
            "Eric Nguyen Van",
            "Sébastien Defoort"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning",
        "abstract":"Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https:\/\/github.com\/xufangzhi\/Genius.\n        △ Less",
        "authors":[
            "Fangzhi Xu",
            "Hang Yan",
            "Chang Ma",
            "Haiteng Zhao",
            "Qiushi Sun",
            "Kanzhi Cheng",
            "Junxian He",
            "Jun Liu",
            "Zhiyong Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Regularized infill criteria for multi-objective Bayesian optimization with application to aircraft design",
        "abstract":"Bayesian optimization is an advanced tool to perform ecient global optimization It consists on enriching iteratively surrogate Kriging models of the objective and the constraints both supposed to be computationally expensive of the targeted optimization problem Nowadays efficient extensions of Bayesian optimization to solve expensive multiobjective problems are of high interest The proposed method in this paper extends the super efficient global optimization with mixture of experts SEGOMOE to solve constrained multiobjective problems To cope with the illposedness of the multiobjective inll criteria different enrichment procedures using regularization techniques are proposed The merit of the proposed approaches are shown on known multiobjective benchmark problems with and without constraints The proposed methods are then used to solve a biobjective application related to conceptual aircraft design with ve unknown design variables and three nonlinear inequality constraints The preliminary results show a reduction of the total cost in terms of function evaluations by a factor of 20 compared to the evolutionary algorithm NSGA-II.\n        △ Less",
        "authors":[
            "Robin Grapin",
            "Youssef Diouane",
            "Joseph Morlier",
            "Nathalie Bartoli",
            "Thierry Lefebvre",
            "Paul Saves",
            "Jasper Bussemaker"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Channel Estimation by Infinite Width Convolutional Networks",
        "abstract":"In wireless communications, estimation of channels in OFDM systems spans frequency and time, which relies on sparse collections of pilot data, posing an ill-posed inverse problem. Moreover, deep learning estimators require large amounts of training data, computational resources, and true channels to produce accurate channel estimates, which are not realistic. To address this, a convolutional neural tangent kernel (CNTK) is derived from an infinitely wide convolutional network whose training dynamics can be expressed by a closed-form equation. This CNTK is used to impute the target matrix and estimate the missing channel response using only the known values available at pilot locations. This is a promising solution for channel estimation that does not require a large training set. Numerical results on realistic channel datasets demonstrate that our strategy accurately estimates the channels without a large dataset and significantly outperforms deep learning methods in terms of speed, accuracy, and computational resources.\n        △ Less",
        "authors":[
            "Mohammed Mallik",
            "Guillaume Villemaud"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Application of machine learning models to predict the relationship between air pollution, ecosystem degradation, and health disparities and lung cancer in Vietnam",
        "abstract":"Lung cancer is one of the major causes of death worldwide, and Vietnam is not an exception. This disease is the second most common type of cancer globally and the second most common cause of death in Vietnam, just after liver cancer, with 23,797 fatal cases and 26,262 new cases, or 14.4% of the disease in 2020. Recently, with rising disease rates in Vietnam causing a huge public health burden, lung cancer continues to hold the top position in attention and care. Especially together with climate change, under a variety of types of pollution, deforestation, and modern lifestyles, lung cancer risks are on red alert, particularly in Vietnam. To understand more about the severe disease sources in Vietnam from a diversity of key factors, including environmental features and the current health state, with a particular emphasis on Vietnam's distinct socioeconomic and ecological context, we utilize large datasets such as patient health records and environmental indicators containing necessary information, such as deforestation rate, green cover rate, air pollution, and lung cancer risks, that is collected from well-known governmental sharing websites. Then, we process and connect them and apply analytical methods (heatmap, information gain, p-value, spearman correlation) to determine causal correlations influencing lung cancer risks. Moreover, we deploy machine learning (ML) models (Decision Tree, Random Forest, Support Vector Machine, K-mean clustering) to discover cancer risk patterns. Our experimental results, leveraged by the aforementioned ML models to identify the disease patterns, are promising, particularly, the models as Random Forest, SVM, and PCA are working well on the datasets and give high accuracy (99%), however, the K means clustering has very low accuracy (10%) and does not fit the datasets.\n        △ Less",
        "authors":[
            "Ngoc Hong Tran",
            "Lan Kim Vien",
            "Ngoc-Thao Thi Le"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Transformer Learns Optimal Variable Selection in Group-Sparse Classification",
        "abstract":"Transformers have demonstrated remarkable success across various applications. However, the success of transformers have not been understood in theory. In this work, we give a case study of how transformers can be trained to learn a classic statistical model with \"group sparsity\", where the input variables form multiple groups, and the label only depends on the variables from one of the groups. We theoretically demonstrate that, a one-layer transformer trained by gradient descent can correctly leverage the attention mechanism to select variables, disregarding irrelevant ones and focusing on those beneficial for classification. We also demonstrate that a well-pretrained one-layer transformer can be adapted to new downstream tasks to achieve good prediction accuracy with a limited number of samples. Our study sheds light on how transformers effectively learn structured data.\n        △ Less",
        "authors":[
            "Chenyang Zhang",
            "Xuran Meng",
            "Yuan Cao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines",
        "abstract":"One of the key safety considerations of battery manufacturing is thermal runaway, the uncontrolled increase in temperature which can lead to fires, explosions, and emissions of toxic gasses. As such, development of automated systems capable of detecting such events is of considerable importance in both academic and industrial contexts. In this work, we investigate the use of deep learning for detecting thermal runaway in the battery production line of VDL Nedcar, a Dutch automobile manufacturer. Specifically, we collect data from the production line to represent both baseline (non thermal runaway) and thermal runaway conditions. Thermal runaway was simulated through the use of external heat and smoke sources. The data consisted of both optical and thermal images which were then preprocessed and fused before serving as input to our models. In this regard, we evaluated three deep-learning models widely used in computer vision including shallow convolutional neural networks, residual neural networks, and vision transformers on two performance metrics. Furthermore, we evaluated these models using explainability methods to gain insight into their ability to capture the relevant feature information from their inputs. The obtained results indicate that the use of deep learning is a viable approach to thermal runaway detection in battery production lines.\n        △ Less",
        "authors":[
            "Athanasios Athanasopoulos",
            "Matúš Mihalák",
            "Marcin Pietrasik"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Gradient Descent Robustly Learns the Intrinsic Dimension of Data in Training Convolutional Neural Networks",
        "abstract":"Modern neural networks are usually highly over-parameterized. Behind the wide usage of over-parameterized networks is the belief that, if the data are simple, then the trained network will be automatically equivalent to a simple predictor. Following this intuition, many existing works have studied different notions of \"ranks\" of neural networks and their relation to the rank of data. In this work, we study the rank of convolutional neural networks (CNNs) trained by gradient descent, with a specific focus on the robustness of the rank to image background noises. Specifically, we point out that, when adding background noises to images, the rank of the CNN trained with gradient descent is affected far less compared with the rank of the data. We support our claim with a theoretical case study, where we consider a particular data model to characterize low-rank clean images with added background noises. We prove that CNNs trained by gradient descent can learn the intrinsic dimension of clean images, despite the presence of relatively large background noises. We also conduct experiments on synthetic and real datasets to further validate our claim.\n        △ Less",
        "authors":[
            "Chenyang Zhang",
            "Peifeng Gao",
            "Difan Zou",
            "Yuan Cao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Task-conditioned Ensemble of Expert Models for Continuous Learning",
        "abstract":"One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed model with new data could be one remedy. However, the question arises as to how we should update the model with new training data so that it retains its accuracy on the old data while adapting to the new data. In this work, we propose a task-conditioned ensemble of models to maintain the performance of the existing model. The method involves an ensemble of expert models based on task membership information. The in-domain models-based on the local outlier concept (different from the expert models) provide task membership information dynamically at run-time to each probe sample. To evaluate the proposed method, we experiment with three setups: the first represents distribution shift between tasks (LivDet-Iris-2017), the second represents distribution shift both between and within tasks (LivDet-Iris-2020), and the third represents disjoint distribution between tasks (Split MNIST). The experiments highlight the benefits of the proposed method. The source code is available at https:\/\/github.com\/iPRoBe-lab\/Continuous_Learning_FE_DM.\n        △ Less",
        "authors":[
            "Renu Sharma",
            "Debasmita Pal",
            "Arun Ross"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MooseAgent: A LLM Based Multi-agent Framework for Automating Moose Simulation",
        "abstract":"The Finite Element Method (FEM) is widely used in engineering and scientific computing, but its pre-processing, solver configuration, and post-processing stages are often time-consuming and require specialized knowledge. This paper proposes an automated solution framework, MooseAgent, for the multi-physics simulation framework MOOSE, which combines large-scale pre-trained language models (LLMs) with a multi-agent system. The framework uses LLMs to understand user-described simulation requirements in natural language and employs task decomposition and multi-round iterative verification strategies to automatically generate MOOSE input files. To improve accuracy and reduce model hallucinations, the system builds and utilizes a vector database containing annotated MOOSE input cards and function documentation. We conducted experimental evaluations on several typical cases, including heat transfer, mechanics, phase field, and multi-physics coupling. The results show that MooseAgent can automate the MOOSE simulation process to a certain extent, especially demonstrating a high success rate when dealing with relatively simple single-physics problems. The main contribution of this research is the proposal of a multi-agent automated framework for MOOSE, which validates its potential in simplifying finite element simulation processes and lowering the user barrier, providing new ideas for the development of intelligent finite element simulation software. The code for the MooseAgent framework proposed in this paper has been open-sourced and is available at https:\/\/github.com\/taozhan18\/MooseAgent\n        △ Less",
        "authors":[
            "Tao Zhang",
            "Zhenhai Liu",
            "Yong Xin",
            "Yongjun Jiao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English",
        "abstract":"The dissemination of online hate speech can have serious negative consequences for individuals, online communities, and entire societies. This and the large volume of hateful online content prompted both practitioners', i.e., in content moderation or law enforcement, and researchers' interest in machine learning models to automatically classify instances of hate speech. Whereas most scientific works address hate speech classification as a binary task, practice often requires a differentiation into sub-types, e.g., according to target, severity, or legality, which may overlap for individual content. Hence, researchers created datasets and machine learning models that approach hate speech classification in textual data as a multi-label problem. This work presents the first systematic and comprehensive survey of scientific literature on this emerging research landscape in English (N=46). We contribute with a concise overview of 28 datasets suited for training multi-label classification models that reveals significant heterogeneity regarding label-set, size, meta-concept, annotation process, and inter-annotator agreement. Our analysis of 24 publications proposing suitable classification models further establishes inconsistency in evaluation and a preference for architectures based on Bidirectional Encoder Representation from Transformers (BERT) and Recurrent Neural Networks (RNNs). We identify imbalanced training data, reliance on crowdsourcing platforms, small and sparse datasets, and missing methodological alignment as critical open issues and formulate ten recommendations for research.\n        △ Less",
        "authors":[
            "Julian Bäumler",
            "Louis Blöcher",
            "Lars-Joel Frey",
            "Xian Chen",
            "Markus Bayer",
            "Christian Reuter"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Fidelity Calibration for Informative Sim-to-Real Adaptation",
        "abstract":"Deep reinforcement learning can seamlessly transfer agile locomotion and navigation skills from the simulator to real world. However, bridging the sim-to-real gap with domain randomization or adversarial methods often demands expert physics knowledge to ensure policy robustness. Even so, cutting-edge simulators may fall short of capturing every real-world detail, and the reconstructed environment may introduce errors due to various perception uncertainties. To address these challenges, we propose Neural Fidelity Calibration (NFC), a novel framework that employs conditional score-based diffusion models to calibrate simulator physical coefficients and residual fidelity domains online during robot execution. Specifically, the residual fidelity reflects the simulation model shift relative to the real-world dynamics and captures the uncertainty of the perceived environment, enabling us to sample realistic environments under the inferred distribution for policy fine-tuning. Our framework is informative and adaptive in three key ways: (a) we fine-tune the pretrained policy only under anomalous scenarios, (b) we build sequential NFC online with the pretrained NFC's proposal prior, reducing the diffusion model's training burden, and (c) when NFC uncertainty is high and may degrade policy improvement, we leverage optimistic exploration to enable hallucinated policy optimization. Our framework achieves superior simulator calibration precision compared to state-of-the-art methods across diverse robots with high-dimensional parametric spaces. We study the critical contribution of residual fidelity to policy improvement in simulation and real-world experiments. Notably, our approach demonstrates robust robot navigation under challenging real-world conditions, such as a broken wheel axle on snowy surfaces.\n        △ Less",
        "authors":[
            "Youwei Yu",
            "Lantao Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On Background Bias of Post-Hoc Concept Embeddings in Computer Vision DNNs",
        "abstract":"The thriving research field of concept-based explainable artificial intelligence (C-XAI) investigates how human-interpretable semantic concepts embed in the latent spaces of deep neural networks (DNNs). Post-hoc approaches therein use a set of examples to specify a concept, and determine its embeddings in DNN latent space using data driven techniques. This proved useful to uncover biases between different target (foreground or concept) classes. However, given that the background is mostly uncontrolled during training, an important question has been left unattended so far: Are\/to what extent are state-of-the-art, data-driven post-hoc C-XAI approaches themselves prone to biases with respect to their backgrounds? E.g., wild animals mostly occur against vegetation backgrounds, and they seldom appear on roads. Even simple and robust C-XAI methods might abuse this shortcut for enhanced performance. A dangerous performance degradation of the concept-corner cases of animals on the road could thus remain undiscovered. This work validates and thoroughly confirms that established Net2Vec-based concept segmentation techniques frequently capture background biases, including alarming ones, such as underperformance on road scenes. For the analysis, we compare 3 established techniques from the domain of background randomization on >50 concepts from 2 datasets, and 7 diverse DNN architectures. Our results indicate that even low-cost setups can provide both valuable insight and improved background robustness.\n        △ Less",
        "authors":[
            "Gesina Schwalbe",
            "Georgii Mikriukov",
            "Edgar Heinert",
            "Stavros Gerolymatos",
            "Mert Keser",
            "Alois Knoll",
            "Matthias Rottmann",
            "Annika Mütze"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Ready, Bid, Go! On-Demand Delivery Using Fleets of Drones with Unknown, Heterogeneous Energy Storage Constraints",
        "abstract":"Unmanned Aerial Vehicles (UAVs) are expected to transform logistics, reducing delivery time, costs, and emissions. This study addresses an on-demand delivery , in which fleets of UAVs are deployed to fulfil orders that arrive stochastically. Unlike previous work, it considers UAVs with heterogeneous, unknown energy storage capacities and assumes no knowledge of the energy consumption models. We propose a decentralised deployment strategy that combines auction-based task allocation with online learning. Each UAV independently decides whether to bid for orders based on its energy storage charge level, the parcel mass, and delivery distance. Over time, it refines its policy to bid only for orders within its capability. Simulations using realistic UAV energy models reveal that, counter-intuitively, assigning orders to the least confident bidders reduces delivery times and increases the number of successfully fulfilled orders. This strategy is shown to outperform threshold-based methods which require UAVs to exceed specific charge levels at deployment. We propose a variant of the strategy which uses learned policies for forecasting. This enables UAVs with insufficient charge levels to commit to fulfilling orders at specific future times, helping to prioritise early orders. Our work provides new insights into long-term deployment of UAV swarms, highlighting the advantages of decentralised energy-aware decision-making coupled with online learning in real-world dynamic environments.\n        △ Less",
        "authors":[
            "Mohamed S. Talamali",
            "Genki Miyauchi",
            "Thomas Watteyne",
            "Micael S. Couceiro",
            "Roderich Gross"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Boosting multi-demographic federated learning for chest x-ray analysis using general-purpose self-supervised representations",
        "abstract":"Reliable artificial intelligence (AI) models for medical image analysis often depend on large and diverse labeled datasets. Federated learning (FL) offers a decentralized and privacy-preserving approach to training but struggles in highly non-independent and identically distributed (non-IID) settings, where institutions with more representative data may experience degraded performance. Moreover, existing large-scale FL studies have been limited to adult datasets, neglecting the unique challenges posed by pediatric data, which introduces additional non-IID variability. To address these limitations, we analyzed n=398,523 adult chest radiographs from diverse institutions across multiple countries and n=9,125 pediatric images, leveraging transfer learning from general-purpose self-supervised image representations to classify pneumonia and cases with no abnormality. Using state-of-the-art vision transformers, we found that FL improved performance only for smaller adult datasets (P<0.001) but degraded performance for larger datasets (P<0.064) and pediatric cases (P=0.242). However, equipping FL with self-supervised weights significantly enhanced outcomes across pediatric cases (P=0.031) and most adult datasets (P<0.008), except the largest dataset (P=0.052). These findings underscore the potential of easily deployable general-purpose self-supervised image representations to address non-IID challenges in clinical FL applications and highlight their promise for enhancing patient outcomes and advancing pediatric healthcare, where data scarcity and variability remain persistent obstacles.\n        △ Less",
        "authors":[
            "Mahshad Lotfinia",
            "Arash Tayebiarasteh",
            "Samaneh Samiei",
            "Mehdi Joodaki",
            "Soroosh Tayebi Arasteh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AstroLLaVA: towards the unification of astronomical data and natural language",
        "abstract":"We present AstroLLaVA, a vision language model for astronomy that enables interaction with astronomical imagery through natural dialogue. By fine-tuning the LLaVA model on a diverse dataset of $\\sim$30k images with captions and question-answer pairs sourced from NASA's `Astronomy Picture of the Day', the European Southern Observatory, and the NASA\/ESA Hubble Space Telescope, we create a model capable of answering open-ended questions about astronomical concepts depicted visually. Our two-stage fine-tuning process adapts the model to both image captioning and visual question answering in the astronomy domain. We demonstrate AstroLLaVA's performance on an astronomical visual question answering benchmark and release the model weights, code, and training set to encourage further open source work in this space. Finally, we suggest a roadmap towards general astronomical data alignment with pre-trained language models, and provide an open space for collaboration towards this end for interested researchers.\n        △ Less",
        "authors":[
            "Sharaf Zaman",
            "Michael J. Smith",
            "Pranav Khetarpal",
            "Rishabh Chakrabarty",
            "Michele Ginolfi",
            "Marc Huertas-Company",
            "Maja Jabłońska",
            "Sandor Kruk",
            "Matthieu Le Lain",
            "Sergio José Rodríguez Méndez",
            "Dimitrios Tanoglidis"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Boosting-inspired online learning with transfer for railway maintenance",
        "abstract":"The integration of advanced sensor technologies with deep learning algorithms has revolutionized fault diagnosis in railway systems, particularly at the wheel-track interface. Although numerous models have been proposed to detect irregularities such as wheel out-of-roundness, they often fall short in real-world applications due to the dynamic and nonstationary nature of railway operations. This paper introduces BOLT-RM (Boosting-inspired Online Learning with Transfer for Railway Maintenance), a model designed to address these challenges using continual learning for predictive maintenance. By allowing the model to continuously learn and adapt as new data become available, BOLT-RM overcomes the issue of catastrophic forgetting that often plagues traditional models. It retains past knowledge while improving predictive accuracy with each new learning episode, using a boosting-like knowledge sharing mechanism to adapt to evolving operational conditions such as changes in speed, load, and track irregularities. The methodology is validated through comprehensive multi-domain simulations of train-track dynamic interactions, which capture realistic railway operating conditions. The proposed BOLT-RM model demonstrates significant improvements in identifying wheel anomalies, establishing a reliable sequence for maintenance interventions.\n        △ Less",
        "authors":[
            "Diogo Risca",
            "Afonso Lourenço",
            "Goreti Marreiros"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Uncovering the Structure of Explanation Quality with Spectral Analysis",
        "abstract":"As machine learning models are increasingly considered for high-stakes domains, effective explanation methods are crucial to ensure that their prediction strategies are transparent to the user. Over the years, numerous metrics have been proposed to assess quality of explanations. However, their practical applicability remains unclear, in particular due to a limited understanding of which specific aspects each metric rewards. In this paper we propose a new framework based on spectral analysis of explanation outcomes to systematically capture the multifaceted properties of different explanation techniques. Our analysis uncovers two distinct factors of explanation quality-stability and target sensitivity-that can be directly observed through spectral decomposition. Experiments on both MNIST and ImageNet show that popular evaluation techniques (e.g., pixel-flipping, entropy) partially capture the trade-offs between these factors. Overall, our framework provides a foundational basis for understanding explanation quality, guiding the development of more reliable techniques for evaluating explanations.\n        △ Less",
        "authors":[
            "Johannes Maeß",
            "Grégoire Montavon",
            "Shinichi Nakajima",
            "Klaus-Robert Müller",
            "Thomas Schnake"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Slicing the Gaussian Mixture Wasserstein Distance",
        "abstract":"Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (MW) distance adapts the Wasserstein metric to GMMs and has been applied in various domains, including domain adaptation, dataset comparison, and reinforcement learning. However, its high computational cost -- arising from repeated Wasserstein distance computations involving matrix square root estimations and an expensive linear program -- limits its scalability to high-dimensional and large-scale problems. To address this, we propose multiple novel slicing-based approximations to the MW distance that significantly reduce computational complexity while preserving key optimal transport properties. From a theoretical viewpoint, we establish several weak and strong equivalences between the introduced metrics, and show the relations to the original MW distance and the well-established sliced Wasserstein distance. Furthermore, we validate the effectiveness of our approach through numerical experiments, demonstrating computational efficiency and applications in clustering, perceptual image comparison, and GMM minimization\n        △ Less",
        "authors":[
            "Moritz Piening",
            "Robert Beinert"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Explainability and Continual Learning meet Federated Learning at the Network Edge",
        "abstract":"As edge devices become more capable and pervasive in wireless networks, there is growing interest in leveraging their collective compute power for distributed learning. However, optimizing learning at the network edge entails unique challenges, particularly when moving beyond conventional settings and objectives. While Federated Learning (FL) has emerged as a key paradigm for distributed model training, critical challenges persist. First, existing approaches often overlook the trade-off between predictive accuracy and interpretability. Second, they struggle to integrate inherently explainable models such as decision trees because their non-differentiable structure makes them not amenable to backpropagation-based training algorithms. Lastly, they lack meaningful mechanisms for continual Machine Learning (ML) model adaptation through Continual Learning (CL) in resource-limited environments. In this paper, we pave the way for a set of novel optimization problems that emerge in distributed learning at the network edge with wirelessly interconnected edge devices, and we identify key challenges and future directions. Specifically, we discuss how Multi-objective optimization (MOO) can be used to address the trade-off between predictive accuracy and explainability when using complex predictive models. Next, we discuss the implications of integrating inherently explainable tree-based models into distributed learning settings. Finally, we investigate how CL strategies can be effectively combined with FL to support adaptive, lifelong learning when limited-size buffers are used to store past data for retraining. Our approach offers a cohesive set of tools for designing privacy-preserving, adaptive, and trustworthy ML solutions tailored to the demands of edge computing and intelligent services.\n        △ Less",
        "authors":[
            "Thomas Tsouparopoulos",
            "Iordanis Koutsopoulos"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LGRPool: Hierarchical Graph Pooling Via Local-Global Regularisation",
        "abstract":"Hierarchical graph pooling(HGP) are designed to consider the fact that conventional graph neural networks(GNN) are inherently flat and are also not multiscale. However, most HGP methods suffer not only from lack of considering global topology of the graph and focusing on the feature learning aspect, but also they do not align local and global features since graphs should inherently be analyzed in a multiscale way. LGRPool is proposed in the present paper as a HGP in the framework of expectation maximization in machine learning that aligns local and global aspects of message passing with each other using a regularizer to force the global topological information to be inline with the local message passing at different scales through the representations at different layers of HGP. Experimental results on some graph classification benchmarks show that it slightly outperforms some baselines.\n        △ Less",
        "authors":[
            "Farshad Noravesh",
            "Reza Haffari",
            "Layki Soon",
            "Arghya Pal"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Early Experience with Confidential Computing Architecture for On-Device Model Protection",
        "abstract":"Deploying machine learning (ML) models on user devices can improve privacy (by keeping data local) and reduce inference latency. Trusted Execution Environments (TEEs) are a practical solution for protecting proprietary models, yet existing TEE solutions have architectural constraints that hinder on-device model deployment. Arm Confidential Computing Architecture (CCA), a new Arm extension, addresses several of these limitations and shows promise as a secure platform for on-device ML. In this paper, we evaluate the performance-privacy trade-offs of deploying models within CCA, highlighting its potential to enable confidential and efficient ML applications. Our evaluations show that CCA can achieve an overhead of, at most, 22% in running models of different sizes and applications, including image classification, voice recognition, and chat assistants. This performance overhead comes with privacy benefits; for example, our framework can successfully protect the model against membership inference attack by an 8.3% reduction in the adversary's success rate. To support further research and early adoption, we make our code and methodology publicly available.\n        △ Less",
        "authors":[
            "Sina Abdollahi",
            "Mohammad Maheri",
            "Sandra Siby",
            "Marios Kogias",
            "Hamed Haddadi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Statistically guided deep learning",
        "abstract":"We present a theoretically well-founded deep learning algorithm for nonparametric regression. It uses over-parametrized deep neural networks with logistic activation function, which are fitted to the given data via gradient descent. We propose a special topology of these networks, a special random initialization of the weights, and a data-dependent choice of the learning rate and the number of gradient descent steps. We prove a theoretical bound on the expected $L_2$ error of this estimate, and illustrate its finite sample size performance by applying it to simulated data. Our results show that a theoretical analysis of deep learning which takes into account simultaneously optimization, generalization and approximation can result in a new deep learning estimate which has an improved finite sample performance.\n        △ Less",
        "authors":[
            "Michael Kohler",
            "Adam Krzyzak"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Physics-informed data-driven control without persistence of excitation",
        "abstract":"We show that data that is not sufficiently informative to allow for system re-identification can still provide meaningful information when combined with external or physical knowledge of the system, such as bounded system matrix norms. We then illustrate how this information can be leveraged for safety and energy minimization problems and to enhance predictions in unmodelled dynamics. This preliminary work outlines key ideas toward using limited data for effective control by integrating physical knowledge of the system and exploiting interpolation conditions.\n        △ Less",
        "authors":[
            "Martina Vanelli",
            "Julien M. Hendrickx"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Toward Realistic Adversarial Attacks in IDS: A Novel Feasibility Metric for Transferability",
        "abstract":"Transferability-based adversarial attacks exploit the ability of adversarial examples, crafted to deceive a specific source Intrusion Detection System (IDS) model, to also mislead a target IDS model without requiring access to the training data or any internal model parameters. These attacks exploit common vulnerabilities in machine learning models to bypass security measures and compromise systems. Although the transferability concept has been widely studied, its practical feasibility remains limited due to assumptions of high similarity between source and target models. This paper analyzes the core factors that contribute to transferability, including feature alignment, model architectural similarity, and overlap in the data distributions that each IDS examines. We propose a novel metric, the Transferability Feasibility Score (TFS), to assess the feasibility and reliability of such attacks based on these factors. Through experimental evidence, we demonstrate that TFS and actual attack success rates are highly correlated, addressing the gap between theoretical understanding and real-world impact. Our findings provide needed guidance for designing more realistic transferable adversarial attacks, developing robust defenses, and ultimately improving the security of machine learning-based IDS in critical systems.\n        △ Less",
        "authors":[
            "Sabrine Ennaji",
            "Elhadj Benkhelifa",
            "Luigi Vincenzo Mancini"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TickIt: Leveraging Large Language Models for Automated Ticket Escalation",
        "abstract":"In large-scale cloud service systems, support tickets serve as a critical mechanism for resolving customer issues and maintaining service quality. However, traditional manual ticket escalation processes encounter significant challenges, including inefficiency, inaccuracy, and difficulty in handling the high volume and complexity of tickets. While previous research has proposed various machine learning models for ticket classification, these approaches often overlook the practical demands of real-world escalations, such as dynamic ticket updates, topic-specific routing, and the analysis of ticket relationships. To bridge this gap, this paper introduces TickIt, an innovative online ticket escalation framework powered by Large Language Models. TickIt enables topic-aware, dynamic, and relationship-driven ticket escalations by continuously updating ticket states, assigning tickets to the most appropriate support teams, exploring ticket correlations, and leveraging category-guided supervised fine-tuning to continuously improve its performance. By deploying TickIt in ByteDance's cloud service platform Volcano Engine, we validate its efficacy and practicality, marking a significant advancement in the field of automated ticket escalation for large-scale cloud service systems.\n        △ Less",
        "authors":[
            "Fengrui Liu",
            "Xiao He",
            "Tieying Zhang",
            "Jianjun Chen",
            "Yi Li",
            "Lihua Yi",
            "Haipeng Zhang",
            "Gang Wu",
            "Rui Shi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Artifact detection and localization in single-channel mobile EEG for sleep research using deep learning and attention mechanisms",
        "abstract":"Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volume of data that novel mobile recording systems generate. We propose a convolutional neural network (CNN) model incorporating a convolutional block attention module (CNN-CBAM) to detect and identify the location of artifacts in the sleep EEG with attention maps. We benchmarked this model against six other machine learning and signal processing approaches. We trained\/tuned all models on 72 manually annotated EEG recordings obtained during home-based monitoring from 18 healthy participants with a mean (SD) age of 68.05 y ($\\pm$5.02). We tested them on 26 separate recordings from 6 healthy participants with a mean (SD) age of 68.33 y ($\\pm$4.08), with contained artifacts in 4\\% of epochs. CNN-CBAM achieved the highest area under the receiver operating characteristic curve (0.88), sensitivity (0.81), and specificity (0.86) when compared to the other approaches. The attention maps from CNN-CBAM localized artifacts within the epoch with a sensitivity of 0.71 and specificity of 0.67. This work demonstrates the feasibility of automating the detection and localization of artifacts in wearable sleep EEG.\n        △ Less",
        "authors":[
            "Khrystyna Semkiv",
            "Jia Zhang",
            "Maria Laura Ferster",
            "Walter Karlen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Generalization Bounds in Hybrid Quantum-Classical Machine Learning Models",
        "abstract":"Hybrid classical-quantum models aim to harness the strengths of both quantum computing and classical machine learning, but their practical potential remains poorly understood. In this work, we develop a unified mathematical framework for analyzing generalization in hybrid models, offering insight into how these systems learn from data. We establish a novel generalization bound of the form $O\\big( \\sqrt{\\frac{T\\log{T}}{N}} + \\fracα{\\sqrt{N}}\\big)$ for $N$ training data points, $T$ trainable quantum gates, and bounded fully-connected layers $||F|| \\leq α$. This bound decomposes cleanly into quantum and classical contributions, extending prior work on both components and clarifying their interaction. We apply our results to the quantum-classical convolutional neural network (QCCNN), an architecture that integrates quantum convolutional layers with classical processing. Alongside the bound, we highlight conceptual limitations of applying classical statistical learning theory in the hybrid setting and suggest promising directions for future theoretical work.\n        △ Less",
        "authors":[
            "Tongyan Wu",
            "Amine Bentellis",
            "Alona Sakhnenko",
            "Jeanette Miriam Lorenz"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Charting the Parrot's Song: A Maximum Mean Discrepancy Approach to Measuring AI Novelty, Originality, and Distinctiveness",
        "abstract":"Current intellectual property frameworks struggle to evaluate the novelty of AI-generated content, relying on subjective assessments ill-suited for comparing effectively infinite AI outputs against prior art. This paper introduces a robust, quantitative methodology grounded in Maximum Mean Discrepancy (MMD) to measure distributional differences between generative processes. By comparing entire output distributions rather than conducting pairwise similarity checks, our approach directly contrasts creative processes--overcoming the computational challenges inherent in evaluating AI outputs against unbounded prior art corpora. Through experiments combining kernel mean embeddings with domain-specific machine learning representations (LeNet-5 for MNIST digits, CLIP for art), we demonstrate exceptional sensitivity: our method distinguishes MNIST digit classes with 95% confidence using just 5-6 samples and differentiates AI-generated art from human art in the AI-ArtBench dataset (n=400 per category; p<0.0001) using as few as 7-10 samples per distribution despite human evaluators' limited discrimination ability (58% accuracy). These findings challenge the \"stochastic parrot\" hypothesis by providing empirical evidence that AI systems produce outputs from semantically distinct distributions rather than merely replicating training data. Our approach bridges technical capabilities with legal doctrine, offering a pathway to modernize originality assessments while preserving intellectual property law's core objectives. This research provides courts and policymakers with a computationally efficient, legally relevant tool to quantify AI novelty--a critical advancement as AI blurs traditional authorship and inventorship boundaries.\n        △ Less",
        "authors":[
            "Anirban Mukherjee",
            "Hannah Hanwen Chang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Systematic Evaluation of Knowledge Graph Embeddings for Gene-Disease Association Prediction",
        "abstract":"Discovery gene-disease links is important in biology and medicine areas, enabling disease identification and drug repurposing. Machine learning approaches accelerate this process by leveraging biological knowledge represented in ontologies and the structure of knowledge graphs. Still, many existing works overlook ontologies explicitly representing diseases, missing causal and semantic relationships between them. The gene-disease association problem naturally frames itself as a link prediction task, where embedding algorithms directly predict associations by exploring the structure and properties of the knowledge graph. Some works frame it as a node-pair classification task, combining embedding algorithms with traditional machine learning algorithms. This strategy aligns with the logic of a machine learning pipeline. However, the use of negative examples and the lack of validated gene-disease associations to train embedding models may constrain its effectiveness. This work introduces a novel framework for comparing the performance of link prediction versus node-pair classification tasks, analyses the performance of state of the art gene-disease association approaches, and compares the different order-based formalizations of gene-disease association prediction. It also evaluates the impact of the semantic richness through a disease-specific ontology and additional links between ontologies. The framework involves five steps: data splitting, knowledge graph integration, embedding, modeling and prediction, and method evaluation. Results show that enriching the semantic representation of diseases slightly improves performance, while additional links generate a greater impact. Link prediction methods better explore the semantic richness encoded in knowledge graphs. Although node-pair classification methods identify all true positives, link prediction methods outperform overall.\n        △ Less",
        "authors":[
            "Catarina Canastra",
            "Cátia Pesquita"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diffusion Models for Robotic Manipulation: A Survey",
        "abstract":"Diffusion generative models have demonstrated remarkable success in visual domains such as image and video generation. They have also recently emerged as a promising approach in robotics, especially in robot manipulations. Diffusion models leverage a probabilistic framework, and they stand out with their ability to model multi-modal distributions and their robustness to high-dimensional input and output spaces. This survey provides a comprehensive review of state-of-the-art diffusion models in robotic manipulation, including grasp learning, trajectory planning, and data augmentation. Diffusion models for scene and image augmentation lie at the intersection of robotics and computer vision for vision-based tasks to enhance generalizability and data scarcity. This paper also presents the two main frameworks of diffusion models and their integration with imitation learning and reinforcement learning. In addition, it discusses the common architectures and benchmarks and points out the challenges and advantages of current state-of-the-art diffusion-based methods.\n        △ Less",
        "authors":[
            "Rosa Wolf",
            "Yitian Shi",
            "Sheng Liu",
            "Rania Rayyes"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Customizing Spider Silk: Generative Models with Mechanical Property Conditioning for Protein Engineering",
        "abstract":"The remarkable mechanical properties of spider silk, including its tensile strength and extensibility, are primarily governed by the repetitive regions of the proteins that constitute the fiber, the major ampullate spidroins (MaSps). However, establishing correlations between mechanical characteristics and repeat sequences is challenging due to the intricate sequence-structure-function relationships of MaSps and the limited availability of annotated datasets. In this study, we present a novel computational framework for designing MaSp repeat sequences with customizable mechanical properties. To achieve this, we developed a lightweight GPT-based generative model by distilling the pre-trained ProtGPT2 protein language model. The distilled model was subjected to multilevel fine-tuning using curated subsets of the Spider Silkome dataset. Specifically, we adapt the model for MaSp repeat generation using 6,000 MaSp repeat sequences and further refine it with 572 repeats associated with experimentally determined fiber-level mechanical properties. Our model generates biologically plausible MaSp repeat regions tailored to specific mechanical properties while also predicting those properties for given sequences. Validation includes sequence-level analysis, assessing physicochemical attributes and expected distribution of key motifs as well as secondary structure compositions. A correlation study using BLAST on the Spider Silkome dataset and a test set of MaSp repeats with known mechanical properties further confirmed the predictive accuracy of the model. This framework advances the rational design of spider silk-inspired biomaterials, offering a versatile tool for engineering protein sequences with tailored mechanical attributes.\n        △ Less",
        "authors":[
            "Neeru Dubey",
            "Elin Karlsson",
            "Miguel Angel Redondo",
            "Johan Reimegård",
            "Anna Rising",
            "Hedvig Kjellström"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Standardization of Weighted Ranking Correlation Coefficients",
        "abstract":"A relevant problem in statistics is defining the correlation of two rankings of a list of items. Kendall's tau and Spearman's rho are two well established correlation coefficients, characterized by a symmetric form that ensures zero expected value between two pairs of rankings randomly chosen with uniform probability. However, in recent years, several weighted versions of the original Spearman and Kendall coefficients have emerged that take into account the greater importance of top ranks compared to low ranks, which is common in many contexts. The weighting schemes break the symmetry, causing a non-zero expected value between two random rankings. This issue is very relevant, as it undermines the concept of uncorrelation between rankings. In this paper, we address this problem by proposing a standardization function $g(x)$ that maps a correlation ranking coefficient $Γ$ in a standard form $g(Γ)$ that has zero expected value, while maintaining the relevant statistical properties of $Γ$.\n        △ Less",
        "authors":[
            "Pierangelo Lombardo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness",
        "abstract":"Fairness in artificial intelligence (AI) prediction models is increasingly emphasized to support responsible adoption in high-stakes domains such as health care and criminal justice. Guidelines and implementation frameworks highlight the importance of both predictive accuracy and equitable outcomes. However, current fairness toolkits often evaluate classification performance disparities in isolation, with limited attention to other critical aspects such as calibration. To address these gaps, we present seeBias, an R package for comprehensive evaluation of model fairness and predictive performance. seeBias offers an integrated evaluation across classification, calibration, and other performance domains, providing a more complete view of model behavior. It includes customizable visualizations to support transparent reporting and responsible AI implementation. Using public datasets from criminal justice and healthcare, we demonstrate how seeBias supports fairness evaluations, and uncovers disparities that conventional fairness metrics may overlook. The R package is available on GitHub, and a Python version is under development.\n        △ Less",
        "authors":[
            "Yilin Ning",
            "Yian Ma",
            "Mingxuan Liu",
            "Xin Li",
            "Nan Liu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability",
        "abstract":"Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as each others' observations. We propose the use of learned beliefs on the underlying state of the system to overcome these challenges and enable reinforcement learning with fully decentralized training and execution. Our approach leverages state information to pre-train a probabilistic belief model in a self-supervised fashion. The resulting belief states, which capture both inferred state information as well as uncertainty over this information, are then used in a state-based reinforcement learning algorithm to create an end-to-end model for cooperative multi-agent reinforcement learning under partial observability. By separating the belief and reinforcement learning tasks, we are able to significantly simplify the policy and value function learning tasks and improve both the convergence speed and the final performance. We evaluate our proposed method on diverse partially observable multi-agent tasks designed to exhibit different variants of partial observability.\n        △ Less",
        "authors":[
            "Paul J. Pritz",
            "Kin K. Leung"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Constrained Machine Learning Through Hyperspherical Representation",
        "abstract":"The problem of ensuring constraints satisfaction on the output of machine learning models is critical for many applications, especially in safety-critical domains. Modern approaches rely on penalty-based methods at training time, which do not guarantee to avoid constraints violations; or constraint-specific model architectures (e.g., for monotonocity); or on output projection, which requires to solve an optimization problem that might be computationally demanding. We present the Hypersherical Constrained Representation, a novel method to enforce constraints in the output space for convex and bounded feasibility regions (generalizable to star domains). Our method operates on a different representation system, where Euclidean coordinates are converted into hyperspherical coordinates relative to the constrained region, which can only inherently represent feasible points. Experiments on a synthetic and a real-world dataset show that our method has predictive performance comparable to the other approaches, can guarantee 100% constraint satisfaction, and has a minimal computational cost at inference time.\n        △ Less",
        "authors":[
            "Gaetano Signorelli",
            "Michele Lombardi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"BOISHOMMO: Holistic Approach for Bangla Hate Speech",
        "abstract":"One of the most alarming issues in digital society is hate speech (HS) on social media. The severity is so high that researchers across the globe are captivated by this domain. A notable amount of work has been conducted to address the identification and alarm system. However, a noticeable gap exists, especially for low-resource languages. Comprehensive datasets are the main problem among the constrained resource languages, such as Bangla. Interestingly, hate speech or any particular speech has no single dimensionality. Similarly, the hate component can simultaneously have multiple abusive attributes, which seems to be missed in the existing datasets. Thus, a multi-label Bangla hate speech dataset named BOISHOMMO has been compiled and evaluated in this work. That includes categories of HS across race, gender, religion, politics, and more. With over two thousand annotated examples, BOISHOMMO provides a nuanced understanding of hate speech in Bangla and highlights the complexities of processing non-Latin scripts. Apart from evaluating with multiple algorithmic approaches, it also highlights the complexities of processing Bangla text and assesses model performance. This unique multi-label approach enriches future hate speech detection and analysis studies for low-resource languages by providing a more nuanced, diverse dataset.\n        △ Less",
        "authors":[
            "Md Abdullah Al Kafi",
            "Sumit Kumar Banshal",
            "Md Sadman Shakib",
            "Showrov Azam",
            "Tamanna Alam Tabashom"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Graph Reduction with Unsupervised Learning in Column Generation: A Routing Application",
        "abstract":"Column Generation (CG) is a popular method dedicated to enhancing computational efficiency in large scale Combinatorial Optimization (CO) problems. It reduces the number of decision variables in a problem by solving a pricing problem. For many CO problems, the pricing problem is an Elementary Shortest Path Problem with Resource Constraints (ESPPRC). Large ESPPRC instances are difficult to solve to near-optimality. Consequently, we use a Graph neural Network (GNN) to reduces the size of the ESPPRC such that it becomes computationally tractable with standard solving techniques. Our GNN is trained by Unsupervised Learning and outputs a distribution for the arcs to be retained in the reduced PP. The reduced PP is solved by a local search that finds columns with large reduced costs and speeds up convergence. We apply our method on a set of Capacitated Vehicle Routing Problems with Time Windows and show significant improvements in convergence compared to simple reduction techniques from the literature. For a fixed computational budget, we improve the objective values by over 9\\% for larger instances. We also analyze the performance of our CG algorithm and test the generalization of our method to different classes of instances than the training data.\n        △ Less",
        "authors":[
            "Abdo Abouelrous",
            "Laurens Bliea",
            "Adriana F. Gabor",
            "Yaoxin Wu",
            "Yingqian Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MixDiT: Accelerating Image Diffusion Transformer Inference with Mixed-Precision MX Quantization",
        "abstract":"Diffusion Transformer (DiT) has driven significant progress in image generation tasks. However, DiT inferencing is notoriously compute-intensive and incurs long latency even on datacenter-scale GPUs, primarily due to its iterative nature and heavy reliance on GEMM operations inherent to its encoder-based structure. To address the challenge, prior work has explored quantization, but achieving low-precision quantization for DiT inferencing with both high accuracy and substantial speedup remains an open problem. To this end, this paper proposes MixDiT, an algorithm-hardware co-designed acceleration solution that exploits mixed Microscaling (MX) formats to quantize DiT activation values. MixDiT quantizes the DiT activation tensors by selectively applying higher precision to magnitude-based outliers, which produce mixed-precision GEMM operations. To achieve tangible speedup from the mixed-precision arithmetic, we design a MixDiT accelerator that enables precision-flexible multiplications and efficient MX precision conversions. Our experimental results show that MixDiT delivers a speedup of 2.10-5.32 times over RTX 3090, with no loss in FID.\n        △ Less",
        "authors":[
            "Daeun Kim",
            "Jinwoo Hwang",
            "Changhun Oh",
            "Jongse Park"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented Generation",
        "abstract":"Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for grounding large language models in external knowledge sources, improving the precision of agents responses. However, high-dimensional language model embeddings, often in the range of hundreds to thousands of dimensions, can present scalability challenges in terms of storage and latency, especially when processing massive financial text corpora. This paper investigates the use of Principal Component Analysis (PCA) to reduce embedding dimensionality, thereby mitigating computational bottlenecks without incurring large accuracy losses. We experiment with a real-world dataset and compare different similarity and distance metrics under both full-dimensional and PCA-compressed embeddings. Our results show that reducing vectors from 3,072 to 110 dimensions provides a sizeable (up to $60\\times$) speedup in retrieval operations and a $\\sim 28.6\\times$ reduction in index size, with only moderate declines in correlation metrics relative to human-annotated similarity scores. These findings demonstrate that PCA-based compression offers a viable balance between retrieval fidelity and resource efficiency, essential for real-time systems such as Zanista AI's \\textit{Newswitch} platform. Ultimately, our study underscores the practicality of leveraging classical dimensionality reduction techniques to scale RAG architectures for knowledge-intensive applications in finance and trading, where speed, memory efficiency, and accuracy must jointly be optimized.\n        △ Less",
        "authors":[
            "Arman Khaledian",
            "Amirreza Ghadiridehkordi",
            "Nariman Khaledian"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Empirical Investigation of Reconstruction-Based Models for Seizure Prediction from ECG Signals",
        "abstract":"Epileptic seizures are sudden neurological disorders characterized by abnormal, excessive neuronal activity in the brain, which is often associated with changes in cardiovascular activity. These disruptions can pose significant physical and psychological challenges for patients. Therefore, accurate seizure prediction can help mitigate these risks by enabling timely interventions, ultimately improving patients' quality of life. Traditionally, EEG signals have been the primary standard for seizure prediction due to their precision in capturing brain activity. However, their high cost, susceptibility to noise, and logistical constraints limit their practicality, restricting their use to clinical settings. In order to overcome these limitations, this study focuses on leveraging ECG signals as an alternative for seizure prediction. In this paper, we present a novel method for predicting seizures based on detecting anomalies in ECG signals during their reconstruction. By extracting time-frequency features and leveraging various advanced deep learning architectures, the proposed method identifies deviations in heart rate dynamics associated with seizure onset. The proposed approach was evaluated using the Siena database and could achieve specificity of 99.16\\%, accuracy of 76.05\\%, and false positive rate (FPR) of 0.01\/h, with an average prediction time of 45 minutes before seizure onset. These results highlight the potential of ECG-based seizure prediction as a patient-friendly alternative to traditional EEG-based methods.\n        △ Less",
        "authors":[
            "Mohammad Reza Chopannavaz",
            "Foad Ghaderi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and Flash",
        "abstract":"Large language models (LLMs) are increasingly being deployed on mobile devices, but the limited DRAM capacity constrains the deployable model size. This paper introduces ActiveFlow, the first LLM inference framework that can achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the scaling up of deployable model sizes. The framework is based on the novel concept of active weight DRAM-flash swapping and incorporates three novel techniques: (1) Cross-layer active weights preloading. It uses the activations from the current layer to predict the active weights of several subsequent layers, enabling computation and data loading to overlap, as well as facilitating large I\/O transfers. (2) Sparsity-aware self-distillation. It adjusts the active weights to align with the dense-model output distribution, compensating for approximations introduced by contextual sparsity. (3) Active weight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation among the hot weight cache, preloaded active weights, and computation-involved weights based on available memory. Results show ActiveFlow achieves the performance-cost Pareto frontier compared to existing efficiency optimization methods.\n        △ Less",
        "authors":[
            "Fucheng Jia",
            "Zewen Wu",
            "Shiqi Jiang",
            "Huiqiang Jiang",
            "Qianxi Zhang",
            "Yuqing Yang",
            "Yunxin Liu",
            "Ju Ren",
            "Deyu Zhang",
            "Ting Cao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Proofs as Explanations: Short Certificates for Reliable Predictions",
        "abstract":"We consider a model for explainable AI in which an explanation for a prediction $h(x)=y$ consists of a subset $S'$ of the training data (if it exists) such that all classifiers $h' \\in H$ that make at most $b$ mistakes on $S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has label $y$ under the assumption that (1) the target function $h^\\star$ belongs to $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example, if $b=0$ and $H$ is the family of linear classifiers in $\\mathbb{R}^d$, and if $x$ lies inside the convex hull of the positive data points in $S$ (and hence every consistent linear classifier labels $x$ as positive), then Carathéodory's theorem states that $x$ lies inside the convex hull of $d+1$ of those points. So, a set $S'$ of size $d+1$ could be released as an explanation for a positive prediction, and would serve as a short proof of correctness of the prediction under the assumption of realizability.\n  In this work, we consider this problem more generally, for general hypothesis classes $H$ and general values $b\\geq 0$. We define the notion of the robust hollow star number of $H$ (which generalizes the standard hollow star number), and show that it precisely characterizes the worst-case size of the smallest certificate achievable, and analyze its size for natural classes. We also consider worst-case distributional bounds on certificate size, as well as distribution-dependent bounds that we show tightly control the sample size needed to get a certificate for any given test example. In particular, we define a notion of the certificate coefficient $\\varepsilon_x$ of an example $x$ with respect to a data distribution $D$ and target function $h^\\star$, and prove matching upper and lower bounds on sample size as a function of $\\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.\n        △ Less",
        "authors":[
            "Avrim Blum",
            "Steve Hanneke",
            "Chirag Pabbaraju",
            "Donya Saless"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image Representations",
        "abstract":"Visual understanding is inherently contextual -- what we focus on in an image depends on the task at hand. For instance, given an image of a person holding a bouquet of flowers, we may focus on either the person such as their clothing, or the type of flowers, depending on the context of interest. Yet, most existing image encoding paradigms represent an image as a fixed, generic feature vector, overlooking the potential needs of prioritizing varying visual information for different downstream use cases. In this work, we introduce FocalLens, a conditional visual encoding method that produces different representations for the same image based on the context of interest, expressed flexibly through natural language. We leverage vision instruction tuning data and contrastively finetune a pretrained vision encoder to take natural language instructions as additional inputs for producing conditional image representations. Extensive experiments validate that conditional image representation from FocalLens better pronounce the visual features of interest compared to generic features produced by standard vision encoders like CLIP. In addition, we show FocalLens further leads to performance improvements on a range of downstream tasks including image-image retrieval, image classification, and image-text retrieval, with an average gain of 5 and 10 points on the challenging SugarCrepe and MMVP-VLM benchmarks, respectively.\n        △ Less",
        "authors":[
            "Cheng-Yu Hsieh",
            "Pavan Kumar Anasosalu Vasu",
            "Fartash Faghri",
            "Raviteja Vemulapalli",
            "Chun-Liang Li",
            "Ranjay Krishna",
            "Oncel Tuzel",
            "Hadi Pouransari"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DRIP: DRop unImportant data Points -- Enhancing Machine Learning Efficiency with Grad-CAM-Based Real-Time Data Prioritization for On-Device Training",
        "abstract":"Selecting data points for model training is critical in machine learning. Effective selection methods can reduce the labeling effort, optimize on-device training for embedded systems with limited data storage, and enhance the model performance. This paper introduces a novel algorithm that uses Grad-CAM to make online decisions about retaining or discarding data points. Optimized for embedded devices, the algorithm computes a unique DRIP Score to quantify the importance of each data point. This enables dynamic decision-making on whether a data point should be stored for potential retraining or discarded without compromising model performance. Experimental evaluations on four benchmark datasets demonstrate that our approach can match or even surpass the accuracy of models trained on the entire dataset, all while achieving storage savings of up to 39\\%. To our knowledge, this is the first algorithm that makes online decisions about data point retention without requiring access to the entire dataset.\n        △ Less",
        "authors":[
            "Marcus Rüb",
            "Daniel Konegen",
            "Axel Sikora",
            "Daniel Mueller-Gritschneder"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset",
        "abstract":"Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neural Architecture Search (NAS) method that directly focuses on identifying architectures that minimize energy consumption while maintaining acceptable accuracy. Unlike previous methods that primarily target vision and language tasks, the approach proposed here specifically addresses tabular datasets. Remarkably, the optimal architecture suggested by this method can reduce energy consumption by up to 92% compared to architectures recommended by conventional NAS.\n        △ Less",
        "authors":[
            "Hoang-Loc La",
            "Phuong Hoai Ha"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"An Adaptive Clustering Scheme for Client Selections in Communication-Efficient Federated Learning",
        "abstract":"Federated learning is a novel decentralized learning architecture. During the training process, the client and server must continuously upload and receive model parameters, which consumes a lot of network transmission resources. Some methods use clustering to find more representative customers, select only a part of them for training, and at the same time ensure the accuracy of training. However, in federated learning, it is not trivial to know what the number of clusters can bring the best training result. Therefore, we propose to dynamically adjust the number of clusters to find the most ideal grouping results. It may reduce the number of users participating in the training to achieve the effect of reducing communication costs without affecting the model performance. We verify its experimental results on the non-IID handwritten digit recognition dataset and reduce the cost of communication and transmission by almost 50% compared with traditional federated learning without affecting the accuracy of the model.\n        △ Less",
        "authors":[
            "Yan-Ann Chen",
            "Guan-Lin Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates",
        "abstract":"Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry -- especially for loose-fitting clothing -- remains an open challenge. We present a novel method for high-fidelity 3D garment reconstruction from single images that bridges 2D and 3D representations. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn rich garment shape priors in a 2D UV space. A key innovation is our mapping model that establishes correspondences between 2D image pixels, UV pattern coordinates, and 3D geometry, enabling joint optimization of both 3D garment meshes and the corresponding 2D patterns by aligning learned priors with image observations. Despite training exclusively on synthetically simulated cloth data, our method generalizes effectively to real-world images, outperforming existing approaches on both tight- and loose-fitting garments. The reconstructed garments maintain physical plausibility while capturing fine geometric details, enabling downstream applications including garment retargeting and texture manipulation.\n        △ Less",
        "authors":[
            "Ren Li",
            "Cong Cao",
            "Corentin Dumery",
            "Yingxuan You",
            "Hao Li",
            "Pascal Fua"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Entropic bounds for conditionally Gaussian vectors and applications to neural networks",
        "abstract":"Using entropic inequalities from information theory, we provide new bounds on the total variation and 2-Wasserstein distances between a conditionally Gaussian law and a Gaussian law with invertible covariance matrix. We apply our results to quantify the speed of convergence to Gaussian of a randomly initialized fully connected neural network and its derivatives - evaluated in a finite number of inputs - when the initialization is Gaussian and the sizes of the inner layers diverge to infinity. Our results require mild assumptions on the activation function, and allow one to recover optimal rates of convergence in a variety of distances, thus improving and extending the findings of Basteri and Trevisan (2023), Favaro et al. (2023), Trevisan (2024) and Apollonio et al. (2024). One of our main tools are the quantitative cumulant estimates established in Hanin (2024). As an illustration, we apply our results to bound the total variation distance between the Bayesian posterior law of the neural network and its derivatives, and the posterior law of the corresponding Gaussian limit: this yields quantitative versions of a posterior CLT by Hron et al. (2022), and extends several estimates by Trevisan (2024) to the total variation metric.\n        △ Less",
        "authors":[
            "Lucia Celli",
            "Giovanni Peccati"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"High-dimensional Clustering and Signal Recovery under Block Signals",
        "abstract":"This paper studies computationally efficient methods and their minimax optimality for high-dimensional clustering and signal recovery under block signal structures. We propose two sets of methods, cross-block feature aggregation PCA (CFA-PCA) and moving average PCA (MA-PCA), designed for sparse and dense block signals, respectively. Both methods adaptively utilize block signal structures, applicable to non-Gaussian data with heterogeneous variances and non-diagonal covariance matrices. Specifically, the CFA method utilizes a block-wise U-statistic to aggregate and select block signals non-parametrically from data with unknown cluster labels. We show that the proposed methods are consistent for both clustering and signal recovery under mild conditions and weaker signal strengths than the existing methods without considering block structures of signals. Furthermore, we derive both statistical and computational minimax lower bounds (SMLB and CMLB) for high-dimensional clustering and signal recovery under block signals, where the CMLBs are restricted to algorithms with polynomial computation complexity. The minimax boundaries partition signals into regions of impossibility and possibility. No algorithm (or no polynomial time algorithm) can achieve consistent clustering or signal recovery if the signals fall into the statistical (or computational) region of impossibility. We show that the proposed CFA-PCA and MA-PCA methods can achieve the CMLBs for the sparse and dense block signal regimes, respectively, indicating the proposed methods are computationally minimax optimal. A tuning parameter selection method is proposed based on post-clustering signal recovery results. Simulation studies are conducted to evaluate the proposed methods. A case study on global temperature change demonstrates their utility in practice.\n        △ Less",
        "authors":[
            "Wu Su",
            "Yumou Qiu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models",
        "abstract":"Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generality of EHR foundation models and the integration of models trained with different vocabularies. To deal with this problem, we propose MedRep for EHR foundation models based on the observational medical outcome partnership (OMOP) common data model (CDM), providing the integrated medical concept representations and the basic data augmentation strategy for patient trajectories. For concept representation learning, we enrich the information of each concept with a minimal definition through large language model (LLM) prompts and enhance the text-based representations through graph ontology of OMOP vocabulary. Trajectory augmentation randomly replaces selected concepts with other similar concepts that have closely related representations to let the model practice with the concepts out-of-vocabulary. Finally, we demonstrate that EHR foundation models trained with MedRep better maintain the prediction performance in external datasets. Our code implementation is publicly available at https:\/\/github.com\/kicarussays\/MedRep.\n        △ Less",
        "authors":[
            "Junmo Kim",
            "Namkyeong Lee",
            "Jiwon Kim",
            "Kwangsoo Kim"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards generalizable single-cell perturbation modeling via the Conditional Monge Gap",
        "abstract":"Learning the response of single-cells to various treatments offers great potential to enable targeted therapies. In this context, neural optimal transport (OT) has emerged as a principled methodological framework because it inherently accommodates the challenges of unpaired data induced by cell destruction during data acquisition. However, most existing OT approaches are incapable of conditioning on different treatment contexts (e.g., time, drug treatment, drug dosage, or cell type) and we still lack methods that unanimously show promising generalization performance to unseen treatments. Here, we propose the Conditional Monge Gap which learns OT maps conditionally on arbitrary covariates. We demonstrate its value in predicting single-cell perturbation responses conditional to one or multiple drugs, a drug dosage, or combinations thereof. We find that our conditional models achieve results comparable and sometimes even superior to the condition-specific state-of-the-art on scRNA-seq as well as multiplexed protein imaging data. Notably, by aggregating data across conditions we perform cross-task learning which unlocks remarkable generalization abilities to unseen drugs or drug dosages, widely outperforming other conditional models in capturing heterogeneity (i.e., higher moments) in the perturbed population. Finally, by scaling to hundreds of conditions and testing on unseen drugs, we narrow the gap between structure-based and effect-based drug representations, suggesting a promising path to the successful prediction of perturbation effects for unseen treatments.\n        △ Less",
        "authors":[
            "Alice Driessen",
            "Benedek Harsanyi",
            "Marianna Rapsomaniki",
            "Jannis Born"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Academic Network Representation via Prediction-Sampling Incorporated Tensor Factorization",
        "abstract":"Accurate representation to an academic network is of great significance to academic relationship mining like predicting scientific impact. A Latent Factorization of Tensors (LFT) model is one of the most effective models for learning the representation of a target network. However, an academic network is often High-Dimensional and Incomplete (HDI) because the relationships among numerous network entities are impossible to be fully explored, making it difficult for an LFT model to learn accurate representation of the academic network. To address this issue, this paper proposes a Prediction-sampling-based Latent Factorization of Tensors (PLFT) model with two ideas: 1) constructing a cascade LFT architecture to enhance model representation learning ability via learning academic network hierarchical features, and 2) introducing a nonlinear activation-incorporated predicting-sampling strategy to more accurately learn the network representation via generating new academic network data layer by layer. Experimental results from the three real-world academic network datasets show that the PLFT model outperforms existing models when predicting the unexplored relationships among network entities.\n        △ Less",
        "authors":[
            "Chunyang Zhang",
            "Xin Liao",
            "Hao Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SortBench: Benchmarking LLMs based on their ability to sort lists",
        "abstract":"Sorting is a tedious but simple task for human intelligence and can be solved fairly easily algorithmically. However, for Large Language Models (LLMs) this task is surprisingly hard, as some properties of sorting are among known weaknesses of LLMs: being faithful to the input data, logical comparisons between values, and strictly differentiating between syntax (used for sorting) and semantics (typically learned by embeddings). Within this paper, we describe the new SortBench benchmark for LLMs that comes with different difficulties and that can be easily scaled in terms of difficulty. We apply this benchmark to seven state-of-the-art LLMs, including current test-time reasoning models. Our results show that while the o3-mini model is very capable at sorting in general, even this can be fooled if strings are defined to mix syntactical and semantical aspects, e.g., by asking to sort numbers written-out as word. Furthermore, all models have problems with the faithfulness to the input of long lists, i.e., they drop items and add new ones. Our results also show that test-time reasoning has a tendency to overthink problems which leads to performance degradation. Finally, models without test-time reasoning like GPT-4o are not much worse than reasoning models.\n        △ Less",
        "authors":[
            "Steffen Herbold"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DeQompile: quantum circuit decompilation using genetic programming for explainable quantum architecture search",
        "abstract":"Demonstrating quantum advantage using conventional quantum algorithms remains challenging on current noisy gate-based quantum computers. Automated quantum circuit synthesis via quantum machine learning has emerged as a promising solution, employing trainable parametric quantum circuits to alleviate this. The circuit ansatz in these solutions is often designed through reinforcement learning-based quantum architecture search when the domain knowledge of the problem and hardware are not effective. However, the interpretability of these synthesized circuits remains a significant bottleneck, limiting their scalability and applicability across diverse problem domains.\n  This work addresses the challenge of explainability in quantum architecture search (QAS) by introducing a novel genetic programming-based decompiler framework for reverse-engineering high-level quantum algorithms from low-level circuit representations. The proposed approach, implemented in the open-source tool DeQompile, employs program synthesis techniques, including symbolic regression and abstract syntax tree manipulation, to distill interpretable Qiskit algorithms from quantum assembly language. Validation of benchmark algorithms demonstrates the efficacy of our tool. By integrating the decompiler with online learning frameworks, this research potentiates explainable QAS by fostering the development of generalizable and provable quantum algorithms.\n        △ Less",
        "authors":[
            "Shubing Xie",
            "Aritra Sarkar",
            "Sebastian Feld"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ELSA: A Style Aligned Dataset for Emotionally Intelligent Language Generation",
        "abstract":"Advancements in emotion aware language processing increasingly shape vital NLP applications ranging from conversational AI and affective computing to computational psychology and creative content generation. Existing emotion datasets either lack emotional granularity or fail to capture necessary stylistic diversity, limiting the advancement of effective emotion conditioned text generation systems. Seeking to bridge this crucial gap between granularity and style diversity, this paper introduces a novel systematically constructed dataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine grained emotion taxonomies adapted from existing sources such as dair ai emotion dataset and GoEmotions taxonomy. This dataset comprises multiple emotionally nuanced variations of original sentences regenerated across distinct contextual styles such as conversational, formal, poetic, and narrative, using advanced Large Language Models LLMs. Rigorous computational evaluation using metrics such as perplexity, embedding variance, readability, lexical diversity, and semantic coherence measures validates the datasets emotional authenticity, linguistic fluency, and textual diversity. Comprehensive metric analyses affirm its potential to support deeper explorations into emotion conditioned style adaptive text generation. By enabling precision tuned emotionally nuanced language modeling, our dataset creates fertile ground for research on fine grained emotional control, prompt driven explanation, interpretability, and style adaptive expressive language generation with LLMs.\n        △ Less",
        "authors":[
            "Vishal Gandhi",
            "Sagar Gandhi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enabling Automatic Differentiation with Mollified Graph Neural Operators",
        "abstract":"Physics-informed neural operators offer a powerful framework for learning solution operators of partial differential equations (PDEs) by combining data and physics losses. However, these physics losses rely on derivatives. Computing these derivatives remains challenging, with spectral and finite difference methods introducing approximation errors due to finite resolution. Here, we propose the mollified graph neural operator (mGNO), the first method to leverage automatic differentiation and compute \\emph{exact} gradients on arbitrary geometries. This enhancement enables efficient training on irregular grids and varying geometries while allowing seamless evaluation of physics losses at randomly sampled points for improved generalization. For a PDE example on regular grids, mGNO paired with autograd reduced the L2 relative data error by 20x compared to finite differences, although training was slower. It can also solve PDEs on unstructured point clouds seamlessly, using physics losses only, at resolutions vastly lower than those needed for finite differences to be accurate enough. On these unstructured point clouds, mGNO leads to errors that are consistently 2 orders of magnitude lower than machine learning baselines (Meta-PDE) for comparable runtimes, and also delivers speedups from 1 to 3 orders of magnitude compared to the numerical solver for similar accuracy. mGNOs can also be used to solve inverse design and shape optimization problems on complex geometries.\n        △ Less",
        "authors":[
            "Ryan Y. Lin",
            "Julius Berner",
            "Valentin Duruisseaux",
            "David Pitt",
            "Daniel Leibovici",
            "Jean Kossaifi",
            "Kamyar Azizzadenesheli",
            "Anima Anandkumar"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"To See or Not to See -- Fingerprinting Devices in Adversarial Environments Amid Advanced Machine Learning",
        "abstract":"The increasing use of the Internet of Things raises security concerns. To address this, device fingerprinting is often employed to authenticate devices, detect adversaries, and identify eavesdroppers in an environment. This requires the ability to discern between legitimate and malicious devices which is achieved by analyzing the unique physical and\/or operational characteristics of IoT devices. In the era of the latest progress in machine learning, particularly generative models, it is crucial to methodically examine the current studies in device fingerprinting. This involves explaining their approaches and underscoring their limitations when faced with adversaries armed with these ML tools. To systematically analyze existing methods, we propose a generic, yet simplified, model for device fingerprinting. Additionally, we thoroughly investigate existing methods to authenticate devices and detect eavesdropping, using our proposed model. We further study trends and similarities between works in authentication and eavesdropping detection and present the existing threats and attacks in these domains. Finally, we discuss future directions in fingerprinting based on these trends to develop more secure IoT fingerprinting schemes.\n        △ Less",
        "authors":[
            "Justin Feng",
            "Nader Sehatbakhsh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Accelerating Multi-Objective Collaborative Optimization of Doped Thermoelectric Materials via Artificial Intelligence",
        "abstract":"The thermoelectric performance of materials exhibits complex nonlinear dependencies on both elemental types and their proportions, rendering traditional trial-and-error approaches inefficient and time-consuming for material discovery. In this work, we present a deep learning model capable of accurately predicting thermoelectric properties of doped materials directly from their chemical formulas, achieving state-of-the-art performance. To enhance interpretability, we further incorporate sensitivity analysis techniques to elucidate how physical descriptors affect the thermoelectric figure of merit (zT). Moreover, we establish a coupled framework that integrates a surrogate model with a multi-objective genetic algorithm to efficiently explore the vast compositional space for high-performance candidates. Experimental validation confirms the discovery of a novel thermoelectric material with superior $zT$ values in the medium-temperature regime.\n        △ Less",
        "authors":[
            "Yuxuan Zeng",
            "Wenhao Xie",
            "Wei Cao",
            "Tan Peng",
            "Yue Hou",
            "Ziyu Wang",
            "Jing Shi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Understanding the Impact of Data Domain Extraction on Synthetic Data Privacy",
        "abstract":"Privacy attacks, particularly membership inference attacks (MIAs), are widely used to assess the privacy of generative models for tabular synthetic data, including those with Differential Privacy (DP) guarantees. These attacks often exploit outliers, which are especially vulnerable due to their position at the boundaries of the data domain (e.g., at the minimum and maximum values). However, the role of data domain extraction in generative models and its impact on privacy attacks have been overlooked. In this paper, we examine three strategies for defining the data domain: assuming it is externally provided (ideally from public data), extracting it directly from the input data, and extracting it with DP mechanisms. While common in popular implementations and libraries, we show that the second approach breaks end-to-end DP guarantees and leaves models vulnerable. While using a provided domain (if representative) is preferable, extracting it with DP can also defend against popular MIAs, even at high privacy budgets.\n        △ Less",
        "authors":[
            "Georgi Ganev",
            "Meenatchi Sundaram Muthu Selva Annamalai",
            "Sofiane Mahiou",
            "Emiliano De Cristofaro"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Network-assisted Interval Reachability for Systems with Control Barrier Function-Based Safe Controllers",
        "abstract":"Control Barrier Functions (CBFs) have been widely utilized in the design of optimization-based controllers and filters for dynamical systems to ensure forward invariance of a given set of safe states. While CBF-based controllers offer safety guarantees, they can compromise the performance of the system, leading to undesirable behaviors such as unbounded trajectories and emergence of locally stable spurious equilibria. Computing reachable sets for systems with CBF-based controllers is an effective approach for runtime performance and stability verification, and can potentially serve as a tool for trajectory re-planning. In this paper, we propose a computationally efficient interval reachability method for performance verification of systems with optimization-based controllers by: (i) approximating the optimization-based controller by a pre-trained neural network to avoid solving optimization problems repeatedly, and (ii) using mixed monotone theory to construct an embedding system that leverages state-of-the-art neural network verification algorithms for bounding the output of the neural network. Results in terms of closeness of solutions of trajectories of the system with the optimization-based controller and the neural network are derived. Using a single trajectory of the embedding system along with our closeness of solutions result, we obtain an over-approximation of the reachable set of the system with optimization-based controllers. Numerical results are presented to corroborate the technical findings.\n        △ Less",
        "authors":[
            "Damola Ajeyemi",
            "Saber Jafarpour",
            "Emiliano Dall'Anese"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner",
        "abstract":"State-based sequence models like RWKV-7 offer a compelling alternative to Transformer architectures, achieving linear complexity while demonstrating greater expressive power in short-context scenarios and enabling state tracking beyond the \\(\\text{TC}^0\\) complexity class. However, RWKV-7 lacks mechanisms for token-parameter interactions and native scalability, limiting its adaptability and growth without retraining. In this paper, we propose \\textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention mechanisms with a fully state-driven approach, integrating token-parameter interactions through a \\textbf{Self-State Encoder} (SSE) mechanism. The SSE repurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as transformation weights to encode token-parameter interactions in a linear, state-driven manner without introducing new trainable matrices or softmax operations, while preserving the autoregressive property of token processing. Meta-State supports progressive model scaling by expanding the WKV state and parameter tokens, reusing existing parameters without retraining. Our approach bridges the gap between state-based modeling, token-parameter interactions, and scalable architectures, offering a flexible framework for efficient and adaptable sequence modeling with linear complexity and constant memory usage.\n        △ Less",
        "authors":[
            "Liu Xiao",
            "Li Zhiyuan",
            "Lin Yueyu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Spectral Normalization for Lipschitz-Constrained Policies on Learning Humanoid Locomotion",
        "abstract":"Reinforcement learning (RL) has shown great potential in training agile and adaptable controllers for legged robots, enabling them to learn complex locomotion behaviors directly from experience. However, policies trained in simulation often fail to transfer to real-world robots due to unrealistic assumptions such as infinite actuator bandwidth and the absence of torque limits. These conditions allow policies to rely on abrupt, high-frequency torque changes, which are infeasible for real actuators with finite bandwidth.\n  Traditional methods address this issue by penalizing aggressive motions through regularization rewards, such as joint velocities, accelerations, and energy consumption, but they require extensive hyperparameter tuning. Alternatively, Lipschitz-Constrained Policies (LCP) enforce finite bandwidth action control by penalizing policy gradients, but their reliance on gradient calculations introduces significant GPU memory overhead. To overcome this limitation, this work proposes Spectral Normalization (SN) as an efficient replacement for enforcing Lipschitz continuity. By constraining the spectral norm of network weights, SN effectively limits high-frequency policy fluctuations while significantly reducing GPU memory usage. Experimental evaluations in both simulation and real-world humanoid robot show that SN achieves performance comparable to gradient penalty methods while enabling more efficient parallel training.\n        △ Less",
        "authors":[
            "Jaeyong Shin",
            "Woohyun Cha",
            "Donghyeon Kim",
            "Junhyeok Cha",
            "Jaeheung Park"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Bringing Structure to Naturalness: On the Naturalness of ASTs",
        "abstract":"Source code comes in different shapes and forms. Previous research has already shown code to be more predictable than natural language as well as highlighted its statistical predictability at the token level: source code can be natural. More recently, the structure of code -- control flow, syntax graphs, abstract syntax trees etc. -- has been successfully used to improve the state-of-the-art on numerous tasks: code suggestion, code summarisation, method naming etc. This body of work implicitly assumes that structured representations of code are similarly statistically predictable, i.e. that a structured view of code is also natural. We consider that this view should be made explicit and propose directly studying the Structured Naturalness Hypothesis. Beyond just naming existing research that assumes this hypothesis and formulating it, we also provide evidence in the case of trees: TreeLSTM models over ASTs for some languages, such as Ruby, are competitive with $n$-gram models while handling the syntax token issue highlighted by previous research 'for free'. For other languages, such as Java or Python, we find tree models to perform worse, suggesting that downstream task improvement is uncorrelated to the language modelling task. Further, we show how such naturalness signals can be employed for near state-of-the-art results on just-in-time defect prediction while forgoing manual feature engineering work.\n        △ Less",
        "authors":[
            "Profir-Petru Pârţachi",
            "Mahito Sugiyama"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DaemonSec: Examining the Role of Machine Learning for Daemon Security in Linux Environments",
        "abstract":"DaemonSec is an early-stage startup exploring machine learning (ML)-based security for Linux daemons, a critical yet often overlooked attack surface. While daemon security remains underexplored, conventional defenses struggle against adaptive threats and zero-day exploits. To assess the perspectives of IT professionals on ML-driven daemon protection, a systematic interview study based on semi-structured interviews was conducted with 22 professionals from industry and academia. The study evaluates adoption, feasibility, and trust in ML-based security solutions. While participants recognized the potential of ML for real-time anomaly detection, findings reveal skepticism toward full automation, limited security awareness among non-security roles, and concerns about patching delays creating attack windows. This paper presents the methods, key findings, and implications for advancing ML-driven daemon security in industry.\n        △ Less",
        "authors":[
            "Sheikh Muhammad Farjad"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"All Optical Echo State Network Reservoir Computing",
        "abstract":"We propose an innovative design for an all-optical Echo State Network (ESN), an advanced type of reservoir computer known for its universal computational capabilities. Our design enables fully optical implementation of arbitrary ESNs, featuring complete flexibility in optical matrix multiplication and nonlinear activation. Leveraging the nonlinear characteristics of stimulated Brillouin scattering (SBS), the architecture efficiently realizes measurement-free operations crucial for reservoir computing. The approach significantly reduces computational overhead and energy consumption compared to traditional software-based methods. Comprehensive simulations validate the system's memory capacity, nonlinear processing strength, and polynomial algebra capabilities, showcasing performance comparable to software ESNs across key benchmark tasks. Our design establishes a feasible, scalable, and universally applicable framework for optical reservoir computing, suitable for diverse machine learning applications.\n        △ Less",
        "authors":[
            "Ishwar S Kaushik",
            "Peter J Ehlers",
            "Daniel Soh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DrivAer Transformer: A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset",
        "abstract":"At the current stage, deep learning-based methods have demonstrated excellent capabilities in evaluating aerodynamic performance, significantly reducing the time and cost required for traditional computational fluid dynamics (CFD) simulations. However, when faced with the task of processing extremely complex three-dimensional (3D) vehicle models, the lack of large-scale datasets and training resources, coupled with the inherent diversity and complexity of the geometry of different vehicle models, means that the prediction accuracy and versatility of these networks are still not up to the level required for current production. In view of the remarkable success of Transformer models in the field of natural language processing and their strong potential in the field of image processing, this study innovatively proposes a point cloud learning framework called DrivAer Transformer (DAT). The DAT structure uses the DrivAerNet++ dataset, which contains high-fidelity CFD data of industrial-standard 3D vehicle shapes. enabling accurate estimation of air drag directly from 3D meshes, thus avoiding the limitations of traditional methods such as 2D image rendering or signed distance fields (SDF). DAT enables fast and accurate drag prediction, driving the evolution of the aerodynamic evaluation process and laying the critical foundation for introducing a data-driven approach to automotive design. The framework is expected to accelerate the vehicle design process and improve development efficiency.\n        △ Less",
        "authors":[
            "Jiaqi He",
            "Xiangwen Luo",
            "Yiping Wang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Local Distance-Preserving Node Embeddings and Their Performance on Random Graphs",
        "abstract":"Learning node representations is a fundamental problem in graph machine learning. While existing embedding methods effectively preserve local similarity measures, they often fail to capture global functions like graph distances. Inspired by Bourgain's seminal work on Hilbert space embeddings of metric spaces (1985), we study the performance of local distance-preserving node embeddings. Known as landmark-based algorithms, these embeddings approximate pairwise distances by computing shortest paths from a small subset of reference nodes (i.e., landmarks). Our main theoretical contribution shows that random graphs, such as Erdős-Rényi random graphs, require lower dimensions in landmark-based embeddings compared to worst-case graphs. Empirically, we demonstrate that the GNN-based approximations for the distances to landmarks generalize well to larger networks, offering a scalable alternative for graph representation learning.\n        △ Less",
        "authors":[
            "My Le",
            "Luana Ruiz",
            "Souvik Dhara"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Distributional Learning with Non-crossing Quantile Network",
        "abstract":"In this paper, we introduce a non-crossing quantile (NQ) network for conditional distribution learning. By leveraging non-negative activation functions, the NQ network ensures that the learned distributions remain monotonic, effectively addressing the issue of quantile crossing. Furthermore, the NQ network-based deep distributional learning framework is highly adaptable, applicable to a wide range of applications, from classical non-parametric quantile regression to more advanced tasks such as causal effect estimation and distributional reinforcement learning (RL). We also develop a comprehensive theoretical foundation for the deep NQ estimator and its application to distributional RL, providing an in-depth analysis that demonstrates its effectiveness across these domains. Our experimental results further highlight the robustness and versatility of the NQ network.\n        △ Less",
        "authors":[
            "Guohao Shen",
            "Runpeng Dai",
            "Guojun Wu",
            "Shikai Luo",
            "Chengchun Shi",
            "Hongtu Zhu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Optimizing Power Grid Topologies with Reinforcement Learning: A Survey of Methods and Challenges",
        "abstract":"Power grid operation is becoming increasingly complex due to the rising integration of renewable energy sources and the need for more adaptive control strategies. Reinforcement Learning (RL) has emerged as a promising approach to power network control (PNC), offering the potential to enhance decision-making in dynamic and uncertain environments. The Learning To Run a Power Network (L2RPN) competitions have played a key role in accelerating research by providing standardized benchmarks and problem formulations, leading to rapid advancements in RL-based methods. This survey provides a comprehensive and structured overview of RL applications for power grid topology optimization, categorizing existing techniques, highlighting key design choices, and identifying gaps in current research. Additionally, we present a comparative numerical study evaluating the impact of commonly applied RL-based methods, offering insights into their practical effectiveness. By consolidating existing research and outlining open challenges, this survey aims to provide a foundation for future advancements in RL-driven power grid optimization.\n        △ Less",
        "authors":[
            "Erica van der Sar",
            "Alessandro Zocca",
            "Sandjai Bhulai"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DRAFT-ing Architectural Design Decisions using LLMs",
        "abstract":"Architectural Knowledge Management (AKM) is crucial for software development but remains challenging due to the lack of standardization and high manual effort. Architecture Decision Records (ADRs) provide a structured approach to capture Architecture Design Decisions (ADDs), but their adoption is limited due to the manual effort involved and insufficient tool support. Our previous work has shown that Large Language Models (LLMs) can assist in generating ADDs. However, simply prompting the LLM does not produce quality ADDs. Moreover, using third-party LLMs raises privacy concerns, while self-hosting them poses resource challenges.\n  To this end, we experimented with different approaches like few-shot, retrieval-augmented generation (RAG) and fine-tuning to enhance LLM's ability to generate ADDs. Our results show that both techniques improve effectiveness. Building on this, we propose Domain Specific Retreival Augumented Few Shot Fine Tuninng, DRAFT, which combines the strengths of all these three approaches for more effective ADD generation. DRAFT operates in two phases: an offline phase that fine-tunes an LLM on generating ADDs augmented with retrieved examples and an online phase that generates ADDs by leveraging retrieved ADRs and the fine-tuned model.\n  We evaluated DRAFT against existing approaches on a dataset of 4,911 ADRs and various LLMs and analyzed them using automated metrics and human evaluations. Results show DRAFT outperforms all other approaches in effectiveness while maintaining efficiency. Our findings indicate that DRAFT can aid architects in drafting ADDs while addressing privacy and resource constraints.\n        △ Less",
        "authors":[
            "Rudra Dhar",
            "Adyansh Kakran",
            "Amey Karan",
            "Karthik Vaidhyanathan",
            "Vasudeva Varma"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neural Encoding and Decoding at Scale",
        "abstract":"Recent work has demonstrated that large-scale, multi-animal models are powerful tools for characterizing the relationship between neural activity and behavior. Current large-scale approaches, however, focus exclusively on either predicting neural activity from behavior (encoding) or predicting behavior from neural activity (decoding), limiting their ability to capture the bidirectional relationship between neural activity and behavior. To bridge this gap, we introduce a multimodal, multi-task model that enables simultaneous Neural Encoding and Decoding at Scale (NEDS). Central to our approach is a novel multi-task-masking strategy, which alternates between neural, behavioral, within-modality, and cross-modality masking. We pretrain our method on the International Brain Laboratory (IBL) repeated site dataset, which includes recordings from 83 animals performing the same visual decision-making task. In comparison to other large-scale models, we demonstrate that NEDS achieves state-of-the-art performance for both encoding and decoding when pretrained on multi-animal data and then fine-tuned on new animals. Surprisingly, NEDS's learned embeddings exhibit emergent properties: even without explicit training, they are highly predictive of the brain regions in each recording. Altogether, our approach is a step towards a foundation model of the brain that enables seamless translation between neural activity and behavior.\n        △ Less",
        "authors":[
            "Yizi Zhang",
            "Yanchen Wang",
            "Mehdi Azabou",
            "Alexandre Andre",
            "Zixuan Wang",
            "Hanrui Lyu",
            "The International Brain Laboratory",
            "Eva Dyer",
            "Liam Paninski",
            "Cole Hurwitz"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Influential Bandits: Pulling an Arm May Change the Environment",
        "abstract":"While classical formulations of multi-armed bandit problems assume that each arm's reward is independent and stationary, real-world applications often involve non-stationary environments and interdependencies between arms. In particular, selecting one arm may influence the future rewards of other arms, a scenario not adequately captured by existing models such as rotting bandits or restless bandits. To address this limitation, we propose the influential bandit problem, which models inter-arm interactions through an unknown, symmetric, positive semi-definite interaction matrix that governs the dynamics of arm losses. We formally define this problem and establish two regret lower bounds, including a superlinear $Ω(T^2 \/ \\log^2 T)$ bound for the standard UCB algorithm and an algorithm-independent $Ω(T)$ bound, which highlight the inherent difficulty of the setting. We then introduce a new algorithm based on a lower confidence bound (LCB) estimator tailored to the structure of the loss dynamics. Under mild assumptions, our algorithm achieves a regret of $O(KT \\log T)$, which is nearly optimal in terms of its dependence on the time horizon. The algorithm is simple to implement and computationally efficient. Empirical evaluations on both synthetic and real-world datasets demonstrate the presence of inter-arm influence and confirm the superior performance of our method compared to conventional bandit algorithms.\n        △ Less",
        "authors":[
            "Ryoma Sato",
            "Shinji Ito"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The More is not the Merrier: Investigating the Effect of Client Size on Federated Learning",
        "abstract":"Federated Learning (FL) has been introduced as a way to keep data local to clients while training a shared machine learning model, as clients train on their local data and send trained models to a central aggregator. It is expected that FL will have a huge implication on Mobile Edge Computing, the Internet of Things, and Cross-Silo FL. In this paper, we focus on the widely used FedAvg algorithm to explore the effect of the number of clients in FL. We find a significant deterioration of learning accuracy for FedAvg as the number of clients increases. To address this issue for a general application, we propose a method called Knowledgeable Client Insertion (KCI) that introduces a very small number of knowledgeable clients to the MEC setting. These knowledgeable clients are expected to have accumulated a large set of data samples to help with training. With the help of KCI, the learning accuracy of FL increases much faster even with a normal FedAvg aggregation technique. We expect this approach to be able to provide great privacy protection for clients against security attacks such as model inversion attacks. Our code is available at https:\/\/github.com\/Eleanor-W\/KCI_for_FL.\n        △ Less",
        "authors":[
            "Eleanor Wallach",
            "Sage Siler",
            "Jing Deng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs",
        "abstract":"Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.\n        △ Less",
        "authors":[
            "Aashiq Muhamed",
            "Jacopo Bonato",
            "Mona Diab",
            "Virginia Smith"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative Analysis of Different Methods for Classifying Polychromatic Sketches",
        "abstract":"Image classification is a significant challenge in computer vision, particularly in domains humans are not accustomed to. As machine learning and artificial intelligence become more prominent, it is crucial these algorithms develop a sense of sight that is on par with or exceeds human ability. For this reason, we have collected, cleaned, and parsed a large dataset of hand-drawn doodles and compared multiple machine learning solutions to classify these images into 170 distinct categories. The best model we found achieved a Top-1 accuracy of 47.5%, significantly surpassing human performance on the dataset, which stands at 41%.\n        △ Less",
        "authors":[
            "Fahd Baba",
            "Devon Mack"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Detecting Credit Card Fraud via Heterogeneous Graph Neural Networks with Graph Attention",
        "abstract":"This study proposes a credit card fraud detection method based on Heterogeneous Graph Neural Network (HGNN) to address fraud in complex transaction networks. Unlike traditional machine learning methods that rely solely on numerical features of transaction records, this approach constructs heterogeneous transaction graphs. These graphs incorporate multiple node types, including users, merchants, and transactions. By leveraging graph neural networks, the model captures higher-order transaction relationships. A Graph Attention Mechanism is employed to dynamically assign weights to different transaction relationships. Additionally, a Temporal Decay Mechanism is integrated to enhance the model's sensitivity to time-related fraud patterns. To address the scarcity of fraudulent transaction samples, this study applies SMOTE oversampling and Cost-sensitive Learning. These techniques strengthen the model's ability to identify fraudulent transactions. Experimental results demonstrate that the proposed method outperforms existing GNN models, including GCN, GAT, and GraphSAGE, on the IEEE-CIS Fraud Detection dataset. The model achieves notable improvements in both accuracy and OC-ROC. Future research may explore the integration of dynamic graph neural networks and reinforcement learning. Such advancements could enhance the real-time adaptability of fraud detection systems and provide more intelligent solutions for financial risk control.\n        △ Less",
        "authors":[
            "Qiuwu Sha",
            "Tengda Tang",
            "Xinyu Du",
            "Jie Liu",
            "Yixian Wang",
            "Yuan Sheng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Particle Hit Clustering and Identification Using Point Set Transformers in Liquid Argon Time Projection Chambers",
        "abstract":"Liquid argon time projection chambers are often used in neutrino physics and dark-matter searches because of their high spatial resolution. The images generated by these detectors are extremely sparse, as the energy values detected by most of the detector are equal to 0, meaning that despite their high resolution, most of the detector is unused in a particular interaction. Instead of representing all of the empty detections, the interaction is usually stored as a sparse matrix, a list of detection locations paired with their energy values. Traditional machine learning methods that have been applied to particle reconstruction such as convolutional neural networks (CNNs), however, cannot operate over data stored in this way and therefore must have the matrix fully instantiated as a dense matrix. Operating on dense matrices requires a lot of memory and computation time, in contrast to directly operating on the sparse matrix. We propose a machine learning model using a point set neural network that operates over a sparse matrix, greatly improving both processing speed and accuracy over methods that instantiate the dense matrix, as well as over other methods that operate over sparse matrices. Compared to competing state-of-the-art methods, our method improves classification performance by 14%, segmentation performance by more than 22%, while taking 80% less time and using 66% less memory. Compared to state-of-the-art CNN methods, our method improves classification performance by more than 86%, segmentation performance by more than 71%, while reducing runtime by 91% and reducing memory usage by 61%.\n        △ Less",
        "authors":[
            "Edgar E. Robles",
            "Alejando Yankelevich",
            "Wenjie Wu",
            "Jianming Bian",
            "Pierre Baldi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression",
        "abstract":"Motivated by robust and quantile regression problems, we investigate the stochastic gradient descent (SGD) algorithm for minimizing an objective function $f$ that is locally strongly convex with a sub--quadratic tail. This setting covers many widely used online statistical methods. We introduce a novel piecewise Lyapunov function that enables us to handle functions $f$ with only first-order differentiability, which includes a wide range of popular loss functions such as Huber loss. Leveraging our proposed Lyapunov function, we derive finite-time moment bounds under general diminishing stepsizes, as well as constant stepsizes. We further establish the weak convergence, central limit theorem and bias characterization under constant stepsize, providing the first geometrical convergence result for sub--quadratic SGD. Our results have wide applications, especially in online statistical methods. In particular, we discuss two applications of our results. 1) Online robust regression: We consider a corrupted linear model with sub--exponential covariates and heavy--tailed noise. Our analysis provides convergence rates comparable to those for corrupted models with Gaussian covariates and noise. 2) Online quantile regression: Importantly, our results relax the common assumption in prior work that the conditional density is continuous and provide a more fine-grained analysis for the moment bounds.\n        △ Less",
        "authors":[
            "Yixuan Zhang",
            "Dongyan Huo",
            "Yudong Chen",
            "Qiaomin Xie"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Efficient measurement of neutral-atom qubits with matched filters",
        "abstract":"Quantum computers require high-fidelity measurement of many qubits to achieve a quantum advantage. Traditional approaches suffer from readout crosstalk for a neutral-atom quantum processor with a tightly spaced array. Although classical machine learning algorithms based on convolutional neural networks can improve fidelity, they are computationally expensive, making it difficult to scale them to large qubit counts. We present two simpler and scalable machine learning algorithms that realize matched filters for the readout problem. One is a local model that focuses on a single qubit, and the other uses information from neighboring qubits in the array to prevent crosstalk among the qubits. We demonstrate error reductions of up to 32% and 43% for the site and array models, respectively, compared to a conventional Gaussian threshold approach. Additionally, our array model uses two orders of magnitude fewer trainable parameters and four orders of magnitude fewer multiplications and nonlinear function evaluations than a recent convolutional neural network approach, with only a minor (3.5%) increase in error across different readout times. Another strength of our approach is its physical interpretability: the learned filter can be visualized to provide insights into experimental imperfections. We also show that a convolutional neural network model for improved can be pruned to have 70x and 4000x fewer parameters, respectively, while maintaining similar errors. Our work shows that simple machine learning approaches can achieve high-fidelity qubit measurements while remaining scalable to systems with larger qubit counts.\n        △ Less",
        "authors":[
            "Robert M. Kent",
            "Linipun Phuttitarn",
            "Chaithanya Naik Mude",
            "Swamit Tannu",
            "Mark Saffman",
            "Gregory Lafyatis",
            "Daniel J. Gauthier"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction",
        "abstract":"The predictions of click through rate (CTR) and conversion rate (CVR) play a crucial role in the success of ad-recommendation systems. A Deep Hierarchical Ensemble Network (DHEN) has been proposed to integrate multiple feature crossing modules and has achieved great success in CTR prediction. However, its performance for CVR prediction is unclear in the conversion ads setting, where an ad bids for the probability of a user's off-site actions on a third party website or app, including purchase, add to cart, sign up, etc. A few challenges in DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve the best trade-off between efficiency and efficacy? 3) What hyper-parameters to choose in each feature-crossing module? Orthogonal to the model architecture, the input personalization features also significantly impact model performance with a high degree of freedom. In this paper, we attack this problem and present our contributions biased to the applied data science side, including:\n  First, we propose a multitask learning framework with DHEN as the single backbone model architecture to predict all CVR tasks, with a detailed study on how to make DHEN work effectively in practice; Second, we build both on-site real-time user behavior sequences and off-site conversion event sequences for CVR prediction purposes, and conduct ablation study on its importance; Last but not least, we propose a self-supervised auxiliary loss to predict future actions in the input sequence, to help resolve the label sparseness issue in CVR prediction.\n  Our method achieves state-of-the-art performance compared to previous single feature crossing modules with pre-trained user personalization features.\n        △ Less",
        "authors":[
            "Jinfeng Zhuang",
            "Yinrui Li",
            "Runze Su",
            "Ke Xu",
            "Zhixuan Shao",
            "Kungang Li",
            "Ling Leng",
            "Han Sun",
            "Meng Qi",
            "Yixiong Meng",
            "Yang Tang",
            "Zhifang Liu",
            "Qifei Shen",
            "Aayush Mudgal"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Rethinking the Foundations for Continual Reinforcement Learning",
        "abstract":"Algorithms and approaches for continual reinforcement learning have gained increasing attention. Much of this early progress rests on the foundations and standard practices of traditional reinforcement learning, without questioning if they are well-suited to the challenges of continual learning agents. We suggest that many core foundations of traditional RL are, in fact, antithetical to the goals of continual reinforcement learning. We enumerate four such foundations: the Markov decision process formalism, a focus on optimal policies, the expected sum of rewards as the primary evaluation metric, and episodic benchmark environments that embrace the other three foundations. Shedding such sacredly held and taught concepts is not easy. They are self-reinforcing in that each foundation depends upon and holds up the others, making it hard to rethink each in isolation. We propose an alternative set of all four foundations that are better suited to the continual learning setting. We hope to spur on others in rethinking the traditional foundations, proposing and critiquing alternatives, and developing new algorithms and approaches enabled by better-suited foundations.\n        △ Less",
        "authors":[
            "Michael Bowling",
            "Esraa Elelimy"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"External-Wrench Estimation for Aerial Robots Exploiting a Learned Model",
        "abstract":"This paper presents an external wrench estimator that uses a hybrid dynamics model consisting of a first-principles model and a neural network. This framework addresses one of the limitations of the state-of-the-art model-based wrench observers: the wrench estimation of these observers comprises the external wrench (e.g. collision, physical interaction, wind); in addition to residual wrench (e.g. model parameters uncertainty or unmodeled dynamics). This is a problem if these wrench estimations are to be used as wrench feedback to a force controller, for example. In the proposed framework, a neural network is combined with a first-principles model to estimate the residual dynamics arising from unmodeled dynamics and parameters uncertainties, then, the hybrid trained model is used to estimate the external wrench, leading to a wrench estimation that has smaller contributions from the residual dynamics, and affected more by the external wrench. This method is validated with numerical simulations of an aerial robot in different flying scenarios and different types of residual dynamics, and the statistical analysis of the results shows that the wrench estimation error has improved significantly compared to a model-based wrench observer using only a first-principles model.\n        △ Less",
        "authors":[
            "Ayham Alharbat",
            "Gabriele Ruscelli",
            "Roberto Diversi",
            "Abeje Mersha"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Bounded Exploration and Intermediate Actions for Data Debiasing",
        "abstract":"The performance of algorithmic decision rules is largely dependent on the quality of training datasets available to them. Biases in these datasets can raise economic and ethical concerns due to the resulting algorithms' disparate treatment of different groups. In this paper, we propose algorithms for sequentially debiasing the training dataset through adaptive and bounded exploration in a classification problem with costly and censored feedback. Our proposed algorithms balance between the ultimate goal of mitigating the impacts of data biases -- which will in turn lead to more accurate and fairer decisions, and the exploration risks incurred to achieve this goal. Specifically, we propose adaptive bounds to limit the region of exploration, and leverage intermediate actions which provide noisy label information at a lower cost. We analytically show that such exploration can help debias data in certain distributions, investigate how {algorithmic fairness interventions} can work in conjunction with our proposed algorithms, and validate the performance of these algorithms through numerical experiments on synthetic and real-world data.\n        △ Less",
        "authors":[
            "Yifan Yang",
            "Yang Liu",
            "Parinaz Naghizadeh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beyond Feature Importance: Feature Interactions in Predicting Post-Stroke Rigidity with Graph Explainable AI",
        "abstract":"This study addresses the challenge of predicting post-stroke rigidity by emphasizing feature interactions through graph-based explainable AI. Post-stroke rigidity, characterized by increased muscle tone and stiffness, significantly affects survivors' mobility and quality of life. Despite its prevalence, early prediction remains limited, delaying intervention. We analyze 519K stroke hospitalization records from the Healthcare Cost and Utilization Project dataset, where 43% of patients exhibited rigidity. We compare traditional approaches such as Logistic Regression, XGBoost, and Transformer with graph-based models like Graphormer and Graph Attention Network. These graph models inherently capture feature interactions and incorporate intrinsic or post-hoc explainability. Our results show that graph-based methods outperform others (AUROC 0.75), identifying key predictors such as NIH Stroke Scale and APR-DRG mortality risk scores. They also uncover interactions missed by conventional models. This research provides a novel application of graph-based XAI in stroke prognosis, with potential to guide early identification and personalized rehabilitation strategies.\n        △ Less",
        "authors":[
            "Jiawei Xu",
            "Yonggeon Lee",
            "Anthony Elkommos Youssef",
            "Eunjin Yun",
            "Tinglin Huang",
            "Tianjian Guo",
            "Hamidreza Saber",
            "Rex Ying",
            "Ying Ding"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LoRAX: LoRA eXpandable Networks for Continual Synthetic Image Attribution",
        "abstract":"As generative AI image technologies become more widespread and advanced, there is a growing need for strong attribution models. These models are crucial for verifying the authenticity of images and identifying the architecture of their originating generative models-key to maintaining media integrity. However, attribution models struggle to generalize to unseen models, and traditional fine-tuning methods for updating these models have shown to be impractical in real-world settings. To address these challenges, we propose LoRA eXpandable Networks (LoRAX), a parameter-efficient class incremental algorithm that adapts to novel generative image models without the need for full retraining. Our approach trains an extremely parameter-efficient feature extractor per continual learning task via Low Rank Adaptation. Each task-specific feature extractor learns distinct features while only requiring a small fraction of the parameters present in the underlying feature extractor's backbone model. Our extensive experimentation shows LoRAX outperforms or remains competitive with state-of-the-art class incremental learning algorithms on the Continual Deepfake Detection benchmark across all training scenarios and memory settings, while requiring less than 3% of the number of trainable parameters per feature extractor compared to the full-rank implementation. LoRAX code is available at: https:\/\/github.com\/mit-ll\/lorax_cil.\n        △ Less",
        "authors":[
            "Danielle Sullivan-Pao",
            "Nicole Tian",
            "Pooya Khorrami"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Orchestrating Agents and Data for Enterprise: A Blueprint Architecture for Compound AI",
        "abstract":"Large language models (LLMs) have gained significant interest in industry due to their impressive capabilities across a wide range of tasks. However, the widespread adoption of LLMs presents several challenges, such as integration into existing applications and infrastructure, utilization of company proprietary data, models, and APIs, and meeting cost, quality, responsiveness, and other requirements. To address these challenges, there is a notable shift from monolithic models to compound AI systems, with the premise of more powerful, versatile, and reliable applications. However, progress thus far has been piecemeal, with proposals for agentic workflows, programming models, and extended LLM capabilities, without a clear vision of an overall architecture. In this paper, we propose a 'blueprint architecture' for compound AI systems for orchestrating agents and data for enterprise applications. In our proposed architecture the key orchestration concept is 'streams' to coordinate the flow of data and instructions among agents. Existing proprietary models and APIs in the enterprise are mapped to 'agents', defined in an 'agent registry' that serves agent metadata and learned representations for search and planning. Agents can utilize proprietary data through a 'data registry' that similarly registers enterprise data of various modalities. Tying it all together, data and task 'planners' break down, map, and optimize tasks and queries for given quality of service (QoS) requirements such as cost, accuracy, and latency. We illustrate an implementation of the architecture for a use-case in the HR domain and discuss opportunities and challenges for 'agentic AI' in the enterprise.\n        △ Less",
        "authors":[
            "Eser Kandogan",
            "Nikita Bhutani",
            "Dan Zhang",
            "Rafael Li Chen",
            "Sairam Gurajada",
            "Estevam Hruschka"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Variational quantum and neural quantum states algorithms for the linear complementarity problem",
        "abstract":"Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of concept, their practical utility in solving real-world problems -- and whether quantum-inspired classical algorithms can match their performance -- remains an open question. We present a novel application of the variational quantum linear solver (VQLS) and its classical neural quantum states-based counterpart, the variational neural linear solver (VNLS), as key components within a minimum map Newton solver for a complementarity-based rigid body contact model. We demonstrate using the VNLS that our solver accurately simulates the dynamics of rigid spherical bodies during collision events. These results suggest that quantum and quantum-inspired linear algebra algorithms can serve as viable alternatives to standard linear algebra solvers for modeling certain physical systems.\n        △ Less",
        "authors":[
            "Saibal De",
            "Oliver Knitter",
            "Rohan Kodati",
            "Paramsothy Jayakumar",
            "James Stokes",
            "Shravan Veerapaneni"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Empowering Vector Architectures for ML: The CAMP Architecture for Matrix Multiplication",
        "abstract":"This study presents the Cartesian Accumulative Matrix Pipeline (CAMP) architecture, a novel approach designed to enhance matrix multiplication in Vector Architectures (VAs) and Single Instruction Multiple Data (SIMD) units. CAMP improves the processing efficiency of Quantized Neural Networks (QNNs). Matrix multiplication is a cornerstone of machine learning applications, and its quantized versions are increasingly popular for more efficient operations. Unfortunately, existing VAs and SIMD-support units struggle to efficiently handle these quantized formats. In this work, we propose CAMP, a simple yet effective architecture that leverages a hybrid multiplier. The CAMP architecture significantly advances the performance of vector architectures in handling quantized data, enabling more efficient execution of matrix multiplication across various platforms, specifically targeting the ARMv8 Scalable Vector Extension (SVE) and edge RISC-V SIMD-based architectures. In addition to increasing throughput, CAMP's architectural design also contributes to energy efficiency, making it an effective solution for low-power applications. Evaluations on a range of Large Language Models (LLMs) and Convolutional Neural Networks (CNNs) demonstrate that matrix multiplication operations using the proposed micro-architecture achieve up to 17$\\times$ and 23$\\times$ performance improvements compared to their respective baselines, the ARM A64FX core and a RISC-V-based edge System-on-Chip (SoC). Furthermore, synthesis and place-and-route (PnR) of the CAMP micro-architecture using Synopsys tools -- targeting ARM TSMC 7nm for A64FX and GlobalFoundries 22nm for the RISC-V SoC -- add only 1\\% and 4\\% area overhead, respectively, compared to the baseline designs.\n        △ Less",
        "authors":[
            "Mohammadreza Esmali Nojehdeh",
            "Hossein Mokhtarnia",
            "Julian Pavon Rivera",
            "Narcis Rodas Quiroga",
            "Roger Figueras Bagué",
            "Enrico Reggiani",
            "Miquel Moreto",
            "Osman Unsal",
            "Adrian Cristal",
            "Eduard Ayguade"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A physics informed neural network approach to simulating ice dynamics governed by the shallow ice approximation",
        "abstract":"In this article we develop a Physics Informed Neural Network (PINN) approach to simulate ice sheet dynamics governed by the Shallow Ice Approximation. This problem takes the form of a time-dependent parabolic obstacle problem. Prior work has used this approach to address the stationary obstacle problem and here we extend it to the time dependent problem. Through comprehensive 1D and 2D simulations, we validate the model's effectiveness in capturing complex free-boundary conditions. By merging traditional mathematical modeling with cutting-edge deep learning methods, this approach provides a scalable and robust solution for predicting temporal variations in ice thickness. To illustrate this approach in a real world setting, we simulate the dynamics of the Devon Ice Cap, incorporating aerogeophysical data from 2000 and 2018.\n        △ Less",
        "authors":[
            "Kapil Chawla",
            "William Holmes"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Between Linear and Sinusoidal: Rethinking the Time Encoder in Dynamic Graph Learning",
        "abstract":"Dynamic graph learning is essential for applications involving temporal networks and requires effective modeling of temporal relationships. Seminal attention-based models like TGAT and DyGFormer rely on sinusoidal time encoders to capture temporal relationships between edge events. In this paper, we study a simpler alternative: the linear time encoder, which avoids temporal information loss caused by sinusoidal functions and reduces the need for high dimensional time encoders. We show that the self-attention mechanism can effectively learn to compute time spans from linear time encodings and extract relevant temporal patterns. Through extensive experiments on six dynamic graph datasets, we demonstrate that the linear time encoder improves the performance of TGAT and DyGFormer in most cases. Moreover, the linear time encoder can lead to significant savings in model parameters with minimal performance loss. For example, compared to a 100-dimensional sinusoidal time encoder, TGAT with a 2-dimensional linear time encoder saves 43% of parameters and achieves higher average precision on five datasets. These results can be readily used to positively impact the design choices of a wide variety of dynamic graph learning architectures. The experimental code is available at: https:\/\/github.com\/hsinghuan\/dg-linear-time.git.\n        △ Less",
        "authors":[
            "Hsing-Huan Chung",
            "Shravan Chaudhari",
            "Xing Han",
            "Yoav Wald",
            "Suchi Saria",
            "Joydeep Ghosh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Benchmarking Suite for Synthetic Aperture Radar Imagery Anomaly Detection (SARIAD) Algorithms",
        "abstract":"Anomaly detection is a key research challenge in computer vision and machine learning with applications in many fields from quality control to radar imaging. In radar imaging, specifically synthetic aperture radar (SAR), anomaly detection can be used for the classification, detection, and segmentation of objects of interest. However, there is no method for developing and benchmarking these methods on SAR imagery. To address this issue, we introduce SAR imagery anomaly detection (SARIAD). In conjunction with Anomalib, a deep-learning library for anomaly detection, SARIAD provides a comprehensive suite of algorithms and datasets for assessing and developing anomaly detection approaches on SAR imagery. SARIAD specifically integrates multiple SAR datasets along with tools to effectively apply various anomaly detection algorithms to SAR imagery. Several anomaly detection metrics and visualizations are available. Overall, SARIAD acts as a central package for benchmarking SAR models and datasets to allow for reproducible research in the field of anomaly detection in SAR imagery. This package is publicly available: https:\/\/github.com\/Advanced-Vision-and-Learning-Lab\/SARIAD.\n        △ Less",
        "authors":[
            "Lucian Chauvina",
            "Somil Guptac",
            "Angelina Ibarrac",
            "Joshua Peeples"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RL-based Control of UAS Subject to Significant Disturbance",
        "abstract":"This paper proposes a Reinforcement Learning (RL)-based control framework for position and attitude control of an Unmanned Aerial System (UAS) subjected to significant disturbance that can be associated with an uncertain trigger signal. The proposed method learns the relationship between the trigger signal and disturbance force, enabling the system to anticipate and counteract the impending disturbances before they occur. We train and evaluate three policies: a baseline policy trained without exposure to the disturbance, a reactive policy trained with the disturbance but without the trigger signal, and a predictive policy that incorporates the trigger signal as an observation and is exposed to the disturbance during training. Our simulation results show that the predictive policy outperforms the other policies by minimizing position deviations through a proactive correction maneuver. This work highlights the potential of integrating predictive cues into RL frameworks to improve UAS performance.\n        △ Less",
        "authors":[
            "Kousheek Chakraborty",
            "Thijs Hof",
            "Ayham Alharbat",
            "Abeje Mersha"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Scaling Laws of Graph Neural Networks for Atomistic Materials Modeling",
        "abstract":"Atomistic materials modeling is a critical task with wide-ranging applications, from drug discovery to materials science, where accurate predictions of the target material property can lead to significant advancements in scientific discovery. Graph Neural Networks (GNNs) represent the state-of-the-art approach for modeling atomistic material data thanks to their capacity to capture complex relational structures. While machine learning performance has historically improved with larger models and datasets, GNNs for atomistic materials modeling remain relatively small compared to large language models (LLMs), which leverage billions of parameters and terabyte-scale datasets to achieve remarkable performance in their respective domains. To address this gap, we explore the scaling limits of GNNs for atomistic materials modeling by developing a foundational model with billions of parameters, trained on extensive datasets in terabyte-scale. Our approach incorporates techniques from LLM libraries to efficiently manage large-scale data and models, enabling both effective training and deployment of these large-scale GNN models. This work addresses three fundamental questions in scaling GNNs: the potential for scaling GNN model architectures, the effect of dataset size on model accuracy, and the applicability of LLM-inspired techniques to GNN architectures. Specifically, the outcomes of this study include (1) insights into the scaling laws for GNNs, highlighting the relationship between model size, dataset volume, and accuracy, (2) a foundational GNN model optimized for atomistic materials modeling, and (3) a GNN codebase enhanced with advanced LLM-based training techniques. Our findings lay the groundwork for large-scale GNNs with billions of parameters and terabyte-scale datasets, establishing a scalable pathway for future advancements in atomistic materials modeling.\n        △ Less",
        "authors":[
            "Chaojian Li",
            "Zhifan Ye",
            "Massimiliano Lupo Pasini",
            "Jong Youl Choi",
            "Cheng Wan",
            "Yingyan Celine Lin",
            "Prasanna Balaprakash"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-view autoencoders for Fake News Detection",
        "abstract":"Given the volume and speed at which fake news spreads across social media, automatic fake news detection has become a highly important task. However, this task presents several challenges, including extracting textual features that contain relevant information about fake news. Research about fake news detection shows that no single feature extraction technique consistently outperforms the others across all scenarios. Nevertheless, different feature extraction techniques can provide complementary information about the textual data and enable a more comprehensive representation of the content. This paper proposes using multi-view autoencoders to generate a joint feature representation for fake news detection by integrating several feature extraction techniques commonly used in the literature. Experiments on fake news datasets show a significant improvement in classification performance compared to individual views (feature representations). We also observed that selecting a subset of the views instead of composing a latent space with all the views can be advantageous in terms of accuracy and computational effort. For further details, including source codes, figures, and datasets, please refer to the project's repository: https:\/\/github.com\/ingrydpereira\/multiview-fake-news.\n        △ Less",
        "authors":[
            "Ingryd V. S. T. Pereira",
            "George D. C. Cavalcanti",
            "Rafael M. O. Cruz"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Differentially Private Selection using Smooth Sensitivity",
        "abstract":"Differentially private selection mechanisms offer strong privacy guarantees for queries aiming to identify the top-scoring element r from a finite set R, based on a dataset-dependent utility function. While selection queries are fundamental in data science, few mechanisms effectively ensure their privacy. Furthermore, most approaches rely on global sensitivity to achieve differential privacy (DP), which can introduce excessive noise and impair downstream inferences. To address this limitation, we propose the Smooth Noisy Max (SNM) mechanism, which leverages smooth sensitivity to yield provably tighter (upper bounds on) expected errors compared to global sensitivity-based methods. Empirical results demonstrate that SNM is more accurate than state-of-the-art differentially private selection methods in three applications: percentile selection, greedy decision trees, and random forests.\n        △ Less",
        "authors":[
            "Iago Chaves",
            "Victor Farias",
            "Amanda Perez",
            "Diego Mesquita",
            "Javam Machado"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Programs as Singularities",
        "abstract":"We develop a correspondence between the structure of Turing machines and the structure of singularities of real analytic functions, based on connecting the Ehrhard-Regnier derivative from linear logic with the role of geometry in Watanabe's singular learning theory. The correspondence works by embedding ordinary (discrete) Turing machine codes into a family of noisy codes which form a smooth parameter space. On this parameter space we consider a potential function which has Turing machines as critical points. By relating the Taylor series expansion of this potential at such a critical point to combinatorics of error syndromes, we relate the local geometry to internal structure of the Turing machine.\n  The potential in question is the negative log-likelihood for a statistical model, so that the structure of the Turing machine and its associated singularity is further related to Bayesian inference. Two algorithms that produce the same predictive function can nonetheless correspond to singularities with different geometries, which implies that the Bayesian posterior can discriminate between distinct algorithmic implementations, contrary to a purely functional view of inference. In the context of singular learning theory our results point to a more nuanced understanding of Occam's razor and the meaning of simplicity in inductive inference.\n        △ Less",
        "authors":[
            "Daniel Murfet",
            "Will Troiani"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Reinforcement Learning for Day-to-day Dynamic Tolling in Tradable Credit Schemes",
        "abstract":"Tradable credit schemes (TCS) are an increasingly studied alternative to congestion pricing, given their revenue neutrality and ability to address issues of equity through the initial credit allocation. Modeling TCS to aid future design and implementation is associated with challenges involving user and market behaviors, demand-supply dynamics, and control mechanisms. In this paper, we focus on the latter and address the day-to-day dynamic tolling problem under TCS, which is formulated as a discrete-time Markov Decision Process and solved using reinforcement learning (RL) algorithms. Our results indicate that RL algorithms achieve travel times and social welfare comparable to the Bayesian optimization benchmark, with generalization across varying capacities and demand levels. We further assess the robustness of RL under different hyperparameters and apply regularization techniques to mitigate action oscillation, which generates practical tolling strategies that are transferable under day-to-day demand and supply variability. Finally, we discuss potential challenges such as scaling to large networks, and show how transfer learning can be leveraged to improve computational efficiency and facilitate the practical deployment of RL-based TCS solutions.\n        △ Less",
        "authors":[
            "Xiaoyi Wu",
            "Ravi Seshadri",
            "Filipe Rodrigues",
            "Carlos Lima Azevedo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
        "abstract":"AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https:\/\/github.com\/SakanaAI\/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.\n        △ Less",
        "authors":[
            "Yutaro Yamada",
            "Robert Tjarko Lange",
            "Cong Lu",
            "Shengran Hu",
            "Chris Lu",
            "Jakob Foerster",
            "Jeff Clune",
            "David Ha"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Vector Quantized-Elites: Unsupervised and Problem-Agnostic Quality-Diversity Optimization",
        "abstract":"Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowledge of the task to define the behavioral space grid, limiting their flexibility and applicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites), a novel Quality-Diversity algorithm that autonomously constructs a structured behavioral space grid using unsupervised learning, eliminating the need for prior task-specific knowledge. At the core of VQ-Elites is the integration of Vector Quantized Variational Autoencoders, which enables the dynamic learning of behavioral descriptors and the generation of a structured, rather than unstructured, behavioral space grid - a significant advancement over existing unsupervised Quality-Diversity approaches. This design establishes VQ-Elites as a flexible, robust, and task-agnostic optimization framework. To further enhance the performance of unsupervised Quality-Diversity algorithms, we introduce two key components: behavioral space bounding and cooperation mechanisms, which significantly improve convergence and performance. We validate VQ-Elites on robotic arm pose-reaching and mobile robot space-covering tasks. The results demonstrate its ability to efficiently generate diverse, high-quality solutions, emphasizing its adaptability, scalability, robustness to hyperparameters, and potential to extend Quality-Diversity optimization to complex, previously inaccessible domains.\n        △ Less",
        "authors":[
            "Constantinos Tsakonas",
            "Konstantinos Chatzilygeroudis"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Task Learning with Multi-Annotation Triplet Loss for Improved Object Detection",
        "abstract":"Triplet loss traditionally relies only on class labels and does not use all available information in multi-task scenarios where multiple types of annotations are available. This paper introduces a Multi-Annotation Triplet Loss (MATL) framework that extends triplet loss by incorporating additional annotations, such as bounding box information, alongside class labels in the loss formulation. By using these complementary annotations, MATL improves multi-task learning for tasks requiring both classification and localization. Experiments on an aerial wildlife imagery dataset demonstrate that MATL outperforms conventional triplet loss in both classification and localization. These findings highlight the benefit of using all available annotations for triplet loss in multi-task learning frameworks.\n        △ Less",
        "authors":[
            "Meilun Zhou",
            "Aditya Dutt",
            "Alina Zare"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Compositional Flows for 3D Molecule and Synthesis Pathway Co-design",
        "abstract":"Many generative applications, such as synthesis-based 3D molecular design, involve constructing compositional objects with continuous features. Here, we introduce Compositional Generative Flows (CGFlow), a novel framework that extends flow matching to generate objects in compositional steps while modeling continuous states. Our key insight is that modeling compositional state transitions can be formulated as a straightforward extension of the flow matching interpolation process. We further build upon the theoretical foundations of generative flow networks (GFlowNets), enabling reward-guided sampling of compositional structures. We apply CGFlow to synthesizable drug design by jointly designing the molecule's synthetic pathway with its 3D binding pose. Our approach achieves state-of-the-art binding affinity on all 15 targets from the LIT-PCBA benchmark, and 5.8$\\times$ improvement in sampling efficiency compared to 2D synthesis-based baseline. To our best knowledge, our method is also the first to achieve state of-art-performance in both Vina Dock (-9.38) and AiZynth success rate (62.2\\%) on the CrossDocked benchmark.\n        △ Less",
        "authors":[
            "Tony Shen",
            "Seonghwan Seo",
            "Ross Irwin",
            "Kieran Didi",
            "Simon Olsson",
            "Woo Youn Kim",
            "Martin Ester"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Self-Bootstrapping for Versatile Test-Time Adaptation",
        "abstract":"In this paper, we seek to develop a versatile test-time adaptation (TTA) objective for a variety of tasks - classification and regression across image-, object-, and pixel-level predictions. We achieve this through a self-bootstrapping scheme that optimizes prediction consistency between the test image (as target) and its deteriorated view. The key challenge lies in devising effective augmentations\/deteriorations that: i) preserve the image's geometric information, e.g., object sizes and locations, which is crucial for TTA on object\/pixel-level tasks, and ii) provide sufficient learning signals for TTA. To this end, we analyze how common distribution shifts affect the image's information power across spatial frequencies in the Fourier domain, and reveal that low-frequency components carry high power and masking these components supplies more learning signals, while masking high-frequency components can not. In light of this, we randomly mask the low-frequency amplitude of an image in its Fourier domain for augmentation. Meanwhile, we also augment the image with noise injection to compensate for missing learning signals at high frequencies, by enhancing the information power there. Experiments show that, either independently or as a plug-and-play module, our method achieves superior results across classification, segmentation, and 3D monocular detection tasks with both transformer and CNN models.\n        △ Less",
        "authors":[
            "Shuaicheng Niu",
            "Guohao Chen",
            "Peilin Zhao",
            "Tianyi Wang",
            "Pengcheng Wu",
            "Zhiqi Shen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning",
        "abstract":"In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus on balancing these two aspects at the network level, lacking sufficient differentiation and fine-grained control of individual neurons. To overcome this limitation, we propose Neuron-level Balance between Stability and Plasticity (NBSP) method, by taking inspiration from the observation that specific neurons are strongly relevant to task-relevant skills. Specifically, NBSP first (1) defines and identifies RL skill neurons that are crucial for knowledge retention through a goal-oriented method, and then (2) introduces a framework by employing gradient masking and experience replay techniques targeting these neurons to preserve the encoded existing skills while enabling adaptation to new tasks. Numerous experimental results on the Meta-World and Atari benchmarks demonstrate that NBSP significantly outperforms existing approaches in balancing stability and plasticity.\n        △ Less",
        "authors":[
            "Jiahua Lan",
            "Sen Zhang",
            "Haixia Pan",
            "Ruijun Liu",
            "Li Shen",
            "Dacheng Tao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fusing Global and Local: Transformer-CNN Synergy for Next-Gen Current Estimation",
        "abstract":"This paper presents a hybrid model combining Transformer and CNN for predicting the current waveform in signal lines. Unlike traditional approaches such as current source models, driver linear representations, waveform functional fitting, or equivalent load capacitance methods, our model does not rely on fixed simplified models of standard-cell drivers or RC loads. Instead, it replaces the complex Newton iteration process used in traditional SPICE simulations, leveraging the powerful sequence modeling capabilities of the Transformer framework to directly predict current responses without iterative solving steps. The hybrid architecture effectively integrates the global feature-capturing ability of Transformers with the local feature extraction advantages of CNNs, significantly improving the accuracy of current waveform predictions.\n  Experimental results demonstrate that, compared to traditional SPICE simulations, the proposed algorithm achieves an error of only 0.0098. These results highlight the algorithm's superior capabilities in predicting signal line current waveforms, timing analysis, and power evaluation, making it suitable for a wide range of technology nodes, from 40nm to 3nm.\n        △ Less",
        "authors":[
            "Junlang Huang",
            "Hao Chen",
            "Li Luo",
            "Yong Cai",
            "Lexin Zhang",
            "Tianhao Ma",
            "Yitian Zhang",
            "Zhong Guan"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Towards Simple Machine Learning Baselines for GNSS RFI Detection",
        "abstract":"Machine learning research in GNSS radio frequency interference (RFI) detection often lacks a clear empirical justification for the choice of deep learning architectures over simpler machine learning approaches. In this work, we argue for a change in research direction-from developing ever more complex deep learning models to carefully assessing their real-world effectiveness in comparison to interpretable and lightweight machine learning baselines. Our findings reveal that state-of-the-art deep learning models frequently fail to outperform simple, well-engineered machine learning methods in the context of GNSS RFI detection. Leveraging a unique large-scale dataset collected by the Swiss Air Force and Swiss Air-Rescue (Rega), and preprocessed by Swiss Air Navigation Services Ltd. (Skyguide), we demonstrate that a simple baseline model achieves 91\\% accuracy in detecting GNSS RFI, outperforming more complex deep learning counterparts. These results highlight the effectiveness of pragmatic solutions and offer valuable insights to guide future research in this critical application domain.\n        △ Less",
        "authors":[
            "Viktor Ivanov",
            "Richard C. Wilson",
            "Maurizio Scaramuzza"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Comparative analysis of Realistic EMF Exposure Estimation from Low Density Sensor Network by Finite & Infinite Neural Networks",
        "abstract":"Understanding the spatial and temporal patterns of environmental exposure to radio-frequency electromagnetic fields (RF-EMF) is essential for conducting risk assessments. These assessments aim to explore potential connections between RF-EMF exposure and its effects on human health, as well as on wildlife and plant life. Existing research has used different machine learning tools for EMF exposure estimation; however, a comparative analysis of these techniques is required to better understand their performance for real-world datasets. In this work, we present both finite and infinite-width convolutional network-based methods to estimate and assess EMF exposure levels from 70 real-world sensors in Lille, France. A comparative analysis has been conducted to analyze the performance of the methods' execution time and estimation accuracy. To improve estimation accuracy for higher-resolution grids, we utilized a preconditioned gradient descent method for kernel estimation. Root Mean Square Error (RMSE) is used as the evaluation criterion for comparing the performance of these deep learning models.\n        △ Less",
        "authors":[
            "Mohammed Mallik",
            "Laurent Clavier",
            "Davy P. Gaillot"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"mixEEG: Enhancing EEG Federated Learning for Cross-subject EEG Classification with Tailored mixup",
        "abstract":"The cross-subject electroencephalography (EEG) classification exhibits great challenges due to the diversity of cognitive processes and physiological structures between different subjects. Modern EEG models are based on neural networks, demanding a large amount of data to achieve high performance and generalizability. However, privacy concerns associated with EEG pose significant limitations to data sharing between different hospitals and institutions, resulting in the lack of large dataset for most EEG tasks. Federated learning (FL) enables multiple decentralized clients to collaboratively train a global model without direct communication of raw data, thus preserving privacy. For the first time, we investigate the cross-subject EEG classification in the FL setting. In this paper, we propose a simple yet effective framework termed mixEEG. Specifically, we tailor the vanilla mixup considering the unique properties of the EEG modality. mixEEG shares the unlabeled averaged data of the unseen subject rather than simply sharing raw data under the domain adaptation setting, thus better preserving privacy and offering an averaged label as pseudo-label. Extensive experiments are conducted on an epilepsy detection and an emotion recognition dataset. The experimental result demonstrates that our mixEEG enhances the transferability of global model for cross-subject EEG classification consistently across different datasets and model architectures. Code is published at: https:\/\/github.com\/XuanhaoLiu\/mixEEG.\n        △ Less",
        "authors":[
            "Xuan-Hao Liu",
            "Bao-Liang Lu",
            "Wei-Long Zheng"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"EquiNO: A Physics-Informed Neural Operator for Multiscale Simulations",
        "abstract":"Multiscale problems are ubiquitous in physics. Numerical simulations of such problems by solving partial differential equations (PDEs) at high resolution are computationally too expensive for many-query scenarios, e.g., uncertainty quantification, remeshing applications, topology optimization, and so forth. This limitation has motivated the application of data-driven surrogate models, where the microscale computations are $\\textit{substituted}$ with a surrogate, usually acting as a black-box mapping between macroscale quantities. These models offer significant speedups but struggle with incorporating microscale physical constraints, such as the balance of linear momentum and constitutive models. In this contribution, we propose Equilibrium Neural Operator (EquiNO) as a $\\textit{complementary}$ physics-informed PDE surrogate for predicting microscale physics and compare it with variational physics-informed neural and operator networks. Our framework, applicable to the so-called multiscale FE$^{\\,2}\\,$ computations, introduces the FE-OL approach by integrating the finite element (FE) method with operator learning (OL). We apply the proposed FE-OL approach to quasi-static problems of solid mechanics. The results demonstrate that FE-OL can yield accurate solutions even when confronted with a restricted dataset during model development. Our results show that EquiNO achieves speedup factors exceeding 8000-fold compared to traditional methods and offers an optimal balance between data-driven and physics-based strategies.\n        △ Less",
        "authors":[
            "Hamidreza Eivazi",
            "Jendrik-Alexander Tröger",
            "Stefan Wittek",
            "Stefan Hartmann",
            "Andreas Rausch"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-user Wireless Image Semantic Transmission over MIMO Multiple Access Channels",
        "abstract":"This paper focuses on a typical uplink transmission scenario over multiple-input multiple-output multiple access channel (MIMO-MAC) and thus propose a multi-user learnable CSI fusion semantic communication (MU-LCFSC) framework. It incorporates CSI as the side information into both the semantic encoders and decoders to generate a proper feature mask map in order to produce a more robust attention weight distribution. Especially for the decoding end, a cooperative successive interference cancellation procedure is conducted along with a cooperative mask ratio generator, which flexibly controls the mask elements of feature mask maps. Numerical results verify the superiority of proposed MU-LCFSC compared to DeepJSCC-NOMA over 3 dB in terms of PSNR.\n        △ Less",
        "authors":[
            "Bingyan Xie",
            "Yongpeng Wu",
            "Feng Shu",
            "Jiangzhou Wang",
            "Wenjun Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Double Directional Wireless Channel Generation: A Statistics-Informed Generative Approach",
        "abstract":"Channel models that represent various operating conditions a communication system might experience are important for design and standardization of any communication system. While statistical channel models have long dominated this space, machine learning (ML) is becoming a popular alternative approach. However, existing approaches have mostly focused on predictive solutions to match instantaneous channel realizations. Other solutions have focused on pathloss modeling, while double-directional (DD) channel representation is needed for a complete description. Motivated by this, we (a) develop a generative solution that uses a hybrid Transformer (hTransformer) model with a low-rank projected attention calculation mechanism and a bi-directional long short-term memory (BiLSTM) layer to generate complete DD channel information and (b) design a domain-knowledge-informed training method to match the generated and true channel realizations' statistics. Our extensive simulation results validate that the generated samples' statistics closely align with the true statistics while mostly outperforming the performance of existing predictive approaches.\n        △ Less",
        "authors":[
            "Md-Ferdous Pervej",
            "Patel Pratik",
            "Koushik Manjunatha",
            "Prasad Shamain",
            "Andreas F. Molisch"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Cat, Rat, Meow: On the Alignment of Language Model and Human Term-Similarity Judgments",
        "abstract":"Small and mid-sized generative language models have gained increasing attention. Their size and availability make them amenable to being analyzed at a behavioral as well as a representational level, allowing investigations of how these levels interact. We evaluate 32 publicly available language models for their representational and behavioral alignment with human similarity judgments on a word triplet task. This provides a novel evaluation setting to probe semantic associations in language beyond common pairwise comparisons. We find that (1) even the representations of small language models can achieve human-level alignment, (2) instruction-tuned model variants can exhibit substantially increased agreement, (3) the pattern of alignment across layers is highly model dependent, and (4) alignment based on models' behavioral responses is highly dependent on model size, matching their representational alignment only for the largest evaluated models.\n        △ Less",
        "authors":[
            "Lorenz Linhardt",
            "Tom Neuhäuser",
            "Lenka Tětková",
            "Oliver Eberle"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing",
        "abstract":"Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely sub-optimal expert pathways-our study reveals that naive expert selection learned from pretraining leaves a surprising 10-20% accuracy gap for improvement. Motivated by this observation, we develop a novel class of test-time optimization methods to re-weight or \"re-mixing\" the experts in different layers jointly for each test sample. Since the test sample's ground truth is unknown, we propose to optimize a surrogate objective defined by the sample's \"successful neighbors\" from a reference set of samples. We introduce three surrogates and algorithms based on mode-finding, kernel regression, and the average loss of similar reference samples\/tasks. To reduce the cost of optimizing whole pathways, we apply our algorithms merely to the core experts' mixing weights in critical layers, which enjoy similar performance but save significant computation. This leads to \"Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)\". We apply C3PO to two recent MoE LLMs and examine it on six widely-used benchmarks. It consistently improves the base model by 7-15% in accuracy and outperforms widely used test-time learning baselines, e.g., in-context learning and prompt\/prefix tuning, by a large margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to outperform LLMs of 7-9B parameters, hence improving MoE's advantages on efficiency. Our thorough ablation study further sheds novel insights on achieving test-time improvement on MoE.\n        △ Less",
        "authors":[
            "Zhongyang Li",
            "Ziyue Li",
            "Tianyi Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory",
        "abstract":"Despite their impressive performance on complex tasks, current language models (LMs) typically operate in a vacuum: Each input query is processed separately, without retaining insights from previous attempts. Here, we present Dynamic Cheatsheet (DC), a lightweight framework that endows a black-box LM with a persistent, evolving memory. Rather than repeatedly re-discovering or re-committing the same solutions and mistakes, DC enables models to store and reuse accumulated strategies, code snippets, and general problem-solving insights at inference time. This test-time learning enhances performance substantially across a range of tasks without needing explicit ground-truth labels or human feedback. Leveraging DC, Claude 3.5 Sonnet's accuracy more than doubled on AIME math exams once it began retaining algebraic insights across questions. Similarly, GPT-4o's success rate on Game of 24 increased from 10% to 99% after the model discovered and reused a Python-based solution. In tasks prone to arithmetic mistakes, such as balancing equations, DC enabled GPT-4o and Claude to reach near-perfect accuracy by recalling previously validated code, whereas their baselines stagnated around 50%. Beyond arithmetic challenges, DC yields notable accuracy gains on knowledge-demanding tasks. Claude achieved a 9% improvement in GPQA-Diamond and an 8% boost on MMLU-Pro problems. Crucially, DC's memory is self-curated, focusing on concise, transferable snippets rather than entire transcript. Unlike finetuning or static retrieval methods, DC adapts LMs' problem-solving skills on the fly, without modifying their underlying parameters. Overall, our findings present DC as a promising approach for augmenting LMs with persistent memory, bridging the divide between isolated inference events and the cumulative, experience-driven learning characteristic of human cognition.\n        △ Less",
        "authors":[
            "Mirac Suzgun",
            "Mert Yuksekgonul",
            "Federico Bianchi",
            "Dan Jurafsky",
            "James Zou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Trading Graph Neural Network",
        "abstract":"This paper proposes a new algorithm -- Trading Graph Neural Network (TGNN) that can structurally estimate the impact of asset features, dealer features and relationship features on asset prices in trading networks. It combines the strength of the traditional simulated method of moments (SMM) and recent machine learning techniques -- Graph Neural Network (GNN). It outperforms existing reduced-form methods with network centrality measures in prediction accuracy. The method can be used on networks with any structure, allowing for heterogeneity among both traders and assets.\n        △ Less",
        "authors":[
            "Xian Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Semantically Encoding Activity Labels for Context-Aware Human Activity Recognition",
        "abstract":"Prior work has primarily formulated CA-HAR as a multi-label classification problem, where model inputs are time-series sensor data and target labels are binary encodings representing whether a given activity or context occurs. These CA-HAR methods either predicted each label independently or manually imposed relationships using graphs. However, both strategies often neglect an essential aspect: activity labels have rich semantic relationships. For instance, walking, jogging, and running activities share similar movement patterns but differ in pace and intensity, indicating that they are semantically related. Consequently, prior CA-HAR methods often struggled to accurately capture these inherent and nuanced relationships, particularly on datasets with noisy labels typically used for CA-HAR or situations where the ideal sensor type is unavailable (e.g., recognizing speech without audio sensors). To address this limitation, we propose SEAL, which leverage LMs to encode CA-HAR activity labels to capture semantic relationships. LMs generate vector embeddings that preserve rich semantic information from natural language. Our SEAL approach encodes input-time series sensor data from smart devices and their associated activity and context labels (text) as vector embeddings. During training, SEAL aligns the sensor data representations with their corresponding activity\/context label embeddings in a shared embedding space. At inference time, SEAL performs a similarity search, returning the CA-HAR label with the embedding representation closest to the input data. Although LMs have been widely explored in other domains, surprisingly, their potential in CA-HAR has been underexplored, making our approach a novel contribution to the field. Our research opens up new possibilities for integrating more advanced LMs into CA-HAR tasks.\n        △ Less",
        "authors":[
            "Wen Ge",
            "Guanyi Mou",
            "Emmanuel O. Agu",
            "Kyumin Lee"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Echo Chamber: RL Post-training Amplifies Behaviors Learned in Pretraining",
        "abstract":"Reinforcement learning (RL)-based fine-tuning has become a crucial step in post-training language models for advanced mathematical reasoning and coding. Following the success of frontier reasoning models, recent work has demonstrated that RL fine-tuning consistently improves performance, even in smaller-scale models; however, the underlying mechanisms driving these improvements are not well-understood. Understanding the effects of RL fine-tuning requires disentangling its interaction with pretraining data composition, hyperparameters, and model scale, but such problems are exacerbated by the lack of transparency regarding the training data used in many existing models. In this work, we present a systematic end-to-end study of RL fine-tuning for mathematical reasoning by training models entirely from scratch on different mixtures of fully open datasets. We investigate the effects of various RL fine-tuning algorithms (PPO, GRPO, and Expert Iteration) across models of different scales. Our study reveals that RL algorithms consistently converge towards a dominant output distribution, amplifying patterns in the pretraining data. We also find that models of different scales trained on the same data mixture will converge to distinct output distributions, suggesting that there are scale-dependent biases in model generalization. Moreover, we find that RL post-training on simpler questions can lead to performance gains on harder ones, indicating that certain reasoning capabilities generalize across tasks. Our findings show that small-scale proxies in controlled settings can elicit interesting insights regarding the role of RL in shaping language model behavior.\n        △ Less",
        "authors":[
            "Rosie Zhao",
            "Alexandru Meterez",
            "Sham Kakade",
            "Cengiz Pehlevan",
            "Samy Jelassi",
            "Eran Malach"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Hodge Laplacians and Hodge Diffusion Maps",
        "abstract":"We introduce Hodge Diffusion Maps, a novel manifold learning algorithm designed to analyze and extract topological information from high-dimensional data-sets. This method approximates the exterior derivative acting on differential forms, thereby providing an approximation of the Hodge Laplacian operator. Hodge Diffusion Maps extend existing non-linear dimensionality reduction techniques, including vector diffusion maps, as well as the theories behind diffusion maps and Laplacian Eigenmaps. Our approach captures higher-order topological features of the data-set by projecting it into lower-dimensional Euclidean spaces using the Hodge Laplacian. We develop a theoretical framework to estimate the approximation error of the exterior derivative, based on sample points distributed over a real manifold. Numerical experiments support and validate the proposed methodology.\n        △ Less",
        "authors":[
            "Alvaro Almeida Gomez",
            "Jorge Duque Franco"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Porting an LLM based Application from ChatGPT to an On-Premise Environment",
        "abstract":"Given the data-intensive nature of Machine Learning (ML) systems in general, and Large Language Models (LLM) in particular, using them in cloud based environments can become a challenge due to legislation related to privacy and security of data. Taking such aspects into consideration implies porting the LLMs to an on-premise environment, where privacy and security can be controlled. In this paper, we study this porting process of a real-life application using ChatGPT, which runs in a public cloud, to an on-premise environment. The application being ported is AIPA, a system that leverages Large Language Models (LLMs) and sophisticated data analytics to enhance the assessment of procurement call bids. The main considerations in the porting process include transparency of open source models and cost of hardware, which are central design choices of the on-premise environment. In addition to presenting the porting process, we evaluate downsides and benefits associated with porting.\n        △ Less",
        "authors":[
            "Teemu Paloniemi",
            "Manu Setälä",
            "Tommi Mikkonen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Efficacy of Semantics-Preserving Transformations in Self-Supervised Learning for Medical Ultrasound",
        "abstract":"Data augmentation is a central component of joint embedding self-supervised learning (SSL). Approaches that work for natural images may not always be effective in medical imaging tasks. This study systematically investigated the impact of data augmentation and preprocessing strategies in SSL for lung ultrasound. Three data augmentation pipelines were assessed: (1) a baseline pipeline commonly used across imaging domains, (2) a novel semantic-preserving pipeline designed for ultrasound, and (3) a distilled set of the most effective transformations from both pipelines. Pretrained models were evaluated on multiple classification tasks: B-line detection, pleural effusion detection, and COVID-19 classification. Experiments revealed that semantics-preserving data augmentation resulted in the greatest performance for COVID-19 classification - a diagnostic task requiring global image context. Cropping-based methods yielded the greatest performance on the B-line and pleural effusion object classification tasks, which require strong local pattern recognition. Lastly, semantics-preserving ultrasound image preprocessing resulted in increased downstream performance for multiple tasks. Guidance regarding data augmentation and preprocessing strategies was synthesized for practitioners working with SSL in ultrasound.\n        △ Less",
        "authors":[
            "Blake VanBerlo",
            "Alexander Wong",
            "Jesse Hoey",
            "Robert Arntfield"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"How do Large Language Models Understand Relevance? A Mechanistic Interpretability Perspective",
        "abstract":"Recent studies have shown that large language models (LLMs) can assess relevance and support information retrieval (IR) tasks such as document ranking and relevance judgment generation. However, the internal mechanisms by which off-the-shelf LLMs understand and operationalize relevance remain largely unexplored. In this paper, we systematically investigate how different LLM modules contribute to relevance judgment through the lens of mechanistic interpretability. Using activation patching techniques, we analyze the roles of various model components and identify a multi-stage, progressive process in generating either pointwise or pairwise relevance judgment. Specifically, LLMs first extract query and document information in the early layers, then process relevance information according to instructions in the middle layers, and finally utilize specific attention heads in the later layers to generate relevance judgments in the required format. Our findings provide insights into the mechanisms underlying relevance assessment in LLMs, offering valuable implications for future research on leveraging LLMs for IR tasks.\n        △ Less",
        "authors":[
            "Qi Liu",
            "Jiaxin Mao",
            "Ji-Rong Wen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Fast Adaptation with Behavioral Foundation Models",
        "abstract":"Unsupervised zero-shot reinforcement learning (RL) has emerged as a powerful paradigm for pretraining behavioral foundation models (BFMs), enabling agents to solve a wide range of downstream tasks specified via reward functions in a zero-shot fashion, i.e., without additional test-time learning or planning. This is achieved by learning self-supervised task embeddings alongside corresponding near-optimal behaviors and incorporating an inference procedure to directly retrieve the latent task embedding and associated policy for any given reward function. Despite promising results, zero-shot policies are often suboptimal due to errors induced by the unsupervised training process, the embedding, and the inference procedure. In this paper, we focus on devising fast adaptation strategies to improve the zero-shot performance of BFMs in a few steps of online interaction with the environment while avoiding any performance drop during the adaptation process. Notably, we demonstrate that existing BFMs learn a set of skills containing more performant policies than those identified by their inference procedure, making them well-suited for fast adaptation. Motivated by this observation, we propose both actor-critic and actor-only fast adaptation strategies that search in the low-dimensional task-embedding space of the pre-trained BFM to rapidly improve the performance of its zero-shot policies on any downstream task. Notably, our approach mitigates the initial \"unlearning\" phase commonly observed when fine-tuning pre-trained RL models. We evaluate our fast adaptation strategies on top of four state-of-the-art zero-shot RL methods in multiple navigation and locomotion domains. Our results show that they achieve 10-40% improvement over their zero-shot performance in a few tens of episodes, outperforming existing baselines.\n        △ Less",
        "authors":[
            "Harshit Sikchi",
            "Andrea Tirinzoni",
            "Ahmed Touati",
            "Yingchen Xu",
            "Anssi Kanervisto",
            "Scott Niekum",
            "Amy Zhang",
            "Alessandro Lazaric",
            "Matteo Pirotta"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows",
        "abstract":"Many real-world applications of flow-based generative models desire a diverse set of samples that cover multiple modes of the target distribution. However, the predominant approach for obtaining diverse sets is not sample-efficient, as it involves independently obtaining many samples from the source distribution and mapping them through the flow until the desired mode coverage is achieved. As an alternative to repeated sampling, we introduce DiverseFlow: a training-free approach to improve the diversity of flow models. Our key idea is to employ a determinantal point process to induce a coupling between the samples that drives diversity under a fixed sampling budget. In essence, DiverseFlow allows exploration of more variations in a learned flow model with fewer samples. We demonstrate the efficacy of our method for tasks where sample-efficient diversity is desirable, such as text-guided image generation with polysemous words, inverse problems like large-hole inpainting, and class-conditional image synthesis.\n        △ Less",
        "authors":[
            "Mashrur M. Morshed",
            "Vishnu Boddeti"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning",
        "abstract":"Recent advances in inference-time compute have significantly improved performance on complex tasks by generating long chains of thought (CoTs) using Large Reasoning Models (LRMs). However, this improved accuracy comes at the cost of high inference latency due to the length of generated reasoning sequences and the autoregressive nature of decoding. Our key insight in tackling these overheads is that LRM inference, and the reasoning that it embeds, is highly tolerant of approximations: complex tasks are typically broken down into simpler steps, each of which brings utility based on the semantic insight it provides for downstream steps rather than the exact tokens it generates. Accordingly, we introduce SpecReason, a system that automatically accelerates LRM inference by using a lightweight model to (speculatively) carry out simpler intermediate reasoning steps and reserving the costly base model only to assess (and potentially correct) the speculated outputs. Importantly, SpecReason's focus on exploiting the semantic flexibility of thinking tokens in preserving final-answer accuracy is complementary to prior speculation techniques, most notably speculative decoding, which demands token-level equivalence at each step. Across a variety of reasoning benchmarks, SpecReason achieves 1.5-2.5$\\times$ speedup over vanilla LRM inference while improving accuracy by 1.0-9.9\\%. Compared to speculative decoding without SpecReason, their combination yields an additional 19.4-44.2\\% latency reduction. We open-source SpecReason at https:\/\/github.com\/ruipeterpan\/specreason.\n        △ Less",
        "authors":[
            "Rui Pan",
            "Yinwei Dai",
            "Zhihao Zhang",
            "Gabriele Oliaro",
            "Zhihao Jia",
            "Ravi Netravali"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Robust Hallucination Detection in LLMs via Adaptive Token Selection",
        "abstract":"Hallucinations in large language models (LLMs) pose significant safety concerns that impede their broader deployment. Recent research in hallucination detection has demonstrated that LLMs' internal representations contain truthfulness hints, which can be harnessed for detector training. However, the performance of these detectors is heavily dependent on the internal representations of predetermined tokens, fluctuating considerably when working on free-form generations with varying lengths and sparse distributions of hallucinated entities. To address this, we propose HaMI, a novel approach that enables robust detection of hallucinations through adaptive selection and learning of critical tokens that are most indicative of hallucinations. We achieve this robustness by an innovative formulation of the Hallucination detection task as Multiple Instance (HaMI) learning over token-level representations within a sequence, thereby facilitating a joint optimisation of token selection and hallucination detection on generation sequences of diverse forms. Comprehensive experimental results on four hallucination benchmarks show that HaMI significantly outperforms existing state-of-the-art approaches.\n        △ Less",
        "authors":[
            "Mengjia Niu",
            "Hamed Haddadi",
            "Guansong Pang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Review of HPC-Accelerated CFD in National Security and Defense",
        "abstract":"Using High-Performance Computing (HPC), Computational Fluid Dynamics (CFD) now serves as an essential component in defense-related national security applications including missile interception and hypersonic propulsion as well as naval stealth optimization and urban hazard dispersion. This review combines two decades of open-source and public-domain research on HPC-accelerated CFD in defense, addressing three key questions: Which security-sensitive simulations have utilized open-source CFD frameworks such as OpenFOAM, SU2 and ADflow? Which HPC techniques, such as MPI domain decomposition and GPU acceleration together with hybrid parallelism best enhance open-source frameworks to manage large defense CFD simulations? Which technological advancements and research voids currently drive the directional development of the field? Examining several research studies sourced from NASA, DoD HPC centers, and academic institutions, scientific contributions have been classified into air, maritime, and space domains. Modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers show how custom open-source solutions support workflows with rapid completion of multi-million cell simulations. The conclusion highlights new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to deliver practical insights for improving CFD use in defense research and development.\n        △ Less",
        "authors":[
            "James Afful"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks",
        "abstract":"Motivated by the growing demand for low-precision arithmetic in computational science, we exploit lower-precision emulation in Python -- widely regarded as the dominant programming language for numerical analysis and machine learning. Low-precision training has revolutionized deep learning by enabling more efficient computation and reduced memory and energy consumption while maintaining model fidelity. To better enable numerical experimentation with and exploration of low precision computation, we developed the Pychop library, which supports customizable floating-point formats and a comprehensive set of rounding modes in Python, allowing users to benefit from fast, low-precision emulation in numerous applications. Pychop also introduces interfaces for both PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural network training and inference with unparalleled flexibility.\n  In this paper, we offer a comprehensive exposition of the design, implementation, validation, and practical application of Pychop, establishing it as a foundational tool for advancing efficient mixed-precision algorithms. Furthermore, we present empirical results on low-precision emulation for image classification and object detection using published datasets, illustrating the sensitivity of the use of low precision and offering valuable insights into its impact. Pychop enables in-depth investigations into the effects of numerical precision, facilitates the development of novel hardware accelerators, and integrates seamlessly into existing deep learning workflows. Software and experimental code are publicly available at https:\/\/github.com\/inEXASCALE\/pychop.\n        △ Less",
        "authors":[
            "Erin Carson",
            "Xinye Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"DG-STMTL: A Novel Graph Convolutional Network for Multi-Task Spatio-Temporal Traffic Forecasting",
        "abstract":"Spatio-temporal traffic prediction is crucial in intelligent transportation systems. The key challenge of accurate prediction is how to model the complex spatio-temporal dependencies and adapt to the inherent dynamics in data. Traditional Graph Convolutional Networks (GCNs) often struggle with static adjacency matrices that introduce domain bias or learnable matrices that may be overfitting to specific patterns. This challenge becomes more complex when considering Multi-Task Learning (MTL). While MTL has the potential to enhance prediction accuracy through task synergies, it can also face significant hurdles due to task interference. To overcome these challenges, this study introduces a novel MTL framework, Dynamic Group-wise Spatio-Temporal Multi-Task Learning (DG-STMTL). DG-STMTL proposes a hybrid adjacency matrix generation module that combines static matrices with dynamic ones through a task-specific gating mechanism. We also introduce a group-wise GCN module to enhance the modelling capability of spatio-temporal dependencies. We conduct extensive experiments on two real-world datasets to evaluate our method. Results show that our method outperforms other state-of-the-arts, indicating its effectiveness and robustness.\n        △ Less",
        "authors":[
            "Wanna Cui",
            "Peizheng Wang",
            "Faliang Yin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Smoothed Distance Kernels for MMDs and Applications in Wasserstein Gradient Flows",
        "abstract":"Negative distance kernels $K(x,y) := - \\|x-y\\|$ were used in the definition of maximum mean discrepancies (MMDs) in statistics and lead to favorable numerical results in various applications. In particular, so-called slicing techniques for handling high-dimensional kernel summations profit from the simple parameter-free structure of the distance kernel. However, due to its non-smoothness in $x=y$, most of the classical theoretical results, e.g. on Wasserstein gradient flows of the corresponding MMD functional do not longer hold true. In this paper, we propose a new kernel which keeps the favorable properties of the negative distance kernel as being conditionally positive definite of order one with a nearly linear increase towards infinity and a simple slicing structure, but is Lipschitz differentiable now. Our construction is based on a simple 1D smoothing procedure of the absolute value function followed by a Riemann-Liouville fractional integral transform. Numerical results demonstrate that the new kernel performs similarly well as the negative distance kernel in gradient descent methods, but now with theoretical guarantees.\n        △ Less",
        "authors":[
            "Nicolaj Rux",
            "Michael Quellmalz",
            "Gabriele Steidl"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Performance of Rank-One Tensor Approximation on Incomplete Data",
        "abstract":"We are interested in the estimation of a rank-one tensor signal when only a portion $\\varepsilon$ of its noisy observation is available. We show that the study of this problem can be reduced to that of a random matrix model whose spectral analysis gives access to the reconstruction performance. These results shed light on and specify the loss of performance induced by an artificial reduction of the memory cost of a tensor via the deletion of a random part of its entries.\n        △ Less",
        "authors":[
            "Hugo Lebeau"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A System for Comprehensive Assessment of RAG Frameworks",
        "abstract":"Retrieval Augmented Generation (RAG) has emerged as a standard paradigm for enhancing the factual accuracy and contextual relevance of Large Language Models (LLMs) by integrating retrieval mechanisms. However, existing evaluation frameworks fail to provide a holistic black-box approach to assessing RAG systems, especially in real-world deployment scenarios. To address this gap, we introduce SCARF (System for Comprehensive Assessment of RAG Frameworks), a modular and flexible evaluation framework designed to benchmark deployed RAG applications systematically. SCARF provides an end-to-end, black-box evaluation methodology, enabling a limited-effort comparison across diverse RAG frameworks. Our framework supports multiple deployment configurations and facilitates automated testing across vector databases and LLM serving strategies, producing a detailed performance report. Moreover, SCARF integrates practical considerations such as response coherence, providing a scalable and adaptable solution for researchers and industry professionals evaluating RAG applications. Using the REST APIs interface, we demonstrate how SCARF can be applied to real-world scenarios, showcasing its flexibility in assessing different RAG frameworks and configurations. SCARF is available at GitHub repository.\n        △ Less",
        "authors":[
            "Mattia Rengo",
            "Senad Beadini",
            "Domenico Alfano",
            "Roberto Abbruzzese"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Revisiting Likelihood-Based Out-of-Distribution Detection by Modeling Representations",
        "abstract":"Out-of-distribution (OOD) detection is critical for ensuring the reliability of deep learning systems, particularly in safety-critical applications. Likelihood-based deep generative models have historically faced criticism for their unsatisfactory performance in OOD detection, often assigning higher likelihood to OOD data than in-distribution samples when applied to image data. In this work, we demonstrate that likelihood is not inherently flawed. Rather, several properties in the images space prohibit likelihood as a valid detection score. Given a sufficiently good likelihood estimator, specifically using the probability flow formulation of a diffusion model, we show that likelihood-based methods can still perform on par with state-of-the-art methods when applied in the representation space of pre-trained encoders. The code of our work can be found at $\\href{https:\/\/github.com\/limchaos\/Likelihood-OOD.git}{\\texttt{https:\/\/github.com\/limchaos\/Likelihood-OOD.git}}$.\n        △ Less",
        "authors":[
            "Yifan Ding",
            "Arturas Aleksandrauskas",
            "Amirhossein Ahmadian",
            "Jonas Unger",
            "Fredrik Lindsten",
            "Gabriel Eilertsen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adaptive Detection of Fast Moving Celestial Objects Using a Mixture of Experts and Physical-Inspired Neural Network",
        "abstract":"Fast moving celestial objects are characterized by velocities across the celestial sphere that significantly differ from the motions of background stars. In observational images, these objects exhibit distinct shapes, contrasting with the typical appearances of stars. Depending on the observational method employed, these celestial entities may be designated as near-Earth objects or asteroids. Historically, fast moving celestial objects have been observed using ground-based telescopes, where the relative stability of stars and Earth facilitated effective image differencing techniques alongside traditional fast moving celestial object detection and classification algorithms. However, the growing prevalence of space-based telescopes, along with their diverse observational modes, produces images with different properties, rendering conventional methods less effective. This paper presents a novel algorithm for detecting fast moving celestial objects within star fields. Our approach enhances state-of-the-art fast moving celestial object detection neural networks by transforming them into physical-inspired neural networks. These neural networks leverage the point spread function of the telescope and the specific observational mode as prior information; they can directly identify moving fast moving celestial objects within star fields without requiring additional training, thereby addressing the limitations of traditional techniques. Additionally, all neural networks are integrated using the mixture of experts technique, forming a comprehensive fast moving celestial object detection algorithm. We have evaluated our algorithm using simulated observational data that mimics various observations carried out by space based telescope scenarios and real observation images. Results demonstrate that our method effectively detects fast moving celestial objects across different observational modes.\n        △ Less",
        "authors":[
            "Peng Jia",
            "Ge Li",
            "Bafeng Cheng",
            "Yushan Li",
            "Rongyu Sun"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data over dialogue: Why artificial intelligence is unlikely to humanise medicine",
        "abstract":"Recently, a growing number of experts in artificial intelligence (AI) and medicine have be-gun to suggest that the use of AI systems, particularly machine learning (ML) systems, is likely to humanise the practice of medicine by substantially improving the quality of clinician-patient relationships. In this thesis, however, I argue that medical ML systems are more likely to negatively impact these relationships than to improve them. In particular, I argue that the use of medical ML systems is likely to comprise the quality of trust, care, empathy, understanding, and communication between clinicians and patients.\n        △ Less",
        "authors":[
            "Joshua Hatherley"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Search-contempt: a hybrid MCTS algorithm for training AlphaZero-like engines with better computational efficiency",
        "abstract":"AlphaZero in 2017 was able to master chess and other games without human knowledge by playing millions of games against itself (self-play), with a computation budget running in the tens of millions of dollars. It used a variant of the Monte Carlo Tree Search (MCTS) algorithm, known as PUCT. This paper introduces search-contempt, a novel hybrid variant of the MCTS algorithm that fundamentally alters the distribution of positions generated in self-play, preferring more challenging positions. In addition, search-contempt has been shown to give a big boost in strength for engines in Odds Chess (where one side receives an unfavorable position from the start). More significantly, it opens up the possibility of training a self-play based engine, in a much more computationally efficient manner with the number of training games running into hundreds of thousands, costing tens of thousands of dollars (instead of tens of millions of training games costing millions of dollars required by AlphaZero). This means that it may finally be possible to train such a program from zero on a standard consumer GPU even with a very limited compute, cost, or time budget.\n        △ Less",
        "authors":[
            "Ameya Joshi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Gradient-based Sample Selection for Faster Bayesian Optimization",
        "abstract":"Bayesian optimization (BO) is an effective technique for black-box optimization. However, its applicability is typically limited to moderate-budget problems due to the cubic complexity in computing the Gaussian process (GP) surrogate model. In large-budget scenarios, directly employing the standard GP model faces significant challenges in computational time and resource requirements. In this paper, we propose a novel approach, gradient-based sample selection Bayesian Optimization (GSSBO), to enhance the computational efficiency of BO. The GP model is constructed on a selected set of samples instead of the whole dataset. These samples are selected by leveraging gradient information to maintain diversity and representation. We provide a theoretical analysis of the gradient-based sample selection strategy and obtain explicit sublinear regret bounds for our proposed framework. Extensive experiments on synthetic and real-world tasks demonstrate that our approach significantly reduces the computational cost of GP fitting in BO while maintaining optimization performance comparable to baseline methods.\n        △ Less",
        "authors":[
            "Qiyu Wei",
            "Haowei Wang",
            "Zirui Cao",
            "Songhao Wang",
            "Richard Allmendinger",
            "Mauricio A Álvarez"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Harnessing Equivariance: Modeling Turbulence with Graph Neural Networks",
        "abstract":"This work proposes a novel methodology for turbulence modeling in Large Eddy Simulation (LES) based on Graph Neural Networks (GNNs), which embeds the discrete rotational, reflectional and translational symmetries of the Navier-Stokes equations into the model architecture. In addition, suitable invariant input and output spaces are derived that allow the GNN models to be embedded seamlessly into the LES framework to obtain a symmetry-preserving simulation setup. The suitability of the proposed approach is investigated for two canonical test cases: Homogeneous Isotropic Turbulence (HIT) and turbulent channel flow. For both cases, GNN models are trained successfully in actual simulations using Reinforcement Learning (RL) to ensure that the models are consistent with the underlying LES formulation and discretization. It is demonstrated for the HIT case that the resulting GNN-based LES scheme recovers rotational and reflectional equivariance up to machine precision in actual simulations. At the same time, the stability and accuracy remain on par with non-symmetry-preserving machine learning models that fail to obey these properties. The same modeling strategy translates well to turbulent channel flow, where the GNN model successfully learns the more complex flow physics and is able to recover the turbulent statistics and Reynolds stresses. It is shown that the GNN model learns a zonal modeling strategy with distinct behaviors in the near-wall and outer regions. The proposed approach thus demonstrates the potential of GNNs for turbulence modeling, especially in the context of LES and RL.\n        △ Less",
        "authors":[
            "Marius Kurz",
            "Andrea Beck",
            "Benjamin Sanderse"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Novel Deep Learning Approach for Emulating Computationally Expensive Postfire Debris Flows",
        "abstract":"Traditional physics-based models of geophysical flows, such as debris flows and landslides that pose significant risks to human lives and infrastructure are computationally expensive, limiting their utility for large-scale parameter sweeps, uncertainty quantification, inversions or real-time applications. This study presents an efficient alternative, a deep learning-based surrogate model built using a modified U-Net architecture to predict the dynamics of runoff-generated debris flows across diverse terrain based on data from physics based simulations. The study area is divided into smaller patches for localized predictions using a patch-predict-stitch methodology (complemented by limited global data to accelerate training). The patches are then combined to reconstruct spatially continuous flow maps, ensuring scalability for large domains. To enable fast training using limited expensive simulations, the deep learning model was trained on data from an ensemble of physics based simulations using parameters generated via Latin Hypercube Sampling and validated on unseen parameter sets and terrain, achieving maximum pointwise errors below 10% and robust generalization. Uncertainty quantification using Monte Carlo methods are enabled using the validated surrogate, which can facilitate probabilistic hazard assessments. This study highlights the potential of deep learning surrogates as powerful tools for geophysical flow analysis, enabling computationally efficient and reliable probabilistic hazard map predictions.\n        △ Less",
        "authors":[
            "Palak Patel",
            "Luke McGuire",
            "Abani Patra"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Quantum Machine Learning: Unveiling Trends, Impacts through Bibliometric Analysis",
        "abstract":"Quantum Machine Learning (QML) is the intersection of two revolutionary fields: quantum computing and machine learning. It promises to unlock unparalleled capabilities in data analysis, model building, and problem-solving by harnessing the unique properties of quantum mechanics. This research endeavors to conduct a comprehensive bibliometric analysis of scientific information pertaining to QML covering the period from 2000 to 2023. An extensive dataset comprising 9493 scholarly works is meticulously examined to unveil notable trends, impact factors, and funding patterns within the domain. Additionally, the study employs bibliometric mapping techniques to visually illustrate the network relationships among key countries, institutions, authors, patent citations and significant keywords in QML research. The analysis reveals a consistent growth in publications over the examined period. The findings highlight the United States and China as prominent contributors, exhibiting substantial publication and citation metrics. Notably, the study concludes that QML, as a research subject, is currently in a formative stage, characterized by robust scholarly activity and ongoing development.\n        △ Less",
        "authors":[
            "Riya Bansal",
            "Nikhil Kumar Rajput"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Relaxing the Markov Requirements on Reinforcement Learning Under Weak Partial Ignorability",
        "abstract":"Incomplete data, confounding effects, and violations of the Markov property are interrelated problems which are ubiquitous in Reinforcement Learning applications. We introduce the concept of ``partial ignorabilty\" and leverage it to establish a novel convergence theorem for adaptive Reinforcement Learning. This theoretical result relaxes the Markov assumption on the stochastic process underlying conventional $Q$-learning, deploying a generalized form of the Robbins-Monro stochastic approximation theorem to establish optimality. This result has clear downstream implications for most active subfields of Reinforcement Learning, with clear paths for extension to the field of Causal Inference.\n        △ Less",
        "authors":[
            "MaryLena Bleile"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Counting Hours, Counting Losses: The Toll of Unpredictable Work Schedules on Financial Security",
        "abstract":"Financial instability has become a significant issue in today's society. While research typically focuses on financial aspects, there is a tendency to overlook time-related aspects of unstable work schedules. The inability to rely on consistent work schedules leads to burnout, work-family conflicts, and financial shocks that directly impact workers' income and assets. Unforeseen fluctuations in earnings pose challenges in financial planning, affecting decisions on savings and spending and ultimately undermining individuals' long-term financial stability and well-being.\n  This issue is particularly evident in sectors where workers experience frequently changing schedules without sufficient notice, including those in the food service and retail sectors, part-time and hourly workers, and individuals with lower incomes. These groups are already more financially vulnerable, and the unpredictable nature of their schedules exacerbates their financial fragility.\n  Our objective is to understand how unforeseen fluctuations in earnings exacerbate financial fragility by investigating the extent to which individuals' financial management depends on their ability to anticipate and plan for the future. To address this question, we develop a simulation framework that models how individuals optimize utility amidst financial uncertainty and the imperative to avoid financial ruin. We employ online learning techniques, specifically adapting workers' consumption policies based on evolving information about their work schedules.\n  With this framework, we show both theoretically and empirically how a worker's capacity to anticipate schedule changes enhances their long-term utility. Conversely, the inability to predict future events can worsen workers' instability. Moreover, our framework enables us to explore interventions to mitigate the problem of schedule uncertainty and evaluate their effectiveness.\n        △ Less",
        "authors":[
            "Pegah Nokhiz",
            "Aravinda Kanchana Ruwanpathirana",
            "Aditya Bhaskara",
            "Suresh Venkatasubramanian"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Merging Embedded Topics with Optimal Transport for Online Topic Modeling on Data Streams",
        "abstract":"Topic modeling is a key component in unsupervised learning, employed to identify topics within a corpus of textual data. The rapid growth of social media generates an ever-growing volume of textual data daily, making online topic modeling methods essential for managing these data streams that continuously arrive over time. This paper introduces a novel approach to online topic modeling named StreamETM. This approach builds on the Embedded Topic Model (ETM) to handle data streams by merging models learned on consecutive partial document batches using unbalanced optimal transport. Additionally, an online change point detection algorithm is employed to identify shifts in topics over time, enabling the identification of significant changes in the dynamics of text streams. Numerical experiments on simulated and real-world data show StreamETM outperforming competitors.\n        △ Less",
        "authors":[
            "Federica Granese",
            "Benjamin Navet",
            "Serena Villata",
            "Charles Bouveyron"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"TOCALib: Optimal control library with interpolation for bimanual manipulation and obstacles avoidance",
        "abstract":"The paper presents a new approach for constructing a library of optimal trajectories for two robotic manipulators, Two-Arm Optimal Control and Avoidance Library (TOCALib). The optimisation takes into account kinodynamic and other constraints within the FROST framework. The novelty of the method lies in the consideration of collisions using the DCOL method, which allows obtaining symbolic expressions for assessing the presence of collisions and using them in gradient-based optimization control methods. The proposed approach allowed the implementation of complex bimanual manipulations. In this paper we used Mobile Aloha as an example of TOCALib application. The approach can be extended to other bimanual robots, as well as to gait control of bipedal robots. It can also be used to construct training data for machine learning tasks for manipulation.\n        △ Less",
        "authors":[
            "Yulia Danik",
            "Dmitry Makarov",
            "Aleksandra Arkhipova",
            "Sergei Davidenko",
            "Aleksandr Panov"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conformalized Generative Bayesian Imaging: An Uncertainty Quantification Framework for Computational Imaging",
        "abstract":"Uncertainty quantification plays an important role in achieving trustworthy and reliable learning-based computational imaging. Recent advances in generative modeling and Bayesian neural networks have enabled the development of uncertainty-aware image reconstruction methods. Current generative model-based methods seek to quantify the inherent (aleatoric) uncertainty on the underlying image for given measurements by learning to sample from the posterior distribution of the underlying image. On the other hand, Bayesian neural network-based approaches aim to quantify the model (epistemic) uncertainty on the parameters of a deep neural network-based reconstruction method by approximating the posterior distribution of those parameters. Unfortunately, an ongoing need for an inversion method that can jointly quantify complex aleatoric uncertainty and epistemic uncertainty patterns still persists. In this paper, we present a scalable framework that can quantify both aleatoric and epistemic uncertainties. The proposed framework accepts an existing generative model-based posterior sampling method as an input and introduces an epistemic uncertainty quantification capability through Bayesian neural networks with latent variables and deep ensembling. Furthermore, by leveraging the conformal prediction methodology, the proposed framework can be easily calibrated to ensure rigorous uncertainty quantification. We evaluated the proposed framework on magnetic resonance imaging, computed tomography, and image inpainting problems and showed that the epistemic and aleatoric uncertainty estimates produced by the proposed framework display the characteristic features of true epistemic and aleatoric uncertainties. Furthermore, our results demonstrated that the use of conformal prediction on top of the proposed framework enables marginal coverage guarantees consistent with frequentist principles.\n        △ Less",
        "authors":[
            "Canberk Ekmekci",
            "Mujdat Cetin"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Distilling Knowledge from Heterogeneous Architectures for Semantic Segmentation",
        "abstract":"Current knowledge distillation (KD) methods for semantic segmentation focus on guiding the student to imitate the teacher's knowledge within homogeneous architectures. However, these methods overlook the diverse knowledge contained in architectures with different inductive biases, which is crucial for enabling the student to acquire a more precise and comprehensive understanding of the data during distillation. To this end, we propose for the first time a generic knowledge distillation method for semantic segmentation from a heterogeneous perspective, named HeteroAKD. Due to the substantial disparities between heterogeneous architectures, such as CNN and Transformer, directly transferring cross-architecture knowledge presents significant challenges. To eliminate the influence of architecture-specific information, the intermediate features of both the teacher and student are skillfully projected into an aligned logits space. Furthermore, to utilize diverse knowledge from heterogeneous architectures and deliver customized knowledge required by the student, a teacher-student knowledge mixing mechanism (KMM) and a teacher-student knowledge evaluation mechanism (KEM) are introduced. These mechanisms are performed by assessing the reliability and its discrepancy between heterogeneous teacher-student knowledge. Extensive experiments conducted on three main-stream benchmarks using various teacher-student pairs demonstrate that our HeteroAKD outperforms state-of-the-art KD methods in facilitating distillation between heterogeneous architectures.\n        △ Less",
        "authors":[
            "Yanglin Huang",
            "Kai Hu",
            "Yuan Zhang",
            "Zhineng Chen",
            "Xieping Gao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Clicks, comments, consequences: Are content creators' socio-structural and platform characteristics shaping the exposure to negative sentiment, offensive language, and hate speech on YouTube?",
        "abstract":"Receiving negative sentiment, offensive comments, or even hate speech is a constant part of the working experience of content creators (CCs) on YouTube - a growing occupational group in the platform economy. This study investigates how socio-structural characteristics such as the age, gender, and race of CCs but also platform features including the number of subscribers, community strength, and the channel topic shape differences in the occurrence of these phenomena on that platform. Drawing on a random sample of n=3,695 YouTube channels from German-speaking countries, we conduct a comprehensive analysis combining digital trace data, enhanced with hand-coded variables to include socio-structural characteristics in social media data. Publicly visible negative sentiment, offensive language, and hate speech are detected with machine- and deep-learning methods using N=40,000,000 comments. Contrary to existing studies our findings indicate that female content creators are confronted with less negative communication. Notably, our analysis reveals that while BIPoC, who work as CCs, receive significantly more negative sentiment, they aren't exposed to more offensive comments or hate speech. Additionally, platform characteristics also play a crucial role, as channels publishing content on conspiracy theories or politics are more frequently subject to negative communication.\n        △ Less",
        "authors":[
            "Sarah Weißmann",
            "Aaron Philipp",
            "Roland Verwiebe",
            "Chiara Osorio Krauter",
            "Nina-Sophie Fritsch",
            "Claudia Buder"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Data Requirement Goal Modeling for Machine Learning Systems",
        "abstract":"Machine Learning (ML) has been integrated into various software and systems. Two main components are essential for training an ML model: the training data and the ML algorithm. Given the critical role of data in ML system development, it has become increasingly important to assess the quality of data attributes and ensure that the data meets specific requirements before its utilization. This work proposes an approach to guide non-experts in identifying data requirements for ML systems using goal modeling. In this approach, we first develop the Data Requirement Goal Model (DRGM) by surveying the white literature to identify and categorize the issues and challenges faced by data scientists and requirement engineers working on ML-related projects. An initial DRGM was built to accommodate common tasks that would generalize across projects. Then, based on insights from both white and gray literature, a customization mechanism is built to help adjust the tasks, KPIs, and goals' importance of different elements within the DRGM. The generated model can aid its users in evaluating different datasets using GRL evaluation strategies. We then validate the approach through two illustrative examples based on real-world projects. The results from the illustrative examples demonstrate that the data requirements identified by the proposed approach align with the requirements of real-world projects, demonstrating the practicality and effectiveness of the proposed framework. The proposed dataset selection customization mechanism and the proposed DRGM are helpful in guiding non-experts in identifying the data requirements for machine learning systems tailored to a specific ML problem. This approach also aids in evaluating different dataset alternatives to choose the optimum dataset for the problem. For future work, we recommend implementing tool support to generate the DRGM based on a chatbot interface.\n        △ Less",
        "authors":[
            "Asma Yamani",
            "Nadeen AlAmoudi",
            "Salma Albilali",
            "Malak Baslyman",
            "Jameleddine Hassine"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ms-Mamba: Multi-scale Mamba for Time-Series Forecasting",
        "abstract":"The problem of Time-series Forecasting is generally addressed by recurrent, Transformer-based and the recently proposed Mamba-based architectures. However, existing architectures generally process their input at a single temporal scale, which may be sub-optimal for many tasks where information changes over multiple time scales. In this paper, we introduce a novel architecture called Multi-scale Mamba (ms-Mamba) to address this gap. ms-Mamba incorporates multiple temporal scales by using multiple Mamba blocks with different sampling rates ($Δ$s). Our experiments on many benchmarks demonstrate that ms-Mamba outperforms state-of-the-art approaches, including the recently proposed Transformer-based and Mamba-based models.\n        △ Less",
        "authors":[
            "Yusuf Meric Karadag",
            "Sinan Kalkan",
            "Ipek Gursel Dino"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Prediction of Usage Probabilities of Shopping-Mall Corridors Using Heterogeneous Graph Neural Networks",
        "abstract":"We present a method based on graph neural network (GNN) for prediction of probabilities of usage of shopping-mall corridors. The heterogeneous graph network of shops and corridor paths are obtained from floorplans of the malls by creating vector layers for corridors, shops and entrances. These are subsequently assimilated into nodes and edges of graphs. The prediction of the usage probability is based on the shop features, namely, the area and usage categories they fall into, and on the graph connecting these shops, corridor junctions and entrances by corridor paths. Though the presented method is applicable for training on datasets obtained from a field survey or from pedestrian-detecting sensors, the target data of the supervised deep-learning work flow in this work are obtained from a probability method. We also include a context-specific representation learning of latent features. The usage-probability prediction is made on each edge, which is a connection by a section of corridor path between the adjacent nodes representing the shops or corridor points. To create a feature for each edge, the hidden-layer feature vectors acquired in the message-passing GNN layers at the nodes of each edge are averaged and concatenated with the vector obtained by their multiplication. These edge-features are then passed to multilayer perceptrons (MLP) to make the final prediction of usage probability on each edge. The samples of synthetic learning dataset for each shopping mall are obtained by changing the shops' usage and area categories, and by subsequently feeding the graph into the probability model.\n  When including different shopping malls in a single dataset, we also propose to consider graph-level features to inform the model with specific identifying features of each mall.\n        △ Less",
        "authors":[
            "Malik M Barakathullah",
            "Immanuel Koh"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Large Language Models through Neuro-Symbolic Integration and Ontological Reasoning",
        "abstract":"Large Language Models (LLMs) demonstrate impressive capabilities in natural language processing but suffer from inaccuracies and logical inconsistencies known as hallucinations. This compromises their reliability, especially in domains requiring factual accuracy. We propose a neuro-symbolic approach integrating symbolic ontological reasoning and machine learning methods to enhance the consistency and reliability of LLM outputs. Our workflow utilizes OWL ontologies, a symbolic reasoner (e.g., HermiT) for consistency checking, and a lightweight machine learning model (logistic regression) for mapping natural language statements into logical forms compatible with the ontology. When inconsistencies between LLM outputs and the ontology are detected, the system generates explanatory feedback to guide the LLM towards a corrected, logically coherent response in an iterative refinement loop. We present a working Python prototype demonstrating this pipeline. Experimental results in a defined domain suggest significant improvements in semantic coherence and factual accuracy of LLM outputs, showcasing the potential of combining LLM fluency with the rigor of formal semantics.\n        △ Less",
        "authors":[
            "Ruslan Idelfonso Magana Vsevolodovna",
            "Marco Monti"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Predicting the Lifespan of Industrial Printheads with Survival Analysis",
        "abstract":"Accurately predicting the lifespan of critical device components is essential for maintenance planning and production optimization, making it a topic of significant interest in both academia and industry. In this work, we investigate the use of survival analysis for predicting the lifespan of production printheads developed by Canon Production Printing. Specifically, we focus on the application of five techniques to estimate survival probabilities and failure rates: the Kaplan-Meier estimator, Cox proportional hazard model, Weibull accelerated failure time model, random survival forest, and gradient boosting. The resulting estimates are further refined using isotonic regression and subsequently aggregated to determine the expected number of failures. The predictions are then validated against real-world ground truth data across multiple time windows to assess model reliability. Our quantitative evaluation using three performance metrics demonstrates that survival analysis outperforms industry-standard baseline methods for printhead lifespan prediction.\n        △ Less",
        "authors":[
            "Dan Parii",
            "Evelyne Janssen",
            "Guangzhi Tang",
            "Charalampos Kouzinopoulos",
            "Marcin Pietrasik"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Kernel Logistic Regression Learning for High-Capacity Hopfield Networks",
        "abstract":"Hebbian learning limits Hopfield network storage capacity (pattern-to-neuron ratio around 0.14). We propose Kernel Logistic Regression (KLR) learning. Unlike linear methods, KLR uses kernels to implicitly map patterns to high-dimensional feature space, enhancing separability. By learning dual variables, KLR dramatically improves storage capacity, achieving perfect recall even when pattern numbers exceed neuron numbers (up to ratio 1.5 shown), and enhances noise robustness. KLR demonstrably outperforms Hebbian and linear logistic regression approaches.\n        △ Less",
        "authors":[
            "Akira Tamamori"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Deep Learning Meets Teleconnections: Improving S2S Predictions for European Winter Weather",
        "abstract":"Predictions on subseasonal-to-seasonal (S2S) timescales--ranging from two weeks to two month--are crucial for early warning systems but remain challenging owing to chaos in the climate system. Teleconnections, such as the stratospheric polar vortex (SPV) and Madden-Julian Oscillation (MJO), offer windows of enhanced predictability, however, their complex interactions remain underutilized in operational forecasting. Here, we developed and evaluated deep learning architectures to predict North Atlantic-European (NAE) weather regimes, systematically assessing the role of remote drivers in improving S2S forecast skill of deep learning models. We implemented (1) a Long Short-term Memory (LSTM) network predicting the NAE regimes of the next six weeks based on previous regimes, (2) an Index-LSTM incorporating SPV and MJO indices, and (3) a ViT-LSTM using a Vision Transformer to directly encode stratospheric wind and tropical outgoing longwave radiation fields. These models are compared with operational hindcasts as well as other AI models. Our results show that leveraging teleconnection information enhances skill at longer lead times. Notably, the ViT-LSTM outperforms ECMWF's subseasonal hindcasts beyond week 4 by improving Scandinavian Blocking (SB) and Atlantic Ridge (AR) predictions. Analysis of high-confidence predictions reveals that NAO-, SB, and AR opportunity forecasts can be associated with SPV variability and MJO phase patterns aligning with established pathways, also indicating new patterns. Overall, our work demonstrates that encoding physically meaningful climate fields can enhance S2S prediction skill, advancing AI-driven subseasonal forecast. Moreover, the experiments highlight the potential of deep learning methods as investigative tools, providing new insights into atmospheric dynamics and predictability.\n        △ Less",
        "authors":[
            "Philine L. Bommer",
            "Marlene Kretschmer",
            "Fiona R. Spuler",
            "Kirill Bykov",
            "Marina M. -C. Höhne"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Beating Transformers using Synthetic Cognition",
        "abstract":"The road to Artificial General Intelligence goes through the generation of episodic reactive behaviors, where the Transformer architecture has been proven to be the state-of-the-art. However, they still fail to develop reasoning. Recently, a novel approach for developing cognitive architectures, called Synthetic Cognition, has been proposed and implemented to develop instantaneous reactive behavior. In this study, we aim to explore the use of Synthetic Cognition to develop episodic reactive behaviors. We propose a mechanism to deal with sequences for the recent implementation of Synthetic Cognition, and test it against DNA foundation models in DNA sequence classification tasks. In our experiments, our proposal clearly outperforms the DNA foundation models, obtaining the best score on more benchmark tasks than the alternatives. Thus, we achieve two goals: expanding Synthetic Cognition to deal with sequences, and beating the Transformer architecture for sequence classification.\n        △ Less",
        "authors":[
            "Alfredo Ibias",
            "Miguel Rodriguez-Galindo",
            "Hector Antona",
            "Guillem Ramirez-Miranda",
            "Enric Guinovart"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"CTSR: Cartesian tensor-based sparse regression for data-driven discovery of high-dimensional invariant governing equations",
        "abstract":"Accurate and concise governing equations are crucial for understanding system dynamics. Recently, data-driven methods such as sparse regression have been employed to automatically uncover governing equations from data, representing a significant shift from traditional first-principles modeling. However, most existing methods focus on scalar equations, limiting their applicability to simple, low-dimensional scenarios, and failing to ensure rotation and reflection invariance without incurring significant computational cost or requiring additional prior knowledge. This paper proposes a Cartesian tensor-based sparse regression (CTSR) technique to accurately and efficiently uncover complex, high-dimensional governing equations while ensuring invariance. Evaluations on two two-dimensional (2D) and two three-dimensional (3D) test cases demonstrate that the proposed method achieves superior accuracy and efficiency compared to the conventional technique.\n        △ Less",
        "authors":[
            "Boqian Zhang",
            "Juanmian Lei",
            "Guoyou Sun",
            "Shuaibing Ding",
            "Jian Guo"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conditional Conformal Risk Adaptation",
        "abstract":"Uncertainty quantification is becoming increasingly important in image segmentation, especially for high-stakes applications like medical imaging. While conformal risk control generalizes conformal prediction beyond standard miscoverage to handle various loss functions such as false negative rate, its application to segmentation often yields inadequate conditional risk control: some images experience very high false negative rates while others have negligibly small ones. We develop Conformal Risk Adaptation (CRA), which introduces a new score function for creating adaptive prediction sets that significantly improve conditional risk control for segmentation tasks. We establish a novel theoretical framework that demonstrates a fundamental connection between conformal risk control and conformal prediction through a weighted quantile approach, applicable to any score function. To address the challenge of poorly calibrated probabilities in segmentation models, we introduce a specialized probability calibration framework that enhances the reliability of pixel-wise inclusion estimates. Using these calibrated probabilities, we propose Calibrated Conformal Risk Adaptation (CCRA) and a stratified variant (CCRA-S) that partitions images based on their characteristics and applies group-specific thresholds to further enhance conditional risk control. Our experiments on polyp segmentation demonstrate that all three methods (CRA, CCRA, and CCRA-S) provide valid marginal risk control and deliver more consistent conditional risk control across diverse images compared to standard approaches, offering a principled approach to uncertainty quantification that is particularly valuable for high-stakes and personalized segmentation applications.\n        △ Less",
        "authors":[
            "Rui Luo",
            "Zhixin Zhou"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Stochastic Smoothed Primal-Dual Algorithms for Nonconvex Optimization with Linear Inequality Constraints",
        "abstract":"We propose smoothed primal-dual algorithms for solving stochastic and smooth nonconvex optimization problems with linear inequality constraints. Our algorithms are single-loop and only require a single stochastic gradient based on one sample at each iteration. A distinguishing feature of our algorithm is that it is based on an inexact gradient descent framework for the Moreau envelope, where the gradient of the Moreau envelope is estimated using one step of a stochastic primal-dual augmented Lagrangian method. To handle inequality constraints and stochasticity, we combine the recently established global error bounds in constrained optimization with a Moreau envelope-based analysis of stochastic proximal algorithms. For obtaining $\\varepsilon$-stationary points, we establish the optimal $O(\\varepsilon^{-4})$ sample complexity guarantee for our algorithms and provide extensions to stochastic linear constraints. We also show how to improve this complexity to $O(\\varepsilon^{-3})$ by using variance reduction and the expected smoothness assumption. Unlike existing methods, the iterations of our algorithms are free of subproblems, large batch sizes or increasing penalty parameters and use dual variable updates to ensure feasibility.\n        △ Less",
        "authors":[
            "Ruichuan Huang",
            "Jiawei Zhang",
            "Ahmet Alacaoglu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Heart Failure Prediction using Modal Decomposition and Masked Autoencoders for Scarce Echocardiography Databases",
        "abstract":"Heart diseases constitute the main cause of international human defunction. According to the World Health Organization (WHO), approximately 18 million deaths happen each year due to precisely heart diseases. In particular, heart failures (HF) press the healthcare industry to develop systems for their early, rapid and effective prediction. In this work, an automatic system which analyses in real-time echocardiography video sequences is proposed for the challenging and more specific task of prediction of heart failure times. This system is based on a novel deep learning framework, and works in two stages. The first one transforms the data included in a database of echocardiography video sequences into a machine learning-compatible collection of annotated images which can be used in the training phase of any kind of machine learning-based framework, including a deep learning one. This initial stage includes the use of the Higher Order Dynamic Mode Decomposition (HODMD) algorithm for both data augmentation and feature extraction. The second stage is focused on building and training a Vision Transformer (ViT). Self-supervised learning (SSL) methods, which have been so far barely explored in the literature about heart failure prediction, are applied to effectively train the ViT from scratch, even with scarce databases of echocardiograms. The designed neural network analyses images from echocardiography sequences to estimate the time in which a heart failure will happen. The results obtained show the efficacy of the HODMD algorithm and the superiority of the proposed system with respect to several established ViT and Convolutional Neural Network (CNN) architectures.\n        △ Less",
        "authors":[
            "Andrés Bell-Navas",
            "María Villalba-Orero",
            "Enrique Lara-Pezzi",
            "Jesús Garicano-Mena",
            "Soledad Le Clainche"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering",
        "abstract":"Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating translations of long, complex passages, and a more ``pragmatic'' approach that assesses how accurately key information is conveyed by a translation in context is needed. We introduce TREQA (Translation Evaluation via Question-Answering), a framework that extrinsically evaluates translation quality by assessing how accurately candidate translations answer reading comprehension questions that target key information in the original source or reference texts. In challenging domains that require long-range understanding, such as literary texts, we show that TREQA is competitive with and, in some cases, outperforms state-of-the-art neural and LLM-based metrics in ranking alternative paragraph-level translations, despite never being explicitly optimized to correlate with human judgments. Furthermore, the generated questions and answers offer interpretability: empirical analysis shows that they effectively target translation errors identified by experts in evaluated datasets. Our code is available at https:\/\/github.com\/deep-spin\/treqa\n        △ Less",
        "authors":[
            "Patrick Fernandes",
            "Sweta Agrawal",
            "Emmanouil Zaranis",
            "André F. T. Martins",
            "Graham Neubig"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Privacy-Preserving Vertical K-Means Clustering",
        "abstract":"Clustering is a fundamental data processing task used for grouping records based on one or more features. In the vertically partitioned setting, data is distributed among entities, with each holding only a subset of those features. A key challenge in this scenario is that computing distances between records requires access to all distributed features, which may be privacy-sensitive and cannot be directly shared with other parties. The goal is to compute the joint clusters while preserving the privacy of each entity's dataset. Existing solutions using secret sharing or garbled circuits implement privacy-preserving variants of Lloyd's algorithm but incur high communication costs, scaling as O(nkt), where n is the number of data points, k the number of clusters, and t the number of rounds. These methods become impractical for large datasets or several parties, limiting their use to LAN settings only. On the other hand, a different line of solutions rely on differential privacy (DP) to outsource the local features of the parties to a central server. However, they often significantly degrade the utility of the clustering outcome due to excessive noise. In this work, we propose a novel solution based on homomorphic encryption and DP, reducing communication complexity to O(n+kt). In our method, parties securely outsource their features once, allowing a computing party to perform clustering operations under encryption. DP is applied only to the clusters' centroids, ensuring privacy with minimal impact on utility. Our solution clusters 100,000 two-dimensional points into five clusters using only 73MB of communication, compared to 101GB for existing works, and completes in just under 3 minutes on a 100Mbps network, whereas existing works take over 1 day. This makes our solution practical even for WAN deployments, all while maintaining accuracy comparable to plaintext k-means algorithms.\n        △ Less",
        "authors":[
            "Federico Mazzone",
            "Trevor Brown",
            "Florian Kerschbaum",
            "Kevin H. Wilson",
            "Maarten Everts",
            "Florian Hahn",
            "Andreas Peter"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Benchmarking Image Embeddings for E-Commerce: Evaluating Off-the Shelf Foundation Models, Fine-Tuning Strategies and Practical Trade-offs",
        "abstract":"We benchmark foundation models image embeddings for classification and retrieval in e-Commerce, evaluating their suitability for real-world applications. Our study spans embeddings from pre-trained convolutional and transformer models trained via supervised, self-supervised, and text-image contrastive learning. We assess full fine-tuning and transfer learning (top-tuning) on six diverse e-Commerce datasets: fashion, consumer goods, cars, food, and retail. Results show full fine-tuning consistently performs well, while text-image and self-supervised embeddings can match its performance with less training. While supervised embeddings remain stable across architectures, SSL and contrastive embeddings vary significantly, often benefiting from top-tuning. Top-tuning emerges as an efficient alternative to full fine-tuning, reducing computational costs. We also explore cross-tuning, noting its impact depends on dataset characteristics. Our findings offer practical guidelines for embedding selection and fine-tuning strategies, balancing efficiency and performance.\n        △ Less",
        "authors":[
            "Urszula Czerwinska",
            "Cenk Bircanoglu",
            "Jeremy Chamoux"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Diffusion Transformers for Tabular Data Time Series Generation",
        "abstract":"Tabular data generation has recently attracted a growing interest due to its different application scenarios. However, generating time series of tabular data, where each element of the series depends on the others, remains a largely unexplored domain. This gap is probably due to the difficulty of jointly solving different problems, the main of which are the heterogeneity of tabular data (a problem common to non-time-dependent approaches) and the variable length of a time series. In this paper, we propose a Diffusion Transformers (DiTs) based approach for tabular data series generation. Inspired by the recent success of DiTs in image and video generation, we extend this framework to deal with heterogeneous data and variable-length sequences. Using extensive experiments on six datasets, we show that the proposed approach outperforms previous work by a large margin.\n        △ Less",
        "authors":[
            "Fabrizio Garuti",
            "Enver Sangineto",
            "Simone Luetto",
            "Lorenzo Forni",
            "Rita Cucchiara"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PhaseGen: A Diffusion-Based Approach for Complex-Valued MRI Data Generation",
        "abstract":"Magnetic resonance imaging (MRI) raw data, or k-Space data, is complex-valued, containing both magnitude and phase information. However, clinical and existing Artificial Intelligence (AI)-based methods focus only on magnitude images, discarding the phase data despite its potential for downstream tasks, such as tumor segmentation and classification. In this work, we introduce $\\textit{PhaseGen}$, a novel complex-valued diffusion model for generating synthetic MRI raw data conditioned on magnitude images, commonly used in clinical practice. This enables the creation of artificial complex-valued raw data, allowing pretraining for models that require k-Space information. We evaluate PhaseGen on two tasks: skull-stripping directly in k-Space and MRI reconstruction using the publicly available FastMRI dataset. Our results show that training with synthetic phase data significantly improves generalization for skull-stripping on real-world data, with an increased segmentation accuracy from $41.1\\%$ to $80.1\\%$, and enhances MRI reconstruction when combined with limited real-world data. This work presents a step forward in utilizing generative AI to bridge the gap between magnitude-based datasets and the complex-valued nature of MRI raw data. This approach allows researchers to leverage the vast amount of avaliable image domain data in combination with the information-rich k-Space data for more accurate and efficient diagnostic tasks. We make our code publicly $\\href{https:\/\/github.com\/TIO-IKIM\/PhaseGen}{\\text{available here}}$.\n        △ Less",
        "authors":[
            "Moritz Rempe",
            "Fabian Hörst",
            "Helmut Becker",
            "Marco Schlimbach",
            "Lukas Rotkopf",
            "Kevin Kröninger",
            "Jens Kleesiek"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Using LLMs for Analyzing AIS Data",
        "abstract":"Recent research in Large Language Models (LLMs), has had a profound impact across various fields, including mobility data science. This paper explores the and experiment with different approaches to using LLMs for analyzing AIS data. We propose a set of carefully designed queries to assess the reasoning capabilities of LLMs in this kind of tasks. Further, we experiment with four different methods: (1) using LLMs as a natural language interface to a spatial database, (2) reasoning on raw data, (3) reasoning on compressed trajectories, and (4) reasoning on semantic trajectories. We investigate the strengths and weaknesses for the four methods, and discuss the findings. The goal is to provide valuable insights for both researchers and practitioners on selecting the most appropriate LLM-based method depending on their specific data analysis objectives.\n        △ Less",
        "authors":[
            "Gaspard Merten",
            "Gilles Dejaegere",
            "Mahmoud Sakr"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"SydneyScapes: Image Segmentation for Australian Environments",
        "abstract":"Autonomous Vehicles (AVs) are being partially deployed and tested across various global locations, including China, the USA, Germany, France, Japan, Korea, and the UK, but with limited demonstrations in Australia. The integration of machine learning (ML) into AV perception systems highlights the need for locally labelled datasets to develop and test algorithms in specific environments. To address this, we introduce SydneyScapes - a dataset tailored for computer vision tasks of image semantic, instance, and panoptic segmentation. This dataset, collected from Sydney and surrounding cities in New South Wales (NSW), Australia, consists of 756 images with high-quality pixel-level annotations. It is designed to assist AV industry and researchers by providing annotated data and tools for algorithm development, testing, and deployment in the Australian context. Additionally, we offer benchmarking results using state-of-the-art algorithms to establish reference points for future research and development. The dataset is publicly available at https:\/\/hdl.handle.net\/2123\/33051.\n        △ Less",
        "authors":[
            "Hongyu Lyu",
            "Julie Stephany Berrio",
            "Mao Shan",
            "Stewart Worrall"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs",
        "abstract":"We present a design called \\emph{Proof of Gradient Optimization} (PoGO) for blockchain consensus, where miners produce verifiable evidence of training large-scale machine-learning models. Building on previous work, we incorporate \\emph{quantized gradients} (4-bit precision) to reduce storage and computation requirements, while still preserving the ability of verifiers to check that real progress has been made on lowering the model's loss. Additionally, we employ Merkle proofs over the full 32-bit model to handle large parameter sets and to enable random leaf checks with minimal on-chain data. We illustrate these ideas using GPT-3 (175B parameters) as a reference example and also refer to smaller but high-performance models (e.g., \\emph{Gemma~3} with 27B parameters). We provide an empirical cost analysis showing that verification is significantly cheaper than training, thanks in part to quantization and sampling. We also discuss the necessity of longer block times (potentially hours) when incorporating meaningful training steps, the trade-offs when using specialized GPU hardware, and how binary diffs may incrementally optimize updates. Finally, we note that fine-tuning can be handled in a similar manner, merely changing the dataset and the manner of sampling but preserving the overall verification flow. Our protocol allows verifiers to issue either \\emph{positive} or \\emph{negative} attestations; these are aggregated at finalization to either confirm the update or slash the miner.\n        △ Less",
        "authors":[
            "José I. Orlicki"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation",
        "abstract":"AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft. Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences.\n        △ Less",
        "authors":[
            "Tuhin Chakrabarty",
            "Philippe Laban",
            "Chien-Sheng Wu"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A taxonomy of epistemic injustice in the context of AI and the case for generative hermeneutical erasure",
        "abstract":"Whether related to machine learning models' epistemic opacity, algorithmic classification systems' discriminatory automation of testimonial prejudice, the distortion of human beliefs via the 'hallucinations' of generative AI, the inclusion of the global South in global AI governance, the execution of bureaucratic violence via algorithmic systems, or located in the interaction with conversational artificial agents epistemic injustice related to AI is a growing concern. Based on a proposed general taxonomy of epistemic injustice, this paper first sketches a taxonomy of the types of epistemic injustice in the context of AI, relying on the work of scholars from the fields of philosophy of technology, political philosophy and social epistemology. Secondly, an additional perspective on epistemic injustice in the context of AI: generative hermeneutical erasure. I argue that this injustice that can come about through the application of Large Language Models (LLMs) and contend that generative AI, when being deployed outside of its Western space of conception, can have effects of conceptual erasure, particularly in the epistemic domain, followed by forms of conceptual disruption caused by a mismatch between AI system and the interlocutor in terms of conceptual frameworks. AI systems' 'view from nowhere' epistemically inferiorizes non-Western epistemologies and thereby contributes to the erosion of their epistemic particulars, gradually contributing to hermeneutical erasure. This work's relevance lies in proposal of a taxonomy that allows epistemic injustices to be mapped in the AI domain and the proposal of a novel form of AI-related epistemic injustice.\n        △ Less",
        "authors":[
            "Warmhold Jan Thomas Mollema"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Adversarial Subspace Generation for Outlier Detection in High-Dimensional Data",
        "abstract":"Outlier detection in high-dimensional tabular data is challenging since data is often distributed across multiple lower-dimensional subspaces -- a phenomenon known as the Multiple Views effect (MV). This effect led to a large body of research focused on mining such subspaces, known as subspace selection. However, as the precise nature of the MV effect was not well understood, traditional methods had to rely on heuristic-driven search schemes that struggle to accurately capture the true structure of the data. Properly identifying these subspaces is critical for unsupervised tasks such as outlier detection or clustering, where misrepresenting the underlying data structure can hinder the performance. We introduce Myopic Subspace Theory (MST), a new theoretical framework that mathematically formulates the Multiple Views effect and writes subspace selection as a stochastic optimization problem. Based on MST, we introduce V-GAN, a generative method trained to solve such an optimization problem. This approach avoids any exhaustive search over the feature space while ensuring that the intrinsic data structure is preserved. Experiments on 42 real-world datasets show that using V-GAN subspaces to build ensemble methods leads to a significant increase in one-class classification performance -- compared to existing subspace selection, feature selection, and embedding methods. Further experiments on synthetic data show that V-GAN identifies subspaces more accurately while scaling better than other relevant subspace selection methods. These results confirm the theoretical guarantees of our approach and also highlight its practical viability in high-dimensional settings.\n        △ Less",
        "authors":[
            "Jose Cribeiro-Ramallo",
            "Federico Matteucci",
            "Paul Enciu",
            "Alexander Jenke",
            "Vadim Arzamasov",
            "Thorsten Strufe",
            "Klemens Böhm"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"GPT Carry-On: Training Foundation Model for Customization Could Be Simple, Scalable and Affordable",
        "abstract":"Modern large language foundation models (LLM) have now entered the daily lives of millions of users. We ask a natural question whether it is possible to customize LLM for every user or every task. From system and industrial economy consideration, general continue-training or fine-tuning still require substantial computation and memory of training GPU nodes, whereas most inference nodes under deployment, possibly with lower-end GPUs, are configured to make forward pass fastest possible. We propose a framework to take full advantages of existing LLMs and systems of online service. We train an additional branch of transformer blocks on the final-layer embedding of pretrained LLMs, which is the base, then a carry-on module merge the base models to compose a customized LLM. We can mix multiple layers, or multiple LLMs specialized in different domains such as chat, coding, math, to form a new mixture of LLM that best fit a new task. As the base model don't need to update parameters, we are able to outsource most computation of the training job on inference nodes, and only train a lightweight carry-on on training nodes, where we consume less than 1GB GPU memory to train a 100M carry-on layer on 30B LLM. We tested Qwen and DeepSeek opensourced models for continue-pretraining and got faster loss convergence. We use it to improve solving math questions with extremely small computation and model size, with 1000 data samples of chain-of-thoughts, and as small as 1 MB parameters of two layer layer carry-on, and the results are promising.\n        △ Less",
        "authors":[
            "Jianqiao Wangni"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM Inference Serving",
        "abstract":"Large language model (LLM) inference serving systems are essential to various LLM-based applications. As demand for LLM services continues to grow, scaling these systems to handle high request rates while meeting latency Service-Level Objectives (SLOs), referred to as effective throughput, becomes critical. However, existing systems often struggle to improve effective throughput, primarily due to a significant decline in Time To First Token (TTFT) SLO attainment. We identify two major causes of this bottleneck: (1) memory-intensive KV cache that limits batch size expansion under GPU memory constraints, and (2) rigid batch composition enforced by the default First-Come-First-Serve scheduling policy. In this paper, we introduce Apt-Serve, a scalable framework designed to enhance effective throughput in LLM inference serving. Apt-Serve features a new hybrid cache scheme that combines KV cache with a memory-efficient hidden cache for reusable input hidden state vectors, allowing large batch sizes and improving request concurrency. Based on the hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism that dynamically optimizes batch composition. We formally define the adaptive scheduling optimization problem and propose an efficient algorithm with theoretical guarantees. Extensive evaluations on three real-world datasets and LLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up to 8.8x improvement in effective throughput compared to the state-of-the-art inference serving systems.\n        △ Less",
        "authors":[
            "Shihong Gao",
            "Xin Zhang",
            "Yanyan Shen",
            "Lei Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"A Mechanism-Learning Deeply Coupled Model for Remote Sensing Retrieval of Global Land Surface Temperature",
        "abstract":"Land surface temperature (LST) retrieval from remote sensing data is pivotal for analyzing climate processes and surface energy budgets. However, LST retrieval is an ill-posed inverse problem, which becomes particularly severe when only a single band is available. In this paper, we propose a deeply coupled framework integrating mechanistic modeling and machine learning to enhance the accuracy and generalizability of single-channel LST retrieval. Training samples are generated using a physically-based radiative transfer model and a global collection of 5810 atmospheric profiles. A physics-informed machine learning framework is proposed to systematically incorporate the first principles from classical physical inversion models into the learning workflow, with optimization constrained by radiative transfer equations. Global validation demonstrated a 30% reduction in root-mean-square error versus standalone methods. Under extreme humidity, the mean absolute error decreased from 4.87 K to 2.29 K (53% improvement). Continental-scale tests across five continents confirmed the superior generalizability of this model.\n        △ Less",
        "authors":[
            "Tian Xie",
            "Menghui Jiang",
            "Huanfeng Shen",
            "Huifang Li",
            "Chao Zeng",
            "Xiaobin Guan",
            "Jun Ma",
            "Guanhao Zhang",
            "Liangpei Zhang"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Intelligent DoS and DDoS Detection: A Hybrid GRU-NTM Approach to Network Security",
        "abstract":"Detecting Denial of Service (DoS) and Distributed Denial of Service (DDoS) attacks remains a critical challenge in cybersecurity. This research introduces a hybrid deep learning model combining Gated Recurrent Units (GRUs) and a Neural Turing Machine (NTM) for enhanced intrusion detection. Trained on the UNSW-NB15 and BoT-IoT datasets, the model employs GRU layers for sequential data processing and an NTM for long-term pattern recognition. The proposed approach achieves 99% accuracy in distinguishing between normal, DoS, and DDoS traffic. These findings offer promising advancements in real-time threat detection and contribute to improved network security across various domains.\n        △ Less",
        "authors":[
            "Caroline Panggabean",
            "Chandrasekar Venkatachalam",
            "Priyanka Shah",
            "Sincy John",
            "Renuka Devi P",
            "Shanmugavalli Venkatachalam"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Traversal Learning Coordination For Lossless And Efficient Distributed Learning",
        "abstract":"In this paper, we introduce Traversal Learning (TL), a novel approach designed to address the problem of decreased quality encountered in popular distributed learning (DL) paradigms such as Federated Learning (FL), Split Learning (SL), and SplitFed Learning (SFL). Traditional FL experiences from an accuracy drop during aggregation due to its averaging function, while SL and SFL face increased loss due to the independent gradient updates on each split network. TL adopts a unique strategy where the model traverses the nodes during forward propagation (FP) and performs backward propagation (BP) on the orchestrator, effectively implementing centralized learning (CL) principles within a distributed environment. The orchestrator is tasked with generating virtual batches and planning the sequential node visits of the model during FP, aligning them with the ordered index of the data within these batches. We conducted experiments on six datasets representing diverse characteristics across various domains. Our evaluation demonstrates that TL is on par with classic CL approaches in terms of accurate inference, thereby offering a viable and robust solution for DL tasks. TL outperformed other DL methods and improved accuracy by 7.85% for independent and identically distributed (IID) datasets, macro F1-score by 1.06% for non-IID datasets, accuracy by 2.60% for text classification, and AUC by 3.88% and 4.54% for medical and financial datasets, respectively. By effectively preserving data privacy while maintaining performance, TL represents a significant advancement in DL methodologies.\n        △ Less",
        "authors":[
            "Erdenebileg Batbaatar",
            "Jeonggeol Kim",
            "Yongcheol Kim",
            "Young Yoon"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Modal Data Fusion for Moisture Content Prediction in Apple Drying",
        "abstract":"Fruit drying is widely used in food manufacturing to reduce product moisture, ensure product safety, and extend product shelf life. Accurately predicting final moisture content (MC) is critically needed for quality control of drying processes. State-of-the-art methods can build deterministic relationships between process parameters and MC, but cannot adequately account for inherent process variabilities that are ubiquitous in fruit drying. To address this gap, this paper presents a novel multi-modal data fusion framework to effectively fuse two modalities of data: tabular data (process parameters) and high-dimensional image data (images of dried apple slices) to enable accurate MC prediction. The proposed modeling architecture permits flexible adjustment of information portion from tabular and image data modalities. Experimental validation shows that the multi-modal approach improves predictive accuracy substantially compared to state-of-the-art methods. The proposed method reduces root-mean-squared errors by 19.3%, 24.2%, and 15.2% over tabular-only, image-only, and standard tabular-image fusion models, respectively. Furthermore, it is demonstrated that our method is robust in varied tabular-image ratios and capable of effectively capturing inherent small-scale process variabilities. The proposed framework is extensible to a variety of other drying technologies.\n        △ Less",
        "authors":[
            "Shichen Li",
            "Chenhui Shao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
        "abstract":"Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https:\/\/github.com\/juzhengz\/LoRI\n        △ Less",
        "authors":[
            "Juzheng Zhang",
            "Jiacheng You",
            "Ashwinee Panda",
            "Tom Goldstein"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Unifying and extending Diffusion Models through PDEs for solving Inverse Problems",
        "abstract":"Diffusion models have emerged as powerful generative tools with applications in computer vision and scientific machine learning (SciML), where they have been used to solve large-scale probabilistic inverse problems. Traditionally, these models have been derived using principles of variational inference, denoising, statistical signal processing, and stochastic differential equations. In contrast to the conventional presentation, in this study we derive diffusion models using ideas from linear partial differential equations and demonstrate that this approach has several benefits that include a constructive derivation of the forward and reverse processes, a unified derivation of multiple formulations and sampling strategies, and the discovery of a new class of models. We also apply the conditional version of these models to solving canonical conditional density estimation problems and challenging inverse problems. These problems help establish benchmarks for systematically quantifying the performance of different formulations and sampling strategies in this study, and for future studies. Finally, we identify and implement a mechanism through which a single diffusion model can be applied to measurements obtained from multiple measurement operators. Taken together, the contents of this manuscript provide a new understanding and several new directions in the application of diffusion models to solving physics-based inverse problems.\n        △ Less",
        "authors":[
            "Agnimitra Dasgupta",
            "Alexsander Marciano da Cunha",
            "Ali Fardisi",
            "Mehrnegar Aminy",
            "Brianna Binder",
            "Bryan Shaddy",
            "Assad A Oberai"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Conditional Data Synthesis Augmentation",
        "abstract":"Reliable machine learning and statistical analysis rely on diverse, well-distributed training data. However, real-world datasets are often limited in size and exhibit underrepresentation across key subpopulations, leading to biased predictions and reduced performance, particularly in supervised tasks such as classification. To address these challenges, we propose Conditional Data Synthesis Augmentation (CoDSA), a novel framework that leverages generative models, such as diffusion models, to synthesize high-fidelity data for improving model performance across multimodal domains including tabular, textual, and image data. CoDSA generates synthetic samples that faithfully capture the conditional distributions of the original data, with a focus on under-sampled or high-interest regions. Through transfer learning, CoDSA fine-tunes pre-trained generative models to enhance the realism of synthetic data and increase sample density in sparse areas. This process preserves inter-modal relationships, mitigates data imbalance, improves domain adaptation, and boosts generalization. We also introduce a theoretical framework that quantifies the statistical accuracy improvements enabled by CoDSA as a function of synthetic sample volume and targeted region allocation, providing formal guarantees of its effectiveness. Extensive experiments demonstrate that CoDSA consistently outperforms non-adaptive augmentation strategies and state-of-the-art baselines in both supervised and unsupervised settings.\n        △ Less",
        "authors":[
            "Xinyu Tian",
            "Xiaotong Shen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Enhancing Player Enjoyment with a Two-Tier DRL and LLM-Based Agent System for Fighting Games",
        "abstract":"Deep reinforcement learning (DRL) has effectively enhanced gameplay experiences and game design across various game genres. However, few studies on fighting game agents have focused explicitly on enhancing player enjoyment, a critical factor for both developers and players. To address this gap and establish a practical baseline for designing enjoyability-focused agents, we propose a two-tier agent (TTA) system and conducted experiments in the classic fighting game Street Fighter II. The first tier of TTA employs a task-oriented network architecture, modularized reward functions, and hybrid training to produce diverse and skilled DRL agents. In the second tier of TTA, a Large Language Model Hyper-Agent, leveraging players' playing data and feedback, dynamically selects suitable DRL opponents. In addition, we investigate and model several key factors that affect the enjoyability of the opponent. The experiments demonstrate improvements from 64. 36% to 156. 36% in the execution of advanced skills over baseline methods. The trained agents also exhibit distinct game-playing styles. Additionally, we conducted a small-scale user study, and the overall enjoyment in the player's feedback validates the effectiveness of our TTA system.\n        △ Less",
        "authors":[
            "Shouren Wang",
            "Zehua Jiang",
            "Fernando Sliva",
            "Sam Earle",
            "Julian Togelius"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"The Role of Machine Learning in Reducing Healthcare Costs: The Impact of Medication Adherence and Preventive Care on Hospitalization Expenses",
        "abstract":"This study reveals the important role of prevention care and medication adherence in reducing hospitalizations. By using a structured dataset of 1,171 patients, four machine learning models Logistic Regression, Gradient Boosting, Random Forest, and Artificial Neural Networks are applied to predict five-year hospitalization risk, with the Gradient Boosting model achieving the highest accuracy of 81.2%. The result demonstrated that patients with high medication adherence and consistent preventive care can reduce 38.3% and 37.7% in hospitalization risk. The finding also suggests that targeted preventive care can have positive Return on Investment (ROI), and therefore ML models can effectively direct personalized interventions and contribute to long-term medical savings.\n        △ Less",
        "authors":[
            "Yixin Zhang",
            "Yisong Chen"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability",
        "abstract":"Recent advancements in multi-modal models have significantly improved vision-language alignment in radiology. However, existing approaches struggle to effectively utilize complex radiology reports for learning, rely on low-resolution images, and offer limited interpretability in attention mechanisms. To address these challenges, we introduce RadZero, a novel similarity-based cross-attention framework for vision-language alignment in radiology with zero-shot multi-task capability. RadZero leverages large language models to extract minimal semantic sentences from radiology reports and employs a multi-positive contrastive learning strategy to effectively capture relationships between images and multiple relevant textual descriptions. It also utilizes a pre-trained vision encoder with additional trainable Transformer layers, allowing efficient high-resolution image processing. By computing similarity between text embeddings and local image patch features, RadZero enables zero-shot inference with similarity probability for classification and pixel-level cross-modal similarity maps for grounding and segmentation. Experimental results on public chest radiograph benchmarks show that RadZero outperforms state-of-the-art methods in zero-shot classification, grounding, and segmentation. Furthermore, cross-modal similarity map analysis highlights its potential for improving explainability in vision-language alignment. Additionally, qualitative evaluation demonstrates RadZero's capability for open-vocabulary semantic segmentation, further validating its effectiveness in medical imaging.\n        △ Less",
        "authors":[
            "Jonggwon Park",
            "Soobum Kim",
            "Byungmu Yoon",
            "Kyoyun Choi"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Leveraging LLMs for Multimodal Retrieval-Augmented Radiology Report Generation via Key Phrase Extraction",
        "abstract":"Automated radiology report generation (RRG) holds potential to reduce radiologists' workload, especially as recent advancements in large language models (LLMs) enable the development of multimodal models for chest X-ray (CXR) report generation. However, multimodal LLMs (MLLMs) are resource-intensive, requiring vast datasets and substantial computational cost for training. To address these challenges, we propose a retrieval-augmented generation approach that leverages multimodal retrieval and LLMs to generate radiology reports while mitigating hallucinations and reducing computational demands. Our method uses LLMs to extract key phrases from radiology reports, effectively focusing on essential diagnostic information. Through exploring effective training strategies, including image encoder structure search, adding noise to text embeddings, and additional training objectives, we combine complementary pre-trained image encoders and adopt contrastive learning between text and semantic image embeddings. We evaluate our approach on MIMIC-CXR dataset, achieving state-of-the-art results on CheXbert metrics and competitive RadGraph F1 metric alongside MLLMs, without requiring LLM fine-tuning. Our method demonstrates robust generalization for multi-view RRG, making it suitable for comprehensive clinical applications.\n        △ Less",
        "authors":[
            "Kyoyun Choi",
            "Byungmu Yoon",
            "Soobum Kim",
            "Jonggwon Park"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Multi-Selection for Recommendation Systems",
        "abstract":"We present the construction of a multi-selection model to answer differentially private queries in the context of recommendation systems. The server sends back multiple recommendations and a ``local model'' to the user, which the user can run locally on its device to select the item that best fits its private features. We study a setup where the server uses a deep neural network (trained on the Movielens 25M dataset as the ground truth for movie recommendation. In the multi-selection paradigm, the average recommendation utility is approximately 97\\% of the optimal utility (as determined by the ground truth neural network) while maintaining a local differential privacy guarantee with $ε$ ranging around 1 with respect to feature vectors of neighboring users. This is in comparison to an average recommendation utility of 91\\% in the non-multi-selection regime under the same constraints.\n        △ Less",
        "authors":[
            "Sahasrajit Sarmasarkar",
            "Zhihao Jiang",
            "Ashish Goel",
            "Aleksandra Korolova",
            "Kamesh Munagala"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models",
        "abstract":"We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for Target Speaker Extraction (TSE) based on the LauraGPT backbone. It employs a small-scale auto-regressive decoder-only language model which takes the continuous representations for both the mixture and the reference speeches and produces the first few layers of the target speech's discrete codec representations. In addition, a one-step encoder-only language model reconstructs the sum of the predicted codec embeddings using both the mixture and the reference information. Our approach achieves superior or comparable performance to existing generative and discriminative TSE models. To the best of our knowledge, LauraTSE is the first single-task TSE model to leverage an auto-regressive decoder-only language model as the backbone.\n        △ Less",
        "authors":[
            "Beilong Tang",
            "Bang Zeng",
            "Ming Li"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"MicroNAS: An Automated Framework for Developing a Fall Detection System",
        "abstract":"This work presents MicroNAS, an automated neural architecture search tool specifically designed to create models optimized for microcontrollers with small memory resources. The ESP32 microcontroller, with 320 KB of memory, is used as the target platform. The artificial intelligence contribution lies in a novel method for optimizing convolutional neural network and gated recurrent unit architectures by considering the memory size of the target microcontroller as a guide. A comparison is made between memory-driven model optimization and traditional two-stage methods, which use pruning, to show the effectiveness of the proposed framework. To demonstrate the engineering application of MicroNAS, a fall detection system (FDS) for lower-limb amputees is developed as a pilot study. A critical challenge in fall detection studies, class imbalance in the dataset, is addressed. The results show that MicroNAS models achieved higher F1-scores than alternative approaches, such as ensemble methods and H2O Automated Machine Learning, presenting a significant step forward in real-time FDS development. Biomechanists using body-worn sensors for activity detection can adopt the open-source code to design machine learning models tailored for microcontroller platforms with limited memory.\n        △ Less",
        "authors":[
            "Seyed Mojtaba Mohasel",
            "John Sheppard",
            "Lindsey K. Molina",
            "Richard R. Neptune",
            "Shane R. Wurdeman",
            "Corey A. Pew"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"Automating quantum feature map design via large language models",
        "abstract":"Quantum feature maps are a key component of quantum machine learning, encoding classical data into quantum states to exploit the expressive power of high-dimensional Hilbert spaces. Despite their theoretical promise, designing quantum feature maps that offer practical advantages over classical methods remains an open challenge. In this work, we propose an agentic system that autonomously generates, evaluates, and refines quantum feature maps using large language models. The system consists of five component: Generation, Storage, Validation, Evaluation, and Review. Using these components, it iteratively improves quantum feature maps. Experiments on the MNIST dataset show that it can successfully discover and refine feature maps without human intervention. The best feature map generated outperforms existing quantum baselines and achieves competitive accuracy compared to classical kernels across MNIST, Fashion-MNIST, and CIFAR-10. Our approach provides a framework for exploring dataset-adaptive quantum features and highlights the potential of LLM-driven automation in quantum algorithm design.\n        △ Less",
        "authors":[
            "Kenya Sakka",
            "Kosuke Mitarai",
            "Keisuke Fujii"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"ClimateBench-M: A Multi-Modal Climate Data Benchmark with a Simple Generative Method",
        "abstract":"Climate science studies the structure and dynamics of Earth's climate system and seeks to understand how climate changes over time, where the data is usually stored in the format of time series, recording the climate features, geolocation, time attributes, etc. Recently, much research attention has been paid to the climate benchmarks. In addition to the most common task of weather forecasting, several pioneering benchmark works are proposed for extending the modality, such as domain-specific applications like tropical cyclone intensity prediction and flash flood damage estimation, or climate statement and confidence level in the format of natural language. To further motivate the artificial general intelligence development for climate science, in this paper, we first contribute a multi-modal climate benchmark, i.e., ClimateBench-M, which aligns (1) the time series climate data from ERA5, (2) extreme weather events data from NOAA, and (3) satellite image data from NASA HLS based on a unified spatial-temporal granularity. Second, under each data modality, we also propose a simple but strong generative method that could produce competitive performance in weather forecasting, thunderstorm alerts, and crop segmentation tasks in the proposed ClimateBench-M. The data and code of ClimateBench-M are publicly available at https:\/\/github.com\/iDEA-iSAIL-Lab-UIUC\/ClimateBench-M.\n        △ Less",
        "authors":[
            "Dongqi Fu",
            "Yada Zhu",
            "Zhining Liu",
            "Lecheng Zheng",
            "Xiao Lin",
            "Zihao Li",
            "Liri Fang",
            "Katherine Tieu",
            "Onkar Bhardwaj",
            "Kommy Weldemariam",
            "Hanghang Tong",
            "Hendrik Hamann",
            "Jingrui He"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    },
    {
        "title":"State Estimation Using Particle Filtering in Adaptive Machine Learning Methods: Integrating Q-Learning and NEAT Algorithms with Noisy Radar Measurements",
        "abstract":"Reliable state estimation is essential for autonomous systems operating in complex, noisy environments. Classical filtering approaches, such as the Kalman filter, can struggle when facing nonlinear dynamics or non-Gaussian noise, and even more flexible particle filters often encounter sample degeneracy or high computational costs in large-scale domains. Meanwhile, adaptive machine learning techniques, including Q-learning and neuroevolutionary algorithms such as NEAT, rely heavily on accurate state feedback to guide learning; when sensor data are imperfect, these methods suffer from degraded convergence and suboptimal performance. In this paper, we propose an integrated framework that unifies particle filtering with Q-learning and NEAT to explicitly address the challenge of noisy measurements. By refining radar-based observations into reliable state estimates, our particle filter drives more stable policy updates (in Q-learning) or controller evolution (in NEAT), allowing both reinforcement learning and neuroevolution to converge faster, achieve higher returns or fitness, and exhibit greater resilience to sensor uncertainty. Experiments on grid-based navigation and a simulated car environment highlight consistent gains in training stability, final performance, and success rates over baselines lacking advanced filtering. Altogether, these findings underscore that accurate state estimation is not merely a preprocessing step, but a vital component capable of substantially enhancing adaptive machine learning in real-world applications plagued by sensor noise.\n        △ Less",
        "authors":[
            "Wonjin Song",
            "Feng Bao"
        ],
        "journal_conference_name":null,
        "publisher":"arXiv",
        "year":2025,
        "doi":null,
        "group_name":null
    }
]