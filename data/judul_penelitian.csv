title,abstract,authors,journal_conference_name,publisher,year,doi,group_name
PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore,"Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity toward specific aspects within atext, enabling more precise opinionminingin domains such as product reviews and social media. However, traditional FGSA approaches often require task-specific architectures and extensive annotated data, limiting their generalization and scalability. To address these challenges, we propose PL-FGSA, a unified prompt learning-based framework implemented using the MindSpore platform, which integrates prompt design with a lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task prompt-augmented generation problem, jointly tackling aspect extraction, sentiment classification, and causal explanation in a unified paradigm. By leveraging prompt-based guidance, PL-FGSA enhances interpretability and achieves strong performance under both full-data and low-resource conditions. Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and MAMS-demonstrate that our model consistently outperforms traditional fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597, respectively. These results validate the effectiveness of prompt-based generalization and highlight the practical value of PL-FGSA for real-world sentiment analysis tasks.","['Zhenkai Qin', 'Jiajing He', 'Qiao Fang']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.14165,Anomali
MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations,"Large Language Models (LLMs) have inherent limitations of faithfulness and factuality, commonly referred to as hallucinations. Several benchmarks have been developed that provide a test bed for factuality evaluation within the context of English-centric datasets, while relying on supplementary informative context like web links ortextpassages but ignoring the available structured factual resources. To this end, Knowledge Graphs (KGs) have been identified as a useful aid for hallucination mitigation, as they provide a structured way to represent the facts about entities and their relations with minimal linguistic overhead. We bridge the lack of KG paths and multilinguality for factual language modeling within the existing hallucination evaluation benchmarks and propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal} framed for generativetextevaluation. As part of our data collection pipeline, wemined140k KG-paths from open-domain KGs, from which we pruned noisy KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation shows an absolute scale increase by approximately 0.12 to 0.36 points for the semantic similarity score in KG-RAG over vanilla QA across multiple languages and multiple models, demonstrating the potential of KG integration. We anticipate MultiHal will foster future research towards several graph-based hallucination mitigation and fact-checking tasks.","['Ernests Lavrinovics', 'Russa Biswas', 'Katja Hose', 'Johannes Bjerva']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.14101,Anomali
Enhanced Multimodal Hate Video Detection via Channel-wise and Modality-wise Fusion,"The rapid rise of video content on platforms such as TikTok and YouTube has transformed information dissemination, but it has also facilitated the spread of harmful content, particularly hate videos. Despite significant efforts to combat hate speech, detecting these videos remains challenging due to their often implicit nature. Current detection methods primarily rely on unimodal approaches, which inadequately capture the complementary features across different modalities. While multimodal techniques offer a broader perspective, many fail to effectively integrate temporal dynamics and modality-wise interactions essential for identifying nuanced hate content. In this paper, we present CMFusion, an enhanced multimodal hate video detection model utilizing a novel Channel-wise and Modality-wise Fusion Mechanism. CMFusion first extracts features fromtext, audio, and video modalities using pre-trained models and then incorporates a temporal cross-attention mechanism to capture dependencies between video and audio streams. The learned features are then processed by channel-wise and modality-wise fusion modules to obtain informative representations of videos. Our extensive experiments on a real-world dataset demonstrate that CMFusion significantly outperforms five widely used baselines in terms of accuracy, precision, recall, and F1 score. Comprehensive ablation studies and parameter analyses further validate our design choices, highlighting the model's effectiveness in detecting hate videos. The source codes will be made publicly available at https://github.com/EvelynZ10/cmfusion.","['Yinghui Zhang', 'Tailin Chen', 'Yuchen Zhang', 'Zeyu Fu']","2024 IEEE International Conference on Data Mining Workshops (ICDMW), Abu Dhabi, United Arab Emirates, 2024, pp. 183-190",arXiv,2025,https://doi.org/10.48550/arXiv.2505.12051,Anomali
TechniqueRAG: Retrieval Augmented Generation for Adversarial Technique Annotation in Cyber Threat Intelligence Text,"Accurately identifying adversarial techniques in securitytextsis critical for effective cyber defense. However, existing methods face a fundamental trade-off: they either rely on generic models with limited domain precision or require resource-intensive pipelines that depend on large labeled datasets and task-specific optimizations, such as custom hard-negativeminingand denoising, resources rarely available in specialized domains.
  We propose TechniqueRAG, a domain-specific retrieval-augmented generation (RAG) framework that bridges this gap by integrating off-the-shelf retrievers, instruction-tuned LLMs, and minimaltext-technique pairs. Our approach addresses data scarcity by fine-tuning only the generation component on limited in-domain examples, circumventing the need for resource-intensive retrieval training. While conventional RAG mitigates hallucination by coupling retrieval and generation, its reliance on generic retrievers often introduces noisy candidates, limiting domain-specific precision. To address this, we enhance retrieval quality and domain specificity through zero-shot LLM re-ranking, which explicitly aligns retrieved candidates with adversarial techniques.
  Experiments on multiple security benchmarks demonstrate that TechniqueRAG achieves state-of-the-art performance without extensive task-specific optimizations or labeled data, while comprehensive analysis provides further insights.","['Ahmed Lekssays', 'Utsav Shukla', 'Husrev Taha Sencar', 'Md Rizwan Parvez']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.11988,Anomali
Conversational Recommendation System using NLP and Sentiment Analysis,"In today's digitally-driven world, the demand for personalized and context-aware recommendations has never been greater. Traditional recommender systems have made significant strides in this direction, but they often lack the ability to tap into the richness of conversational data. This paper represents a novel approach to recommendation systems by integrating conversational insights into the recommendation process. The Conversational Recommender System integrates cutting-edge technologies such as deep learning, leveraging machine learning algorithms like Apriori for Association RuleMining, Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and Long Short-Term Memory (LTSM). Furthermore, sophisticated voice recognition technologies, including Hidden Markov Models (HMMs) and Dynamic Time Warping (DTW) algorithms, play a crucial role in accurate speech-to-textconversion, ensuring robust performance in diverse environments. The methodology incorporates a fusion of content-based and collaborative recommendation approaches, enhancing them with NLP techniques. This innovative integration ensures a more personalized and context-aware recommendation experience, particularly in marketing applications.","['Piyush Talegaonkar', 'Siddhant Hole', 'Shrinesh Kamble', 'Prashil Gulechha', 'Deepali Salapurkar']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.11933,Anomali
Multiplicative and mining property for stability numbers of graphs,"$f$-vertex stability number$vs_f(G)=\min\{|X|: X\subseteq V(G) \enspace \text{and} \enspace f(G-X)\neq f(G)\}$, and $f$-edge stability number is defined similarly by setting $X\subseteq E(G)$. In this paper, for multiplicative andmininginvariant $f$, we give some general bounds for $f$-vertex/edge stability numbers of graphs and some results about the relations between the $f$-vertex/edge stability numbers of graphs and their components.","['Metrose Metsidik', 'Lixiao Xiao']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.11782,Anomali
FALCON: False-Negative Aware Learning of Contrastive Negatives in Vision-Language Pretraining,"False negatives pose a critical challenge in vision-language pretraining (VLP) due to the many-to-many correspondence between images andtextsin large-scale datasets. These false negatives introduce conflicting supervision signals that degrade the learned embedding space and diminish the effectiveness of hard negative sampling. In this paper, we propose FALCON (False-negative Aware Learning of COntrastive Negatives), a learning-based mini-batch construction strategy that adaptively balances the trade-off between hard and false negatives during VLP. Rather than relying on fixed heuristics, FALCON employs a negativeminingscheduler that dynamically selects negative samples of appropriate hardness for each anchor instance during mini-batch construction, guided by a proxy for cross-modal alignment improvement. Experimental results demonstrate that FALCON significantly improves performance across two widely adopted VLP frameworks (ALBEF, BLIP-2) and a broad range of downstream tasks and evaluation settings, underscoring its effectiveness and robustness in mitigating the impact of false negatives.","['Myunsoo Kim', 'Seong-Woong Shim', 'Byung-Jun Lee']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.11192,Anomali
Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning,"Large Language Models (LLMs) have demonstrated significant improvements in reasoning capabilities through supervised fine-tuning and reinforcement learning. However, when training reasoning models, these approaches are primarily applicable to specific domains such as mathematics and programming, which imposes fundamental constraints on the breadth and scalability of training data. In contrast, continual pretraining (CPT) offers the advantage of not requiring task-specific signals. Nevertheless, how to effectively synthesize training data for reasoning and how such data affect a wide range of domains remain largely unexplored. This study provides a detailed evaluation of Reasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden thought processes underlyingtexts, based on the premise thattextsare the result of the author's thinking process. Specifically, we apply Reasoning CPT to Gemma2-9B using synthetic data with hidden thoughts derived from STEM and Law corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis reveals that Reasoning CPT consistently improves performance across all evaluated domains. Notably, reasoning skills acquired in one domain transfer effectively to others; the performance gap with conventional methods widens as problem difficulty increases, with gains of up to 8 points on the most challenging problems. Furthermore, models trained with hidden thoughts learn to adjust the depth of their reasoning according to problem difficulty.","['Yoichi Ishibashi', 'Taro Yano', 'Masafumi Oyamada']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.10182,Anomali
Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach,"Classroom discourse is an essential vehicle through which teaching and learning take place. Assessing different characteristics of discursive practices and linking them to student learning achievement enhances the understanding of teaching quality. Traditional assessments rely on manual coding of classroom observation protocols, which is time-consuming and costly. Despite many studies utilizing AI techniques to analyze classroom discourse at the utterance level, investigations into the evaluation of discursive practices throughout an entire lesson segment remain limited. To address this gap, our study proposes a noveltext-centered multimodal fusion architecture to assess the quality of three discourse components grounded in the Global Teaching InSights (GTI) observation protocol: Nature of Discourse, Questioning, and Explanations. First, we employ attention mechanisms to capture inter- and intra-modal interactions from transcript, audio, and video streams. Second, a multi-task learning approach is adopted to jointly predict the quality scores of the three components. Third, we formulate the task as an ordinal classification problem to account for rating level order. The effectiveness of these designed elements is demonstrated through an ablation study on the GTI Germany dataset containing 92 videotaped math lessons. Our results highlight the dominant role oftextmodality in approaching this task. Integrating acoustic features enhances the model's consistency with human ratings, achieving an overall Quadratic Weighted Kappa score of 0.384, comparable to human inter-rater reliability (0.326). Our study lays the groundwork for the future development of automated discourse quality assessment to support teacher professional development through timely feedback on multidimensional discourse practices.","['Ruikun Hou', 'Babette Bühler', 'Tim Fütterer', 'Efe Bozkir', 'Peter Gerjets', 'Ulrich Trautwein', 'Enkelejda Kasneci']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.07902,Anomali
Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning,"Multimodal news contains a wealth of information and is easily affected by deepfake modeling attacks. To combat the latest image andtextgeneration methods, we present a new Multimodal Fake News Detection dataset (MFND) containing 11 manipulated types, designed to detect and localize highly authentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning (SDML) model for fake news, which fully uses unimodal and mutual modal features tominethe intrinsic semantics of news. Under shallow inference, we propose the momentum distillation-based light punishment contrastive learning for fine-grained uniform spatial image andtextsemantic alignment, and an adaptive cross-modal fusion module to enhance mutual modal features. Under deep inference, we design a two-branch framework to augment the image andtextunimodal features, respectively merging with mutual modalities features, for four predictions via dedicated detection and localization projections. Experiments on both mainstream and our proposed datasets demonstrate the superiority of the model. Codes and dataset are released at https://github.com/yunan-wang33/sdml.","['Ye Zhu', 'Yunan Wang', 'Zitong Yu']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.06796,Anomali
Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining,"The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive SampleMining(PSM), a novel framework thatminespositive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of theirtextqueries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.","['Lu Dong', 'Haiyu Zhang', 'Hongjie Zhang', 'Yifei Huang', 'Zhen-Hua Ling', 'Yu Qiao', 'Limin Wang', 'Yali Wang']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.06557,Anomali
Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI,"The construction of experimental datasets is essential for expanding the scope of data-driven scientific discovery. Recent advances in natural language processing (NLP) have facilitated automatic extraction of structured data from unstructured scientific literature. While existing approaches-multi-step and direct methods-offer valuable capabilities, they also come with limitations when applied independently. Here, we propose a novel hybridtext-miningframework that integrates the advantages of both methods to convert unstructured scientifictextinto structured data. Our approach first transforms rawtextinto entity-recognizedtext, and subsequently into structured form. Furthermore, beyond the overall data structuring framework, we also enhance entity recognition performance by introducing an entity marker-a simple yet effective technique that uses symbolic annotations to highlight target entities. Specifically, our entity marker-based hybrid approach not only consistently outperforms previous entity recognition approaches across three benchmark datasets (MatScholar, SOFC, and SOFC slot NER) but also improve the quality of final structured data-yielding up to a 58% improvement in entity-level F1 score and up to 83% improvement in relation-level F1 score compared to direct approach.","['Junhyeong Lee', 'Jong Min Yuk', 'Chan-Woo Lee']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.05864,Anomali
REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM,"Vision Large Language Models (VLLMs) represent a significant advancement in artificial intelligence by integrating image-processing capabilities with textual understanding, thereby enhancing user interactions and expanding application domains. However, their increased complexity introduces novel safety and ethical challenges, particularly in multi-modal and multi-turn conversations. Traditional safety evaluation frameworks, designed fortext-based, single-turn interactions, are inadequate for addressing these complexities. To bridge this gap, we introduce the REVEAL (Responsible Evaluation of Vision-Enabled AI LLMs) Framework, a scalable and automated pipeline for evaluating image-input harms in VLLMs. REVEAL includes automated imagemining, synthetic adversarial data generation, multi-turn conversational expansion using crescendo attack strategies, and comprehensive harm assessment through evaluators like GPT-4o.
  We extensively evaluated five state-of-the-art VLLMs, GPT-4o, Llama-3.2, Qwen2-VL, Phi3.5V, and Pixtral, across three important harm categories: sexual harm, violence, and misinformation. Our findings reveal that multi-turn interactions result in significantly higher defect rates compared to single-turn evaluations, highlighting deeper vulnerabilities in VLLMs. Notably, GPT-4o demonstrated the most balanced performance as measured by our Safety-Usability Index (SUI) followed closely by Pixtral. Additionally, misinformation emerged as a critical area requiring enhanced contextual defenses. Llama-3.2 exhibited the highest MT defect rate ($16.55 \%$) while Qwen2-VL showed the highest MT refusal rate ($19.1 \%$).","['Madhur Jindal', 'Saurabh Deshpande']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.04673,Anomali
Uncertainty-Aware Prototype Semantic Decoupling for Text-Based Person Search in Full Images,"Text-based pedestrian search (TBPS) in full images aims to locate a target pedestrian in untrimmed images using natural language descriptions. However, in complex scenes with multiple pedestrians, existing methods are limited by uncertainties in detection and matching, leading to degraded performance. To address this, we propose UPD-TBPS, a novel framework comprising three modules: Multi-granularity Uncertainty Estimation (MUE), Prototype-based Uncertainty Decoupling (PUD), and Cross-modal Re-identification (ReID). MUE conducts multi-granularity queries to identify potential targets and assigns confidence scores to reduce early-stage uncertainty. PUD leverages visual context decoupling and prototypeminingto extract features of the target pedestrian described in the query. It separates and learns pedestrian prototype representations at both the coarse-grained cluster level and the fine-grained individual level, thereby reducing matching uncertainty. ReID evaluates candidates with varying confidence levels, improving detection and retrieval accuracy. Experiments on CUHK-SYSU-TBPS and PRW-TBPS datasets validate the effectiveness of our framework.","['Zengli Luo', 'Canlong Zhang', 'Xiaochun Lu', 'Zhixin Li', 'Zhiwen Wang']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.03567,Anomali
Robust Fairness Vision-Language Learning for Medical Image Analysis,"The advent of Vision-Language Models (VLMs) in medical image analysis has the potential to help process multimodal inputs and increase performance over traditional inference methods. However, when considering the domain in which these models will be implemented, fairness and robustness are important to ensure the model stays true for any patient. In this paper, we introduce a framework for ensuring robustness and fairness of VLM models. This framework modifies the loss function at training by identifying and adjusting faulty image-textpairs through a Dynamic Bad PairMiningalgorithm and also utilizing Sinkhorn distance to ensure the loss distributions of protected groups do not deviate from the total loss. Experimental testing of our framework shows up to a 8.6\% improvement when looking at equity-scaled AUC.","['Sparsh Bansal', 'Mingyang Wu', 'Xin Wang', 'Shu Hu']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.03153,Anomali
Exploring new Approaches for Information Retrieval through Natural Language Processing,"This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficienttextindexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argumentmining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.","['Manak Raj', 'Nidhi Mishra']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.02199,Anomali
TeMTG: Text-Enhanced Multi-Hop Temporal Graph Modeling for Audio-Visual Video Parsing,"Audio-Visual Video Parsing (AVVP) task aims to parse the event categories and occurrence times from audio and visual modalities in a given video. Existing methods usually focus on implicitly modeling audio and visual features through weak labels, withoutminingsemantic relationships for different modalities and explicit modeling of event temporal dependencies. This makes it difficult for the model to accurately parse event information for each segment under weak supervision, especially when high similarity between segmental modal features leads to ambiguous event boundaries. Hence, we propose a multimodal optimization framework, TeMTG, that combinestextenhancement and multi-hop temporal graph modeling. Specifically, we leverage pre-trained multimodal models to generate modality-specifictextembeddings, and fuse them with audio-visual features to enhance the semantic representation of these features. In addition, we introduce a multi-hop temporal graph neural network, which explicitly models the local temporal relationships between segments, capturing the temporal continuity of both short-term and long-range events. Experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance in multiple key indicators in the LLP dataset.","['Yaru Chen', 'Peiliang Zhang', 'Fei Li', 'Faegheh Sardari', 'Ruohao Guo', 'Zhenbo Li', 'Wenwu Wang']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.02096,Anomali
Multi-agents based User Values Mining for Recommendation,"Recommender systems have rapidly evolved and become integral to many online services. However, existing systems sometimes produce unstable and unsatisfactory recommendations that fail to align with users' fundamental and long-term preferences. This is because they primarily focus on extracting shallow and short-term interests from user behavior data, which is inherently dynamic and challenging to model. Unlike these transient interests, user values are more stable and play a crucial role in shaping user behaviors, such as purchasing items and consuming content. Incorporating user values into recommender systems can help stabilize recommendation performance and ensure results better reflect users' latent preferences. However, acquiring user values is typically difficult and costly. To address this challenge, we leverage the strong language understanding, zero-shot inference, and generalization capabilities of Large Language Models (LLMs) to extract user values from users' historical interactions. Unfortunately, direct extraction using LLMs presents several challenges such as length constraints and hallucination. To overcome these issues, we propose ZOOM, a zero-shot multi-LLM collaborative framework for effective and accurate user value extraction. In ZOOM, we applytextsummarization techniques to condense item content while preserving essential meaning. To mitigate hallucinations, ZOOM introduces two specialized agent roles: evaluators and supervisors, to collaboratively generate accurate user values. Extensive experiments on two widely used recommendation datasets with two state-of-the-art recommendation models demonstrate the effectiveness and generalization of our framework in automatic user valueminingand recommendation performance improvement.","['Lijian Chen', 'Wei Yuan', 'Tong Chen', 'Xiangyu Zhao', 'Nguyen Quoc Viet Hung', 'Hongzhi Yin']",,arXiv,2025,https://doi.org/10.48550/arXiv.2505.00981,Anomali
"Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval","Training effective dense retrieval models often relies on hard negative (HN) examplesminedfrom the document corpus via methods like BM25 or cross-encoders (CE), processes that can be computationally demanding and require full corpus access. This paper introduces a different approach, an end-to-end pipeline where a Large Language Model (LLM) first generates a query from a passage, and then generates a hard negative example using \emph{only} that querytext. This corpus-free negative generation contrasts with standardminingtechniques. We evaluated this \textsc{LLM Query $\rightarrow$ LLM HN} approach against traditional \textsc{LLM Query $\rightarrow$ BM25 HN} and \textsc{LLM Query $\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several BEIR benchmark datasets. Our results show the proposed all-LLM pipeline achieves performance identical to both the BM25 and the computationally intensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics. This demonstrates that our corpus-free negative generation method matches the effectiveness of complex, corpus-dependentminingtechniques, offering a potentially simpler and more efficient pathway for training high-performance retrievers without sacrificing results. We make the dataset including the queries and the hard-negatives for all three methods publicly available https://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449.",['Aarush Sinha'],,arXiv,2025,https://doi.org/10.48550/arXiv.2504.21015,Anomali
Exploiting Inter-Sample Correlation and Intra-Sample Redundancy for Partially Relevant Video Retrieval,"Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video that is partially relevant to thetextquery. The primary challenge in PRVR arises from the semantic asymmetry between textual and visual modalities, as videos often contain substantial content irrelevant to the query. Existing methods coarsely align paired videos andtextqueries to construct the semantic space, neglecting the critical cross-modal dual nature inherent in this task: inter-sample correlation and intra-sample redundancy. To this end, we propose a novel PRVR framework to systematically exploit these two characteristics. Our framework consists of three core modules. First, the Inter Correlation Enhancement (ICE) module captures inter-sample correlation by identifying semantically similar yet unpairedtextqueries and video moments, combining them to form pseudo-positive pairs for more robust semantic space construction. Second, the Intra RedundancyMining(IRM) module mitigates intra-sample redundancy byminingredundant video moment features and treating them as hard negative samples, thereby encouraging the model to learn more discriminative representations. Finally, to reinforce these modules, we introduce the Temporal Coherence Prediction (TCP) module, which enhances feature discrimination by training the model to predict the original temporal order of randomly shuffled video frames and moments. Extensive experiments on three datasets demonstrate the superiority of our approach compared to previous methods, achieving state-of-the-art results.","['Junlong Ren', 'Gangjian Zhang', 'Yu Hu', 'Jian Shu', 'Hao Wang']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.19637,Anomali
Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents,"Theminingof synthesis conditions for metal-organic frameworks (MOFs) is a significant focus in materials science. However, identifying the precise synthesis conditions for specific MOFs within the vast array of possibilities presents a considerable challenge. Large Language Models (LLMs) offer a promising solution to this problem. We leveraged the capabilities of LLMs, specifically gpt-4o-mini, as core agents to integrate various MOF-related agents, including synthesis, attribute, and chemical information agents. This integration culminated in the development of MOFh6, an LLM tool designed to streamline the MOF synthesis process. MOFh6 allows users to query in multiple formats, such as submitting scientific literature, or inquiring about specific MOF codes or structural properties. The tool analyzes these queries to provide optimal synthesis conditions and generates model files for density functional theory pre modeling. We believe MOFh6 will enhance efficiency in the MOF synthesis of all researchers.","['Zuhong Lin', 'Daoyuan Ren', 'Kai Ran', 'Sun Jing', 'Xiaotiang Huang', 'Haiyang He', 'Pengxu Pan', 'Xiaohang Zhang', 'Ying Fang', 'Tianying Wang', 'Minli Wu', 'Zhanglin Li', 'Xiaochuan Zhang', 'Haipu Li', 'Jingjing Yao']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.18880,Anomali
300 μs optical cavity storage time and $\mathbf{10^{-7}}$ active RAM cancellation for $\mathbf{10^{-19}}$ laser frequency stabilisation,"Frequency stabilisation of lasers to optical reference cavities is an established method to achieve state-of-the-art stability. The strengths of this method are the high discriminator coefficient of optical cavities, and the low-noise extraction of the stabilisation signal using modulation techniques. In this Letter we report beyond state-of-the-art performance on both of these fundamentals, unlocking $10^{-19}$ fractional frequency laser stabilisation. We employ a 68 cm long cavity to realise an optical storage time of 300 microseconds, achieving ultrahigh frequency discrimination. We develop a simple and robust scheme to actively cancel residual amplitude modulation (RAM) at the $10^{-7}$ level in an annealed-proton-exchanged lithium-niobate waveguide electro-optic-modulator (EOM).","['Adam L. Parke', 'Marco Schioppo']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.16731,Anomali
Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models,"Diagnostic imaging relies on interpreting both images and radiology reports, but the growing data volumes place significant pressure on medical experts, yielding increased errors and workflow backlogs. Medical vision-language models (med-VLMs) have emerged as a powerful framework to efficiently process multimodal imaging data, particularly in chest X-ray (CXR) evaluations, albeit their performance hinges on how well image andtextrepresentations are aligned. Existing alignment methods, predominantly based on contrastive learning, prioritize separation between disease classes over segregation of fine-grained pathology attributes like location, size or severity, leading to suboptimal representations. Here, we propose MedTrim (Meta-entity-driven Tripletmining), a novel method that enhances image-textalignment through multimodal triplet learning synergistically guided by disease class as well as adjectival and directional pathology descriptors. Unlike common alignment methods that separate broad disease classes, MedTrim leverages structured meta-entity information to preserve subtle but clinically significant intra-class variations. For this purpose, we first introduce an ontology-based entity recognition module that extracts pathology-specific meta-entities from CXR reports, as annotations on pathology attributes are rare in public datasets. For refined sample selection in tripletmining, we then introduce a novel score function that captures an aggregate measure of inter-sample similarity based on disease classes and adjectival/directional descriptors. Lastly, we introduce a multimodal triplet alignment objective for explicit within- and cross-modal alignment between samples sharing detailed pathology characteristics. Our demonstrations indicate that MedTrim improves performance in downstream retrieval and classification tasks compared to state-of-the-art alignment methods.","['Saban Ozturk', 'Melih B. Yilmaz', 'Muti Kara', 'M. Talat Yavuz', 'Aykut Koç', 'Tolga Çukur']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.15929,Anomali
What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns,"Prompt engineering for large language models is challenging, as even small prompt perturbations or model changes can significantly impact the generated outputtexts. Existing evaluation methods, either automated metrics or human evaluation, have limitations, such as providing limited insights or being labor-intensive. We propose Spotlight, a new approach that combines both automation and human analysis. Based on dataminingtechniques, we automatically distinguish between random (decoding) variations and systematic differences in language model outputs. This process provides token patterns that describe the systematic differences and guide the user in manually analyzing the effects of their prompt and model changes efficiently. We create three benchmarks to quantitatively test the reliability of token pattern extraction methods and demonstrate that our approach provides new insights into established prompt data. From a human-centric perspective, through demonstration studies and a user study, we show that our token pattern approach helps users understand the systematic differences of language model outputs, and we are able to discover relevant differences caused by prompt and model changes (e.g. related to gender or culture), thus supporting the prompt engineering process and human-centric model behavior research.","['Michael A. Hedderich', 'Anyi Wang', 'Raoyuan Zhao', 'Florian Eichin', 'Barbara Plank']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.15815,Anomali
Teach Me How to Denoise: A Universal Framework for Denoising Multi-modal Recommender Systems via Guided Calibration,"The surge in multimedia content has led to the development of Multi-Modal Recommender Systems (MMRecs), which use diverse modalities such astext, images, videos, and audio for more personalized recommendations. However, MMRecs struggle with noisy data caused by misalignment among modal content and the gap between modal semantics and recommendation semantics. Traditional denoising methods are inadequate due to the complexity of multi-modal data. To address this, we propose a universal guided in-sync distillation denoising framework for multi-modal recommendation (GUIDER), designed to improve MMRecs by denoising user feedback. Specifically, GUIDER uses a re-calibration strategy to identify clean and noisy interactions from modal content. It incorporates a Denoising Bayesian Personalized Ranking (DBPR) loss function to handle implicit user feedback. Finally, it applies a denoising knowledge distillation objective based on Optimal Transport distance to guide the alignment from modality representations to recommendation semantics. GUIDER can be seamlessly integrated into existing MMRecs methods as a plug-and-play solution. Experimental results on four public datasets demonstrate its effectiveness and generalizability. Our source code is available at https://github.com/Neon-Jing/Guider","['Hongji Li', 'Hanwen Du', 'Youhua Li', 'Junchen Fu', 'Chunxiao Li', 'Ziyi Zhuang', 'Jiakang Li', 'Yongxin Ni']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.14214,Anomali
A Survey on Cross-Modal Interaction Between Music and Multimodal Data,"Multimodal learning has driven innovation across various industries, particularly in the field of music. By enabling more intuitive interaction experiences and enhancing immersion, it not only lowers the entry barriers to the music but also increases its overall appeal. This survey aims to provide a comprehensive review of multimodal tasks related to music, outlining how music contributes to multimodal learning and offering insights for researchers seeking to expand the boundaries of computational music. Unliketextand images, which are often semantically or visually intuitive, music primarily interacts with humans through auditory perception, making its data representation inherently less intuitive. Therefore, this paper first introduces the representations of music and provides an overview of music datasets. Subsequently, we categorize cross-modal interactions between music and multimodal data into three types: music-driven cross-modal interactions, music-oriented cross-modal interactions, and bidirectional music cross-modal interactions. For each category, we systematically trace the development of relevant sub-tasks, analyze existing limitations, and discuss emerging trends. Furthermore, we provide a comprehensive summary of datasets and evaluation metrics used in multimodal tasks related to music, offering benchmark references for future research. Finally, we discuss the current challenges in cross-modal interactions involving music and propose potential directions for future research.","['Sifei Li', 'Mining Tan', 'Feier Shen', 'Minyan Luo', 'Zijiao Yin', 'Fan Tang', 'Weiming Dong', 'Changsheng Xu']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.12796,Anomali
QualiTagger: Automating software quality detection in issue trackers,"A systems quality is a major concern for development teams when it evolve. Understanding the effects of a loss of quality in the codebase is crucial to avoid side effects like the appearance of technical debt. Although the identification of these qualities in software requirements described in natural language has been investigated, most of the results are often not applicable in practice, and rely on having been validated on small datasets and limited amount of projects. For many years, machine learning (ML) techniques have been proved as a valid technique to identify and tag terms described in natural language. In order to advance previous works, in this research we use cutting edge models like Transformers, together with a vast datasetminedand curated from GitHub, to identify whattextis usually associated with different quality properties. We also study the distribution of such qualities in issue trackers from openly accessible software repositories, and we evaluate our approach both with students from a software engineering course and with its application to recognize security labels in industry.","['Karthik Shivashankar', 'Rafael Capilla', 'Maren Maritsdatter Kruke', 'Mili Orucevic', 'Antonio Martini']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.11053,Anomali
UP-Person: Unified Parameter-Efficient Transfer Learning for Text-based Person Retrieval,"Text-based Person Retrieval (TPR) as a multi-modal task, which aims to retrieve the target person from a pool of candidate images given atextdescription, has recently garnered considerable attention due to the progress of contrastive visual-language pre-trained model. Prior works leverage pre-trained CLIP to extract person visual and textual features and fully fine-tune the entire network, which have shown notable performance improvements compared to uni-modal pre-training models. However, full-tuning a large model is prone to overfitting and hinders the generalization ability. In this paper, we propose a novel Unified Parameter-Efficient Transfer Learning (PETL) method forText-based Person Retrieval (UP-Person) to thoroughly transfer the multi-modal knowledge from CLIP. Specifically, UP-Person simultaneously integrates three lightweight PETL components including Prefix, LoRA and Adapter, where Prefix and LoRA are devised together tominelocal information with task-specific information prompts, and Adapter is designed to adjust global feature representations. Additionally, two vanilla submodules are optimized to adapt to the unified architecture of TPR. For one thing, S-Prefix is proposed to boost attention of prefix and enhance the gradient propagation of prefix tokens, which improves the flexibility and performance of the vanilla prefix. For another thing, L-Adapter is designed in parallel with layer normalization to adjust the overall distribution, which can resolve conflicts caused by overlap and interaction among multiple submodules. Extensive experimental results demonstrate that our UP-Person achieves state-of-the-art results across various person retrieval datasets, including CUHK-PEDES, ICFG-PEDES and RSTPReid while merely fine-tuning 4.7\% parameters. Code is available at https://github.com/Liu-Yating/UP-Person.","['Yating Liu', 'Yaowei Li', 'Xiangyuan Lan', 'Wenming Yang', 'Zimo Liu', 'Qingmin Liao']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.10084,Anomali
Improving Multimodal Hateful Meme Detection Exploiting LMM-Generated Knowledge,"Memes have become a dominant form of communication in social media in recent years. Memes are typically humorous and harmless, however there are also memes that promote hate speech, being in this way harmful to individuals and groups based on their identity. Therefore, detecting hateful content in memes has emerged as a task of critical importance. The need for understanding the complex interactions of images and their embeddedtextrenders the hateful meme detection a challenging multimodal task. In this paper we propose to address the aforementioned task leveraging knowledge encoded in powerful Large Multimodal Models (LMM). Specifically, we propose to exploit LMMs in a two-fold manner. First, by extracting knowledge oriented to the hateful meme detection task in order to build strong meme representations. Specifically, generic semantic descriptions and emotions that the images along with their embeddedtextselicit are extracted, which are then used to train a simple classification head for hateful meme detection. Second, by developing a novel hardminingapproach introducing directly LMM-encoded knowledge to the training process, providing further improvements. We perform extensive experiments on two datasets that validate the effectiveness of the proposed method, achieving state-of-the-art performance. Our code and trained models are publicly available at: https://github.com/IDT-ITI/LMM-CLIP-meme.","['Maria Tzelepi', 'Vasileios Mezaris']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.09914,Anomali
RAKG:Document-level Retrieval Augmented Knowledge Graph Construction,"With the rise of knowledge graph based retrieval-augmented generation (RAG) techniques such as GraphRAG and Pike-RAG, the role of knowledge graphs in enhancing the reasoning capabilities of large language models (LLMs) has become increasingly prominent. However, traditional Knowledge Graph Construction (KGC) methods face challenges like complex entity disambiguation, rigid schema definition, and insufficient cross-document knowledge integration. This paper focuses on the task of automatic document-level knowledge graph construction. It proposes the Document-level Retrieval Augmented Knowledge Graph Construction (RAKG) framework. RAKG extracts pre-entities fromtextchunks and utilizes these pre-entities as queries for RAG, effectively addressing the issue of long-context forgetting in LLMs and reducing the complexity of Coreference Resolution. In contrast to conventional KGC methods, RAKG more effectively captures global information and the interconnections among disparate nodes, thereby enhancing the overall performance of the model. Additionally, we transfer the RAG evaluation framework to the KGC field and filter and evaluate the generated knowledge graphs, thereby avoiding incorrectly generated entities and relationships caused by hallucinations in LLMs. We further developed theMINEdataset by constructing standard knowledge graphs for each article and experimentally validated the performance of RAKG. The results show that RAKG achieves an accuracy of 95.91 % on theMINEdataset, a 6.2 % point improvement over the current best baseline, GraphRAG (89.71 %). The code is available at https://github.com/LMMApplication/RAKG.","['Hairong Zhang', 'Jiaheng Si', 'Guohang Yan', 'Boyuan Qi', 'Pinlong Cai', 'Song Mao', 'Ding Wang', 'Botian Shi']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.09823,Anomali
Topic mining based on fine-tuning Sentence-BERT and LDA,"Research background: With the continuous development of society, consumers pay more attention to the key information of product fine-grained attributes when shopping. Research purposes: This study will fine tune the Sentence-BERT word embedding model and LDA model,minethe subject characteristics in online reviews of goods, and show consumers the details of various aspects of goods. Research methods: First, the Sentence-BERT model was fine tuned in the field of e-commerce online reviews, and the online reviewtextwas converted into a word vector set with richer semantic information; Secondly, the vectorized word set is input into the LDA model for topic feature extraction; Finally, focus on the key functions of the product through keyword analysis under the theme. Results: This study compared this model with other word embedding models and LDA models, and compared it with common topic extraction methods. The theme consistency of this model is 0.5 higher than that of other models, which improves the accuracy of theme extraction","['Jianheng Li', 'Lirong Chen']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.07984,Anomali
Geological Inference from Textual Data using Word Embeddings,"This research explores the use of Natural Language Processing (NLP) techniques to locate geological resources, with a specific focus on industrial minerals. By using word embeddings trained with the GloVe model, we extract semantic relationships between target keywords and a corpus of geologicaltexts. Thetextis filtered to retain only words with geographical significance, such as city names, which are then ranked by their cosine similarity to the target keyword. Dimensional reduction techniques, including Principal Component Analysis (PCA), Autoencoder, Variational Autoencoder (VAE), and VAE with Long Short-Term Memory (VAE-LSTM), are applied to enhance feature extraction and improve the accuracy of semantic relations.
  For benchmarking, we calculate the proximity between the ten cities most semantically related to the target keyword and identifiedminelocations using the haversine equation. The results demonstrate that combining NLP with dimensional reduction techniques provides meaningful insights into the spatial distribution of natural resources. Although the result shows to be in the same region as the supposed location, the accuracy has room for improvement.","['Nanmanas Linphrachaya', 'Irving Gómez-Méndez', 'Adil Siripatana']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.07490,Anomali
Releasing Differentially Private Event Logs Using Generative Models,"In recent years, the industry has been witnessing an extended usage of processminingand automated event data analysis. Consequently, there is a rising significance in addressing privacy apprehensions related to the inclusion of sensitive and private information within event data utilized by processminingalgorithms. State-of-the-art research mainly focuses on providing quantifiable privacy guarantees, e.g., via differential privacy, for trace variants that are used by the main processminingtechniques, e.g., process discovery. However, privacy preservation techniques designed for the release of trace variants are still insufficient to meet all the demands of industry-scale utilization. Moreover, ensuring privacy guarantees in situations characterized by a high occurrence of infrequent trace variants remains a challenging endeavor. In this paper, we introduce two novel approaches for releasing differentially private trace variants based on trained generative models. With TraVaG, we leverage \textit{Generative Adversarial Networks} (GANs) to sample from a privatized implicit variant distribution. Our second method employs \textit{Denoising Diffusion Probabilistic Models} that reconstruct artificial trace variants from noise via trained Markov chains. Both methods offer industry-scale benefits and elevate the degree of privacy assurances, particularly in scenarios featuring a substantial prevalence of infrequent variants. Also, they overcome the shortcomings of conventional privacy preservation techniques, such as bounding the length of variants and introducing fake variants. Experimental results on real-life event data demonstrate that our approaches surpass state-of-the-art techniques in terms of privacy guarantees and utility preservation.","['Frederik Wangelik', 'Majid Rafiei', 'Mahsa Pourbafrani', 'Wil M. P. van der Aalst']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.06418,Anomali
Indexing Strings with Utilities,"Applications in domains ranging from bioinformatics to advertising feature strings that come with numerical scores (utilities). The utilities quantify the importance, interest, profit, or risk of the letters occurring at every position of a string. Motivated by the ever-increasing rate of generating such data, as well as by their importance in several domains, we introduce Useful String Indexing (USI), a natural generalization of the classic String Indexing problem. Given a string $S$ (thetext) of length $n$, USI asks for preprocessing $S$ into a compact data structure supporting the following queries efficiently: given a shorter string $P$ (the pattern), return the global utility $U(P)$ of $P$ in $S$, where $U$ is a function that maps any string $P$ to a utility score based on the utilities of the letters of every occurrence of $P$ in $S$. Our work also makes the following contributions: (1) We propose a novel and efficient data structure for USI based on finding the top-$K$ frequent substrings of $S$. (2) We propose a linear-space data structure that can be used tominethe top-$K$ frequent substrings of $S$ or to tune the parameters of the USI data structure. (3) We propose a novel space-efficient algorithm for estimating the set of the top-$K$ frequent substrings of $S$, thus improving the construction space of the data structure for USI. (4) We show that popular space-efficient top-$K$ frequent itemminingstrategies employed by state-of-the-art algorithms do not smoothly translate from items to substrings. (5) Using billion-letter datasets, we experimentally demonstrate that: (i) our top-$K$ frequent substringminingalgorithms are accurate and scalable, unlike two state-of-the-art methods; and (ii) our USI data structures are up to $15$ times faster in querying than $4$ nontrivial baselines while occupying the same space with them.","['Giulia Bernardini', 'Huiping Chen', 'Alessio Conte', 'Roberto Grossi', 'Veronica Guerrini', 'Grigorios Loukides', 'Nadia Pisanti', 'and Solon P. Pissis']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.05917,Anomali
Constructing the Truth: Text Mining and Linguistic Networks in Public Hearings of Case 03 of the Special Jurisdiction for Peace (JEP),"Case 03 of the Special Jurisdiction for Peace (JEP), focused on the so-called false positives in Colombia, represents one of the most harrowing episodes of the Colombian armed conflict. This article proposes an innovative methodology based on natural language analysis and semantic co-occurrence models to explore, systematize, and visualize narrative patterns present in the public hearings of victims and appearing parties. By constructing skipgram networks and analyzing their modularity, the study identifies thematic clusters that reveal regional and procedural status differences, providing empirical evidence on dynamics of victimization, responsibility, and acknowledgment in this case. This computational approach contributes to the collective construction of both judicial and extrajudicial truth, offering replicable tools for other transitional justice cases. The work is grounded in the pillars of truth, justice, reparation, and non-repetition, proposing a critical and in-depth reading of contested memories.","['Juan Sosa', 'Alejandro Urrego-López', 'Cesar Prieto', 'Emma J. Camargo-Díaz']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.04325,Anomali
Mapping Technological Futures: Anticipatory Discourse Through Text Mining,"The volatility and unpredictability of emerging technologies, such as artificial intelligence (AI), generate significant uncertainty, which is widely discussed on social media. This study examines anticipatory discourse surrounding technological futures by analysing 1.5 million posts from 400 key opinion leaders (KOLs) published on the X platform (from 2021 to 2023). Using advancedtextminingtechniques, including BERTopic modelling, sentiment, emotion, and attitude analyses, the research identifies 100 distinct topics reflecting anticipated tech-driven futures. Our findings emphasize the dual role of KOLs in framing \textit{present futures} -- optimistic visions of transformative technologies like AI and IoT -- and influencing \textit{future presents}, where these projections shape contemporary societal and geopolitical debates. Positive emotions such as Hope dominate, outweighing Anxiety, particularly in topics like ``Machine Learning, Data Science, and Deep Learning,'' while discussions around ``Climate Change'' and ``War, Ukraine, and Trump People'' elicit \textit{Anxiety}. By framing technologies as solutions to societal challenges, KOLs act as mediators of societal narratives, bridging imagined futures and current realities. These insights underscore their pivotal role in directing public attention with emerging technologies during periods of heightened uncertainty, advancing our understanding of anticipatory discourse in technology-mediated contexts.","['Maciej Skorski', 'Alina Landowska', 'Krzysztof Rajda']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.02853,Anomali
SelfMedHPM: Self Pre-training With Hard Patches Mining Masked Autoencoders For Medical Image Segmentation,"In recent years, deep learning methods such as convolutional neural network (CNN) and transformers have made significant progress in CT multi-organ segmentation. However, CT multi-organ segmentation methods based on masked image modeling (MIM) are very limited. There are already methods using MAE for CT multi-organ segmentation task, we believe that the existing methods do not identify the most difficult areas to reconstruct. To this end, we propose a MIM self-training framework with hard patchesminingmasked autoencoders for CT multi-organ segmentation tasks (selfMedHPM). The method performs ViT self-pretraining on the training set of the target data and introduces an auxiliary loss predictor, which first predicts the patch loss and determines the location of the next mask. SelfMedHPM implementation is better than various competitive methods in abdominal CT multi-organ segmentation and body CT multi-organ segmentation. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for abdomen mult-organ segmentation and the SinoMed Whole Body (SMWB) dataset for body multi-organ segmentation tasks.","['Yunhao Lv', 'Lingyu Chen', 'Jian Wang', 'Yangxi Li', 'Fang Chen']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.02524,Anomali
"Design and Implementation of the Transparent, Interpretable, and Multimodal (TIM) AR Personal Assistant","The concept of an AI assistant for task guidance is rapidly shifting from a science fiction staple to an impending reality. Such a system is inherently complex, requiring models for perceptual grounding, attention, and reasoning, an intuitive interface that adapts to the performer's needs, and the orchestration of data streams from many sensors. Moreover, all data acquired by the system must be readily available for post-hoc analysis to enable developers to understand performer behavior and quickly detect failures. We introduce TIM, the first end-to-end AI-enabled task guidance system in augmented reality which is capable of detecting both the user and scene as well as providing adaptable, just-in-time feedback. We discuss the system challenges and propose design solutions. We also demonstrate how TIM adapts to domain applications with varying needs, highlighting how the system components can be customized for each scenario.","['Erin McGowan', 'Joao Rulff', 'Sonia Castelo', 'Guande Wu', 'Shaoyu Chen', 'Roque Lopez', 'Bea Steers', 'Iran R. Roman', 'Fabio F. Dias', 'Jing Qian', 'Parikshit Solunke', 'Michael Middleton', 'Ryan McKendrick', 'Claudio T. Silva']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.02197,Anomali
LLMs4SchemaDiscovery: A Human-in-the-Loop Workflow for Scientific Schema Mining with Large Language Models,"Extracting structured information from unstructuredtextis crucial for modeling real-world processes, but traditional schemaminingrelies on semi-structured data, limiting scalability. This paper introduces schema-miner, a novel tool that combines large language models with human feedback to automate and refine schema extraction. Through an iterative workflow, it organizes properties fromtext, incorporates expert input, and integrates domain-specific ontologies for semantic depth. Applied to materials science--specifically atomic layer deposition--schema-miner demonstrates that expert-guided LLMs generate semantically rich schemas suitable for diverse real-world applications.","['Sameer Sadruddin', ""Jennifer D'Souza"", 'Eleni Poupaki', 'Alex Watkins', 'Hamed Babaei Giglou', 'Anisa Rula', 'Bora Karasulu', 'Sören Auer', 'Adrie Mackus', 'Erwin Kessels']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.00752,Anomali
IHC-LLMiner: Automated extraction of tumour immunohistochemical profiles from PubMed abstracts using large language models,"Immunohistochemistry (IHC) is essential in diagnostic pathology and biomedical research, offering critical insights into protein expression and tumour biology. This study presents an automated pipeline, IHC-LLMiner, for extracting IHC-tumour profiles from PubMed abstracts, leveraging advanced biomedicaltextmining. There are two subtasks: abstract classification (include/exclude as relevant) and IHC-tumour profile extraction on relevant included abstracts. The best-performing model, ""Gemma-2 finetuned"", achieved 91.5% accuracy and an F1 score of 91.4, outperforming GPT4-O by 9.5% accuracy with 5.9 times faster inference time. From an initial dataset of 107,759 abstracts identified for 50 immunohistochemical markers, the classification task identified 30,481 relevant abstracts (Include) using the Gemma-2 finetuned model. For IHC-tumour profile extraction, the Gemma-2 finetuned model achieved the best performance with 63.3% Correct outputs. Extracted IHC-tumour profiles (tumour types and markers) were normalised to Unified Medical Language System (UMLS) concepts to ensure consistency and facilitate IHC-tumour profile landscape analysis. The extracted IHC-tumour profiles demonstrated excellent concordance with available online summary data and provided considerable added value in terms of both missing IHC-tumour profiles and quantitative assessments. Our proposed LLM based pipeline provides a practical solution for large-scale IHC-tumour profile datamining, enhancing the accessibility and utility of such data for research and clinical applications as well as enabling the generation of quantitative and structured data to support cancer-specific knowledge base development. Models and training datasets are available at https://github.com/knowlab/IHC-LLMiner.","['Yunsoo Kim', 'Michal W. S. Ong', 'Daniel W. Rogalsky', 'Manuel Rodriguez-Justo', 'Honghan Wu', 'Adam P. Levine']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.00748,Anomali
Multilingual Sentiment Analysis of Summarized Texts: A Cross-Language Study of Text Shortening Effects,"Summarization significantly impacts sentiment analysis across languages with diverse morphologies. This study examines extractive and abstractive summarization effects on sentiment classification in English, German, French, Spanish, Italian, Finnish, Hungarian, and Arabic. We assess sentiment shifts post-summarization using multilingual transformers (mBERT, XLM-RoBERTa, T5, and BART) and language-specific models (FinBERT, AraBERT). Results show extractive summarization better preserves sentiment, especially in morphologically complex languages, while abstractive summarization improves readability but introduces sentiment distortion, affecting sentiment accuracy. Languages with rich inflectional morphology, such as Finnish, Hungarian, and Arabic, experience greater accuracy drops than English or German. Findings emphasize the need for language-specific adaptations in sentiment analysis and propose a hybrid summarization approach balancing readability and sentiment preservation. These insights benefit multilingual sentiment applications, including social media monitoring, market analysis, and cross-lingual opinionmining.","['Mikhail Krasitskii', 'Grigori Sidorov', 'Olga Kolesnikova', 'Liliana Chanona Hernandez', 'Alexander Gelbukh']",,arXiv,2025,https://doi.org/10.48550/arXiv.2504.00265,Anomali
Four Things People Should Know About Migraines,"Migraine literacy among the public is known to be low, and this lack of understanding has a negative impact on migraineurs' quality of life. To understand this impact, we usetextminingmethods to study migraine discussion on the Reddit social media platform. We summarize the findings in the form of ""four things people should know about chronic migraines"": it is a serious disease that affects people of all ages, it can be triggered by many different factors, it affects women more than men, and it can get worse in combination with the COVID-19 virus.","['Mohammad S. Parsa', 'Lukasz Golab']",The 8th International Conference on Health Informatics & Medical Systems (HIMS'2022),arXiv,2025,https://doi.org/10.48550/arXiv.2504.00011,Anomali
CrossMuSim: A Cross-Modal Framework for Music Similarity Retrieval with LLM-Powered Text Description Sourcing and Mining,"Music similarity retrieval is fundamental for managing and exploring relevant content from large collections in streaming platforms. This paper presents a novel cross-modal contrastive learning framework that leverages the open-ended nature oftextdescriptions to guide music similarity modeling, addressing the limitations of traditional uni-modal approaches in capturing complex musical relationships. To overcome the scarcity of high-qualitytext-music paired data, this paper introduces a dual-source data acquisition approach combining online scraping and LLM-based prompting, where carefully designed prompts leverage LLMs' comprehensive music knowledge to generate contextually rich descriptions. Exten1sive experiments demonstrate that the proposed framework achieves significant performance improvements over existing benchmarks through objective metrics, subjective evaluations, and real-world A/B testing on the Huawei Music streaming platform.","['Tristan Tsoi', 'Jiajun Deng', 'Yaolong Ju', 'Benno Weck', 'Holger Kirchhoff', 'Simon Lui']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.23128,Anomali
DemoQuanDT: A Carrier-Grade QKD Network,"Quantum Key Distribution Networks (QKDN) enable secure communication even in the age of powerful quantum computers. In the hands of a network operator, which can offer its service to many users, the economic viability of a QKDN increases significantly. The highly challenging operator-user relationship in a large-scale network setting demands additional requirements to ensure carrier-grade operation. Addressing this challenge, this work presents a carrier-grade QKDN architecture, which combines the functional QKDN architecture with the operational perspective of a network operator, ultimately enhancing the economic viability of QKDN. The focus is on the network and key management aspects of a QKDN while assuming state-of-the-art commercial QKD-Modules. The presented architecture was rolled out within an in-field demonstrator, connecting the cities of Berlin and Bonn over a link distance of 923 km across Germany. We could show, that the proposed network architecture is feasible, integrable, and scalable making it suitable for deployment in real-world networks. Overall, the presented carrier-grade QKDN architecture promises to serve as a blueprint for network operators providing QKD-based services to their customers.","['P. Horoschenkoff', 'J. Henrich', 'R. Böhn', 'I. Khan', 'J. Rödiger', 'M. Gunkel', 'M. Bauch', 'J. Benda', 'P. Bläcker', 'E. Eichhammer', 'U. Eismann', 'G. Frenck', 'H. Griesser', 'W. Jontofsohn', 'N. Kopshoff', 'S. Röhrich', 'F. Seidl', 'N. Schark', 'E. Sollner', 'D. von Blanckenburg', 'A. Heinemann', 'M. Stiemerling', 'M. Gärtner']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.21186,Anomali
Solving the Correlation Cluster LP in Sublinear Time,"Correlation Clustering is a fundamental and widely-studied problem in unsupervised learning and datamining. The input is a graph and the goal is to construct a clustering minimizing the number of inter-cluster edges plus the number of missing intra-cluster edges.
  CCL+24 introduced the cluster LP for Correlation Clustering, which they argued captures the problem much more succinctly than previous linear programming formulations. However, the cluster LP has exponential size, with a variable for every possible set of vertices in the input graph. Nevertheless, CCL+24 showed how to find a feasible solution for the cluster LP in time$O(n^{\text{poly}(1/\eps)})$with objective value at most $(1+ε)$ times the value of an optimal solution for the respective Correlation Clustering instance. Furthermore, they showed how to round a solution to the cluster LP, yielding a $(1.437+\eps)$-approximation algorithm for the Correlation Clustering problem.
  The main technical result of this paper is a new approach to find a feasible solution for the cluster LP with objective value at most $(1+ε)$ of the optimum in time$\widetilde O(2^{\text{poly}(1/\eps)} n)$, where $n$ is the number of vertices in the graph. We also show how to implement the rounding within the same time bounds, thus achieving a fast $(1.437+\eps)$-approximation algorithm for the Correlation Clustering problem. This bridges the gap between state-of-the-art methods for approximating Correlation Clustering and the recent focus on fast algorithms.","['Nairen Cao', 'Vincent Cohen-Addad', 'Shi Li', 'Euiwoong Lee', 'David Rasmussen Lolck', 'Alantha Newman', 'Mikkel Thorup', 'Lukas Vogl', 'Shuyi Yan', 'Hanwen Zhang']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.20883,Anomali
Exploring CLIP's Dense Knowledge for Weakly Supervised Semantic Segmentation,"Weakly Supervised Semantic Segmentation (WSSS) with image-level labels aims to achieve pixel-level predictions using Class Activation Maps (CAMs). Recently, Contrastive Language-Image Pre-training (CLIP) has been introduced in WSSS. However, recent methods primarily focus on image-textalignment for CAM generation, while CLIP's potential in patch-textalignment remains unexplored. In this work, we propose ExCEL to explore CLIP's dense knowledge via a novel patch-textalignment paradigm for WSSS. Specifically, we proposeTextSemantic Enrichment (TSE) and Visual Calibration (VC) modules to improve the dense alignment across bothtextand vision modalities. To maketextembeddings semantically informative, our TSE module applies Large Language Models (LLMs) to build a dataset-wide knowledge base and enriches thetextrepresentations with an implicit attribute-hunting process. Tominefine-grained knowledge from visual features, our VC module first proposes Static Visual Calibration (SVC) to propagate fine-grained knowledge in a non-parametric manner. Then Learnable Visual Calibration (LVC) is further proposed to dynamically shift the frozen features towards distributions with diverse semantics. With these enhancements, ExCEL not only retains CLIP's training-free advantages but also significantly outperforms other state-of-the-art methods with much less training cost on PASCAL VOC and MS COCO.","['Zhiwei Yang', 'Yucong Meng', 'Kexue Fu', 'Feilong Tang', 'Shuo Wang', 'Zhijian Song']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.20826,Anomali
BeLightRec: A lightweight recommender system enhanced with BERT,"The trend of dataminingusing deep learning models on graph neural networks has proven effective in identifying object features through signal encoders and decoders, particularly in recommendation systems utilizing collaborative filtering methods. Collaborative filtering exploits similarities between users and items from historical data. However, it overlooks distinctive information, such as item names and descriptions. The semantic data of items should be furtherminedusing models in the natural language processing field. Thus, items can be compared usingtextclassification, similarity assessments, or identifying analogous sentence pairs. This research proposes combining two sources of item similarity signals: one from collaborative filtering and one from the semantic similarity measure between item names and descriptions. These signals are integrated into a graph convolutional neural network to optimize model weights, thereby providing accurate recommendations. Experiments are also designed to evaluate the contribution of each signal group to the recommendation results.","['Manh Mai Van', 'Tin T. Tran']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.20206,Anomali
CoMAC: Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions,"Recent advancements in AI-driven conversational agents have exhibited immense potential of AI applications. Effective response generation is crucial to the success of these agents. While extensive research has focused on leveraging multiple auxiliary data sources (e.g., knowledge bases and personas) to enhance response generation, existing methods often struggle to efficiently extract relevant information from these sources. There are still clear limitations in the ability to combine versatile conversational capabilities with adherence to known facts and adaptation to large variations in user preferences and belief systems, which continues to hinder the wide adoption of conversational AI tools. This paper introduces a novel method, Conversational Agent for Multi-Source Auxiliary Context with Sparse and Symmetric Latent Interactions (CoMAC), for conversation generation, which employs specialized encoding streams and post-fusion grounding networks for multiple data sources to identify relevant persona and knowledge information for the conversation. CoMAC also leverages a noveltextsimilarity metric that allows bi-directional information sharing among multiple sources and focuses on a selective subset of meaningful words. Our experiments show that CoMAC improves the relevant persona and knowledge prediction accuracies and response generation quality significantly over two state-of-the-art methods.","['Junfeng Liu', 'Christopher T. Symons', 'Ranga Raju Vatsavai']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.19274,Anomali
"Large Language Models powered Network Attack Detection: Architecture, Opportunities and Case Study","Network attack detection is a pivotal technology to identify network anomaly and classify malicious traffic. Large Language Models (LLMs) are trained on a vast corpus oftext, have amassed remarkable capabilities of context-understanding and commonsense knowledge. This has opened up a new door for network threat detection. Researchers have already initiated discussions regarding the application of LLMs on specific cyber-security tasks. Unfortunately, there is still a lack of comprehensive elaboration how tomineLLMs' potentials in network threat detections, as well as the opportunities and challenges. In this paper, we mainly focus on the classification of malicious traffic from the perspective of LLMs' capability. We present a holistic view of the architecture of LLM-powered network attack detection, including Pre-training, Fine-tuning, and Detection. Especially, by exploring the knowledge and capabilities of LLM, we identify three distinct roles LLM can act in network attack detection: \textit{Classifier, Encoder, and Predictor}. For each of them, the modeling paradigm, opportunities and challenges are elaborated. Finally, we present our design on LLM-powered DDoS detection as a case study. The proposed framework attains accurate detection on carpet bombing DDoS by exploiting LLMs' capabilities in contextualmining. The evaluation shows its efficacy, exhibiting a nearly $35$\% improvement compared to existing systems.","['Xinggong Zhang', 'Qingyang Li', 'Yunpeng Tan', 'Zongming Guo', 'Lei Zhang', 'Yong Cui']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.18487,Anomali
Regional House Price Dynamics in Australia: Insights into Lifestyle and Mining Dynamics through PCA,"This report applies Principal Component Analysis (PCA) to regional house price indexes to uncover dominant trends in Australia's housing market. Regions are assigned PCA-derived scores that reveal which underlying market forces are most influential in each area, enabling broad classification of local housing markets. The approach highlights where price movements tend to align across regions, even those geographically distant. The three most dominant trends are described in detail and, together with the regional scores, provide objective tools for policymakers, researchers, and real estate professionals.",['Willem Sijp'],,arXiv,2025,https://doi.org/10.48550/arXiv.2503.18332,Anomali
Multivariate Gaussian Topic Modelling: A novel approach to discover topics with greater semantic coherence,"An important aspect oftextmininginvolves information retrieval in form of discovery of semantic themes (topics) from documents using topic modelling. While generative topic models like Latent Dirichlet Allocation (LDA) elegantly model topics as probability distributions and are useful in identifying latent topics from large document corpora with minimal supervision, they suffer from difficulty in topic interpretability and reduced performance in shortertexts. Here we propose a novel Multivariate Gaussian Topic modelling (MGD) approach. In this approach topics are presented as Multivariate Gaussian Distributions and documents as Gaussian Mixture Models. Using EM algorithm, the various constituent Multivariate Gaussian Distributions and their corresponding parameters are identified. Analysis of the parameters helps identify the keywords having the highest variance and mean contributions to the topic, and from these key-words topic annotations are carried out. This approach is first applied on a synthetic dataset to demonstrate the interpretability benefits vis-à-vis LDA. A real-world application of this topic model is demonstrated in analysis of risks and hazards at a petrochemical plant by applying the model on safety incident reports to identify the major latent hazards plaguing the plant. This model achieves a higher mean topic coherence of 0.436 vis-à-vis 0.294 for LDA.","['Satyajeet Sahoo', 'Jhareswar Maiti', 'Virendra Kumar Tewari']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.15036,Anomali
Multi-modal Time Series Analysis: A Tutorial and Survey,"Multi-modal time series analysis has recently emerged as a prominent research area in datamining, driven by the increasing availability of diverse data modalities, such astext, images, and structured tabular data from real-world sources. However, effective analysis of multi-modal time series is hindered by data heterogeneity, modality gap, misalignment, and inherent noise. Recent advancements in multi-modal time series methods have exploited the multi-modal context via cross-modal interactions based on deep learning methods, significantly enhancing various downstream tasks. In this tutorial and survey, we present a systematic and up-to-date overview of multi-modal time series datasets and methods. We first state the existing challenges of multi-modal time series analysis and our motivations, with a brief introduction of preliminaries. Then, we summarize the general pipeline and categorize existing methods through a unified cross-modal interaction framework encompassing fusion, alignment, and transference at different levels (\textit{i.e.}, input, intermediate, output), where key concepts and ideas are highlighted. We also discuss the real-world applications of multi-modal analysis for both standard and spatial time series, tailored to general and specific domains. Finally, we discuss future research directions to help practitioners explore and exploit multi-modal time series. The up-to-date resources are provided in the GitHub repository: https://github.com/UConn-DSIS/Multi-modal-Time-Series-Analysis","['Yushan Jiang', 'Kanghui Ning', 'Zijie Pan', 'Xuyang Shen', 'Jingchao Ni', 'Wenchao Yu', 'Anderson Schneider', 'Haifeng Chen', 'Yuriy Nevmyvaka', 'Dongjin Song']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.13709,Anomali
A Comprehensive Survey on Visual Concept Mining in Text-to-image Diffusion Models,"Text-to-image diffusion models have made significant advancements in generating high-quality, diverse images fromtextprompts. However, the inherent limitations of textual signals often prevent these models from fully capturing specific concepts, thereby reducing their controllability. To address this issue, several approaches have incorporated personalization techniques, utilizing reference images tominevisual concept representations that complement textual inputs and enhance the controllability oftext-to-image diffusion models. Despite these advances, a comprehensive, systematic exploration of visual conceptminingremains limited. In this paper, we categorize existing research into four key areas: Concept Learning, Concept Erasing, Concept Decomposition, and Concept Combination. This classification provides valuable insights into the foundational principles of Visual ConceptMining(VCM) techniques. Additionally, we identify key challenges and propose future research directions to propel this important and interesting field forward.","['Ziqiang Li', 'Jun Li', 'Lizhi Xiong', 'Zhangjie Fu', 'Zechao Li']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.13576,Anomali
Multi-Granular Multimodal Clue Fusion for Meme Understanding,"With the continuous emergence of various social media platforms frequently used in daily life, the multimodal meme understanding (MMU) task has been garnering increasing attention. MMU aims to explore and comprehend the meanings of memes from various perspectives by performing tasks such as metaphor recognition, sentiment analysis, intention detection, and offensiveness detection. Despite making progress, limitations persist due to the loss of fine-grained metaphorical visual clue and the neglect of multimodaltext-image weak correlation. To overcome these limitations, we propose a multi-granular multimodal clue fusion model (MGMCF) to advance MMU. Firstly, we design an object-level semanticminingmodule to extract object-level image feature clues, achieving fine-grained feature clue extraction and enhancing the model's ability to capture metaphorical details and semantics. Secondly, we propose a brand-new global-local cross-modal interaction model to address the weak correlation betweentextand images. This model facilitates effective interaction between global multimodal contextual clues and local unimodal feature clues, strengthening their representations through a bidirectional cross-modal attention mechanism. Finally, we devise a dual-semantic guided training strategy to enhance the model's understanding and alignment of multimodal representations in the semantic space. Experiments conducted on the widely-used MET-MEME bilingual dataset demonstrate significant improvements over state-of-the-art baselines. Specifically, there is an 8.14% increase in precision for offensiveness detection task, and respective accuracy enhancements of 3.53%, 3.89%, and 3.52% for metaphor recognition, sentiment analysis, and intention detection tasks. These results, underpinned by in-depth analyses, underscore the effectiveness and potential of our approach for advancing MMU.","['Li Zheng', 'Hao Fei', 'Ting Dai', 'Zuquan Peng', 'Fei Li', 'Huisheng Ma', 'Chong Teng', 'Donghong Ji']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.12560,Anomali
How Can Time Series Analysis Benefit From Multiple Modalities? A Survey and Outlook,"Time series analysis (TSA) is a longstanding research topic in the dataminingcommunity and has wide real-world significance. Compared to ""richer"" modalities such as language and vision, which have recently experienced explosive development and are densely connected, the time-series modality remains relatively underexplored and isolated. We notice that many recent TSA works have formed a new research field, i.e., Multiple Modalities for TSA (MM4TSA). In general, these MM4TSA works follow a common motivation: how TSA can benefit from multiple modalities. This survey is the first to offer a comprehensive review and a detailed outlook for this emerging field. Specifically, we systematically discuss three benefits: (1) reusing foundation models of other modalities for efficient TSA, (2) multimodal extension for enhanced TSA, and (3) cross-modality interaction for advanced TSA. We further group the works by the introduced modality type, includingtext, images, audio, tables, and others, within each perspective. Finally, we identify the gaps with future opportunities, including the reused modalities selections, heterogeneous modality combinations, and unseen tasks generalizations, corresponding to the three benefits. We release an up-to-date GitHub repository that includes key papers and resources.","['Haoxin Liu', 'Harshavardhan Kamarthi', 'Zhiyuan Zhao', 'Shangqing Xu', 'Shiyu Wang', 'Qingsong Wen', 'Tom Hartvigsen', 'Fei Wang', 'B. Aditya Prakash']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.11835,Anomali
Annotating Scientific Uncertainty: A comprehensive model using linguistic patterns and comparison with existing approaches,"UnScientify, a system designed to detect scientific uncertainty in scholarly fulltext. The system utilizes a weakly supervised technique to identify verbally expressed uncertainty in scientifictextsand their authorial references. The core methodology of UnScientify is based on a multi-faceted pipeline that integrates span pattern matching, complex sentence analysis and author reference checking. This approach streamlines the labeling and annotation processes essential for identifying scientific uncertainty, covering a variety of uncertainty expression types to support diverse applications including information retrieval,textminingand scientific document processing. The evaluation results highlight the trade-offs between modern large language models (LLMs) and the UnScientify system. UnScientify, which employs more traditional techniques, achieved superior performance in the scientific uncertainty detection task, attaining an accuracy score of 0.808. This finding underscores the continued relevance and efficiency of UnScientify's simple rule-based and pattern matching strategy for this specific application. The results demonstrate that in scenarios where resource efficiency, interpretability, and domain-specific adaptability are critical, traditional methods can still offer significant advantages.","['Panggih Kusuma Ningrum', 'Philipp Mayr', 'Nina Smirnova', 'Iana Atanassova']",2025,arXiv,2025,https://doi.org/10.48550/arXiv.2503.11376,Anomali
Predicting Clinical Outcomes with Waveform LSTMs,"Dataminingand machine learning hold great potential to enable health systems to systematically use data and analytics to identify inefficiencies and best practices that improve care and reduce costs. Waveform data offers particularly detailed information on how patient health evolves over time and has the potential to significantly improve prediction accuracy on multiple benchmarks, but has been widely under-utilized, largely because of the challenges in working with these large and complex datasets. This study evaluates the potential of leveraging clinical waveform data to improve prediction accuracy on a single benchmark task: the risk of mortality in the intensive care unit. We identify significant potential from this data, beating the existing baselines for both logistic regression and deep learning models.",['Michael Albada'],,arXiv,2025,https://doi.org/10.48550/arXiv.2503.10925,Anomali
A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization,"Automatic patent summarization approaches that help in the patent analysis and comprehension procedure are in high demand due to the colossal growth of innovations. The development of natural language processing (NLP),textmining, and deep learning has notably amplified the efficacy oftextsummarization models for abundant types of documents. Summarizing patenttextremains a pertinent challenge due to the labyrinthine writing style of these documents, which includes technical and legal intricacies. Additionally, these patent document contents are considerably lengthier than archetypal documents, which complicates the process of extracting pertinent information for summarization. Embodying extractive and abstractivetextsummarization methodologies into a hybrid framework, this study proposes a system for efficiently creating abstractive summaries of patent records. The procedure involves leveraging the LexRank graph-based algorithm to retrieve the important sentences from input parenttexts, then utilizing a Bidirectional Auto-Regressive Transformer (BART) model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for producingtextsummaries. This is accompanied by methodical testing and evaluation strategies. Furthermore, the author employed certain meta-learning techniques to achieve Domain Generalization (DG) of the abstractive component across multiple patent fields.","['Nevidu Jayatilleke', 'Ruvan Weerasinghe']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.10354,Anomali
Leveraging Retrieval Augmented Generative LLMs For Automated Metadata Description Generation to Enhance Data Catalogs,"Data catalogs serve as repositories for organizing and accessing diverse collection of data assets, but their effectiveness hinges on the ease with which business users can look-up relevant content. Unfortunately, many data catalogs within organizations suffer from limited searchability due to inadequate metadata like asset descriptions. Hence, there is a need of content generation solution to enrich and curate metadata in a scalable way. This paper explores the challenges associated with metadata creation and proposes a unique prompt enrichment idea of leveraging existing metadata content using retrieval based few-shot technique tied with generative large language models (LLM). The literature also considers finetuning an LLM on existing content and studies the behavior of few-shot pretrained LLM (Llama, GPT3.5) vis-à-vis few-shot finetuned LLM (Llama2-7b) by evaluating their performance based on accuracy, factual grounding, and toxicity. Our preliminary results exhibit more than 80% Rouge-1 F1 for the generated content. This implied 87%- 88% of instances accepted as is or curated with minor edits by data stewards. By automatically generating descriptions for tables and columns in most accurate way, the research attempts to provide an overall framework for enterprises to effectively scale metadata curation and enrich its data catalog thereby vastly improving the data catalog searchability and overall usability.","['Mayank Singh', 'Abhijeet Kumar', 'Sasidhar Donaparthi', 'Gayatri Karambelkar']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.09003,Anomali
MegaSR: Mining Customized Semantics and Expressive Guidance for Image Super-Resolution,"Pioneeringtext-to-image (T2I) diffusion models have ushered in a new era of real-world image super-resolution (Real-ISR), significantly enhancing the visual perception of reconstructed images. However, existing methods typically integrate uniform abstract textual semantics across all blocks, overlooking the distinct semantic requirements at different depths and the fine-grained, concrete semantics inherently present in the images themselves. Moreover, relying solely on a single type of guidance further disrupts the consistency of reconstruction. To address these issues, we propose MegaSR, a novel framework thatminescustomized block-wise semantics and expressive guidance for diffusion-based ISR. Compared to uniform textual semantics, MegaSR enables flexible adaptation to multi-granularity semantic awareness by dynamically incorporating image attributes at each block. Furthermore, we experimentally identify HED edge maps, depth maps, and segmentation maps as the most expressive guidance, and propose a multi-stage aggregation strategy to modulate them into the T2I models. Extensive experiments demonstrate the superiority of MegaSR in terms of semantic richness and structural consistency.","['Xinrui Li', 'Jianlong Wu', 'Xinchuan Huang', 'Chong Chen', 'Weili Guan', 'Xian-Sheng Hua', 'Liqiang Nie']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.08096,Anomali
Text-RGBT Person Retrieval: Multilevel Global-Local Cross-Modal Alignment and A High-quality Benchmark,"The performance of traditionaltext-image person retrieval task is easily affected by lighting variations due to imaging limitations of visible spectrum sensors. In this work, we design a novel task calledtext-RGBT person retrieval that integrates complementary benefits from thermal and visible modalities for robust person retrieval in challenging environments. Aligningtextand multi-modal visual representations is the key issue intext-RGBT person retrieval, but the heterogeneity between visible and thermal modalities may interfere with the alignment of visual andtextmodalities. To handle this problem, we propose a Multi-level Global-local cross-modal Alignment Network (MGANet), which sufficientlyminesthe relationships between modality-specific and modality-collaborative visual with thetext, fortext-RGBT person retrieval. To promote the research and development of this field, we create a high-qualitytext-RGBT person retrieval dataset, RGBT-PEDES. RGBT-PEDES contains 1,822 identities from different age groups and genders with 4,723 pairs of calibrated RGB and thermal images, and covers high-diverse scenes from both daytime and nighttime with a various of challenges such as occlusion, weak alignment and adverse lighting conditions. Additionally, we carefully annotate 7,987 fine-grained textual descriptions for all RGBT person image pairs. Extensive experiments on RGBT-PEDES demonstrate that our method outperforms existingtext-image person retrieval methods. The code and dataset will be released upon the acceptance.","['Yifei Deng', 'Zhengyu Chen', 'Ziheng Xu', 'Chenglong Li', 'Jin Tang']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.07950,Anomali
Hyperoctant Search Clustering: A Method for Clustering Data in High-Dimensional Hyperspheres,"Clustering of high-dimensional data sets is a growing need in artificial intelligence, machine learning and pattern recognition. In this paper, we propose a new clustering method based on a combinatorial-topological approach applied to regions of space defined by signs of coordinates (hyperoctants). In high-dimensional spaces, this approach often reduces the size of the dataset while preserving sufficient topological features. According to a density criterion, the method builds clusters of data points based on the partitioning of a graph, whose vertices represent hyperoctants, and whose edges connect neighboring hyperoctants under the Levenshtein distance. We call this method HyperOctant Search Clustering. We prove some mathematical properties of the method. In order to as assess its performance, we choose the application of topic detection, which is an important task intextmining. Our results suggest that our method is more stable under variations of the main hyperparameter, and remarkably, it is not only a clustering method, but also a tool to explore the dataset from a topological perspective, as it directly provides information about the number of hyperoctants where there are data points. We also discuss the possible connections between our clustering method and other research fields.","['Mauricio Toledo-Acosta', 'Luis Ángel Ramos-García', 'Jorge Hermosillo-Valadez']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.07917,Anomali
Text-IRSTD: Leveraging Semantic Text to Promote Infrared Small Target Detection in Complex Scenes,"Infrared small target detection is currently a hot and challenging task in computer vision. Existing methods usually focus onminingvisual features of targets, which struggles to cope with complex and diverse detection scenarios. The main reason is that infrared small targets have limited image information on their own, thus relying only on visual features fails to discriminate targets and interferences, leading to lower detection performance. To address this issue, we introduce a novel approach leveraging semantictextto guide infrared small target detection, calledText-IRSTD. It innovatively expands classical IRSTD totext-guided IRSTD, providing a new research idea. On the one hand, we devise a novel fuzzy semantictextprompt to accommodate ambiguous target categories. On the other hand, we propose a progressive cross-modal semantic interaction decoder (PCSID) to facilitate information fusion betweentextsand images. In addition, we construct a new benchmark consisting of 2,755 infrared images of different scenarios with fuzzy semantic textual annotations, called FZDT. Extensive experimental results demonstrate that our method achieves better detection performance and target contour recovery than the state-of-the-art methods. Moreover, proposedText-IRSTD shows strong generalization and wide application prospects in unseen detection scenarios. The dataset and code will be publicly released after acceptance of this paper.","['Feng Huang', 'Shuyuan Zheng', 'Zhaobing Qiu', 'Huanxian Liu', 'Huanxin Bai', 'Liqiong Chen']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.07249,Anomali
DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL,"Recenttext-to-SQL systems powered by large language models (LLMs) have demonstrated remarkable performance in translating natural language queries into SQL. However, these systems often struggle with complex database structures and domain-specific queries, as they primarily focus on enhancing logical reasoning and SQL syntax while overlooking the critical need for comprehensive database understanding. To address this limitation, we propose DB-Explore, a novel framework that systematically aligns LLMs with database knowledge through automated exploration and instruction synthesis. DB-Explore constructs database graphs to capture complex relational schemas, leverages GPT-4 to systematicallyminestructural patterns and semantic knowledge, and synthesizes instructions to distill this knowledge for efficient fine-tuning of LLMs. Our framework enables comprehensive database understanding through diverse sampling strategies and automated instruction generation, bridging the gap between database structures and language models. Experiments conducted on the SPIDER and BIRD benchmarks validate the effectiveness of DB-Explore, achieving an execution accuracy of 67.0% on BIRD and 87.8% on SPIDER. Notably, our open-source implementation based on Qwen2.5-Coder-7B achieves state-of-the-art results at minimal computational cost, outperforming several GPT-4-drivenText-to-SQL systems.","['Haoyuan Ma', 'Yongliang Shen', 'Hengwei Liu', 'Wenqi Zhang', 'Haolei Xu', 'Qiuying Peng', 'Jun Wang', 'Weiming Lu']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.04959,Anomali
"Comparative Analysis Based on DeepSeek, ChatGPT, and Google Gemini: Features, Techniques, Performance, Future Prospects","Nowadays, DeepSeek, ChatGPT, and Google Gemini are the most trending and exciting Large Language Model (LLM) technologies for reasoning, multimodal capabilities, and general linguistic performance worldwide. DeepSeek employs a Mixture-of-Experts (MoE) approach, activating only the parameters most relevant to the task at hand, which makes it especially effective for domain-specific work. On the other hand, ChatGPT relies on a dense transformer model enhanced through reinforcement learning from human feedback (RLHF), and then Google Gemini actually uses a multimodal transformer architecture that integratestext, code, and images into a single framework. However, by using those technologies, people can be able tominetheir desiredtext, code, images, etc, in a cost-effective and domain-specific inference. People may choose those techniques based on the best performance. In this regard, we offer a comparative study based on the DeepSeek, ChatGPT, and Gemini techniques in this research. Initially, we focus on their methods and materials, appropriately including the data selection criteria. Then, we present state-of-the-art features of DeepSeek, ChatGPT, and Gemini based on their applications. Most importantly, we show the technological comparison among them and also cover the dataset analysis for various applications. Finally, we address extensive research areas and future potential guidance regarding LLM-based AI research for the community.","['Anichur Rahman', 'Shahariar Hossain Mahir', 'Md Tanjum An Tashrif', 'Airin Afroj Aishi', 'Md Ahsan Karim', 'Dipanjali Kundu', 'Tanoy Debnath', 'Md. Abul Ala Moududi', 'MD. Zunead Abedin Eidmum']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.04783,Anomali
RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval via Radiology Report Mining,"Developing advanced medical imaging retrieval systems is challenging due to the varying definitions of `similar images' across different medical contexts. This challenge is compounded by the lack of large-scale, high-quality medical imaging retrieval datasets and benchmarks. In this paper, we propose a novel methodology that leverages dense radiology reports to define image-wise similarity ordering at multiple granularities in a scalable and fully automatic manner. Using this approach, we construct two comprehensive medical imaging retrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans, providing detailed image-image ranking annotations conditioned on diverse anatomical structures. Furthermore, we develop two retrieval systems, RadIR-CXR and model-ChestCT, which demonstrate superior performance in traditional image-image and image-report retrieval tasks. These systems also enable flexible, effective image retrieval conditioned on specific anatomical structures described intext, achieving state-of-the-art results on 77 out of 78 metrics.","['Tengfei Zhang', 'Ziheng Zhao', 'Chaoyi Wu', 'Xiao Zhou', 'Ya Zhang', 'Yangfeng Wang', 'Weidi Xie']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.04653,Anomali
Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions,"Multiple previous studies have reported suboptimal performance of LLMs in biomedicaltextmining. By analyzing failure patterns in these evaluations, we identified three primary challenges for LLMs in biomedical corpora: (1) LLMs fail to learn implicit dataset-specific nuances from supervised data, (2) The common formatting requirements of discriminative tasks limit the reasoning capabilities of LLMs particularly for LLMs that lack test-time compute, and (3) LLMs struggle to adhere to annotation guidelines and match exact schemas, which hinders their ability to understand detailed annotation requirements which is essential in biomedical annotation workflow. We experimented with prompt engineering techniques targeted to the above issues, and developed a pipeline that dynamically extracts instructions from annotation guidelines. Our results show that frontier LLMs can approach or surpass the performance of SOTA BERT-based models with minimal reliance on manually annotated data and without fine-tuning. Furthermore, we performed model distillation on a closed-source LLM, demonstrating that a BERT model trained exclusively on synthetic data annotated by LLMs can also achieve a practical performance. Based on these findings, we explored the feasibility of partially replacing manual annotation with LLMs in production scenarios for biomedicaltextmining.","['Yichong Zhao', 'Susumu Goto']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.03261,Anomali
GenColor: Generative Color-Concept Association in Visual Design,"Existing approaches for color-concept association typically rely on query-based image referencing, and color extraction from image references. However, these approaches are effective only for common concepts, and are vulnerable to unstable image referencing and varying image conditions. Our formative study with designers underscores the need for primary-accent color compositions and context-dependent colors (e.g., 'clear' vs. 'polluted' sky) in design. In response, we introduce a generative approach forminingsemantically resonant colors leveraging images generated bytext-to-image models. Our insight is that contemporarytext-to-image models can resemble visual patterns from large-scale real-world data. The framework comprises three stages: concept instancing produces generative samples using diffusion models,text-guided image segmentation identifies concept-relevant regions within the image, and color association extracts primarily accompanied by accent colors. Quantitative comparisons with expert designs validate our approach's effectiveness, and we demonstrate the applicability through cases in various design scenarios and a gallery.","['Yihan Hou', 'Xingchen Zeng', 'Yusong Wang', 'Manling Yang', 'Xiaojiao Chen', 'Wei Zeng']",,arXiv,2025,https://doi.org/10.48550/arXiv.2503.03236,Anomali
Electrocatalyst discovery through text mining and multi-objective optimization,"The discovery and optimization of high-performance materials is the basis for advancing energy conversion technologies. To understand composition-property relationships, all available data sources should be leveraged: experimental results, predictions from simulations, and latent knowledge from scientifictexts. Among these three,text-based data sources are still not used to their full potential. We present an approach combiningtextmining, Word2Vec representations of materials and properties, and Pareto front analysis for the prediction of high-performance candidate materials for electrocatalysis in regions where other data sources are scarce or non-existent. Candidate compositions are evaluated on the basis of their similarity to the terms `conductivity' and `dielectric', which enables reaction-specific candidate composition predictions for oxygen reduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions. This, combined with Pareto optimization, allows us to significantly reduce the pool of candidate compositions to high-performing compositions. Our predictions, which are purely based ontextdata, match the measured electrochemical activity very well.","['Lei Zhang', 'Markus Stricker']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.20860,Anomali
Sentiment Analysis of Movie Reviews Using BERT,Sentiment Analysis (SA) or opinionminingis analysis of emotions and opinions from any kind oftext. SA helps in tracking peoples viewpoints and it is an important factor when it comes to social media monitoring product and brand recognition customer satisfaction customer loyalty advertising and promotions success and product acceptance. That is why SA is one of the active research areas in Natural Language Processing (NLP). SA is applied on data sourced from various media platforms tominesentiment knowledge from them. Various approaches have been deployed in the literature to solve the problem. Most techniques devise complex and sophisticated frameworks in order to attain optimal accuracy. This work aims to finetune Bidirectional Encoder Representations from Transformers (BERT) with Bidirectional Long Short-Term Memory (BiLSTM) for movie reviews sentiment analysis and still provide better accuracy than the State-of-The-Art (SOTA) methods. The paper also shows how sentiment analysis can be applied if someone wants to recommend a certain movie for example by computing overall polarity of its sentiments predicted by the model. That is our proposed method serves as an upper-bound baseline in prediction of a predominant reaction to a movie. To compute overall polarity a heuristic algorithm is applied to BERTBiLSTM output vector. Our model can be extended to three-class four-class or any fine-grained classification and apply overall polarity computation again. This is intended to be exploited in future work.,"['Gibson Nkhata', 'Usman Anjum', 'Justin Zhan']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.18841,Anomali
Inverse Materials Design by Large Language Model-Assisted Generative Framework,"Deep generative models hold great promise for inverse materials design, yet their efficiency and accuracy remain constrained by data scarcity and model architecture. Here, we introduce AlloyGAN, a closed-loop framework that integrates Large Language Model (LLM)-assistedtextminingwith Conditional Generative Adversarial Networks (CGANs) to enhance data diversity and improve inverse design. Taking alloy discovery as a case study, AlloyGAN systematically refines material candidates through iterative screening and experimental validation. For metallic glasses, the framework predicts thermodynamic properties with discrepancies of less than 8% from experiments, demonstrating its robustness. By bridging generative AI with domain knowledge and validation workflows, AlloyGAN offers a scalable approach to accelerate the discovery of materials with tailored properties, paving the way for broader applications in materials science.","['Yun Hao', 'Che Fan', 'Beilin Ye', 'Wenhao Lu', 'Zhen Lu', 'Peilin Zhao', 'Zhifeng Gao', 'Qingyao Wu', 'Yanhui Liu', 'Tongqi Wen']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.18127,Anomali
Multi-modal and Metadata Capture Model for Micro Video Popularity Prediction,"As short videos have become the primary form of content consumption across various industries, accurately predicting their popularity has become key to enhancing user engagement and optimizing business strategies. This report presents a solution for the 2024 INFORMS DataMiningChallenge, focusing on our developed 3M model (Multi-modal and Metadata Capture Model), which is a multi-modal popularity prediction model. The 3M model integrates video, audio, descriptions, and metadata to fully explore the multidimensional information of short videos. We employ a retriever-based method to retrieve relevant instances from a multi-modal memory bank, filtering similar videos based on visual, acoustic, andtext-based features for prediction. Additionally, we apply a random masking method combined with a semi-supervised model for incomplete multi-modalities to leverage the metadata of videos. Ultimately, we use a network to synthesize both approaches, significantly improving the accuracy of predictions. Compared to traditional tag-based algorithms, our model outperforms existing methods on the validation set, showing a notable increase in prediction accuracy. Our research not only offers a new perspective on understanding the drivers of short video popularity but also provides valuable data support for identifying market opportunities, optimizing advertising strategies, and enhancing content creation. We believe that the innovative methodology proposed in this report provides practical tools and valuable insights for professionals in the field of short video popularity prediction, helping them effectively address future challenges.","['Jiacheng Lu', 'Mingyuan Xiao', 'Weijian Wang', 'Yuxin Du', 'Zhengze Wu', 'Cheng Hua']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.17038,Anomali
An Improved Deep Learning Model for Word Embeddings Based Clustering for Large Text Datasets,"In this paper, an improved clustering technique for large textual datasets by leveraging fine-tuned word embeddings is presented. WEClustering technique is used as the base model. WEClustering model is fur-ther improvements incorporating fine-tuning contextual embeddings, advanced dimensionality reduction methods, and optimization of clustering algorithms. Experimental results on benchmark datasets demon-strate significant improvements in clustering metrics such as silhouette score, purity, and adjusted rand index (ARI). An increase of 45% and 67% of median silhouette score is reported for the proposed WE-Clustering_K++ (based on K-means) and WEClustering_A++ (based on Agglomerative models), respec-tively. The proposed technique will help to bridge the gap between semantic understanding and statistical robustness for large-scaletext-miningtasks.","['Vijay Kumar Sutrakar', 'Nikhil Mogre']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.16139,Anomali
Generative AI Training and Copyright Law,"Training generative AI models requires extensive amounts of data. A common practice is to collect such data through web scraping. Yet, much of what has been and is collected is copyright protected. Its use may be copyright infringement. In the USA, AI developers rely on ""fair use"" and in Europe, the prevailing view is that the exception for ""Textand DataMining"" (TDM) applies. In a recent interdisciplinary tandem-study, we have argued in detail that this is actually not the case because generative AI training fundamentally differs from TDM. In this article, we share our main findings and the implications for both public and corporate research on generative models. We further discuss how the phenomenon of training data memorization leads to copyright issues independently from the ""fair use"" and TDM exceptions. Finally, we outline how the ISMIR could contribute to the ongoing discussion about fair practices with respect to generative AI that satisfy all stakeholders.","['Tim W. Dornis', 'Sebastian Stober']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.15858,Anomali
ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval,"The objective in this paper is to improve the performance oftext-to-image retrieval. To this end, we introduce a new framework that can boost the performance of large-scale pre-trained vision-language models, so that they can be used fortext-to-image re-ranking. The approach, Enhanced Language-Image Pre-training (ELIP), uses thetextquery, via a simple MLP mapping network, to predict a set of visual prompts to condition the ViT image encoding. ELIP can easily be applied to the commonly used CLIP, SigLIP and BLIP-2 networks. To train the architecture with limited computing resources, we develop a 'student friendly' best practice, involving global hard samplemining, and curation of a large-scale dataset. On the evaluation side, we set up two new out-of-distribution (OOD) benchmarks, Occluded COCO and ImageNet-R, to assess the zero-shot generalisation of the models to different domains. The results demonstrate that ELIP significantly boosts CLIP/SigLIP/SigLIP-2text-to-image retrieval performance and outperforms BLIP-2 on several benchmarks, as well as providing an easy means to adapt to OOD datasets.","['Guanqi Zhan', 'Yuanpei Liu', 'Kai Han', 'Weidi Xie', 'Andrew Zisserman']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.15682,Anomali
SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training,"We present a framework for pre-training of 3D hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed SimHand. Pre-training with large-scale images achieves promising results in various tasks, but prior methods for 3D hand pose pre-training have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our pre-training method with contrastive learning. Specifically, we collect over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands: pairs of non-identical samples with similar hand poses. We then propose a novel contrastive learning method that embeds similar hand pairs closer in the feature space. Our method not only learns from similar samples but also adaptively weights the contrastive learning loss based on inter-sample distance, leading to additional performance gains. Our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method (PeCLR) in various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on AssemblyHands.
  Our code is available at https://github.com/ut-vision/SiMHand.","['Nie Lin', 'Takehiko Ohkawa', 'Yifei Huang', 'Mingfang Zhang', 'Minjie Cai', 'Ming Li', 'Ryosuke Furuta', 'Yoichi Sato']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.15251,Anomali
ConFit v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining,"A reliable resume-job matching system helps a company recommend suitable candidates from a pool of resumes and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction labels in resume-job datasets are sparse. We introduce ConFit v2, an improvement over ConFit to tackle this sparsity problem. We propose two techniques to enhance the encoder's contrastive training process: augmenting job data with hypothetical reference resume generated by a large language model; and creating high-quality hard negatives from unlabeled resume/job pairs using a novel hard-negativeminingstrategy. We evaluate ConFit v2 on two real-world datasets and demonstrate that it outperforms ConFit and prior methods (including BM25 and OpenAItext-embedding-003), achieving an average absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking and resume-ranking tasks.","['Xiao Yu', 'Ruize Xu', 'Chengyuan Xue', 'Jinzhong Zhang', 'Xu Ma', 'Zhou Yu']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.12361,Anomali
Mining Social Determinants of Health for Heart Failure Patient 30-Day Readmission via Large Language Model,"Heart Failure (HF) affects millions of Americans and leads to high readmission rates, posing significant healthcare challenges. While Social Determinants of Health (SDOH) such as socioeconomic status and housing stability play critical roles in health outcomes, they are often underrepresented in structured EHRs and hidden in unstructured clinical notes. This study leverages advanced large language models (LLMs) to extract SDOHs from clinicaltextand uses logistic regression to analyze their association with HF readmissions. By identifying key SDOHs (e.g. tobacco usage, limited transportation) linked to readmission risk, this work also offers actionable insights for reducing readmissions and improving patient care.","['Mingchen Shao', 'Youjeong Kang', 'Xiao Hu', 'Hyunjung Gloria Kwak', 'Carl Yang', 'Jiaying Lu']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.12158,Anomali
Automatic target validation based on neuroscientific literature mining for tractography,"Target identification for tractography studies requires solid anatomical knowledge validated by an extensive literature review across species for each seed structure to be studied. Manual literature review to identify targets for a given seed region is tedious and potentially subjective. Therefore, complementary approaches would be useful. We propose to usetext-miningmodels to automatically suggest potential targets from the neuroscientific literature, full-textarticles and abstracts, so that they can be used for anatomical connection studies and more specifically for tractography. We appliedtext-miningmodels to three structures: two well-studied structures, since validated deep brain stimulation targets, the internal globus pallidus and the subthalamic nucleus and, the nucleus accumbens, an exploratory target for treating psychiatric disorders. We performed a systematic review of the literature to document the projections of the three selected structures and compared it with the targets proposed bytext-miningmodels, both in rat and primate (including human). We ran probabilistic tractography on the nucleus accumbens and compared the output with the results of thetext-miningmodels and literature review. Overall,text-miningthe literature could find three times as many targets as two man-weeks of curation could. The overall efficiency of thetext-miningagainst literature review in our study was 98% recall (at 36% precision), meaning that over all the targets for the three selected seeds, only one target has been missed bytext-mining. We demonstrate that connectivity for a structure of interest can be extracted from a very large amount of publications and abstracts. We believe this tool will be useful in helping the neuroscience community to facilitate connectivity studies of particular brain regions. Thetextminingtools used for the study are part of the HBP Neuroinformatics Platform, publicly available at http://connectivity-brainer.rhcloud.com","['Xavier Vasques', 'Renaud Richardet', 'Sean L Hill', 'David Slater', 'Jean-Cedric Chappelier', 'Etienne Pralong', 'Jocelyne Bloch', 'Bogdan Draganski', 'Laura Cif']",Front Neuroanat. 2015 May 27;9:66,arXiv,2025,https://doi.org/10.48550/arXiv.2502.11597,Anomali
Graph Neural Network-based Spectral Filtering Mechanism for Imbalance Classification in Network Digital Twin,"Graph neural networks are gaining attention in fifth-generation (5G) core network digital twins, which are data-driven complex systems with numerous components. Analyzing these data can be challenging due to rare failure types, leading to imbalanced classification in multiclass settings. Digital twins of 5G networks increasingly employ graph classification as the main method for identifying failure types. However, the skewed distribution of failure occurrences is a significant class-imbalance problem that prevents practical graph datamining. Previous studies have not sufficiently addressed this complex problem. This paper, proposes class-Fourier GNN (CF-GNN) that introduces a class-oriented spectral filtering mechanism to ensure precise classification by estimating a unique spectral filter for each class. This work employs eigenvalue and eigenvector spectral filtering to capture and adapt to variations in minority classes, ensuring accurate class-specific feature discrimination, and adept at graph representation learning for complex local structures among neighbors in an end-to-end setting. The extensive experiments demonstrate that the proposed CF-GNN could help create new techniques for enhancing classifiers and investigate the characteristics of the multiclass imbalanced data in a network digital twin system.","['Abubakar Isah', 'Ibrahim Aliyu', 'Sulaiman Muhammad Rashid', 'Jaehyung Park', 'Minsoo Hahn', 'Jinsul Kim']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.11505,Anomali
KGGen: Extracting Knowledge Graphs from Plain Text with Language Models,"Recent interest in building foundation models for KGs has highlighted a fundamental challenge: knowledge-graph data is relatively scarce. The best-known KGs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated KGs are in short supply, automatically extracted KGs are of questionable quality. We present a solution to this data scarcity problem in the form of atext-to-KG generator (KGGen), a package that uses language models to create high-quality graphs from plaintext. Unlike other KG extractors, KGGen clusters related entities to reduce sparsity in extracted KGs. KGGen is available as a Python library (\texttt{pip install kg-gen}), making it accessible to everyone. Along with KGGen, we release the first benchmark, Measure of of Information in Nodes and Edges (MINE), that tests an extractor's ability to produce a useful KG from plaintext. We benchmark our new tool against existing extractors and demonstrate far superior performance.","['Belinda Mo', 'Kyssen Yu', 'Joshua Kazdan', 'Proud Mpala', 'Lisa Yu', 'Chris Cundy', 'Charilaos Kanatsoulis', 'Sanmi Koyejo']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.09956,Anomali
Unveiling Global Discourse Structures: Theoretical Analysis and NLP Applications in Argument Mining,"Particularly in the structure of global discourse, coherence plays a pivotal role in humantextcomprehension and is a hallmark of high-qualitytext. This is especially true for persuasivetexts, where coherent argument structures support claims effectively. This paper discusses and proposes methods for detecting, extracting and representing these global discourse structures in a proccess called Argument(ation)Mining. We begin by defining key terms and processes of discourse structure analysis, then continue to summarize existing research on the matter, and identify shortcomings in current argument component extraction and classification methods. Furthermore, we will outline an architecture for argumentminingthat focuses on making models more generalisable while overcoming challenges in the current field of research by utilizing novel NLP techniques. This paper reviews current knowledge, summarizes recent works, and outlines our NLP pipeline, aiming to contribute to the theoretical understanding of global discourse structures.",['Christopher van Le'],,arXiv,2025,https://doi.org/10.48550/arXiv.2502.08371,Anomali
Adapting Multilingual Embedding Models to Historical Luxembourgish,"The growing volume of digitized historicaltextsrequires effective semantic search usingtextembeddings. However, pre-trained multilingual models face challenges with historical content due to OCR noise and outdated spellings. This study examines multilingual embeddings for cross-lingual semantic search in historical Luxembourgish (LB), a low-resource language. We collect historical Luxembourgish news articles from various periods and use GPT-4o for sentence segmentation and translation, generating 20,000 parallel training sentences per language pair. Additionally, we create a semantic search (Historical LB BitextMining) evaluation set and find that existing models perform poorly on cross-lingual search for historical Luxembourgish. Using our historical and additional modern parallel training data, we adapt several multilingual embedding models through contrastive learning or knowledge distillation and increase accuracy significantly for all models. We release our adapted models and historical Luxembourgish-German/French/English bitexts to support further research.","['Andrianos Michail', 'Corina Julia Raclé', 'Juri Opitz', 'Simon Clematide']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.07938,Anomali
Visual Text Mining with Progressive Taxonomy Construction for Environmental Studies,"Environmental experts have developed the DPSIR (Driver, Pressure, State, Impact, Response) framework to systematically study and communicate key relationships between society and the environment. Using this framework requires experts to construct a DPSIR taxonomy from a corpus, annotate the documents, and identify DPSIR variables and relationships, which is laborious and inflexible. Automating it with conventionaltextminingfaces technical challenges, primarily because the taxonomy often begins with abstract definitions, which experts progressively refine and contextualize as they annotate the corpus. In response, we develop GreenMine, a system that supports interactivetextminingwith prompt engineering. The system implements a prompting pipeline consisting of three simple and evaluable subtasks. In each subtask, the DPSIR taxonomy can be defined in natural language and iteratively refined as experts analyze the corpus. To support users evaluate the taxonomy, we introduce an uncertainty score based on response consistency. Then, we design a radial uncertainty chart that visualizes uncertainties and corpus topics, which supports interleaved evaluation and exploration. Using the system, experts can progressively construct the DPSIR taxonomy and annotate the corpus with LLMs. Using real-world interview transcripts, we present a case study to demonstrate the capability of the system in supporting interactiveminingof DPSIR relationships, and an expert review in the form of collaborative discussion to understand the potential and limitations of the system. We discuss the lessons learned from developing the system and future opportunities for supporting interactivetextminingin knowledge-intensive tasks for other application scenarios.","['Sam Yu-Te Lee', 'Cheng-Wei Hung', 'Mei-Hua Yuan', 'Kwan-Liu Ma']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.05731,Anomali
Toward Copyright Integrity and Verifiability via Multi-Bit Watermarking for Intelligent Transportation Systems,"Intelligent transportation systems (ITS) use advanced technologies such as artificial intelligence to significantly improve traffic flow management efficiency, and promote the intelligent development of the transportation industry. However, if the data in ITS is attacked, such as tampering or forgery, it will endanger public safety and cause social losses. Therefore, this paper proposes a watermarking that can verify the integrity of copyright in response to the needs of ITS, termed ITSmark. ITSmark focuses on functions such as extracting watermarks, verifying permission, and tracing tampered locations. The scheme uses the copyright information to build the multi-bit space and divides this space into multiple segments. These segments will be assigned to tokens. Thus, the next token is determined by its segment which contains the copyright. In this way, the obtained data contains the custom watermark. To ensure the authorization, key parameters are encrypted during copyright embedding to obtain cipher data. Only by possessing the correct cipher data and private key, can the user entirely extract the watermark. Experiments show that ITSmark surpasses baseline performances in data quality, extraction accuracy, and unforgeability. It also shows unique capabilities of permission verification and tampered location tracing, which ensures the security of extraction and the reliability of copyright verification. Furthermore, ITSmark can also customize the watermark embedding position and proportion according to user needs, making embedding more flexible.","['Yihao Wang', 'Lingxiao Li', 'Yifan Tang', 'Ru Zhang', 'Jianyi Liu']","IEEE Transactions on Intelligent Transportation Systems, 07 February 2025",arXiv,2025,https://doi.org/10.48550/arXiv.2502.05425,Anomali
Mining Unstructured Medical Texts With Conformal Active Learning,"The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructuredtextin EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution forminingdata from unstructuredtexts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeledtexts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats.","['Juliano Genari', 'Guilherme Tegoni Goedert']",,arXiv,2025,https://doi.org/10.48550/arXiv.2502.04372,Anomali
An Empirical Study on the Impact of Code Duplication-aware Refactoring Practices on Quality Metrics,"Context: Code refactoring is widely recognized as an essential software engineering practice that improves the understandability and maintainability of source code. Several studies attempted to detect refactoring activities throughminingsoftware repositories, allowing one to collect, analyze, and get actionable data-driven insights about refactoring practices within software projects. Objective: Our goal is to identify, among the various quality models presented in the literature, the ones that align with the developer's vision of eliminating duplicates of code, when they explicitly mention that they refactor the code to improve them. Method: We extract a corpus of 332 refactoring commits applied and documented by developers during their daily changes from 128 open-source Java projects. In particular, we extract 32 structural metrics from which we identify code duplicate removal commits with their corresponding refactoring operations, as perceived by software engineers. Thereafter, we empirically analyze the impact of these refactoring operations on a set of common state-of-the-art design quality metrics. Results: The statistical analysis of the results obtained shows that (i) some state-of-the-art metrics are capable of capturing the developer's intention of removing code duplication; and (ii) some metrics are being more emphasized than others. We confirm that various structural metrics can effectively represent code duplication, leading to different impacts on software quality. Some metrics contribute to improvements, while others may lead to degradation. Conclusion: Most of the mapped metrics associated with the main quality attributes successfully capture developers' intentions for removing code duplicates, as is evident from the commit messages. However, certain metrics do not fully capture these intentions",['Eman Abdullah AlOmar'],,arXiv,2025,https://doi.org/10.48550/arXiv.2502.04073,Anomali
Cross-Lingual Transfer for Low-Resource Natural Language Processing,"Natural Language Processing (NLP) has seen remarkable advances in recent years, particularly with the emergence of Large Language Models that have achieved unprecedented performance across many tasks. However, these developments have mainly benefited a small number of high-resource languages such as English. The majority of languages still face significant challenges due to the scarcity of training data and computational resources. To address this issue, this thesis focuses on cross-lingual transfer learning, a research area aimed at leveraging data and models from high-resource languages to improve NLP performance for low-resource languages. Specifically, we focus on Sequence Labeling tasks such as Named Entity Recognition, Opinion Target Extraction, and ArgumentMining.
  The research is structured around three main objectives: (1) advancing data-based cross-lingual transfer learning methods through improved translation and annotation projection techniques, (2) developing enhanced model-based transfer learning approaches utilizing state-of-the-art multilingual models, and (3) applying these methods to real-world problems while creating open-source resources that facilitate future research in low-resource NLP.
  More specifically, this thesis presents a new method to improve data-based transfer with T-Projection, a state-of-the-art annotation projection method that leveragestext-to-textmultilingual models and machine translation systems. T-Projection significantly outperforms previous annotation projection methods by a wide margin. For model-based transfer, we introduce a constrained decoding algorithm that enhances cross-lingual Sequence Labeling in zero-shot settings usingtext-to-textmodels. Finally, we develop Medical mT5, the first multilingualtext-to-textmedical model, demonstrating the practical impact of our research on real-world applications.",['Iker García-Ferrero'],,arXiv,2025,https://doi.org/10.48550/arXiv.2502.02722,Anomali
Enhancing Aspect-based Sentiment Analysis with ParsBERT in Persian Language,"In the era of pervasive internet use and the dominance of social networks, researchers face significant challenges in Persiantextminingincluding the scarcity of adequate datasets in Persian and the inefficiency of existing language models. This paper specifically tackles these challenges, aiming to amplify the efficiency of language models tailored to the Persian language. Focusing on enhancing the effectiveness of sentiment analysis, our approach employs an aspect-based methodology utilizing the ParsBERT model, augmented with a relevant lexicon. The study centers on sentiment analysis of user opinions extracted from the Persian website 'Digikala.' The experimental results not only highlight the proposed method's superior semantic capabilities but also showcase its efficiency gains with an accuracy of 88.2% and an F1 score of 61.7. The importance of enhancing language models in this context lies in their pivotal role in extracting nuanced sentiments from user-generated content, ultimately advancing the field of sentiment analysis in Persiantextminingby increasing efficiency and accuracy.","['Farid Ariai', 'Maryam Tayefeh Mahmoudi', 'Ali Moeini']","Journal of AI and Data Mining, 2024, 12(1): 1-14",arXiv,2025,https://doi.org/10.48550/arXiv.2502.01091,Anomali
Patterns and Purposes: A Cross-Journal Analysis of AI Tool Usage in Academic Writing,"This study investigates the use of AI tools in academic writing through analysis of AI usage declarations in journals. Using a mixed-methods approach combining content analysis, statistical analysis, andtextmining, this research analyzed 168 AI declarations from 8,859 articles across 27 categories. Results show that ChatGPT dominates academic writing assistance (77% usage), with significant differences in tool usage between native and non-native English speakers (p = 0.0483) and between international and non-international teams (p = 0.0012). The study reveals that improving readability (51%) and grammar checking (22%) are the primary purposes of AI tool usage. These findings provide insights for journal policy development and understanding the evolving role of AI in academic writing.",['Ziyang Xu'],,arXiv,2025,https://doi.org/10.48550/arXiv.2502.00632,Anomali
"Mining for Species, Locations, Habitats, and Ecosystems from Scientific Papers in Invasion Biology: A Large-Scale Exploratory Study with Large Language Models","This paper presents an exploratory study that harnesses the capabilities of large language models (LLMs) tominekey ecological entities from invasion biology literature. Specifically, we focus on extracting species names, their locations, associated habitats, and ecosystems, information that is critical for understanding species spread, predicting future invasions, and informing conservation efforts. Traditionaltextminingapproaches often struggle with the complexity of ecological terminology and the subtle linguistic patterns found in thesetexts. By applying general-purpose LLMs without domain-specific fine-tuning, we uncover both the promise and limitations of using these models for ecological entity extraction. In doing so, this study lays the groundwork for more advanced, automated knowledge extraction tools that can aid researchers and practitioners in understanding and managing biological invasions.","[""Jennifer D'Souza"", 'Zachary Laubach', 'Tarek Al Mustafa', 'Sina Zarrieß', 'Robert Frühstückl', 'Phyllis Illari']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.18287,Anomali
Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning,"In real-world scientific discovery, human beings always make use of the accumulated prior knowledge with imagination pick select one or a few most promising hypotheses from large and noisy data analysis results. In this study, we introduce a new type of graph structure, thetext-numeric graph (TNG), which is defined as graph entities and associations have bothtext-attributed information and numeric information. The TNG is an ideal data structure model for novel scientific discovery via graph reasoning because it integrates human-understandable textual annotations or prior knowledge, with numeric values that represent the observed or activation levels of graph entities or associations in different samples. Together both the textual information and numeric values determine the importance of graph entities and associations in graph reasoning for novel scientific knowledge discovery. We further propose integrating large language models (LLMs) and graph neural networks (GNNs) to analyze the TNGs for graph understanding and reasoning. To demonstrate the utility, we generated thetext-omic(numeric) signaling graphs (TOSG), as one type of TNGs, in which all graphs have the same entities, associations and annotations, but have sample-specific entity numeric (omic) values using single cell RNAseq (scRNAseq) datasets of different diseases. We proposed joint LLM-GNN models for key entityminingand signaling pathwayminingon the TOSGs. The evaluation results showed the LLM-GNN and TNGs models significantly improve classification accuracy and network inference. In conclusion, the TNGs and joint LLM-GNN models are important approaches for scientific discovery.","['Haoran Song', 'Jiarui Feng', 'Guangfu Li', 'Michael Province', 'Philip Payne', 'Yixin Chen', 'Fuhai Li']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.16361,Anomali
Reliable Pseudo-labeling via Optimal Transport with Attention for Short Text Clustering,"Shorttextclustering has gained significant attention in the dataminingcommunity. However, the limited valuable information contained in shorttextsoften leads to low-discriminative representations, increasing the difficulty of clustering. This paper proposes a novel shorttextclustering framework, called Reliable \textbf{P}seudo-labeling via \textbf{O}ptimal \textbf{T}ransport with \textbf{A}ttention for ShortTextClustering (\textbf{POTA}), that generate reliable pseudo-labels to aid discriminative representation learning for clustering. Specially, \textbf{POTA} first implements an instance-level attention mechanism to capture the semantic relationships among samples, which are then incorporated as a semantic consistency regularization term into an optimal transport problem. By solving this OT problem, we can yield reliable pseudo-labels that simultaneously account for sample-to-sample semantic consistency and sample-to-cluster global structure information. Additionally, the proposed OT can adaptively estimate cluster distributions, making \textbf{POTA} well-suited for varying degrees of imbalanced datasets. Then, we utilize the pseudo-labels to guide contrastive learning to generate discriminative representations and achieve efficient clustering. Extensive experiments demonstrate \textbf{POTA} outperforms state-of-the-art methods. The code is available at: \href{https://github.com/YZH0905/POTA-STC/tree/main}{https://github.com/YZH0905/POTA-STC/tree/main}.","['Zhihao Yao', 'Jixuan Yin', 'Bo Li']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.15194,Anomali
Multi-Modality Collaborative Learning for Sentiment Analysis,"Multimodal sentiment analysis (MSA) identifies individuals' sentiment states in videos by integrating visual, audio, andtextmodalities. Despite progress in existing methods, the inherent modality heterogeneity limits the effective capture of interactive sentiment features across modalities. In this paper, by introducing a Multi-Modality Collaborative Learning (MMCL) framework, we facilitate cross-modal interactions and capture enhanced and complementary features from modality-common and modality-specific representations, respectively. Specifically, we design a parameter-free decoupling module and separate uni-modality into modality-common and modality-specific components through semantics assessment of cross-modal elements. For modality-specific representations, inspired by the act-reward mechanism in reinforcement learning, we design policy models to adaptivelyminecomplementary sentiment features under the guidance of a joint reward. For modality-common representations, intra-modal attention is employed to highlight crucial components, playing enhanced roles among modalities. Experimental results, including superiority evaluations on four databases, effectiveness verification of each module, and assessment of complementary features, demonstrate that MMCL successfully learns collaborative features across modalities and significantly improves performance. The code can be available at https://github.com/smwanghhh/MMCL.","['Shanmin Wang', 'Chengguang Liu', 'Qingshan Liu']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.12424,Anomali
KPL: Training-Free Medical Knowledge Mining of Vision-Language Models,"Visual Language Models such as CLIP excel in image recognition due to extensive image-textpre-training. However, applying the CLIP inference in zero-shot classification, particularly for medical image diagnosis, faces challenges due to: 1) the inadequacy of representing image classes solely with single category names; 2) the modal gap between the visual andtextspaces generated by CLIP encoders. Despite attempts to enrich disease descriptions with large language models, the lack of class-specific knowledge often leads to poor performance. In addition, empirical evidence suggests that existing proxy learning methods for zero-shot image classification on natural image datasets exhibit instability when applied to medical datasets. To tackle these challenges, we introduce the Knowledge Proxy Learning (KPL) tomineknowledge from CLIP. KPL is designed to leverage CLIP's multimodal understandings for medical image classification throughTextProxy Optimization and Multimodal Proxy Learning. Specifically, KPL retrieves image-relevant knowledge descriptions from the constructed knowledge-enhanced base to enrich semantictextproxies. It then harnesses input images and these descriptions, encoded via CLIP, to stably generate multimodal proxies that boost the zero-shot classification performance. Extensive experiments conducted on both medical and natural image datasets demonstrate that KPL enables effective zero-shot image classification, outperforming all baselines. These findings highlight the great potential in this paradigm ofminingknowledge from CLIP for medical image classification and broader areas.","['Jiaxiang Liu', 'Tianxiang Hu', 'Jiawei Du', 'Ruiyuan Zhang', 'Joey Tianyi Zhou', 'Zuozhu Liu']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.11231,Anomali
A Multi-tiered Solution for Personalized Baggage Item Recommendations using FastText and Association Rule Mining,"This paper introduces an intelligent baggage item recommendation system to optimize packing for air travelers by providing tailored suggestions based on specific travel needs and destinations. Using FastText word embeddings and Association RuleMining(ARM), the system ensures efficient luggage space utilization, compliance with weight limits, and an enhanced travel experience. The methodology comprises four phases: (1) data collection and preprocessing with pre-trained FastText embeddings fortextrepresentation and similarity scoring (2) a content-based recommendation system enriched by user search history (3) application of ARM to user interactions to uncover meaningful item associations and (4) integration of FastText and ARM for accurate, personalized recommendations. Performance is evaluated using metrics such as coverage, support, confidence, lift, leverage, and conviction. Results demonstrate the system's effectiveness in providing relevant suggestions, improving customer satisfaction, and simplifying the packing process. These insights advance personalized recommendations, targeted marketing, and product optimization in air travel and beyond.","['Mudavath Ravi', 'Atul Negi']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.09359,Anomali
How Far are App Secrets from Being Stolen? A Case Study on Android,"Android apps can hold secret strings of themselves such as cloud service credentials or encryption keys. Leakage of such secret strings can induce unprecedented consequences like monetary losses or leakage of user private information. In practice, various security issues were reported because many apps failed to protect their secrets. However, little is known about the types, usages, exploitability, and consequences of app secret leakage issues. While a large body of literature has been devoted to studying user private information leakage, there is no systematic study characterizing app secret leakage issues. How far are Android app secrets from being stolen?
  To bridge this gap, we conducted the first systematic study to characterize app secret leakage issues in Android apps based on 575 potential app secrets sampled from 14,665 popular Android apps on Google Play. We summarized the common categories of leaked app secrets, assessed their security impacts and disclosed app bad practices in storing app secrets. We devised atextminingstrategy using regular expressions and demonstrated that numerous app secrets can be easily stolen, even from the highly popular Android apps on Google. In a follow-up study, we harvested 3,711 distinct exploitable app secrets through automatic analysis. Our findings highlight the prevalence of this problem and call for greater attention to app secret protection.","['Lili Wei', 'Heqing Huang', 'Shing-Chi Cheung', 'Kevin Li']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.07805,Anomali
Recommending the right academic programs: An interest mining approach using BERTopic,"Prospective students face the challenging task of selecting a university program that will shape their academic and professional careers. For decision-makers and support services, it is often time-consuming and extremely difficult to match personal interests with suitable programs due to the vast and complex catalogue information available. This paper presents the first information system that provides students with efficient recommendations based on both program content and personal preferences. BERTopic, a powerful topic modeling algorithm, is used that leveragestextembedding techniques to generate topic representations. It enables us tomineinterest topics from all course descriptions, representing the full body of knowledge taught at the institution. Underpinned by the student's individual choice of topics, a shortlist of the most relevant programs is computed through statistical backtracking in the knowledge map, a novel characterization of the program-course relationship. This approach can be applied to a wide range of educational settings, including professional and vocational training. A case study at a post-secondary school with 80 programs and over 5,000 courses shows that the system provides immediate and effective decision support. The presented interest topics are meaningful, leading to positive effects such as serendipity, personalization, and fairness, as revealed by a qualitative study involving 65 students. Over 98% of users indicated that the recommendations aligned with their interests, and about 94% stated they would use the tool in the future. Quantitative analysis shows the system can be configured to ensure fairness, achieving 98% program coverage while maintaining a personalization score of 0.77. These findings suggest that this real-time, user-centered, data-driven system could improve the program selection process.","['Alessandro Hill', 'Kalen Goo', 'Puneet Agarwal']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.06581,Anomali
Biomedical Relation Extraction via Adaptive Document-Relation Cross-Mapping and Concept Unique Identifier,"Document-Level Biomedical Relation Extraction (Bio-RE) aims to identify relations between biomedical entities within extensivetexts, serving as a crucial subfield of biomedicaltextmining. Existing Bio-RE methods struggle with cross-sentence inference, which is essential for capturing relations spanning multiple sentences. Moreover, previous methods often overlook the incompleteness of documents and lack the integration of external knowledge, limiting contextual richness. Besides, the scarcity of annotated data further hampers model training. Recent advancements in large language models (LLMs) have inspired us to explore all the above issues for document-level Bio-RE. Specifically, we propose a document-level Bio-RE framework via LLM Adaptive Document-Relation Cross-Mapping (ADRCM) Fine-Tuning and Concept Unique Identifier (CUI) Retrieval-Augmented Generation (RAG). First, we introduce the Iteration-of-REsummary (IoRs) prompt for solving the data scarcity issue. In this way, Bio-RE task-specific synthetic data can be generated by guiding ChatGPT to focus on entity relations and iteratively refining synthetic data. Next, we propose ADRCM fine-tuning, a novel fine-tuning recipe that establishes mappings across different documents and relations, enhancing the model's contextual understanding and cross-sentence inference capabilities. Finally, during the inference, a biomedical-specific RAG approach, named CUI RAG, is designed to leverage CUIs as indexes for entities, narrowing the retrieval scope and enriching the relevant document contexts. Experiments conducted on three Bio-RE datasets (GDA, CDR, and BioRED) demonstrate the state-of-the-art performance of our proposed method by comparing it with other related works.","['Yufei Shang', 'Yanrong Guo', 'Shijie Hao', 'Richang Hong']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.05155,Anomali
Music and art: a study in cross-modal interpretation,"Our study has investigated the effect of music on the experience of viewing art, investigating the factors which create a sense of connectivity between the two forms. We worked with 138 participants, and included multiple choice and open-ended questions. For the latter, we performed both a qualitative analysis and also sentiment analysis usingtext-mining. We investigated the relationship between the user experience and the emotions in the artwork and music. We found that, besides emotion, theme, story, and to a lesser extent music tempo were factors which helped form connections between artwork and music. Overall, participants rated the music as being helpful in developing an appreciation of the art. We propose guidelines for using music to enhance the experience of viewing art, and we propose directions for future research.","['Paul Warren', 'Paul Mulholland', 'Naomi Barker']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.05101,Anomali
Spherical Double K-Means: a co-clustering approach for text data analysis,"Intextanalysis, Spherical K-means (SKM) is a specialized k-means clustering algorithm widely utilized for grouping documents represented in high-dimensional, sparse term-document matrices, often normalized using techniques like TF-IDF. Researchers frequently seek to cluster not only documents but also the terms associated with them into coherent groups. To address this dual clustering requirement, we introduce Spherical Double K-Means (SDKM), a novel methodology that simultaneously clusters documents and terms. This approach offers several advantages: first, by integrating the clustering of documents and terms, SDKM provides deeper insights into the relationships between content and vocabulary, enabling more effective topic identification and keyword extraction. Additionally, the two-level clustering assists in understanding both overarching themes and specific terminologies within document clusters, enhancing interpretability. SDKM effectively handles the high dimensionality and sparsity inherent intextdata by utilizing cosine similarity, leading to improved computational efficiency. Moreover, the method captures dynamic changes in thematic content over time, making it well-suited for applications in rapidly evolving fields. Ultimately, SDKM presents a comprehensive framework for advancingtextminingefforts, facilitating the uncovering of nuanced patterns and structures that are critical for robust data analysis. We apply SDKM to the corpus of US presidential inaugural addresses, spanning from George Washington in 1789 to Joe Biden in 2021. Our analysis reveals distinct clusters of words and documents that correspond to significant historical themes and periods, showcasing the method's ability to facilitate a deeper understanding of the data. Our findings demonstrate the efficacy of SDKM in uncovering underlying patterns in textual data.","['Ilaria Bombelli', 'Domenica Fioredistella Iezzi', 'Emiliano Seri', 'Maurizio Vichi']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.04562,Anomali
Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text Classification,"Textclassification is a fundamental task in datamining, pivotal to various applications such as tabular understanding and recommendation. Although neural network-based models, such as CNN and BERT, have demonstrated remarkable performance intextclassification, their effectiveness heavily relies on abundant labeled training data. This dependency makes these models less effective in dynamic few-shottextclassification, where labeled data is scarce, and new target labels frequently appear based on application needs. Recently, large language models (LLMs) have shown promise due to their extensive pretraining and contextual understanding ability. Current approaches provide LLMs withtextinputs, candidate labels, and additional side information (e.g., descriptions) to classifytexts. However, their effectiveness is hindered by the increased input size and the noise introduced through side information processing. To address these limitations, we propose a graph-based online retrieval-augmented generation framework, namely GORAG, for dynamic few-shottextclassification. Rather than treating each input independently, GORAG constructs and maintains a weighted graph by extracting side information across all targettexts. In this graph,textkeywords and labels are represented as nodes, with edges indicating the correlations between them. To model these correlations, GORAG employs an edge weighting mechanism to prioritize the importance and reliability of extracted information and dynamically retrieves relevant context using a minimum-cost spanning tree tailored for eachtextinput. Empirical evaluations demonstrate that GORAG outperforms existing approaches by providing more comprehensive and precise contextual information.","['Yubo Wang', 'Haoyang Li', 'Fei Teng', 'Lei Chen']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.02844,Anomali
Applying Text Mining to Analyze Human Question Asking in Creativity Research,"Creativity relates to the ability to generate novel and effective ideas in the areas of interest. How are such creative ideas generated? One possible mechanism that supports creative ideation and is gaining increased empirical attention is by asking questions. Question asking is a likely cognitive mechanism that allows defining problems, facilitating creative problem solving. However, much is unknown about the exact role of questions in creativity. This work presents an attempt to applytextminingmethods to measure the cognitive potential of questions, taking into account, among others, (a) question type, (b) question complexity, and (c) the content of the answer. This contribution summarizes the history of questionminingas a part of creativity research, along with the natural language processing methods deemed useful or helpful in the study. In addition, a novel approach is proposed, implemented, and applied to five datasets. The experimental results obtained are comprehensively analyzed, suggesting that natural language processing has a role to play in creative research.","['Anna Wróblewska', 'Marceli Korbin', 'Yoed N. Kenett', 'Daniel Dan', 'Maria Ganzha', 'Marcin Paprzycki']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.02090,Anomali
"Analyzing Aviation Safety Narratives with LDA, NMF and PLSA: A Case Study Using Socrata Datasets","This study explores the application of topic modelling techniques Latent Dirichlet Allocation (LDA), Nonnegative Matrix Factorization (NMF), and Probabilistic Latent Semantic Analysis (PLSA) on the Socrata dataset spanning from 1908 to 2009. Categorized by operator type (military, commercial, and private), the analysis identified key themes such as pilot error, mechanical failure, weather conditions, and training deficiencies. The study highlights the unique strengths of each method: LDA ability to uncover overlapping themes, NMF production of distinct and interpretable topics, and PLSA nuanced probabilistic insights despite interpretative complexity. Statistical analysis revealed that PLSA achieved a coherence score of 0.32 and a perplexity value of -4.6, NMF scored 0.34 and 37.1, while LDA achieved the highest coherence of 0.36 but recorded the highest perplexity at 38.2. These findings demonstrate the value of topic modelling in extracting actionable insights from unstructured aviation safety narratives, aiding in the identification of risk factors and areas for improvement across sectors. Future directions include integrating additional contextual variables, leveraging neural topic models, and enhancing aviation safety protocols. This research provides a foundation for advancedtext-miningapplications in aviation safety management.","['Aziida Nanyonga', 'Graham Wild']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.01690,Anomali
Multi-Modal Video Feature Extraction for Popularity Prediction,"This work aims to predict the popularity of short videos using the videos themselves and their related features. Popularity is measured by four key engagement metrics: view count, like count, comment count, and share count. This study employs video classification models with different architectures and training methods as backbone networks to extract video modality features. Meanwhile, the cleaned video captions are incorporated into a carefully designed prompt framework, along with the video, as input for video-to-textgeneration models, which generate detailedtext-based video content understanding. Thesetextsare then encoded into vectors using a pre-trained BERT model. Based on the six sets of vectors mentioned above, a neural network is trained for each of the four prediction metrics. Moreover, the study conducts dataminingand feature engineering based on the video and tabular data, constructing practical features such as the total frequency of hashtag appearances, the total frequency of mention appearances, video duration, frame count, frame rate, and total time online. Multiple machine learning models are trained, and the most stable model, XGBoost, is selected. Finally, the predictions from the neural network and XGBoost models are averaged to obtain the final result.","['Haixu Liu', 'Wenning Wang', 'Haoxiang Zheng', 'Penghao Jiang', 'Qirui Wang', 'Ruiqing Yan', 'Qiuzhuang Sun']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.01422,Anomali
Generative AI and LLMs in Industry: A text-mining Analysis and Critical Evaluation of Guidelines and Policy Statements Across Fourteen Industrial Sectors,"The rise of Generative AI (GAI) and Large Language Models (LLMs) has transformed industrial landscapes, offering unprecedented opportunities for efficiency and innovation while raising critical ethical, regulatory, and operational challenges. This study conducts atext-based analysis of 160 guidelines and policy statements across fourteen industrial sectors, utilizing systematic methods andtext-miningtechniques to evaluate the governance of these technologies. By examining global directives, industry practices, and sector-specific policies, the paper highlights the complexities of balancing innovation with ethical accountability and equitable access. The findings provide actionable insights and recommendations for fostering responsible, transparent, and safe integration of GAI and LLMs in diverse industry contexts.","['Junfeng Jiao', 'Saleh Afroogh', 'Kevin Chen', 'David Atkinson', 'Amit Dhurandhar']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.00957,Anomali
The Text Classification Pipeline: Starting Shallow going Deeper,"Textclassification stands as a cornerstone within the realm of Natural Language Processing (NLP), particularly when viewed through computer science and engineering. The past decade has seen deep learning revolutionizetextclassification, propelling advancements intextretrieval, categorization, information extraction, and summarization. The scholarly literature includes datasets, models, and evaluation criteria, with English being the predominant language of focus, despite studies involving Arabic, Chinese, Hindi, and others. The efficacy oftextclassification models relies heavily on their ability to capture intricate textual relationships and non-linear correlations, necessitating a comprehensive examination of the entiretextclassification pipeline.
  In the NLP domain, a plethora oftextrepresentation techniques and model architectures have emerged, with Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs) at the forefront. These models are adept at transforming extensive textual data into meaningful vector representations encapsulating semantic information. The multidisciplinary nature oftextclassification, encompassing datamining, linguistics, and information retrieval, highlights the importance of collaborative research to advance the field. This work integrates traditional and contemporarytextminingmethodologies, fostering a holistic understanding oftextclassification.","['Marco Siino', 'Ilenia Tinnirello', 'Marco La Cascia']",,arXiv,2025,https://doi.org/10.48550/arXiv.2501.00174,Anomali
Integrating Natural Language Processing Techniques of Text Mining Into Financial System: Applications and Limitations,"The financial sector, a pivotal force in economic development, increasingly uses the intelligent technologies such as natural language processing to enhance data processing and insight extraction. This research paper through a review process of the time span of 2018-2023 explores the use oftextminingas natural language processing techniques in various components of the financial system including asset pricing, corporate finance, derivatives, risk management, and public finance and highlights the need to address the specific problems in the discussion section. We notice that most of the research materials combined probabilistic with vector-space models, andtext-data with numerical ones. The most used technique regarding information processing is the information classification technique and the most used algorithms include the long-short term memory and bidirectional encoder models. The research noticed that new specific algorithms are developed and the focus of the financial system is mainly on asset pricing component. The research also proposes a path from engineering perspective for researchers who need to analyze financialtext. The challenges regardingtextminingperspective such as data quality, context-adaption and model interpretability need to be solved so to integrate advanced natural language processing models and techniques in enhancing financial analysis and prediction. Keywords: Financial System (FS), Natural Language Processing (NLP), Software andTextEngineering, Probabilistic, Vector-Space, Models, Techniques, TextData, Financial Analysis.","['Denisa Millo', 'Blerina Vika', 'Nevila Baci']",International Journal on Technical and Physical Problems of Engineering (IJTPE); Published by International Organization of IOTPE;December 2024;Issue 61;Volume 16; Number 4;Pages 1-6,arXiv,2024,https://doi.org/10.48550/arXiv.2412.20438,Anomali
Left-handed representation in top 100 male professional tennis players: Multi-disciplinary perspectives,"A commonly held opinion is that left-handed tennis players are overrepresented compared to the percentage of left-handers within the general population. This study provides the domain insights supported by data analysis that could help inform the decision of parents and coaches considering whether a child should start playing tennis as left- or right-handed when there is no strong arm-handed dominance. Compared to the commonly cited figure of about 10% of left-handed male population, data analysis from the official ATP web site for the top 100 ranked tennis players over the past decades (1985-2016) shows evidence of overrepresentation of left-handed elite tennis players (about 15%). The insights and data analysis can inform the handedness decision, advance coaching and strategic game concepts, enhance media coverage/analytics, left-handed facts and statistics, and inform tennis equipment manufacturing.","['Boris Bačić', 'Ali Ghazala']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.20360,Anomali
Text-Driven Tumor Synthesis,"Tumor synthesis can generate examples that AI often misses or over-detects, improving AI performance by training on these challenging cases. However, existing synthesis methods, which are typically unconditional -- generating images from random variables -- or conditioned only by tumor shapes, lack controllability over specific tumor characteristics such as texture, heterogeneity, boundaries, and pathology type. As a result, the generated tumors may be overly similar or duplicates of existing training data, failing to effectively address AI's weaknesses. We propose a newtext-driven tumor synthesis approach, termed TextoMorph, that provides textual control over tumor characteristics. This is particularly beneficial for examples that confuse the AI the most, such as early tumor detection (increasing Sensitivity by +8.5%), tumor segmentation for precise radiotherapy (increasing DSC by +6.3%), and classification between benign and malignant tumors (improving Sensitivity by +8.2%). By incorporatingtextminedfrom radiology reports into the synthesis process, we increase the variability and controllability of the synthetic tumors to target AI's failure cases more precisely. Moreover, TextoMorph uses contrastive learning across differenttextsand CT scans, significantly reducing dependence on scarce image-report pairs (only 141 pairs used in this study) by leveraging a large corpus of 34,035 radiology reports. Finally, we have developed rigorous tests to evaluate synthetic tumors, includingText-Driven Visual Turing Test and Radiomics Pattern Analysis, showing that our synthetic tumors is realistic and diverse in texture, heterogeneity, boundaries, and pathology.","['Xinran Li', 'Yi Shuai', 'Chen Liu', 'Qi Chen', 'Qilong Wu', 'Pengfei Guo', 'Dong Yang', 'Can Zhao', 'Pedro R. A. S. Bassi', 'Daguang Xu', 'Kang Wang', 'Yang Yang', 'Alan Yuille', 'Zongwei Zhou']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.18589,Anomali
Emoji Retrieval from Gibberish or Garbled Social Media Text: A Novel Methodology and A Case Study,"Emojis are widely used across social media platforms but are often lost in noisy or garbledtext, posing challenges for data analysis and machine learning. Conventional preprocessing approaches recommend removing suchtext, risking the loss of emojis and their contextual meaning. This paper proposes a three-step reverse-engineering methodology to retrieve emojis from garbledtextin social media posts. The methodology also identifies reasons for the generation of suchtextduring social media datamining. To evaluate its effectiveness, the approach was applied to 509,248 Tweets about the Mpox outbreak, a dataset referenced in about 30 prior works that failed to retrieve emojis from garbledtext. Our method retrieved 157,748 emojis from 76,914 Tweets. Improvements intextreadability and coherence were demonstrated through metrics such as Flesch Reading Ease, Flesch-Kincaid Grade Level, Coleman-Liau Index, Automated Readability Index, Dale-Chall Readability Score,TextStandard, and Reading Time. Additionally, the frequency of individual emojis and their patterns of usage in these Tweets were analyzed, and the results are presented.","['Shuqi Cui', 'Nirmalya Thakur', 'Audrey Poon']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.18046,Anomali
DRT: Deep Reasoning Translation via Long Chain-of-Thought,"Recently, O1-like models have emerged as representative examples, illustrating the effectiveness of long chain-of-thought (CoT) in reasoning tasks such as math and coding tasks. In this paper, we introduce DRT, an attempt to bring the success of long CoT to neural machine translation (MT). Specifically, in view of the literature books that might involve similes and metaphors, translating thesetextsto a target language is very difficult in practice due to cultural differences. In such cases, literal translation often fails to convey the intended meaning effectively. Even for professional human translators, considerable thought must be given to preserving semantics throughout the translation process. To simulate LLMs' long thought ability in MT, we firstminesentences containing similes or metaphors from existing literature books, and then develop a multi-agent framework to translate these sentences via long thought. In the multi-agent framework, a translator is used to iteratively translate the source sentence under the suggestions provided by an advisor. To ensure the effectiveness of the long thoughts, an evaluator is also employed to quantify the translation quality in each round. In this way, we collect tens of thousands of long-thought MT data, which is used to train our DRT. Using Qwen2.5 and LLama-3.1 as the backbones, DRT models can learn the thought process during machine translation, and outperform vanilla LLMs as well as LLMs which are simply fine-tuning on the paired sentences without long thought, showing its effectiveness.","['Jiaan Wang', 'Fandong Meng', 'Yunlong Liang', 'Jie Zhou']",,arXiv,2025,https://doi.org/10.48550/arXiv.2412.17498,Anomali
An Exploration of Pattern Mining with ChatGPT,"This paper takes an exploratory approach to examine the use of ChatGPT for patternmining. It proposes an eight-step collaborative process that combines human insight with AI capabilities to extract patterns from known uses. The paper offers a practical demonstration of this process by creating a pattern language for integrating Large Language Models (LLMs) with data sources and tools. LLMs, such as ChatGPT, are a new class of AI models that have been trained on large amounts oftext, and can create new content, includingtext, images, or video. The paper also argues for adding affordances of the underlying components as a new element of pattern descriptions. The primary audience of the paper includes pattern writers interested in patternminingusing LLMs.",['Michael Weiss'],,arXiv,2024,https://doi.org/10.48550/arXiv.2412.16814,Anomali
Business Analysis: User Attitude Evaluation and Prediction Based on Hotel User Reviews and Text Mining,"In the post-pandemic era, the hotel industry plays a crucial role in economic recovery, with consumer sentiment increasingly influencing market trends. This study utilizes advanced natural language processing (NLP) and the BERT model to analyze user reviews, extracting insights into customer satisfaction and guiding service improvements. By transforming reviews into feature vectors, the BERT model accurately classifies emotions, uncovering patterns of satisfaction and dissatisfaction. This approach provides valuable data for hotel management, helping them refine service offerings and improve customer experiences. From a financial perspective, understanding sentiment is vital for predicting market performance, as shifts in consumer sentiment often correlate with stock prices and overall industry performance. Additionally, the study addresses data imbalance in sentiment analysis, employing techniques like oversampling and undersampling to enhance model robustness. The results offer actionable insights not only for the hotel industry but also for financial analysts, aiding in market forecasts and investment decisions. This research highlights the potential of sentiment analysis to drive business growth, improve financial outcomes, and enhance competitive advantage in the dynamic tourism and hospitality sectors, thereby contributing to the broader economic landscape.","['Ruochun Zhao', 'Yue Hao', 'Xuechen Li']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.16744,Anomali
Research on Violent Text Detection System Based on BERT-fasttext Model,"In the digital age of today, the internet has become an indispensable platform for people's lives, work, and information exchange. However, the problem of violenttextproliferation in the network environment has arisen, which has brought about many negative effects. In view of this situation, it is particularly important to build an effective system for cutting off violenttext. The study of violenttextcutting off based on the BERT-fasttext model has significant meaning. BERT is a pre-trained language model with strong natural language understanding ability, which can deeplymineand analyzetextsemantic information; Fasttext itself is an efficienttextclassification tool with low complexity and good effect, which can quickly provide basic judgments fortextprocessing. By combining the two and applying them to the system for cutting off violenttext, on the one hand, it can accurately identify violenttext, and on the other hand, it can efficiently and reasonably cut off the content, preventing harmful information from spreading freely on the network. Compared with the single BERT model and fasttext, the accuracy was improved by 0.7% and 0.8%, respectively. Through this model, it is helpful to purify the network environment, maintain the health of network information, and create a positive, civilized, and harmonious online communication space for netizens, driving the development of social networking, information dissemination, and other aspects in a more benign direction.","['Yongsheng Yang', 'Xiaoying Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.16455,Anomali
Learning from Massive Human Videos for Universal Humanoid Pose Control,"Scalable learning of humanoid robots is crucial for their deployment in real-world applications. While traditional approaches primarily rely on reinforcement learning or teleoperation to achieve whole-body control, they are often limited by the diversity of simulated environments and the high costs of demonstration collection. In contrast, human videos are ubiquitous and present an untapped source of semantic and motion information that could significantly enhance the generalization capabilities of humanoid robots. This paper introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot poses with correspondingtext-based motion descriptions, designed to leverage this abundant data. Humanoid-X is curated through a comprehensive pipeline: dataminingfrom the Internet, video caption generation, motion retargeting of humans to humanoid robots, and policy learning for real-world deployment. With Humanoid-X, we further train a large humanoid model, UH-1, which takestextinstructions as input and outputs corresponding actions to control a humanoid robot. Extensive simulated and real-world experiments validate that our scalable training approach leads to superior generalization intext-based humanoid control, marking a significant step toward adaptable, real-world-ready humanoid robots.","['Jiageng Mao', 'Siheng Zhao', 'Siqi Song', 'Tianheng Shi', 'Junjie Ye', 'Mingtong Zhang', 'Haoran Geng', 'Jitendra Malik', 'Vitor Guizilini', 'Yue Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.14172,Anomali
Bringing Multimodality to Amazon Visual Search System,"Image to image matching has been well studied in the computer vision community. Previous studies mainly focus on training a deep metric learning model matching visual patterns between the query image and gallery images. In this study, we show that pure image-to-image matching suffers from false positives caused by matching to local visual patterns. To alleviate this issue, we propose to leverage recent advances in vision-language pretraining research. Specifically, we introduce additional image-textalignment losses into deep metric learning, which serve as constraints to the image-to-image matching loss. With additional alignments between thetext(e.g., product title) and image pairs, the model can learn concepts from both modalities explicitly, which avoids matching low-level visual features. We progressively develop two variants, a 3-tower and a 4-tower model, where the latter takes one more shorttextquery input. Through extensive experiments, we show that this change leads to a substantial improvement to the image to image matching problem. We further leveraged this model for multimodal search, which takes both image and reformulationtextqueries to improve search quality. Both offline and online experiments show strong improvements on the main metrics. Specifically, we see 4.95% relative improvement on image matching click through rate with the 3-tower model and 1.13% further improvement from the 4-tower model.","['Xinliang Zhu', 'Michael Huang', 'Han Ding', 'Jinyu Yang', 'Kelvin Chen', 'Tao Zhou', 'Tal Neiman', 'Ouye Xie', 'Son Tran', 'Benjamin Yao', 'Doug Gray', 'Anuj Bindal', 'Arnab Dhua']","Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024",arXiv,2024,https://doi.org/10.48550/arXiv.2412.13364,Anomali
Feature engineering vs. deep learning for paper section identification: Toward applications in Chinese medical literature,"Section identification is an important task for library science, especially knowledge management. Identifying the sections of a paper would help filter noise in entity and relation extraction. In this research, we studied the paper section identification problem in the context of Chinese medical literature analysis, where the subjects, methods, and results are more valuable from a physician's perspective. Based on previous studies on English literature section identification, we experiment with the effective features to use with classic machine learning algorithms to tackle the problem. It is found that Conditional Random Fields, which consider sentence interdependency, is more effective in combining different feature sets, such as bag-of-words, part-of-speech, and headings, for Chinese literature section identification. Moreover, we find that classic machine learning algorithms are more effective than generic deep learning models for this problem. Based on these observations, we design a novel deep learning model, the Structural Bidirectional Long Short-Term Memory (SLSTM) model, which models word and sentence interdependency together with the contextual information. Experiments on a human-curated asthma literature dataset show that our approach outperforms the traditional machine learning methods and other deep learning methods and achieves close to 90% precision and recall in the task. The model shows good potential for use in othertextminingtasks. The research has significant methodological and practical implications.","['Sijia Zhou', 'Xin Li']","Information Processing and Management, 2020, 57(3), 102206",arXiv,2024,https://doi.org/10.48550/arXiv.2412.11125,Anomali
Low-Resource Fast Text Classification Based on Intra-Class and Inter-Class Distance Calculation,"In recent years,textclassification methods based on neural networks and pre-trained models have gained increasing attention and demonstrated excellent performance. However, these methods still have some limitations in practical applications: (1) They typically focus only on the matching similarity between sentences. However, there exists implicit high-value information both within sentences of the same class and across different classes, which is very crucial for classification tasks. (2) Existing methods such as pre-trained language models and graph-based approaches often consume substantial memory for training andtext-graph construction. (3) Although some low-resource methods can achieve good performance, they often suffer from excessively long processing times. To address these challenges, we propose a low-resource and fasttextclassification model called LFTC. Our approach begins by constructing a compressor list for each class to fullyminethe regularity information within intra-class data. We then remove redundant information irrelevant to the target classification to reduce processing time. Finally, we compute the similarity distance betweentextpairs for classification. We evaluate LFTC on 9 publicly available benchmark datasets, and the results demonstrate significant improvements in performance and processing time, especially under limited computational and data resources, highlighting its superior advantages.","['Yanxu Mao', 'Peipei Liu', 'Tiehan Cui', 'Congying Liu', 'Datao You']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.09922,Anomali
EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM,"Significant achievements in personalization of diffusion models have been witnessed. Conventional tuning-free methods mostly encode multiple reference images by averaging their image embeddings as the injection condition, but such an image-independent operation cannot perform interaction among images to capture consistent visual elements within multiple references. Although the tuning-based Low-Rank Adaptation (LoRA) can effectively extract consistent elements within multiple images through the training process, it necessitates specific finetuning for each distinct image group. This paper introduces EasyRef, a novel plug-and-play adaptation method that enables diffusion models to be conditioned on multiple reference images and thetextprompt. To effectively exploit consistent visual elements within multiple images, we leverage the multi-image comprehension and instruction-following capabilities of the multimodal large language model (MLLM), prompting it to capture consistent visual elements based on the instruction. Besides, injecting the MLLM's representations into the diffusion process through adapters can easily generalize to unseen domains,miningthe consistent visual elements within unseen data. To mitigate computational costs and enhance fine-grained detail preservation, we introduce an efficient reference aggregation strategy and a progressive training scheme. Finally, we introduce MRBench, a new multi-reference image generation benchmark. Experimental results demonstrate EasyRef surpasses both tuning-free methods like IP-Adapter and tuning-based methods like LoRA, achieving superior aesthetic quality and robust zero-shot generalization across diverse domains.","['Zhuofan Zong', 'Dongzhi Jiang', 'Bingqi Ma', 'Guanglu Song', 'Hao Shao', 'Dazhong Shen', 'Yu Liu', 'Hongsheng Li']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.09618,Anomali
Mining Word Boundaries from Speech-Text Parallel Data for Cross-domain Chinese Word Segmentation,"Inspired by early research on exploring naturally annotated data for Chinese Word Segmentation (CWS), and also by recent research on integration of speech andtextprocessing, this work for the first time proposes to explicitlymineword boundaries from speech-textparallel data. We employ the Montreal Forced Aligner (MFA) toolkit to perform character-level alignment on speech-textdata, giving pauses as candidate word boundaries. Based on detailed analysis of collected pauses, we propose an effective probability-based strategy for filtering unreliable word boundaries. To more effectively utilize word boundaries as extra training data, we also propose a robust complete-then-train (CTT) strategy. We conduct cross-domain CWS experiments on two target domains, i.e., ZX and AISHELL2. We have annotated about 1,000 sentences as the evaluation data of AISHELL2. Experiments demonstrate the effectiveness of our proposed approach.","['Xuebin Wang', 'Lei Zhang', 'Zhenghua Li', 'Shilin Zhou', 'Chen Gong', 'Yang Hou']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.09045,Anomali
Comparative Opinion Mining in Product Reviews: Multi-perspective Prompt-based Learning,"Comparative reviews are pivotal in understanding consumer preferences and influencing purchasing decisions. Comparative Quintuple Extraction (COQE) aims to identify five key components intext: the target entity, compared entities, compared aspects, opinions on these aspects, and polarity. Extracting precise comparative information from product reviews is challenging due to nuanced language and sequential task errors in traditional methods. To mitigate these problems, we propose MTP-COQE, an end-to-end model designed for COQE. Leveraging multi-perspective prompt-based learning, MTP-COQE effectively guides the generative model in comparative opinionminingtasks. Evaluation on the Camera-COQE (English) and VCOM (Vietnamese) datasets demonstrates MTP-COQE's efficacy in automating COQE, achieving superior performance with a 1.41% higher F1 score than the previous baseline models on the English dataset. Additionally, we designed a strategy to limit the generative model's creativity to ensure the output meets expectations. We also performed data augmentation to address data imbalance and to prevent the model from becoming biased towards dominant samples.","['Hai-Yen Thi Nguyen', 'Cam-Van Thi Nguyen']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.08508,Anomali
DRUM: Learning Demonstration Retriever for Large MUlti-modal Models,"Recently, large language models (LLMs) have demonstrated impressive capabilities in dealing with new tasks with the help of in-context learning (ICL). In the study of Large Vision-Language Models (LVLMs), when implementing ICL, researchers usually adopts the naive strategies like fixed demonstrations across different samples, or selecting demonstrations directly via a visual-language embedding model. These methods does not guarantee the configured demonstrations fit the need of the LVLMs. To address this issue, we now propose a novel framework, \underline{d}emonstration \underline{r}etriever for large m\underline{u}lti-modal \underline{m}odel (DRUM), which fine-tunes the visual-language embedding model to better meet the LVLM's needs. First, we discuss the retrieval strategies for a visual-language task, assuming an embedding model is given. And we propose to concate the image andtextembeddings to enhance the retrieval performance. Second, we propose to re-rank the demonstrations retrieved by the embedding model via the LVLM's feedbacks, and calculate a list-wise ranking loss for training the embedding model. Third, we propose an iterative demonstrationminingstrategy to improve the training of the embedding model. Through extensive experiments on 3 types of visual-language tasks, 7 benchmark datasets, our DRUM framework is proven to be effective in boosting the LVLM's in-context learning performance via retrieving more proper demonstrations.","['Ellen Yi-Ge', 'Jiechao Gao', 'Wei Han', 'Wei Zhu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.07619,Anomali
Generative AI Impact on Labor Market: Analyzing ChatGPT's Demand in Job Advertisements,"The rapid advancement of Generative AI (Gen AI) technologies, particularly tools like ChatGPT, is significantly impacting the labor market by reshaping job roles and skill requirements. This study examines the demand for ChatGPT-related skills in the U.S. labor market by analyzing job advertisements collected from major job platforms between May and December 2023. Usingtextminingand topic modeling techniques, we extracted and analyzed the Gen AI-related skills that employers are hiring for. Our analysis identified five distinct ChatGPT-related skill sets: general familiarity, creative content generation, marketing, advanced functionalities (such as prompt engineering), and product development. In addition, the study provides insights into job attributes such as occupation titles, degree requirements, salary ranges, and other relevant job characteristics. These findings highlight the increasing integration of Gen AI across various industries, emphasizing the growing need for both foundational knowledge and advanced technical skills. The study offers valuable insights into the evolving demands of the labor market, as employers seek candidates equipped to leverage generative AI tools to improve productivity, streamline processes, and drive innovation.","['Mahdi Ahmadi', 'Neda Khosh Kheslat', 'Adebola Akintomide']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.07042,Anomali
Depression detection from Social Media Bangla Text Using Recurrent Neural Networks,"Emotion artificial intelligence is a field of study that focuses on figuring out how to recognize emotions, especially in the area oftextmining. Today is the age of social media which has opened a door for us to share our individual expressions, emotions, and perspectives on any event. We can analyze sentiment on social media posts to detect positive, negative, or emotional behavior toward society. One of the key challenges in sentiment analysis is to identify depressedtextfrom social mediatextthat is a root cause of mental ill-health. Furthermore, depression leads to severe impairment in day-to-day living and is a major source of suicide incidents. In this paper, we apply natural language processing techniques on Facebooktextsfor conducting emotion analysis focusing on depression using multiple machine learning algorithms. Preprocessing steps like stemming, stop word removal, etc. are used to clean the collected data, and feature extraction techniques like stylometric feature, TF-IDF, word embedding, etc. are applied to the collected dataset which consists of 983textscollected from social media posts. In the process of class prediction, LSTM, GRU, support vector machine, and Naive-Bayes classifiers have been used. We have presented the results using the primary classification metrics including F1-score, and accuracy. This work focuses on depression detection from social media posts to help psychologists to analyze sentiment from shared posts which may reduce the undesirable behaviors of depressed individuals through diagnosis and treatment.","['Sultan Ahmed', 'Salman Rakin', 'Mohammad Washeef Ibn Waliur', 'Nuzhat Binte Islam', 'Billal Hossain', 'Md. Mostofa Akbar']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.05861,Anomali
Shifting NER into High Gear: The Auto-AdvER Approach,"This paper presents a case study on the development of Auto-AdvER, a specialised named entity recognition schema and dataset fortextin the car advertisement genre. Developed with industry needs in mind, Auto-AdvER is designed to enhancetextmininganalytics in this domain and contributes a linguistically unique NER dataset. We present a schema consisting of three labels: ""Condition"", ""Historic"" and ""Sales Options"". We outline the guiding principles for annotation, describe the methodology for schema development, and show the results of an annotation study demonstrating inter-annotator agreement of 92% F1-Score. Furthermore, we compare the performance by using encoder-only models: BERT, DeBERTaV3 and decoder-only open and closed source Large Language Models (LLMs): Llama, Qwen, GPT-4 and Gemini. Our results show that the class of LLMs outperforms the smaller encoder-only models. However, the LLMs are costly and far from perfect for this task. We present this work as a stepping stone toward more fine-grained analysis and discuss Auto-AdvER's potential impact on advertisement analytics and customer insights, including applications such as the analysis of market dynamics and data-driven predictive maintenance. Our schema, as well as our associated findings, are suitable for both private and public entities considering named entity recognition in the automotive domain, or other specialist domains.","['Filippos Ventirozos', 'Ioanna Nteka', 'Tania Nandy', 'Jozef Baca', 'Peter Appleby', 'Matthew Shardlow']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.05655,Anomali
Linq-Embed-Mistral Technical Report,"This report explores the enhancement oftextretrieval performance using advanced data refinement techniques. We develop Linq-Embed-Mistral\footnote{\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}} by building on the E5-mistral and Mistral-7B-v0.1 models, focusing on sophisticated data crafting, data filtering, and negativeminingmethods, which are highly tailored to each task, applied to both existing benchmark dataset and highly tailored synthetic dataset generated via large language models (LLMs). Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024), achieving an average score of 68.2 across 56 datasets, and ranks 1st among all models for retrieval tasks on the MTEB leaderboard with a performance score of 60.2. This performance underscores its superior capability in enhancing search precision and reliability. Our contributions include advanced data refinement methods that significantly improve model performance on benchmark and synthetic datasets, techniques for homogeneous task ordering and mixed task fine-tuning to enhance model generalization and stability, and a streamlined evaluation process using 4-bit precision and a light retrieval evaluation set, which accelerates validation without sacrificing accuracy.","['Chanyeol Choi', 'Junseong Kim', 'Seolhwa Lee', 'Jihoon Kwon', 'Sangmo Gu', 'Yejin Kim', 'Minkyung Cho', 'Jy-yong Sohn']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.03223,Anomali
"Deep Learning, Machine Learning, Advancing Big Data Analytics and Management","Advancements in artificial intelligence, machine learning, and deep learning have catalyzed the transformation of big data analytics and management into pivotal domains for research and application. This work explores the theoretical foundations, methodological advancements, and practical implementations of these technologies, emphasizing their role in uncovering actionable insights from massive, high-dimensional datasets. The study presents a systematic overview of data preprocessing techniques, including data cleaning, normalization, integration, and dimensionality reduction, to prepare raw data for analysis. Core analytics methodologies such as classification, clustering, regression, and anomaly detection are examined, with a focus on algorithmic innovation and scalability. Furthermore, thetextdelves into state-of-the-art frameworks for dataminingand predictive modeling, highlighting the role of neural networks, support vector machines, and ensemble methods in tackling complex analytical challenges. Special emphasis is placed on the convergence of big data with distributed computing paradigms, including cloud and edge computing, to address challenges in storage, computation, and real-time analytics. The integration of ethical considerations, including data privacy and compliance with global standards, ensures a holistic perspective on data management. Practical applications across healthcare, finance, marketing, and policy-making illustrate the real-world impact of these technologies. Through comprehensive case studies and Python-based implementations, this work equips researchers, practitioners, and data enthusiasts with the tools to navigate the complexities of modern data analytics. It bridges the gap between theory and practice, fostering the development of innovative solutions for managing and leveraging data in the era of artificial intelligence.","['Weiche Hsieh', 'Ziqian Bi', 'Keyu Chen', 'Benji Peng', 'Sen Zhang', 'Jiawei Xu', 'Jinlang Wang', 'Caitlyn Heqi Yin', 'Yichao Zhang', 'Pohsun Feng', 'Yizhu Wen', 'Tianyang Wang', 'Ming Li', 'Chia Xin Liang', 'Jintao Ren', 'Qian Niu', 'Silin Chen', 'Lawrence K. Q. Yan', 'Han Xu', 'Hong-Ming Tseng', 'Xinyuan Song', 'Bowen Jing', 'Junjie Yang', 'Junhao Song', 'Junyu Liu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.02187,Anomali
CoRNStack: High-Quality Contrastive Data for Better Code Retrieval and Reranking,"Effective code retrieval plays a crucial role in advancing code generation, bug fixing, and software maintenance, particularly as software systems increase in complexity. While current code embedding models have demonstrated promise in retrieving code snippets for small-scale, well-defined tasks, they often underperform in more demanding real-world applications such as bug localization within GitHub repositories. We hypothesize that a key issue is their reliance on noisy and inconsistent datasets for training, which impedes their ability to generalize to more complex retrieval scenarios. To address these limitations, we introduce CoRNStack, a large-scale, high-quality contrastive training dataset for code that spans multiple programming languages. This dataset is curated using consistency filtering to eliminate noisy positives and is further enriched withminedhard negatives, thereby facilitating more effective learning. We demonstrate that contrastive training of embedding models using CoRNStack leads to state-of-the-art performance across a variety of code retrieval tasks. Furthermore, the dataset can be leveraged for training code reranking models, a largely underexplored area compared totextreranking. Our finetuned code reranking model significantly improves the ranking quality over the retrieved results. Finally, by employing our code retriever and reranker together, we demonstrate significant improvements in function localization for GitHub issues, an important component of real-world software development.","['Tarun Suresh', 'Revanth Gangi Reddy', 'Yifei Xu', 'Zach Nussbaum', 'Andriy Mulyar', 'Brandon Duderstadt', 'Heng Ji']",,arXiv,2025,https://doi.org/10.48550/arXiv.2412.01007,Anomali
Lightweight Contenders: Navigating Semi-Supervised Text Mining through Peer Collaboration and Self Transcendence,"The semi-supervised learning (SSL) strategy in lightweight models requires reducing annotated samples and facilitating cost-effective inference. However, the constraint on model parameters, imposed by the scarcity of training labels, limits the SSL performance. In this paper, we introduce PS-NET, a novel framework tailored for semi-supervisedtextminingwith lightweight models. PS-NET incorporates online distillation to train lightweight student models by imitating the Teacher model. It also integrates an ensemble of student peers that collaboratively instruct each other. Additionally, PS-NET implements a constant adversarial perturbation schema to further self-augmentation by progressive generalizing. Our PS-NET, equipped with a 2-layer distilled BERT, exhibits notable performance enhancements over SOTA lightweight SSL frameworks of FLiText and DisCo in SSLtextclassification with extremely rare labelled data.","['Qianren Mao', 'Weifeng Jiang', 'Junnan Liu', 'Chenghua Lin', 'Qian Li', 'Xianqing Wen', 'Jianxin Li', 'Jinhu Lu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.00883,Anomali
Improving Vietnamese Legal Document Retrieval using Synthetic Data,"In the field of legal information retrieval, effective embedding-based models are essential for accurate question-answering systems. However, the scarcity of large annotated datasets poses a significant challenge, particularly for Vietnamese legaltexts. To address this issue, we propose a novel approach that leverages large language models to generate high-quality, diverse synthetic queries for Vietnamese legal passages. This synthetic data is then used to pre-train retrieval models, specifically bi-encoder and ColBERT, which are further fine-tuned using contrastive loss withminedhard negatives. Our experiments demonstrate that these enhancements lead to strong improvement in retrieval accuracy, validating the effectiveness of synthetic data and pre-training techniques in overcoming the limitations posed by the lack of large labeled datasets in the Vietnamese legal domain.","['Son Pham Tien', 'Hieu Nguyen Doan', 'An Nguyen Dai', 'Sang Dinh Viet']",,arXiv,2024,https://doi.org/10.48550/arXiv.2412.00657,Anomali
All Seeds Are Not Equal: Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds,"Text-to-image diffusion models have demonstrated remarkable capability in generating realistic images from arbitrarytextprompts. However, they often produce inconsistent results for compositional prompts such as ""two dogs"" or ""a penguin on the right of a bowl"". Understanding these inconsistencies is crucial for reliable image generation. In this paper, we highlight the significant role of initial noise in these inconsistencies, where certain noise patterns are more reliable for compositional prompts than others. Our analyses reveal that different initial random seeds tend to guide the model to place objects in distinct image areas, potentially adhering to specific patterns of camera angles and image composition associated with the seed. To improve the model's compositional ability, we propose a method forminingthese reliable cases, resulting in a curated training set of generated images without requiring any manual annotation. By fine-tuningtext-to-image models on these generated images, we significantly enhance their compositional capabilities. For numerical composition, we observe relative increases of 29.3% and 19.5% for Stable Diffusion and PixArt-α, respectively. Spatial composition sees even larger gains, with 60.7% for Stable Diffusion and 21.1% for PixArt-α.","['Shuangqi Li', 'Hieu Le', 'Jingyi Xu', 'Mathieu Salzmann']",,arXiv,2025,https://doi.org/10.48550/arXiv.2411.18810,Anomali
Interactive Visual Assessment for Text-to-Image Generation Models,"Visual generation models have achieved remarkable progress in computer graphics applications but still face significant challenges in real-world deployment. Current assessment approaches for visual generation tasks typically follow an isolated three-phase framework: test input collection, model output generation, and user assessment. These fashions suffer from fixed coverage, evolving difficulty, and data leakage risks, limiting their effectiveness in comprehensively evaluating increasingly complex generation models. To address these limitations, we propose DyEval, an LLM-powered dynamic interactive visual assessment framework that facilitates collaborative evaluation between humans and generative models fortext-to-image systems. DyEval features an intuitive visual interface that enables users to interactively explore and analyze model behaviors, while adaptively generating hierarchical, fine-grained, and diverse textual inputs to continuously probe the capability boundaries of the models based on their feedback. Additionally, to provide interpretable analysis for users to further improve tested models, we develop a contextual reflection module thatminesfailure triggers of test inputs and reflects model potential failure patterns supporting in-depth analysis using the logical reasoning ability of LLM. Qualitative and quantitative experiments demonstrate that DyEval can effectively help users identify max up to 2.56 times generation failures than conventional methods, and uncover complex and rare failure patterns, such as issues with pronoun generation and specific cultural context generation. Our framework provides valuable insights for improving generative models and has broad implications for advancing the reliability and capabilities of visual generation systems across various domains.","['Xiaoyue Mi', 'Fan Tang', 'Juan Cao', 'Qiang Sheng', 'Ziyao Huang', 'Peng Li', 'Yang Liu', 'Tong-Yee Lee']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.15509,Anomali
Cross-Modal Pre-Aligned Method with Global and Local Information for Remote-Sensing Image and Text Retrieval,"Remote sensing cross-modaltext-image retrieval (RSCTIR) has gained attention for its utility in informationmining. However, challenges remain in effectively integrating global and local information due to variations in remote sensing imagery and ensuring proper feature pre-alignment before modal fusion, which affects retrieval accuracy and efficiency. To address these issues, we propose CMPAGL, a cross-modal pre-aligned method leveraging global and local information. Our Gswin transformer block combines local window self-attention and global-local window cross-attention to capture multi-scale features. A pre-alignment mechanism simplifies modal fusion training, improving retrieval performance. Additionally, we introduce a similarity matrix reweighting (SMR) algorithm for reranking, and enhance the triplet loss function with an intra-class distance term to optimize feature learning. Experiments on four datasets, including RSICD and RSITMD, validate CMPAGL's effectiveness, achieving up to 4.65% improvement in R@1 and 2.28% in mean Recall (mR) over state-of-the-art methods.","['Zengbao Sun', 'Ming Zhao', 'Gaorui Liu', 'André Kaup']","IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 1-18, 2024, Art no. 4709118",arXiv,2024,https://doi.org/10.48550/arXiv.2411.14704,Anomali
Writing Style Matters: An Examination of Bias and Fairness in Information Retrieval Systems,"The rapid advancement of Language Model technologies has opened new opportunities, but also introduced new challenges related to bias and fairness. This paper explores the uncharted territory of potential biases in state-of-the-art universaltextembedding models towards specific document and query writing styles within Information Retrieval (IR) systems. Our investigation reveals that different embedding models exhibit different preferences of document writing style, while more informal and emotive styles are less favored by most embedding models. In terms of query writing styles, many embedding models tend to match the style of the query with the style of the retrieved documents, but some show a consistent preference for specific styles.Textembedding models fine-tuned on synthetic data generated by LLMs display a consistent preference for certain style of generated data. These biases intextembedding based IR systems can inadvertently silence or marginalize certain communication styles, thereby posing a significant threat to fairness in information retrieval. Finally, we also compare the answer styles of Retrieval Augmented Generation (RAG) systems based on different LLMs and find out that mosttextembedding models are biased towards LLM's answer styles when used as evaluation metrics for answer correctness. This study sheds light on the critical issue of writing style based bias in IR systems, offering valuable insights for the development of more fair and robust models.",['Hongliu Cao'],,arXiv,2024,https://doi.org/10.48550/arXiv.2411.13173,Anomali
Towards a Classification of Open-Source ML Models and Datasets for Software Engineering,"Background: Open-Source Pre-Trained Models (PTMs) and datasets provide extensive resources for various Machine Learning (ML) tasks, yet these resources lack a classification tailored to Software Engineering (SE) needs. Aims: We apply an SE-oriented classification to PTMs and datasets on a popular open-source ML repository, Hugging Face (HF), and analyze the evolution of PTMs over time. Method: We conducted a repositoryminingstudy. We started with a systematically gathered database of PTMs and datasets from the HF API. Our selection was refined by analyzing model and dataset cards and metadata, such as tags, and confirming SE relevance using Gemini 1.5 Pro. All analyses are replicable, with a publicly accessible replication package. Results: The most common SE task among PTMs and datasets is code generation, with a primary focus on software development and limited attention to software management. Popular PTMs and datasets mainly target software development. Among ML tasks,textgeneration is the most common in SE PTMs and datasets. There has been a marked increase in PTMs for SE since 2023 Q2. Conclusions: This study underscores the need for broader task coverage to enhance the integration of ML within SE practices.","['Alexandra González', 'Xavier Franch', 'David Lo', 'Silverio Martínez-Fernández']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.09683,Anomali
Recommender systems and reinforcement learning for human-building interaction and context-aware support: A text mining-driven review of scientific literature,"The indoor environment significantly impacts human health and well-being; enhancing health and reducing energy consumption in these settings is a central research focus. With the advancement of Information and Communication Technology (ICT), recommendation systems and reinforcement learning (RL) have emerged as promising approaches to induce behavioral changes to improve the indoor environment and energy efficiency of buildings. This study aims to employtextminingand Natural Language Processing (NLP) techniques to thoroughly examine the connections among these approaches in the context of human-building interaction and occupant context-aware support. The study analyzed 27,595 articles from the ScienceDirect database, revealing extensive use of recommendation systems and RL for space optimization, location recommendations, and personalized control suggestions. Furthermore, this review underscores the vast potential for expanding recommender systems and RL applications in buildings and indoor environments. Fields ripe for innovation include predictive maintenance, building-related product recommendation, and optimization of environments tailored for specific needs, such as sleep and productivity enhancements based on user feedback. The study also notes the limitations of the method in capturing subtle academic nuances. Future improvements could involve integrating and fine-tuning pre-trained language models to better interpret complextexts.","['Wenhao Zhang', 'Matias Quintana', 'Clayton Miller']",Energy Build. 2025;329(115247):115247,arXiv,2025,https://doi.org/10.48550/arXiv.2411.08734,Anomali
EUR/USD Exchange Rate Forecasting incorporating Text Mining Based on Pre-trained Language Models and Deep Learning Methods,"This study introduces a novel approach for EUR/USD exchange rate forecasting that integrates deep learning, textual analysis, and particle swarm optimization (PSO). By incorporating online news and analysistextsas qualitative data, the proposed PSO-LSTM model demonstrates superior performance compared to traditional econometric and machine learning models. The research employs advancedtextminingtechniques, including sentiment analysis using the RoBERTa-Large model and topic modeling with LDA. Empirical findings underscore the significant advantage of incorporating textual data, with the PSO-LSTM model outperforming benchmark models such as SVM, SVR, ARIMA, and GARCH. Ablation experiments reveal the contribution of each textual data category to the overall forecasting performance. The study highlights the transformative potential of artificial intelligence in finance and paves the way for future research in real-time forecasting and the integration of alternative data sources.","['Xiangyu Shi', 'Hongcheng Ding', 'Salaar Faroog', 'Deshinta Arrova Dewi', 'Shamsul Nahar Abdullah', 'Bahiah A Malek']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.07560,Anomali
Learning From Graph-Structured Data: Addressing Design Issues and Exploring Practical Applications in Graph Representation Learning,"Graphs serve as fundamental descriptors for systems composed of interacting elements, capturing a wide array of data types, from molecular interactions to social networks and knowledge graphs. In this paper, we present an exhaustive review of the latest advancements in graph representation learning and Graph Neural Networks (GNNs). GNNs, tailored to handle graph-structured data, excel in deriving insights and predictions from intricate relational information, making them invaluable for tasks involving such data. Graph representation learning, a pivotal approach in analyzing graph-structured data, facilitates numerous downstream tasks and applications across machine learning, datamining, biomedicine, and healthcare.
  Our work delves into the capabilities of GNNs, examining their foundational designs and their application in addressing real-world challenges. We introduce a GNN equipped with an advanced high-order pooling function, adept at capturing complex node interactions within graph-structured data. This pooling function significantly enhances the GNN's efficacy in both node- and graph-level tasks. Additionally, we propose a molecular graph generative model with a GNN as its core framework. This GNN backbone is proficient in learning invariant and equivariant molecular characteristics. Employing these features, the molecular graph generative model is capable of simultaneously learning and generating molecular graphs with atom-bond structures and precise atom positions. Our models undergo thorough experimental evaluations and comparisons with established methods, showcasing their superior performance in addressing diverse real-world challenges with various datasets.",['Chenqing Hua'],,arXiv,2024,https://doi.org/10.48550/arXiv.2411.07269,Anomali
Greenback Bears and Fiscal Hawks: Finance is a Jungle and Text Embeddings Must Adapt,"Financial documents are filled with specialized terminology, arcane jargon, and curious acronyms that pose challenges for general-purposetextembeddings. Yet, fewtextembeddings specialized for finance have been reported in the literature, perhaps in part due to a lack of public datasets and benchmarks. We present BAM embeddings, a set oftextembeddings finetuned on a carefully constructed dataset of 14.3M query-passage pairs. Demonstrating the benefits of domain-specific training, BAM embeddings achieve Recall@1 of 62.8% on a held-out test set, vs. only 39.2% for the best general-purposetextembedding from OpenAI. Further, BAM embeddings increase question answering accuracy by 8% on FinanceBench and show increased sensitivity to the finance-specific elements that are found in detailed, forward-looking and company and date-specific queries. To support further research we describe our approach in detail, quantify the importance of hard negativeminingand dataset scale.","['Peter Anderson', 'Mano Vikash Janardhanan', 'Jason He', 'Wei Cheng', 'Charlie Flanagan']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.07142,Anomali
Gen-AI for User Safety: A Survey,"Machine Learning and dataminingtechniques (i.e. supervised and unsupervised techniques) are used across domains to detect user safety violations. Examples include classifiers used to detect whether an email is spam or a web-page is requesting bank login information. However, existing ML/DM classifiers are limited in their ability to understand natural languages w.r.t the context and nuances. The aforementioned challenges are overcome with the arrival of Gen-AI techniques, along with their inherent ability w.r.t translation between languages, fine-tuning between various tasks and domains.
  In this manuscript, we provide a comprehensive overview of the various work done while using Gen-AI techniques w.r.t user safety. In particular, we first provide the various domains (e.g. phishing, malware, content moderation, counterfeit, physical safety) across which Gen-AI techniques have been applied. Next, we provide how Gen-AI techniques can be used in conjunction with various data modalities i.e.text, images, videos, audio, executable binaries to detect violations of user-safety. Further, also provide an overview of how Gen-AI techniques can be used in an adversarial setting. We believe that this work represents the first summarization of Gen-AI techniques for user-safety.","['Akshar Prabhu Desai', 'Tejasvi Ravi', 'Mohammad Luqman', 'Mohit Sharma', 'Nithya Kota', 'Pranjul Yadav']","2024 IEEE International Conference on Big Data (BigData), Washington, DC, USA, 2024, pp. 5315-5324",arXiv,2024,https://doi.org/10.48550/arXiv.2411.06606,Anomali
Personalized News Recommendation System via LLM Embedding and Co-Occurrence Patterns,"In the past two years, large language models (LLMs) have achieved rapid development and demonstrated remarkable emerging capabilities. Concurrently, with powerful semantic understanding and reasoning capabilities, LLMs have significantly empowered the rapid advancement of the recommendation system field. Specifically, in news recommendation (NR), systems must comprehend and process a vast amount of clicked newstextto infer the probability of candidate news clicks. This requirement exceeds the capabilities of traditional NR models but aligns well with the strengths of LLMs. In this paper, we propose a novel NR algorithm to reshape the news model via LLM Embedding and Co-Occurrence Pattern (LECOP). On one hand, we fintuned LLM by contrastive learning using large-scale datasets to encode news, which can fully explore the semantic information of news to thoroughly identify user preferences. On the other hand, we explored multiple co-occurrence patterns tominecollaborative information. Those patterns include news ID co-occurrence, Item-Item keywords co-occurrence and Intra-Item keywords co-occurrence. The keywords mentioned above are all generated by LLM. As far as we know, this is the first time that constructing such detailed Co-Occurrence Patterns via LLM to capture collaboration. Extensive experiments demonstrate the superior performance of our proposed novel method","['Zheng Li', 'Kai Zhange']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.06046,Anomali
Machine learning for prediction of dose-volume histograms of organs-at-risk in prostate cancer from simple structure volume parameters,"Dose prediction is an area of ongoing research that facilitates radiotherapy planning. Most commercial models utilise imaging data and intense computing resources. This study aimed to predict the dose-volume of rectum and bladder from volumes of target, at-risk structure organs and their overlap regions using machine learning. Dose-volume information of 94 patients with prostate cancer planned for 6000cGy in 20 fractions was exported from the treatment planning system astextfiles andminedto create a training dataset. Several statistical modelling, machine learning methods, and a new fuzzy rule-based prediction (FRBP) model were explored and validated on an independent dataset of 39 patients. The median absolute error was 2.0%-3.7% for bladder and 1.7-2.4% for rectum in the 4000-6420cGy range. For 5300cGy, 5600cGy and 6000cGy, the median difference was less than 2.5% for rectum and 3.8% for bladder. The FRBP model produced errors of 1.2%, 1.3%, 0.9% and 1.6%, 1.2%, 0.1% for the rectum and bladder respectively at these dose levels. These findings indicate feasibility of obtaining accurate predictions of the clinically important dose-volume parameters for rectum and bladder using just the volumes of these structures.","['Saheli Saha', 'Debasmita Banerjee', 'Rishi Ram', 'Gowtham Reddy', 'Debashree Guha', 'Arnab Sarkar', 'Bapi Dutta', 'Moses ArunSingh S', 'Suman Chakraborty', 'Indranil Mallick']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.05378,Anomali
Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale,"Large Language Models (LLMs) face significant challenges at inference time due to their high computational demands. To address this, we present Performance-Guided Knowledge Distillation (PGKD), a cost-effective and high-throughput solution for productiontextclassification applications. PGKD utilizes teacher-student Knowledge Distillation to distill the knowledge of LLMs into smaller, task-specific models. PGKD establishes an active learning routine between the student model and the LLM; the LLM continuously generates new training data leveraging hard-negativemining, student model validation performance, and early-stopping protocols to inform the data generation. By employing a cyclical, performance-aware approach tailored for highly multi-class, sparsely annotated datasets prevalent in industrialtextclassification, PGKD effectively addresses training challenges and outperforms traditional BERT-base models and other knowledge distillation methods on several multi-class classification datasets. Additionally, cost and latency benchmarking reveals that models fine-tuned with PGKD are up to 130X faster and 25X less expensive than LLMs for inference on the same classification task. While PGKD is showcased fortextclassification tasks, its versatile framework can be extended to any LLM distillation task, including language generation, making it a powerful tool for optimizing performance across a wide range of AI applications.","['Flavio Di Palo', 'Prateek Singhi', 'Bilal Fadlallah']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.05045,Anomali
BhasaAnuvaad: A Speech Translation Dataset for 13 Indian Languages,"Automatic Speech Translation (AST) datasets for Indian languages remain critically scarce, with public resources covering fewer than 10 of the 22 official languages. This scarcity has resulted in AST systems for Indian languages lagging far behind those available for high-resource languages like English. In this paper, we first evaluate the performance of widely-used AST systems on Indian languages, identifying notable performance gaps and challenges. Our findings show that while these systems perform adequately on read speech, they struggle significantly with spontaneous speech, including disfluencies like pauses and hesitations. Additionally, there is a striking absence of systems capable of accurately translating colloquial and informal language, a key aspect of everyday communication. To this end, we introduce BhasaAnuvaad, the largest publicly available dataset for AST involving 13 out of 22 scheduled Indian languages and English spanning over 44,400 hours and 17Mtextsegments. BhasaAnuvaad contains data for English speech to Indictext, as well as Indic speech to Englishtext. This dataset comprises three key categories: (1) Curated datasets from existing resources, (2) Large-scale webmining, and (3) Synthetic data generation. By offering this diverse and expansive dataset, we aim to bridge the resource gap and promote advancements in AST for Indian languages.","['Sparsh Jain', 'Ashwin Sankar', 'Devilal Choudhary', 'Dhairya Suman', 'Nikhil Narasimhan', 'Mohammed Safi Ur Rahman Khan', 'Anoop Kunchukuttan', 'Mitesh M Khapra', 'Raj Dabre']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.04699,Anomali
DISCO: DISCovering Overfittings as Causal Rules for Text Classification Models,"With the rapid advancement of neural language models, the deployment of over-parameterized models has surged, increasing the need for interpretable explanations comprehensible to human inspectors. Existing post-hoc interpretability methods, which often focus on unigram features of single input textual instances, fail to capture the models' decision-making process fully. Additionally, many methods do not differentiate between decisions based on spurious correlations and those based on a holistic understanding of the input. Our paper introduces DISCO, a novel method for discovering global, rule-based explanations by identifying causal n-gram associations with model predictions. This method employs a scalable sequenceminingtechnique to extract relevanttextspans from training data, associate them with model predictions, and conduct causality checks to distill robust rules that elucidate model behavior. These rules expose potential overfitting and provide insights into misleading feature combinations. We validate DISCO through extensive testing, demonstrating its superiority over existing methods in offering comprehensive insights into complex model behaviors. Our approach successfully identifies all shortcuts manually introduced into the training data (100% detection rate on the MultiRC dataset), resulting in an 18.8% regression in model performance -- a capability unmatched by any other method. Furthermore, DISCO supports interactive explanations, enabling human inspectors to distinguish spurious causes in the rule-based output. This alleviates the burden of abundant instance-wise explanations and helps assess the model's risk when encountering out-of-distribution (OOD) data.","['Zijian Zhang', 'Vinay Setty', 'Yumeng Wang', 'Avishek Anand']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.04649,Anomali
MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs,"State-of-the-art retrieval models typically address a straightforward search scenario, in which retrieval tasks are fixed (e.g., finding a passage to answer a specific question) and only a single modality is supported for both queries and retrieved results. This paper introduces techniques for advancing information retrieval with multimodal large language models (MLLMs), enabling a broader search scenario, termed universal multimodal retrieval, where multiple modalities and diverse retrieval tasks are accommodated. To this end, we first study fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16 retrieval tasks. Our empirical results show that the fine-tuned MLLM retriever is capable of understanding challenging queries, composed of bothtextand image, but it underperforms compared to a smaller CLIP retriever in cross-modal retrieval tasks due to the modality bias exhibited by MLLMs. To address the issue, we propose modality-aware hard negativeminingto mitigate the modality bias exhibited by MLLM retrievers. Second, we propose continuously fine-tuning the universal multimodal retriever to enhance itstextretrieval capability while preserving multimodal retrieval capability. As a result, our model, MM-Embed, achieves state-of-the-art performance on the multimodal retrieval benchmark M-BEIR, which spans multiple domains and tasks, while also surpassing the state-of-the-arttextretrieval model, NV-Embed-v1, on the MTEB retrieval benchmark. We also explore prompting the off-the-shelf MLLMs as zero-shot rerankers to refine the ranking of the candidates from the multimodal retriever. We find that, through prompt-and-reranking, MLLMs can further improve multimodal retrieval when the user queries (e.g.,text-image composed queries) are more complex and challenging to understand. These findings also pave the way for advancing universal multimodal retrieval in the future.","['Sheng-Chieh Lin', 'Chankyu Lee', 'Mohammad Shoeybi', 'Jimmy Lin', 'Bryan Catanzaro', 'Wei Ping']",,arXiv,2025,https://doi.org/10.48550/arXiv.2411.02571,Anomali
Enhancing Retrieval Performance: An Ensemble Approach For Hard Negative Mining,"Ranking consistently emerges as a primary focus in information retrieval research. Retrieval and ranking models serve as the foundation for numerous applications, including web search, open domain QA, enterprise domain QA, andtext-based recommender systems. Typically, these models undergo training on triplets consisting of binary relevance assignments, comprising one positive and one negative passage. However, their utilization involves a context where a significantly more nuanced understanding of relevance is necessary, especially when re-ranking a large pool of potentially relevant passages. Although collecting positive examples through user feedback like impressions or clicks is straightforward, identifying suitable negative pairs from a vast pool of possibly millions or even billions of documents possess a greater challenge. Generating a substantial number of negative pairs is often necessary to maintain the high quality of the model. Several approaches have been suggested in literature to tackle the issue of selecting suitable negative pairs from an extensive corpus. This study focuses on explaining the crucial role of hard negatives in the training process of cross-encoder models, specifically aiming to explain the performance gains observed with hard negative sampling compared to random sampling. We have developed a robust hard negativeminingtechnique for efficient training of cross-encoder re-rank models on an enterprise dataset which has domain specific context. We provide a novel perspective to enhance retrieval models, ultimately influencing the performance of advanced LLM systems like Retrieval-Augmented Generation (RAG) and Reasoning and Action Agents (ReAct). The proposed approach demonstrates that learning both similarity and dissimilarity simultaneously with cross-encoders improves performance of retrieval systems.",['Hansa Meghwani'],,arXiv,2024,https://doi.org/10.48550/arXiv.2411.02404,Anomali
Cosmogenic Muon Background Characterization for the Colorado Underground Research Institute (CURIE),"We present the characterization of cosmogenic muon backgrounds for the Colorado Underground Research Institute (CURIE), located in the Edgar ExperimentalMine(EEM) in Idaho Springs, Colorado. The CURIE facility at the EEM offers a versatile shallow underground environment, with accessible horizontal tunnel access and stable rock formations ideal for low-background physics experiments. We have measured the total underground muon flux in two locations, Site 0 and Site 1, yielding values of $φ$ = 0.246 $\pm$ 0.020$_{sys.}$ $\pm$ 0.012$_{stat.}$ and 0.239 $\pm$ 0.025$_{sys.}$ $\pm$ 0.010$_{stat.}$$μ\text{/}m^{2}\text{/}s$, respectively. We have utilized GEANT4 and PROPOSAL Monte Carlo simulations with Daemonflux and MUTE to model the muon flux at both sites, as well as an additional future location. We find good agreement between measurement and simulations, demonstrating the first instance of this computational framework being successfully used for depths $<$ 1 km.w.e. The measured underground flux corresponds to a factor of 700 reduction compared to the sea level flux. Additionally, we present a new depth-intensity relationship to normalize the mountain overburden to an equivalent flat depth, enabling direct comparison with other underground facilities. We report an average equivalent vertical depth of 0.415 $\pm$ 0.027 km.w.e. Based on our measurements, this work highlights the facility's capability for hosting low-background experiments, addressing the demand for shallow underground research spaces.","['Dakota K. Keblbeck', 'Eric Mayotte', 'Uwe Greife', 'Kyle G. Leach', 'Wouter Van De Pontseele']",,arXiv,2024,https://doi.org/10.48550/arXiv.2411.01626,Anomali
"Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges","Natural Language Processing (NLP) is revolutionising the way legal professionals and laypersons operate in the legal field. The considerable potential for NLP in the legal sector, especially in developing computational tools for various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a final selection of 133 after manual filtering. It explores foundational concepts related to NLP in the legal domain, illustrating the unique aspects and challenges of processing legaltexts, such as extensive document length, complex language, and limited open legal datasets. We provide an overview of NLP tasks specific to legaltext, such as Legal Document Summarisation, legal Named Entity Recognition, Legal Question Answering, Legal ArgumentMining, LegalTextClassification, and Legal Judgement Prediction. In the section on legal Language Models (LMs), we analyse both developed LMs and approaches for adapting general LMs to the legal domain. Additionally, we identify 16 Open Research Challenges, including bias in Artificial Intelligence applications, the need for more robust and interpretable models, and improving explainability to handle the complexities of legal language and reasoning.","['Farid Ariai', 'Gianluca Demartini']",,arXiv,2025,https://doi.org/10.48550/arXiv.2410.21306,Anomali
Informed Deep Abstaining Classifier: Investigating noise-robust training for diagnostic decision support systems,"Image-based diagnostic decision support systems (DDSS) utilizing deep learning have the potential to optimize clinical workflows. However, developing DDSS requires extensive datasets with expert annotations and is therefore costly. Leveraging report contents from radiological data bases with Natural Language Processing to annotate the corresponding image data promises to replace labor-intensive manual annotation. Asmining""real world"" databases can introduce label noise, noise-robust training losses are of great interest. However, current noise-robust losses do not consider noise estimations that can for example be derived based on the performance of the automatic label generator used. In this study, we expand the noise-robust Deep Abstaining Classifier (DAC) loss to an Informed Deep Abstaining Classifier (IDAC) loss by incorporating noise level estimations during training. Our findings demonstrate that IDAC enhances the noise robustness compared to DAC and several state-of-the-art loss functions. The results are obtained on various simulated noise levels using a public chest X-ray data set. These findings are reproduced on an in-house noisy data set, where labels were extracted from the clinical systems of the University Hospital Bonn by atext-based transformer. The IDAC can therefore be a valuable tool for researchers, companies or clinics aiming to develop accurate and reliable DDSS from routine clinical data.","['Helen Schneider', 'Sebastian Nowak', 'Aditya Parikh', 'Yannik C. Layer', 'Maike Theis', 'Wolfgang Block', 'Alois M. Sprinkart', 'Ulrike Attenberger', 'Rafet Sifa']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.21014,Anomali
Large Language Model-Guided Prediction Toward Quantum Materials Synthesis,"The synthesis of inorganic crystalline materials is essential for modern technology, especially in quantum materials development. However, designing efficient synthesis workflows remains a significant challenge due to the precise experimental conditions and extensive trial and error. Here, we present a framework using large language models (LLMs) to predict synthesis pathways for inorganic materials, including quantum materials. Our framework contains three models: LHS2RHS, predicting products from reactants; RHS2LHS, predicting reactants from products; and TGT2CEQ, generating full chemical equations for target compounds. Fine-tuned on atext-minedsynthesis database, our model raises accuracy from under 40% with pretrained models, to under 80% using conventional fine-tuning, and further to around 90% with our proposed generalized Tanimoto similarity, while maintaining robust to additional synthesis steps. Our model further demonstrates comparable performance across materials with varying degrees of quantumness quantified using quantum weight, indicating that LLMs offer a powerful tool to predict balanced chemical equations for quantum materials discovery.","['Ryotaro Okabe', 'Zack West', 'Abhijatmedhi Chotrattanapituk', 'Mouyang Cheng', 'Denisse Córdova Carrizales', 'Weiwei Xie', 'Robert J. Cava', 'Mingda Li']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.20976,Anomali
ByteNet: Rethinking Multimedia File Fragment Classification through Visual Perspectives,"Multimedia file fragment classification (MFFC) aims to identify file fragment types, e.g., image/video, audio, andtextwithout system metadata. It is of vital importance in multimedia storage and communication. Existing MFFC methods typically treat fragments as 1D byte sequences and emphasize the relations between separate bytes (interbytes) for classification. However, the more informative relations inside bytes (intrabytes) are overlooked and seldom investigated. By looking inside bytes, the bit-level details of file fragments can be accessed, enabling a more accurate classification. Motivated by this, we first propose Byte2Image, a novel visual representation model that incorporates previously overlooked intrabyte information into file fragments and reinterprets these fragments as 2D grayscale images. This model involves a sliding byte window to reveal the intrabyte information and a rowwise stacking of intrabyte ngrams for embedding fragments into a 2D space. Thus, complex interbyte and intrabyte correlations can beminedsimultaneously using powerful vision networks. Additionally, we propose an end-to-end dual-branch network ByteNet to enhance robust correlationminingand feature representation. ByteNet makes full use of the raw 1D byte sequence and the converted 2D image through a shallow byte branch feature extraction (BBFE) and a deep image branch feature extraction (IBFE) network. In particular, the BBFE, composed of a single fully-connected layer, adaptively recognizes the co-occurrence of several some specific bytes within the raw byte sequence, while the IBFE, built on a vision Transformer, effectivelyminesthe complex interbyte and intrabyte correlations from the converted image. Experiments on the two representative benchmarks, including 14 cases, validate that our proposed method outperforms state-of-the-art approaches on different cases by up to 12.2%.","['Wenyang Liu', 'Kejun Wu', 'Tianyi Liu', 'Yi Wang', 'Kim-Hui Yap', 'Lap-Pui Chau']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.20855,Anomali
MambaCPU: Enhanced Correlation Mining with State Space Models for CPU Performance Prediction,"Forecasting CPU performance, which involves estimating performance scores based on hardware characteristics during operation, is crucial for computational system design and resource management. This research field currently faces two primary challenges. First, the diversity of CPU products and the specialized nature of hardware characteristics make real-world data collection difficult. Second, existing approaches, whether reliant on hardware simulation models or machine learning, suffer from significant drawbacks, such as lengthy simulation cycles, low prediction accuracy, and neglect of characteristic correlations. To address these issues, we first gathered, preprocessed, and standardized historical data from the 4th Generation Intel Xeon Scalable Processors across various benchmark suites to create a new dataset named PerfCastDB. Subsequently, we developed a novel network, MambaCPU (MaC), as the baseline model for the PerfCastDB dataset. This model employs the mamba structure to explore global dependencies and correlations among multiple characteristics. The use of intra- and inter-group attention mechanisms further refines correlations within and between characteristic groups. These techniques enhance MaC's capability to analyze andminecomplex multivariate correlations. Comparative experiments on the PerfCastDB dataset demonstrate that MaC surpasses existing methods, confirming its effectiveness. Additionally, we have open-sourced part of the dataset and the MaC code at \url{https://github.com/xiaoman-liu/MaC} to facilitate further research.",['Xiaoman Liu'],,arXiv,2024,https://doi.org/10.48550/arXiv.2410.19297,Anomali
Making Social Platforms Accessible: Emotion-Aware Speech Generation with Integrated Text Analysis,"Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotonetext-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with thetextto synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers' emotional states or accents (referred to as the one-to-many problem oftextto speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-awareText-to-Speech (TTS) synthesis system that derives the conveyed emotion fromtextinput and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications.","['Suparna De', 'Ionut Bostan', 'Nishanth Sastry']",16th International Conference on Advances in Social Networks Analysis and Mining -ASONAM-2024,arXiv,2024,https://doi.org/10.48550/arXiv.2410.19199,Anomali
Dialectal and Low-Resource Machine Translation for Aromanian,"This paper presents the process of building a neural machine translation system with support for English, Romanian, and Aromanian - an endangered Eastern Romance language. The primary contribution of this research is twofold: (1) the creation of the most extensive Aromanian-Romanian parallel corpus to date, consisting of 79,000 sentence pairs, and (2) the development and comparative analysis of several machine translation models optimized for Aromanian. To accomplish this, we introduce a suite of auxiliary tools, including a language-agnostic sentence embedding model fortextminingand automated evaluation, complemented by a diacritics conversion system for different writing standards. This research brings contributions to both computational linguistics and language preservation efforts by establishing essential resources for a historically under-resourced language. All datasets, trained models, and associated tools are public: https://huggingface.co/aronlp and https://arotranslate.com","['Alexandru-Iulius Jerpelea', 'Alina Rădoi', 'Sergiu Nisioi']",,arXiv,2025,https://doi.org/10.48550/arXiv.2410.17728,Anomali
Proactive security defense: cyber threat intelligence modeling for connected autonomous vehicles,"Cybersecurity has become a crucial concern in the field of connected autonomous vehicles. Cyber threat intelligence (CTI), as the collection of cyber threat information, offers an ideal way for responding to emerging cyber threats and realizing proactive security defense. However, instant analysis and modeling of vehicle cybersecurity data is a fundamental challenge since its complex and professional context. In this paper, we suggest an automotive CTI modeling framework, Actim, to extract and analyse the interrelated relationships among cyber threat elements. Specifically, we first design a vehicle security-safety conceptual ontology model to depict various threat entity classes and their relations. Then, we manually annotate the first automobile CTI corpus by using real cybersecurity data, which comprises 908 threat intelligencetexts, including 8195 entities and 4852 relationships. To effectively extract cyber threat entities and their relations, we propose an automotive CTIminingmodel based on cross-sentence context. Experiment results show that the proposed BERT-DocHiatt-BiLSTM-LSTM model exceeds the performance of existing methods. Finally, we define entity-relation matching rules and create a CTI knowledge graph that structurally fuses various elements of cyber threats. The Actim framework enablesminingthe intrinsic connections among threat entities, providing valuable insight on the evolving cyber threat landscape.","['Yinghui Wang', 'Yilong Ren', 'Zhiyong Cui', 'Haiyang Yu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.16016,Anomali
Mining Asymmetric Intertextuality,"This paper introduces a new task in Natural Language Processing (NLP) and Digital Humanities (DH):MiningAsymmetric Intertextuality. Asymmetric intertextuality refers to one-sided relationships betweentexts, where onetextcites, quotes, or borrows from another without reciprocation. These relationships are common in literature and historicaltexts, where a later work references aclassical or oldertextthat remain static.
  We propose a scalable and adaptive approach forminingasymmetric intertextuality, leveraging a split-normalize-merge paradigm. In this approach, documents are split into smaller chunks, normalized into structured data using LLM-assisted metadata extraction, and merged during querying to detect both explicit and implicit intertextual relationships. Our system handles intertextuality at various levels, from direct quotations to paraphrasing and cross-document influence, using a combination of metadata filtering, vector similarity search, and LLM-based verification.
  This method is particularly well-suited for dynamically growing corpora, such as expanding literary archives or historical databases. By enabling the continuous integration of new documents, the system can scale efficiently, making it highly valuable for digital humanities practitioners in literacy studies, historical research and related fields.","['Pak Kin Lau', 'Stuart Michael McManus']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.15145,Anomali
Large Language Models as a Tool for Mining Object Knowledge,"Commonsense knowledge is essential for machines to reason about the world. Large language models (LLMs) have demonstrated their ability to perform almost human-liketextgeneration. Despite this success, they fall short as trustworthy intelligent systems, due to the opacity of the basis for their answers and a tendency to confabulate facts when questioned about obscure entities or technical domains. We hypothesize, however, that their general knowledge about objects in the everyday world is largely sound. Based on that hypothesis, this paper investigates LLMs' ability to formulate explicit knowledge about common physical artifacts, focusing on their parts and materials. Our work distinguishes between the substances that comprise an entire object and those that constitute its parts$\unicode{x2014}$a previously underexplored distinction in knowledge base construction. Using few-shot with five in-context examples and zero-shot multi-step prompting, we produce a repository of data on the parts and materials of about 2,300 objects and their subtypes. Our evaluation demonstrates LLMs' coverage and soundness in extracting knowledge. This contribution to knowledgeminingshould prove useful to AI research on reasoning about object structure and composition and serve as an explicit knowledge source (analogous to knowledge graphs) for LLMs performing multi-hop question answering.","['Hannah YoungEun An', 'Lenhart K. Schubert']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.12959,Anomali
Beyond Right and Wrong: Mitigating Cold Start in Knowledge Tracing Using Large Language Model and Option Weight,"Knowledge Tracing (KT) is vital in educational datamining, enabling personalized learning by tracking learners' knowledge states and forecasting their academic outcomes. This study introduces the LOKT (Large Language Model Option-weighted Knowledge Tracing) model to address the cold start problem where limited historical data available using large language models (LLMs). While traditional KT models have incorporated option weights, our research extends this by integrating these weights into an LLM-based KT framework. Moving beyond the binary classification of correct and incorrect responses, we emphasize that different types of incorrect answers offer valuable insights into a learner's knowledge state. By converting these responses intotext-based ordinal categories, we enable LLMs to assess learner understanding with greater clarity, although our approach focuses on the final knowledge state rather than the progression of learning over time. Using five public datasets, we demonstrate that the LOKT model sustains high predictive accuracy even with limited data, effectively addressing both ""learner cold-start"" and ""system cold-start"" scenarios. These findings showcase LOKT's potential to enhance LLM-based learning tools and support early-stage personalization.","['JongWoo Kim', 'SeongYeub Chu', 'Bryan Wong', 'Mun Yi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.12872,Anomali
Skill Learning Using Process Mining for Large Language Model Plan Generation,"Large language models (LLMs) hold promise for generating plans for complex tasks, but their effectiveness is limited by sequential execution, lack of control flow models, and difficulties in skill retrieval. Addressing these issues is crucial for improving the efficiency and interpretability of plan generation as LLMs become more central to automation and decision-making. We introduce a novel approach to skill learning in LLMs by integrating processminingtechniques, leveraging process discovery for skill acquisition, process models for skill storage, and conformance checking for skill retrieval. Our methods enhancetext-based plan generation by enabling flexible skill discovery, parallel execution, and improved interpretability. Experimental results suggest the effectiveness of our approach, with our skill retrieval method surpassing state-of-the-art accuracy baselines under specific conditions.","['Andrei Cosmin Redis', 'Mohammadreza Fani Sani', 'Bahram Zarrin', 'Andrea Burattin']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.12870,Anomali
CLIP-DFGS: A Hard Sample Mining Method for CLIP in Generalizable Person Re-Identification,"Recent advancements in pre-trained vision-language models like CLIP have shown promise in person re-identification (ReID) applications. However, their performance in generalizable person re-identification tasks remains suboptimal. The large-scale and diverse image-textpairs used in CLIP's pre-training may lead to a lack or insufficiency of certain fine-grained features. In light of these challenges, we propose a hard sampleminingmethod called DFGS (Depth-First Graph Sampler), based on depth-first search, designed to offer sufficiently challenging samples to enhance CLIP's ability to extract fine-grained features. DFGS can be applied to both the image encoder and thetextencoder in CLIP. By leveraging the powerful cross-modal learning capabilities of CLIP, we aim to apply our DFGS method to extract challenging samples and form mini-batches with high discriminative difficulty, providing the image model with more efficient and challenging samples that are difficult to distinguish, thereby enhancing the model's ability to differentiate between individuals. Our results demonstrate significant improvements over other methods, confirming the effectiveness of DFGS in providing challenging samples that enhance CLIP's performance in generalizable person re-identification.","['Huazhong Zhao', 'Lei Qi', 'Xin Geng']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.11255,Anomali
Vision-guided and Mask-enhanced Adaptive Denoising for Prompt-based Image Editing,"Text-to-image diffusion models have demonstrated remarkable progress in synthesizing high-quality images fromtextprompts, which boosts researches on prompt-based image editing that edits a source image according to a target prompt. Despite their advances, existing methods still encounter three key issues: 1) limited capacity of thetextprompt in guiding target image generation, 2) insufficientminingof word-to-patch and patch-to-patch relationships for grounding editing areas, and 3) unified editing strength for all regions during each denoising step. To address these issues, we present a Vision-guided and Mask-enhanced Adaptive Editing (ViMAEdit) method with three key novel designs. First, we propose to leverage image embeddings as explicit guidance to enhance the conventional textual prompt-based denoising process, where a CLIP-based target image embedding estimation strategy is introduced. Second, we devise a self-attention-guided iterative editing area grounding strategy, which iteratively exploits patch-to-patch relationships conveyed by self-attention maps to refine those word-to-patch relationships contained in cross-attention maps. Last, we present a spatially adaptive variance-guided sampling, which highlights sampling variances for critical image regions to promote the editing capability. Experimental results demonstrate the superior editing capacity of ViMAEdit over all existing methods.","['Kejie Wang', 'Xuemeng Song', 'Meng Liu', 'Jin Yuan', 'Weili Guan']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.10496,Anomali
Leveraging Customer Feedback for Multi-modal Insight Extraction,"Businesses can benefit from customer feedback in different modalities, such astextand images, to enhance their products and services. However, it is difficult to extract actionable and relevant pairs oftextsegments and images from customer feedback in a single pass. In this paper, we propose a novel multi-modal method that fuses image andtextinformation in a latent space and decodes it to extract the relevant feedback segments using an image-textgroundedtextdecoder. We also introduce a weakly-supervised data generation technique that produces training data for this task. We evaluate our model on unseen data and demonstrate that it can effectivelymineactionable insights from multi-modal customer feedback, outperforming the existing baselines by $14$ points in F1 score.","['Sandeep Sricharan Mukku', 'Abinesh Kanagarajan', 'Pushpendu Ghosh', 'Chetan Aggarwal']",2024.naacl-industry.22,arXiv,2024,https://doi.org/10.48550/arXiv.2410.09999,Anomali
Dying Clusters Is All You Need -- Deep Clustering With an Unknown Number of Clusters,"Finding meaningful groups, i.e., clusters, in high-dimensional data such as images ortextswithout labeled data at hand is an important challenge in datamining. In recent years, deep clustering methods have achieved remarkable results in these tasks. However, most of these methods require the user to specify the number of clusters in advance. This is a major limitation since the number of clusters is typically unknown if labeled data is unavailable. Thus, an area of research has emerged that addresses this problem. Most of these approaches estimate the number of clusters separated from the clustering process. This results in a strong dependency of the clustering result on the quality of the initial embedding. Other approaches are tailored to specific clustering processes, making them hard to adapt to other scenarios. In this paper, we propose UNSEEN, a general framework that, starting from a given upper bound, is able to estimate the number of clusters. To the best of our knowledge, it is the first method that can be easily combined with various deep clustering algorithms. We demonstrate the applicability of our approach by combining UNSEEN with the popular deep clustering algorithms DCN, DEC, and DKM and verify its effectiveness through an extensive experimental evaluation on several image and tabular datasets. Moreover, we perform numerous ablations to analyze our approach and show the importance of its components. The code is available at: https://github.com/collinleiber/UNSEEN","['Collin Leiber', 'Niklas Strauß', 'Matthias Schubert', 'Thomas Seidl']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.09491,Anomali
RealEra: Semantic-level Concept Erasure via Neighbor-Concept Mining,"The remarkable development oftext-to-image generation models has raised notable security concerns, such as the infringement of portrait rights and the generation of inappropriate content. Concept erasure has been proposed to remove the model's knowledge about protected and inappropriate concepts. Although many methods have tried to balance the efficacy (erasing target concepts) and specificity (retaining irrelevant concepts), they can still generate abundant erasure concepts under the steering of semantically related inputs. In this work, we propose RealEra to address this ""concept residue"" issue. Specifically, we first introduce the mechanism of neighbor-conceptmining, digging out the associated concepts by adding random perturbation into the embedding of erasure concept, thus expanding the erasing range and eliminating the generations even through associated concept inputs. Furthermore, to mitigate the negative impact on the generation of irrelevant concepts caused by the expansion of erasure scope, RealEra preserves the specificity through the beyond-concept regularization. This makes irrelevant concepts maintain their corresponding spatial position, thereby preserving their normal generation performance. We also employ the closed-form solution to optimize weights of U-Net for the cross-attention alignment, as well as the prediction noise alignment with the LoRA module. Extensive experiments on multiple benchmarks demonstrate that RealEra outperforms previous concept erasing methods in terms of superior erasing efficacy, specificity, and generality. More details are available on our project page https://realerasing.github.io/RealEra/ .","['Yufan Liu', 'Jinyang An', 'Wanqian Zhang', 'Ming Li', 'Dayan Wu', 'Jingzi Gu', 'Zheng Lin', 'Weiping Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.09140,Anomali
Humanity in AI: Detecting the Personality of Large Language Models,"Questionnaires are a common method for detecting the personality of Large Language Models (LLMs). However, their reliability is often compromised by two main issues: hallucinations (where LLMs produce inaccurate or irrelevant responses) and the sensitivity of responses to the order of the presented options. To address these issues, we propose combiningtextminingwith questionnaires method.Textminingcan extract psychological features from the LLMs' responses without being affected by the order of options. Furthermore, because this method does not rely on specific answers, it reduces the influence of hallucinations. By normalizing the scores from both methods and calculating the root mean square error, our experiment results confirm the effectiveness of this approach. To further investigate the origins of personality traits in LLMs, we conduct experiments on both pre-trained language models (PLMs), such as BERT and GPT, as well as conversational models (ChatLLMs), such as ChatGPT. The results show that LLMs do contain certain personalities, for example, ChatGPT and ChatGLM exhibit the personality traits of 'Conscientiousness'. Additionally, we find that the personalities of LLMs are derived from their pre-trained data. The instruction data used to train ChatLLMs can enhance the generation of data containing personalities and expose their hidden personality. We compare the results with the human average personality score, and we find that the personality of FLAN-T5 in PLMs and ChatGPT in ChatLLMs is more similar to that of a human, with score differences of 0.34 and 0.22, respectively.","['Baohua Zhan', 'Yongyi Huang', 'Wenyao Cui', 'Huaping Zhang', 'Jianyun Shang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.08545,Anomali
DISCO: A Hierarchical Disentangled Cognitive Diagnosis Framework for Interpretable Job Recommendation,"The rapid development of online recruitment platforms has created unprecedented opportunities for job seekers while concurrently posing the significant challenge of quickly and accurately pinpointing positions that align with their skills and preferences. Job recommendation systems have significantly alleviated the extensive search burden for job seekers by optimizing user engagement metrics, such as clicks and applications, thus achieving notable success. In recent years, a substantial amount of research has been devoted to developing effective job recommendation models, primarily focusing ontext-matching based and behavior modeling based methods. While these approaches have realized impressive outcomes, it is imperative to note that research on the explainability of recruitment recommendations remains profoundly unexplored. To this end, in this paper, we propose DISCO, a hierarchical Disentanglement based Cognitive diagnosis framework, aimed at flexibly accommodating the underlying representation learning model for effective and interpretable job recommendations. Specifically, we first design a hierarchical representation disentangling module to explicitlyminethe hierarchical skill-related factors implied in hidden representations of job seekers and jobs. Subsequently, we propose level-aware association modeling to enhance information communication and robust representation learning both inter- and intra-level, which consists of the interlevel knowledge influence module and the level-wise contrastive learning. Finally, we devise an interaction diagnosis module incorporating a neural diagnosis function for effectively modeling the multi-level recruitment interaction process between job seekers and jobs, which introduces the cognitive measurement theory.","['Xiaoshan Yu', 'Chuan Qin', 'Qi Zhang', 'Chen Zhu', 'Haiping Ma', 'Xingyi Zhang', 'Hengshu Zhu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.07671,Anomali
Training-free LLM-generated Text Detection by Mining Token Probability Sequences,"Large language models (LLMs) have demonstrated remarkable capabilities in generating high-qualitytextsacross diverse domains. However, the potential misuse of LLMs has raised significant concerns, underscoring the urgent need for reliable detection of LLM-generatedtexts. Conventional training-based detectors often struggle with generalization, particularly in cross-domain and cross-model scenarios. In contrast, training-free methods, which focus on inherent discrepancies through carefully designed statistical features, offer improved generalization and interpretability. Despite this, existing training-free detection methods typically rely on globaltextsequence statistics, neglecting the modeling of local discriminative features, thereby limiting their detection efficacy. In this work, we introduce a novel training-free detector, termed \textbf{Lastde} that synergizes local and global statistics for enhanced detection. For the first time, we introduce time series analysis to LLM-generatedtextdetection, capturing the temporal dynamics of token probability sequences. By integrating these local statistics with global ones, our detector reveals significant disparities between human and LLM-generatedtexts. We also propose an efficient alternative, \textbf{Lastde++} to enable real-time detection. Extensive experiments on six datasets involving cross-domain, cross-model, and cross-lingual detection scenarios, under both white-box and black-box settings, demonstrated that our method consistently achieves state-of-the-art performance. Furthermore, our approach exhibits greater robustness against paraphrasing attacks compared to existing baseline methods.","['Yihuai Xu', 'Yongwei Wang', 'Yifei Bi', 'Huangsen Cao', 'Zhouhan Lin', 'Yu Zhao', 'Fei Wu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.06072,Anomali
Latent Feature Mining for Predictive Model Enhancement with Large Language Models,"Predictive modeling often faces challenges due to limited data availability and quality, especially in domains where collected features are weakly correlated with outcomes and where additional feature collection is constrained by ethical or practical difficulties. Traditional machine learning (ML) models struggle to incorporate unobserved yet critical factors. In this work, we introduce an effective approach to formulate latent featureminingastext-to-textpropositional logical reasoning. We propose FLAME (Faithful Latent FeatureMiningfor Predictive Model Enhancement), a framework that leverages large language models (LLMs) to augment observed features with latent features and enhance the predictive power of ML models in downstream tasks. Our framework is generalizable across various domains with necessary domain-specific adaptation, as it is designed to incorporate contextual information unique to each area, ensuring effective transfer to different areas facing similar data availability challenges. We validate our framework with two case studies: (1) the criminal justice system, a domain characterized by limited and ethically challenging data collection; (2) the healthcare domain, where patient privacy concerns and the complexity of medical data limit comprehensive feature collection. Our results show that inferred latent features align well with ground truth labels and significantly enhance the downstream classifier.","['Bingxuan Li', 'Pengyi Shi', 'Amy Ward']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.04347,Anomali
"Trends, Advancements and Challenges in Intelligent Optimization in Satellite Communication","Efficient satellite communications play an enormously important role in all of our daily lives. This includes the transmission of data for communication purposes, the operation of IoT applications or the provision of data for ground stations. More and more, AI-based methods are finding their way into these areas. This paper gives an overview of current research in the field of intelligent optimization of satellite communication. For this purpose, atext-miningbased literature review was conducted and the identified papers were thematically clustered and analyzed. The identified clusters cover the main topics of routing, resource allocation and, load balancing. Through such a clustering of the literature in overarching topics, a structured analysis of the research papers was enabled, allowing the identification of latest technologies and approaches as well as research needs for intelligent optimization of satellite communication.","['Philippe Krajsic', 'Viola Suess', 'Zehong Cao', 'Ryszard Kowalczyk', 'Bogdan Franczyk']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.03674,Anomali
BadCM: Invisible Backdoor Attack Against Cross-Modal Learning,"Despite remarkable successes in unimodal learning tasks, backdoor attacks against cross-modal learning are still underexplored due to the limited generalization and inferior stealthiness when involving multiple modalities. Notably, since works in this area mainly inherit ideas from unimodal visual attacks, they struggle with dealing with diverse cross-modal attack circumstances and manipulating imperceptible trigger samples, which hinders their practicability in real-world applications. In this paper, we introduce a novel bilateral backdoor to fill in the missing pieces of the puzzle in the cross-modal backdoor and propose a generalized invisible backdoor framework against cross-modal learning (BadCM). Specifically, a cross-modalminingscheme is developed to capture the modality-invariant components as target poisoning areas, where well-designed trigger patterns injected into these regions can be efficiently recognized by the victim models. This strategy is adapted to different image-textcross-modal models, making our framework available to various attack scenarios. Furthermore, for generating poisoned samples of high stealthiness, we conceive modality-specific generators for visual and linguistic modalities that facilitate hiding explicit trigger patterns in modality-invariant regions. To the best of our knowledge, BadCM is the first invisible backdoor method deliberately designed for diverse cross-modal attacks within one unified framework. Comprehensive experimental evaluations on two typical applications, i.e., cross-modal retrieval and VQA, demonstrate the effectiveness and generalization of our method under multiple kinds of attack scenarios. Moreover, we show that BadCM can robustly evade existing backdoor defenses. Our code is available at https://github.com/xandery-geek/BadCM.","['Zheng Zhang', 'Xu Yuan', 'Lei Zhu', 'Jingkuan Song', 'Liqiang Nie']","IEEE Transactions on Image Processing, vol. 33, pp. 2558-2571, 2024",arXiv,2024,https://doi.org/10.48550/arXiv.2410.02182,Anomali
GADFA: Generator-Assisted Decision-Focused Approach for Opinion Expressing Timing Identification,"The advancement oftextgeneration models has granted us the capability to produce coherent and convincingtexton demand. Yet, in real-life circumstances, individuals do not continuously generatetextor voice their opinions. For instance, consumers pen product reviews after weighing the merits and demerits of a product, and professional analysts issue reports following significant news releases. In essence, opinion expression is typically prompted by particular reasons or signals. Despite long-standing developments in opinionmining, the appropriate timing for expressing an opinion remains largely unexplored. To address this deficit, our study introduces an innovative task - the identification of news-triggered opinion expressing timing. We ground this task in the actions of professional stock analysts and develop a novel dataset for investigation. Our approach is decision-focused, leveragingtextgeneration models to steer the classification model, thus enhancing overall performance. Our experimental findings demonstrate that thetextgenerated by our model contributes fresh insights from various angles, effectively aiding in identifying the optimal timing for opinion expression.","['Chung-Chi Chen', 'Hiroya Takamura', 'Ichiro Kobayashi', 'Yusuke Miyao', 'Hsin-Hsi Chen']",,arXiv,2024,https://doi.org/10.48550/arXiv.2410.01169,Anomali
Exploring Gen-AI applications in building research and industry: A review,"This paper investigates the transformative potential of Generative AI (Gen-AI) technologies, particularly large language models, within the building industry. By leveraging these advanced AI tools, the study explores their application across key areas such as automated compliance checking and building design assistance. The research highlights how Gen-AI can automate labor-intensive processes, significantly improving efficiency and reducing costs in building practices. The paper first discusses the two widely applied fundamental models-Transformer and Diffusion model-and summarizes current pathways for accessing Gen-AI models and the most common techniques for customizing them. It then explores applications fortextgeneration, such as compliance checking, control support, datamining, and building simulation input file editing. Additionally, it examines image generation, including direct generation through diffusion models and indirect generation through language model-supported template creation based on existing Computer-Aided Design or other design tools with rendering. The paper concludes with a comprehensive analysis of the current capabilities of Gen-AI in the building industry, outlining future directions for research and development, with the goal of paving the way for smarter, more effective, and responsive design, construction, and operational practices.","['Hanlong Wan', 'Jian Zhang', 'Yan Chen', 'Weili Xu', 'Fan Feng']",PNNL-SA-203362,arXiv,2025,this https URL,Anomali
Mining Your Own Secrets: Diffusion Classifier Scores for Continual Personalization of Text-to-Image Diffusion Models,"Personalizedtext-to-image diffusion models have grown popular for their ability to efficiently acquire a new concept from user-definedtextdescriptions and a few images. However, in the real world, a user may wish to personalize a model on multiple concepts but one at a time, with no access to the data from previous concepts due to storage/privacy concerns. When faced with this continual learning (CL) setup, most personalization methods fail to find a balance between acquiring new concepts and retaining previous ones -- a challenge that continual personalization (CP) aims to solve. Inspired by the successful CL methods that rely on class-specific information for regularization, we resort to the inherent class-conditioned density estimates, also known as diffusion classifier (DC) scores, for continual personalization oftext-to-image diffusion models. Namely, we propose using DC scores for regularizing the parameter-space and function-space oftext-to-image diffusion models, to achieve continual personalization. Using several diverse evaluation setups, datasets, and metrics, we show that our proposed regularization-based CP methods outperform the state-of-the-art C-LoRA, and other baselines. Finally, by operating in the replay-free CL setup and on low-rank adapters, our method incurs zero storage and parameter overhead, respectively, over the state-of-the-art. Our project page: https://srvcodes.github.io/continual_personalization/","['Saurav Jha', 'Shiqi Yang', 'Masato Ishii', 'Mengjie Zhao', 'Christian Simon', 'Muhammad Jehanzeb Mirza', 'Dong Gong', 'Lina Yao', 'Shusuke Takahashi', 'Yuki Mitsufuji']",,arXiv,2025,https://doi.org/10.48550/arXiv.2410.00700,Anomali
VMAD: Visual-enhanced Multimodal Large Language Model for Zero-Shot Anomaly Detection,"Zero-shot anomaly detection (ZSAD) recognizes and localizes anomalies in previously unseen objects by establishing feature mapping between textual prompts and inspection images, demonstrating excellent research value in flexible industrial manufacturing. However, existing ZSAD methods are limited by closed-world settings, struggling to unseen defects with predefined prompts. Recently, adapting Multimodal Large Language Models (MLLMs) for Industrial Anomaly Detection (IAD) presents a viable solution. Unlike fixed-prompt methods, MLLMs exhibit a generative paradigm with open-endedtextinterpretation, enabling more adaptive anomaly analysis. However, this adaption faces inherent challenges as anomalies often manifest in fine-grained regions and exhibit minimal visual discrepancies from normal samples. To address these challenges, we propose a novel framework VMAD (Visual-enhanced MLLM Anomaly Detection) that enhances MLLM with visual-based IAD knowledge and fine-grained perception, simultaneously providing precise detection and comprehensive analysis of anomalies. Specifically, we design a Defect-Sensitive Structure Learning scheme that transfers patch-similarities cues from visual branch to our MLLM for improved anomaly discrimination. Besides, we introduce a novel visual projector, Locality-enhanced Token Compression, whichminesmulti-level features in local contexts to enhance fine-grained detection. Furthermore, we introduce the Real Industrial Anomaly Detection (RIAD), a comprehensive IAD dataset with detailed anomaly descriptions and analyses, offering a valuable resource for MLLM-based IAD development. Extensive experiments on zero-shot benchmarks, including MVTec-AD, Visa, WFDD, and RIAD datasets, demonstrate our superior performance over state-of-the-art methods. The code and dataset will be available soon.","['Huilin Deng', 'Hongchen Luo', 'Wei Zhai', 'Yang Cao', 'Yu Kang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.20146,Anomali
Depression detection in social media posts using transformer-based models and auxiliary features,"The detection of depression in social media posts is crucial due to the increasing prevalence of mental health issues. Traditional machine learning algorithms often fail to capture intricate textual patterns, limiting their effectiveness in identifying depression. Existing studies have explored various approaches to this problem but often fall short in terms of accuracy and robustness. To address these limitations, this research proposes a neural network architecture leveraging transformer-based models combined with metadata and linguistic markers. The study employs DistilBERT, extracting information from the last four layers of the transformer, applying learned weights, and averaging them to create a rich representation of the inputtext. This representation, augmented by metadata and linguistic markers, enhances the model's comprehension of each post. Dropout layers prevent overfitting, and a Multilayer Perceptron (MLP) is used for final classification. Data augmentation techniques, inspired by the Easy Data Augmentation (EDA) methods, are also employed to improve model performance. Using BERT, random insertion and substitution of phrases generate additional training data, focusing on balancing the dataset by augmenting underrepresented classes. The proposed model achieves weighted Precision, Recall, and F1-scores of 84.26%, 84.18%, and 84.15%, respectively. The augmentation techniques significantly enhance model performance, increasing the weighted F1-score from 72.59% to 84.15%.","['Marios Kerasiotis', 'Loukas Ilias', 'Dimitris Askounis']","Volume 14, article number 196, (2024)",arXiv,2024,https://doi.org/10.48550/arXiv.2409.20048,Anomali
Customized Information and Domain-centric Knowledge Graph Construction with Large Language Models,"In this paper we propose a novel approach based on knowledge graphs to provide timely access to structured information, to enable actionable technology intelligence, and improve cyber-physical systems planning. Our framework encompasses atextminingprocess, which includes information retrieval, keyphrase extraction, semantic network creation, and topic map visualization. Following this data exploration process, we employ a selective knowledge graph construction (KGC) approach supported by an electronics and innovation ontology-backed pipeline for multi-objective decision-making with a focus on cyber-physical systems. We apply our methodology to the domain of automotive electrical systems to demonstrate the approach, which is scalable. Our results demonstrate that our construction process outperforms GraphGPT as well as our bi-LSTM and transformer REBEL with a pre-defined dataset by several times in terms of class recognition, relationship construction and correct ""sublass of"" categorization. Additionally, we outline reasoning applications and provide a comparison with Wikidata to show the differences and advantages of the approach.","['Frank Wawrzik', 'Matthias Plaue', 'Savan Vekariya', 'Christoph Grimm']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.20010,Anomali
FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image Classification,"The expensive fine-grained annotation and data scarcity have become the primary obstacles for the widespread adoption of deep learning-based Whole Slide Images (WSI) classification algorithms in clinical practice. Unlike few-shot learning methods in natural images that can leverage the labels of each image, existing few-shot WSI classification methods only utilize a small number of fine-grained labels or weakly supervised slide labels for training in order to avoid expensive fine-grained annotation. They lack sufficientminingof available WSIs, severely limiting WSI classification performance. To address the above issues, we propose a novel and efficient dual-tier few-shot learning paradigm for WSI classification, named FAST. FAST consists of a dual-level annotation strategy and a dual-branch classification framework. Firstly, to avoid expensive fine-grained annotation, we collect a very small number of WSIs at the slide level, and annotate an extremely small number of patches. Then, to fullyminingthe available WSIs, we use all the patches and available patch labels to build a cache branch, which utilizes the labeled patches to learn the labels of unlabeled patches and through knowledge retrieval for patch classification. In addition to the cache branch, we also construct a prior branch that includes learnable prompt vectors, using thetextencoder of visual-language models for patch classification. Finally, we integrate the results from both branches to achieve WSI classification. Extensive experiments on binary and multi-class datasets demonstrate that our proposed method significantly surpasses existing few-shot classification methods and approaches the accuracy of fully supervised methods with only 0.22$\%$ annotation costs. All codes and models will be publicly available on https://github.com/fukexue/FAST.","['Kexue Fu', 'Xiaoyuan Luo', 'Linhao Qu', 'Shuo Wang', 'Ying Xiong', 'Ilias Maglogiannis', 'Longxiang Gao', 'Manning Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.19720,Anomali
DiMB-RE: Mining the Scientific Literature for Diet-Microbiome Associations,"Objective: To develop a corpus annotated for diet-microbiome associations from the biomedical literature and train natural language processing (NLP) models to identify these associations, thereby improving the understanding of their role in health and disease, and supporting personalized nutrition strategies. Materials and Methods: We constructed DiMB-RE, a comprehensive corpus annotated with 15 entity types (e.g., Nutrient, Microorganism) and 13 relation types (e.g., INCREASES, IMPROVES) capturing diet-microbiome associations. We fine-tuned and evaluated state-of-the-art NLP models for named entity, trigger, and relation extraction as well as factuality detection using DiMB-RE. In addition, we benchmarked two generative large language models (GPT-4o-mini and GPT-4o) on a subset of the dataset in zero- and one-shot settings. Results: DiMB-RE consists of 14,450 entities and 4,206 relationships from 165 publications (including 30 full-textResults sections). Fine-tuned NLP models performed reasonably well for named entity recognition (0.800 F1 score), while end-to-end relation extraction performance was modest (0.445 F1). The use of Results section annotations improved relation extraction. The impact of trigger detection was mixed. Generative models showed lower accuracy compared to fine-tuned models. Discussion: To our knowledge, DiMB-RE is the largest and most diverse corpus focusing on diet-microbiome interactions. NLP models fine-tuned on DiMB-RE exhibit lower performance compared to similar corpora, highlighting the complexity of information extraction in this domain. Misclassified entities, missed triggers, and cross-sentence relations are the major sources of relation extraction errors. Conclusions: DiMB-RE can serve as a benchmark corpus for biomedical literaturemining. DiMB-RE and the NLP models are available at https://github.com/ScienceNLP-Lab/DiMB-RE.","['Gibong Hong', 'Veronica Hindle', 'Nadine M. Veasley', 'Hannah D. Holscher', 'Halil Kilicoglu']",,arXiv,2025,this https URL,Anomali
INSIGHTBUDDY-AI: Medication Extraction and Entity Linking using Large Language Models and Ensemble Learning,"Medication Extraction andMiningplay an important role in healthcare NLP research due to its practical applications in hospital settings, such as their mapping into standard clinical knowledge bases (SNOMED-CT, BNF, etc.). In this work, we investigate state-of-the-art LLMs intextminingtasks on medications and their related attributes such as dosage, route, strength, and adverse effects. In addition, we explore different ensemble learning methods (\textsc{Stack-Ensemble} and \textsc{Voting-Ensemble}) to augment the model performances from individual LLMs. Our ensemble learning result demonstrated better performances than individually fine-tuned base models BERT, RoBERTa, RoBERTa-L, BioBERT, BioClinicalBERT, BioMedRoBERTa, ClinicalBERT, and PubMedBERT across general and specific domains. Finally, we build up an entity linking function to map extracted medical terminologies into the SNOMED-CT codes and the British National Formulary (BNF) codes, which are further mapped to the Dictionary of Medicines and Devices (dm+d), and ICD. Our model's toolkit and desktop applications are publicly available (at \url{https://github.com/HECTA-UoM/ensemble-NER}).","['Pablo Romero', 'Lifeng Han', 'Goran Nenadic']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.19467,Anomali
Early Joint Learning of Emotion Information Makes MultiModal Model Understand You Better,"In this paper, we present our solutions for emotion recognition in the sub-challenges of Multimodal Emotion Recognition Challenge (MER2024). To mitigate the modal competition issue between audio andtext, we adopt an early fusion strategy based on a large language model, where joint training of audio andtextis conducted initially. And the joint Audio-Textmodal feature will be late-fused with other unimodal features. In order to solve the problems of data insufficiency and class imbalance, We use multiple turns of multi-model voting for datamining. Moreover, to enhance the quality of audio features, we employ speech source separation to preprocess audios. Our model ranks \textbf{2nd} in both MER2024-SEMI and MER2024-NOISE, validating our method's effectiveness.","['Mengying Ge', 'Mingyang Li', 'Dongkai Tang', 'Pengbo Li', 'Kuo Liu', 'Shuhao Deng', 'Songbai Pu', 'Long Liu', 'Yang Song', 'Tao Zhang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.18971,Anomali
Dark Miner: Defend against undesired generation for text-to-image diffusion models,"Text-to-image diffusion models have been demonstrated with undesired generation due to unfiltered large-scale training data, such as sexual images and copyrights, necessitating the erasure of undesired concepts. Most existing methods focus on modifying the generation probabilities conditioned on thetextscontaining target concepts. However, they fail to guarantee the desired generation oftextsunseen in the training phase, especially for the adversarialtextsfrom malicious attacks. In this paper, we analyze the erasure task and point out that existing methods cannot guarantee the minimization of the total probabilities of undesired generation. To tackle this problem, we propose Dark Miner. It entails a recurring three-stage process that comprisesmining, verifying, and circumventing. This method greedilyminesembeddings with maximum generation probabilities of target concepts and more effectively reduces their generation. In the experiments, we evaluate its performance on the inappropriateness, object, and style concepts. Compared with the previous methods, our method achieves better erasure and defense results, especially under multiple adversarial attacks, while preserving the native generation capability of the models. Our code will be available at https://github.com/RichardSunnyMeng/DarkMiner-offical-codes.","['Zheling Meng', 'Bo Peng', 'Xiaochuan Jin', 'Yue Jiang', 'Jing Dong', 'Wei Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.17682,Anomali
Tuning Into Bias: A Computational Study of Gender Bias in Song Lyrics,"The application oftextminingmethods is becoming increasingly prevalent, particularly within Humanities and Computational Social Sciences, as well as in a broader range of disciplines. This paper presents an analysis of gender bias in English song lyrics using topic modeling and bias measurement techniques. Leveraging BERTopic, we cluster a dataset of 537,553 English songs into distinct topics and analyze their temporal evolution. Our results reveal a significant thematic shift in song lyrics over time, transitioning from romantic themes to a heightened focus on the sexualization of women. Additionally, we observe a substantial prevalence of profanity and misogynistic content across various topics, with a particularly high concentration in the largest thematic cluster. To further analyse gender bias across topics and genres in a quantitative way, we employ the Single Category Word Embedding Association Test (SC-WEAT) to calculate bias scores for word embeddings trained on the most prominent topics as well as individual genres. The results indicate a consistent male bias in words associated with intelligence and strength, while appearance and weakness words show a female bias. Further analysis highlights variations in these biases across topics, illustrating the interplay between thematic content and gender stereotypes in song lyrics.","['Danqing Chen', 'Adithi Satish', 'Rasul Khanbayov', 'Carolin M. Schuster', 'Georg Groh']",,arXiv,2025,https://doi.org/10.48550/arXiv.2409.15949,Anomali
Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training,"Emojis have gained immense popularity on social platforms, serving as a common means to supplement or replacetext. However, existing dataminingapproaches generally either completely ignore or simply treat emojis as ordinary Unicode characters, which may limit the model's ability to grasp the rich semantic information in emojis and the interaction between emojis andtexts. Thus, it is necessary to release the emoji's power in social media datamining. To this end, we first construct a heterogeneous graph consisting of three types of nodes, i.e. post, word and emoji nodes to improve the representation of different elements in posts. The edges are also well-defined to model how these three elements interact with each other. To facilitate the sharing of information among post, word and emoji nodes, we propose a graph pre-train framework fortextand emoji co-modeling, which contains two graph pre-training tasks: node-level graph contrastive learning and edge-level link reconstruction learning. Extensive experiments on the Xiaohongshu and Twitter datasets with two types of downstream tasks demonstrate that our approach proves significant improvement over previous strong baseline methods.","['Zhou Zhang', 'Dongzeng Tan', 'Jiaan Wang', 'Yilong Chen', 'Jiarong Xu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.14552,Anomali
Contrastive Learning for Knowledge-Based Question Generation in Large Language Models,"With the rapid development of artificial intelligence technology, especially the increasingly widespread application of question-and-answer systems, high-quality question generation has become a key component in supporting the development of these systems. This article focuses on knowledge-based question generation technology, which aims to enable computers to simulate the human questioning process based on understanding specifictextsor knowledge bases. In light of the issues of hallucination and knowledge gaps present in large-scale language models when applied to knowledge-intensive tasks, this paper proposes an enhanced question generation method that incorporates contrastive learning. This method utilizes multiple models to jointlyminedomain knowledge and uses contrastive learning to guide the model in reducing noise and hallucinations in generation. Experimental results show that by designing prompts containing contrasting examples, the model's performance in question generation improves considerably, particularly when contrasting instructions and examples are used simultaneously, leading to the highest quality of generated questions and improved accuracy. These results demonstrate that the method proposed in this study, which combines contrasting context and chain-of-thought prompts, can effectively improve both the quality and the practicality of question generation.","['Zhenhong Zhang', 'Jiajing Chen', 'Weiyan Shi', 'Lingjie Yi', 'Chihang Wang', 'Qian Yu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.13994,Anomali
The Impact of Implicit Government Guarantee on Credit Rating of Municipal Investment Bonds,"One type of bond with the most implicit government guarantee is municipal investment bonds. In recent years, there have been an increasing number of downgrades in the credit ratings of municipal bonds, which has led some people to question whether the implicit government guarantee may affect the objectivity of the bond ratings? This paper usestextminingmethods tominerelevant policy documents related to municipal investment bond issuance, and calculates the implicit guarantee strength of municipal investment bonds based on the PMC index model. It further analyzes the impact of the implicit guarantee strength of municipal bonds on their credit evaluation. The study found that the implicit government guarantee on municipal investment bonds does indeed help to raise the credit ratings assigned by credit rating agencies. The study found that, moreover, the government's implicit guarantee has a more pronounced effect in boosting credit ratings in less developed western regions.","['Yan Zhang', 'Yixiang Tian', 'Lin Chen']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.13957,Anomali
"Cross-Target Stance Detection: A Survey of Techniques, Datasets, and Challenges","Stance detection is the task of determining the viewpoint expressed in atexttowards a given target. A specific direction within the task focuses on cross-target stance detection, where a model trained on samples pertaining to certain targets is then applied to a new, unseen target. With the increasing need to analyze andminingviewpoints and opinions online, the task has recently seen a significant surge in interest. This review paper examines the advancements in cross-target stance detection over the last decade, highlighting the evolution from basic statistical methods to contemporary neural and LLM-based models. These advancements have led to notable improvements in accuracy and adaptability. Innovative approaches include the use of topic-grouped attention and adversarial learning for zero-shot detection, as well as fine-tuning techniques that enhance model robustness. Additionally, prompt-tuning methods and the integration of external knowledge have further refined model performance. A comprehensive overview of the datasets used for evaluating these models is also provided, offering valuable insights into the progress and challenges in the field. We conclude by highlighting emerging directions of research and by suggesting avenues for future work in the task.","['Parisa Jamadi Khiabani', 'Arkaitz Zubiaga']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.13594,Anomali
Implicit Government Guarantee Measurement Based on PMC Index Model,"The implicit government guarantee hampers the recognition and management of risks by all stakeholders in the bond market, and it has led to excessive debt for local governments or state-owned enterprises. To prevent the risk of local government debt defaults and reduce investors' expectations of implicit government guarantees, various regulatory departments have issued a series of policy documents related to municipal investment bonds. By employingtextminingtechniques on policy documents related to municipal investment bond, and utilizing the PMC index model to assess the effectiveness of policy documents. This paper proposes a novel method for quantifying the intensity of implicit governmental guarantees based on PMC index model. The intensity of implicit governmental guarantees is inversely correlated with the PMC index of policies aimed at de-implicitizing governmental guarantees. Then as these policies become more effective, the intensity of implicit governmental guarantees diminishes correspondingly. These findings indicate that recent policies related to municipal investment bond have indeed succeeded in reducing implicit governmental guarantee intensity, and these policies have achieved the goal of risk management. Furthermore, it was showed that the intensity of implicit governmental guarantee affected by diverse aspects of these policies such as effectiveness, clarity, and specificity, as well as incentive and assurance mechanisms.","['Yan Zhang', 'Yixiang Tian', 'Lin Chen', 'Qi Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.12831,Anomali
MEXMA: Token-level objectives improve sentence representations,"Current pre-trained cross-lingual sentence encoders approaches use sentence-level objectives only. This can lead to loss of information, especially for tokens, which then degrades the sentence representation. We propose MEXMA, a novel approach that integrates both sentence-level and token-level objectives. The sentence representation in one language is used to predict masked tokens in another language, with both the sentence representation and all tokens directly updating the encoder. We show that adding token-level objectives greatly improves the sentence representation quality across several tasks. Our approach outperforms current pre-trained cross-lingual sentence encoders on bi-textminingas well as several downstream tasks. We also analyse the information encoded in our tokens, and how the sentence representation is built from them.","['João Maria Janeiro', 'Benjamin Piwowarski', 'Patrick Gallinari', 'Loïc Barrault']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.12737,Anomali
Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI,"Writtentextsreflect an author's perspective, making the thorough analysis of literature a key research method in fields such as the humanities and social sciences. However, conventionaltextminingtechniques like sentiment analysis and topic modeling are limited in their ability to capture the hierarchical narrative structures that reveal deeper argumentative patterns. To address this gap, we propose a method that leverages large language models (LLMs) to extract and organize these structures into a hierarchical framework. We validate this approach by analyzing public opinions on generative AI collected by Japan's Agency for Cultural Affairs, comparing the narratives of supporters and critics. Our analysis provides clearer visualization of the factors influencing divergent opinions on generative AI, offering deeper insights into the structures of agreement and disagreement.","['Riona Matsuoka', 'Hiroki Matsumoto', 'Takahiro Yoshida', 'Tomohiro Watanabe', 'Ryoma Kondo', 'Ryohei Hisano']","Proceedings of Jinmoncon 2024, IPSJ SIG Computers and the Humanities",arXiv,2024,https://doi.org/10.48550/arXiv.2409.11032,Anomali
beeFormer: Bridging the Gap Between Semantic and Interaction Similarity in Recommender Systems,"Recommender systems often usetext-side information to improve their predictions, especially in cold-start or zero-shot recommendation scenarios, where traditional collaborative filtering approaches cannot be used. Many approaches totext-miningside information for recommender systems have been proposed over recent years, with sentence Transformers being the most prominent one. However, these models are trained to predict semantic similarity without utilizing interaction data with hidden patterns specific to recommender systems. In this paper, we propose beeFormer, a framework for training sentence Transformer models with interaction data. We demonstrate that our models trained with beeFormer can transfer knowledge between datasets while outperforming not only semantic similarity sentence Transformers but also traditional collaborative filtering methods. We also show that training on multiple datasets from different domains accumulates knowledge in a single model, unlocking the possibility of training universal, domain-agnostic sentence Transformer models tominetextrepresentations for recommender systems. We release the source code, trained models, and additional details allowing replication of our experiments at https://github.com/recombee/beeformer.","['Vojtěch Vančura', 'Pavel Kordík', 'Milan Straka']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.10309,Anomali
MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face Forgery Detection,"The rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (FFD) techniques. Although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises andtextsare not fully explored, which limits the generalization capability of the model. In addition, most FFD methods tend to identify facial images generated by GAN, but struggle to detect unseen diffusion-synthesized ones. To address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (CLIP), to achieve generalizable diffusion face forgery detection (DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP (MFCLIP) model, whichminescomprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of DFFD. Specifically, we devise a fine-grained language encoder (FLE) that extracts fine global language features from hierarchicaltextprompts. We design a multi-modal vision encoder (MVE) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them tominegeneral visual forgery traces. Moreover, we build an innovative plug-and-play sample pair attention (SPA) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. Extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations.","['Yaning Zhang', 'Tianyi Wang', 'Zitong Yu', 'Zan Gao', 'Linlin Shen', 'Shengyong Chen']",,arXiv,2025,https://doi.org/10.48550/arXiv.2409.09724,Anomali
Enumerating Minimal Unsatisfiable Cores of LTLf formulas,"Linear Temporal Logic over finite traces ($\text{LTL}_f$) is a widely used formalism with applications in AI, processmining, model checking, and more. The primary reasoning task for$\text{LTL}_f$is satisfiability checking; yet, the recent focus on explainable AI has increased interest in analyzing inconsistent formulas, making the enumeration of minimal explanations for infeasibility a relevant task also for$\text{LTL}_f$. This paper introduces a novel technique for enumerating minimal unsatisfiable cores (MUCs) of an$\text{LTL}_f$specification. The main idea is to encode a$\text{LTL}_f$formula into an Answer Set Programming (ASP) specification, such that the minimal unsatisfiable subsets (MUSes) of the ASP program directly correspond to the MUCs of the original$\text{LTL}_f$specification. Leveraging recent advancements in ASP solving yields a MUC enumerator achieving good performance in experiments conducted on established benchmarks from the literature.","['Antonio Ielo', 'Giuseppe Mazzotta', 'Rafael Peñaloza', 'Francesco Ricca']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.09485,Anomali
Detection and Classification of Twitter Users' Opinions on Drought Crises in Iran Using Machine Learning Techniques,"The main objective of this research is to identify and classify the opinions of Persian-speaking Twitter users related to drought crises in Iran and subsequently develop a model for detecting these opinions on the platform. To achieve this, a model has been developed using machine learning andtextminingmethods to detect the opinions of Persian-speaking Twitter users regarding the drought issues in Iran. The statistical population for the research included 42,028 drought-related tweets posted over a one-year period. These tweets were extracted from Twitter using keywords related to the drought crises in Iran. Subsequently, a sample of 2,300 tweets was qualitatively analyzed, labeled, categorized, and examined. Next, a four-category classification of users` opinions regarding drought crises and Iranians' resilience to these crises was identified. Based on these four categories, a machine learning model based on logistic regression was trained to predict and detect various opinions in Twitter posts. The developed model exhibits an accuracy of 66.09% and an F-score of 60%, indicating that this model has good performance for detecting Iranian Twitter users' opinions regarding drought crises. The ability to detect opinions regarding drought crises on platforms like Twitter using machine learning methods can intelligently represent the resilience level of the Iranian society in the face of these crises, and inform policymakers in this area about changes in public opinion.","['Somayeh Labafi', 'Leila Rabiei', 'Zeinab Rajabi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.07611,Anomali
E-commerce Webpage Recommendation Scheme Base on Semantic Mining and Neural Networks,"In e-commerce websites, webminingweb page recommendation technology has been widely used. However, recommendation solutions often cannot meet the actual application needs of online shopping users. To address this problem, this paper proposes an e-commerce web page recommendation solution that combines semantic webminingand BP neural networks. First, the web logs of user searches are processed, and 5 features are extracted: content priority, time consumption priority, online shopping users' explicit/implicit feedback on the website, recommendation semantics and input deviation amount. Then, these features are used as input features of the BP neural network to classify and identify the priority of the final output web page. Finally, the web pages are sorted according to priority and recommended to users. This project uses book sales webpages as samples for experiments. The results show that this solution can quickly and accurately identify the webpages required by users.","['Wenchao Zhao', 'Xiaoyi Liu', 'Ruilin Xu', 'Lingxi Xiao', 'Muqing Li']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.07033,Anomali
Think-on-Process: Dynamic Process Generation for Collaborative Development of Multi-Agent System,"Software development is a collaborative endeavor that requires individuals from different departments to work together in order to collectively develop a high-quality software system. In this context, people have begun to explore a method that leverages multi-agent systems based on LLMs to carry out software development. However, existing research tends to rigidly fix the software development process in a framework in code form, thus failing to dynamically adjust the software development process in real-time to meet the more flexible and variable software environment. In this paper, we propose a dynamic process generation framework, named ToP (Think-on-Process). The core idea of ToP is to leverage experiential knowledge (i.e., process models) to guide LLMs in generating software development processes (i.e., instances). These instances will guide multi-agent in software development and employ a compiler to provide feedback on the development outcomes. Subsequently, we utilize heuristic algorithms to filter the instances and apply processminingalgorithms to derive process model. Finally, the process model will be converted intotext, formatted as prompts, to enhance the ability of LLMs to generate other instances. Experiments demonstrate that our framework ToP significantly enhances the dynamic process generation capability of the GPT-3.5 and GPT-4 for five categories of software development tasks.","['Leilei Lin', 'Yingming Zhou', 'Wenlong Chen', 'Chen Qian']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.06568,Anomali
Efficient Rare Temporal Pattern Mining in Time Series,"Time series data from various domains is continuously growing, and extracting and analyzing temporal patterns within these series can provide valuable insights. Temporal patternmining(TPM) extends traditional patternminingby incorporating event time intervals into patterns, making them more expressive but also increasing the computational complexity in terms of time and space. One important type of temporal pattern is the rare temporal pattern (RTP), which occurs infrequently but with high confidence.Miningthese rare patterns poses several challenges, for example, the low support threshold can lead to a combinatorial explosion and the generation of many irrelevant patterns. To address this, an efficient approach tominerare temporal patterns is essential. This paper introduces the Rare Temporal PatternMiningfrom Time Series (RTPMfTS) method, designed to discover rare temporal patterns. The key contributions of this work are as follows: (1) An end-to-end RTPMfTS process that takes time series data as input and outputs rare temporal patterns. (2) A highly efficient Rare Temporal PatternMining(RTPM) algorithm, which leverages optimized data structures for fast event and pattern retrieval, as well as effective pruning techniques to accelerate theminingprocess. (3) A comprehensive experimental evaluation of RTPM, demonstrating that it outperforms the baseline in both runtime and memory efficiency.","['Van Ho Long', 'Nguyen Ho', 'Trinh Le Cong', 'Anh-Vu Dinh-Duc', 'Tu Nguyen Ngoc']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.05042,Anomali
LCA and energy efficiency in buildings: mapping more than twenty years of research,"Research on Life Cycle Assessment (LCA) is being conducted in various sectors, from analyzing building materials and components to comprehensive evaluations of entire structures. However, reviews of the existing literature have been unable to provide a comprehensive overview of research in this field, leaving scholars without a definitive guideline for future investigations. This paper aims to fill this gap, mapping more than twenty years of research. Using an innovative methodology that combines social network analysis andtextmining, the paper examined 8024 scientific abstracts. The authors identified seven key thematic groups, building and sustainability clusters (BSCs). To assess their significance in the broader discourse on building and sustainability, the semantic brand score (SBS) indicator was applied. Additionally, building and sustainability trends were tracked, focusing on the LCA concept. The major research topics mainly relate to building materials and energy efficiency. In addition to presenting an innovative approach to reviewing extensive literature domains, the article also provides insights into emerging and underdeveloped themes, outlining crucial future research directions.","['F. Asdrubali', 'A. Fronzetti Colladon', 'L. Segneri', 'D. M. Gandola']","Energy and Buildings 321, 114684 (2024)",arXiv,2024,https://doi.org/10.48550/arXiv.2409.00065,Anomali
OnDiscuss: An Epistemic Network Analysis Learning Analytics Visualization Tool for Evaluating Asynchronous Online Discussions,"Asynchronous online discussions are common assignments in both hybrid and online courses to promote critical thinking and collaboration among students. However, the evaluation of these assignments can require considerable time and effort from instructors. We created OnDiscuss, a learning analytics visualization tool for instructors that utilizestextminingalgorithms and Epistemic Network Analysis (ENA) to generate visualizations of student discussion data.Textminingis used to generate an initial codebook for the instructor as well as automatically code the data. This tool allows instructors to edit their codebook and then dynamically view the resulting ENA networks for the entire class and individual students. Through empirical investigation, we assess this tool's effectiveness to help instructors in analyzing asynchronous online discussion assignments.","['Yanye Luther', 'Marcia Moraes', 'Sudipto Ghosh', 'James Folkestad']",,arXiv,2024,https://doi.org/10.48550/arXiv.2409.00051,Anomali
Evolving Text Data Stream Mining,"Atextstream is an ordered sequence oftextdocuments generated over time. A massive amount of suchtextdata is generated by online social platforms every day. Designing an algorithm for suchtextstreams to extract useful information is a challenging task due to unique properties of the stream such as infinite length, data sparsity, and evolution. Thereby, learning useful information from such streaming data under the constraint of limited time and memory has gained increasing attention. During the past decade, although manytextstreamminingalgorithms have proposed, there still exists some potential issues. First, high-dimensionaltextdata heavily degrades the learning performance until the model either works on subspace or reduces the global feature space. The second issue is to extract semantictextrepresentation of documents and capture evolving topics over time. Moreover, the problem of label scarcity exists, whereas existing approaches work on the full availability of labeled data. To deal with these issues, in this thesis, new learning models are proposed for clustering and multi-label learning ontextstreams.",['Jay Kumar'],,arXiv,2024,https://doi.org/10.48550/arXiv.2409.00010,Anomali
MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms,"Vision Language Place Recognition (VLVPR) enhances robot localization performance by incorporating natural language descriptions from images. By utilizing language information, VLVPR directs robot place matching, overcoming the constraint of solely depending on vision. The essence of multimodal fusion lies inminingthe complementary information between different modalities. However, general fusion methods rely on traditional neural architectures and are not well equipped to capture the dynamics of cross modal interactions, especially in the presence of complex intra modal and inter modal correlations. To this end, this paper proposes a novel coarse to fine and end to end connected cross modal place recognition framework, called MambaPlace. In the coarse localization stage, thetextdescription and 3D point cloud are encoded by the pretrained T5 and instance encoder, respectively. They are then processed usingTextAttention Mamba (TAM) and Point Clouds Mamba (PCM) for data enhancement and alignment. In the subsequent fine localization stage, the features of thetextdescription and 3D point cloud are cross modally fused and further enhanced through cascaded Cross Attention Mamba (CCAM). Finally, we predict the positional offset from the fusedtextpoint cloud features, achieving the most accurate localization. Extensive experiments show that MambaPlace achieves improved localization accuracy on the KITTI360Pose dataset compared to the state of the art methods.","['Tianyi Shang', 'Zhenyu Li', 'Pengjie Xu', 'Jinwei Qiao']",,arXiv,2025,https://doi.org/10.48550/arXiv.2408.15740,Anomali
Conan-embedding: General Text Embedding with More and Better Negative Samples,"With the growing popularity of RAG, the capabilities of embedding models are gaining increasing attention. Embedding models are primarily trained through contrastive loss learning, with negative examples being a key component. Previous work has proposed various hard negativeminingstrategies, but these strategies are typically employed as preprocessing steps. In this paper, we propose the conan-embedding model, which maximizes the utilization of more and higher-quality negative examples. Specifically, since the model's ability to handle preprocessed negative examples evolves during training, we propose dynamic hard negativeminingmethod to expose the model to more challenging negative examples throughout the training process. Secondly, contrastive learning requires as many negative examples as possible but is limited by GPU memory constraints. Therefore, we use a Cross-GPU balancing Loss to provide more negative examples for embedding training and balance the batch size across multiple tasks. Moreover, we also discovered that the prompt-response pairs from LLMs can be used for embedding training. Our approach effectively enhances the capabilities of embedding models, currently ranking first on the Chinese leaderboard of Massivetextembedding benchmark","['Shiyu Li', 'Yang Tang', 'Shizhe Chen', 'Xi Chen']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.15710,Anomali
Towards Graph Prompt Learning: A Survey and Beyond,"Large-scale ""pre-train and prompt learning"" paradigms have demonstrated remarkable adaptability, enabling broad applications across diverse domains such as question answering, image recognition, and multimodal retrieval. This approach fully leverages the potential of large-scale pre-trained models, reducing downstream data requirements and computational costs while enhancing model applicability across various tasks. Graphs, as versatile data structures that capture relationships between entities, play pivotal roles in fields such as social network analysis, recommender systems, and biological graphs. Despite the success of pre-train and prompt learning paradigms in Natural Language Processing (NLP) and Computer Vision (CV), their application in graph domains remains nascent. In graph-structured data, not only do the node and edge features often have disparate distributions, but the topological structures also differ significantly. This diversity in graph data can lead to incompatible patterns or gaps between pre-training and fine-tuning on downstream graphs. We aim to bridge this gap by summarizing methods for alleviating these disparities. This includes exploring prompt design methodologies, comparing related techniques, assessing application scenarios and datasets, and identifying unresolved problems and challenges. This survey categorizes over 100 relevant works in this field, summarizing general design principles and the latest applications, includingtext-attributed graphs, molecules, proteins, and recommendation systems. Through this extensive review, we provide a foundational understanding of graph prompt learning, aiming to impact not only the graphminingcommunity but also the broader Artificial General Intelligence (AGI) community.","['Qingqing Long', 'Yuchen Yan', 'Peiyan Zhang', 'Chen Fang', 'Wentao Cui', 'Zhiyuan Ning', 'Meng Xiao', 'Ning Cao', 'Xiao Luo', 'Lingjun Xu', 'Shiyue Jiang', 'Zheng Fang', 'Chong Chen', 'Xian-Sheng Hua', 'Yuanchun Zhou']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.14520,Anomali
Analysis of child development facts and myths using text mining techniques and classification models,"The rapid dissemination of misinformation on the internet complicates the decision-making process for individuals seeking reliable information, particularly parents researching child development topics. This misinformation can lead to adverse consequences, such as inappropriate treatment of children based on myths. While previous research has utilizedtext-miningtechniques to predict child abuse cases, there has been a gap in the analysis of child development myths and facts. This study addresses this gap by applyingtextminingtechniques and classification models to distinguish between myths and facts about child development, leveraging newly gathered data from publicly available websites. The research methodology involved several stages. First,textminingtechniques were employed to pre-process the data, ensuring enhanced accuracy. Subsequently, the structured data was analysed using six robust Machine Learning (ML) classifiers and one Deep Learning (DL) model, with two feature extraction techniques applied to assess their performance across three different training-testing splits. To ensure the reliability of the results, cross-validation was performed using both k-fold and leave-one-out methods. Among the classification models tested, Logistic Regression (LR) demonstrated the highest accuracy, achieving a 90% accuracy with the Bag-of-Words (BoW) feature extraction technique. LR stands out for its exceptional speed and efficiency, maintaining low testing time per statement (0.97 microseconds). These findings suggest that LR, when combined with BoW, is effective in accurately classifying child development information, thus providing a valuable tool for combating misinformation and assisting parents in making informed decisions.","['Mehedi Tajrian', 'Azizur Rahman', 'Muhammad Ashad Kabir', 'Md Rafiqul Islam']","Heliyon, 2024, 10 (17)",arXiv,2024,https://doi.org/10.48550/arXiv.2408.13091,Anomali
What Do You Want? User-centric Prompt Generation for Text-to-image Synthesis via Multi-turn Guidance,"The emergence oftext-to-image synthesis (TIS) models has significantly influenced digital image creation by producing high-quality visuals from written descriptions. Yet these models heavily rely on the quality and specificity of textual prompts, posing a challenge for novice users who may not be familiar with TIS-model-preferred prompt writing. Existing solutions relieve this via automatic model-preferred prompt generation from user queries. However, this single-turn manner suffers from limited user-centricity in terms of result interpretability and user interactivity. To address these issues, we propose DialPrompt, a multi-turn dialogue-based TIS prompt generation model that emphasises user-centricity. DialPrompt is designed to follow a multi-turn guidance workflow, where in each round of dialogue the model queries user with their preferences on possible optimization dimensions before generating the final TIS prompt. To achieve this, wemined15 essential dimensions for high-quality prompts from advanced users and curated a multi-turn dataset. Through training on this dataset, DialPrompt can improve interpretability by allowing users to understand the correlation between specific phrases and image attributes. Additionally, it enables greater user control and engagement in the prompt generation process, leading to more personalized and visually satisfying outputs. Experiments indicate that DialPrompt achieves a competitive result in the quality of synthesized images, outperforming existing prompt engineering approaches by 5.7%. Furthermore, in our user evaluation, DialPrompt outperforms existing approaches by 46.5% in user-centricity score and is rated 7.9/10 by 19 human reviewers.","['Yilun Liu', 'Minggui He', 'Feiyu Yao', 'Yuhe Ji', 'Shimin Tao', 'Jingzhou Du', 'Duan Li', 'Jian Gao', 'Li Zhang', 'Hao Yang', 'Boxing Chen', 'Osamu Yoshie']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.12910,Anomali
Exploiting Student Parallelism for Efficient GPU Inference of BERT-like Models in Online Services,"Due to high accuracy, BERT-like models have been widely adopted bytextminingand web searching. However, large BERT-like models suffer from inefficient online inference, facing the following two problems on GPUs: (1) their high accuracy relies on the large model depth, which linearly increases the sequential computation on GPUs; (2) stochastic and dynamic online workloads cause extra costs from batching and paddings. Therefore, we present \sys for the real-world setting of GPU inference on online workloads. At its core, \sys adopts stacking distillation and boosting ensemble, distilling the original deep model into a group of shallow but virtually stacked student models running in parallel. This enables \sys to achieve a lower model depth (e.g., two layers) than the others and the lowest inference latency while maintaining accuracy. In addition, adaptive student pruning realizes dynamic student numbers according to changing online workloads. Especially for occasional workload bursts, it can temporarily decrease the student number with minimal accuracy loss to improve system throughput. We conduct comprehensive experiments to verify the effectiveness, whose results show that \sys outperforms the baselines by $4.1\times\sim 1.6\times$ in latency while maintaining accuracy and achieves up to $22.27\times$ higher throughput for workload bursts.","['Weiyan Wang', 'Yilun Jin', 'Yiming Zhang', 'Victor Junqiu Wei', 'Han Tian', 'Li Chen', 'Jinbao Xue', 'Yangyu Tao', 'Di Wang', 'Kai Chen']",,arXiv,2025,https://doi.org/10.48550/arXiv.2408.12526,Anomali
Sentiment and Emotion-aware Multi-criteria Fuzzy Group Decision Making System,"In today's world, making decisions as a group is common, whether choosing a restaurant or deciding on a holiday destination. Group decision-making (GDM) systems play a crucial role by facilitating consensus among participants with diverse preferences. Discussions are one of the main tools people use to make decisions. When people discuss alternatives, they use natural language to express their opinions. Traditional GDM systems generally require participants to provide explicit opinion values to the system. However, in real-life scenarios, participants often express their opinions through sometext(e.g., in comments, social media, messengers, etc.). This paper introduces a sentiment and emotion-aware multi-criteria fuzzy GDM system designed to enhance consensus-reaching effectiveness in group settings. This system incorporates natural language processing to analyze sentiments and emotions expressed in textual data, enabling an understanding of participant opinions besides the explicit numerical preference inputs. Once all the experts have provided their preferences for the alternatives, the individual preferences are aggregated into a single collective preference matrix. This matrix represents the collective expert opinion regarding the other options. Then, sentiments, emotions, and preference scores are inputted into a fuzzy inference system to get the overall score. The proposed system was used for a small decision-making process - choosing the hotel for a vacation by a group of friends. Our findings demonstrate that integrating sentiment and emotion analysis into GDM systems allows everyone's feelings and opinions to be considered during discussions and significantly improves consensus among participants.","['Adilet Yerkin', 'Pakizar Shamoi', 'Elnara Kadyrgali']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.11976,Anomali
Diagnosing and Remedying Knowledge Deficiencies in LLMs via Label-free Curricular Meaningful Learning,"Large Language Models (LLMs) are versatile and demonstrate impressive generalization ability byminingand learning information from extensive unlabeledtext. However, they still exhibit reasoning mistakes, often stemming from knowledge deficiencies, which can affect their trustworthiness and reliability. Although users can provide diverse and comprehensive queries, obtaining sufficient and effective feedback is demanding. Furthermore, evaluating LLMs comprehensively with limited labeled samples is difficult. This makes it a challenge to diagnose and remedy the deficiencies of LLMs through rich label-free user queries. To tackle this challenge, we propose a label-free curricular meaningful learning framework (LaMer). LaMer first employs relative entropy to automatically diagnose and quantify the knowledge deficiencies of LLMs in a label-free setting. Next, to remedy the diagnosed knowledge deficiencies, we apply curricular meaningful learning: first, we adopt meaningful learning to adaptively synthesize augmentation data according to the severity of the deficiencies, and then design a curricular deficiency remedy strategy to remedy the knowledge deficiencies of LLMs progressively. Experiments show that LaMer efficiently and effectively diagnoses and remedies knowledge deficiencies in LLMs, improving various LLMs across seven out-of-distribution (OOD) reasoning and language understanding benchmarks, achieving comparable results to baselines with just 40\% training data. LaMer even surpasses methods that rely on labeled datasets for deficiency diagnosis. In application, our label-free method can offer an effective knowledge deficiency diagnostic tool for efficient LLM development.","['Kai Xiong', 'Xiao Ding', 'Li Du', 'Jiahao Ying', 'Ting Liu', 'Bing Qin', 'Yixin Cao']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.11431,Anomali
Mistral-SPLADE: LLMs for better Learned Sparse Retrieval,"Learned Sparse Retrievers (LSR) have evolved into an effective retrieval strategy that can bridge the gap between traditional keyword-based sparse retrievers and embedding-based dense retrievers. At its core, learned sparse retrievers try to learn the most important semantic keyword expansions from a query and/or document which can facilitate better retrieval with overlapping keyword expansions. LSR like SPLADE has typically been using encoder only models with MLM (masked language modeling) style objective in conjunction with known ways of retrieval performance improvement such as hard negativemining, distillation, etc. In this work, we propose to use decoder-only model for learning semantic keyword expansion. We posit, decoder only models that have seen much higher magnitudes of data are better equipped to learn keyword expansions needed for improved retrieval. We use Mistral as the backbone to develop our Learned Sparse Retriever similar to SPLADE and train it on a subset of sentence-transformer data which is often used for trainingtextembedding models. Our experiments support the hypothesis that a sparse retrieval model based on decoder only large language model (LLM) surpasses the performance of existing LSR systems, including SPLADE and all its variants. The LLM based model (Echo-Mistral-SPLADE) now stands as a state-of-the-art learned sparse retrieval model on the BEIRtextretrieval benchmark.","['Meet Doshi', 'Vishwajeet Kumar', 'Rudra Murthy', 'Vignesh P', 'Jaydeep Sen']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.11119,Anomali
GLIMMER: Incorporating Graph and Lexical Features in Unsupervised Multi-Document Summarization,"Pre-trained language models are increasingly being used in multi-document summarization tasks. However, these models need large-scale corpora for pre-training and are domain-dependent. Other non-neural unsupervised summarization approaches mostly rely on key sentence extraction, which can lead to information loss. To address these challenges, we propose a lightweight yet effective unsupervised approach called GLIMMER: a Graph and LexIcal features based unsupervised Multi-docuMEnt summaRization approach. It first constructs a sentence graph from the source documents, then automatically identifies semantic clusters bymininglow-level features from rawtexts, thereby improving intra-cluster correlation and the fluency of generated sentences. Finally, it summarizes clusters into natural sentences. Experiments conducted on Multi-News, Multi-XScience and DUC-2004 demonstrate that our approach outperforms existing unsupervised approaches. Furthermore, it surpasses state-of-the-art pre-trained multi-document summarization models (e.g. PEGASUS and PRIMERA) under zero-shot settings in terms of ROUGE scores. Additionally, human evaluations indicate that summaries generated by GLIMMER achieve high readability and informativeness scores. Our code is available at https://github.com/Oswald1997/GLIMMER.","['Ran Liu', 'Ming Liu', 'Min Yu', 'Jianguo Jiang', 'Gang Li', 'Dan Zhang', 'Jingyuan Li', 'Xiang Meng', 'Weiqing Huang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.10115,Anomali
Understanding Enthymemes in Argument Maps: Bridging Argument Mining and Logic-based Argumentation,"Argumentminingis natural language processing technology aimed at identifying arguments intext. Furthermore, the approach is being developed to identify the premises and claims of those arguments, and to identify the relationships between arguments including support and attack relationships. In this paper, we assume that an argument map contains the premises and claims of arguments, and support and attack relationships between them, that have been identified by argumentmining. So from a piece oftext, we assume an argument map is obtained automatically by natural language processing. However, to understand and to automatically analyse that argument map, it would be desirable to instantiate that argument map with logical arguments. Once we have the logical representation of the arguments in an argument map, we can use automated reasoning to analyze the argumentation (e.g. check consistency of premises, check validity of claims, and check the labelling on each arc corresponds with thw logical arguments). We address this need by using classical logic for representing the explicit information in thetext, and using default logic for representing the implicit information in thetext. In order to investigate our proposal, we consider some specific options for instantiation.","['Jonathan Ben-Naim', 'Victor David', 'Anthony Hunter']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.08648,Anomali
LLMI3D: MLLM-based 3D Perception from a Single 2D Image,"Recent advancements in autonomous driving, augmented reality, robotics, and embodied intelligence have necessitated 3D perception algorithms. However, current 3D perception methods, especially specialized small models, exhibit poor generalization in open scenarios. On the other hand, multimodal large language models (MLLMs) excel in general capacity but underperform in 3D tasks, due to weak 3D local spatial object perception, poortext-based geometric numerical output, and inability to handle camera focal variations. To address these challenges, we propose the following solutions: Spatial-Enhanced Local FeatureMiningfor better spatial feature extraction, 3D Query Token-Derived Info Decoding for precise geometric regression, and Geometry Projection-Based 3D Reasoning for handling camera focal length variations. We employ parameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a powerful 3D perception MLLM. Additionally, we have constructed the IG3D dataset, which provides fine-grained descriptions and question-answer annotations. Extensive experiments demonstrate that our LLMI3D achieves state-of-the-art performance, outperforming other methods by a large margin.","['Fan Yang', 'Sicheng Zhao', 'Yanhao Zhang', 'Hui Chen', 'Haonan Lu', 'Jungong Han', 'Guiguang Ding']",,arXiv,2025,https://doi.org/10.48550/arXiv.2408.07422,Anomali
FLASH: Federated Learning-Based LLMs for Advanced Query Processing in Social Networks through RAG,"Our paper introduces a novel approach to social network information retrieval and user engagement through a personalized chatbot system empowered by Federated Learning GPT. The system is designed to seamlessly aggregate and curate diverse social media data sources, including user posts, multimedia content, and trending news. Leveraging Federated Learning techniques, the GPT model is trained on decentralized data sources to ensure privacy and security while providing personalized insights and recommendations. Users interact with the chatbot through an intuitive interface, accessing tailored information and real-time updates on social media trends and user-generated content. The system's innovative architecture enables efficient processing of input files, parsing and enrichingtextdata with metadata, and generating relevant questions and answers using advanced language models. By facilitating interactive access to a wealth of social network information, this personalized chatbot system represents a significant advancement in social media communication and knowledge dissemination.","['Sai Puppala', 'Ismail Hossain', 'Md Jahangir Alam', 'Sajedul Talukder']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.05242,Anomali
MaterioMiner -- An ontology-based text mining dataset for extraction of process-structure-property entities,"While large language models learn sound statistical representations of the language and information therein, ontologies are symbolic knowledge representations that can complement the former ideally. Research at this critical intersection relies on datasets that intertwine ontologies andtextcorpora to enable training and comprehensive benchmarking of neurosymbolic models. We present the MaterioMiner dataset and the linked materials mechanics ontology where ontological concepts from the mechanics of materials domain are associated with textual entities within the literature corpus. Another distinctive feature of the dataset is its eminently fine-granular annotation. Specifically, 179 distinct classes are manually annotated by three raters within four publications, amounting to a total of 2191 entities that were annotated and curated. Conceptual work is presented for the symbolic representation of causal composition-process-microstructure-property relationships. We explore the annotation consistency between the three raters and perform fine-tuning of pre-trained models to showcase the feasibility of named-entity recognition model training. Reusing the dataset can foster training and benchmarking of materials language models, automated ontology construction, and knowledge graph generation from textual data.","['Ali Riza Durmaz', 'Akhil Thomas', 'Lokesh Mishra', 'Rachana Niranjan Murthy', 'Thomas Straub']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.04661,Anomali
Attention Mechanism and Context Modeling System for Text Mining Machine Translation,"This paper advances a novel architectural schema anchored upon the Transformer paradigm and innovatively amalgamates the K-means categorization algorithm to augment the contextual apprehension capabilities of the schema. The transformer model performs well in machine translation tasks due to its parallel computing power and multi-head attention mechanism. However, it may encounter contextual ambiguity or ignore local features when dealing with highly complex language structures. To circumvent this constraint, this exposition incorporates the K-Means algorithm, which is used to stratify the lexis and idioms of the input textual matter, thereby facilitating superior identification and preservation of the local structure and contextual intelligence of the language. The advantage of this combination is that K-Means can automatically discover the topic or concept regions in thetext, which may be directly related to translation quality. Consequently, the schema contrived herein enlists K-Means as a preparatory phase antecedent to the Transformer and recalibrates the multi-head attention weights to assist in the discrimination of lexis and idioms bearing analogous semantics or functionalities. This ensures the schema accords heightened regard to the contextual intelligence embodied by these clusters during the training phase, rather than merely focusing on locational intelligence.","['Yuwei Zhang', 'Junming Huang', 'Sitong Liu', 'Zexi Chen', 'Zizheng Li']",,arXiv,2025,https://doi.org/10.48550/arXiv.2408.04216,Anomali
A Debiased Nearest Neighbors Framework for Multi-Label Text Classification,"Multi-LabelTextClassification (MLTC) is a practical yet challenging task that involves assigning multiple non-exclusive labels to each document. Previous studies primarily focus on capturing label correlations to assist label prediction by introducing special labeling schemes, designing specific model structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor ($k$NN) framework has shown promise by retrieving labeled samples as references tominelabel co-occurrence information in the embedding space. However, two critical biases, namely embedding alignment bias and confidence estimation bias, are often overlooked, adversely affecting prediction performance. In this paper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC, specifically designed to mitigate these biases. To address embedding alignment bias, we propose a debiased contrastive learning strategy, enhancing neighbor consistency on label co-occurrence. For confidence estimation bias, we present a debiased confidence estimation strategy, improving the adaptive combination of predictions from $k$NN and inductive binary classifications. Extensive experiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2, Amazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method. Besides, our method does not introduce any extra parameters.","['Zifeng Cheng', 'Zhiwei Jiang', 'Yafeng Yin', 'Zhaoling Chen', 'Cong Wang', 'Shiping Ge', 'Qiguo Huang', 'Qing Gu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.03202,Anomali
BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba,"The advancement of natural language processing (NLP) in biology hinges on models' ability to interpret intricate biomedical literature. Traditional models often struggle with the complex and domain-specific language in this field. In this paper, we present BioMamba, a pre-trained model specifically designed for biomedicaltextmining. BioMamba builds upon the Mamba architecture and is pre-trained on an extensive corpus of biomedical literature. Our empirical studies demonstrate that BioMamba significantly outperforms models like BioBERT and general-domain Mamba across various biomedical tasks. For instance, BioMamba achieves a 100 times reduction in perplexity and a 4 times reduction in cross-entropy loss on the BioASQ test set. We provide an overview of the model architecture, pre-training process, and fine-tuning techniques. Additionally, we release the code and trained model to facilitate further research.","['Ling Yue', 'Sixue Xing', 'Yingzhou Lu', 'Tianfan Fu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2408.02600,Anomali
Harvesting Textual and Structured Data from the HAL Publication Repository,"HAL (\textit{Hyper Articles en Ligne}) is the French national publication repository, used by most higher education and research organizations for their open science policy. Although it is a rich repository of academic documents, its potential for advanced research has not been fully explored. We present HALvest, a unique dataset that bridges the gap between citation networks and the fulltextof HAL-submitted articles to help with authorship attribution and verification. This first iteration consists of approximately 700,000 documents, spanning 56 languages across 13 identified domains. We transform articles' metadata into a citation network, producing a heterogeneous graph. This graph includes uniquely identified authors on HAL, as well as all open-access documents and their references. Finally, wemine14.5 million high-quality sequence pairs from HALvest for contrastive learning purposes. By providing different views of HAL, suited for modern machine learning, we aim to assist practitioners in better analyzing and interpreting research dynamics.","['Francis Kulumba', 'Wissam Antoun', 'Guillaume Vimont', 'Laurent Romary']",,arXiv,2025,https://doi.org/10.48550/arXiv.2407.20595,Anomali
ActivityCLIP: Enhancing Group Activity Recognition by Mining Complementary Information from Text to Supplement Image Modality,"Previous methods usually only extract the image modality's information to recognize group activity. However,miningimage information is approaching saturation, making it difficult to extract richer information. Therefore, extracting complementary information from other modalities to supplement image information has become increasingly important. In fact, action labels provide cleartextinformation to express the action's semantics, which existing methods often overlook. Thus, we propose ActivityCLIP, a plug-and-play method forminingthetextinformation contained in the action labels to supplement the image information for enhancing group activity recognition. ActivityCLIP consists oftextand image branches, where thetextbranch is plugged into the image branch (The off-the-shelf image-based method). Thetextbranch includes Image2Text and relation modeling modules. Specifically, we propose the knowledge transfer module, Image2Text, which adapts image information intotextinformation extracted by CLIP via knowledge distillation. Further, to keep our method convenient, we add fewer trainable parameters based on the relation module of the image branch to model interaction relation in thetextbranch. To show our method's generality, we replicate three representative methods by ActivityCLIP, which adds only limited trainable parameters, achieving favorable performance improvements for each method. We also conduct extensive ablation studies and compare our method with state-of-the-art methods to demonstrate the effectiveness of ActivityCLIP.","['Guoliang Xu', 'Jianqin Yin', 'Feng Zhou', 'Yonghao Dang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.19820,Anomali
Symmetrical Joint Learning Support-query Prototypes for Few-shot Segmentation,"We propose Sym-Net, a novel framework for Few-Shot Segmentation (FSS) that addresses the critical issue of intra-class variation by jointly learning both query and support prototypes in a symmetrical manner. Unlike previous methods that generate query prototypes solely by matching query features to support prototypes, which is a form of bias learning towards the few-shot support samples, Sym-Net leverages a balanced symmetrical learning approach for both query and support prototypes, ensuring that the learning process does not favor one set (support or query) over the other. One of main modules of Sym-Net is the visual-textalignment-based prototype aggregation module, which is not just query-guided prototype refinement, it is a jointly learning from both support and query samples, which makes the model beneficial for handling intra-class discrepancies and allows it to generalize better to new, unseen classes. Specifically, a parameter-free prior mask generation module is designed to accurately localize both local and global regions of the query object by using sliding windows of different sizes and a self-activation kernel to suppress incorrect background matches. Additionally, to address the information loss caused by spatial pooling during prototype learning, a top-down hyper-correlation module is integrated to capture multi-scale spatial relationships between support and query images. This approach is further jointly optimized by implementing a co-optimized hard tripletminingstrategy. Experimental results show that the proposed Sym-Net outperforms state-of-the-art models, which demonstrates that jointly learning support-query prototypes in a symmetrical manner for FSS offers a promising direction to enhance segmentation performance with limited annotated data.","['Qun Li', 'Baoquan Sun', 'Fu Xiao', 'Yonggang Qi', 'Bir Bhanu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.19306,Anomali
Embedding And Clustering Your Data Can Improve Contrastive Pretraining,"Recent studies of large-scale contrastive pretraining in thetextembedding domain show that using single-source minibatches, rather than mixed-source minibatches, can substantially improve overall model accuracy. In this work, we explore extending training data stratification beyond source granularity by leveraging a pretrainedtextembedding model and the classic k-means clustering algorithm to further split training data apart by the semantic clusters within each source. Experimentally, we observe a notable increase in NDCG@10 when pretraining a BERT-basedtextembedding model on query-passage pairs from the MSMARCO passage retrieval dataset. Additionally, we conceptually connect our clustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B methodology and the nearest-neighbor-based hard-negativeminingaspect of the ANCE methodology and discuss how this unified view motivates future lines of research on the organization of contrastive pretraining data.",['Luke Merrick'],,arXiv,2024,https://doi.org/10.48550/arXiv.2407.18887,Anomali
Constructing the CORD-19 Vaccine Dataset,"We introduce new dataset 'CORD-19-Vaccination' to cater to scientists specifically looking into COVID-19 vaccine-related research. This dataset is extracted from CORD-19 dataset [Wang et al., 2020] and augmented with new columns for language detail, author demography, keywords, and topic per paper. Facebook's fastText model is used to identify languages [Joulin et al., 2016]. To establish author demography (author affiliation, lab/institution location, and lab/institution country columns) we processed the JSON file for each paper and then further enhanced using Google's search API to determine country values. 'Yake' was used to extract keywords from the title, abstract, and body of each paper and the LDA (Latent Dirichlet Allocation) algorithm was used to add topic information [Campos et al., 2020, 2018a,b]. To evaluate the dataset, we demonstrate a question-answering task like the one used in the CORD-19 Kaggle challenge [Goldbloom et al., 2022]. For further evaluation, sequential sentence classification was performed on each paper's abstract using the model from Dernoncourt et al. [2016]. We partially hand annotated the training dataset and used a pre-trained BERT-PubMed layer. 'CORD- 19-Vaccination' contains 30k research papers and can be immensely valuable for NLP research such astextmining, information extraction, and question answering, specific to the domain of COVID-19 vaccine research.","['Manisha Singh', 'Divy Sharma', 'Alonso Ma', 'Bridget Tyree', 'Margaret Mitchell']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.18471,Anomali
PiPa++: Towards Unification of Domain Adaptive Semantic Segmentation via Self-supervised Learning,"Unsupervised domain adaptive segmentation aims to improve the segmentation accuracy of models on target domains without relying on labeled data from those domains. This approach is crucial when labeled target domain data is scarce or unavailable. It seeks to align the feature representations of the source domain (where labeled data is available) and the target domain (where only unlabeled data is present), thus enabling the model to generalize well to the target domain. Current image- and video-level domain adaptation have been addressed using different and specialized frameworks, training strategies and optimizations despite their underlying connections. In this paper, we propose a unified framework PiPa++, which leverages the core idea of ``comparing'' to (1) explicitly encourage learning of discriminative pixel-wise features with intraclass compactness and inter-class separability, (2) promote the robust feature learning of the identical patch against different contexts or fluctuations, and (3) enable the learning of temporal continuity under dynamic environments. With the designed task-smart contrastive sampling strategy, PiPa++ enables theminingof more informative training samples according to the task demand. Extensive experiments demonstrate the effectiveness of our method on both image-level and video-level domain adaption benchmarks. Moreover, the proposed method is compatible with other UDA approaches to further improve the performance without introducing extra parameters.","['Mu Chen', 'Zhedong Zheng', 'Yi Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.17101,Anomali
Early screening of potential breakthrough technologies with enhanced interpretability: A patent-specific hierarchical attention network model,"Despite the usefulness of machine learning approaches for the early screening of potential breakthrough technologies, their practicality is often hindered by opaque models. To address this, we propose an interpretable machine learning approach to predicting future citation counts from patenttextsusing a patent-specific hierarchical attention network (PatentHAN) model. Central to this approach are (1) a patent-specific pre-trained language model, capturing the meanings of technical words in patent claims, (2) a hierarchical network structure, enabling detailed analysis at the claim level, and (3) a claim-wise self-attention mechanism, revealing pivotal claims during the screening process. A case study of 35,376 pharmaceutical patents demonstrates the effectiveness of our approach in early screening of potential breakthrough technologies while ensuring interpretability. Furthermore, we conduct additional analyses using different language models and claim types to examine the robustness of the approach. It is expected that the proposed approach will enhance expert-machine collaboration in identifying breakthrough technologies, providing new insight derived fromtextmininginto technological value.","['Jaewoong Choi', 'Janghyeok Yoon', 'Changyong Lee']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.16939,Anomali
DreamDissector: Learning Disentangled Text-to-3D Generation from 2D Diffusion Priors,"Text-to-3D generation has recently seen significant progress. To enhance its practicality in real-world applications, it is crucial to generate multiple independent objects with interactions, similar to layer-compositing in 2D image editing. However, existingtext-to-3D methods struggle with this task, as they are designed to generate either non-independent objects or independent objects lacking spatially plausible interactions. Addressing this, we propose DreamDissector, atext-to-3D method capable of generating multiple independent objects with interactions. DreamDissector accepts a multi-objecttext-to-3D NeRF as input and produces independent textured meshes. To achieve this, we introduce the Neural Category Field (NeCF) for disentangling the input NeRF. Additionally, we present the Category Score Distillation Sampling (CSDS), facilitated by a Deep ConceptMining(DCM) module, to tackle the concept gap issue in diffusion models. By leveraging NeCF and CSDS, we can effectively derive sub-NeRFs from the original scene. Further refinement enhances geometry and texture. Our experimental results validate the effectiveness of DreamDissector, providing users with novel means to control 3D synthesis at the object level and potentially opening avenues for various creative applications in the future.","['Zizheng Yan', 'Jiapeng Zhou', 'Fanpeng Meng', 'Yushuang Wu', 'Lingteng Qiu', 'Zisheng Ye', 'Shuguang Cui', 'Guanying Chen', 'Xiaoguang Han']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.16260,Anomali
Spatiotemporal Graph Guided Multi-modal Network for Livestreaming Product Retrieval,"With the rapid expansion of e-commerce, more consumers have become accustomed to making purchases via livestreaming. Accurately identifying the products being sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a fundamental and daunting challenge. The LPR task encompasses three primary dilemmas in real-world scenarios: 1) the recognition of intended products from distractor products present in the background; 2) the video-image heterogeneity that the appearance of products showcased in live streams often deviates substantially from standardized product images in stores; 3) there are numerous confusing products with subtle visual nuances in the shop. To tackle these challenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN). First, we employ atext-guided attention mechanism that leverages the spoken content of salespeople to guide the model to focus toward intended products, emphasizing their salience over cluttered background products. Second, a long-range spatiotemporal graph network is further designed to achieve both instance-level interaction and frame-level matching, solving the misalignment caused by video-image heterogeneity. Third, we propose a multi-modal hard examplemining, assisting the model in distinguishing highly similar products with fine-grained features across the video-image-textdomain. Through extensive quantitative and qualitative experiments, we demonstrate the superior performance of our proposed SGMN model, surpassing the state-of-the-art methods by a substantial margin. The code is available at https://github.com/Huxiaowan/SGMN.","['Xiaowan Hu', 'Yiyi Chen', 'Yan Li', 'Minquan Wang', 'Haoqian Wang', 'Quan Chen', 'Han Li', 'Peng Jiang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.16248,Anomali
Predicting Stock Prices with FinBERT-LSTM: Integrating News Sentiment Analysis,"The stock market's ascent typically mirrors the flourishing state of the economy, whereas its decline is often an indicator of an economic downturn. Therefore, for a long time, significant correlation elements for predicting trends in financial stock markets have been widely discussed, and people are becoming increasingly interested in the task of financialtextmining. The inherent instability of stock prices makes them acutely responsive to fluctuations within the financial markets. In this article, we use deep learning networks, based on the history of stock prices and articles of financial, business, technical news that introduce market information to predict stock prices. We illustrate the enhancement of predictive precision by integrating weighted news categories into the forecasting model. We developed a pre-trained NLP model known as FinBERT, designed to discern the sentiments within financialtexts. Subsequently, we advanced this model by incorporating the sophisticated Long Short Term Memory (LSTM) architecture, thus constructing the innovative FinBERT-LSTM model. This model utilizes news categories related to the stock market structure hierarchy, namely market, industry, and stock related news categories, combined with the stock market's stock price situation in the previous week for prediction. We selected NASDAQ-100 index stock data and trained the model on Benzinga news articles, and utilized Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and Accuracy as the key metrics for the assessment and comparative analysis of the model's performance. The results indicate that FinBERT-LSTM performs the best, followed by LSTM, and DNN model ranks third in terms of effectiveness.","['Wenjun Gu', 'Yihao Zhong', 'Shizun Li', 'Changsong Wei', 'Liting Dong', 'Zhuoyue Wang', 'Chao Yan']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.16150,Anomali
NV-Retriever: Improving text embedding models with effective hard-negative mining,"Textembedding models have been popular for information retrieval applications such as semantic search and Question-Answering systems based on Retrieval-Augmented Generation (RAG). Those models are typically Transformer models that are fine-tuned with contrastive learning objectives. One of the challenging aspects of fine-tuning embedding models is the selection of high quality hard-negative passages for contrastive learning. In this paper we introduce a family of positive-awareminingmethods that use the positive relevance score as an anchor for effective false negative removal, leading to faster training and more accurate retrieval models. We provide an ablation study on hard-negativeminingmethods over their configurations, exploring different teacher and base models. We further demonstrate the efficacy of our proposedminingmethods at scale with the NV-Retriever-v1 model, which scores 60.9 on MTEB Retrieval (BEIR) benchmark and placed 1st when it was published to the MTEB Retrieval on July, 2024.","['Gabriel de Souza P. Moreira', 'Radek Osmulski', 'Mengyao Xu', 'Ronay Ak', 'Benedikt Schifferer', 'Even Oldridge']",,arXiv,2025,https://doi.org/10.48550/arXiv.2407.15831,Anomali
SQLfuse: Enhancing Text-to-SQL Performance through Comprehensive LLM Synergy,"Text-to-SQL conversion is a critical innovation, simplifying the transition from complex SQL to intuitive natural language queries, especially significant given SQL's prevalence in the job market across various roles. The rise of Large Language Models (LLMs) like GPT-3.5 and GPT-4 has greatly advanced this field, offering improved natural language understanding and the ability to generate nuanced SQL statements. However, the potential of open-source LLMs inText-to-SQL applications remains underexplored, with many frameworks failing to leverage their full capabilities, particularly in handling complex database queries and incorporating feedback for iterative refinement. Addressing these limitations, this paper introduces SQLfuse, a robust system integrating open-source LLMs with a suite of tools to enhanceText-to-SQL translation's accuracy and usability. SQLfuse features four modules: schemamining, schema linking, SQL generation, and a SQL critic module, to not only generate but also continuously enhance SQL query quality. Demonstrated by its leading performance on the Spider Leaderboard and deployment by Ant Group, SQLfuse showcases the practical merits of open-source LLMs in diverse business contexts.","['Tingkai Zhang', 'Chaoyu Chen', 'Cong Liao', 'Jun Wang', 'Xudong Zhao', 'Hang Yu', 'Jianchao Wang', 'Jianguo Li', 'Wenhui Shi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.14568,Anomali
Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models,"The vast amount of biomedical information available today presents a significant challenge for investigators seeking to digest, process, and understand these findings effectively. Large Language Models (LLMs) have emerged as powerful tools to navigate this complex and challenging data landscape. However, LLMs may lead to hallucinatory responses, making Retrieval Augmented Generation (RAG) crucial for achieving accurate information. In this protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease Distinction), a comprehensive workflow designed to support investigators with knowledge integration and hypothesis generation, identifying validated paths forward. Relevant biomedical information from publications and knowledge bases are reviewed, integrated, and extracted viatext-miningassociation analysis and explainable graph prediction models on disease nodes, forecasting potential links among drugs and diseases. These analyses, along with biomedicaltexts, are integrated into a framework that facilitates user-directed mechanism elucidation as well as hypothesis exploration through RAG-enabled LLMs. A clinical use-case demonstrates RUGGED's ability to evaluate and recommend therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy (DCM), analyzing prescribed drugs for molecular interactions and unexplored uses. The platform minimizes LLM hallucinations, offers actionable insights, and improves the investigation of novel therapeutics.","['Alexander R. Pelletier', 'Joseph Ramirez', 'Irsyad Adam', 'Simha Sankar', 'Yu Yan', 'Ding Wang', 'Dylan Steinecke', 'Wei Wang', 'Peipei Ping']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.12888,Anomali
ISPO: An Integrated Ontology of Symptom Phenotypes for Semantic Integration of Traditional Chinese Medical Data,"Symptom phenotypes are one of the key types of manifestations for diagnosis and treatment of various disease conditions. However, the diversity of symptom terminologies is one of the major obstacles hindering the analysis and knowledge sharing of various types of symptom-related medical data particularly in the fields of Traditional Chinese Medicine (TCM). Objective: This study aimed to construct an Integrated Ontology of symptom phenotypes (ISPO) to support the dataminingof Chinese EMRs and real-world study in TCM field. Methods: To construct an integrated ontology of symptom phenotypes (ISPO), we manually annotated classical TCM textbooks and large-scale Chinese electronic medical records (EMRs) to collect symptom terms with support from a medicaltextannotation system. Furthermore, to facilitate the semantic interoperability between different terminologies, we incorporated public available biomedical vocabularies by manual mapping between Chinese terms and English terms with cross-references to source vocabularies. In addition, we evaluated the ISPO using independent clinical EMRs to provide a high-usable medical ontology for clinical data analysis. Results: By integrating 78,696 inpatient cases of EMRs, 5 biomedical vocabularies, 21 TCM books and dictionaries, ISPO provides 3,147 concepts, 23,475 terms, and 55,552 definition or contextualtexts. Adhering to the taxonomical structure of the related anatomical systems of symptom phenotypes, ISPO provides 12 top-level categories and 79 middle-level sub-categories. The validation of data analysis showed the ISPO has a coverage rate of 95.35%, 98.53% and 92.66% for symptom terms with occurrence rates of 0.5% in additional three independent curated clinical datasets, which can demonstrate the significant value of ISPO in mapping clinical terms to ontologies.","['Zixin Shu', 'Rui Hua', 'Dengying Yan', 'Chenxia Lu', 'Ning Xu', 'Jun Li', 'Hui Zhu', 'Jia Zhang', 'Dan Zhao', 'Chenyang Hui', 'Junqiu Ye', 'Chu Liao', 'Qi Hao', 'Wen Ye', 'Cheng Luo', 'Xinyan Wang', 'Chuang Cheng', 'Xiaodong Li', 'Baoyan Liu', 'Xiaji Zhou', 'Runshun Zhang', 'Min Xu', 'Xuezhong Zhou']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.12851,Anomali
A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting,"Recently, Large Language Models (LLMs) have demonstrated great potential in various dataminingtasks, such as knowledge question answering, mathematical reasoning, and commonsense reasoning. However, the reasoning capability of LLMs on temporal event forecasting has been under-explored. To systematically investigate their abilities in temporal event forecasting, we conduct a comprehensive evaluation of LLM-based methods for temporal event forecasting. Due to the lack of a high-quality dataset that involves both graph and textual data, we first construct a benchmark dataset, named MidEast-TE-mini. Based on this dataset, we design a series of baseline methods, characterized by various input formats and retrieval augmented generation (RAG) modules. From extensive experiments, we find that directly integrating rawtextsinto the input of LLMs does not enhance zero-shot extrapolation performance. In contrast, fine-tuning LLMs with rawtextscan significantly improve performance. Additionally, LLMs enhanced with retrieval modules can effectively capture temporal relational patterns hidden in historical events. However, issues such as popularity bias and the long-tail problem persist in LLMs, particularly in the retrieval-augmented generation (RAG) method. These findings not only deepen our understanding of LLM-based event forecasting methods but also highlight several promising research directions. We consider that this comprehensive evaluation, along with the identified research opportunities, will significantly contribute to future research on temporal event forecasting through LLMs.","['He Chang', 'Chenchen Ye', 'Zhulin Tao', 'Jie Wu', 'Zhengmao Yang', 'Yunshan Ma', 'Xianglin Huang', 'Tat-Seng Chua']",,arXiv,2025,https://doi.org/10.48550/arXiv.2407.11638,Anomali
Comparing Complex Concepts with Transformers: Matching Patent Claims Against Natural Language Text,"A key capability in managing patent applications or a patent portfolio is comparing claims to othertext, e.g. a patent specification. Because the language of claims is different from language used elsewhere in the patent application or in non-patenttext, this has been challenging for computer based natural language processing. We test two new LLM-based approaches and find that both provide substantially better performance than previously published values. The ability to match dense information from one domain against much more distributed information expressed in a different vocabulary may also be useful beyond the intellectual property space.","['Matthias Blume', 'Ghobad Heidari', 'Christoph Hewel']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.10351,Anomali
Data Adaptive Traceback for Vision-Language Foundation Models in Image Classification,"Vision-language foundation models have been incredibly successful in a wide range of downstream computer vision tasks using adaptation methods. However, due to the high cost of obtaining pre-training datasets, pairs with weak image-textcorrelation in the data exist in large numbers. We call them weak-paired samples. Due to the limitations of these weak-paired samples, the pre-training model are unable tomineall the knowledge from pre-training data. The existing adaptation methods do not consider the missing knowledge, which may lead to crucial task-related knowledge for the downstream tasks being ignored. To address this issue, we propose a new adaptation framework called Data Adaptive Traceback (DAT). Specifically, we utilize a zero-shot-based method to extract the most downstream task-related subset of the pre-training data to enable the downstream tasks. Furthermore, we adopt a pseudo-label-based semi-supervised technique to reuse the pre-training images and a vision-language contrastive learning method to address the confirmation bias issue in semi-supervised learning. We conduct extensive experiments that show our proposed DAT approach meaningfully improves various benchmark datasets performance over traditional adaptation methods by simply.","['Wenshuo Peng', 'Kaipeng Zhang', 'Yue Yang', 'Hao Zhang', 'Yu Qiao']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.08787,Anomali
End-To-End Causal Effect Estimation from Unstructured Natural Language Data,"Knowing the effect of an intervention is critical for human decision-making, but current approaches for causal effect estimation rely on manual data collection and structuring, regardless of the causal assumptions. This increases both the cost and time-to-completion for studies. We show how large, diverse observationaltextdata can beminedwith large language models (LLMs) to produce inexpensive causal effect estimates under appropriate causal assumptions. We introduce NATURAL, a novel family of causal effect estimators built with LLMs that operate over datasets of unstructuredtext. Our estimators use LLM conditional distributions (over variables of interest, given thetextdata) to assist in the computation of classical estimators of causal effect. We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information. We prepare six (two synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline. NATURAL estimators demonstrate remarkable performance, yielding causal effect estimates that fall within 3 percentage points of their ground truth counterparts, including on real-world Phase 3/4 clinical trials. Our results suggest that unstructuredtextdata is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.","['Nikita Dhawan', 'Leonardo Cotta', 'Karen Ullrich', 'Rahul G. Krishnan', 'Chris J. Maddison']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.07018,Anomali
Multimodal Chain-of-Thought Reasoning via ChatGPT to Protect Children from Age-Inappropriate Apps,"Mobile applications (Apps) could expose children to inappropriate themes such as sexual content, violence, and drug use. Maturity rating offers a quick and effective method for potential users, particularly guardians, to assess the maturity levels of apps. Determining accurate maturity ratings for mobile apps is essential to protect children's health in today's saturated digital marketplace. Existing approaches to maturity rating are either inaccurate (e.g., self-reported rating by developers) or costly (e.g., manual examination). In the literature, there are fewtext-mining-based approaches to maturity rating. However, each app typically involves multiple modalities, namely app description in thetext, and screenshots in the image. In this paper, we present a framework for determining app maturity levels that utilize multimodal large language models (MLLMs), specifically ChatGPT-4 Vision. Powered by Chain-of-Thought (CoT) reasoning, our framework systematically leverages ChatGPT-4 to process multimodal app data (i.e., textual descriptions and screenshots) and guide the MLLM model through a step-by-step reasoning pathway from initial content analysis to final maturity rating determination. As a result, through explicitly incorporating CoT reasoning, our framework enables ChatGPT to understand better and apply maturity policies to facilitate maturity rating. Experimental results indicate that the proposed method outperforms all baseline models and other fusion strategies.","['Chuanbo Hu', 'Bin Liu', 'Minglei Yin', 'Yilu Zhou', 'Xin Li']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.06309,Anomali
Potential of Multimodal Large Language Models for Data Mining of Medical Images and Free-text Reports,"Medical images and radiology reports are crucial for diagnosing medical conditions, highlighting the importance of quantitative analysis for clinical decision-making. However, the diversity and cross-source heterogeneity of these data challenge the generalizability of current data-miningmethods. Multimodal large language models (MLLMs) have recently transformed many domains, significantly affecting the medical field. Notably, Gemini-Vision-series (Gemini) and GPT-4-series (GPT-4) models have epitomized a paradigm shift in Artificial General Intelligence (AGI) for computer vision, showcasing their potential in the biomedical domain. In this study, we evaluated the performance of the Gemini, GPT-4, and 4 popular large models for an exhaustive evaluation across 14 medical imaging datasets, including 5 medical imaging categories (dermatology, radiology, dentistry, ophthalmology, and endoscopy), and 3 radiology report datasets. The investigated tasks encompass disease classification, lesion segmentation, anatomical localization, disease diagnosis, report generation, and lesion detection. Our experimental results demonstrated that Gemini-series models excelled in report generation and lesion detection but faces challenges in disease classification and anatomical localization. Conversely, GPT-series models exhibited proficiency in lesion segmentation and anatomical localization but encountered difficulties in disease diagnosis and lesion detection. Additionally, both the Gemini series and GPT series contain models that have demonstrated commendable generation efficiency. While both models hold promise in reducing physician workload, alleviating pressure on limited healthcare resources, and fostering collaboration between clinical practitioners and artificial intelligence technologies, substantial enhancements and comprehensive validations remain imperative before clinical deployment.","['Yutong Zhang', 'Yi Pan', 'Tianyang Zhong', 'Peixin Dong', 'Kangni Xie', 'Yuxiao Liu', 'Hanqi Jiang', 'Zhengliang Liu', 'Shijie Zhao', 'Tuo Zhang', 'Xi Jiang', 'Dinggang Shen', 'Tianming Liu', 'Xin Zhang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.05758,Anomali
Enhancing Label-efficient Medical Image Segmentation with Text-guided Diffusion Models,"Aside from offering state-of-the-art performance in medical image generation, denoising diffusion probabilistic models (DPM) can also serve as a representation learner to capture semantic information and potentially be used as an image representation for downstream tasks, e.g., segmentation. However, these latent semantic representations rely heavily on labor-intensive pixel-level annotations as supervision, limiting the usability of DPM in medical image segmentation. To address this limitation, we propose an enhanced diffusion segmentation model, called TextDiff, that improves semantic representation through inexpensive medicaltextannotations, thereby explicitly establishing semantic representation and language correspondence for diffusion models. Concretely, TextDiff extracts intermediate activations of the Markov step of the reverse diffusion process in a pretrained diffusion model on large-scale natural images and learns additional expert knowledge by combining them with complementary and readily available diagnostictextinformation. TextDiff freezes the dual-branch multi-modal structure andminesthe latent alignment of semantic features in diffusion models with diagnostic descriptions by only training the cross-attention mechanism and pixel classifier, making it possible to enhance semantic representation with inexpensivetext. Extensive experiments on public QaTa-COVID19 and MoNuSeg datasets show that our TextDiff is significantly superior to the state-of-the-art multi-modal segmentation methods with only a few training samples.",['Chun-Mei Feng'],,arXiv,2024,https://doi.org/10.48550/arXiv.2407.05323,Anomali
The Solution for Language-Enhanced Image New Category Discovery,"Treatingtextsas images, combining prompts with textual labels for prompt tuning, and leveraging the alignment properties of CLIP have been successfully applied in zero-shot multi-label image recognition. Nonetheless, relying solely on textual labels to store visual information is insufficient for representing the diversity of visual objects. In this paper, we propose reversing the training process of CLIP and introducing the concept of Pseudo Visual Prompts. These prompts are initialized for each object category and pre-trained on large-scale, low-cost sentence data generated by large language models. This processminesthe aligned visual information in CLIP and stores it in class-specific visual prompts. We then employ contrastive learning to transfer the stored visual information to the textual labels, enhancing their visual representation capacity. Additionally, we introduce a dual-adapter module that simultaneously leverages knowledge from the original CLIP and new learning knowledge derived from downstream datasets. Benefiting from the pseudo visual prompts, our method surpasses the state-of-the-art not only on clean annotatedtextdata but also on pseudotextdata generated by large language models.","['Haonan Xu', 'Dian Chao', 'Xiangyu Wu', 'Zhonghua Wan', 'Yang Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.04994,Anomali
"Leveraging Data Mining, Active Learning, and Domain Adaptation in a Multi-Stage, Machine Learning-Driven Approach for the Efficient Discovery of Advanced Acidic Oxygen Evolution Electrocatalysts","Developing advanced catalysts for acidic oxygen evolution reaction (OER) is crucial for sustainable hydrogen production. This study introduces a novel, multi-stage machine learning (ML) approach to streamline the discovery and optimization of complex multi-metallic catalysts. Our method integrates datamining, active learning, and domain adaptation throughout the materials discovery process. Unlike traditional trial-and-error methods, this approach systematically narrows the exploration space using domain knowledge with minimized reliance on subjective intuition. Then the active learning module efficiently refines element composition and synthesis conditions through iterative experimental feedback. The process culminated in the discovery of a promising Ru-Mn-Ca-Pr oxide catalyst. Our workflow also enhances theoretical simulations with domain adaptation strategy, providing deeper mechanistic insights aligned with experimental findings. By leveraging diverse data sources and multiple ML strategies, we establish an efficient pathway for electrocatalyst discovery and optimization. This comprehensive, data-driven approach represents a paradigm shift and potentially new benchmark in electrocatalysts research.","['Rui Ding', 'Jianguo Liu', 'Kang Hua', 'Xuebin Wang', 'Xiaoben Zhang', 'Minhua Shao', 'Yuxin Chen', 'Junhong Chen']",,arXiv,2024,https://doi.org/10.48550/arXiv.2407.04877,Anomali
OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation,"Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M, are either with low quality or too large for most research institutions. Therefore, it is challenging but crucial to collect a precise high-qualitytext-video pairs for T2V generation. 2) Ignoring to fully utilize textual information. Recent T2V methods have focused on vision transformers, using a simple cross attention module for video generation, which falls short of thoroughly extracting semantic information fromtextprompt. To address these issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive captions. This open-scenario dataset contains over 1 milliontext-video pairs, facilitating research on T2V generation. Furthermore, we curate 433K 1080p videos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition video generation. Additionally, we propose a novel Multi-modal Video Diffusion Transformer (MVDiT) capable ofminingboth structure information from visual tokens and semantic information fromtexttokens. Extensive experiments and ablation studies verify the superiority of OpenVid-1M over previous datasets and the effectiveness of our MVDiT.","['Kepan Nan', 'Rui Xie', 'Penghao Zhou', 'Tiehan Fan', 'Zhenheng Yang', 'Zhijie Chen', 'Xiang Li', 'Jian Yang', 'Ying Tai']",,arXiv,2025,https://doi.org/10.48550/arXiv.2407.02371,Anomali
Mining Reasons For And Against Vaccination From Unstructured Data Using Nichesourcing and AI Data Augmentation,"We present Reasons For and Against Vaccination (RFAV), a dataset for predicting reasons for and against vaccination, and scientific authorities used to justify them, annotated through nichesourcing and augmented using GPT4 and GPT3.5-Turbo. We show how it is possible tominethese reasons in non-structuredtext, under different task definitions, despite the high level of subjectivity involved and explore the impact of artificially augmented data using in-context learning with GPT4 and GPT3.5-Turbo. We publish the dataset and the trained models along with the annotation manual used to train annotators and define the task.","['Damián Ariel Furman', 'Juan Junqueras', 'Z. Burçe Gümüslü', 'Edgar Altszyler', 'Joaquin Navajas', 'Ophelia Deroy', 'Justin Sulik']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.19951,Anomali
The global landscape of academic guidelines for generative AI and Large Language Models,"The integration of Generative Artificial Intelligence (GAI) and Large Language Models (LLMs) in academia has spurred a global discourse on their potential pedagogical benefits and ethical considerations. Positive reactions highlight some potential, such as collaborative creativity, increased access to education, and empowerment of trainers and trainees. However, negative reactions raise concerns about ethical complexities, balancing innovation and academic integrity, unequal access, and misinformation risks. Through a systematic survey andtext-mining-based analysis of global and national directives, insights from independent research, and eighty university-level guidelines, this study provides a nuanced understanding of the opportunities and challenges posed by GAI and LLMs in education. It emphasizes the importance of balanced approaches that harness the benefits of these technologies while addressing ethical considerations and ensuring equitable access and educational outcomes. The paper concludes with recommendations for fostering responsible innovation and ethical practices to guide the integration of GAI and LLMs in academia.","['Junfeng Jiao', 'Saleh Afroogh', 'Kevin Chen', 'David Atkinson', 'Amit Dhurandhar']",,arXiv,2025,https://doi.org/10.48550/arXiv.2406.18842,Anomali
InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for Multi-modal Sarcasm Detection,"Sarcasm in social media, often expressed throughtext-image combinations, poses challenges for sentiment analysis and intentionmining. Current multi-modal sarcasm detection methods have been demonstrated to overly rely on spurious cues within the textual modality, revealing a limited ability to genuinely identify sarcasm through nuancedtext-image interactions. To solve this problem, we propose InterCLIP-MEP, which introduces Interactive CLIP (InterCLIP) with an efficient training strategy to extract enrichedtext-image representations by embedding cross-modal information directly into each encoder. Additionally, we design a Memory-Enhanced Predictor (MEP) with a dynamic dual-channel memory that stores valuable test sample knowledge during inference, acting as a non-parametric classifier for robust sarcasm recognition. Experiments on two benchmarks demonstrate that InterCLIP-MEP achieves state-of-the-art performance, with significant accuracy and F1 score improvements on MMSD and MMSD2.0. Our code is available at https://github.com/CoderChen01/InterCLIP-MEP.","['Junjie Chen', 'Hang Yu', 'Subin Huang', 'Sanmin Liu', 'Linfeng Zhang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.16464,Anomali
Hierarchical Compression of Text-Rich Graphs via Large Language Models,"Text-rich graphs, prevalent in dataminingcontexts like e-commerce and academic graphs, consist of nodes with textual features linked by various relations. Traditional graph machine learning models, such as Graph Neural Networks (GNNs), excel in encoding the graph structural information, but have limited capability in handling richtexton graph nodes. Large Language Models (LLMs), noted for their superiortextunderstanding abilities, offer a solution for processing thetextin graphs but face integration challenges due to their limitation for encoding graph structures and their computational complexities when dealing with extensivetextin large neighborhoods of interconnected nodes. This paper introduces ``Hierarchical Compression'' (HiCom), a novel method to align the capabilities of LLMs with the structure oftext-rich graphs. HiCom processestextin a node's neighborhood in a structured manner by organizing the extensive textual information into a more manageable hierarchy and compressing nodetextstep by step. Therefore, HiCom not only preserves the contextual richness of thetextbut also addresses the computational challenges of LLMs, which presents an advancement in integrating thetextprocessing power of LLMs with the structural complexities oftext-rich graphs. Empirical results show that HiCom can outperform both GNNs and LLM backbones for node classification on e-commerce and citation graphs. HiCom is especially effective for nodes from a dense region in a graph, where it achieves a 3.48% average performance improvement on five datasets while being more efficient than LLM backbones.","['Shichang Zhang', 'Da Zheng', 'Jiani Zhang', 'Qi Zhu', 'Xiang song', 'Soji Adeshina', 'Christos Faloutsos', 'George Karypis', 'Yizhou Sun']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.11884,Anomali
Mining Open Semantics from CLIP: A Relation Transition Perspective for Few-Shot Learning,"Contrastive Vision-Language Pre-training(CLIP) demonstrates impressive zero-shot capability. The key to improve the adaptation of CLIP to downstream task with few exemplars lies in how to effectively model and transfer the useful knowledge embedded in CLIP. Previous workminesthe knowledge typically based on the limited visual samples and close-set semantics (i.e., within target category set of downstream task). However, the aligned CLIP image/textencoders contain abundant relationships between visual features and almost infinite open semantics, which may benefit the few-shot learning but remains unexplored. In this paper, we propose tomineopen semantics as anchors to perform a relation transition from image-anchor relationship to image-target relationship to make predictions. Specifically, we adopt a transformer module which takes the visual feature as ""Query"", thetextfeatures of the anchors as ""Key"" and the similarity matrix between thetextfeatures of anchor and target classes as ""Value"". In this way, the output of such a transformer module represents the relationship between the image and target categories, i.e., the classification predictions. To avoid manually selecting the open semantics, we make the [CLASS] token of inputtextembedding learnable. We conduct extensive experiments on eleven representative classification datasets. The results show that our method performs favorably against previous state-of-the-arts considering few-shot classification settings.","['Cilin Yan', 'Haochen Wang', 'Xiaolong Jiang', 'Yao Hu', 'Xu Tang', 'Guoliang Kang', 'Efstratios Gavves']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.11252,Anomali
Text Grafting: Near-Distribution Weak Supervision for Minority Classes in Text Classification,"For extremely weak-supervisedtextclassification, pioneer research generates pseudo labels byminingtextssimilar to the class names from the raw corpus, which may end up with very limited or even no samples for the minority classes. Recent works have started to generate the relevanttextsby prompting LLMs using the class names or definitions; however, there is a high risk that LLMs cannot generate in-distribution (i.e., similar to the corpus where thetextclassifier will be applied) data, leading to ungeneralizable classifiers. In this paper, we combine the advantages of these two approaches and propose to bridge the gap via a novel framework, \emph{textgrafting}, which aims to obtain clean and near-distribution weak supervision for minority classes. Specifically, we first use LLM-based logits tominemasked templates from the raw corpus, which have a high potential for data synthesis into the target minority class. Then, the templates are filled by state-of-the-art LLMs to synthesize near-distributiontextsfalling into minority classes.Textgrafting shows significant improvement over directminingor synthesis on minority classes. We also use analysis and case studies to comprehend the property oftextgrafting.","['Letian Peng', 'Yi Gu', 'Chengyu Dong', 'Zihan Wang', 'Jingbo Shang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.11115,Anomali
Mining comorbidities: a brief survey,"In this manuscript we will present a brief overview of the comorbidity concept. We will start by laying its foundations and its definitions and then describing the role that machine learning can hold inminingand defining it. The purpose of this short survey is to present a brief overview of the definition of comorbidity as a concept, and showing some of the latest applications and potentialities for the application of natural language processing andtextminingtechniques.",['Giovanna Maria Dimitri'],,arXiv,2024,https://doi.org/10.48550/arXiv.2406.10696,Anomali
Discovering influential text using convolutional neural networks,"Experimental methods for estimating the impacts oftexton human evaluation have been widely used in the social sciences. However, researchers in experimental settings are usually limited to testing a small number of pre-specifiedtexttreatments. While efforts tomineunstructuredtextsfor features that causally affect outcomes have been ongoing in recent years, these models have primarily focused on the topics or specific words oftext, which may not always be the mechanism of the effect. We connect these efforts with NLP interpretability techniques and present a method for flexibly discovering clusters of similartextphrases that are predictive of human reactions totextsusing convolutional neural networks. When used in an experimental setting, this method can identifytexttreatments and their effects under certain assumptions. We apply the method to two datasets. The first enables direct validation of the model's ability to detect phrases known to cause the outcome. The second demonstrates its ability to flexibly discovertexttreatments with varying textual structures. In both cases, the model learns a greater variety oftexttreatments compared to benchmark methods, and thesetextfeatures quantitatively meet or exceed the ability of benchmark methods to predict the outcome.","['Megan Ayers', 'Luke Sanford', 'Margaret Roberts', 'Eddie Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.10086,Anomali
Application of Natural Language Processing in Financial Risk Detection,"This paper explores the application of Natural Language Processing (NLP) in financial risk detection. By constructing an NLP-based financial risk detection model, this study aims to identify and predict potential risks in financial documents and communications. First, the fundamental concepts of NLP and its theoretical foundation, includingtextminingmethods, NLP model design principles, and machine learning algorithms, are introduced. Second, the process oftextdata preprocessing and feature extraction is described. Finally, the effectiveness and predictive performance of the model are validated through empirical research. The results show that the NLP-based financial risk detection model performs excellently in risk identification and prediction, providing effective risk management tools for financial institutions. This study offers valuable references for the field of financial risk management, utilizing advanced NLP techniques to improve the accuracy and efficiency of financial risk detection.","['Liyang Wang', 'Yu Cheng', 'Ao Xiang', 'Jingyu Zhang', 'Haowei Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.09765,Anomali
Exploring Traffic Crash Narratives in Jordan Using Text Mining Analytics,"This study explores traffic crash narratives in an attempt to inform and enhance effective traffic safety policies usingtext-mininganalytics.Textminingtechniques are employed to unravel key themes and trends within the narratives, aiming to provide a deeper understanding of the factors contributing to traffic crashes. This study collected crash data from five major freeways in Jordan that cover narratives of 7,587 records from 2018-2022. An unsupervised learning method was adopted to learn the pattern from crash data. Varioustextminingtechniques, such as topic modeling, keyword extraction, and Word Co-Occurrence Network, were also used to reveal the co-occurrence of crash patterns. Results show thattextmininganalytics is a promising method and underscore the multifactorial nature of traffic crashes, including intertwining human decisions and vehicular conditions. The recurrent themes across all analyses highlight the need for a balanced approach to road safety, merging both proactive and reactive measures. Emphasis on driver education and awareness around animal-related incidents is paramount.","['Shadi Jaradat', 'Taqwa I. Alhadidi', 'Huthaifa I. Ashqar', 'Ahmed Hossain', 'Mohammed Elhenawy']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.09438,Anomali
A Generative Marker Enhanced End-to-End Framework for Argument Mining,"ArgumentMining(AM) involves identifying and extracting Argumentative Components (ACs) and their corresponding Argumentative Relations (ARs). Most of the prior works have broken down these tasks into multiple sub-tasks. Existing end-to-end setups primarily use the dependency parsing approach. This work introduces a generative paradigm-based end-to-end framework argTANL. argTANL frames the argumentative structures into label-augmentedtext, called Augmented Natural Language (ANL). This framework jointly extracts both ACs and ARs from a given argumentativetext. Additionally, this study explores the impact of Argumentative and Discourse markers on enhancing the model's performance within the proposed framework. Two distinct frameworks, Marker-Enhanced argTANL (ME-argTANL) and argTANL with specialized Marker-Based Fine-Tuning, are proposed to achieve this. Extensive experiments are conducted on three standard AM benchmarks to demonstrate the superior performance of the ME-argTANL.","['Nilmadhab Das', 'Vishal Choudhary', 'V. Vijaya Saradhi', 'Ashish Anand']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.08606,Anomali
MM-KWS: Multi-modal Prompts for Multilingual User-defined Keyword Spotting,"In this paper, we propose MM-KWS, a novel approach to user-defined keyword spotting leveraging multi-modal enrollments oftextand speech templates. Unlike previous methods that focus solely on eithertextor speech features, MM-KWS extracts phoneme,text, and speech embeddings from both modalities. These embeddings are then compared with the query speech embedding to detect the target keywords. To ensure the applicability of MM-KWS across diverse languages, we utilize a feature extractor incorporating several multilingual pre-trained models. Subsequently, we validate its effectiveness on Mandarin and English tasks. In addition, we have integrated advanced data augmentation tools for hard caseminingto enhance MM-KWS in distinguishing confusable words. Experimental results on the LibriPhrase and WenetPhrase datasets demonstrate that MM-KWS outperforms prior methods significantly.","['Zhiqi Ai', 'Zhiyong Chen', 'Shugong Xu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.07310,Anomali
In-Context Learning and Fine-Tuning GPT for Argument Mining,"Large Language Models (LLMs) have become ubiquitous in NLP and deep learning. In-Context Learning (ICL) has been suggested as a bridging paradigm between the training-free and fine-tuning LLMs settings. In ICL, an LLM is conditioned to solve tasks by means of a few solved demonstration examples included as prompt. ArgumentMining(AM) aims to extract the complex argumentative structure of atext, and Argument Type Classification (ATC) is an essential sub-task of AM. We introduce an ICL strategy for ATC combining kNN-based examples selection and majority vote ensembling. In the training-free ICL setting, we show that GPT-4 is able to leverage relevant information from only a few demonstration examples and achieve very competitive classification accuracy on ATC. We further set up a fine-tuning strategy incorporating well-crafted structural features given directly in textual form. In this setting, GPT-3.5 achieves state-of-the-art performance on ATC. Overall, these results emphasize the emergent ability of LLMs to grasp global discursive flow in rawtextin both off-the-shelf and fine-tuned setups.","['Jérémie Cabessa', 'Hugo Hernault', 'Umer Mushtaq']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.06699,Anomali
UnSupDLA: Towards Unsupervised Document Layout Analysis,"Document layout analysis is a key area in document research, involving techniques liketextminingand visual analysis. Despite various methods developed to tackle layout analysis, a critical but frequently overlooked problem is the scarcity of labeled data needed for analyses. With the rise of internet use, an overwhelming number of documents are now available online, making the process of accurately labeling them for research purposes increasingly challenging and labor-intensive. Moreover, the diversity of documents online presents a unique set of challenges in maintaining the quality and consistency of these labels, further complicating document layout analysis in the digital era. To address this, we employ a vision-based approach for analyzing document layouts designed to train a network without labels. Instead, we focus on pre-training, initially generating simple object masks from the unlabeled document images. These masks are then used to train a detector, enhancing object detection and segmentation performance. The model's effectiveness is further amplified through several unsupervised training iterations, continuously refining its performance. This approach significantly advances document layout analysis, particularly precision and efficiency, without labels.","['Talha Uddin Sheikh', 'Tahira Shehzadi', 'Khurram Azeem Hashmi', 'Didier Stricker', 'Muhammad Zeshan Afzal']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.06236,Anomali
Text Analysis of ETDs in ProQuest Dissertations and Theses (PQDT) Global (2016-2018),"The information explosion in the form of ETDs poses the challenge of management and extraction of appropriate knowledge for decision-making. Thus, the present study forwards a solution to the above problem by applying topicminingand prediction modeling tools to 263 ETDs submitted to the PQDT Global database during 2016-18 in the field of library science. This study was divided into two phases. The first phase determined the core topics from the ETDs using Topic-Modeling-Tool (TMT), which was based on latent dirichlet allocation (LDA), whereas the second phase employed prediction analysis using RapidMinerplatform to annotate the future research articles on the basis of the modeled topics. The core topics (tags) for the studied period were found to be book history, school librarian, public library, communicative ecology, and informatics followed bytextnetwork and trend analysis on the high probability cooccurred words. Lastly, a prediction model using Support Vector Machine (SVM) classifier was created in order to accurately predict the placement of future ETDs going to be submitted to PQDT Global under the five modeled topics (a to e). The tested dataset against the trained data set for the predictive performed perfectly.",['Manika Lamba'],,arXiv,2024,https://doi.org/10.48550/arXiv.2406.06076,Anomali
Low-Rank Similarity Mining for Multimodal Dataset Distillation,"Though dataset distillation has witnessed rapid development in recent years, the distillation of multimodal data, e.g., image-textpairs, poses unique and under-explored challenges. Unlike unimodal data, image-textcontrastive learning (ITC) data lack inherent categorization and should instead place greater emphasis on modality correspondence. In this work, we propose Low-Rank SimilarityMining(LoRS) for multimodal dataset distillation, that concurrently distills a ground truth similarity matrix with image-textpairs, and leverages low-rank factorization for efficiency and scalability. The proposed approach brings significant improvement to the existing algorithms, marking a significant contribution to the field of visual-language dataset distillation. We advocate adopting LoRS as a foundational synthetic data setup for image-textdataset distillation. Our code is available at https://github.com/silicx/LoRS_Distill.","['Yue Xu', 'Zhilin Lin', 'Yusong Qiu', 'Cewu Lu', 'Yong-Lu Li']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.03793,Anomali
Tiny models from tiny data: Textual and null-text inversion for few-shot distillation,"Few-shot learning deals with problems such as image classification using very few training examples. Recent vision foundation models show excellent few-shot transfer abilities, but are large and slow at inference. Using knowledge distillation, the capabilities of high-performing but slow models can be transferred to tiny, efficient models. However, common distillation methods require a large set of unlabeled data, which is not available in the few-shot setting. To overcome this lack of data, there has been a recent interest in using synthetic data. We expand on this line of research by presenting a novel diffusion model inversion technique (TINT) combining the diversity of textual inversion with the specificity of null-textinversion. Using this method in a few-shot distillation pipeline leads to state-of-the-art accuracy among small student models on popular benchmarks, while being significantly faster than prior work. Popular few-shot benchmarks involve evaluation over a large number of episodes, which is computationally cumbersome for methods involving synthetic data generation. We also present a theoretical analysis on how the accuracy estimator variance depends on the number of episodes and query examples, and use these results to lower the computational effort required for method evaluation. Finally, to further motivate the use of generative models in few-shot distillation, we demonstrate that our method outperforms training on real dataminedfrom the dataset used in the original diffusion model training. Source code is available at https://github.com/pixwse/tiny2.","['Erik Landolsi', 'Fredrik Kahl']",,arXiv,2025,https://doi.org/10.48550/arXiv.2406.03146,Anomali
Readability-guided Idiom-aware Sentence Simplification (RISS) for Chinese,"Chinese sentence simplification faces challenges due to the lack of large-scale labeled parallel corpora and the prevalence of idioms. To address these challenges, we propose Readability-guided Idiom-aware Sentence Simplification (RISS), a novel framework that combines data augmentation techniques with lexcial simplification. RISS introduces two key components: (1) Readability-guided Paraphrase Selection (RPS), a method formininghigh-quality sentence pairs, and (2) Idiom-aware Simplification (IAS), a model that enhances the comprehension and simplification of idiomatic expressions. By integrating RPS and IAS using multi-stage and multi-task learning strategies, RISS outperforms previous state-of-the-art methods on two Chinese sentence simplification datasets. Furthermore, RISS achieves additional improvements when fine-tuned on a small labeled dataset. Our approach demonstrates the potential for more effective and accessible Chinesetextsimplification.","['Jingshen Zhang', 'Xinglu Chen', 'Xinying Qiu', 'Zhimin Wang', 'Wenhe Feng']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.02974,Anomali
FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models,"Recent research in federated large language models (LLMs) has primarily focused on enabling clients to fine-tune their locally deployed homogeneous LLMs collaboratively or on transferring knowledge from server-based LLMs to small language models (SLMs) at downstream clients. However, a significant gap remains in the simultaneous mutual enhancement of both the server's LLM and clients' SLMs. To bridge this gap, we propose FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models. This framework is designed to adaptively transfer knowledge from the server's LLM to clients' SLMs while concurrently enriching the LLM with clients' unique domain insights. We facilitate token alignment using minimum edit distance (MinED) and then selective mutual knowledge transfer between client-side SLMs and a server-side LLM, aiming to collectively enhance their performance. Through extensive experiments across three distinct scenarios, we evaluate the effectiveness of FedMKT using various public LLMs and SLMs on a range of NLPtextgeneration tasks. Empirical results demonstrate that FedMKT simultaneously boosts the performance of both LLMs and SLMs.","['Tao Fan', 'Guoqiang Ma', 'Yan Kang', 'Hanlin Gu', 'Yuanfeng Song', 'Lixin Fan', 'Kai Chen', 'Qiang Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.02224,Anomali
"No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard Negatives via CLIP Knowledge and LLMs","In this study, we explore an alternative approach to enhance contrastivetext-image-3D alignment in the absence of textual descriptions for 3D objects. We introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP knowledge about textual and 2D data to compute the neural perceived similarity between two 3D samples. We employ the proposed methods tomine3D hard negatives, establishing a multimodal contrastive pipeline with hard negative weighting via a custom loss function. We train on different configurations of the proposed hard negativeminingapproach, and we evaluate the accuracy of our models in 3D classification and on the cross-modal retrieval benchmark, testing image-to-shape and shape-to-image retrieval. Results demonstrate that our approach, even without explicittextalignment, achieves comparable or superior performance on zero-shot and standard 3D classification, while significantly improving both image-to-shape and shape-to-image retrieval compared to previous methods.","['Cristian Sbrolli', 'Matteo Matteucci']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.02202,Anomali
ProGEO: Generating Prompts through Image-Text Contrastive Learning for Visual Geo-localization,"Visual Geo-localization (VG) refers to the process to identify the location described in query images, which is widely applied in robotics field and computer vision tasks, such as autonomous driving, metaverse, augmented reality, and SLAM. In fine-grained images lacking specifictextdescriptions, directly applying pure visual methods to represent neighborhood features often leads to the model focusing on overly fine-grained features, unable to fullyminethe semantic information in the images. Therefore, we propose a two-stage training method to enhance visual performance and use contrastive learning tominechallenging samples. We first leverage the multi-modal description capability of CLIP (Contrastive Language-Image Pretraining) to create a set of learnabletextprompts for each geographic image feature to form vague descriptions. Then, by utilizing dynamictextprompts to assist the training of the image encoder, we enable the image encoder to learn better and more generalizable visual features. This strategy of applyingtextto purely visual tasks addresses the challenge of using multi-modal models for geographic images, which often suffer from a lack of precise descriptions, making them difficult to utilize widely. We validate the effectiveness of the proposed strategy on several large-scale visual geo-localization datasets, and our method achieves competitive results on multiple visual geo-localization datasets. Our code and model are available at https://github.com/Chain-Mao/ProGEO.","['Chen Mao', 'Jingqi Hu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.01906,Anomali
FactGenius: Combining Zero-Shot Prompting and Fuzzy Relation Mining to Improve Fact Verification with Knowledge Graphs,"Fact-checking is a crucial natural language processing (NLP) task that verifies the truthfulness of claims by considering reliable evidence. Traditional methods are often limited by labour-intensive data curation and rule-based approaches. In this paper, we present FactGenius, a novel method that enhances fact-checking by combining zero-shot prompting of large language models (LLMs) with fuzzytextmatching on knowledge graphs (KGs). Leveraging DBpedia, a structured linked data dataset derived from Wikipedia, FactGenius refines LLM-generated connections using similarity measures to ensure accuracy. The evaluation of FactGenius on the FactKG, a benchmark dataset for fact verification, demonstrates that it significantly outperforms existing baselines, particularly when fine-tuning RoBERTa as a classifier. The two-stage approach of filtering and validating connections proves crucial, achieving superior performance across various reasoning types and establishing FactGenius as a promising tool for robust fact-checking. The code and materials are available at https://github.com/SushantGautam/FactGenius.",['Sushant Gautam'],,arXiv,2024,https://doi.org/10.48550/arXiv.2406.01311,Anomali
Annotation Guidelines-Based Knowledge Augmentation: Towards Enhancing Large Language Models for Educational Text Classification,"Various machine learning approaches have gained significant popularity for the automated classification of educationaltextto identify indicators of learning engagement -- i.e. learning engagement classification (LEC). LEC can offer comprehensive insights into human learning processes, attracting significant interest from diverse research communities, including Natural Language Processing (NLP), Learning Analytics, and Educational DataMining. Recently, Large Language Models (LLMs), such as ChatGPT, have demonstrated remarkable performance in various NLP tasks. However, their comprehensive evaluation and improvement approaches in LEC tasks have not been thoroughly investigated. In this study, we propose the Annotation Guidelines-based Knowledge Augmentation (AGKA) approach to improve LLMs. AGKA employs GPT 4.0 to retrieve label definition knowledge from annotation guidelines, and then applies the random under-sampler to select a few typical examples. Subsequently, we conduct a systematic evaluation benchmark of LEC, which includes six LEC datasets covering behavior classification (question and urgency level), emotion classification (binary and epistemic emotion), and cognition classification (opinion and cognitive presence). The study results demonstrate that AGKA can enhance non-fine-tuned LLMs, particularly GPT 4.0 and Llama 3 70B. GPT 4.0 with AGKA few-shot outperforms full-shot fine-tuned models such as BERT and RoBERTa on simple binary classification datasets. However, GPT 4.0 lags in multi-class tasks that require a deep understanding of complex semantic information. Notably, Llama 3 70B with AGKA is a promising combination based on open-source LLM, because its performance is on par with closed-source GPT 4.0 with AGKA. In addition, LLMs struggle to distinguish between labels with similar names in multi-class classification.","['Shiqi Liu', 'Sannyuya Liu', 'Lele Sha', 'Zijie Zeng', 'Dragan Gasevic', 'Zhi Liu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.00954,Anomali
Formality Style Transfer in Persian,"This study explores the formality style transfer in Persian, particularly relevant in the face of the increasing prevalence of informal language on digital platforms, which poses challenges for existing Natural Language Processing (NLP) tools. The aim is to transform informaltextinto formal while retaining the original meaning, addressing both lexical and syntactic differences. We introduce a novel model, Fa-BERT2BERT, based on the Fa-BERT architecture, incorporating consistency learning and gradient-based dynamic weighting. This approach improves the model's understanding of syntactic variations, balancing loss components effectively during training. Our evaluation of Fa-BERT2BERT against existing methods employs new metrics designed to accurately measure syntactic and stylistic changes. Results demonstrate our model's superior performance over traditional techniques across various metrics, including BLEU, BERT score, Rouge-l, and proposed metrics underscoring its ability to adeptly navigate the complexities of Persian language style transfer. This study significantly contributes to Persian language processing by enhancing the accuracy and functionality of NLP models and thereby supports the development of more efficient and reliable NLP applications, capable of handling language style transformation effectively, thereby streamlining content moderation, enhancing dataminingresults, and facilitating cross-cultural communication.","['Parastoo Falakaflaki', 'Mehrnoush Shamsfard']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.00867,Anomali
A lexicon obtained and validated by a data-driven approach for organic residues valorization in emerging and developing countries,"Thetextminingmethod presented in this paper was used for annotation of terms related to biological transformation and valorization of organic residues in agriculture in low and middle-income country. Specialized lexicon was obtained through different steps: corpus and extraction of terms, annotation of extracted terms, selection of relevant terms.","['Christiane Rakotomalala', 'Jean-Marie Paillat', 'Frédéric Feder', 'Angel Avadí', 'Laurent Thuriès', 'Marie-Liesse Vermeire', 'Jean-Michel Médoc', 'Tom Wassenaar', 'Caroline Hottelart', 'Lilou Kieffer', 'Elisa Ndjie', 'Mathieu Picart', 'Jorel Tchamgoue', 'Alvin Tulle', 'Laurine Valade', 'Annie Boyer', 'Marie-Christine Duchamp', 'Mathieu Roche']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.00682,Anomali
Large Language Model Pruning,"We surely enjoy the larger the better models for their superior performance in the last couple of years when both the hardware and software support the birth of such extremely huge models. The applied fields includetextminingand others. In particular, the success of LLMs ontextunderstanding andtextgeneration draws attention from researchers who have worked on NLP and related areas for years or even decades. On the side, LLMs may suffer from problems like model overfitting, hallucination, and device limitation to name a few. In this work, we suggest a model pruning technique specifically focused on LLMs. The proposed methodology emphasizes the explainability of deep learning models. By having the theoretical foundation, we obtain a trustworthy deep model so that huge models with a massive number of model parameters become not quite necessary. A mutual information-based estimation is adopted to find neurons with redundancy to eliminate. Moreover, an estimator with well-tuned parameters helps to find precise estimation to guide the pruning procedure. At the same time, we also explore the difference between pruning on large-scale models vs. pruning on small-scale models. The choice of pruning criteria is sensitive in small models but not for large-scale models. It is a novel finding through this work. Overall, we demonstrate the superiority of the proposed model to the state-of-the-art models.","['Hanjuan Huang', 'Hao-Jia Song', 'Hsing-Kuo Pao']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.00030,Anomali
Exploration of Attention Mechanism-Enhanced Deep Learning Models in the Mining of Medical Textual Data,"The research explores the utilization of a deep learning model employing an attention mechanism in medicaltextmining. It targets the challenge of analyzing unstructuredtextinformation within medical data. This research seeks to enhance the model's capability to identify essential medical information by incorporating deep learning and attention mechanisms. This paper reviews the basic principles and typical model architecture of attention mechanisms and shows the effectiveness of their application in the tasks of disease prediction, drug side effect monitoring, and entity relationship extraction. Aiming at the particularity of medicaltexts, an adaptive attention model integrating domain knowledge is proposed, and its ability to understand medical terms and process complex contexts is optimized. The experiment verifies the model's effectiveness in improving task accuracy and robustness, especially when dealing with longtext. The future research path of enhancing model interpretation, realizing cross-domain knowledge transfer, and adapting to low-resource scenarios is discussed in the research outlook, which provides a new perspective and method support for intelligent medical information processing and clinical decision assistance. Finally, cross-domain knowledge transfer and adaptation strategies for low-resource scenarios, providing theoretical basis and technical reference for promoting the development of intelligent medical information processing and clinical decision support systems.","['Lingxi Xiao', 'Muqing Li', 'Yinqiu Feng', 'Meiqi Wang', 'Ziyi Zhu', 'Zexi Chen']",,arXiv,2024,https://doi.org/10.48550/arXiv.2406.00016,Anomali
ESG-FTSE: A corpus of news articles with ESG relevance labels and use cases,"We present ESG-FTSE, the first corpus comprised of news articles with Environmental, Social and Governance (ESG) relevance annotations. In recent years, investors and regulators have pushed ESG investing to the mainstream due to the urgency of climate change. This has led to the rise of ESG scores to evaluate an investment's credentials as socially responsible. While demand for ESG scores is high, their quality varies wildly. Quantitative techniques can be applied to improve ESG scores, thus, responsible investing. To contribute to resource building for ESG and financialtextmining, we pioneer the ESG-FTSE corpus. We further present the first of its kind ESG annotation schema. It has three levels: a binary classification (relevant versus irrelevant news articles), ESG classification (ESG-related news articles), and target company. Both supervised and unsupervised learning experiments for ESG relevance detection were conducted to demonstrate that the corpus can be used in different settings to derive accurate ESG predictions. Keywords: corpus annotation, ESG labels, annotation schema, news article, natural language processing","['Mariya Pavlova', 'Bernard Casey', 'Miaosen Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.20218,Anomali
Student Answer Forecasting: Transformer-Driven Answer Choice Prediction for Language Learning,"Intelligent Tutoring Systems (ITS) enhance personalized learning by predicting student answers to provide immediate and customized instruction. However, recent research has primarily focused on the correctness of the answer rather than the student's performance on specific answer choices, limiting insights into students' thought processes and potential misconceptions. To address this gap, we present MCQStudentBert, an answer forecasting model that leverages the capabilities of Large Language Models (LLMs) to integrate contextual understanding of students' answering history along with thetextof the questions and answers. By predicting the specific answer choices students are likely to make, practitioners can easily extend the model to new answer choices or remove answer choices for the same multiple-choice question (MCQ) without retraining the model. In particular, we compare MLP, LSTM, BERT, and Mistral 7B architectures to generate embeddings from students' past interactions, which are then incorporated into a finetuned BERT's answer-forecasting mechanism. We apply our pipeline to a dataset of language learning MCQ, gathered from an ITS with over 10,000 students to explore the predictive accuracy of MCQStudentBert, which incorporates student interaction patterns, in comparison to correct answer prediction and traditional mastery-learning feature-based approaches. This work opens the door to more personalized content, modularization, and granular support.","['Elena Grazia Gado', 'Tommaso Martorella', 'Luca Zunino', 'Paola Mejia-Domenzain', 'Vinitra Swamy', 'Jibril Frej', 'Tanja Käser']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.20079,Anomali
Knowledge-grounded Adaptation Strategy for Vision-language Models: Building Unique Case-set for Screening Mammograms for Residents Training,"A visual-language model (VLM) pre-trained on natural images andtextpairs poses a significant barrier when applied to medical contexts due to domain shift. Yet, adapting or fine-tuning these VLMs for medical use presents considerable hurdles, including domain misalignment, limited access to extensive datasets, and high-class imbalances. Hence, there is a pressing need for strategies to effectively adapt these VLMs to the medical domain, as such adaptations would prove immensely valuable in healthcare applications. In this study, we propose a framework designed to adeptly tailor VLMs to the medical domain, employing selective sampling and hard-negativeminingtechniques for enhanced performance in retrieval tasks. We validate the efficacy of our proposed approach by implementing it across two distinct VLMs: the in-domain VLM (MedCLIP) and out-of-domain VLMs (ALBEF). We assess the performance of these models both in their original off-the-shelf state and after undergoing our proposed training strategies, using two extensive datasets containing mammograms and their corresponding reports. Our evaluation spans zero-shot, few-shot, and supervised scenarios. Through our approach, we observe a notable enhancement in Recall@K performance for the image-textretrieval task.","['Aisha Urooj Khan', 'John Garrett', 'Tyler Bradshaw', 'Lonie Salkowski', 'Jiwoong Jason Jeong', 'Amara Tariq', 'Imon Banerjee']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.19675,Anomali
A Novel Approach for Automated Design Information Mining from Issue Logs,"Software architectures are usually meticulously designed to address multiple quality concerns and support long-term maintenance. However, due to the imbalance between the cost and value for developers to document design rationales (i.e., the design alternatives and the underlying arguments for making or rejecting decisions), these rationales are often obsolete or even missing. The lack of design knowledge has motivated a number of studies to extract design information from various platforms in recent years. Unfortunately, despite the wealth of discussion records related to design information provided by platforms like open-source communities, existing research often overlooks the underlying arguments behind alternatives due to challenges such as the intricate semantics of discussions and the lack of benchmarks for design rationale extraction. In this paper, we propose a novel method, named by DRMiner, to automaticallyminelatent design rationales from developers' live discussion in open-source community (i.e., issue logs in Jira). To better identify solutions and the arguments supporting them, DRMiner skillfully decomposes the problem into multipletextclassification tasks and tackles them using prompt tuning of language models and customizedtext-related features. To evaluate DRMiner, we acquire issue logs from Cassandra, Flink, and Solr repositories in Jira, and then annotate and process them under a rigorous scheme, ultimately forming a dataset for design rationalemining. Experimental results show that DRMiner achieves an F1 score of 65% forminingdesign rationales, outperforming all baselines with a 7% improvement over GPT-4.0. Furthermore, we investigate the usefulness of the design rationalesminedby DRMiner for automated program repair (APR) and find that the design rationales significantly enhance APR, achieving 14 times higher full-match repairs on average.","['Jiuang Zhao', 'Zitian Yang', 'Li Zhang', 'Xiaoli Lian', 'Donghao Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.19623,Anomali
Transcending Fusion: A Multi-Scale Alignment Method for Remote Sensing Image-Text Retrieval,"Remote Sensing Image-TextRetrieval (RSITR) is pivotal for knowledge services and dataminingin the remote sensing (RS) domain. Considering the multi-scale representations in image content andtextvocabulary can enable the models to learn richer representations and enhance retrieval. Current multi-scale RSITR approaches typically align multi-scale fused image features withtextfeatures, but overlook aligning image-textpairs at distinct scales separately. This oversight restricts their ability to learn joint representations suitable for effective retrieval. We introduce a novel Multi-Scale Alignment (MSA) method to overcome this limitation. Our method comprises three key innovations: (1) Multi-scale Cross-Modal Alignment Transformer (MSCMAT), which computes cross-attention between single-scale image features and localizedtextfeatures, integrating global textual context to derive a matching score matrix within a mini-batch, (2) a multi-scale cross-modal semantic alignment loss that enforces semantic alignment across scales, and (3) a cross-scale multi-modal semantic consistency loss that uses the matching matrix from the largest scale to guide alignment at smaller scales. We evaluated our method across multiple datasets, demonstrating its efficacy with various visual backbones and establishing its superiority over existing state-of-the-art methods. The GitHub URL for our project is: https://github.com/yr666666/MSA","['Rui Yang', 'Shuang Wang', 'Yingping Han', 'Yuanheng Li', 'Dong Zhao', 'Dou Quan', 'Yanhe Guo', 'Licheng Jiao']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.18959,Anomali
Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models,"Long-context modeling capabilities are important for large language models (LLMs) in various applications. However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit strong semantic dependencies across long contexts. In this study, we propose a dataminingframework \textbf{ProLong} that can assign each training sample with a long dependency score, which can be used to rank and filter samples that are more advantageous for enhancing long-context modeling abilities in LLM training. Specifically, we first use delta perplexity scores to measure the \textit{Dependency Strength} betweentextsegments in a given document. Then we refine this metric based on the \textit{Dependency Distance} of these segments to incorporate spatial relationships across long-contexts. Final results are calibrated with a \textit{Dependency Specificity} metric to prevent trivial dependencies introduced by repetitive patterns. Moreover, a random sampling approach is proposed to optimize the computational efficiency of ProLong. Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities.","['Longze Chen', 'Ziqiang Liu', 'Wanwei He', 'Yunshui Li', 'Run Luo', 'Min Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.17915,Anomali
NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models,"Decoder-only LLM-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purposetextembedding tasks, including dense vector-based retrieval. In this work, we introduce NV-Embed, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility. For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negativemining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed-v1 and NV-Embed-v2 models obtained the No.1 position on the MTEB leaderboard (as of May 24 and August 30, 2024, respectively) across 56 tasks, demonstrating the sustained effectiveness of the proposed methods over time. It also achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB. We further provide the analysis of model compression techniques for generalist embedding models.","['Chankyu Lee', 'Rajarshi Roy', 'Mengyao Xu', 'Jonathan Raiman', 'Mohammad Shoeybi', 'Bryan Catanzaro', 'Wei Ping']",,arXiv,2025,https://doi.org/10.48550/arXiv.2405.17428,Anomali
Predicting Likely-Vulnerable Code Changes: Machine Learning-based Vulnerability Protections for Android Open Source Project,"This paper presents a framework that selectively triggers security reviews for incoming source code changes. Functioning as a review bot within a code review service, the framework can automatically request additional security reviews at pre-submit time before the code changes are submitted to a source code repository. Because performing such secure code reviews add cost, the framework employs a classifier trained to identify code changes with a high likelihood of vulnerabilities. The online classifier leverages various types of input features to analyze the review patterns, track the software engineering process, andminespecifictextpatterns within given code changes. The classifier and its features are meticulously chosen and optimized using data from the submitted code changes and reported vulnerabilities in Android Open Source Project (AOSP). The evaluation results demonstrate that our Vulnerability Prevention (VP) framework identifies approximately 80% of the vulnerability-inducing code changes in the dataset with a precision ratio of around 98% and a false positive rate of around 1.7%. We discuss the implications of deploying the VP framework in multi-project settings and future directions for Android security research. This paper explores and validates our approach to code change-granularity vulnerability prediction, offering a preventive technique for software security by preemptively detecting vulnerable code changes before submission.",['Keun Soo Yim'],,arXiv,2024,https://doi.org/10.48550/arXiv.2405.16655,Anomali
Modes of Analyzing Disinformation Narratives With AI/ML/Text Mining to Assist in Mitigating the Weaponization of Social Media,"This paper highlights the developing need for quantitative modes for capturing and monitoring malicious communication in social media. There has been a deliberate ""weaponization"" of messaging through the use of social networks including by politically oriented entities both state sponsored and privately run. The article identifies a use of AI/ML characterization of generalized ""mal-info,"" a broad term which includes deliberate malicious narratives similar with hate speech, which adversely impact society. A key point of the discussion is that this mal-info will dramatically increase in volume, and it will become essential for sharable quantifying tools to provide support for human expert intervention. Despite attempts to introduce moderation on major platforms like Facebook and X/Twitter, there are now established alternative social networks that offer completely unmoderated spaces. The paper presents an introduction to these platforms and the initial results of a qualitative and semi-quantitative analysis of characteristic mal-info posts. The authors perform a rudimentarytextminingfunction for a preliminary characterization in order to evaluate the modes for better-automated monitoring. The action examines several inflammatory terms usingtextanalysis and, importantly, discusses the use of generative algorithms by one political agent in particular, providing some examples of the potential risks to society. This latter is of grave concern, and monitoring tools must be established. This paper presents a preliminary step to selecting relevant sources and to setting a foundation for characterizing the mal-info, which must be monitored. The AI/ML methods provide a means for semi-quantitative signature capture. The impending use of ""mal-GenAI"" is presented.","['Andy Skumanich', 'Han Kyul Kim']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.15987,Anomali
Enhancing Adverse Drug Event Detection with Multimodal Dataset: Corpus Creation and Model Development,"Theminingof adverse drug events (ADEs) is pivotal in pharmacovigilance, enhancing patient safety by identifying potential risks associated with medications, facilitating early detection of adverse events, and guiding regulatory decision-making. Traditional ADE detection methods are reliable but slow, not easily adaptable to large-scale operations, and offer limited information. With the exponential increase in data sources like social media content, biomedical literature, and Electronic Medical Records (EMR), extracting relevant ADE-related information from these unstructuredtextsis imperative. Previous ADEminingstudies have focused ontext-based methodologies, overlooking visual cues, limiting contextual comprehension, and hindering accurate interpretation. To address this gap, we present a MultiModal Adverse Drug Event (MMADE) detection dataset, merging ADE-related textual information with visual aids. Additionally, we introduce a framework that leverages the capabilities of LLMs and VLMs for ADE detection by generating detailed descriptions of medical images depicting ADEs, aiding healthcare professionals in visually identifying adverse events. Using our MMADE dataset, we showcase the significance of integrating visual cues from images to enhance overall performance. This approach holds promise for patient safety, ADE awareness, and healthcare accessibility, paving the way for further exploration in personalized healthcare.","['Pranab Sahoo', 'Ayush Kumar Singh', 'Sriparna Saha', 'Aman Chadha', 'Samrat Mondal']",2024.findings-acl.667,arXiv,2024,https://doi.org/10.48550/arXiv.2405.15766,Anomali
MentalQA: An Annotated Arabic Corpus for Questions and Answers of Mental Healthcare,"Mental health disorders significantly impact people globally, regardless of background, education, or socioeconomic status. However, access to adequate care remains a challenge, particularly for underserved communities with limited resources.Textminingtools offer immense potential to support mental healthcare by assisting professionals in diagnosing and treating patients. This study addresses the scarcity of Arabic mental health resources for developing such tools. We introduce MentalQA, a novel Arabic dataset featuring conversational-style question-and-answer (QA) interactions. To ensure data quality, we conducted a rigorous annotation process using a well-defined schema with quality control measures. Data was collected from a question-answering medical platform. The annotation schema for mental health questions and corresponding answers draws upon existing classification schemes with some modifications. Question types encompass six distinct categories: diagnosis, treatment, anatomy \& physiology, epidemiology, healthy lifestyle, and provider choice. Answer strategies include information provision, direct guidance, and emotional support. Three experienced annotators collaboratively annotated the data to ensure consistency. Our findings demonstrate high inter-annotator agreement, with Fleiss' Kappa of $0.61$ for question types and $0.98$ for answer strategies. In-depth analysis revealed insightful patterns, including variations in question preferences across age groups and a strong correlation between question types and answer strategies. MentalQA offers a valuable foundation for developing Arabictextminingtools capable of supporting mental health professionals and individuals seeking information.","['Hassan Alhuzali', 'Ashwag Alasmari', 'Hamad Alsaleh']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.12619,Anomali
Mining the Explainability and Generalization: Fact Verification Based on Self-Instruction,"Fact-checking based on commercial LLMs has become mainstream. Although these methods offer high explainability, it falls short in accuracy compared to traditional fine-tuning approaches, and data security is also a significant concern. In this paper, we propose a self-instruction based fine-tuning approach for fact-checking that balances accuracy and explainability. Our method consists of Data Augmentation and Improved DPO fine-tuning. The former starts by instructing the model to generate both positive and negative explanations based on claim-evidence pairs and labels, then sampling the dataset according to our customized difficulty standards. The latter employs our proposed improved DPO to fine-tune the model using the generated samples. We fine-tune the smallest-scale LLaMA-7B model and evaluate it on the challenging fact-checking datasets FEVEROUS and HOVER, utilizing four fine-tuning methods and three few-shot learning methods for comparison. The experiments demonstrate that our approach not only retains accuracy comparable to, or even surpassing, traditional fine-tuning methods, but also generates fluent explanationtext. Moreover, it also exhibit high generalization performance. Our method is the first to leverage self-supervised learning for fact-checking and innovatively combines contrastive learning and improved DPO in fine-tuning LLMs, as shown in the experiments.","['Guangyao Lu', 'Yulin Liu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.12579,Anomali
Zero-Shot Stance Detection using Contextual Data Generation with LLMs,"Stance detection, the classification of attitudes expressed in atexttowards a specific topic, is vital for applications like fake news detection and opinionmining. However, the scarcity of labeled data remains a challenge for this task. To address this problem, we propose Dynamic Model Adaptation with Contextual Data Generation (DyMoAdapt) that combines Few-Shot Learning and Large Language Models. In this approach, we aim to fine-tune an existing model at test time. We achieve this by generating new topic-specific data using GPT-3. This method could enhance performance by allowing the adaptation of the model to new topics. However, the results did not increase as we expected. Furthermore, we introduce the Multi Generated Topic VAST (MGT-VAST) dataset, which extends VAST using GPT-3. In this dataset, each context is associated with multiple topics, allowing the model to understand the relationship between contexts and various potential topics","['Ghazaleh Mahmoudi', 'Babak Behkamkia', 'Sauleh Eetemadi']",AAAI-2024 Workshop on Public Sector LLMs: Algorithmic and Sociotechnical Design,arXiv,2024,https://doi.org/10.48550/arXiv.2405.11637,Anomali
DEMO: A Statistical Perspective for Efficient Image-Text Matching,"Image-textmatching has been a long-standing problem, which seeks to connect vision and language through semantic understanding. Due to the capability to manage large-scale raw data, unsupervised hashing-based approaches have gained prominence recently. They typically construct a semantic similarity structure using the natural distance, which subsequently provides guidance to the model optimization process. However, the similarity structure could be biased at the boundaries of semantic distributions, causing error accumulation during sequential optimization. To tackle this, we introduce a novel hashing approach termed Distribution-based StructureMiningwith Consistency Learning (DEMO) for efficient image-textmatching. From a statistical view, DEMO characterizes each image using multiple augmented views, which are considered as samples drawn from its intrinsic semantic distribution. Then, we employ a non-parametric distribution divergence to ensure a robust and precise similarity structure. In addition, we introduce collaborative consistency learning which not only preserves the similarity structure in the Hamming space but also encourages consistency between retrieval distribution from different directions in a self-supervised manner. Through extensive experiments on three benchmark image-textmatching datasets, we demonstrate that DEMO achieves superior performance compared with many state-of-the-art methods.","['Fan Zhang', 'Xian-Sheng Hua', 'Chong Chen', 'Xiao Luo']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.11496,Anomali
"Span-Aggregatable, Contextualized Word Embeddings for Effective Phrase Mining","Dense vector representations for sentences made significant progress in recent years as can be seen on sentence similarity tasks. Real-world phrase retrieval applications, on the other hand, still encounter challenges for effective use of dense representations. We show that when target phrases reside inside noisy context, representing the full sentence with a single dense vector, is not sufficient for effective phrase retrieval. We therefore look into the notion of representing multiple, sub-sentence, consecutive word spans, each with its own dense vector. We show that this technique is much more effective for phrasemining, yet requires considerable compute to obtain useful span representations. Accordingly, we make an argument for contextualized word/token embeddings that can be aggregated for arbitrary word spans while maintaining the span's semantic meaning. We introduce a modification to the common contrastive loss used for sentence embeddings that encourages word embeddings to have this property. To demonstrate the effect of this method we present a dataset based on the STS-B dataset with additional generatedtext, that requires finding the best matching paraphrase residing in a larger context and report the degree of similarity to the origin phrase. We demonstrate on this dataset, how our proposed method can achieve better results without significant increase to compute.","['Eyal Orbach', 'Lev Haikin', 'Nelly David', 'Avi Faizakof']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.07263,Anomali
TAI++: Text as Image for Multi-Label Image Classification by Co-Learning Transferable Prompt,"The recent introduction of prompt tuning based on pre-trained vision-language models has dramatically improved the performance of multi-label image classification. However, some existing strategies that have been explored still have drawbacks, i.e., either exploiting massive labeled visual data at a high cost or usingtextdata only fortextprompt tuning and thus failing to learn the diversity of visual knowledge. Hence, the application scenarios of these methods are limited. In this paper, we propose a pseudo-visual prompt~(PVP) module for implicit visual prompt tuning to address this problem. Specifically, we first learn the pseudo-visual prompt for each category,miningdiverse visual knowledge by the well-aligned space of pre-trained vision-language models. Then, a co-learning strategy with a dual-adapter module is designed to transfer visual knowledge from pseudo-visual prompt totextprompt, enhancing their visual representation abilities. Experimental results on VOC2007, MS-COCO, and NUSWIDE datasets demonstrate that our method can surpass state-of-the-art~(SOTA) methods across various settings for multi-label image classification tasks. The code is available at https://github.com/njustkmg/PVP.","['Xiangyu Wu', 'Qing-Yuan Jiang', 'Yang Yang', 'Yi-Feng Wu', 'Qing-Guo Chen', 'Jianfeng Lu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.06926,Anomali
Can Large Language Models Replicate ITS Feedback on Open-Ended Math Questions?,"Intelligent Tutoring Systems (ITSs) often contain an automated feedback component, which provides a predefined feedback message to students when they detect a predefined error. To such a feedback component, we often resort to template-based approaches. These approaches require significant effort from human experts to detect a limited number of possible student errors and provide corresponding feedback. This limitation is exemplified in open-ended math questions, where there can be a large number of different incorrect errors. In our work, we examine the capabilities of large language models (LLMs) to generate feedback for open-ended math questions, similar to that of an established ITS that uses a template-based approach. We fine-tune both open-source and proprietary LLMs on real student responses and corresponding ITS-provided feedback. We measure the quality of the generated feedback usingtextsimilarity metrics. We find that open-source and proprietary models both show promise in replicating the feedback they see during training, but do not generalize well to previously unseen student errors. These results suggest that despite being able to learn the formatting of feedback, LLMs are not able to fully understand mathematical errors made by students.","['Hunter McNichols', 'Jaewook Lee', 'Stephen Fancsali', 'Steve Ritter', 'Andrew Lan']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.06414,Anomali
Machine Learning-based NLP for Emotion Classification on a Cholera X Dataset,"Recent social media posts on the cholera outbreak in Hammanskraal have highlighted the diverse range of emotions people experienced in response to such an event. The extent of people's opinions varies greatly depending on their level of knowledge and information about the disease. The documented re-search about Cholera lacks investigations into the classification of emotions. This study aims to examine the emotions expressed in social media posts about Chol-era. A dataset of 23,000 posts was extracted and pre-processed. The Python Nat-ural Language Toolkit (NLTK) sentiment analyzer library was applied to deter-minethe emotional significance of eachtext. Additionally, Machine Learning (ML) models were applied for emotion classification, including Long short-term memory (LSTM), Logistic regression, Decision trees, and the Bidirectional En-coder Representations from Transformers (BERT) model. The results of this study demonstrated that LSTM achieved the highest accuracy of 75%. Emotion classification presents a promising tool for gaining a deeper understanding of the impact of Cholera on society. The findings of this study might contribute to the development of effective interventions in public health strategies.","['Paul Jideani', 'Aurona Gerber']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.04897,Anomali
COM3D: Leveraging Cross-View Correspondence and Cross-Modal Mining for 3D Retrieval,"In this paper, we investigate an open research task of cross-modal retrieval between 3D shapes and textual descriptions. Previous approaches mainly rely on point cloud encoders for feature extraction, which may ignore key inherent features of 3D shapes, including depth, spatial hierarchy, geometric continuity, etc. To address this issue, we propose COM3D, making the first attempt to exploit the cross-view correspondence and cross-modalminingto enhance the retrieval performance. Notably, we augment the 3D features through a scene representation transformer, to generate cross-view correspondence features of 3D shapes, which enrich the inherent features and enhance their compatibility withtextmatching. Furthermore, we propose to optimize the cross-modal matching process based on the semi-hard negative exampleminingmethod, in an attempt to improve the learning efficiency. Extensive quantitative and qualitative experiments demonstrate the superiority of our proposed COM3D, achieving state-of-the-art results on the Text2Shape dataset.","['Hao Wu', 'Ruochong LI', 'Hao Wang', 'Hui Xiong']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.04103,Anomali
Liberating Seen Classes: Boosting Few-Shot and Zero-Shot Text Classification via Anchor Generation and Classification Reframing,"Few-shot and zero-shottextclassification aim to recognize samples from novel classes with limited labeled samples or no labeled samples at all. While prevailing methods have shown promising performance via transferring knowledge from seen classes to unseen classes, they are still limited by (1) Inherent dissimilarities among classes make the transformation of features learned from seen classes to unseen classes both difficult and inefficient. (2) Rare labeled novel samples usually cannot provide enough supervision signals to enable the model to adjust from the source distribution to the target distribution, especially for complicated scenarios. To alleviate the above issues, we propose a simple and effective strategy for few-shot and zero-shottextclassification. We aim to liberate the model from the confines of seen classes, thereby enabling it to predict unseen categories without the necessity of training on seen classes. Specifically, forminingmore related unseen category knowledge, we utilize a large pre-trained language model to generate pseudo novel samples, and select the most representative ones as category anchors. After that, we convert the multi-class classification task into a binary classification task and use the similarities of query-anchor pairs for prediction to fully leverage the limited supervision signals. Extensive experiments on six widely used public datasets show that our proposed method can outperform other strong baselines significantly in few-shot and zero-shot tasks, even without using any seen class samples.","['Han Liu', 'Siyang Zhao', 'Xiaotong Zhang', 'Feng Zhang', 'Wei Wang', 'Fenglong Ma', 'Hongyang Chen', 'Hong Yu', 'Xianchao Zhang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.03565,Anomali
Knowledge-aware Text-Image Retrieval for Remote Sensing Images,"Image-based retrieval in large Earth observation archives is challenging because one needs to navigate across thousands of candidate matches only with the query image as a guide. By usingtextas information supporting the visual query, the retrieval system gains in usability, but at the same time faces difficulties due to the diversity of visual signals that cannot be summarized by a short caption only. For this reason, as a matching-based task, cross-modaltext-image retrieval often suffers from information asymmetry betweentextsand images. To address this challenge, we propose a Knowledge-awareText-Image Retrieval (KTIR) method for remote sensing images. Byminingrelevant information from an external knowledge graph, KTIR enriches thetextscope available in the search query and alleviates the information gaps betweentextsand images for better matching. Moreover, by integrating domain-specific knowledge, KTIR also enhances the adaptation of pre-trained vision-language models to remote sensing applications. Experimental results on three commonly used remote sensingtext-image retrieval benchmarks show that the proposed knowledge-aware method leads to varied and consistent retrievals, outperforming state-of-the-art retrieval methods.","['Li Mi', 'Xianjie Dai', 'Javiera Castillo-Navarro', 'Devis Tuia']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.03373,Anomali
101 Billion Arabic Words Dataset,"In recent years, Large Language Models have revolutionized the field of natural language processing, showcasing an impressive rise predominantly in English-centric domains. These advancements have set a global benchmark, inspiring significant efforts toward developing Arabic LLMs capable of understanding and generating the Arabic language with remarkable accuracy. Despite these advancements, a critical challenge persists: the potential bias in Arabic LLMs, primarily attributed to their reliance on datasets comprising English data that has been translated into Arabic. This reliance not only compromises the authenticity of the generated content but also reflects a broader issue -the scarcity of original quality Arabic linguistic data. This study aims to address the data scarcity in the Arab world and to encourage the development of Arabic Language Models that are true to both the linguistic and nuances of the region. We undertook a large-scale dataminingproject, extracting a substantial volume oftextfrom the Common Crawl WET files, specifically targeting Arabic content. The extracted data underwent a rigorous cleaning and deduplication process, using innovative techniques to ensure the integrity and uniqueness of the dataset. The result is the 101 Billion Arabic Words Dataset, the largest Arabic dataset available to date, which can significantly contribute to the development of authentic Arabic LLMs. This study not only highlights the potential for creating linguistically and culturally accurate Arabic LLMs but also sets a precedent for future research in enhancing the authenticity of Arabic language models.","['Manel Aloui', 'Hasna Chouikhi', 'Ghaith Chaabane', 'Haithem Kchaou', 'Chehir Dhaouadi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.01590,Anomali
An Exploratory Case Study on Data Breach Journalism,"This paper explores the novel topic of data breach journalism and data breach news through the case of databreaches.net, a news outlet dedicated to data breaches and related cyber crime. Motivated by the issues in traditional crime news and crime journalism, the case is explored by the means oftextmining. According to the results, the outlet has kept a steady publishing pace, mainly focusing on plain and short reporting but with generally high-quality source material for the news articles. Despite these characteristics, the news articles exhibit fairly strong sentiments, which is partially expected due to the presence of emotionally laden crime and the long history of sensationalism in crime news. The news site has also covered the full scope of data breaches, although many of these are fairly traditional, exposing personal identifiers and financial details of the victims. Also hospitals and the healthcare sector stand out. With these results, the paper advances the study of data breaches by considering these from the perspective of media and journalism.","['Jukka Ruohonen', 'Kalle Hjerppe', 'Maximilian von Zastrow']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.01446,Anomali
WIBA: What Is Being Argued? A Comprehensive Approach to Argument Mining,"We propose WIBA, a novel framework and suite of methods that enable the comprehensive understanding of ""What Is Being Argued"" across contexts. Our approach develops a comprehensive framework that detects: (a) the existence, (b) the topic, and (c) the stance of an argument, correctly accounting for the logical dependence among the three tasks. Our algorithm leverages the fine-tuning and prompt-engineering of Large Language Models. We evaluate our approach and show that it performs well in all the three capabilities. First, we develop and release an Argument Detection model that can classify a piece oftextas an argument with an F1 score between 79% and 86% on three different benchmark datasets. Second, we release a language model that can identify the topic being argued in a sentence, be it implicit or explicit, with an average similarity score of 71%, outperforming current naive methods by nearly 40%. Finally, we develop a method for Argument Stance Classification, and evaluate the capability of our approach, showing it achieves a classification F1 score between 71% and 78% across three diverse benchmark datasets. Our evaluation demonstrates that WIBA allows the comprehensive understanding of What Is Being Argued in large corpora across diverse contexts, which is of core interest to many applications in linguistics, communication, and social and computer science. To facilitate accessibility to the advancements outlined in this work, we release WIBA as a free open access platform (wiba.dev).","['Arman Irani', 'Ju Yeon Park', 'Kevin Esterling', 'Michalis Faloutsos']",,arXiv,2024,https://doi.org/10.48550/arXiv.2405.00828,Anomali
ThangDLU at #SMM4H 2024: Encoder-decoder models for classifying text data on social disorders in children and adolescents,"This paper describes our participation in Task 3 and Task 5 of the #SMM4H (Social MediaMiningfor Health) 2024 Workshop, explicitly targeting the classification challenges within tweet data. Task 3 is a multi-class classification task centered on tweets discussing the impact of outdoor environments on symptoms of social anxiety. Task 5 involves a binary classification task focusing on tweets reporting medical disorders in children. We applied transfer learning from pre-trained encoder-decoder models such as BART-base and T5-small to identify the labels of a set of given tweets. We also presented some data augmentation methods to see their impact on the model performance. Finally, the systems obtained the best F1 score of 0.627 in Task 3 and the best F1 score of 0.841 in Task 5.","['Hoang-Thang Ta', 'Abu Bakar Siddiqur Rahman', 'Lotfollah Najjar', 'Alexander Gelbukh']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.19714,Anomali
T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients,"The development of machine learning applications has increased significantly in recent years, motivated by the remarkable ability of learning-powered systems to discover and generalize intricate patterns hidden in massive datasets. Modern learning models, while powerful, often exhibit a complexity level that renders them opaque black boxes, lacking transparency and hindering our understanding of their decision-making processes. Opacity challenges the practical application of machine learning, especially in critical domains requiring informed decisions. Explainable Artificial Intelligence (XAI) addresses that challenge, unraveling the complexity of black boxes by providing explanations. Feature attribution/importance XAI stands out for its ability to delineate the significance of input features in predictions. However, most attribution methods have limitations, such as instability, when divergent explanations result from similar or the same instance. This work introduces T-Explainer, a novel additive attribution explainer based on the Taylor expansion that offers desirable properties such as local accuracy and consistency. We demonstrate T-Explainer's effectiveness and stability over multiple runs in quantitative benchmark experiments against well-known attribution methods. Additionally, we provide several tools to evaluate and visualize explanations, turning T-Explainer into a comprehensive XAI framework.","['Evandro S. Ortigossa', 'Fábio F. Dias', 'Brian Barr', 'Claudio T. Silva', 'Luis Gustavo Nonato']",,arXiv,2025,https://doi.org/10.48550/arXiv.2404.16495,Anomali
Lost in Recursion: Mining Rich Event Semantics in Knowledge Graphs,"Our world is shaped by events of various complexity. This includes both small-scale local events like local farmer markets and large complex events like political and military conflicts. The latter are typically not observed directly but through the lenses of intermediaries like newspapers or social media. In other words, we do not witness the unfolding of such events directly but are confronted with narratives surrounding them. Such narratives capture different aspects of a complex event and may also differ with respect to the narrator. Thus, they provide a rich semantics concerning real-world events. In this paper, we show how narratives concerning complex events can be constructed and utilized. We provide a formal representation of narratives based on recursive nodes to represent multiple levels of detail and discuss how narratives can be bound to event-centric knowledge graphs. Additionally, we provide an algorithm based on incremental prompting techniques thatminessuch narratives fromtextsto account for different perspectives on complex events. Finally, we show the effectiveness and future research directions in a proof of concept.","['Florian Plötzky', 'Niklas Kiehne', 'Wolf-Tilo Balke']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.16405,Anomali
Hi-Gen: Generative Retrieval For Large-Scale Personalized E-commerce Search,"Leveraging generative retrieval (GR) techniques to enhance search systems is an emerging methodology that has shown promising results in recent years. In GR, atext-to-textmodel maps string queries directly to relevant document identifiers (docIDs), dramatically simplifying the retrieval process. However, when applying most GR models in large-scale E-commerce for personalized item search, we must face two key problems in encoding and decoding. (1) Existing docID generation methods ignore the encoding of efficiency information, which is critical in E-commerce. (2) The positional information is important in decoding docIDs, while prior studies have not adequately discriminated the significance of positional information or well exploited the inherent interrelation among these positions. To overcome these problems, we introduce an efficient Hierarchical encoding-decoding Generative retrieval method (Hi-Gen) for large-scale personalized E-commerce search systems. Specifically, we first design a representation learning model using metric learning to learn discriminative feature representations of items to capture semantic relevance and efficiency information. Then, we propose a category-guided hierarchical clustering scheme that makes full use of the semantic and efficiency information of items to facilitate docID generation. Finally, we design a position-aware loss to discriminate the importance of positions andminethe inherent interrelation between different tokens at the same position. This loss boosts the performance of the language model used in the decoding stage. Besides, we propose two variants of Hi-Gen (Hi-Gen-I2I and Hi-Gen-Cluster) to support online real-time large-scale recall in the online serving process. Hi-Gen gets 3.30% and 4.62% improvements over SOTA for Recall@1 on the public and industry datasets, respectively.","['Yanjing Wu', 'Yinfu Feng', 'Jian Wang', 'Wenji Zhou', 'Yunan Ye', 'Rong Xiao', 'Jun Xiao']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.15675,Anomali
Multi-Modal Prompt Learning on Blind Image Quality Assessment,"Image Quality Assessment (IQA) models benefit significantly from semantic information, which allows them to treat different types of objects distinctly. Currently, leveraging semantic information to enhance IQA is a crucial research direction. Traditional methods, hindered by a lack of sufficiently annotated data, have employed the CLIP image-textpretraining model as their backbone to gain semantic awareness. However, the generalist nature of these pre-trained Vision-Language (VL) models often renders them suboptimal for IQA-specific tasks. Recent approaches have attempted to address this mismatch using prompt technology, but these solutions have shortcomings. Existing prompt-based VL models overly focus on incremental semantic information fromtext, neglecting the rich insights available from visual data analysis. This imbalance limits their performance improvements in IQA tasks. This paper introduces an innovative multi-modal prompt-based methodology for IQA. Our approach employs carefully crafted prompts that synergisticallymineincremental semantic information from both visual and linguistic data. Specifically, in the visual branch, we introduce a multi-layer prompt structure to enhance the VL model's adaptability. In thetextbranch, we deploy a dual-prompt scheme that steers the model to recognize and differentiate between scene category and distortion type, thereby refining the model's capacity to assess image quality. Our experimental findings underscore the effectiveness of our method over existing Blind Image Quality Assessment (BIQA) approaches. Notably, it demonstrates competitive performance across various datasets. Our method achieves Spearman Rank Correlation Coefficient (SRCC) values of 0.961(surpassing 0.946 in CSIQ) and 0.941 (exceeding 0.930 in KADID), illustrating its robustness and accuracy in diverse contexts.","['Wensheng Pan', 'Timin Gao', 'Yan Zhang', 'Runze Hu', 'Xiawu Zheng', 'Enwei Zhang', 'Yuting Gao', 'Yutao Liu', 'Yunhang Shen', 'Ke Li', 'Shengchuan Zhang', 'Liujuan Cao', 'Rongrong Ji']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.14949,Anomali
StoryTTS: A Highly Expressive Text-to-Speech Dataset with Rich Textual Expressiveness Annotations,"While acoustic expressiveness has long been studied in expressivetext-to-speech (ETTS), the inherent expressiveness intextlacks sufficient attention, especially for ETTS of artistic works. In this paper, we introduce StoryTTS, a highly ETTS dataset that contains rich expressiveness both in acoustic and textual perspective, from the recording of a Mandarin storytelling show. A systematic and comprehensive labeling framework is proposed for textual expressiveness. We analyze and define speech-related textual expressiveness in StoryTTS to include five distinct dimensions through linguistics, rhetoric, etc. Then we employ large language models and prompt them with a few manual annotation examples for batch annotation. The resulting corpus contains 61 hours of consecutive and highly prosodic speech equipped with accuratetexttranscriptions and rich textual expressiveness annotations. Therefore, StoryTTS can aid future ETTS research to fullyminethe abundant intrinsic textual and acoustic features. Experiments are conducted to validate that TTS models can generate speech with improved expressiveness when integrating with the annotated textual labels in StoryTTS.","['Sen Liu', 'Yiwei Guo', 'Xie Chen', 'Kai Yu']","IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024, pp. 11521-11525",arXiv,2024,https://doi.org/10.48550/arXiv.2404.14946,Anomali
Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction,"The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks, such astextsummarization andtextmining. Previous approaches often generate tables that directly replicate information from thetext, limiting their applicability in broader contexts, astext-to-table generation in real-life scenarios necessitates information extraction, reasoning, and integration. However, there is a lack of both datasets and methodologies towards this task. In this paper, we introduce LiveSum, a new benchmark dataset created for generating summary tables of competitions based on real-time commentarytexts. We evaluate the performances of state-of-the-art LLMs on this task in both fine-tuning and zero-shot settings, and additionally propose a novel pipeline called $T^3$(Text-Tuple-Table) to improve their performances. Extensive experimental results demonstrate that LLMs still struggle with this task even after fine-tuning, while our approach can offer substantial performance gains without explicit training. Further analyses demonstrate that our method exhibits strong generalization abilities, surpassing previous approaches on several othertext-to-table datasets. Our code and data can be found at https://github.com/HKUST-KnowComp/LiveSum.","['Zheye Deng', 'Chunkit Chan', 'Weiqi Wang', 'Yuxi Sun', 'Wei Fan', 'Tianshi Zheng', 'Yauwai Yim', 'Yangqiu Song']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.14215,Anomali
MambaUIE&SR: Unraveling the Ocean's Secrets with Only 2.8 GFLOPs,"Underwater Image Enhancement (UIE) techniques aim to address the problem of underwater image degradation due to light absorption and scattering. In recent years, both Convolution Neural Network (CNN)-based and Transformer-based methods have been widely explored. In addition, combining CNN and Transformer can effectively combine global and local information for enhancement. However, this approach is still affected by the secondary complexity of the Transformer and cannot maximize the performance. Recently, the state-space model (SSM) based architecture Mamba has been proposed, which excels in modeling long distances while maintaining linear complexity. This paper explores the potential of this SSM-based model for UIE from both efficiency and effectiveness perspectives. However, the performance of directly applying Mamba is poor because local fine-grained features, which are crucial for image enhancement, cannot be fully utilized. Specifically, we customize the MambaUIE architecture for efficient UIE. Specifically, we introduce visual state space (VSS) blocks to capture global contextual information at the macro level whilemininglocal information at the micro level. Also, for these two kinds of information, we propose a Dynamic Interaction Block (DIB) and Spatial feed-forward Network (SGFN) for intra-block feature aggregation. MambaUIE is able to efficiently synthesize global and local information and maintains a very small number of parameters with high accuracy. Experiments on UIEB datasets show that our method reduces GFLOPs by 67.4% (2.715G) relative to the SOTA method. To the best of our knowledge, this is the first UIE model constructed based on SSM that breaks the limitation of FLOPs on accuracy in UIE. The official repository of MambaUIE at https://github.com/1024AILab/MambaUIE.","['Zhihao Chen', 'Yiyuan Ge']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.13884,Anomali
Automated Text Mining of Experimental Methodologies from Biomedical Literature,"Biomedical literature is a rapidly expanding field of science and technology. Classification of biomedicaltextsis an essential part of biomedicine research, especially in the field of biology. This work proposes the fine-tuned DistilBERT, a methodology-specific, pre-trained generative classification language model forminingbiomedicinetexts. The model has proven its effectiveness in linguistic understanding capabilities and has reduced the size of BERT models by 40\% but by 60\% faster. The main objective of this project is to improve the model and assess the performance of the model compared to the non-fine-tuned model. We used DistilBert as a support model and pre-trained on a corpus of 32,000 abstracts and completetextarticles; our results were impressive and surpassed those of traditional literature classification methods by using RNN or LSTM. Our aim is to integrate this highly specialised and specific model into different research industries.",['Ziqing Guo'],,arXiv,2024,https://doi.org/10.48550/arXiv.2404.13779,Anomali
Incubating Text Classifiers Following User Instruction with Nothing but LLM,"In this paper, we aim to generatetextclassification data given arbitrary class definitions (i.e., user instruction), so one can train a smalltextclassifier without any human annotation or raw corpus. Compared with pioneer attempts, our proposed Incubator is the first framework that can handle complicated and even mutually dependent classes (e.g., ""TED Talk given by Educator"" and ""Other""). Specifically, Incubator is an LLM firstly tuned on the instruction-to-data mappings that we obtained from classification datasets and descriptions on HuggingFace together with in-context augmentation by GPT-4. We then refine Incubator by learning on the cluster centers of semantic textual embeddings to emphasize the uniformity and semantic diversity in generations. We compare Incubator on various classification tasks with strong baselines such as direct LLM-based inference and training data generation by prompt engineering. Experiments show Incubator is able to (1) perform well on traditional benchmarks, (2) take label dependency and user preference into consideration, and (3) enable logicaltextminingby incubating multiple classifiers.","['Letian Peng', 'Jingbo Shang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.10877,Anomali
Coherent control of an optical tweezer phonon laser,"The creation and manipulation of coherence continues to capture the attention of scientists and engineers. The optical laser is a canonical example of a system that, in principle, exhibits complete coherence. Recent research has focused on the creation of coherent, laser-like states in other physical systems. The phonon laser is one example where it is possible to amplify self-sustained mechanical oscillations. A single mode phonon laser in a levitated optical tweezer has been demonstrated through appropriate balance of active feedback gain and damping. In this work, coherent control of the dynamics of an optical tweezer phonon laser is used to share coherence between its different modes of oscillation, creating a multimode phonon laser. The coupling of the modes is achieved by periodically rotating the asymmetric optical potential in the transverse focal plane of the trapping beam via trap laser polarization rotation. The presented theory and experiment demonstrate that coherence can be transferred across different modes of an optical tweezer phonon laser, and are a step toward using these systems for precision measurement and quantum information processing.","['Kai Zhang', 'Kewen Xiao', 'Danika Luntz-Martin', 'Ping Sun', 'S. Sharma', 'M. Bhattacharya', 'A. N. Vamivakas']","Opt. Express 32, 14735-14745 (2024)",arXiv,2024,https://doi.org/10.48550/arXiv.2404.10173,Anomali
Text-to-Song: Towards Controllable Music Generation Incorporating Vocals and Accompaniment,"A song is a combination of singing voice and accompaniment. However, existing works focus on singing voice synthesis and music generation independently. Little attention was paid to explore song synthesis. In this work, we propose a novel task calledtext-to-song synthesis which incorporating both vocals and accompaniments generation. We develop Melodist, a two-stagetext-to-song method that consists of singing voice synthesis (SVS) and vocal-to-accompaniment (V2A) synthesis. Melodist leverages tri-tower contrastive pretraining to learn more effectivetextrepresentation for controllable V2A synthesis. A Chinese song datasetminedfrom a music website is built up to alleviate data scarcity for our research. The evaluation results on our dataset demonstrate that Melodist can synthesize songs with comparable quality and style consistency. Audio samples can be found in https://text2songMelodist.github.io/Sample/.","['Zhiqing Hong', 'Rongjie Huang', 'Xize Cheng', 'Yongqi Wang', 'Ruiqi Li', 'Fuming You', 'Zhou Zhao', 'Zhimeng Zhang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.09313,Anomali
Prior-agnostic Multi-scale Contrastive Text-Audio Pre-training for Parallelized TTS Frontend Modeling,"Over the past decade, a series of unflagging efforts have been dedicated to developing highly expressive and controllabletext-to-speech (TTS) systems. In general, the holistic TTS comprises two interconnected components: the frontend module and the backend module. The frontend excels in capturing linguistic representations from the rawtextinput, while the backend module converts linguistic cues to speech. The research community has shown growing interest in the study of the frontend component, recognizing its pivotal role intext-to-speech systems, includingTextNormalization (TN), Prosody Boundary Prediction (PBP), and Polyphone Disambiguation (PD). Nonetheless, the limitations posed by insufficient annotated textual data and the reliance on homogeneoustextsignals significantly undermine the effectiveness of its supervised learning. To evade this obstacle, a novel two-stage TTS frontend prediction pipeline, named TAP-FM, is proposed in this paper. Specifically, during the first learning phase, we present a Multi-scale ContrastiveText-audio Pre-training protocol (MC-TAP), which hammers at acquiring richer insights via multi-granularity contrastive pre-training in an unsupervised manner. Instead ofmininghomogeneous features in prior pre-training approaches, our framework demonstrates the ability to delve deep into both global and localtext-audio semantic and acoustic representations. Furthermore, a parallelized TTS frontend model is delicately devised to execute TN, PD, and PBP prediction tasks, respectively in the second stage. Finally, extensive experiments illustrate the superiority of our proposed method, achieving state-of-the-art performance.","['Quanxiu Wang', 'Hui Huang', 'Mingjie Wang', 'Yong Dai', 'Jinzuomu Zhong', 'Benlai Tang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.09192,Anomali
PM2: A New Prompting Multi-modal Model Paradigm for Few-shot Medical Image Classification,"Few-shot learning has been successfully applied to medical image classification as only very few medical examples are available for training. Due to the challenging problem of limited number of annotated medical images, image representations should not be solely derived from a single image modality which is insufficient for characterizing concept classes. In this paper, we propose a new prompting multi-modal model paradigm on medical image classification based on multi-modal foundation models, called PM2. Besides image modality,PM2 introduces another supplementarytextinput, known as prompt, to further describe corresponding image or concept classes and facilitate few-shot learning across diverse modalities. To better explore the potential of prompt engineering, we empirically investigate five distinct prompt schemes under the new paradigm. Furthermore, linear probing in multi-modal models acts as a linear classification head taking as input only class token, which ignores completely merits of rich statistics inherent in high-level visual tokens. Thus, we alternatively perform a linear classification on feature distribution of visual tokens and class token simultaneously. To effectivelyminesuch rich statistics, a global covariance pooling with efficient matrix power normalization is used to aggregate visual tokens. Then we study and combine two classification heads. One is shared for class token of image from vision encoder and prompt representation encoded bytextencoder. The other is to classification on feature distribution of visual tokens from vision encoder. Extensive experiments on three medical datasets show that our PM2 significantly outperforms counterparts regardless of prompt schemes and achieves state-of-the-art performance.","['Zhenwei Wang', 'Qiule Sun', 'Bingbing Zhang', 'Pengfei Wang', 'Jianxin Zhang', 'Qiang Zhang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.08915,Anomali
Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck,"Recent advances in language modeling consist in pretraining highly parameterized neural networks on extremely large web-minedtextcorpora. Training and inference with such models can be costly in practice, which incentivizes the use of smaller counterparts. However, it has been observed that smaller models can suffer from saturation, characterized as a drop in performance at some advanced point in training followed by a plateau. In this paper, we find that such saturation can be explained by a mismatch between the hidden dimension of smaller models and the high rank of the target contextual probability distribution. This mismatch affects the performance of the linear prediction head used in such models through the well-known softmax bottleneck phenomenon. We measure the effect of the softmax bottleneck in various settings and find that models based on less than 1000 hidden dimensions tend to adopt degenerate latent representations in late pretraining, which leads to reduced evaluation performance.","['Nathan Godey', 'Éric de la Clergerie', 'Benoît Sagot']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.07647,Anomali
Leveraging Data Augmentation for Process Information Extraction,"Business Process Modeling projects often require formal process models as a central component. High costs associated with the creation of such formal process models motivated many different fields of research aimed at automated generation of process models from readily available data. These include processminingon event logs, and generating business process models from natural languagetexts. Research in the latter field is regularly faced with the problem of limited data availability, hindering both evaluation and development of new techniques, especially learning-based ones.
  To overcome this data scarcity issue, in this paper we investigate the application of data augmentation for natural languagetextdata. Data augmentation methods are well established in machine learning for creating new, synthetic data without human assistance. We find that many of these methods are applicable to the task of business process information extraction, improving the accuracy of extraction. Our study shows, that data augmentation is an important component in enabling machine learning methods for the task of business process model generation from natural languagetext, where currently mostly rule-based systems are still state of the art. Simple data augmentation techniques improved the $F_1$ score of mention extraction by 2.9 percentage points, and the $F_1$ of relation extraction by $4.5$. To better understand how data augmentation alters human annotatedtexts, we analyze the resultingtext, visualizing and discussing the properties of augmented textual data.
  We make all code and experiments results publicly available.","['Julian Neuberger', 'Leonie Doll', 'Benedict Engelmann', 'Lars Ackermann', 'Stefan Jablonski']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.07501,Anomali
Goal-guided Generative Prompt Injection Attack on Large Language Models,"Current large language models (LLMs) provide a strong foundation for large-scale user-oriented natural language tasks. A large number of users can easily inject adversarialtextor instructions through the user interface, thus causing LLMs model security challenges. Although there is currently a large amount of research on prompt injection attacks, most of these black-box attacks use heuristic strategies. It is unclear how these heuristic strategies relate to the success rate of attacks and thus effectively improve model robustness. To solve this problem, we redefine the goal of the attack: to maximize the KL divergence between the conditional probabilities of the cleantextand the adversarialtext. Furthermore, we prove that maximizing the KL divergence is equivalent to maximizing the Mahalanobis distance between the embedded representation $x$ and $x'$ of the cleantextand the adversarialtextwhen the conditional probability is a Gaussian distribution and gives a quantitative relationship on $x$ and $x'$. Then we designed a simple and effective goal-guided generative prompt injection strategy (G2PIA) to find an injectiontextthat satisfies specific constraints to achieve the optimal attack effect approximately. It is particularly noteworthy that our attack method is a query-free black-box attack method with low computational cost. Experimental results on seven LLM models and four datasets show the effectiveness of our attack method.","['Chong Zhang', 'Mingyu Jin', 'Qinkai Yu', 'Chengzhi Liu', 'Haochen Xue', 'Xiaobo Jin']",IEEE International Conference on Data Mining 2024,arXiv,2024,https://doi.org/10.48550/arXiv.2404.07234,Anomali
ClinLinker: Medical Entity Linking of Clinical Concept Mentions in Spanish,"Advances in natural language processing techniques, such as named entity recognition and normalization to widely used standardized terminologies like UMLS or SNOMED-CT, along with the digitalization of electronic health records, have significantly advanced clinicaltextanalysis. This study presents ClinLinker, a novel approach employing a two-phase pipeline for medical entity linking that leverages the potential of in-domain adapted language models for biomedicaltextmining: initial candidate retrieval using a SapBERT-based bi-encoder and subsequent re-ranking with a cross-encoder, trained by following a contrastive-learning strategy to be tailored to medical concepts in Spanish. This methodology, focused initially on content in Spanish, substantially outperforming multilingual language models designed for the same purpose. This is true even for complex scenarios involving heterogeneous medical terminologies and being trained on a subset of the original data. Our results, evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our approach's performance on two distinct clinical entity linking Gold Standard corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures), outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in MedProcNER, both normalized to SNOMED-CT codes. These findings highlight our approach's ability to address language-specific nuances and set a new benchmark in entity linking, offering a potent tool for enhancing the utility of digital medical records. The resulting system is of practical value, both for large scale automatic generation of structured data derived from clinical records, as well as for exhaustive extraction and harmonization of predefined clinical variables of interest.","['Fernando Gallego', 'Guillermo López-García', 'Luis Gasco-Sánchez', 'Martin Krallinger', 'Francisco J. Veredas']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.06367,Anomali
Relation Extraction Using Large Language Models: A Case Study on Acupuncture Point Locations,"In acupuncture therapy, the accurate location of acupoints is essential for its effectiveness. The advanced language understanding capabilities of large language models (LLMs) like Generative Pre-trained Transformers (GPT) present a significant opportunity for extracting relations related to acupoint locations from textual knowledge sources. This study aims to compare the performance of GPT with traditional deep learning models (Long Short-Term Memory (LSTM) and Bidirectional Encoder Representations from Transformers for BiomedicalTextMining(BioBERT)) in extracting acupoint-related location relations and assess the impact of pretraining and fine-tuning on GPT's performance. We utilized the World Health Organization Standard Acupuncture Point Locations in the Western Pacific Region (WHO Standard) as our corpus, which consists of descriptions of 361 acupoints. Five types of relations ('direction_of,' 'distance_of,' 'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints were annotated. Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5, fine-tuned GPT-3.5, as well as pre-trained GPT-4. Performance metrics included micro-average exact match precision, recall, and F1 scores. Our results demonstrate that fine-tuned GPT-3.5 consistently outperformed other models in F1 scores across all relation types. Overall, it achieved the highest micro-average F1 score of 0.92. This study underscores the effectiveness of LLMs like GPT in extracting relations related to acupoint locations, with implications for accurately modeling acupuncture knowledge and promoting standard implementation in acupuncture training and practice. The findings also contribute to advancing informatics applications in traditional and complementary medicine, showcasing the potential of LLMs in natural language processing.","['Yiming Li', 'Xueqing Peng', 'Jianfu Li', 'Xu Zuo', 'Suyuan Peng', 'Donghong Pei', 'Cui Tao', 'Hua Xu', 'Na Hong']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.05415,Anomali
MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems,"Recent advancements in large language models, such as GPT-4, have demonstrated remarkable capabilities in processing standard queries. Despite these advancements, their performance substantially declines in \textbf{advanced mathematical problems requiring complex, multi-step logical reasoning}. To enhance their inferential capabilities, current research has delved into \textit{prompting engineering}, exemplified by methodologies such as the Tree of Thought and Graph of Thought. Nonetheless, these existing approaches encounter two significant limitations. Firstly, their effectiveness in tackling complex mathematical problems is somewhat constrained. Secondly, the necessity to design distinct prompts for individual problems hampers their generalizability. In response to these limitations, this paper introduces the \textit{Multi-Agent System for conditionalMining} (\textbf{MACM}) prompting method. It not only resolves intricate mathematical problems but also demonstrates strong generalization capabilities across various mathematical contexts. With the assistance of MACM, the accuracy of GPT-4 Turbo on the most challenging level five mathematical problems in the MATH dataset increase from$\mathbf{54.68\%} \text{ to } \mathbf{76.73\%}$. The code is available in \url{https://github.com/bin123apple/MACM}.","['Bin Lei', 'Yi Zhang', 'Shan Zuo', 'Ali Payani', 'Caiwen Ding']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.04735,Anomali
Advancing Aspect-Based Sentiment Analysis through Deep Learning Models,"Aspect-based sentiment analysis predicts sentiment polarity with fine granularity. While graph convolutional networks (GCNs) are widely utilized for sentimental feature extraction, their naive application for syntactic feature extraction can compromise information preservation. This study introduces an innovative edge-enhanced GCN, named SentiSys, to navigate the syntactic graph while preserving intact feature information, leading to enhanced performance. Specifically,we first integrate a bidirectional long short-term memory (Bi-LSTM) network and a self-attention-based transformer. This combination facilitates effectivetextencoding, preventing the loss of information and predicting long dependencytext. A bidirectional GCN (Bi-GCN) with message passing is then employed to encode relationships between entities. Additionally, unnecessary information is filtered out using an aspect-specific masking technique. To validate the effectiveness of our proposed model, we conduct extensive evaluation experiments on four benchmark datasets. The experimental results demonstrate enhanced performance in aspect-based sentiment analysis with the use of SentiSys.","['Chen Li', 'Huidong Tang', 'Jinli Zhang', 'Xiujing Guo', 'Debo Cheng', 'Yasuhiko Morimoto']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.03259,Anomali
Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems,"Large language models (LLMs) are trained ontext-only data that go far beyond the languages with paired speech andtextdata. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-textmining. To match speech andtextin many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingualtextunderstanding capabilities to match speech andtextin languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech andtextin 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these languages. Additionally, our model demonstrates cross-lingual speech andtextmatching, which is further enhanced by readily available machine translation data.","['Frank Palma Gomez', 'Ramon Sanabria', 'Yun-hsuan Sung', 'Daniel Cer', 'Siddharth Dalmia', 'Gustavo Hernandez Abrego']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.01616,Anomali
Mining Sequential Patterns in Uncertain Databases Using Hierarchical Index Structure,"In this uncertain world, data uncertainty is inherent in many applications and its importance is growing drastically due to the rapid development of modern technologies. Nowadays, researchers have paid more attention tominepatterns in uncertain databases. A few recent works attempt tominefrequent uncertain sequential patterns. Despite their success, they are incompetent to reduce the number of false-positive pattern generation in theirminingprocess and maintain the patterns efficiently. In this paper, we propose multiple theoretically tightened pruning upper bounds that remarkably reduce theminingspace. A novel hierarchical structure is introduced to maintain the patterns in a space-efficient way. Afterward, we develop a versatile framework formininguncertain sequential patterns that can effectively handle weight constraints as well. Besides, with the advent of incremental uncertain databases, existing works are not scalable. There exist several incremental sequential patternminingalgorithms, but they are limited tominein precise databases. Therefore, we propose a new technique to adapt our framework tominepatterns when the database is incremental. Finally, we conduct extensive experiments on several real-life datasets and show the efficacy of our framework in different applications.","['Kashob Kumar Roy', 'Md Hasibul Haque Moon', 'Md Mahmudur Rahman', 'Chowdhury Farhan Ahmed', 'Carson K. Leung']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.01347,Anomali
Block-Diagonal Guided DBSCAN Clustering,"Cluster analysis plays a crucial role in databasemining, and one of the most widely used algorithms in this field is DBSCAN. However, DBSCAN has several limitations, such as difficulty in handling high-dimensional large-scale data, sensitivity to input parameters, and lack of robustness in producing clustering results. This paper introduces an improved version of DBSCAN that leverages the block-diagonal property of the similarity graph to guide the clustering procedure of DBSCAN. The key idea is to construct a graph that measures the similarity between high-dimensional large-scale data points and has the potential to be transformed into a block-diagonal form through an unknown permutation, followed by a cluster-ordering procedure to generate the desired permutation. The clustering structure can be easily determined by identifying the diagonal blocks in the permuted graph. We propose a gradient descent-based method to solve the proposed problem. Additionally, we develop a DBSCAN-based points traversal algorithm that identifies clusters with high densities in the graph and generates an augmented ordering of clusters. The block-diagonal structure of the graph is then achieved through permutation based on the traversal order, providing a flexible foundation for both automatic and interactive cluster analysis. We introduce a split-and-refine algorithm to automatically search for all diagonal blocks in the permuted graph with theoretically optimal guarantees under specific cases. We extensively evaluate our proposed approach on twelve challenging real-world benchmark clustering datasets and demonstrate its superior performance compared to the state-of-the-art clustering method on every dataset.",['Weibing Zhao'],,arXiv,2024,https://doi.org/10.48550/arXiv.2404.01341,Anomali
"Automatic detection of relevant information, predictions and forecasts in financial news through topic modelling with Latent Dirichlet Allocation","Financial news items are unstructured sources of information that can beminedto extract knowledge for market screening applications. Manual extraction of relevant information from the continuous stream of finance-related news is cumbersome and beyond the skills of many investors, who, at most, can follow a few sources and authors. Accordingly, we focus on the analysis of financial news to identify relevanttextand, within thattext, forecasts and predictions. We propose a novel Natural Language Processing (NLP) system to assist investors in the detection of relevant financial events in unstructured textual sources by considering both relevance and temporality at the discursive level. Firstly, we segment thetextto group together closely relatedtext. Secondly, we apply co-reference resolution to discover internal dependencies within segments. Finally, we perform relevant topic modelling with Latent Dirichlet Allocation (LDA) to separate relevant from less relevanttextand then analyse the relevanttextusing a Machine Learning-oriented temporal approach to identify predictions and speculative statements. We created an experimental data set composed of 2,158 financial news items that were manually labelled by NLP researchers to evaluate our solution. The ROUGE-L values for the identification of relevanttextand predictions/forecasts were 0.662 and 0.982, respectively. To our knowledge, this is the first work to jointly consider relevance and temporality at the discursive level. It contributes to the transfer of human associative discourse capabilities to expert systems through the combination of multi-paragraph topic segmentation and co-reference resolution to separate author expression patterns, topic modelling with LDA to detect relevanttext, and discursive temporality analysis to identify forecasts and predictions within thistext.","['Silvia García-Méndez', 'Francisco de Arriba-Pérez', 'Ana Barros-Vila', 'Francisco J. González-Castaño', 'Enrique Costa-Montenegro']",,arXiv,2024,https://doi.org/10.48550/arXiv.2404.01338,Anomali
MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions,"Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent works leveragetextinstructions to allow users to more freely express their search intents. However, they primarily focus on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is thattextinstructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions via foundation models. Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relationsminedfrom the web, MagicLens achieves results comparable with or better than prior best on eight benchmarks of various image retrieval tasks, while maintaining high parameter efficiency with a significantly smaller model size. Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens. Code and models are publicly available at https://open-vision-language.github.io/MagicLens/.","['Kai Zhang', 'Yi Luan', 'Hexiang Hu', 'Kenton Lee', 'Siyuan Qiao', 'Wenhu Chen', 'Yu Su', 'Ming-Wei Chang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.19651,Anomali
"Unleashing the Power of AI. A Systematic Review of Cutting-Edge Techniques in AI-Enhanced Scientometrics, Webometrics, and Bibliometrics","Purpose: The study aims to analyze the synergy of Artificial Intelligence (AI), with scientometrics, webometrics, and bibliometrics to unlock and to emphasize the potential of the applications and benefits of AI algorithms in these fields.
  Design/methodology/approach: By conducting a systematic literature review, our aim is to explore the potential of AI in revolutionizing the methods used to measure and analyze scholarly communication, identify emerging research trends, and evaluate the impact of scientific publications. To achieve this, we implemented a comprehensive search strategy across reputable databases such as ProQuest, IEEE Explore, EBSCO, Web of Science, and Scopus. Our search encompassed articles published from January 1, 2000, to September 2022, resulting in a thorough review of 61 relevant articles.
  Findings: (i) Regarding scientometrics, the application of AI yields various distinct advantages, such as conducting analyses of publications, citations, research impact prediction, collaboration, research trend analysis, and knowledge mapping, in a more objective and reliable framework. (ii) In terms of webometrics, AI algorithms are able to enhance web crawling and data collection, web link analysis, web content analysis, social media analysis, web impact analysis, and recommender systems. (iii) Moreover, automation of data collection, analysis of citations, disambiguation of authors, analysis of co-authorship networks, assessment of research impact,textmining, and recommender systems are considered as the potential of AI integration in the field of bibliometrics.
  Originality/value: This study covers the particularly new benefits and potential of AI-enhanced scientometrics, webometrics, and bibliometrics to highlight the significant prospects of the synergy of this integration through AI.","['Hamid Reza Saeidnia', 'Elaheh Hosseini', 'Shadi Abdoli', 'Marcel Ausloos']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.18838,Anomali
A Large Language Model Guided Topic Refinement Mechanism for Short Text Modeling,"Modeling topics effectively in shorttexts, such as tweets and news snippets, is crucial to capturing rapidly evolving social trends. Existing topic models often struggle to accurately capture the underlying semantic patterns of shorttexts, primarily due to the sparse nature of such data. This nature oftextsleads to an unavoidable lack of co-occurrence information, which hinders the coherence and granularity ofminedtopics. This paper introduces a novel model-agnostic mechanism, termed Topic Refinement, which leverages the advancedtextcomprehension capabilities of Large Language Models (LLMs) for short-texttopic modeling. Unlike traditional methods, this post-processing mechanism enhances the quality of topics extracted by various topic modeling methods through prompt engineering. We guide LLMs in identifying semantically intruder words within the extracted topics and suggesting coherent alternatives to replace these words. This process mimics human-like identification, evaluation, and refinement of the extracted topics. Extensive experiments on four diverse datasets demonstrate that Topic Refinement boosts the topic quality and improves the performance in topic-relatedtextclassification tasks.","['Shuyu Chang', 'Rui Wang', 'Peng Ren', 'Qi Wang', 'Haiping Huang']",,arXiv,2025,https://doi.org/10.48550/arXiv.2403.17706,Anomali
The Solution for the CVPR 2023 1st foundation model challenge-Track2,"In this paper, we propose a solution for cross-modal transportation retrieval. Due to the cross-domain problem of traffic images, we divide the problem into two sub-tasks of pedestrian retrieval and vehicle retrieval through a simple strategy. In pedestrian retrieval tasks, we use IRRA as the base model and specifically design an Attribute Classification tominethe knowledge implied by attribute labels. More importantly, We use the strategy of Inclusion Relation Matching to make the image-textpairs with inclusion relation have similar representation in the feature space. For the vehicle retrieval task, we use BLIP as the base model. Since aligning the color attributes of vehicles is challenging, we introduce attribute-based object detection techniques to add color patch blocks to vehicle images for color data augmentation. This serves as strong prior information, helping the model perform the image-textalignment. At the same time, we incorporate labeled attributes into the image-textalignment loss to learn fine-grained alignment and prevent similar images andtextsfrom being incorrectly separated. Our approach ranked first in the final B-board test with a score of 70.9.","['Haonan Xu', 'Yurui Huang', 'Sishun Pan', 'Zhihao Guan', 'Yi Xu', 'Yang Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.17702,Anomali
Decoding excellence: Mapping the demand for psychological traits of operations and supply chain professionals through text mining,"The current study proposes an innovative methodology for the profiling of psychological traits of Operations Management (OM) and Supply Chain Management (SCM) professionals. We use innovative methods and tools oftextminingand social network analysis to map the demand for relevant skills from a set of job descriptions, with a focus on psychological characteristics. The proposed approach aims to evaluate the market demand for specific traits by combining relevant psychological constructs,textminingtechniques, and an innovative measure, namely, the Semantic Brand Score. We apply the proposed methodology to a dataset of job descriptions for OM and SCM professionals, with the objective of providing a mapping of their relevant required skills, including psychological characteristics. In addition, the analysis is then detailed by considering the region of the organization that issues the job description, its organizational size, and the seniority level of the open position in order to understand their nuances. Finally, topic modeling is used to examine key components and their relative significance in job descriptions. By employing a novel methodology and considering contextual factors, we provide an innovative understanding of the attitudinal traits that differentiate professionals. This research contributes to talent management, recruitment practices, and professional development initiatives, since it provides new figures and perspectives to improve the effectiveness and success of Operations Management and Supply Chain Management professionals.","['S. Di Luozzo', 'A. Fronzetti Colladon', 'M. M. Schiraldi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.17546,Anomali
Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval,"We study the zero-shot Composed Image Retrieval (ZS-CIR) task, which is to retrieve the target image given a reference image and a description without training on the triplet datasets. Previous works generate pseudo-word tokens by projecting the reference image features to thetextembedding space. However, they focus on the global visual representation, ignoring the representation of detailed attributes, e.g., color, object number and layout. To address this challenge, we propose a Knowledge-Enhanced Dual-stream zero-shot composed image retrieval framework (KEDs). KEDs implicitly models the attributes of the reference images by incorporating a database. The database enriches the pseudo-word tokens by providing relevant images and captions, emphasizing shared attribute information in various aspects. In this way, KEDs recognizes the reference image from diverse perspectives. Moreover, KEDs adopts an extra stream that aligns pseudo-word tokens with textual concepts, leveraging pseudo-tripletsminedfrom image-textpairs. The pseudo-word tokens generated in this stream are explicitly aligned with fine-grained semantics in thetextembedding space. Extensive experiments on widely used benchmarks, i.e. ImageNet-R, COCO object, Fashion-IQ and CIRR, show that KEDs outperforms previous zero-shot composed image retrieval methods.","['Yucheng Suo', 'Fan Ma', 'Linchao Zhu', 'Yi Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.16005,Anomali
Improving Sampling Methods for Fine-tuning SentenceBERT in Text Streams,"The proliferation of textual data on the Internet presents a unique opportunity for institutions and companies to monitor public opinion about their services and products. Given the rapid generation of such data, thetextstreamminingsetting, which handles sequentially arriving, potentially infinitetextstreams, is often more suitable than traditional batch learning. While pre-trained language models are commonly employed for their high-qualitytextvectorization capabilities in streaming contexts, they face challenges adapting to concept drift - the phenomenon where the data distribution changes over time, adversely affecting model performance. Addressing the issue of concept drift, this study explores the efficacy of seventextsampling methods designed to selectively fine-tune language models, thereby mitigating performance degradation. We precisely assess the impact of these methods on fine-tuning the SBERT model using four different loss functions. Our evaluation, focused on Macro F1-score and elapsed time, employs twotextstream datasets and an incremental SVM classifier to benchmark performance. Our findings indicate that Softmax loss and Batch All Triplets loss are particularly effective fortextstream classification, demonstrating that larger sample sizes generally correlate with improved macro F1-scores. Notably, our proposed WordPieceToken ratio sampling method significantly enhances performance with the identified loss functions, surpassing baseline results.","['Cristiano Mesquita Garcia', 'Alessandro Lameiras Koerich', 'Alceu de Souza Britto Jr', 'Jean Paul Barddal']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.15455,Anomali
Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review,"The year 2023 marked a significant surge in the exploration of applying large language model (LLM) chatbots, notably ChatGPT, across various disciplines. We surveyed the applications of ChatGPT in bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedicaltextmining, drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future developments.","['Jinge Wang', 'Zien Cheng', 'Qiuming Yao', 'Li Liu', 'Dong Xu', 'Gangqing Hu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.15274,Anomali
Methods for Generating Drift in Text Streams,"Systems and individuals produce data continuously. On the Internet, people share their knowledge, sentiments, and opinions, provide reviews about services and products, and so on. Automatically learning from these textual data can provide insights to organizations and institutions, thus preventing financial impacts, for example. To learn from textual data over time, the machine learning system must account for concept drift. Concept drift is a frequent phenomenon in real-world datasets and corresponds to changes in data distribution over time. For instance, a concept drift occurs when sentiments change or a word's meaning is adjusted over time. Although concept drift is frequent in real-world applications, benchmark datasets with labeled drifts are rare in the literature. To bridge this gap, this paper provides four textual drift generation methods to ease the production of datasets with labeled drifts. These methods were applied to Yelp and Airbnb datasets and tested using incremental classifiers respecting the streamminingparadigm to evaluate their ability to recover from the drifts. Results show that all methods have their performance degraded right after the drifts, and the incremental SVM is the fastest to run and recover the previous performance levels regarding accuracy and Macro F1-Score.","['Cristiano Mesquita Garcia', 'Alessandro Lameiras Koerich', 'Alceu de Souza Britto Jr', 'Jean Paul Barddal']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.12328,Anomali
TnT-LLM: Text Mining at Scale with Large Language Models,"Transforming unstructuredtextinto structured and meaningful forms, organized by useful category labels, is a fundamental step intextminingfor downstream analysis and application. However, most existing methods for producing label taxonomies and buildingtext-based label classifiers still rely heavily on domain expertise and manual curation, making the process expensive and time-consuming. This is particularly challenging when the label space is under-specified and large-scale data annotations are unavailable. In this paper, we address these challenges with Large Language Models (LLMs), whose prompt-based interface facilitates the induction and use of large-scale pseudo labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate the process of end-to-end label generation and assignment with minimal human effort for any given use-case. In the first phase, we introduce a zero-shot, multi-stage reasoning approach which enables LLMs to produce and refine a label taxonomy iteratively. In the second phase, LLMs are used as data labelers that yield training samples so that lightweight supervised classifiers can be reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis of user intent and conversational domain for Bing Copilot (formerly Bing Chat), an open-domain chat-based search engine. Extensive experiments using both human and automatic evaluation metrics demonstrate that TnT-LLM generates more accurate and relevant label taxonomies when compared against state-of-the-art baselines, and achieves a favorable balance between accuracy and efficiency for classification at scale. We also share our practical experiences and insights on the challenges and opportunities of using LLMs for large-scaletextminingin real-world applications.","['Mengting Wan', 'Tara Safavi', 'Sujay Kumar Jauhar', 'Yujin Kim', 'Scott Counts', 'Jennifer Neville', 'Siddharth Suri', 'Chirag Shah', 'Ryen W White', 'Longqi Yang', 'Reid Andersen', 'Georg Buscher', 'Dhruv Joshi', 'Nagu Rangan']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.12173,Anomali
Mixture-of-Prompt-Experts for Multi-modal Semantic Understanding,"Deep multimodal semantic understanding that goes beyond the mere superficial content relationmininghas received increasing attention in the realm of artificial intelligence. The challenges of collecting and annotating high-quality multi-modal data have underscored the significance of few-shot learning. In this paper, we focus on two critical tasks under this context: few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis (MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on the unified vision-language model (VLM). Specifically, we design three experts of soft prompts: atextprompt and an image prompt that extract modality-specific features to enrich the single-modal representation, and a unified prompt to assist multi-modal interaction. Additionally, we reorganize Transformer layers into several blocks and introduce cross-modal prompt attention between adjacent blocks, which smoothens the transition from single-modal representation to multi-modal fusion. On both MSD and MSA datasets in few-shot setting, our proposed model not only surpasses the 8.2B model InstructBLIP with merely 2% parameters (150M), but also significantly outperforms other widely-used prompt methods on VLMs or task-specific methods.","['Zichen Wu', 'Hsiu-Yuan Huang', 'Fanyi Qu', 'Yunfang Wu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.11311,Anomali
A Big Data Approach to Understand Sub-national Determinants of FDI in Africa,"Various macroeconomic and institutional factors hinder FDI inflows, including corruption, trade openness, access to finance, and political instability. Existing research mostly focuses on country-level data, with limited exploration of firm-level data, especially in developing countries. Recognizing this gap, recent calls for research emphasize the need for qualitative data analysis to delve into FDI determinants, particularly at the regional level. This paper proposes a novel methodology, based ontextminingand social network analysis, to get information from more than 167,000 online news articles to quantify regional-level (sub-national) attributes affecting FDI ownership in African companies. Our analysis extends information on obstacles to industrial development as mapped by the World Bank Enterprise Surveys. Findings suggest that regional (sub-national) structural and institutional characteristics can play an important role in determining foreign ownership.","['A. Fronzetti Colladon', 'R. Vestrelli', 'S. Bait', 'M. M. Schiraldi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.10239,Anomali
A comprehensive study on Frequent Pattern Mining and Clustering categories for topic detection in Persian text stream,"Topic detection is a complex process and depends on language because it somehow needs to analyzetext. There have been few studies on topic detection in Persian, and the existing algorithms are not remarkable. Therefore, we aimed to study topic detection in Persian. The objectives of this study are: 1) to conduct an extensive study on the best algorithms for topic detection, 2) to identify necessary adaptations to make these algorithms suitable for the Persian language, and 3) to evaluate their performance on Persian social networktexts. To achieve these objectives, we have formulated two research questions: First, considering the lack of research in Persian, what modifications should be made to existing frameworks, especially those developed in English, to make them compatible with Persian? Second, how do these algorithms perform, and which one is superior? There are various topic detection methods that can be categorized into different categories. Frequent pattern and clustering are selected for this research, and a hybrid of both is proposed as a new category. Then, ten methods from these three categories are selected. All of them are re-implemented from scratch, changed, and adapted with Persian. These ten methods encompass different types of topic detection methods and have shown good performance in English. Thetextof Persian social network posts is used as the dataset. Additionally, a new multiclass evaluation criterion, called FS, is used in this paper for the first time in the field of topic detection. Approximately 1.4 billion tokens are processed during experiments. The results indicate that if we are searching for keyword-topics that are easily understandable by humans, the hybrid category is better. However, if the aim is to cluster posts for further analysis, the frequent pattern category is more suitable.","['Elnaz Zafarani-Moattar', 'Mohammad Reza Kangavari', 'Amir Masoud Rahmani']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.10237,Anomali
Enhancing Readmission Prediction with Deep Learning: Extracting Biomedical Concepts from Clinical Texts,"Hospital readmission, defined as patients being re-hospitalized shortly after discharge, is a critical concern as it impacts patient outcomes and healthcare costs. Identifying patients at risk of readmission allows for timely interventions, reducing re-hospitalization rates and overall treatment costs. This study focuses on predicting patient readmission within less than 30 days usingtextminingtechniques applied to discharge reporttextsfrom electronic health records (EHR). Various machine learning and deep learning methods were employed to develop a classification model for this purpose. A novel aspect of this research involves leveraging the Bio-Discharge Summary Bert (BDSS) model along with principal component analysis (PCA) feature extraction to preprocess data for deep learning model input. Our analysis of the MIMIC-III dataset indicates that our approach, which combines the BDSS model with a multilayer perceptron (MLP), outperforms state-of-the-art methods. This model achieved a recall of 94% and an area under the curve (AUC) of 75%, showcasing its effectiveness in predicting patient readmissions. This study contributes to the advancement of predictive modeling in healthcare by integratingtextminingtechniques with deep learning algorithms to improve patient outcomes and optimize resource allocation.","['Rasoul Samani', 'Mohammad Dehghani', 'Fahime Shahrokh']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.09722,Anomali
PreConfig: A Pretrained Model for Automating Network Configuration,"Manual network configuration automation (NCA) tools face significant challenges in versatility and flexibility due to their reliance on extensive domain expertise and manual design, limiting their adaptability to diverse scenarios and complex application needs. This paper introduces PreConfig, an innovative NCA tool that leverages a pretrained language model for automating network configuration tasks. PreConfig is designed to address the complexity and variety of NCA tasks by framing them astext-to-texttransformation problems, thus unifying the tasks of configuration generation, translation, and analysis under a single, versatile model. Our approach overcomes existing tools' limitations by utilizing advances in natural language processing to automatically comprehend and generate network configurations without extensive manual re-engineering. We confront the challenges of integrating domain-specific knowledge into pretrained models and the scarcity of supervision data in the network configuration field. Our solution involves constructing a specialized corpus and further pretraining on network configuration data, coupled with a novel dataminingtechnique for generating task supervision data. The proposed model demonstrates robustness in configuration generation, translation, and analysis, outperforming conventional tools in handling complex networking environments. The experimental results validate the effectiveness of PreConfig, establishing a new direction for automating network configuration tasks with pretrained language models.","['Fuliang Li', 'Haozhi Lang', 'Jiajie Zhang', 'Jiaxing Shen', 'Xingwei Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.09369,Anomali
Answering Diverse Questions via Text Attached with Key Audio-Visual Clues,"Audio-visual question answering (AVQA) requires reference to video content and auditory information, followed by correlating the question to predict the most precise answer. Althoughminingdeeper layers of audio-visual information to interact with questions facilitates the multimodal fusion process, the redundancy of audio-visual parameters tends to reduce the generalization of the inference engine to multiple question-answer pairs in a single video. Indeed, the natural heterogeneous relationship between audiovisuals andtextmakes the perfect fusion challenging, to prevent high-level audio-visual semantics from weakening the network's adaptability to diverse question types, we propose a framework for performing mutual correlation distillation (MCD) to aid question inference. MCD is divided into three main steps: 1) firstly, the residual structure is utilized to enhance the audio-visual soft associations based on self-attention, then key local audio-visual features relevant to the question context are captured hierarchically by shared aggregators and coupled in the form of clues with specific question vectors. 2) Secondly, knowledge distillation is enforced to align audio-visual-textpairs in a shared latent space to narrow the cross-modal semantic gap. 3) And finally, the audio-visual dependencies are decoupled by discarding the decision-level integrations. We evaluate the proposed method on two publicly available datasets containing multiple question-and-answer pairs, i.e., Music-AVQA and AVQA. Experiments show that our method outperforms other state-of-the-art methods, and one interesting finding behind is that removing deep audio-visual features during inference can effectively mitigate overfitting. The source code is released at http://github.com/rikeilong/MCD-forAVQA.","['Qilang Ye', 'Zitong Yu', 'Xin Liu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.06679,Anomali
Advancing Chinese biomedical text mining with community challenges,"Objective: This study aims to review the recent advances in community challenges for biomedicaltextminingin China. Methods: We collected information of evaluation tasks released in community challenges of biomedicaltextmining, including task description, dataset description, data source, task type and related links. A systematic summary and comparative analysis were conducted on various biomedical natural language processing tasks, such as named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction,textclassification,textsimilarity, knowledge graph construction, question answering,textgeneration, and large language model evaluation. Results: We identified 39 evaluation tasks from 6 community challenges that spanned from 2017 to 2023. Our analysis revealed the diverse range of evaluation task types and data sources in biomedicaltextmining. We explored the potential clinical applications of these community challenge tasks from a translational biomedical informatics perspective. We compared with their English counterparts, and discussed the contributions, limitations, lessons and guidelines of these community challenges, while highlighting future directions in the era of large language models. Conclusion: Community challenge evaluation competitions have played a crucial role in promoting technology innovation and fostering interdisciplinary collaboration in the field of biomedicaltextmining. These challenges provide valuable platforms for researchers to develop state-of-the-art solutions.","['Hui Zong', 'Rongrong Wu', 'Jiaxue Cha', 'Weizhe Feng', 'Erman Wu', 'Jiakun Li', 'Aibin Shao', 'Liang Tao', 'Zuofeng Li', 'Buzhou Tang', 'Bairong Shen']",Journal of Biomedical Informatics. 2024;157:104716.,arXiv,2024,https://doi.org/10.48550/arXiv.2403.04261,Anomali
Federated Recommendation via Hybrid Retrieval Augmented Generation,"Federated Recommendation (FR) emerges as a novel paradigm that enables privacy-preserving recommendations. However, traditional FR systems usually represent users/items with discrete identities (IDs), suffering from performance degradation due to the data sparsity and heterogeneity in FR. On the other hand, Large Language Models (LLMs) as recommenders have proven effective across various recommendation scenarios. Yet, LLM-based recommenders encounter challenges such as low inference efficiency and potential hallucination, compromising their performance in real-world scenarios. To this end, we propose GPT-FedRec, a federated recommendation framework leveraging ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism. GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval process,miningID-based user patterns andtext-based item features. Next, the retrieved results are converted intotextprompts and fed into GPT for re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims to extract generalized features from data and exploit pretrained knowledge within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the RAG approach also prevents LLM hallucination, improving the recommendation performance for real-world users. Experimental results on diverse benchmark datasets demonstrate the superior performance of GPT-FedRec against state-of-the-art baseline methods.","['Huimin Zeng', 'Zhenrui Yue', 'Qian Jiang', 'Dong Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.04256,Anomali
RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules,"Weakly supervisedtextclassification (WSTC), also called zero-shot or datalesstextclassification, has attracted increasing attention due to its applicability in classifying a mass oftextswithin the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we develop a prompting PLM-based approach named RulePrompt for the WSTC task, consisting of a ruleminingmodule and a rule-enhanced pseudo label generation module, plus a self-supervised fine-tuning module to make the PLM align with this task. Within this framework, the inaccurate pseudo labels assigned totextsand the imprecise logical rules associated with categories mutually enhance each other in an alternative manner. That establishes a self-iterative closed loop of knowledge (rule) acquisition and utilization, with seed words serving as the starting point. Extensive experiments validate the effectiveness and robustness of our approach, which markedly outperforms state-of-the-art weakly supervised methods. What is more, our approach yields interpretable category rules, proving its advantage in disambiguating easily-confused categories.","['Miaomiao Li', 'Jiaqi Zhu', 'Yang Wang', 'Yi Yang', 'Yilin Li', 'Hongan Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.02932,Anomali
DeepBioisostere: Discovering Bioisosteres with Deep Learning for a Fine Control of Multiple Molecular Properties,"Optimizing molecules to improve their properties is a fundamental challenge in drug design. For a fine-tuning of molecular properties without losing bio-activity validated in advance, the concept of bioisosterism has emerged. Many in silico methods have been proposed for discovering bioisosteres, but they require expert knowledge for their applications or are restricted to known databases. Here, we introduce DeepBioisostere, a deep generative model to design suitable bioisosteric replacements. Our model allows an end-to-end chemical replacement by intelligently selecting fragments for removal and insertion along with their attachment orientation. Through various scenarios of multiple property control, we showcase the model's capability to modulate specific properties, addressing the challenge in molecular optimization. Our model's innovation lies in its capacity to design a bioisosteric replacement reflecting the compatibility with the surroundings of the modification site, facilitating the control of sophisticated properties like drug-likeness. DeepBioisostere can also provide previously unseen bioisosteric replacements, highlighting its capability for exploring diverse chemical modifications rather than justminingthem from known databases. Lastly, we employed DeepBioisostere to improve the sensitivity of a known SARS-CoV-2 main protease inhibitor to the E166V mutant that exhibits drug resistance to the inhibitor, demonstrating its potential application in lead optimization.","['Hyeongwoo Kim', 'Seokhyun Moon', 'Wonho Zhung', 'Jaechang Lim', 'Woo Youn Kim']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.02706,Anomali
"Recommending Missed Citations Identified by Reviewers: A New Task, Dataset and Baselines","Citing comprehensively and appropriately has become a challenging task with the explosive growth of scientific publications. Current citation recommendation systems aim to recommend a list of scientific papers for a giventextcontext or a draft paper. However, none of the existing work focuses on already included citations of full papers, which are imperfect and still have much room for improvement. In the scenario of peer reviewing, it is a common phenomenon that submissions are identified as missing vital citations by reviewers. This may lead to a negative impact on the credibility and validity of the research presented. To help improve citations of full papers, we first define a novel task of Recommending Missed Citations Identified by Reviewers (RMC) and construct a corresponding expert-labeled dataset called CitationR. We conduct an extensive evaluation of several state-of-the-art methods on CitationR. Furthermore, we propose a new framework RMCNet with an Attentive Reference Encoder moduleminingthe relevance between papers, already-made citations, and missed citations. Empirical results prove that RMC is challenging, with the proposed architecture outperforming previous methods in all metrics. We release our dataset and benchmark models to motivate future research on this challenging new task.","['Kehan Long', 'Shasha Li', 'Pancheng Wang', 'Chenlong Bao', 'Jintao Tang', 'Ting Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.01873,Anomali
Measuring Technological Convergence in Encryption Technologies with Proximity Indices: A Text Mining and Bibliometric Analysis using OpenAlex,"Identifying technological convergence among emerging technologies in cybersecurity is crucial for advancing science and fostering innovation. Unlike previous studies focusing on the binary relationship between a paper and the concept it attributes to technology, our approach utilizes attribution scores to enhance the relationships between research papers, combining keywords, citation rates, and collaboration status with specific technological concepts. The proposed method integratestextminingand bibliometric analyses to formulate and predict technological proximity indices for encryption technologies using the ""OpenAlex"" catalog. Our case study findings highlight a significant convergence between blockchain and public-key cryptography, evidenced by the increasing proximity indices. These results offer valuable strategic insights for those contemplating investments in these domains.","['Alessandro Tavazzi', 'Dimitri Percia David', 'Julian Jang-Jaccard', 'Alain Mermoud']",,arXiv,2024,https://doi.org/10.48550/arXiv.2403.01601,Anomali
Text mining in education,"The explosive growth of online education environments is generating a massive volume of data, specially intextformat from forums, chats, social networks, assessments, essays, among others. It produces exciting challenges on how tominetextdata in order to find useful knowledge for educational stakeholders. Despite the increasing number of educational applications oftextminingpublished recently, we have not found any paper surveying them. In this line, this work presents a systematic overview of the current status of the EducationalTextMiningfield. Our final goal is to answer three main research questions: Which are thetextminingtechniques most used in educational environments? Which are the most used educational resources? And which are the main applications or educational goals? Finally, we outline the conclusions and the more interesting future trends.","['R. Ferreira-Mello', 'M. Andre', 'A. Pinheiro', 'E. Costa', 'C. Romero']",Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery (2019); 9(6):e1332,arXiv,2024,https://doi.org/10.48550/arXiv.2403.00769,Anomali
TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision,"Hierarchicaltextclassification aims to categorize each document into a set of classes in a label taxonomy, which is a fundamental webtextminingtask with broad applications such as web content analysis and semantic indexing. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchicaltextclassification with a minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) have shown competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchicaltextclassification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in thetextcorpus that can serve as additional class-indicative features. To tackle the above challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced weakly-supervised hierarchicaltextClassification, which combines the general knowledge of LLMs and task-specific featuresminedfrom an unlabeled corpus. TELEClass automatically enriches the raw taxonomy with class-indicative features for better label space understanding and utilizes novel LLM-based data annotation and generation methods specifically tailored for the hierarchical setting. Experiments show that TELEClass can significantly outperform previous baselines while achieving comparable performance to zero-shot prompting of LLMs with drastically less inference cost.","['Yunyi Zhang', 'Ruozhen Yang', 'Xueqiang Xu', 'Rui Li', 'Jinfeng Xiao', 'Jiaming Shen', 'Jiawei Han']",,arXiv,2025,https://doi.org/10.48550/arXiv.2403.00165,Anomali
Mobile Health Text Misinformation Identification Using Mobile Data Mining,"More than six million people died of the COVID-19 by April 2022. The heavy casualties have put people on great and urgent alert and people try to find all kinds of information to keep them from being inflected by the coronavirus. This research tries to find out whether the mobile healthtextinformation sent to peoples devices is correct as smartphones becoming the major information source for people. The proposed method uses various mobile information retrieval and dataminingtechnologies including lexical analysis, stopword elimination, stemming, and decision trees to classify the mobile healthtextinformation to one of the following classes: (i) true, (ii) fake, (iii) misinformative, (iv) disinformative, and (v) neutral. Experiment results show the accuracy of the proposed method is above the threshold value 50 percentage, but is not optimal. It is because the problem, mobiletextmisinformation identification, is intrinsically difficult.","['Wen-Chen Hu', 'Sanjaikanth E Vadakkethil Somanathan Pillai', 'Abdelrahman Ahmed ElSaid']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.19280,Anomali
DeepEraser: Deep Iterative Context Mining for Generic Text Eraser,"In this work, we present DeepEraser, an effective deep network for generictextremoval. DeepEraser utilizes a recurrent architecture that erases thetextin an image via iterative operations. Our idea comes from the process of erasing pencil script, where thetextarea designated for removal is subject to continuous monitoring and thetextis attenuated progressively, ensuring a thorough and clean erasure. Technically, at each iteration, an innovative erasing module is deployed, which not only explicitly aggregates the previous erasing progress but alsominesadditional semantic context to erase the targettext. Through iterative refinements, thetextregions are progressively replaced with more appropriate content and finally converge to a relatively accurate status. Furthermore, a custom mask generation strategy is introduced to improve the capability of DeepEraser for adaptivetextremoval, as opposed to indiscriminately removing all thetextin an image. Our DeepEraser is notably compact with only 1.4M parameters and trained in an end-to-end manner. To verify its effectiveness, extensive experiments are conducted on several prevalent benchmarks, including SCUT-Syn, SCUT-EnsText, and Oxford Synthetictextdataset. The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generalization ability in custom masktextremoval. The codes and pre-trained models are available at https://github.com/fh2019ustc/DeepEraser","['Hao Feng', 'Wendi Wang', 'Shaokai Liu', 'Jiajun Deng', 'Wengang Zhou', 'Houqiang Li']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.19108,Anomali
GISTEmbed: Guided In-sample Selection of Training Negatives for Text Embedding Fine-tuning,"Embedding models are integral to AI applications like semantic search, personalized recommendations, and retrieval augmented generation for LLMs, necessitating high-quality training data. However, the limited scalability of manual data curation prompts the need for automated methods to ensure data integrity. Traditional unsupervised tripletminingautomates training data generation, crucial for embedding model training, yet inadvertently injects biases and noise, thereby degrading model performance. Addressing this, we introduce GISTEmbed, a novel strategy that enhances in-batch negative selection during contrastive training through a guide model. This approach departs from reliance on random sampling and equal utility assumption of batch negatives, significantly reducing noise from data quality issues and improving model fine-tuning. Benchmarked against the MassiveTextEmbedding Benchmark (MTEB), GISTEmbed showcases consistent performance improvements across various model sizes and achieves state-of-the-art results in select categories. This framework enables significant enhancements for smaller models by leveraging the capabilities of powerful yet resource-intensive large models. GISTEmbed can potentially revolutionize the creation of highly efficient, smaller models, democratizing access to advanced AI technologies. Making these technologies more accessible and cost-effective, especially for applications constrained by resources, significantly expands the impact and accessibility of state-of-the-art AI solutions across diverse sectors.",['Aivin V. Solatorio'],,arXiv,2024,https://doi.org/10.48550/arXiv.2402.16829,Anomali
Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models,"Temporal knowledge graph question answering (TKGQA) poses a significant challenge task, due to the temporal constraints hidden in questions and the answers sought from dynamic structured knowledge. Although large language models (LLMs) have made considerable progress in their reasoning ability over structured data, their application to the TKGQA task is a relatively unexplored area. This paper first proposes a novel generative temporal knowledge graph question answering framework, GenTKGQA, which guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation. First, we exploit LLM's intrinsic knowledge tominetemporal constraints and structural links in the questions without extra training, thus narrowing down the subgraph search space in both temporal and structural dimensions. Next, we design virtual knowledge indicators to fuse the graph neural network signals of the subgraph and thetextrepresentations of the LLM in a non-shallow way, which helps the open-source LLM deeply understand the temporal order and structural dependencies among the retrieved facts through instruction tuning. Experimental results on two widely used datasets demonstrate the superiority of our model.","['Yifu Gao', 'Linbo Qiao', 'Zhigang Kan', 'Zhihua Wen', 'Yongquan He', 'Dongsheng Li']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.16568,Anomali
Technical Report on the Pangram AI-Generated Text Classifier,"We present PangramText, a transformer-based neural network trained to distinguishtextwritten by large language models fromtextwritten by humans. PangramTextoutperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 38 times lower error rates on a comprehensive benchmark comprised of 10textdomains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q&A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negativeminingwith synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that PangramTextis not biased against nonnative English speakers and generalizes to domains and models unseen during training.","['Bradley Emi', 'Max Spero']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.14873,Anomali
Effects of term weighting approach with and without stop words removing on Arabic text classification,"Classifyingtextis a method for categorizing documents into pre-established groups.Textdocuments must be prepared and represented in a way that is appropriate for the algorithms used for dataminingprior to classification. As a result, a number of term weighting strategies have been created in the literature to enhancetextcategorization algorithms' functionality. This study compares the effects of Binary and Term frequency weighting feature methodologies on thetext'sclassification method when stop words are eliminated once and when they are not. In recognition of assessing the effects of prior weighting of features approaches on classification results in terms of accuracy, recall, precision, and F-measure values, we used an Arabic data set made up of 322 documents divided into six main topics (agriculture, economy, health, politics, science, and sport), each of which contains 50 documents, with the exception of the health category, which contains 61 documents. The results demonstrate that for all metrics, the term frequency feature weighting approach with stop word removal outperforms the binary approach, while for accuracy, recall, and F-Measure, the binary approach outperforms the TF approach without stop word removal. However, for precision, the two approaches produce results that are very similar. Additionally, it is clear from the data that, using the same phrase weighting approach, stop word removing increases classification accuracy.","[""Esra'a Alhenawi"", 'Ruba Abu Khurma', 'Pedro A. Castillo', 'Maribel G. Arenas']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.14867,Anomali
Is ChatGPT the Future of Causal Text Mining? A Comprehensive Evaluation and Analysis,"Causality is fundamental in human cognition and has drawn attention in diverse research fields. With growing volumes of textual data, discerning causalities withintextdata is crucial, and causaltextminingplays a pivotal role in extracting meaningful patterns. This study conducts comprehensive evaluations of ChatGPT's causaltextminingcapabilities. Firstly, we introduce a benchmark that extends beyond general English datasets, including domain-specific and non-English datasets. We also provide an evaluation framework to ensure fair comparisons between ChatGPT and previous approaches. Finally, our analysis outlines the limitations and future challenges in employing ChatGPT for causaltextmining. Specifically, our analysis reveals that ChatGPT serves as a good starting point for various datasets. However, when equipped with a sufficient amount of training data, previous models still surpass ChatGPT's performance. Additionally, ChatGPT suffers from the tendency to falsely recognize non-causal sequences as causal sequences. These issues become even more pronounced with advanced versions of the model, such as GPT-4. In addition, we highlight the constraints of ChatGPT in handling complex causality types, including both intra/inter-sentential and implicit causality. The model also faces challenges with effectively leveraging in-context learning and domain adaptation. We release our code to support further research and development in this field.","['Takehiro Takayanagi', 'Masahiro Suzuki', 'Ryotaro Kobayashi', 'Hiroki Sakaji', 'Kiyoshi Izumi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.14484,Anomali
STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning,"As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts tomineand annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set oftextexemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\%-18\%$ and rare-class F-1 score by $17\%-40\%$ on multipletextclassification datasets over common active learning methods within the class-imbalanced cold-start setting.","['Nathan Beck', 'Adithya Iyer', 'Rishabh Iyer']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.13468,Anomali
CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning,"Advances in graph machine learning (ML) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules. While early graph ML methods focused primarily on small organic molecules, recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to address. Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. However, the most exciting applications of graph ML will be in their generative capabilities, which is currently not at par with other domains such as images ortext.
  We invite the graph ML community to address these open challenges by presenting two new chemically-informed large-scale inorganic (CHILI) nanomaterials datasets: A medium-scale dataset (with overall >6M nodes, >49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (CHILI-3K) and a large-scale dataset (with overall >183M nodes, >1.2B edges) of nanomaterials generated from experimentally determined crystal structures (CHILI-100K). We define 11 property prediction tasks and 6 structure prediction tasks, which are of special interest for nanomaterial research. We benchmark the performance of a wide array of baseline methods and use these benchmarking results to highlight areas which need future work. To the best of our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial datasets of this scale -- both on the individual graph level and of the dataset as a whole -- and the only nanomaterials datasets with high structural and elemental diversity.","['Ulrik Friis-Jensen', 'Frederik L. Johansen', 'Andy S. Anker', 'Erik B. Dam', 'Kirsten M. Ø. Jensen', 'Raghavendra Selvan']",KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2024,arXiv,2024,https://doi.org/10.48550/arXiv.2402.13221,Anomali
HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools,"With the exponential growth of the life science literature, biomedicaltextmining(BTM) has become an essential technology for accelerating the extraction of insights from publications. Identifying named entities (e.g., diseases, drugs, or genes) intextsand their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents. However, tools for these two steps are rarely applied in the same context in which they were developed. Instead, they are applied in the wild, i.e., on application-dependenttextcollections different from those used for the tools' training, varying, e.g., in focus, genre, style, andtexttype. This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications. Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied systematically to corpora not used during their training. Based on a survey of 28 published systems, we selected five for an in-depth analysis on three publicly available corpora encompassing four different entity types. Comparison between tools results in a mixed picture and shows that, in a cross-corpus setting, the performance is significantly lower than the one reported in an in-corpus setting. HunFlair2 showed the best performance on average, being closely followed by PubTator. Our results indicate that users of BTM tools should expect diminishing performances when applying them in the wild compared to original publications and show that further research is necessary to make BTM tools more robust.","['Mario Sänger', 'Samuele Garda', 'Xing David Wang', 'Leon Weber-Genzel', 'Pia Droop', 'Benedikt Fuchs', 'Alan Akbik', 'Ulf Leser']","Bioinformatics, Volume 40, Number 10, 2024, btae564, Oxford University Press",arXiv,2024,https://doi.org/10.48550/arXiv.2402.12372,Anomali
Can Large Language Models perform Relation-based Argument Mining?,"Argumentmining(AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components fromtext. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) with ten datasets.","['Deniz Gorur', 'Antonio Rago', 'Francesca Toni']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.11243,Anomali
Construction of a Syntactic Analysis Map for Yi Shui School through Text Mining and Natural Language Processing Research,"Entity and relationship extraction is a crucial component in natural language processing tasks such as knowledge graph construction, question answering system design, and semantic analysis. Most of the information of the Yishui school of traditional Chinese Medicine (TCM) is stored in the form of unstructured classical Chinesetext. The key information extraction of TCMtextsplays an important role inminingand studying the academic schools of TCM. In order to solve these problems efficiently using artificial intelligence methods, this study constructs a word segmentation and entity relationship extraction model based on conditional random fields under the framework of natural language processing technology to identify and extract the entity relationship of traditional Chinese medicinetexts, and uses the common weighting technology of TF-IDF information retrieval and dataminingto extract important key entity information in different ancient books. The dependency syntactic parser based on neural network is used to analyze the grammatical relationship between entities in each ancient book article, and it is represented as a tree structure visualization, which lays the foundation for the next construction of the knowledge graph of Yishui school and the use of artificial intelligence methods to carry out the research of TCM academic schools.","['Hanqing Zhao', 'Yuehan Li']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.10743,Anomali
Multi-Fidelity Methods for Optimization: A Survey,"Real-world black-box optimization often involves time-consuming or costly experiments and simulations. Multi-fidelity optimization (MFO) stands out as a cost-effective strategy that balances high-fidelity accuracy with computational efficiency through a hierarchical fidelity approach. This survey presents a systematic exploration of MFO, underpinned by a noveltextminingframework based on a pre-trained language model. We delve deep into the foundational principles and methodologies of MFO, focusing on three core components -- multi-fidelity surrogate models, fidelity management strategies, and optimization techniques. Additionally, this survey highlights the diverse applications of MFO across several key domains, including machine learning, engineering design optimization, and scientific discovery, showcasing the adaptability and effectiveness of MFO in tackling complex computational challenges. Furthermore, we also envision several emerging challenges and prospects in the MFO landscape, spanning scalability, the composition of lower fidelities, and the integration of human-in-the-loop approaches at the algorithmic level. We also address critical issues related to benchmarking and the advancement of open science within the MFO community. Overall, this survey aims to catalyze further research and foster collaborations in MFO, setting the stage for future innovations and breakthroughs in the field.","['Ke Li', 'Fan Li']",COLALab Report #2024005,arXiv,2024,https://doi.org/10.48550/arXiv.2402.09638,Anomali
Prompt4Vis: Prompting Large Language Models with Example Mining and Schema Filtering for Tabular Data Visualization,"Data visualization (DV) systems are increasingly recognized for their profound capability to uncover insights from vast datasets, gaining attention across both industry and academia. Crafting data queries is an essential process within certain declarative visualization languages (DVLs, e.g., Vega-Lite, EChart.). The evolution of natural language processing (NLP) technologies has streamlined the use of natural language interfaces to visualize tabular data, offering a more accessible and intuitive user experience. However, current methods for converting natural language questions into data visualization queries, such as Seq2Vis, ncNet, and RGVisNet, despite utilizing complex neural network architectures, still fall short of expectations and have great room for improvement.
  Large language models (LLMs) such as ChatGPT and GPT-4, have established new benchmarks in a variety of NLP tasks, fundamentally altering the landscape of the field. Inspired by these advancements, we introduce a novel framework, Prompt4Vis, leveraging LLMs and in-context learning to enhance the performance of generating data visualization from natural language. Prompt4Vis comprises two key components: (1) a multi-objective exampleminingmodule, designed to find out the truly effective examples that strengthen the LLM's in-context learning capabilities fortext-to-vis; (2) a schema filtering module, which is proposed to simplify the schema of the database. Extensive experiments through 5-fold cross-validation on the NVBench dataset demonstrate the superiority of Prompt4Vis, which notably surpasses the state-of-the-art (SOTA) RGVisNet by approximately 35.9% and 71.3% on dev and test sets, respectively. To the best of our knowledge, Prompt4Vis is the first work that introduces in-context learning into thetext-to-vis for generating data visualization queries.","['Shuaimin Li', 'Xuanang Chen', 'Yuanfeng Song', 'Yunze Song', 'Chen Zhang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.07909,Anomali
A step towards the integration of machine learning and small area estimation,"The use of machine-learning techniques has grown in numerous research areas. Currently, it is also widely used in statistics, including the official statistics for data collection (e.g. satellite imagery, web scraping andtextmining, data cleaning, integration and imputation) but also for data analysis. However, the usage of these methods in survey sampling including small area estimation is still very limited. Therefore, we propose a predictor supported by these algorithms which can be used to predict any population or subpopulation characteristics based on cross-sectional and longitudinal data. Machine learning methods have already been shown to be very powerful in identifying and modelling complex and nonlinear relationships between the variables, which means that they have very good properties in case of strong departures from the classic assumptions. Therefore, we analyse the performance of our proposal under a different set-up, in our opinion of greater importance in real-life surveys. We study only small departures from the assumed model, to show that our proposal is a good alternative in this case as well, even in comparison with optimal methods under the model. What is more, we propose the method of the accuracy estimation of machine learning predictors, giving the possibility of the accuracy comparison with classic methods, where the accuracy is measured as in survey sampling practice. The solution of this problem is indicated in the literature as one of the key issues in integration of these approaches. The simulation studies are based on a real, longitudinal dataset, freely available from the Polish Local Data Bank, where the prediction problem of subpopulation characteristics in the last period, with ""borrowing strength"" from other subpopulations and time periods, is considered.","['Tomasz Żądło', 'Adam Chwila']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.07521,Anomali
Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models,"The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness oftextgeneration from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.","['Zhibo Hu', 'Chen Wang', 'Yanfeng Shu', 'Helen', 'Paik', 'Liming Zhu']","KDD '24: The 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining Barcelona Spain August 25 - 29, 2024",arXiv,2024,https://doi.org/10.48550/arXiv.2402.07179,Anomali
How to Refactor this Code? An Exploratory Study on Developer-ChatGPT Refactoring Conversations,"Large Language Models (LLMs), like ChatGPT, have gained widespread popularity and usage in various software engineering tasks, including refactoring, testing, code review, and program comprehension. Despite recent studies delving into refactoring documentation in commit messages, issues, and code review, little is known about how developers articulate their refactoring needs when interacting with ChatGPT. In this paper, our goal is to explore conversations between developers and ChatGPT related to refactoring to better understand how developers identify areas for improvement in code and how ChatGPT addresses developers' needs. Our approach relies ontextminingrefactoring-related conversations from 17,913 ChatGPT prompts and responses, and investigating developers' explicit refactoring intention. Our results reveal that (1) developer-ChatGPT conversations commonly involve generic and specific terms/phrases; (2) developers often make generic refactoring requests, while ChatGPT typically includes the refactoring intention; and (3) various learning settings when prompting ChatGPT in the context of refactoring. We envision that our findings contribute to a broader understanding of the collaboration between developers and AI models, in the context of code refactoring, with implications for model improvement, tool development, and best practices in software engineering.","['Eman Abdullah AlOmar', 'Anushkrishna Venkatakrishnan', 'Mohamed Wiem Mkaouer', 'Christian D. Newman', 'Ali Ouni']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.06013,Anomali
Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature,"Background: The emergence of generative AI tools, empowered by Large Language Models (LLMs), has shown powerful capabilities in generating content. To date, the assessment of the usefulness of such content, generated by what is known as prompt engineering, has become an interesting research question. Objectives Using the mean of prompt engineering, we assess the similarity and closeness of such contents to real literature produced by scientists. Methods In this exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to generate clinical content to be compared with literature counterparts, (2) we assess the similarities of the contents generated by comparing them with counterparts from biomedical literature. Our approach is to usetext-miningapproaches to compare documents and associated bigrams and to use network analysis to assess the terms' centrality. Results The experiments demonstrated that ChatGPT outperformed Google Bard in cosine document similarity (38% to 34%), Jaccard document similarity (23% to 19%), TF-IDF bigram similarity (47% to 41%), and term network centrality (degree and closeness). We also found new links that emerged in ChatGPT bigram networks that did not exist in literature bigram networks. Conclusions: The obtained similarity results show that ChatGPT outperformed Google Bard in document similarity, bigrams, and degree and closeness centrality. We also observed that ChatGPT offers linkage to terms that are connected in the literature. Such connections could inspire asking interesting questions and generate new hypotheses.","['Jakub Klimczak', 'Ahmed Abdeen Hamed']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.05116,Anomali
LLMs Meet VLMs: Boost Open Vocabulary Object Detection with Fine-grained Descriptors,"Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grainedtextdescription of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-textalignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterativeminingand refining visually oriented textual descriptors for precise region-textalignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.","['Sheng Jin', 'Xueying Jiang', 'Jiaxing Huang', 'Lewei Lu', 'Shijian Lu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.04630,Anomali
PatSTEG: Modeling Formation Dynamics of Patent Citation Networks via The Semantic-Topological Evolutionary Graph,"Patent documents in the patent database (PatDB) are crucial for research, development, and innovation as they contain valuable technical information. However, PatDB presents a multifaceted challenge compared to publicly available preprocessed databases due to the intricate nature of the patenttextand the inherent sparsity within the patent citation network. Although patenttextanalysis and citation analysis bring new opportunities to explore patent datamining, no existing work exploits the complementation of them. To this end, we propose a joint semantic-topological evolutionary graph learning approach (PatSTEG) to model the formation dynamics of patent citation networks. More specifically, we first create a real-world dataset of Chinese patents named CNPat and leverage its patenttextsand citations to construct a patent citation network. Then, PatSTEG is modeled to study the evolutionary dynamics of patent citation formation by considering the semantic and topological information jointly. Extensive experiments are conducted on CNPat and public datasets to prove the superiority of PatSTEG over other state-of-the-art methods. All the results provide valuable references for patent literature research and technical exploration.","['Ran Miao', 'Xueyu Chen', 'Liang Hu', 'Zhifei Zhang', 'Minghua Wan', 'Qi Zhang', 'Cairong Zhao']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.02158,Anomali
A Case Study on Filtering for End-to-End Speech Translation,"It is relatively easy tominea large parallel corpus for any machine learning task, such as speech-to-textor speech-to-speech translation. Although theseminedcorpora are large in volume, their quality is questionable. This work shows that the simplest filtering technique can trim down these big, noisy datasets to a more manageable, clean dataset. We also show that using this clean dataset can improve the model's performance, as in the case of the multilingual-to-English Speech Translation (ST) model, where, on average, we obtain a 4.65 BLEU score improvement.","['Md Mahfuz Ibn Alam', 'Antonios Anastasopoulos']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.01945,Anomali
"Leveraging Social Media Data to Identify Factors Influencing Public Attitude Towards Accessibility, Socioeconomic Disparity and Public Transportation","This study proposes a novel method to understand the factors affecting individuals' perception of transport accessibility, socioeconomic disparity, and public infrastructure. As opposed to the time consuming and expensive survey-based approach, this method can generate organic large-scale responses from social media and develop statistical models to understand individuals' perceptions of various transportation issues. This study retrieved and analyzed 36,098 tweets from New York City from March 19, 2020, to May 15, 2022. A state-of-the-art natural language processing algorithm is used fortextminingand classification. A data fusion technique has been adopted to generate a series of socioeconomic traits that are used as explanatory variables in the model. The model results show that females and individuals of Asian origin tend to discuss transportation accessibility more than their counterparts, with those experiencing high neighborhood traffic also being more vocal. However, disadvantaged individuals, including the unemployed and those living in low-income neighborhoods or in areas with high natural hazard risks, tend to communicate less about such issues. As for socioeconomic disparity, individuals of Asian origin and those experiencing various types of air pollution are more likely to discuss these topics on Twitter, often with a negative sentiment. However, unemployed, or disadvantaged individuals, as well as those living in areas with high natural hazard risks or expected losses, are less inclined to tweet about this subject. Lack of internet accessibility could be a reason why many disadvantaged individuals do not tweet about transport accessibility and subsidized internet could be a possible solution.","['Khondhaker Al Momin', 'Arif Mohaimin Sadri', 'Md Sami Hasnine']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.01682,Anomali
A Scalable and Automated Framework for Tracking the likely Adoption of Emerging Technologies,"While new technologies are expected to revolutionise and become game-changers in improving the efficiencies and practises of our daily lives, it is also critical to investigate and understand the barriers and opportunities faced by their adopters. Such findings can serve as an additional feature in the decision-making process when analysing the risks, costs, and benefits of adopting an emerging technology in a particular setting. Although several studies have attempted to perform such investigations, these approaches adopt a qualitative data collection methodology which is limited in terms of the size of the targeted participant group and is associated with a significant manual overhead when transcribing and inferring results. This paper presents a scalable and automated framework for tracking likely adoption and/or rejection of new technologies from a large landscape of adopters. In particular, a large corpus of social mediatextscontaining references to emerging technologies was compiled.Textminingtechniques were applied to extract sentiments expressed towards technology aspects. In the context of the problem definition herein, we hypothesise that the expression of positive sentiment infers an increase in the likelihood of impacting a technology user's acceptance to adopt, integrate, and/or use the technology, and negative sentiment infers an increase in the likelihood of impacting the rejection of emerging technologies by adopters. To quantitatively test our hypothesis, a ground truth analysis was performed to validate that the sentiment captured by thetextminingapproach is comparable to the results given by human annotators when asked to label whether suchtextspositively or negatively impact their outlook towards adopting an emerging technology.","['Lowri Williams', 'Eirini Anthi', 'Pete Burnap']",,arXiv,2024,https://doi.org/10.48550/arXiv.2402.01670,Anomali
Deep Active Learning for Data Mining from Conflict Text Corpora,"High-resolution event data on armed conflict and related processes have revolutionized the study of political contention with datasets like UCDP GED, ACLED etc. However, most of these datasets limit themselves to collecting spatio-temporal (high-resolution) and intensity data. Information on dynamics, such as targets, tactics, purposes etc. are rarely collected owing to the extreme workload of collecting data. However, most datasets rely on a rich corpus of textual data allowing furtherminingof further information connected to each event. This paper proposes one such approach that is inexpensive and high performance, leveraging active learning - an iterative process of improving a machine learning model based on sequential (guided) human input. Active learning is employed to then step-wise train (fine-tuning) of a large, encoder-only language model adapted for extracting sub-classes of events relating to conflict dynamics. The approach shows performance similar to human (gold-standard) coding while reducing the amount of required human annotation by as much as 99%.",['Mihai Croicu'],,arXiv,2024,https://doi.org/10.48550/arXiv.2402.01577,Anomali
Building Contextual Knowledge Graphs for Personalized Learning Recommendations using Text Mining and Semantic Graph Completion,"Modelling learning objects (LO) within their context enables the learner to advance from a basic, remembering-level, learning objective to a higher-order one, i.e., a level with an application- and analysis objective. While hierarchical data models are commonly used in digital learning platforms, using graph-based models enables representing the context of LOs in those platforms. This leads to a foundation for personalized recommendations of learning paths. In this paper, the transformation of hierarchical data models into knowledge graph (KG) models of LOs usingtextminingis introduced and evaluated. We utilize customtextminingpipelines tominesemantic relations between elements of an expert-curated hierarchical model. We evaluate the KG structure and relation extraction using graph quality-control metrics and the comparison of algorithmic semantic-similarities to expert-defined ones. The results show that the relations in the KG are semantically comparable to those defined by domain experts, and that the proposed KG improves representing and linking the contexts of LOs through increasing graph communities and betweenness centrality.","['Hasan Abu-Rasheed', 'Mareike Dornhöfer', 'Christian Weber', 'Gábor Kismihók', 'Ulrike Buchmann', 'Madjid Fathi']",2023 IEEE International Conference on Advanced Learning Technologies (ICALT),arXiv,2024,https://doi.org/10.48550/arXiv.2401.13609,Anomali
Topic Modelling: Going Beyond Token Outputs,"Topic modelling is atextminingtechnique for identifying salient themes from a number of documents. The output is commonly a set of topics consisting of isolated tokens that often co-occur in such documents. Manual effort is often associated with interpreting a topic's description from such tokens. However, from a human's perspective, such outputs may not adequately provide enough information to infer the meaning of the topics; thus, their interpretability is often inaccurately understood. Although several studies have attempted to automatically extend topic descriptions as a means of enhancing the interpretation of topic models, they rely on external language sources that may become unavailable, must be kept up-to-date to generate relevant results, and present privacy issues when training on or processing data. This paper presents a novel approach towards extending the output of traditional topic modelling methods beyond a list of isolated tokens. This approach removes the dependence on external sources by using the textual data itself by extracting high-scoring keywords and mapping them to the topic model's token outputs. To measure the interpretability of the proposed outputs against those of the traditional topic modelling approach, independent annotators manually scored each output based on their quality and usefulness, as well as the efficiency of the annotation task. The proposed approach demonstrated higher quality and usefulness, as well as higher efficiency in the annotation task, in comparison to the outputs of a traditional topic modelling method, demonstrating an increase in their interpretability.","['Lowri Williams', 'Eirini Anthi', 'Laura Arman', 'Pete Burnap']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.12990,Anomali
A Comprehensive Survey of Text Classification Techniques and Their Research Applications: Observational and Experimental Insights,"The exponential growth of textual data presents substantial challenges in management and analysis, notably due to high storage and processing costs.Textclassification, a vital aspect oftextmining, provides robust solutions by enabling efficient categorization and organization oftextdata. These techniques allow individuals, researchers, and businesses to derive meaningful patterns and insights from large volumes oftext. This survey paper introduces a comprehensive taxonomy specifically designed fortextclassification based on research fields. The taxonomy is structured into hierarchical levels: research field-based category, research field-based sub-category, methodology-based technique, methodology sub-technique, and research field applications. We employ a dual evaluation approach: empirical and experimental. Empirically, we assesstextclassification techniques across four critical criteria. Experimentally, we compare and rank the methodology sub-techniques within the same methodology technique and within the same overall research field sub-category. This structured taxonomy, coupled with thorough evaluations, provides a detailed and nuanced understanding oftextclassification algorithms and their applications, empowering researchers to make informed decisions based on precise, field-specific insights.","['Kamal Taha', 'Paul D. Yoo', 'Chan Yeun', 'Aya Taha']",100664,arXiv,2024,https://doi.org/10.48550/arXiv.2401.12982,Anomali
Examining the Role of Peer Acknowledgements on Social Annotations: Unraveling the Psychological Underpinnings,"This study explores the impact of peer acknowledgement on learner engagement and implicit psychological attributes in written annotations on an online social reading platform. Participants included 91 undergraduates from a large North American University. Using log file data, we analyzed the relationship between learners' received peer acknowledgement and their subsequent annotation behaviours using cross-lag regression. Higher peer acknowledgements correlate with increased initiation of annotations and responses to peer annotations. By applyingtextminingtechniques and calculating Shapley values to analyze 1,969 social annotation entries, we identified prominent psychological themes within three dimensions (i.e., affect, cognition, and motivation) that foster peer acknowledgment in digital social annotation. These themes include positive affect, openness to learning and discussion, and expression of motivation. The findings assist educators in improving online learning communities and provide guidance to technology developers in designing effective prompts, drawing from both implicit psychological cues and explicit learning behaviours.","['Xiaoshan Huang', 'Haolun Wu', 'Xue Liu', 'Susanne Lajoie']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.12956,Anomali
ActionHub: A Large-scale Action Video Description Dataset for Zero-shot Action Recognition,"Zero-shot action recognition (ZSAR) aims to learn an alignment model between videos and class descriptions of seen actions that is transferable to unseen actions. Thetextqueries (class descriptions) used in existing ZSAR works, however, are often short action names that fail to capture the rich semantics in the videos, leading to misalignment. With the intuition that video content descriptions (e.g., video captions) can provide rich contextual information of visual concepts in videos, we propose to utilize human annotated video descriptions to enrich the semantics of the class descriptions of each action. However, all existing action video description datasets are limited in terms of the number of actions, the semantics of video descriptions, etc. To this end, we collect a large-scale action video descriptions dataset named ActionHub, which covers a total of 1,211 common actions and provides 3.6 million action video descriptions. With the proposed ActionHub dataset, we further propose a novel Cross-modality and Cross-action Modeling (CoCo) framework for ZSAR, which consists of a Dual Cross-modality Alignment module and a Cross-action InvarianceMiningmodule. Specifically, the Dual Cross-modality Alignment module utilizes both action labels and video descriptions from ActionHub to obtain rich class semantic features for feature alignment. The Cross-action InvarianceMiningmodule exploits a cycle-reconstruction process between the class semantic feature spaces of seen actions and unseen actions, aiming to guide the model to learn cross-action invariant representations. Extensive experimental results demonstrate that our CoCo framework significantly outperforms the state-of-the-art on three popular ZSAR benchmarks (i.e., Kinetics-ZSAR, UCF101 and HMDB51) under two different learning protocols in ZSAR. We will release our code, models, and the proposed ActionHub dataset.","['Jiaming Zhou', 'Junwei Liang', 'Kun-Yu Lin', 'Jinrui Yang', 'Wei-Shi Zheng']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.11654,Anomali
End-to-End Argument Mining over Varying Rhetorical Structures,"Rhetorical Structure Theory implies no single discourse interpretation of atext, and the limitations of RST parsers further exacerbate inconsistent parsing of similar structures. Therefore, it is important to take into account that the same argumentative structure can be found in semantically similartextswith varying rhetorical structures. In this work, the differences between paraphrases within the same argument scheme are evaluated from a rhetorical perspective. The study proposes a deep dependency parsing model to assess the connection between rhetorical and argument structures. The model utilizes rhetorical relations; RST structures of paraphrases serve as training data augmentations. The method allows for end-to-end argumentation analysis using a rhetorical tree instead of a word sequence. It is evaluated on the bilingual Microtexts corpus, and the first results on fully-fledged argument parsing for the Russian version of the corpus are reported. The results suggest that argumentminingcan benefit from multiple variants of discourse structure.",['Elena Chistova'],"Findings of the Association for Computational Linguistics: ACL 2023, 3376-3391",arXiv,2024,https://doi.org/10.48550/arXiv.2401.11218,Anomali
BioFinBERT: Finetuning Large Language Models (LLMs) to Analyze Sentiment of Press Releases and Financial Text Around Inflection Points of Biotech Stocks,"Large language models (LLMs) are deep learning algorithms being used to perform natural language processing tasks in various fields, from social sciences to finance and biomedical sciences. Developing and training a new LLM can be very computationally expensive, so it is becoming a common practice to take existing LLMs and finetune them with carefully curated datasets for desired applications in different fields. Here, we present BioFinBERT, a finetuned LLM to perform financial sentiment analysis of publictextassociated with stocks of companies in the biotechnology sector. The stocks of biotech companies developing highly innovative and risky therapeutic drugs tend to respond very positively or negatively upon a successful or failed clinical readout or regulatory approval of their drug, respectively. These clinical or regulatory results are disclosed by the biotech companies via press releases, which are followed by a significant stock response in many cases. In our attempt to design a LLM capable of analyzing the sentiment of these press releases,we first finetuned BioBERT, a biomedical language representation model designed for biomedicaltextmining, using financial textual databases. Our finetuned model, termed BioFinBERT, was then used to perform financial sentiment analysis of various biotech-related press releases and financialtextaround inflection points that significantly affected the price of biotech stocks.","['Valentina Aparicio', 'Daniel Gordon', 'Sebastian G. Huayamares', 'Yuhuai Luo']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.11011,Anomali
CMFN: Cross-Modal Fusion Network for Irregular Scene Text Recognition,"Scenetextrecognition, as a cross-modal task involving vision andtext, is an important research topic in computer vision. Most existing methods use language models to extract semantic information for optimizing visual recognition. However, the guidance of visual cues is ignored in the process of semanticmining, which limits the performance of the algorithm in recognizing irregular scenetext. To tackle this issue, we propose a novel cross-modal fusion network (CMFN) for irregular scenetextrecognition, which incorporates visual cues into the semanticminingprocess. Specifically, CMFN consists of a position self-enhanced encoder, a visual recognition branch and an iterative semantic recognition branch. The position self-enhanced encoder provides character sequence position encoding for both the visual recognition branch and the iterative semantic recognition branch. The visual recognition branch carries out visual recognition based on the visual features extracted by CNN and the position encoding information provided by the position self-enhanced encoder. The iterative semantic recognition branch, which consists of a language recognition module and a cross-modal fusion gate, simulates the way that human recognizes scenetextand integrates cross-modal visual cues fortextrecognition. The experiments demonstrate that the proposed CMFN algorithm achieves comparable performance to state-of-the-art algorithms, indicating its effectiveness.","['Jinzhi Zheng', 'Ruyi Ji', 'Libo Zhang', 'Yanjun Wu', 'Chen Zhao']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.10041,Anomali
CPCL: Cross-Modal Prototypical Contrastive Learning for Weakly Supervised Text-based Person Re-Identification,"Weakly supervisedtext-based person re-identification (TPRe-ID) seeks to retrieve images of a target person using textual descriptions, without relying on identity annotations and is more challenging and practical. The primary challenge is the intra-class differences, encompassing intra-modal feature variations and cross-modal semantic gaps. Prior works have focused on instance-level samples and ignored prototypical features of each person which are intrinsic and invariant. Toward this, we propose a Cross-Modal Prototypical Contrastive Learning (CPCL) method. In practice, the CPCL introduces the CLIP model to weakly supervised TPRe-ID for the first time, mapping visual and textual instances into a shared latent space. Subsequently, the proposed Prototypical Multi-modal Memory (PMM) module captures associations between heterogeneous modalities of image-textpairs belonging to the same person through the Hybrid Cross-modal Matching (HCM) module in a many-to-many mapping fashion. Moreover, the Outlier Pseudo LabelMining(OPLM) module further distinguishes valuable outlier samples from each modality, enhancing the creation of more reliable clusters byminingimplicit relationships between image-textpairs. Experimental results demonstrate that our proposed CPCL attains state-of-the-art performance on all three public datasets, with a significant improvement of 11.58%, 8.77% and 5.25% in Rank@1 accuracy on CUHK-PEDES, ICFG-PEDES and RSTPReid datasets, respectively. The code is available at https://github.com/codeGallery24/CPCL.","['Yanwei Zheng', 'Xinpeng Zhao', 'Chuanlin Lan', 'Xiaowei Zhang', 'Bowen Huang', 'Jibin Yang', 'Dongxiao Yu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.10011,Anomali
Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework,"In the field of scientific computing, many problem-solving approaches tend to focus only on the process and final outcome, even in AI for science, there is a lack of deep multimodal informationminingbehind the data, missing a multimodal framework akin to that in the image-textdomain. In this paper, we take Symbolic Regression(SR) as our focal point and, drawing inspiration from the BLIP model in the image-textdomain, propose a scientific computing multimodal framework based on Function Images (Funcimg) and Operation Tree Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In SR experiments, we validate the advantages of Botfip in low-complexity SR problems, showcasing its potential. As a MED framework, Botfip holds promise for future applications in a broader range of scientific computing problems.","['Tianhao Chen', 'Pengbo Xu', 'Haibiao Zheng']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.09748,Anomali
Towards Automatic Translation of Machine Learning Visual Insights to Analytical Assertions,"We present our vision for developing an automated tool capable of translating visual properties observed in Machine Learning (ML) visualisations into Python assertions. The tool aims to streamline the process of manually verifying these visualisations in the ML development cycle, which is critical as real-world data and assumptions often change post-deployment. In a prior study, wemined$54,070$ Jupyter notebooks from Github and created a catalogue of $269$ semantically related visualisation-assertion (VA) pairs. Building on this catalogue, we propose to build a taxonomy that organises the VA pairs based on ML verification tasks. The input feature space comprises of a rich source of informationminedfrom the Jupyter notebooks -- visualisations, Python source code, and associated markdowntext. The effectiveness of various AI models, including traditional NLP4Code models and modern Large Language Models, will be compared using established machine translation metrics and evaluated through a qualitative study with human participants. The paper also plans to address the challenge of extending the existing VA pair dataset with additional pairs from Kaggle and to compare the tool's effectiveness with commercial generative AI models like ChatGPT. This research not only contributes to the field of ML system validation but also explores novel ways to leverage AI for automating and enhancing software engineering practices in ML.","['Arumoy Shome', 'Luis Cruz', 'Arie van Deursen']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.07696,Anomali
Taec: a Manually annotated text dataset for trait and phenotype extraction and entity linking in wheat breeding literature,"Wheat varieties show a large diversity of traits and phenotypes. Linking them to genetic variability is essential for shorter and more efficient wheat breeding programs. Newly desirable wheat variety traits include disease resistance to reduce pesticide use, adaptation to climate change, resistance to heat and drought stresses, or low gluten content of grains. Wheat breeding experiments are documented by a large body of scientific literature and observational data obtained in-field and under controlled conditions. The cross-referencing of complementary information from the literature and observational data is essential to the study of the genotype-phenotype relationship and to the improvement of wheat selection. The scientific literature on genetic marker-assisted selection describes much information about the genotype-phenotype relationship. However, the variety of expressions used to refer to traits and phenotype values in scientific articles is a hinder to finding information and cross-referencing it. When trained adequately by annotated examples, recenttextminingmethods perform highly in named entity recognition and linking in the scientific domain. While several corpora contain annotations of human and animal phenotypes, currently, no corpus is available for training and evaluating named entity recognition and entity-linking methods in plant phenotype literature. The Triticum aestivum trait Corpus is a new gold standard for traits and phenotypes of wheat. It consists of 540 PubMed references fully annotated for trait, phenotype, and species named entities using the Wheat Trait and Phenotype Ontology and the species taxonomy of the National Center for Biotechnology Information. A study of the performance of tools trained on the Triticum aestivum trait Corpus shows that the corpus is suitable for the training and evaluation of named entity recognition and linking.","['Claire Nédellec', 'Clara Sauvion', 'Robert Bossy', 'Mariya Borovikova', 'Louise Deléger']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.07447,Anomali
Two Directions for Clinical Data Generation with Large Language Models: Data-to-Label and Label-to-Data,"Large language models (LLMs) can generate natural languagetextsfor various domains and tasks, but their potential for clinicaltextmining, a domain with scarce, sensitive, and imbalanced medical data, is underexplored. We investigate whether LLMs can augment clinical data for detecting Alzheimer's Disease (AD)-related signs and symptoms from electronic health records (EHRs), a challenging task that requires high expertise. We create a novel pragmatic taxonomy for AD sign and symptom progression based on expert knowledge, which guides LLMs to generate synthetic data following two different directions: ""data-to-label"", which labels sentences from a public EHR collection with AD-related signs and symptoms; and ""label-to-data"", which generates sentences with AD-related signs and symptoms based on the label definition. We train a system to detect AD-related signs and symptoms from EHRs, using three datasets: (1) a gold dataset annotated by human experts on longitudinal EHRs of AD patients; (2) a silver dataset created by the data-to-label method; and (3) a bronze dataset created by the label-to-data method. We find that using the silver and bronze datasets improves the system performance, outperforming the system using only the gold dataset. This shows that LLMs can generate synthetic clinical data for a complex task by incorporating expert knowledge, and our label-to-data method can produce datasets that are free of sensitive information, while maintaining acceptable quality.","['Rumeng Li', 'Xun Wang', 'Hong Yu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2401.06774,Anomali
GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model,"Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images,text, and audio. Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios. The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications. However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods. Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency. The main approach is via optimising the training process of the generator parameters. With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models. Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms. The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm. The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN","['Zhiyu Zhu', 'Huaming Chen', 'Xinyi Wang', 'Jiayu Zhang', 'Zhibo Jin', 'Kim-Kwang Raymond Choo', 'Jun Shen', 'Dong Yuan']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.06031,Anomali
YOIO: You Only Iterate Once by mining and fusing multiple necessary global information in the optical flow estimation,"Occlusions pose a significant challenge to optical flow algorithms that even rely on global evidences. We consider an occluded point to be one that is imaged in the reference frame but not in the next. Estimating the motion of these points is extremely difficult, particularly in the two-frame setting. Previous work only used the current frame as the only input, which could not guarantee providing correct global reference information for occluded points, and had problems such as long calculation time and poor accuracy in predicting optical flow at occluded points. To enable both high accuracy and efficiency, We fullymineand utilize the spatiotemporal information provided by the frame pair, design a loopback judgment algorithm to ensure that correct global reference information is obtained,minemultiple necessary global information, and design an efficient refinement module that fuses these global information. Specifically, we propose a YOIO framework, which consists of three main components: an initial flow estimator, a multiple global information extraction module, and a unified refinement module. We demonstrate that optical flow estimates in the occluded regions can be significantly improved in only one iteration without damaging the performance in non-occluded regions. Compared with GMA, the optical flow prediction accuracy of this method in the occluded area is improved by more than 10%, and the occ_out area exceeds 15%, while the calculation time is 27% shorter. This approach, running up to 18.9fps with 436*1024 image resolution, obtains new state-of-the-art results on the challenging Sintel dataset among all published and unpublished approaches that can run in real-time, suggesting a new paradigm for accurate and efficient optical flow estimation.","['Yu Jing', 'Tan Yujuan', 'Ren Ao', 'Liu Duo']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.05879,Anomali
A Toolbox for Modelling Engagement with Educational Videos,"With the advancement and utility of Artificial Intelligence (AI), personalising education to a global population could be a cornerstone of new educational systems in the future. This work presents the PEEKC dataset and the TrueLearn Python library, which contains a dataset and a series of online learner state models that are essential to facilitate research on learner engagement modelling.TrueLearn family of models was designed following the ""open learner"" concept, using humanly-intuitive user representations. This family of scalable, online models also help end-users visualise the learner models, which may in the future facilitate user interaction with their models/recommenders. The extensive documentation and coding examples make the library highly accessible to both machine learning developers and educational dataminingand learning analytics practitioners. The experiments show the utility of both the dataset and the library with predictive performance significantly exceeding comparative baseline models. The dataset contains a large amount of AI-related educational videos, which are of interest for building and validating AI-specific educational recommenders.","['Yuxiang Qiu', 'Karim Djemili', 'Denis Elezi', 'Aaneel Shalman', 'María Pérez-Ortiz', 'Emine Yilmaz', 'John Shawe-Taylor', 'Sahan Bulathwela']",,arXiv,2023,https://doi.org/10.48550/arXiv.2401.05424,Anomali
Exact representation and efficient approximations of linear model predictive control laws via HardTanh type deep neural networks,"Deep neural networks have revolutionized many fields, including image processing, inverse problems,textminingand more recently, give very promising results in systems and control. Neural networks with hidden layers have a strong potential as an approximation framework of predictive control laws as they usually yield better approximation quality and smaller memory requirements than existing explicit (multi-parametric) approaches. In this paper, we first show that neural networks with HardTanh activation functions can exactly represent predictive control laws of linear time-invariant systems. We derive theoretical bounds on the minimum number of hidden layers and neurons that a HardTanh neural network should have to exactly represent a given predictive control law. The choice of HardTanh deep neural networks is particularly suited for linear predictive control laws as they usually require less hidden layers and neurons than deep neural networks with ReLU units for representing exactly continuous piecewise affine (or equivalently min-max) maps. In the second part of the paper we bring the physics of the model and standard optimization techniques into the architecture design, in order to eliminate the disadvantages of the black-box HardTanh learning. More specifically, we design trainable unfolded HardTanh deep architectures for learning linear predictive control laws based on two standard iterative optimization algorithms, i.e., projected gradient descent and accelerated projected gradient descent. We also study the performance of the proposed HardTanh type deep neural networks on a linear model predictive control application.","['Daniela Lupu', 'Ion Necoara']","Systems and Control Letters, 2024",arXiv,2024,https://doi.org/10.48550/arXiv.2401.05076,Anomali
Towards Explainable Artificial Intelligence (XAI): A Data Mining Perspective,"Given the complexity and lack of transparency in deep neural networks (DNNs), extensive efforts have been made to make these systems more interpretable or explain their behaviors in accessible terms. Unlike most reviews, which focus on algorithmic and model-centric perspectives, this work takes a ""data-centric"" view, examining how data collection, processing, and analysis contribute to explainable AI (XAI). We categorize existing work into three categories subject to their purposes: interpretations of deep models, referring to feature attributions and reasoning processes that correlate data points with model outputs; influences of training data, examining the impact of training data nuances, such as data valuation and sample anomalies, on decision-making processes; and insights of domain knowledge, discovering latent patterns and fostering new knowledge from data and models to advance social values and scientific discovery. Specifically, we distill XAI methodologies into dataminingoperations on training and testing data across modalities, such as images,text, and tabular data, as well as on training logs, checkpoints, models and other DNN behavior descriptors. In this way, our study offers a comprehensive, data-centric examination of XAI from a lens of dataminingmethods and applications.","['Haoyi Xiong', 'Xuhong Li', 'Xiaofei Zhang', 'Jiamin Chen', 'Xinhao Sun', 'Yuchen Li', 'Zeyi Sun', 'Mengnan Du']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.04374,Anomali
IDoFew: Intermediate Training Using Dual-Clustering in Language Models for Few Labels Text Classification,"Language models such as Bidirectional Encoder Representations from Transformers (BERT) have been very effective in various Natural Language Processing (NLP) andtextminingtasks includingtextclassification. However, some tasks still pose challenges for these models, includingtextclassification with limited labels. This can result in a cold-start problem. Although some approaches have attempted to address this problem through single-stage clustering as an intermediate training step coupled with a pre-trained language model, which generates pseudo-labels to improve classification, these methods are often error-prone due to the limitations of the clustering algorithms. To overcome this, we have developed a novel two-stage intermediate clustering with subsequent fine-tuning that models the pseudo-labels reliably, resulting in reduced prediction errors. The key novelty in our model, IDoFew, is that the two-stage clustering coupled with two different clustering algorithms helps exploit the advantages of the complementary algorithms that reduce the errors in generating reliable pseudo-labels for fine-tuning. Our approach has shown significant improvements compared to strong comparative models.","['Abdullah Alsuhaibani', 'Hamad Zogan', 'Imran Razzak', 'Shoaib Jameel', 'Guandong Xu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.04025,Anomali
Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training,"Image captioning aims at generating descriptive and meaningful textual descriptions of images, enabling a broad range of vision-language applications. Prior works have demonstrated that harnessing the power of Contrastive Image Language Pre-training (CLIP) offers a promising approach to achieving zero-shot captioning, eliminating the need for expensive caption annotations. However, the widely observed modality gap in the latent space of CLIP harms the performance of zero-shot captioning by breaking the alignment between paired image-textfeatures. To address this issue, we conduct an analysis on the CLIP latent space which leads to two findings. Firstly, we observe that the CLIP's visual feature of image subregions can achieve closer proximity to the paired caption due to the inherent information loss intextdescriptions. In addition, we show that the modality gap between a paired image-textcan be empirically modeled as a zero-mean Gaussian distribution. Motivated by the findings, we propose a novel zero-shot image captioning framework withtext-only training to reduce the modality gap. In particular, we introduce a subregion feature aggregation to leverage local region information, which produces a compact visual representation for matchingtextrepresentation. Moreover, we incorporate a noise injection and CLIP reranking strategy to boost captioning performance. We also extend our framework to build a zero-shot VQA pipeline, demonstrating its generality. Through extensive experiments on common captioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that our method achieves remarkable performance improvements. Code is available at https://github.com/Artanic30/MacCap.","['Longtian Qiu', 'Shan Ning', 'Xuming He']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.02347,Anomali
Text mining arXiv: a look through quantitative finance papers,"This paper explores articles hosted on the arXiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research. Employingtextminingtechniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arXiv from 1997 to 2022. We extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain. Additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches.",['Michele Leonardo Bianchi'],,arXiv,2024,https://doi.org/10.48550/arXiv.2401.01751,Anomali
Context-Guided Spatio-Temporal Video Grounding,"Spatio-temporal video grounding (or STVG) task aims at locating a spatio-temporal tube for a specific instance given atextquery. Despite advancements, current methods easily suffer the distractors or heavy object appearance variations in videos due to insufficient object information from thetext, leading to degradation. Addressing this, we propose a novel framework, context-guided STVG (CG-STVG), whichminesdiscriminative instance context for object in videos and applies it as a supplementary guidance for target localization. The key of CG-STVG lies in two specially designed modules, including instance context generation (ICG), which focuses on discovering visual context information (in both appearance and motion) of the instance, and instance context refinement (ICR), which aims to improve the instance context from ICG by eliminating irrelevant or even harmful information from the context. During grounding, ICG, together with ICR, are deployed at each decoding stage of a Transformer architecture for instance context learning. Particularly, instance context learned from one decoding stage is fed to the next stage, and leveraged as a guidance containing rich and discriminative object feature to enhance the target-awareness in decoding feature, which conversely benefits generating better new instance context for improving localization finally. Compared to existing methods, CG-STVG enjoys object information intextquery and guidance fromminedinstance visual context for more accurate target localization. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, CG-STVG sets new state-of-the-arts in m_tIoU and m_vIoU on all of them, showing its efficacy. The code will be released at https://github.com/HengLan/CGSTVG.","['Xin Gu', 'Heng Fan', 'Yan Huang', 'Tiejian Luo', 'Libo Zhang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2401.01578,Anomali
An internet reviews topic hierarchy mining method based on modified continuous renormalization procedure,"Miningthe hierarchical structure of Internet review topics and realizing a fine classification of reviewtextscan help alleviate users' information overload. However, existing hierarchical topic classification methods primarily rely on external corpora and human intervention. This study proposes a Modified Continuous Renormalization (MCR) procedure that acts on the keyword co-occurrence network with fractal characteristics to achieve the topic hierarchymining. First, the fractal characteristics in the keyword co-occurrence network of Internet reviewtextare identified using a box-covering algorithm for the first time. Then, the MCR algorithm established on the edge adjacency entropy and the box distance is proposed to obtain the topic hierarchy in the keyword co-occurrence network. Verification data from the Dangdang.com book reviews shows that the MCR constructs topic hierarchies with greater coherence and independence than the HLDA and the Louvain algorithms. Finally, reliable reviewtextclassification is achieved using the MCR extended bottom level topic categories. The accuracy rate (P), recall rate (R) and F1 value of Internet reviewtextclassification obtained from the MCR-based topic hierarchy are significantly improved compared to four targettextclassification algorithms.","['Lin Qi', 'Feiyan Guo', 'Jian Zhang', 'Yuwei Wang']","Fractals, Vol. 30, No. 7 (2022) 2250134",arXiv,2024,https://doi.org/10.48550/arXiv.2401.01118,Anomali
Language Model as an Annotator: Unsupervised Context-aware Quality Phrase Generation,"Phraseminingis a fundamentaltextminingtask that aims to identify quality phrases from context. Nevertheless, the scarcity of extensive gold labels datasets, demanding substantial annotation efforts from experts, renders this task exceptionally challenging. Furthermore, the emerging, infrequent, and domain-specific nature of quality phrases presents further challenges in dealing with this task. In this paper, we propose LMPhrase, a novel unsupervised context-aware quality phraseminingframework built upon large pre-trained language models (LMs). Specifically, we firstminequality phrases as silver labels by employing a parameter-free probing technique called Perturbed Masking on the pre-trained language model BERT (coined as Annotator). In contrast to typical statistic-based or distantly-supervised methods, our silver labels, derived from large pre-trained language models, take into account rich contextual information contained in the LMs. As a result, they bring distinct advantages in preserving informativeness, concordance, and completeness of quality phrases. Secondly, training a discriminative span prediction model heavily relies on massive annotated data and is likely to face the risk of overfitting silver labels. Alternatively, we formalize phrase tagging task as the sequence generation problem by directly fine-tuning on the Sequence-to-Sequence pre-trained language model BART with silver labels (coined as Generator). Finally, we merge the quality phrases from both the Annotator and Generator as the final predictions, considering their complementary nature and distinct characteristics. Extensive experiments show that our LMPhrase consistently outperforms all the existing competitors across two different granularity phraseminingtasks, where each task is tested on two different domain datasets.","['Zhihao Zhang', 'Yuan Zuo', 'Chenghua Lin', 'Junjie Wu']",Knowledge-Based Systems 2024,arXiv,2023,https://doi.org/10.48550/arXiv.2312.17349,Anomali
Efficient High-Quality Clustering for Large Bipartite Graphs,"A bipartite graph contains inter-set edges between two disjoint vertex sets, and is widely used to model real-world data, such as user-item purchase records, author-article publications, and biological interactions between drugs and proteins. k-Bipartite Graph Clustering (k-BGC) is to partition the target vertex set in a bipartite graph into k disjoint clusters. The clustering quality is important to the utility of k-BGC in various applications like social network analysis, recommendation systems,textmining, and bioinformatics, to name a few. Existing approaches to k-BGC either output clustering results with compromised quality due to inadequate exploitation of high-order information between vertices, or fail to handle sizable bipartite graphs with billions of edges.
  Motivated by this, this paper presents two efficient k-BGC solutions, HOPE and HOPE+, which achieve state-of-the-art performance on large-scale bipartite graphs. HOPE obtains high scalability and effectiveness through a new k-BGC problem formulation based on the novel notion of high-order perspective (HOP) vectors and an efficient technique for low-rank approximation of HOP vectors. HOPE+ further elevates the k-BGC performance to another level with a judicious problem transformation and a highly efficient two-stage optimization framework. Two variants, HOPE+ (FNEM) and HOPE+ (SNEM) are designed when either the Frobenius norm or spectral norm is applied in the transformation. Extensive experiments, comparing HOPE and HOPE+ against 13 competitors on 10 real-world datasets, exhibit that our solutions, especially HOPE+, are superior to existing methods in terms of result quality, while being up to orders of magnitude faster. On the largest dataset MAG with 1.1 billion edges, HOPE+ is able to produce clusters with the highest clustering accuracy within 31 minutes, which is unmatched by any existing solution for k-BGC.","['Renchi Yang', 'Jieming Shi']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.16926,Anomali
"AHAM: Adapt, Help, Ask, Model -- Harvesting LLMs for literature mining","In an era marked by a rapid increase in scientific publications, researchers grapple with the challenge of keeping pace with field-specific advances. We present the `AHAM' methodology and a metric that guides the domain-specific \textbf{adapt}ation of the BERTopic topic modeling framework to improve scientifictextanalysis. By utilizing the LLaMa2 generative language model, we generate topic definitions via one-shot learning by crafting prompts with the \textbf{help} of domain experts to guide the LLM for literatureminingby \textbf{asking} it to model the topic names. For inter-topic similarity evaluation, we leverage metrics from language generation and translation processes to assess lexical and semantic similarity of the generated topics. Our system aims to reduce both the ratio of outlier topics to the total number of topics and the similarity between topic definitions. The methodology has been assessed on a newly gathered corpus of scientific papers on literature-based discovery. Through rigorous evaluation by domain experts, AHAM has been validated as effective in uncovering intriguing and novel insights within broad research areas. We explore the impact of domain adaptation of sentence-transformers for the task of topic \textbf{model}ing using two datasets, each specialized to specific scientific domains within arXiv and medarxiv. We evaluate the impact of data size, the niche of adaptation, and the importance of domain adaptation. Our results suggest a strong interaction between domain adaptation and topic modeling precision in terms of outliers and topic definitions.","['Boshko Koloski', 'Nada Lavrač', 'Bojan Cestnik', 'Senja Pollak', 'Blaž Škrlj', 'Andrej Kastrin']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.15784,Anomali
Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs,"The unique capabilities of Large Language Models (LLMs), such as the natural languagetextgeneration ability, position them as strong candidates for providing explanation for recommendations. However, despite the size of the LLM, most existing models struggle to produce zero-shot explanations reliably. To address this issue, we propose a framework called Logic-Scaffolding, that combines the ideas of aspect-based explanation and chain-of-thought prompting to generate explanations through intermediate reasoning steps. In this paper, we share our experience in building the framework and present an interactive demonstration for exploring our results.","['Behnam Rahdari', 'Hao Ding', 'Ziwei Fan', 'Yifei Ma', 'Zhuotong Chen', 'Anoop Deoras', 'Branislav Kveton']",,arXiv,2024,https://doi.org/10.48550/arXiv.2312.14345,Anomali
Bootstrap Masked Visual Modeling via Hard Patches Mining,"Masked visual modeling has attracted much attention due to its promising potential in learning generalizable representations. Typical approaches urge models to predict specific contents of masked tokens, which can be intuitively considered as teaching a student (the model) to solve given problems (predicting masked contents). Under such settings, the performance is highly correlated with mask strategies (the difficulty of provided problems). We argue that it is equally important for the model to stand in the shoes of a teacher to produce challenging problems by itself. Intuitively, patches with high values of reconstruction loss can be regarded as hard samples, and masking those hard patches naturally becomes a demanding reconstruction task. To empower the model as a teacher, we propose Hard PatchesMining(HPM), predicting patch-wise losses and subsequently determining where to mask. Technically, we introduce an auxiliary loss predictor, which is trained with a relative objective to prevent overfitting to exact loss values. Also, to gradually guide the training procedure, we propose an easy-to-hard mask strategy. Empirically, HPM brings significant improvements under both image and video benchmarks. Interestingly, solely incorporating the extra loss prediction objective leads to better representations, verifying the efficacy of determining where is hard to reconstruct. The code is available at https://github.com/Haochen-Wang409/HPM.","['Haochen Wang', 'Junsong Fan', 'Yuxi Wang', 'Kaiyou Song', 'Tiancai Wang', 'Xiangyu Zhang', 'Zhaoxiang Zhang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.13714,Anomali
Debiasing Sample Loadings and Scores in Exponential Family PCA for Sparse Count Data,"Multivariate count data with many zeros frequently occur in a variety of application areas such astextminingwith a document-term matrix and cluster analysis with microbiome abundance data. Exponential family PCA (Collins et al., 2001) is a widely used dimension reduction tool to understand and capture the underlying low-rank structure of count data. It produces principal component scores by fitting Poisson regression models with estimated loadings as covariates. This tends to result in extreme scores for sparse count data significantly deviating from true scores. We consider two major sources of bias in this estimation procedure and propose ways to reduce their effects. First, the discrepancy between true loadings and their estimates under a limited sample size largely degrades the quality of score estimates. By treating estimated loadings as covariates with bias and measurement errors, we debias score estimates, using the iterative bootstrap method for loadings and considering classical measurement error models. Second, the existence of MLE bias is often ignored in score estimation, but this bias could be removed through well-known MLE bias reduction methods. We demonstrate the effectiveness of the proposed bias correction procedure through experiments on both simulated data and real data.","['Ruochen Huang', 'Yoonkyung Lee']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.13430,Anomali
Weakly Supervised Open-Vocabulary Object Detection,"Despite weakly supervised object detection (WSOD) being a promising step toward evading strong instance-level annotations, its capability is confined to closed-set categories within a single training dataset. In this paper, we propose a novel weakly supervised open-vocabulary object detection framework, namely WSOVOD, to extend traditional WSOD to detect novel concepts and utilize diverse datasets with only image-level annotations. To achieve this, we explore three vital strategies, including dataset-level feature adaptation, image-level salient object localization, and region-level vision-language alignment. First, we perform data-aware feature extraction to produce an input-conditional coefficient, which is leveraged into dataset attribute prototypes to identify dataset bias and help achieve cross-dataset generalization. Second, a customized location-oriented weakly supervised region proposal network is proposed to utilize high-level semantic layouts from the category-agnostic segment anything model to distinguish object boundaries. Lastly, we introduce a proposal-concept synchronized multiple-instance network, i.e., objectminingand refinement with visual-semantic alignment, to discover objects matched to thetextembeddings of concepts. Extensive experiments on Pascal VOC and MS COCO demonstrate that the proposed WSOVOD achieves new state-of-the-art compared with previous WSOD methods in both close-set object localization and detection tasks. Meanwhile, WSOVOD enables cross-dataset and open-vocabulary learning to achieve on-par or even better performance than well-established fully-supervised open-vocabulary object detection (FSOVOD).","['Jianghang Lin', 'Yunhang Shen', 'Bingquan Wang', 'Shaohui Lin', 'Ke Li', 'Liujuan Cao']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.12437,Anomali
Coreference Graph Guidance for Mind-Map Generation,"Mind-map generation aims to process a document into a hierarchical structure to show its central idea and branches. Such a manner is more conducive to understanding the logic and semantics of the document than plaintext. Recently, a state-of-the-art method encodes the sentences of a document sequentially and converts them to a relation graph via sequence-to-graph. Though this method is efficient to generate mind-maps in parallel, its mechanism focuses more on sequential features while hardly capturing structural information. Moreover, it's difficult to model long-range semantic relations. In this work, we propose a coreference-guided mind-map generation network (CMGN) to incorporate external structure knowledge. Specifically, we construct a coreference graph based on the coreference semantic relationship to introduce the graph structure information. Then we employ a coreference graph encoder tominethe potential governing relations between sentences. In order to exclude noise and better utilize the information of the coreference graph, we adopt a graph enhancement module in a contrastive learning manner. Experimental results demonstrate that our model outperforms all the existing methods. The case study further proves that our model can more accurately and concisely reveal the structure and semantics of a document. Code and data are available at https://github.com/Cyno2232/CMGN.","['Zhuowei Zhang', 'Mengting Hu', 'Yinhao Bai', 'Zhen Zhang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.11997,Anomali
User Modeling in the Era of Large Language Models: Current Research and Future Directions,"User modeling (UM) aims to discover patterns or learn representations from user data about the characteristics of a specific user, such as profile, preference, and personality. The user models enable personalization and suspiciousness detection in many online applications such as recommendation, education, and healthcare. Two common types of user data aretextand graph, as the data usually contain a large amount of user-generated content (UGC) and online interactions. The research oftextand graphminingis developing rapidly, contributing many notable solutions in the past two decades. Recently, large language models (LLMs) have shown superior performance on generating, understanding, and even reasoning overtextdata. The approaches of user modeling have been equipped with LLMs and soon become outstanding. This article summarizes existing research about how and why LLMs are great tools of modeling and understanding UGC. Then it reviews a few categories of large language models for user modeling (LLM-UM) approaches that integrate the LLMs withtextand graph-based methods in different ways. Then it introduces specific LLM-UM techniques for a variety of UM applications. Finally, it presents remaining challenges and future directions in the LLM-UM research. We maintain the reading list at: https://github.com/TamSiuhin/LLM-UM-Reading","['Zhaoxuan Tan', 'Meng Jiang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.11518,Anomali
An Extended Variational Mode Decomposition Algorithm Developed Speech Emotion Recognition Performance,"Emotion recognition (ER) from speech signals is a robust approach since it cannot be imitated like facial expression ortextbased sentiment analysis. Valuable information underlying the emotions are significant for human-computer interactions enabling intelligent machines to interact with sensitivity in the real world. Previous ER studies through speech signal processing have focused exclusively on associations between different signal mode decomposition methods and hidden informative features. However, improper decomposition parameter selections lead to informative signal component losses due to mode duplicating and mixing. In contrast, the current study proposes VGG-optiVMD, an empowered variational mode decomposition algorithm, to distinguish meaningful speech features and automatically select the number of decomposed modes and optimum balancing parameter for the data fidelity constraint by assessing their effects on the VGG16 flattening output layer. Various feature vectors were employed to train the VGG16 network on different databases and assess VGG-optiVMD reproducibility and reliability. One, two, and three-dimensional feature vectors were constructed by concatenating Mel-frequency cepstral coefficients, Chromagram, Mel spectrograms, Tonnetz diagrams, and spectral centroids. Results confirmed a synergistic relationship between the fine-tuning of the signal sample rate and decomposition parameters with classification accuracy, achieving state-of-the-art 96.09% accuracy in predicting seven emotions on the Berlin EMO-DB database.","['David Hason Rudd', 'Huan Huo', 'Guandong Xu']","Advances in Knowledge Discovery and Data Mining. PAKDD 2023. Lecture Notes in Computer Science(), vol 13937. Springer, Cham",arXiv,2023,https://doi.org/10.48550/arXiv.2312.10937,Anomali
Sentiment Analysis and Text Analysis of the Public Discourse on Twitter about COVID-19 and MPox,"Miningand analysis of the big data of Twitter conversations have been of significant interest to the scientific community in the fields of healthcare, epidemiology, big data, data science, computer science, and their related areas, as can be seen from several works in the last few years that focused on sentiment analysis and other forms oftextanalysis of tweets related to Ebola, E-Coli, Dengue, Human Papillomavirus, Middle East Respiratory Syndrome, Measles, Zika virus, H1N1, influenza like illness, swine flu, flu, Cholera, Listeriosis, cancer, Liver Disease, Inflammatory Bowel Disease, kidney disease, lupus, Parkinsons, Diphtheria, and West Nile virus. The recent outbreaks of COVID-19 and MPox have served as catalysts for Twitter usage related to seeking and sharing information, views, opinions, and sentiments involving both of these viruses. None of the prior works in this field analyzed tweets focusing on both COVID-19 and MPox simultaneously. To address this research gap, a total of 61,862 tweets that focused on MPox and COVID-19 simultaneously, posted between 7 May 2022 and 3 March 2023, were studied. The findings and contributions of this study are manifold. First, the results of sentiment analysis using the VADER approach show that nearly half the tweets had a negative sentiment. It was followed by tweets that had a positive sentiment and tweets that had a neutral sentiment, respectively. Second, this paper presents the top 50 hashtags used in these tweets. Third, it presents the top 100 most frequently used words in these tweets after performing tokenization, removal of stopwords, and word frequency analysis. Finally, a comprehensive comparative study that compares the contributions of this paper with 49 prior works in this field is presented to further uphold the relevance and novelty of this work.",['Nirmalya Thakur'],,arXiv,2023,https://doi.org/10.48550/arXiv.2312.10580,Anomali
WikiMuTe: A web-sourced dataset of semantic descriptions for music audio,"Multi-modal deep learning techniques for matching free-formtextwith music have shown promising results in the field of Music Information Retrieval (MIR). Prior work is often based on large proprietary data while publicly available datasets are few and small in size. In this study, we present WikiMuTe, a new and open dataset containing rich semantic descriptions of music. The data is sourced from Wikipedia's rich catalogue of articles covering musical works. Using a dedicatedtext-miningpipeline, we extract both long and short-form descriptions covering a wide range of topics related to music content such as genre, style, mood, instrumentation, and tempo. To show the use of this data, we train a model that jointly learnstextand audio representations and performs cross-modal retrieval. The model is evaluated on two tasks: tag-based music retrieval and music auto-tagging. The results show that while our approach has state-of-the-art performance on multiple tasks, but still observe a difference in performance depending on the data used for training.","['Benno Weck', 'Holger Kirchhoff', 'Peter Grosche', 'Xavier Serra']","The Version of Record of this contribution is published in MultiMedia Modeling. MMM 2024. Lecture Notes in Computer Science, vol 14565. Springer, Cham",arXiv,2023,https://doi.org/10.48550/arXiv.2312.09207,Anomali
Object Recognition from Scientific Document based on Compartment Refinement Framework,"With the rapid development of the internet in the past decade, it has become increasingly important to extract valuable information from vast resources efficiently, which is crucial for establishing a comprehensive digital ecosystem, particularly in the context of research surveys and comprehension. The foundation of these tasks focuses on accurate extraction and deepminingof data from scientific documents, which are essential for building a robust data infrastructure. However, parsing raw data or extracting data from complex scientific documents have been ongoing challenges. Current data extraction methods for scientific documents typically use rule-based (RB) or machine learning (ML) approaches. However, using rule-based methods can incur high coding costs for articles with intricate typesetting. Conversely, relying solely on machine learning methods necessitates annotation work for complex content types within the scientific document, which can be costly. Additionally, few studies have thoroughly defined and explored the hierarchical layout within scientific documents. The lack of a comprehensive definition of the internal structure and elements of the documents indirectly impacts the accuracy oftextclassification and object recognition tasks. From the perspective of analyzing the standard layout and typesetting used in the specified publication, we propose a new document layout analysis framework called CTBR(Compartment &TextBlocks Refinement). Firstly, we define scientific documents into hierarchical divisions: base domain, compartment, andtextblocks. Next, we conduct an in-depth exploration and classification of the meanings oftextblocks. Finally, we utilize the results oftextblock classification to implement object recognition within scientific documents based on rule-based compartment segmentation.","['Jinghong Li', 'Wen Gu', 'Koichi Ota', 'Shinobu Hasegawa']","SN COMPUT. SCI. 5, 816 (2024)",arXiv,2024,https://doi.org/10.48550/arXiv.2312.09038,Anomali
Federated Learning for Short Text Clustering,"Shorttextclustering has been popularly studied for its significance inminingvaluable insights from many shorttexts. In this paper, we focus on the federated shorttextclustering (FSTC) problem, i.e., clustering shorttextsthat are distributed in different clients, which is a realistic problem under privacy requirements. Compared with the centralized shorttextclustering problem that shorttextsare stored on a central server, the FSTC problem has not been explored yet. To fill this gap, we propose a Federated Robust ShortTextClustering (FSTC) framework. FSTC includes two main modules, i.e., robust shorttextclustering module and federated cluster center aggregation module. The robust shorttextclustering module aims to train an effective shorttextclustering model with local data in each client. We innovatively combine optimal transport to generate pseudo-labels with Gaussian-uniform mixture model to ensure the reliability of the pseudo-supervised data. The federated cluster center aggregation module aims to exchange knowledge across clients without sharing local raw data in an efficient way. The server aggregates the local cluster centers from different clients and then sends the global centers back to all clients in each communication round. Our empirical studies on three shorttextclustering datasets demonstrate that FSTC significantly outperforms the federated shorttextclustering baselines.","['Mengling Hu', 'Chaochao Chen', 'Weiming Liu', 'Xinting Liao', 'Xiaolin Zheng']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.07556,Anomali
GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos,"We address the task of generating temporally consistent and physically plausible images of actions and object state transformations. Given an input image and atextprompt describing the targeted transformation, our generated images preserve the environment and transform objects in the initial image. Our contributions are threefold. First, we leverage a large body of instructional videos and automaticallyminea dataset of triplets of consecutive frames corresponding to initial object states, actions, and resulting object transformations. Second, equipped with this data, we develop and train a conditioned diffusion model dubbed GenHowTo. Third, we evaluate GenHowTo on a variety of objects and actions and show superior performance compared to existing methods. In particular, we introduce a quantitative evaluation where GenHowTo achieves 88% and 74% on seen and unseen interaction categories, respectively, outperforming prior work by a large margin.","['Tomáš Souček', 'Dima Damen', 'Michael Wray', 'Ivan Laptev', 'Josef Sivic']",,arXiv,2024,https://doi.org/10.48550/arXiv.2312.07322,Anomali
MAFA: Managing False Negatives for Vision-Language Pre-training,"We consider a critical issue of false negatives in Vision-Language Pre-training (VLP), a challenge that arises from the inherent many-to-many correspondence of image-textpairs in large-scale web-crawled datasets. The presence of false negatives can impede achieving optimal performance and even lead to a significant performance drop. To address this challenge, we propose MAFA (MAnaging FAlse negatives), which consists of two pivotal components building upon the recently developed GRouped mIni-baTch sampling (GRIT) strategy: 1) an efficient connectionminingprocess that identifies and converts false negatives into positives, and 2) label smoothing for the image-textcontrastive (ITC) loss. Our comprehensive experiments verify the effectiveness of MAFA across multiple downstream tasks, emphasizing the crucial role of addressing false negatives in VLP, potentially even surpassing the importance of addressing false positives. In addition, the compatibility of MAFA with the recent BLIP-family model is also demonstrated. Code is available at https://github.com/jaeseokbyun/MAFA.","['Jaeseok Byun', 'Dohoon Kim', 'Taesup Moon']",,arXiv,2024,https://doi.org/10.48550/arXiv.2312.06112,Anomali
Mining Gaze for Contrastive Learning toward Computer-Assisted Diagnosis,"Obtaining large-scale radiology reports can be difficult for medical images due to various reasons, limiting the effectiveness of contrastive pre-training in the medical image domain and underscoring the need for alternative methods. In this paper, we propose eye-tracking as an alternative totextreports, as it allows for the passive collection of gaze signals without disturbing radiologist's routine diagnosis process. By tracking the gaze of radiologists as they read and diagnose medical images, we can understand their visual attention and clinical reasoning. When a radiologist has similar gazes for two medical images, it may indicate semantic similarity for diagnosis, and these images should be treated as positive pairs when pre-training a computer-assisted diagnosis (CAD) network through contrastive learning. Accordingly, we introduce the Medical contrastive Gaze Image Pre-training (McGIP) as a plug-and-play module for contrastive learning frameworks. McGIP uses radiologist's gaze to guide contrastive pre-training. We evaluate our method using two representative types of medical images and two common types of gaze data. The experimental results demonstrate the practicality of McGIP, indicating its high potential for various clinical scenarios and applications.","['Zihao Zhao', 'Sheng Wang', 'Qian Wang', 'Dinggang Shen']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.06069,Anomali
A Generic NLI approach for Classification of Sentiment Associated with Therapies,"This paper describes our system for addressing SMM4H 2023 Shared Task 2 on ""Classification of sentiment associated with therapies (aspect-oriented)"". In our work, we adopt an approach based on Natural language inference (NLI) to formulate this task as a sentence pair classification problem, and train transformer models to predict sentiment associated with a therapy on a giventext. Our best model achieved 75.22\% F1-score which was 11\% (4\%) more than the mean (median) score of all teams' submissions.","['Rajaraman Kanagasabai', 'Anitha Veeramani']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.03737,Anomali
UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity,"Existingtext-based person retrieval datasets often have relatively coarse-grainedtextannotations. This hinders the model to comprehend the fine-grained semantics of querytextsin real scenarios. To address this problem, we contribute a new benchmark named \textbf{UFineBench} fortext-based person retrieval with ultra-fine granularity.
  Firstly, we construct a new \textbf{dataset} named UFine6926. We collect a large number of person images and manually annotate each image with two detailed textual descriptions, averaging 80.8 words each. The average word count is three to four times that of the previous datasets. In addition of standard in-domain evaluation, we also propose a special \textbf{evaluation paradigm} more representative of real scenarios. It contains a new evaluation set with cross domains, cross textual granularity and cross textual styles, named UFine3C, and a new evaluation metric for accurately measuring retrieval ability, named mean Similarity Distribution (mSD). Moreover, we propose CFAM, a more efficient \textbf{algorithm} especially designed fortext-based person retrieval with ultra fine-grainedtexts. It achieves fine granularityminingby adopting a shared cross-modal granularity decoder and hard negative match mechanism.
  With standard in-domain evaluation, CFAM establishes competitive performance across various datasets, especially on our ultra fine-grained UFine6926. Furthermore, by evaluating on UFine3C, we demonstrate that training on our UFine6926 significantly improves generalization to real scenarios compared with other coarse-grained datasets. The dataset and code will be made publicly available at \url{https://github.com/Zplusdragon/UFineBench}.","['Jialong Zuo', 'Hanyu Zhou', 'Ying Nie', 'Feng Zhang', 'Tianyu Guo', 'Nong Sang', 'Yunhe Wang', 'Changxin Gao']",,arXiv,2024,https://doi.org/10.48550/arXiv.2312.03441,Anomali
Concept Drift Adaptation in Text Stream Mining Settings: A Systematic Review,"The society produces textual data online in several ways, e.g., via reviews and social media posts. Therefore, numerous researchers have been working on discovering patterns in textual data that can indicate peoples' opinions, interests, etc. Most tasks regarding natural language processing are addressed using traditional machine learning methods and static datasets. This setting can lead to several problems, e.g., outdated datasets and models, which degrade in performance over time. This is particularly true regarding concept drift, in which the data distribution changes over time. Furthermore,textstreaming scenarios also exhibit further challenges, such as the high speed at which data arrives over time. Models for stream scenarios must adhere to the aforementioned constraints while learning from the stream, thus storingtextsfor limited periods and consuming low memory. This study presents a systematic literature review regarding concept drift adaptation intextstream scenarios. Considering well-defined criteria, we selected 48 papers published between 2018 and August 2024 to unravel aspects such astextdrift categories, detection types, model update mechanisms, streamminingtasks addressed, andtextrepresentation methods and their update mechanisms. Furthermore, we discussed drift visualization and simulation and listed real-world datasets used in the selected papers. Finally, we brought forward a discussion on existing works in the area, also highlighting open challenges and future research directions for the community.","['Cristiano Mesquita Garcia', 'Ramon Simoes Abilio', 'Alessandro Lameiras Koerich', 'Alceu de Souza Britto Jr.', 'Jean Paul Barddal']",ACM Transactions on Intelligent Systems and Technology. 2024,arXiv,2024,https://doi.org/10.48550/arXiv.2312.02901,Anomali
Scholarly Knowledge Graph Construction from Published Software Packages,"The value of structured scholarly knowledge for research and society at large is well understood, but producing scholarly knowledge (i.e., knowledge traditionally published in articles) in structured form remains a challenge. We propose an approach for automatically extracting scholarly knowledge from published software packages by static analysis of their metadata and contents (scripts and data) and populating a scholarly knowledge graph with the extracted knowledge. Our approach is based onminingscientific software packages linked to article publications by extracting metadata and analyzing the Abstract Syntax Tree (AST) of the source code to obtain information about the used and produced data as well as operations performed on data. The resulting knowledge graph includes articles, software packages metadata, and computational techniques applied to input data utilized as materials in research work. The knowledge graph also includes the results reported as scholarly knowledge in articles.","['Muhammad Haris', 'Sören Auer', 'Markus Stocker']",,arXiv,2023,https://doi.org/10.48550/arXiv.2312.01065,Anomali
When Graph Convolution Meets Double Attention: Online Privacy Disclosure Detection with Multi-Label Text Classification,"With the rise of Web 2.0 platforms such as online social media, people's private information, such as their location, occupation and even family information, is often inadvertently disclosed through online discussions. Therefore, it is important to detect such unwanted privacy disclosures to help alert people affected and the online platform. In this paper, privacy disclosure detection is modeled as a multi-labeltextclassification (MLTC) problem, and a new privacy disclosure detection model is proposed to construct an MLTC classifier for detecting online privacy disclosures. This classifier takes an online post as the input and outputs multiple labels, each reflecting a possible privacy disclosure. The proposed presentation method combines three different sources of information, the inputtextitself, the label-to-textcorrelation and the label-to-label correlation. A double-attention mechanism is used to combine the first two sources of information, and a graph convolutional network (GCN) is employed to extract the third source of information that is then used to help fuse features extracted from the first two sources of information. Our extensive experimental results, obtained on a public dataset of privacy-disclosing posts on Twitter, demonstrated that our proposed privacy disclosure detection method significantly and consistently outperformed other state-of-the-art methods in terms of all key performance indicators.","['Zhanbo Liang', 'Jie Guo', 'Weidong Qiu', 'Zheng Huang', 'Shujun Li']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.15917,Anomali
"One-bit Supervision for Image Classification: Problem, Solution, and Beyond","This paper presents one-bit supervision, a novel setting of learning with fewer labels, for image classification. Instead of training model using the accurate label of each sample, our setting requires the model to interact with the system by predicting the class label of each sample and learn from the answer whether the guess is correct, which provides one bit (yes or no) of information. An intriguing property of the setting is that the burden of annotation largely alleviates in comparison to offering the accurate label. There are two keys to one-bit supervision, which are (i) improving the guess accuracy and (ii) making good use of the incorrect guesses. To achieve these goals, we propose a multi-stage training paradigm and incorporate negative label suppression into an off-the-shelf semi-supervised learning algorithm. Theoretical analysis shows that one-bit annotation is more efficient than full-bit annotation in most cases and gives the conditions of combining our approach with active learning. Inspired by this, we further integrate the one-bit supervision framework into the self-supervised learning algorithm which yields an even more efficient training schedule. Different from training from scratch, when self-supervised learning is used for initialization, both hard exampleminingand class balance are verified effective in boosting the learning performance. However, these two frameworks still need full-bit labels in the initial stage. To cast off this burden, we utilize unsupervised domain adaptation to train the initial model and conduct pure one-bit annotations on the target dataset. In multiple benchmarks, the learning efficiency of the proposed approach surpasses that using full-bit, semi-supervised supervision.","['Hengtong Hu', 'Lingxi Xie', 'Xinyue Hue', 'Richang Hong', 'Qi Tian']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.15225,Anomali
Estimation of the User Contribution Rate by Leveraging Time Sequence in Pairwise Matching function-point between Users Feedback and App Updating Log,"Mobile applications have become an inseparable part of people's daily life. Nonetheless, the market competition is extremely fierce, and apps lacking recognition among most users are susceptible to market elimination. To this end, developers must swiftly and accurately apprehend the requirements of the wider user base to effectively strategize and promote their apps' orderly and healthy evolution. The rate at which general user requirements are adopted by developers, or user contribution, is a very valuable metric that can be an important tool for app developers or software engineering researchers to measure or gain insight into the evolution of app requirements and predict the evolution of app software. Regrettably, the landscape lacks refined quantitative analysis approaches and tools for this pivotal indicator. To address this problem, this paper exploratively proposes a quantitative analysis approach based on the temporal correlation perception that exists in the app update log and user reviews, which provides a feasible solution for quantitatively obtaining the user contribution. The main idea of this scheme is to consider valid user reviews as user requirements and app update logs as developer responses, and tomineand analyze the pairwise and chronological relationships existing between the two bytextcomputing, thus constructing a feasible approach for quantitatively calculating user contribution. To demonstrate the feasibility of the approach, this paper collects data from four Chinese apps in the App Store in mainland China and one English app in the U.S. region, including 2,178 update logs and 4,236,417 user reviews, and from the results of the experiment, it was found that 16.6%-43.2% of the feature of these apps would be related to the drive from the online popular user requirements.","['Shiqi Duan', 'Jianxun Liu', 'Yong Xiao', 'Xiangping Zhang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.15179,Anomali
Uncovering Gender Stereotypes in Video Game Character Designs: A Multi-Modal Analysis of Honor of Kings,"In this paper, we conduct a comprehensive analysis of gender stereotypes in the character design of Honor of Kings, a popular multiplayer online battle arena (MOBA) game in China. We probe gender stereotypes through the lens of role assignments, visual designs, spoken lines, and background stories, combining qualitative analysis andtextminingbased on the moral foundation theory. Male heroes are commonly designed as masculine fighters with power and female heroes as feminine ""ornaments"" with ideal looks. We contribute with a culture-aware and multi-modal understanding of gender stereotypes in games, leveragingtext-, visual-, and role-based evidence.","['Bingqing Liu', 'Kyrie Zhixuan Zhou', 'Danlei Zhu', 'Jaihyun Park']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.14226,Anomali
Analyzing the Evolution and Maintenance of ML Models on Hugging Face,"Hugging Face (HF) has established itself as a crucial platform for the development and sharing of machine learning (ML) models. This repositoryminingstudy, which delves into more than 380,000 models using data gathered via the HF Hub API, aims to explore the community engagement, evolution, and maintenance around models hosted on HF, aspects that have yet to be comprehensively explored in the literature. We first examine the overall growth and popularity of HF, uncovering trends in ML domains, framework usage, authors grouping and the evolution of tags and datasets used. Throughtextanalysis of model card descriptions, we also seek to identify prevalent themes and insights within the developer community. Our investigation further extends to the maintenance aspects of models, where we evaluate the maintenance status of ML models, classify commit messages into various categories (corrective, perfective, and adaptive), analyze the evolution across development stages of commits metrics and introduce a new classification system that estimates the maintenance status of models based on multiple attributes. This study aims to provide valuable insights about ML model maintenance and evolution that could inform future model development strategies on platforms like HF.","['Joel Castaño', 'Silverio Martínez-Fernández', 'Xavier Franch', 'Justus Bogner']",,arXiv,2024,https://doi.org/10.48550/arXiv.2311.13380,Anomali
MatGD: Materials Graph Digitizer,"We have developed MatGD (Material Graph Digitizer), which is a tool for digitizing a data line from scientific graphs. The algorithm behind the tool consists of four steps: (1) identifying graphs within subfigures, (2) separating axes and data sections, (3) discerning the data lines by eliminating irrelevant graph objects and matching with the legend, and (4) data extraction and saving. From the 62,534 papers in the areas of batteries, catalysis, and MOFs, 501,045 figures weremined. Remarkably, our tool showcased performance with over 99% accuracy in legend marker andtextdetection. Moreover, its capability for data line separation stood at 66%, which is much higher compared to other existing figureminingtools. We believe that this tool will be integral to collecting both past and future data from publications, and these data can be used to train various machine learning models that can enhance material predictions and new materials discovery.","['Jaewoong Lee', 'Wonseok Lee', 'Jihan Kim']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.12806,Anomali
Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks,"Objective: Most existing fine-tuned biomedical large language models (LLMs) focus on enhancing performance in monolingual biomedical question answering and conversation tasks. To investigate the effectiveness of the fine-tuned LLMs on diverse biomedical NLP tasks in different languages, We present Taiyi, a bilingual fine-tuned LLM for diverse biomedical tasks. Materials and Methods: We first curated a comprehensive collection of 140 existing biomedicaltextminingdatasets (102 English and 38 Chinese datasets) across over 10 task types. Subsequently, a two-stage strategy is proposed for supervised fine-tuning to optimize the model performance across varied tasks. Results: Experimental results on 13 test sets covering named entity recognition, relation extraction,textclassification, question answering tasks demonstrate that Taiyi achieves superior performance compared to general LLMs. The case study involving additional biomedical NLP tasks further shows Taiyi's considerable potential for bilingual biomedical multi-tasking. Conclusion: Leveraging rich high-quality biomedical corpora and developing effective fine-tuning strategies can significantly improve the performance of LLMs within the biomedical domain. Taiyi shows the bilingual multi-tasking capability through supervised fine-tuning. However, those tasks such as information extraction that are not generation tasks in nature remain challenging for LLM-based generative approaches, and they still underperform the conventional discriminative approaches of smaller language models.","['Ling Luo', 'Jinzhong Ning', 'Yingwen Zhao', 'Zhijun Wang', 'Zeyuan Ding', 'Peng Chen', 'Weiru Fu', 'Qinyu Han', 'Guangtao Xu', 'Yunzhi Qiu', 'Dinghao Pan', 'Jiru Li', 'Hao Li', 'Wenduo Feng', 'Senbo Tu', 'Yuqi Liu', 'Zhihao Yang', 'Jian Wang', 'Yuanyuan Sun', 'Hongfei Lin']","Journal of the American Medical Informatics Association, 2024, ocae037",arXiv,2023,https://doi.org/10.48550/arXiv.2311.11608,Anomali
Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India,"In March 2020, the World Health Organisation declared COVID-19 a global pandemic as it spread to nearly every country. By mid-2021, India had introduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure successful vaccination in a densely populated country like India, understanding public sentiment was crucial. Social media, particularly Reddit with over 430 million users, played a vital role in disseminating information. This study employs dataminingtechniques to analyze Reddit data and gauge Indian sentiments towards COVID-19 vaccines. Using Python'sTextBlob library, comments are annotated to assess general sentiments. Results show that most Reddit users in India expressed neutrality about vaccination, posing a challenge for the Indian government's efforts to vaccinate a significant portion of the population.","['Milind Gupta', 'Abhishek Kaushik']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.11435,Anomali
"A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications","Sentiment analysis (SA) is an emerging field intextmining. It is the process of computationally identifying and categorizing opinions expressed in a piece oftextover different social media platforms. Social media plays an essential role in knowing the customer mindset towards a product, services, and the latest market trends. Most organizations depend on the customer's response and feedback to upgrade their offered products and services. SA or opinionminingseems to be a promising research area for various domains. It plays a vital role in analyzing big data generated daily in structured and unstructured formats over the internet. This survey paper defines sentiment and its recent research and development in different domains, including voice, images, videos, andtext. The challenges and opportunities of sentiment analysis are also discussed in the paper.
  \keywords{Sentiment Analysis, Machine Learning, Lexicon-based approach, Deep Learning, Natural Language Processing}","['Sudhanshu Kumar', 'Partha Pratim Roy', 'Debi Prosad Dogra', 'Byung-Gyu Kim']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.11250,Anomali
Extracting periodontitis diagnosis in clinical notes with RoBERTa and regular expression,"This study aimed to utilizetextprocessing and natural language processing (NLP) models tomineclinical notes for the diagnosis of periodontitis and to evaluate the performance of a named entity recognition (NER) model on different regular expression (RE) methods. Two complexity levels of RE methods were used to extract and generate the training data. The SpaCy package and RoBERTa transformer models were used to build the NER model and evaluate its performance with the manual-labeled gold standards. The comparison of the RE methods with the gold standard showed that as the complexity increased in the RE algorithms, the F1 score increased from 0.3-0.4 to around 0.9. The NER models demonstrated excellent predictions, with the simple RE method showing 0.84-0.92 in the evaluation metrics, and the advanced and combined RE method demonstrating 0.95-0.99 in the evaluation. This study provided an example of the benefit of combining NER methods and NLP models in extracting target information from free-textto structured data and fulfilling the need for missing diagnoses from unstructured notes.","['Yao-Shun Chuang', 'Chun-Teh Lee', 'Ryan Brandon', 'Trung Duong Tran', 'Oluwabunmi Tokede', 'Muhammad F. Walji', 'Xiaoqian Jiang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.10809,Anomali
"A Systematic Review of Aspect-based Sentiment Analysis: Domains, Methods, and Trends","Aspect-based sentiment analysis (ABSA) is a fine-grained type of sentiment analysis that identifies aspects and their associated opinions from a giventext. With the surge of digital opinionatedtextdata, ABSA gained increasing popularity for its ability tominemore detailed and targeted insights. Many review papers on ABSA subtasks and solution methodologies exist, however, few focus on trends over time or systemic issues relating to research application domains, datasets, and solution approaches. To fill the gap, this paper presents a systematic literature review (SLR) of ABSA studies with a focus on trends and high-level relationships among these fundamental components. This review is one of the largest SLRs on ABSA. To our knowledge, it is also the first to systematically examine the interrelations among ABSA research and data distribution across domains, as well as trends in solution paradigms and approaches. Our sample includes 727 primary studies screened from 8550 search results without time constraints via an innovative automatic filtering process. Our quantitative analysis not only identifies trends in nearly two decades of ABSA research development but also unveils a systemic lack of dataset and domain diversity as well as domain mismatch that may hinder the development of future ABSA research. We discuss these findings and their implications and propose suggestions for future research.","['Yan Cathy Hua', 'Paul Denny', 'Katerina Taskova', 'Jörg Wicker']","Artif Intell Rev 57, 296 (2024)",arXiv,2024,https://doi.org/10.48550/arXiv.2311.10777,Anomali
A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest,"Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend theminedQA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specifictextcorpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.","['Ruohong Zhang', 'Luyu Gao', 'Chen Zheng', 'Zhen Fan', 'Guokun Lai', 'Zheng Zhang', 'Fangzhou Ai', 'Yiming Yang', 'Hongxia Yang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.10614,Anomali
Polygonal Cone Control Barrier Functions (PolyC2BF) for safe navigation in cluttered environments,"In fields such asmining, search and rescue, and archaeological exploration, ensuring real-time, collision-free navigation of robots in confined, cluttered environments is imperative. Despite the value of established path planning algorithms, they often face challenges in convergence rates and handling dynamic infeasibilities. Alternative techniques like collision cones struggle to accurately represent complex obstacle geometries. This paper introduces a novel category of control barrier functions, known as Polygonal Cone Control Barrier Function (PolyC2BF), which addresses overestimation and computational complexity issues. The proposed PolyC2BF, formulated as a Quadratic Programming (QP) problem, proves effective in facilitating collision-free movement of multiple robots in complex environments. The efficacy of this approach is further demonstrated through PyBullet simulations on quadruped (unicycle model), and crazyflie 2.1 (quadrotor model) in cluttered environments.","['Manan Tayal', 'Shishir Kolathaya']",,arXiv,2024,https://doi.org/10.48550/arXiv.2311.08787,Anomali
Insights into Classifying and Mitigating LLMs' Hallucinations,"The widespread adoption of large language models (LLMs) across diverse AI applications is proof of the outstanding achievements obtained in several tasks, such astextmining,textgeneration, and question answering. However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as ""Hallucinations"". They manifest intextgeneration systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation. This paper delves into the underlying causes of AI hallucination and elucidates its significance in artificial intelligence. In particular, Hallucination classification is tackled over several tasks (Machine Translation, Question and Answer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and Visual Question Answer). Additionally, we explore potential strategies to mitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our research addresses this critical issue within the HeReFaNMi (Health-Related Fake News Mitigation) project, generously supported by NGI Search, dedicated to combating Health-Related Fake News dissemination on the Internet. This endeavour represents a concerted effort to safeguard the integrity of information dissemination in an age of evolving AI technologies.","['Alessandro Bruno', 'Pier Luigi Mazzeo', 'Aladine Chetouani', 'Marouane Tliba', 'Mohamed Amine Kerkouri']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.08117,Anomali
Data and models for stance and premise detection in COVID-19 tweets: insights from the Social Media Mining for Health (SMM4H) 2022 shared task,"The COVID-19 pandemic has sparked numerous discussions on social media platforms, with users sharing their views on topics such as mask-wearing and vaccination. To facilitate the evaluation of neural models for stance detection and premise classification, we organized the Social MediaMiningfor Health (SMM4H) 2022 Shared Task 2. This competition utilized manually annotated posts on three COVID-19-related topics: school closures, stay-at-home orders, and wearing masks. In this paper, we extend the previous work and present newly collected data on vaccination from Twitter to assess the performance of models on a different topic. To enhance the accuracy and effectiveness of our evaluation, we employed various strategies to aggregate tweettextswith claims, including models with feature-level (early) fusion and dual-view architectures from SMM4H 2022 leaderboard. Our primary objective was to create a valuable dataset and perform an extensive experimental evaluation to support future research in argumentminingin the health domain.","['Vera Davydova', 'Huabin Yang', 'Elena Tutubalina']","Journal of Biomedical Informatics, 2023",arXiv,2023,https://doi.org/10.48550/arXiv.2311.08057,Anomali
Pretrain like Your Inference: Masked Tuning Improves Zero-Shot Composed Image Retrieval,"Zero-shot composed image retrieval (ZS-CIR), which takes a textual modification and a reference image as a query to retrieve a target image without triplet labeling, has gained more and more attention in datamining. Current ZS-CIR research mainly relies on the generalization ability of pre-trained vision-language models, e.g., CLIP. However, the pre-trained vision-language models and CIR tasks have substantial discrepancies, where the vision-language models focus on learning the similarities but CIR aims to learn the modifications of the image guided bytext. In this paper, we introduce a novel unlabeled and pre-trained masked tuning approach, which reduces the gap between the pre-trained vision-language model and the downstream CIR task. First, to reduce the gap, we reformulate the contrastive learning of the vision-language model as the CIR task, where we randomly mask input image patches to generate $\langle$masked image,text, image$\rangle$ triplet from an image-textpair. Then, we propose a simple but novel pre-trained masked tuning method, which uses thetextand the masked image to learn the modifications of the original image. With such a simple design, the proposed masked tuning can learn to better capture fine-grainedtext-guided modifications. Extensive experimental results demonstrate the significant superiority of our approach over the baseline models on four ZS-CIR datasets, including FashionIQ, CIRR, CIRCO, and GeneCIS. Our codes are available at https://github.com/Chen-Junyang-cn/PLI","['Junyang Chen', 'Hanjiang Lai']",,arXiv,2025,https://doi.org/10.48550/arXiv.2311.07622,Anomali
BIDRN: A Method of Bidirectional Recurrent Neural Network for Sentiment Analysis,"Textminingresearch has grown in importance in recent years due to the tremendous increase in the volume of unstructured textual data. This has resulted in immense potential as well as obstacles in the sector, which may be efficiently addressed with adequate analytical and study methods. Deep Bidirectional Recurrent Neural Networks are used in this study to analyze sentiment. The method is categorized as sentiment polarity analysis because it may generate a dataset with sentiment labels. This dataset can be used to train and evaluate sentiment analysis models capable of extracting impartial opinions. This paper describes the Sentiment Analysis-Deep Bidirectional Recurrent Neural Networks (SA-BDRNN) Scheme, which seeks to overcome the challenges and maximize the potential oftextminingin the context of Big Data. The current study proposes a SA-DBRNN Scheme that attempts to give a systematic framework for sentiment analysis in the context of student input on institution choice. The purpose of this study is to compare the effectiveness of the proposed SA- DBRNN Scheme to existing frameworks to establish a robust deep neural network that might serve as an adequate classification model in the field of sentiment analysis.","['D Muthusankar', 'P Kaladevi', 'V R Sadasivam', 'R Praveen']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.07296,Anomali
MatNexus: A Comprehensive Text Mining and Analysis Suite for Materials Discover,"MatNexus is a specialized software for the automated collection, processing, and analysis oftextfrom scientific articles. Through an integrated suite of modules, the MatNexus facilitates the retrieval of scientific articles, processes textual data for insights, generates vector representations suitable for machine learning, and offers visualization capabilities for word embeddings. With the vast volume of scientific publications, MatNexus stands out as an end-to-end tool for researchers aiming to gain insights from scientific literature in material science, making the exploration of materials, such as the electrocatalyst examples we show here, efficient and insightful.","['Lei Zhang', 'Markus Stricker']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.06303,Anomali
Active Mining Sample Pair Semantics for Image-text Matching,"Recently, commonsense learning has been a hot topic in image-textmatching. Although it can describe more graphic correlations, commonsense learning still has some shortcomings: 1) The existing methods are based on triplet semantic similarity measurement loss, which cannot effectively match the intractable negative in image-textsample pairs. 2) The weak generalization ability of the model leads to the poor effect of image andtextmatching on large-scale datasets. According to these shortcomings. This paper proposes a novel image-textmatching model, called ActiveMiningSample Pair Semantics image-textmatching model (AMSPS). Compared with the single semantic learning mode of the commonsense learning model with triplet loss function, AMSPS is an active learning idea. Firstly, the proposed Adaptive Hierarchical Reinforcement Loss (AHRL) has diversified learning modes. Its active learning mode enables the model to more focus on the intractable negative samples to enhance the discriminating ability. In addition, AMSPS can also adaptivelyminemore hidden relevant semantic representations from uncommented items, which greatly improves the performance and generalization ability of the model. Experimental results on Flickr30K and MSCOCO universal datasets show that our proposed method is superior to advanced comparison methods.","['Yongfeng Chena', 'Jin Liua', 'Zhijing Yang', 'Ruihan Chena', 'Junpeng Tan']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.05425,Anomali
Automated Annotation of Scientific Texts for ML-based Keyphrase Extraction and Validation,"Advanced omics technologies and facilities generate a wealth of valuable data daily; however, the data often lacks the essential metadata required for researchers to find and search them effectively. The lack of metadata poses a significant challenge in the utilization of these datasets. Machine learning-based metadata extraction techniques have emerged as a potentially viable approach to automatically annotating scientific datasets with the metadata necessary for enabling effective search.Textlabeling, usually performed manually, plays a crucial role in validating machine-extracted metadata. However, manual labeling is time-consuming; thus, there is an need to develop automatedtextlabeling techniques in order to accelerate the process of scientific innovation. This need is particularly urgent in fields such as environmental genomics and microbiome science, which have historically received less attention in terms of metadata curation and creation of gold-standardtextminingdatasets.
  In this paper, we present two novel automatedtextlabeling approaches for the validation of ML-generated metadata for unlabeledtexts, with specific applications in environmental genomics. Our techniques show the potential of two new ways to leverage existing information about the unlabeledtextsand the scientific domain. The first technique exploits relationships between different types of data sources related to the same research study, such as publications and proposals. The second technique takes advantage of domain-specific controlled vocabularies or ontologies. In this paper, we detail applying these approaches for ML-generated metadata validation. Our results show that the proposed label assignment approaches can generate both generic and highly-specifictextlabels for the unlabeledtexts, with up to 44% of the labels matching with those suggested by a ML keyword extraction algorithm.","['Oluwamayowa O. Amusat', 'Harshad Hegde', 'Christopher J. Mungall', 'Anna Giannakou', 'Neil P. Byers', 'Dan Gunter', 'Kjiersten Fagnan', 'Lavanya Ramakrishnan']","Database (2024), Vol. 2024: article ID baae093, 1-18",arXiv,2023,https://doi.org/10.48550/arXiv.2311.05042,Anomali
In nomine patris... Elements for a semantics of medieval paternity,"This article examines medieval concepts of paternity and father-son relationships through the digital analysis of medieval textual corpora. Although historians have access to enormous digital collections in 2023, they have rarely fully exploited these resources. The author proposes a historical semantic approach to this theme, using modeling tools andtextminingin general, to analyze the evolution of terms related to paternity. The study proposes three conclusions: 1. a semantic break occurred in the semantic field of paternity at the turn of Antiquity and the Early Middle Ages. The meaning of pater and its derivatives changed radically over the course of the 4th-6th centuries, particularly as a result of the influence of the dogma of the Christian Trinity. Medieval fatherhood was multidimensional, encompassing both biological and spiritual aspects, in other words, complex relationships between multiple carnal and spiritual (i.e. divine) fathers. 2. The role of spiritual kinship is crucial to understanding medieval fatherhood, as the work of Anita Guerreau-Jalabert and J{é}r{ô}me Baschet has already shown. Initially attributed to God, this ''ideal paternity'' (paternitas) gradually extended to members of the Church (popes, bishops, abbots), underlining at the same time the growing importance of spiritual kinship over biological kinship over the centuries studied. 3. To reveal these structures, invisible to the naked eye, an interdisciplinary approach is rigorously required. Complementary investigations into the lemmas mater, filia, frater and other family terms are required. The use of digital tools and historical semantic analysis opens up new perspectives for researchers in history, anthropology, linguistics and datamining, enabling them to explore the representation systems of ancient societies in depth and with nuance.",['Nicolas Perreaux'],"L{é}o Dumont, Octave Julien et St{é}phane Lamass{é} (dir.), Histoire de mots. Saisir le pass{é} gr{â}ce aux donn{é}es textuelles, {É}ditions de la Sorbonne, Paris, 2023, {É}ditions de la Sorbonne, A para{î}tre",arXiv,2023,https://doi.org/10.48550/arXiv.2311.04907,Anomali
Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining,"Contemporary large-scale visual language models (VLMs) exhibit strong representation capacities, making them ubiquitous for enhancing image andtextunderstanding tasks. They are often trained in a contrastive manner on a large and diverse corpus of images and correspondingtextcaptions scraped from the internet. Despite this, VLMs often struggle with compositional reasoning tasks which require a fine-grained understanding of the complex interactions of objects and their attributes. This failure can be attributed to two main factors: 1) Contrastive approaches have traditionally focused onminingnegative examples from existing datasets. However, theminednegative examples might not be difficult for the model to discriminate from the positive. An alternative tominingwould be negative sample generation 2) But existing generative approaches primarily focus on generating hard negativetextsassociated with a given image.Miningin the other direction, i.e., generating negative image samples associated with a giventexthas been ignored. To overcome both these limitations, we propose a framework that not onlyminesin both directions but also generates challenging negative samples in both modalities, i.e., images andtexts. Leveraging these generative hard negative samples, we significantly enhance VLMs' performance in tasks involving multimodal compositional reasoning. Our code and dataset are released at https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html.","['Ugur Sahin', 'Hang Li', 'Qadeer Khan', 'Daniel Cremers', 'Volker Tresp']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.03964,Anomali
Bilingual Corpus Mining and Multistage Fine-Tuning for Improving Machine Translation of Lecture Transcripts,"Lecture transcript translation helps learners understand online courses, however, building a high-quality lecture machine translation system lacks publicly available parallel corpora. To address this, we examine a framework for parallel corpusmining, which provides a quick and effective way tominea parallel corpus from publicly available lectures on Coursera. To create the parallel corpora, we propose a dynamic programming based sentence alignment algorithm which leverages the cosine similarity of machine-translated sentences. The sentence alignment F1 score reaches 96%, which is higher than using the BERTScore, LASER, or sentBERT methods. For both English--Japanese and English--Chinese lecture translations, we extracted parallel corpora of approximately 50,000 lines and created development and test sets through manual filtering for benchmarking translation performance. Through machine translation experiments, we show that theminedcorpora enhance the quality of lecture transcript translation when used in conjunction with out-of-domain parallel corpora via multistage fine-tuning. Furthermore, this study also suggests guidelines for gathering and cleaning corpora,miningparallel sentences, cleaning noise in themineddata, and creating high-quality evaluation splits. For the sake of reproducibility, we have released the corpora as well as the code to create them. The dataset is available at https://github.com/shyyhs/CourseraParallelCorpusMining.","['Haiyue Song', 'Raj Dabre', 'Chenhui Chu', 'Atsushi Fujita', 'Sadao Kurohashi']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.03696,Anomali
Text Augmentations with R-drop for Classification of Tweets Self Reporting Covid-19,"This paper presents models created for the Social MediaMiningfor Health 2023 shared task. Our team addressed the first task, classifying tweets that self-report Covid-19 diagnosis. Our approach involves a classification model that incorporates diverse textual augmentations and utilizes R-drop to augment data and mitigate overfitting, boosting model efficacy. Our leading model, enhanced with R-drop and augmentations like synonym substitution, reserved words, and back translations, outperforms the task mean and median scores. Our system achieves an impressive F1 score of 0.877 on the test set.","['Sumam Francis', 'Marie-Francine Moens']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.03420,Anomali
DAIL: Data Augmentation for In-Context Learning via Self-Paraphrase,"In-Context Learning (ICL) combined with pre-trained large language models has achieved promising results on various NLP tasks. However, ICL requires high-quality annotated demonstrations which might not be available in real-world scenarios. To overcome this limitation, we propose \textbf{D}ata \textbf{A}ugmentation for \textbf{I}n-Context \textbf{L}earning (\textbf{DAIL}). DAIL leverages the intuition that large language models are more familiar with the content generated by themselves. It first utilizes the language model to generate paraphrases of the test sample and employs majority voting to determine the final result based on individual predictions. Our extensive empirical evaluation shows that DAIL outperforms the standard ICL method and other ensemble-based methods in the low-resource scenario. Additionally, we explore the use of voting consistency as a confidence score of the model when the logits of predictions are inaccessible. We believe our work will stimulate further research on ICL in low-resource settings.","['Dawei Li', 'Yaxuan Li', 'Dheeraj Mekala', 'Shuyao Li', 'Yulin wang', 'Xueqi Wang', 'William Hogan', 'Jingbo Shang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2311.03319,Anomali
Content Significance Distribution of Sub-Text Blocks in Articles and Its Application to Article-Organization Assessment,"We explore how to capture the significance of a sub-textblock in an article and how it may be used fortextminingtasks. A sub-textblock is a sub-sequence of sentences in the article. We formulate the notion of content significance distribution (CSD) of sub-textblocks, referred to as CSD of the first kind and denoted by CSD-1. In particular, we leverage Hugging Face's SentenceTransformer to generate contextual sentence embeddings, and use MoverScore overtextembeddings to measure how similar a sub-textblock is to the entiretext. To overcome the exponential blowup on the number of sub-textblocks, we present an approximation algorithm and show that the approximated CSD-1 is almost identical to the exact CSD-1. Under this approximation, we show that the average and median CSD-1's for news, scholarly research, argument, and narrative articles share the same pattern. We also show that under a certain linear transformation, the complement of the cumulative distribution function of the beta distribution with certain values of $α$ and $β$ resembles a CSD-1 curve. We then use CSD-1's to extract linguistic features to train an SVC classifier for assessing how well an article is organized. Through experiments, we show that this method achieves high accuracy for assessing student essays. Moreover, we study CSD of sentence locations, referred to as CSD of the second kind and denoted by CSD-2, and show that average CSD-2's for different types of articles possess distinctive patterns, which either conform common perceptions of article structures or provide rectification with minor deviation.","['You Zhou', 'Jie Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2311.01673,Anomali
An energy-based comparative analysis of common approaches to text classification in the Legal domain,"Most Machine Learning research evaluates the best solutions in terms of performance. However, in the race for the best performing model, many important aspects are often overlooked when, on the contrary, they should be carefully considered. In fact, sometimes the gaps in performance between different approaches are neglectable, whereas factors such as production costs, energy consumption, and carbon footprint must take into consideration. Large Language Models (LLMs) are extensively adopted to address NLP problems in academia and industry. In this work, we present a detailed quantitative comparison of LLM and traditional approaches (e.g. SVM) on the LexGLUE benchmark, which takes into account both performance (standard indices) and alternative metrics such as timing, power consumption and cost, in a word: the carbon-footprint. In our analysis, we considered the prototyping phase (model selection by training-validation-test iterations) and in-production phases separately, since they follow different implementation procedures and also require different resources. The results indicate that very often, the simplest algorithms achieve performance very close to that of large LLMs but with very low power consumption and lower resource demands. The results obtained could suggest companies to include additional evaluations in the choice of Machine Learning (ML) solutions.","['Sinan Gultekin', 'Achille Globo', 'Andrea Zugarini', 'Marco Ernandes', 'Leonardo Rigutini']","Computer Science & Information Technology (CS & IT) ISSN 2231-5403 Volume 14, Number 02, January 2024",arXiv,2024,https://doi.org/10.48550/arXiv.2311.01256,Anomali
Floquet engineering with spatially non-uniform driving fields,"In Floquet engineering, we apply a time-periodic modulation to change the effective behavior of a wave system. In this work, we generalize Floquet engineering to exploit spatial degrees of freedom, expanding the scope of effective behaviors we can access. We develop a perturbative procedure to engineer space-time dependent driving forces that effectively transform broad classes of tight-binding systems into one another. We demonstrate several applications, including removing disorder, undoing Anderson localization, and enhancing localization to an extreme in spatially modulated waveguides. This procedure straightforwardly extends to other types of physical systems and different Floquet driving field implementations.","['Stella T. Schindler', 'Hanan Herzig Sheinfux']",MIT-CTP 5593,arXiv,2025,https://doi.org/10.48550/arXiv.2311.00845,Anomali
A Few-Shot Learning Focused Survey on Recent Named Entity Recognition and Relation Classification Methods,"Named Entity Recognition (NER) and Relation Classification (RC) are important steps in extracting information from unstructuredtextand formatting it into a machine-readable format. We present a survey of recent deep learning models that address named entity recognition and relation classification, with focus on few-shot learning performance. Our survey is helpful for researchers in knowing the recent techniques intextminingand extracting structured information from rawtext.","['Sakher Khalil Alqaaidi', 'Elika Bozorgi', 'Afsaneh Shams', 'Krzysztof Kochut']",,arXiv,2024,https://doi.org/10.48550/arXiv.2310.19055,Anomali
Sentence Bag Graph Formulation for Biomedical Distant Supervision Relation Extraction,"We introduce a novel graph-based framework for alleviating key challenges in distantly-supervised relation extraction and demonstrate its effectiveness in the challenging and important domain of biomedical data. Specifically, we propose a graph view of sentence bags referring to an entity pair, which enables message-passing based aggregation of information related to the entity pair over the sentence bag. The proposed framework alleviates the common problem of noisy labeling in distantly supervised relation extraction and also effectively incorporates inter-dependencies between sentences within a bag. Extensive experiments on two large-scale biomedical relation datasets and the widely utilized NYT dataset demonstrate that our proposed framework significantly outperforms the state-of-the-art methods for biomedical distant supervision relation extraction while also providing excellent performance for relation extraction in the generaltextminingdomain.","['Hao Zhang', 'Yang Liu', 'Xiaoyan Liu', 'Tianming Liang', 'Gaurav Sharma', 'Liang Xue', 'Maozu Guo']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.18912,Anomali
Text Augmented Spatial-aware Zero-shot Referring Image Segmentation,"In this paper, we study a challenging task of zero-shot referring image segmentation. This task aims to identify the instance mask that is most related to a referring expression without training on pixel-level annotations. Previous research takes advantage of pre-trained cross-modal models, e.g., CLIP, to align instance-level masks with referring expressions. %Yet, CLIP only considers image-textpair level alignment, which neglects fine-grained image region and complex sentence matching. Yet, CLIP only considers the global-level alignment of image-textpairs, neglecting fine-grained matching between the referring sentence and local image regions. To address this challenge, we introduce aTextAugmented Spatial-aware (TAS) zero-shot referring image segmentation framework that is training-free and robust to various visual encoders. TAS incorporates a mask proposal network for instance-level mask extraction, atext-augmented visual-textmatching score forminingthe image-textcorrelation, and a spatial rectifier for mask post-processing. Notably, thetext-augmented visual-textmatching score leverages a $P$ score and an $N$-score in addition to the typical visual-textmatching score. The $P$-score is utilized to close the visual-textdomain gap through a surrogate captioning model, where the score is computed between the surrogate model-generatedtextsand the referring expression. The $N$-score considers the fine-grained alignment of region-textpairs via negative phrasemining, encouraging the masked image to be repelled from themineddistracting phrases. Extensive experiments are conducted on various datasets, including RefCOCO, RefCOCO+, and RefCOCOg. The proposed method clearly outperforms state-of-the-art zero-shot referring image segmentation methods.","['Yucheng Suo', 'Linchao Zhu', 'Yi Yang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.18049,Anomali
M2C: Towards Automatic Multimodal Manga Complement,"Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages,textcontamination, and aging, resulting in missing comictextcontent and seriously hindering human comprehension. In other words, the Multimodal Manga Complement (M2C) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for vision and language understanding. To this end, we first propose the Multimodal Manga Complement task by establishing a new M2C benchmark dataset covering two languages. First, we design a manga argumentation method called MCoT tomineevent knowledge in comics with large language models. Then, an effective baseline FVP-M$^{2}$ using fine-grained visual prompts is proposed to support manga complement. Extensive experimental results show the effectiveness of FVP-M$^{2}$ method for Multimodal Mange Complement.","['Hongcheng Guo', 'Boyang Wang', 'Jiaqi Bai', 'Jiaheng Liu', 'Jian Yang', 'Zhoujun Li']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.17130,Anomali
Test-time Augmentation for Factual Probing,"Factual probing is a method that uses prompts to test if a language model ""knows"" certain world knowledge facts. A problem in factual probing is that small changes to the prompt can lead to large changes in model output. Previous work aimed to alleviate this problem by optimizing prompts viatextminingor fine-tuning. However, such approaches are relation-specific and do not generalize to unseen relation types. Here, we propose to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time. Experiments show improved model calibration, i.e., with TTA, model confidence better reflects prediction accuracy. Improvements in prediction accuracy are observed for some models, but for other models, TTA leads to degradation. Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA.","['Go Kamoda', 'Benjamin Heinzerling', 'Keisuke Sakaguchi', 'Kentaro Inui']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.17121,Anomali
Efficient Online String Matching through Linked Weak Factors,"Online string matching is a computational problem involving the search for patterns or substrings in a largetextdataset, with the pattern andtextbeing processed sequentially, without prior access to the entiretext. Its relevance stems from applications in data compression, datamining,textediting, and bioinformatics, where rapid and efficient pattern matching is crucial. Various solutions have been proposed over the past few decades, employing diverse techniques. Recently, weak recognition approaches have attracted increasing attention. This paper presents Hash Chain, a new algorithm based on a robust weak factor recognition approach that connects adjacent factors through hashing. Despite its O(nm) complexity, the algorithm exhibits a sublinear behavior in practice and achieves superior performance compared to the most effective algorithms.","['Matthew N. Palmer', 'Simone Faro', 'Stefano Scafiti']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.15711,Anomali
Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls,"Building and analysing knowledge graphs (KGs) to aid drug discovery is a topical area of research. A salient feature of KGs is their ability to combine many heterogeneous data sources in a format that facilitates discovering connections. The utility of KGs has been exemplified in areas such as drug repurposing, with insights made through manual exploration and modelling of the data. In this article, we discuss promises and pitfalls of using natural language processing (NLP) tomineunstructuredtexttypically from scientific literature as a data source for KGs. This draws on our experience of initially parsing structured data sources such as ChEMBL as the basis for data within a KG, and then enriching or expanding upon them using NLP. The fundamental promise of NLP for KGs is the automated extraction of data from millions of documents a task practically impossible to do via human curation alone. However, there are many potential pitfalls in NLP-KG pipelines such as incorrect named entity recognition and ontology linking all of which could ultimately lead to erroneous inferences and conclusions.","['J. Charles G. Jeynes', 'Tim James', 'Matthew Corney']",Methods Mol Biol . 2024:2716:223-240,arXiv,2023,https://doi.org/10.48550/arXiv.2310.15572,Anomali
"Let the Pretrained Language Models ""Imagine"" for Short Texts Topic Modeling","Topic models are one of the compelling methods for discovering latent semantics in a document collection. However, it assumes that a document has sufficient co-occurrence information to be effective. However, in shorttexts, co-occurrence information is minimal, which results in feature sparsity in document representation. Therefore, existing topic models (probabilistic or neural) mostly fail tominepatterns from them to generate coherent topics. In this paper, we take a new approach to short-texttopic modeling to address the data-sparsity issue by extending shorttextinto longer sequences using existing pre-trained language models (PLMs). Besides, we provide a simple solution extending a neural topic model to reduce the effect of noisy out-of-topicstextgeneration from PLMs. We observe that our model can substantially improve the performance of short-texttopic modeling. Extensive experiments on multiple real-world datasets under extreme data sparsity scenarios show that our models can generate high-quality topics outperforming state-of-the-art models.","['Pritom Saha Akash', 'Jie Huang', 'Kevin Chen-Chuan Chang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.15420,Anomali
Unraveling the Skillsets of Data Scientists: Text Mining Analysis of Dutch University Master Programs in Data Science and Artificial Intelligence,"The growing demand for data scientists in the global labor market and the Netherlands has led to a rise in data science and artificial intelligence (AI) master programs offered by universities. However, there is still a lack of clarity regarding the specific skillsets of data scientists. This study aims to address this issue by employing Correlated Topic Modeling (CTM) to analyse the content of 41 master programs offered by seven Dutch universities. We assess the differences and similarities in the core skills taught by these programs, determine the subject-specific and general nature of the skills, and provide a comparison between the different types of universities offering these programs. Our findings reveal that research, data processing, statistics and ethics are the predominant skills taught in Dutch data science and AI master programs, with general universities emphasizing research skills and technical universities focusing more on IT and electronic skills. This study contributes to a better understanding of the diverse skillsets of data scientists, which is essential for employers, universities, and prospective students.","['Mathijs J. Mol', 'Barbara Belfi', 'Zsuzsa Bakk']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.14726,Anomali
Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining,"This paper presents an overview of the ImageArg shared task, the first multimodal ArgumentMiningshared task co-located with the 10th Workshop on ArgumentMiningat EMNLP 2023. The shared task comprises two classification subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image Persuasiveness Classification. The former determines the stance of a tweet containing an image and a piece oftexttoward a controversial topic (e.g., gun control and abortion). The latter determines whether the image makes the tweettextmore persuasive. The shared task received 31 submissions for Subtask-A and 21 submissions for Subtask-B from 9 different teams across 6 countries. The top submission in Subtask-A achieved an F1-score of 0.8647 while the best submission in Subtask-B achieved an F1-score of 0.5561.","['Zhexiong Liu', 'Mohamed Elaraby', 'Yang Zhong', 'Diane Litman']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.12172,Anomali
Automated Attribute Extraction from Legal Proceedings,"The escalating number of pending cases is a growing concern world-wide. Recent advancements in digitization have opened up possibilities for leveraging artificial intelligence (AI) tools in the processing of legal documents. Adopting a structured representation for legal documents, as opposed to a mere bag-of-words flattextrepresentation, can significantly enhance processing capabilities. With the aim of achieving this objective, we put forward a set of diverse attributes for criminal case proceedings. We use a state-of-the-art sequence labeling framework to automatically extract attributes from the legal documents. Moreover, we demonstrate the efficacy of the extracted attributes in a downstream task, namely legal judgment prediction.","['Subinay Adhikary', 'Sagnik Das', 'Sagnik Saha', 'Procheta Sen', 'Dwaipayan Roy', 'Kripabandhu Ghosh']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.12131,Anomali
Image Clustering with External Guidance,"The core of clustering is incorporating prior knowledge to construct supervision signals. From classic k-means based on data compactness to recent contrastive clustering guided by self-supervision, the evolution of clustering methods intrinsically corresponds to the progression of supervision signals. At present, substantial efforts have been devoted tomininginternal supervision signals from data. Nevertheless, the abundant external knowledge such as semantic descriptions, which naturally conduces to clustering, is regrettably overlooked. In this work, we propose leveraging external knowledge as a new supervision signal to guide clustering, even though it seems irrelevant to the given data. To implement and validate our idea, we design an externally guided clustering method (Text-Aided Clustering, TAC), which leverages the textual semantics of WordNet to facilitate image clustering. Specifically, TAC first selects and retrieves WordNet nouns that best distinguish images to enhance the feature discriminability. Then, to improve image clustering performance, TAC collaboratestextand image modalities by mutually distilling cross-modal neighborhood information. Experiments demonstrate that TAC achieves state-of-the-art performance on five widely used and three more challenging image clustering benchmarks, including the full ImageNet-1K dataset.","['Yunfan Li', 'Peng Hu', 'Dezhong Peng', 'Jiancheng Lv', 'Jianping Fan', 'Xi Peng']",ICML 2024 (Oral),arXiv,2024,https://doi.org/10.48550/arXiv.2310.11989,Anomali
Knowledge Extraction and Distillation from Large-Scale Image-Text Colonoscopy Records Leveraging Large Language and Vision Models,"The development of artificial intelligence systems for colonoscopy analysis often necessitates expert-annotated image datasets. However, limitations in dataset size and diversity impede model performance and generalisation. Image-textcolonoscopy records from routine clinical practice, comprising millions of images andtextreports, serve as a valuable data source, though annotating them is labour-intensive. Here we leverage recent advancements in large language and vision models and propose EndoKED, a dataminingparadigm for deep knowledge extraction and distillation. EndoKED automates the transformation of raw colonoscopy records into image datasets with pixel-level annotation. We validate EndoKED using multi-centre datasets of raw colonoscopy records (~1 million images), demonstrating its superior performance in training polyp detection and segmentation models. Furthermore, the EndoKED pre-trained vision backbone enables data-efficient and generalisable learning for optical biopsy, achieving expert-level performance in both retrospective and prospective validation.","['Shuo Wang', 'Yan Zhu', 'Xiaoyuan Luo', 'Zhiwei Yang', 'Yizhe Zhang', 'Peiyao Fu', 'Manning Wang', 'Zhijian Song', 'Quanlin Li', 'Pinghong Zhou', 'Yike Guo']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.11173,Anomali
A Computational Approach to Style in American Poetry,"We develop a quantitative method to assess the style of American poems and to visualize a collection of poems in relation to one another. Qualitative poetry criticism helped guide our development of metrics that analyze various orthographic, syntactic, and phonemic features. These features are used to discover comprehensive stylistic information from a poem's multi-layered latent structure, and to compute distances between poems in this space. Visualizations provide ready access to the analytical components. We demonstrate our method on several collections of poetry, showing that it better delineates poetry style than the traditional word-occurrence features that are used in typicaltextanalysis algorithms. Our method has potential applications to academic research oftexts, to research of the intuitive personal response to poetry, and to making recommendations to readers based on their favorite poems.","['David M. Kaplan', 'David M. Blei']","Seventh IEEE International Conference on Data Mining (ICDM 2007), Omaha, NE, USA, 2007, pp. 553-558",arXiv,2023,https://doi.org/10.48550/arXiv.2310.09357,Anomali
Multi-Robot Geometric Task-and-Motion Planning for Collaborative Manipulation Tasks,"We address multi-robot geometric task-and-motion planning (MR-GTAMP) problems in synchronous, monotone setups. The goal of the MR-GTAMP problem is to move objects with multiple robots to goal regions in the presence of other movable objects. We focus on collaborative manipulation tasks where the robots have to adopt intelligent collaboration strategies to be successful and effective, i.e., decide which robot should move which objects to which positions, and perform collaborative actions, such as handovers. To endow robots with these collaboration capabilities, we propose to first collect occlusion and reachability information for each robot by calling motion-planning algorithms. We then propose a method that uses the collected information to build a graph structure which captures the precedence of the manipulations of different objects and supports the implementation of a mixed-integer program to guide the search for highly effective collaborative task-and-motion plans. The search process for collaborative task-and-motion plans is based on a Monte-Carlo Tree Search (MCTS) exploration strategy to achieve exploration-exploitation balance. We evaluate our framework in two challenging MR-GTAMP domains and show that it outperforms two state-of-the-art baselines with respect to the planning time, the resulting plan length and the number of objects moved. We also show that our framework can be applied to undergroundminingoperations where a robotic arm needs to coordinate with an autonomous roof bolter. We demonstrate plan execution in two roof-bolting scenarios both in simulation and on robots.","['Hejia Zhang', 'Shao-Hung Chan', 'Jie Zhong', 'Jiaoyang Li', 'Peter Kolapo', 'Sven Koenig', 'Zach Agioutantis', 'Steven Schafrik', 'Stefanos Nikolaidis']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.08802,Anomali
The Uncertainty-based Retrieval Framework for Ancient Chinese CWS and POS,"Automatic analysis for modern Chinese has greatly improved the accuracy oftextminingin related fields, but the study of ancient Chinese is still relatively rare. Ancienttextdivision and lexical annotation are important parts of classical literature comprehension, and previous studies have tried to construct auxiliary dictionary and other fused knowledge to improve the performance. In this paper, we propose a framework for ancient Chinese Word Segmentation and Part-of-Speech Tagging that makes a twofold effort: on the one hand, we try to capture the wordhood semantics; on the other hand, we re-predict the uncertain samples of baseline model by introducing external knowledge. The performance of our architecture outperforms pre-trained BERT with CRF and existing tools such as Jiayan.","['Pengyu Wang', 'Zhichen Ren']","Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages, 2022, 164-168",arXiv,2023,https://doi.org/10.48550/arXiv.2310.08496,Anomali
Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval,"Image-textretrieval has developed rapidly in recent years. However, it is still a challenge in remote sensing due to visual-semantic imbalance, which leads to incorrect matching of non-semantic visual and textual features. To solve this problem, we propose a novel Direction-Oriented Visual-semantic Embedding Model (DOVE) tominethe relationship between vision and language. Our highlight is to conduct visual and textual representations in latent space, directing them as close as possible to a redundancy-free regional visual representation. Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the distance between the final visual and textual embeddings in the latent semantic space, oriented by regional visual features. Meanwhile, a lightweight DiggingTextGenome Assistant (DTGA) is designed to expand the range of tractable textual representation and enhance global word-level semantic connections using less attention operations. Ultimately, we exploit a global visual-semantic constraint to reduce single visual dependency and serve as an external constraint for the final visual and textual representations. The effectiveness and superiority of our method are verified by extensive experiments including parameter evaluation, quantitative comparison, ablation studies and visual analysis, on two benchmark datasets, RSICD and RSITMD.","['Qing Ma', 'Jiancheng Pan', 'Cong Bai']",,arXiv,2024,https://doi.org/10.48550/arXiv.2310.08276,Anomali
An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT,"This paper conducts a comprehensive investigation into applying large language models, particularly on BioBERT, in healthcare. It begins with thoroughly examining previous natural language processing (NLP) approaches in healthcare, shedding light on the limitations and challenges these methods face. Following that, this research explores the path that led to the incorporation of BioBERT into healthcare applications, highlighting its suitability for addressing the specific requirements of tasks related to biomedicaltextmining. The analysis outlines a systematic methodology for fine-tuning BioBERT to meet the unique needs of the healthcare domain. This approach includes various components, including the gathering of data from a wide range of healthcare sources, data annotation for tasks like identifying medical entities and categorizing them, and the application of specialized preprocessing techniques tailored to handle the complexities found in biomedicaltexts. Additionally, the paper covers aspects related to model evaluation, with a focus on healthcare benchmarks and functions like processing of natural language in biomedical, question-answering, clinical document classification, and medical entity recognition. It explores techniques to improve the model's interpretability and validates its performance compared to existing healthcare-focused language models. The paper thoroughly examines ethical considerations, particularly patient privacy and data security. It highlights the benefits of incorporating BioBERT into healthcare contexts, including enhanced clinical decision support and more efficient information retrieval. Nevertheless, it acknowledges the impediments and complexities of this integration, encompassing concerns regarding data privacy, transparency, resource-intensive requirements, and the necessity for model customization to align with diverse healthcare domains.","['Shyni Sharaf', 'V. S. Anoop']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.07282,Anomali
Textiverse: A Scalable Visual Analytics System for Exploring Geotagged and Timestamped Text Corpora,"We propose Textiverse, a big data approach formininggeotagged timestamped textual data on a map, such as for Twitter feeds, crime reports, or restaurant reviews. We use a scalable data management pipeline that extracts keyphrases from online databases in parallel. We speed up this time-consuming step so that it outpaces the content creation rate of popular social media. The result is presented in a web-based interface that integrates with Google Maps to visualize textual content of massive scale. The visual design is based on aggregating spatial regions into discrete sites and rendering each such site as a circular tag cloud. To demonstrate the intended use of our technique, we first show how it can be used to characterize the U.S.\ National Science Foundation funding status based on all 489,151 awards. We then apply the same technique on visually representing a more spatially scattered and linguistically informal dataset: 1.2 million Twitter posts about the Android mobile operating system.","['Caroline Berger', 'Hanjun Xian', 'Krishna Madhavan', 'Niklas Elmqvist']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.07242,Anomali
Argumentative Stance Prediction: An Exploratory Study on Multimodality and Few-Shot Learning,"To advance argumentative stance prediction as a multimodal problem, the First Shared Task in Multimodal ArgumentMininghosted stance prediction in crucial social topics of gun control and abortion. Our exploratory study attempts to evaluate the necessity of images for stance prediction in tweets and compare out-of-the-boxtext-based large-language models (LLM) in few-shot settings against fine-tuned unimodal and multimodal models. Our work suggests an ensemble of fine-tunedtext-based language models (0.817 F1-score) outperforms both the multimodal (0.677 F1-score) andtext-based few-shot prediction using a recent state-of-the-art LLM (0.550 F1-score). In addition to the differences in performance, our findings suggest that the multimodal models tend to perform better when image content is summarized as natural language over their native pixel structure and, using in-context examples improves few-shot performance of LLMs.","['Arushi Sharma', 'Abhibha Gupta', 'Maneesh Bilalpur']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.07093,Anomali
"TILFA: A Unified Framework for Text, Image, and Layout Fusion in Argument Mining","A main goal of ArgumentMining(AM) is to analyze an author's stance. Unlike previous AM datasets focusing only ontext, the shared task at the 10th Workshop on ArgumentMiningintroduces a dataset including bothtextand images. Importantly, these images contain both visual elements and optical characters. Our new framework, TILFA (A Unified Framework forText, Image, and Layout Fusion in ArgumentMining), is designed to handle this mixed data. It excels at not only understandingtextbut also detecting optical characters and recognizing layout details in images. Our model significantly outperforms existing baselines, earning our team, KnowComp, the 1st place in the leaderboard of Argumentative Stance Classification subtask in this shared task.","['Qing Zong', 'Zhaowei Wang', 'Baixuan Xu', 'Tianshi Zheng', 'Haochen Shi', 'Weiqi Wang', 'Yangqiu Song', 'Ginny Y. Wong', 'Simon See']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.05210,Anomali
ForeSeer: Product Aspect Forecasting Using Temporal Graph Embedding,"Developingtextminingapproaches tomineaspects from customer reviews has been well-studied due to its importance in understanding customer needs and product attributes. In contrast, it remains unclear how to predict the future emerging aspects of a new product that currently has little review information. This task, which we named product aspect forecasting, is critical for recommending new products, but also challenging because of the missing reviews. Here, we propose ForeSeer, a novel textualminingand product embedding approach progressively trained on temporal product graphs for this novel product aspect forecasting task. ForeSeer transfers reviews from similar products on a large product graph and exploits these reviews to predict aspects that might emerge in future reviews. A key novelty of our method is to jointly provide review, product, and aspect embeddings that are both time-sensitive and less affected by extremely imbalanced aspect frequencies. We evaluated ForeSeer on a real-world product review system containing 11,536,382 reviews and 11,000 products over 3 years. We observe that ForeSeer substantially outperformed existing approaches with at least 49.1\% AUPRC improvement under the real setting where aspect associations are not given. ForeSeer further improves future link prediction on the product graph and the review aspect association prediction. Collectively, Foreseer offers a novel framework for review forecasting by effectively integrating reviewtext, product network, and temporal information, opening up new avenues for online shopping recommendation and e-commerce applications.","['Zixuan Liu', 'Gaurush Hiranandani', 'Kun Qian', 'Eddie W. Huang', 'Yi Xu', 'Belinda Zeng', 'Karthik Subbian', 'Sheng Wang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.04865,Anomali
Literature Based Discovery (LBD): Towards Hypothesis Generation and Knowledge Discovery in Biomedical Text Mining,"Biomedical knowledge is growing in an astounding pace with a majority of this knowledge is represented as scientific publications.Textminingtools and methods represents automatic approaches for extracting hidden patterns and trends from this semi structured and unstructured data. In BiomedicalTextmining, Literature Based Discovery (LBD) is the process of automatically discovering novel associations between medical terms otherwise mentioned in disjoint literature sets. LBD approaches proven to be successfully reducing the discovery time of potential associations that are hidden in the vast amount of scientific literature. The process focuses on creating concept profiles for medical terms such as a disease or symptom and connecting it with a drug and treatment based on the statistical significance of the shared profiles. This knowledge discovery approach introduced in 1989 still remains as a core task intextmining. Currently the ABC principle based two approaches namely open discovery and closed discovery are mostly explored in LBD process. This review starts with general introduction abouttextminingfollowed by biomedicaltextminingand introduces various literature resources such as MEDLINE, UMLS, MESH, and SemMedDB. This is followed by brief introduction of the core ABC principle and its associated two approaches open discovery and closed discovery in LBD process. This review also discusses the deep learning applications in LBD by reviewing the role of transformer models and neural networks based LBD models and its future aspects. Finally, reviews the key biomedical discoveries generated through LBD approaches in biomedicine and conclude with the current limitations and future directions of LBD.","['Balu Bhasuran', 'Gurusamy Murugesan', 'Jeyakumar Natarajan']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.03766,Anomali
Procedural Text Mining with Large Language Models,"Recent advancements in the field of Natural Language Processing, particularly the development of large-scale language models that are pretrained on vast amounts of knowledge, are creating novel opportunities within the realm of Knowledge Engineering. In this paper, we investigate the usage of large language models (LLMs) in both zero-shot and in-context learning settings to tackle the problem of extracting procedures from unstructured PDFtextin an incremental question-answering fashion. In particular, we leverage the current state-of-the-art GPT-4 (Generative Pre-trained Transformer 4) model, accompanied by two variations of in-context learning that involve an ontology with definitions of procedures and steps and a limited number of samples of few-shot learning. The findings highlight both the promise of this approach and the value of the in-context learning customisations. These modifications have the potential to significantly address the challenge of obtaining sufficient training data, a hurdle often encountered in deep learning-based Natural Language Processing techniques for procedure extraction.","['Anisa Rula', ""Jennifer D'Souza""]",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.03376,Anomali
Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models,"Clinicaltexts, represented in electronic medical records (EMRs), contain rich medical information and are essential for disease prediction, personalised information recommendation, clinical decision support, and medication patternminingand measurement. Relation extractions between medication mentions and temporal information can further help clinicians better understand the patients' treatment history. To evaluate the performances of deep learning (DL) and large language models (LLMs) in medication extraction and temporal relations classification, we carry out an empirical investigation of \textbf{MedTem} project using several advanced learning structures including BiLSTM-CRF and CNN-BiLSTM for a clinical domain named entity recognition (NER), and BERT-CNN for temporal relation extraction (RE), in addition to the exploration of different word embedding techniques. Furthermore, we also designed a set of post-processing roles to generate structured output on medications and the temporal relation. Our experiments show that CNN-BiLSTM slightly wins the BiLSTM-CRF model on the i2b2-2009 clinical NER task yielding 75.67, 77.83, and 78.17 for precision, recall, and F1 scores using Macro Average. BERT-CNN model also produced reasonable evaluation scores 64.48, 67.17, and 65.03 for P/R/F1 using Macro Avg on the temporal relation extraction test set from i2b2-2012 challenges. Code and Tools from MedTem will be hosted at \url{https://github.com/HECTA-UoM/MedTem}","['Hangyu Tu', 'Lifeng Han', 'Goran Nenadic']",,arXiv,2023,https://doi.org/10.48550/arXiv.2310.02229,Anomali
Beyond Co-occurrence: Multi-modal Session-based Recommendation,"Session-based recommendation is devoted to characterizing preferences of anonymous users based on short sessions. Existing methods mostly focus onmininglimited item co-occurrence patterns exposed by item ID within sessions, while ignoring what attracts users to engage with certain items is rich multi-modal information displayed on pages. Generally, the multi-modal information can be classified into two categories: descriptive information (e.g., item images and descriptiontext) and numerical information (e.g., price). In this paper, we aim to improve session-based recommendation by modeling the above multi-modal information holistically. There are mainly three issues to reveal user intent from multi-modal information: (1) How to extract relevant semantics from heterogeneous descriptive information with different noise? (2) How to fuse these heterogeneous descriptive information to comprehensively infer user interests? (3) How to handle probabilistic influence of numerical information on user behaviors? To solve above issues, we propose a novel multi-modal session-based recommendation (MMSBR) that models both descriptive and numerical information under a unified framework. Specifically, a pseudo-modality contrastive learning is devised to enhance the representation learning of descriptive information. Afterwards, a hierarchical pivot transformer is presented to fuse heterogeneous descriptive information. Moreover, we represent numerical information with Gaussian distribution and design a Wasserstein self-attention to handle the probabilistic influence mode. Extensive experiments on three real-world datasets demonstrate the effectiveness of the proposed MMSBR. Further analysis also proves that our MMSBR can alleviate the cold-start problem in SBR effectively.","['Xiaokun Zhang', 'Bo Xu', 'Fenglong Ma', 'Chenliang Li', 'Liang Yang', 'Hongfei Lin']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.17037,Anomali
A review of Design of Experiments courses offered to undergraduate students at American universities,"Design of Experiments (DoE) is a relevant class to undergraduate students in the sciences, because it teaches them how to plan, conduct, and analyze experiments. In the literature on DoE, there are several contributions to its pedagogy, such as easy-to-use class experiments, virtual experiments, and software to construct experimental designs. However, there are virtually no systematic evaluations of the actual DoE pedagogy. To address this issue, we build the first database of DoE courses offered to undergraduate students in the United States. The database has records on courses offered from 2019 to 2022 at the best universities in the US News Best National Universities ranking of 2022. Specifically, it has data on 18 general and content-specific features of 206 courses. To study the DoE pedagogy, we analyze the database using descriptive statistics andtextmining. Based on our analysis, we provide instructors with recommendations and teaching material to enhance their DoE courses. The database and material are included in the supplement of this article.","['Alan R. Vazquez', 'Xiaocong Xuan']",,arXiv,2024,https://doi.org/10.48550/arXiv.2309.16961,Anomali
Clinical Trial Recommendations Using Semantics-Based Inductive Inference and Knowledge Graph Embeddings,"Designing a new clinical trial entails many decisions, such as defining a cohort and setting the study objectives to name a few, and therefore can benefit from recommendations based on exhaustiveminingof past clinical trial records. Here, we propose a novel recommendation methodology, based on neural embeddings trained on a first-of-a-kind knowledge graph of clinical trials. We addressed several important research questions in this context, including designing a knowledge graph (KG) for clinical trial data, effectiveness of various KG embedding (KGE) methods for it, a novel inductive inference using KGE, and its use in generating recommendations for clinical trial design. We used publicly available data from clinicaltrials.gov for the study. Results show that our recommendations approach achieves relevance scores of 70%-83%, measured as thetextsimilarity to actual clinical trial elements, and the most relevant recommendation can be found near the top of list. Our study also suggests potential improvement in training KGE using node semantics.","['Murthy V. Devarakonda', 'Smita Mohanty', 'Raja Rao Sunkishala', 'Nag Mallampalli', 'Xiong Liu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.15979,Anomali
Object-Centric Open-Vocabulary Image-Retrieval with Aggregated Features,"The task of open-vocabulary object-centric image retrieval involves the retrieval of images containing a specified object of interest, delineated by an open-settextquery. As working on large image datasets becomes standard, solving this task efficiently has gained significant practical importance. Applications include targeted performance analysis of retrieved images using ad-hoc queries and hard exampleminingduring training. Recent advancements in contrastive-based open vocabulary systems have yielded remarkable breakthroughs, facilitating large-scale open vocabulary image retrieval. However, these approaches use a single global embedding per image, thereby constraining the system's ability to retrieve images containing relatively small object instances. Alternatively, incorporating local embeddings from detection pipelines faces scalability challenges, making it unsuitable for retrieval from large databases.
  In this work, we present a simple yet effective approach to object-centric open-vocabulary image retrieval. Our approach aggregates dense embeddings extracted from CLIP into a compact representation, essentially combining the scalability of image retrieval pipelines with the object identification capabilities of dense detection methods. We show the effectiveness of our scheme to the task by achieving significantly better results than global feature approaches on three datasets, increasing accuracy by up to 15 mAP points. We further integrate our scheme into a large scale retrieval framework and demonstrate our method's advantages in terms of scalability and interpretability.","['Hila Levi', 'Guy Heller', 'Dan Levi', 'Ethan Fetaya']",,arXiv,2024,https://doi.org/10.48550/arXiv.2309.14999,Anomali
Machine-assisted quantitizing designs: augmenting humanities and social sciences with artificial intelligence,"The increasing capacities of large language models (LLMs) have been shown to present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, by automating complex qualitative tasks otherwise typically carried out by human researchers. While numerous benchmarking studies have assessed the analytic prowess of LLMs, there is less focus on operationalizing this capacity for inference and hypothesis testing. Addressing this challenge, a systematic framework is argued for here, building on mixed methods quantitizing and converting design principles, and feature analysis from linguistics, to transparently integrate human expertise and machine scalability. Replicability and statistical robustness are discussed, including how to incorporate machine annotator error rates in subsequent inference. The approach is discussed and demonstrated in over a dozen LLM-assisted case studies, covering 9 diverse languages, multiple disciplines and tasks, including analysis of themes, stances, ideas, and genre compositions; linguistic and semantic annotation, interviews,textminingand event cause inference in noisy historical data, literary social network construction, metadata imputation, and multimodal visual cultural analytics. Using hypothesis-driven topic classification instead of ""distant reading"" is discussed. The replications among the experiments also illustrate how tasks previously requiring protracted team effort or complex computational pipelines can now be accomplished by an LLM-assisted scholar in a fraction of the time. Importantly, the approach is not intended to replace, but to augment and scale researcher expertise and analytic practices. With these opportunities in sight, qualitative skills and the ability to pose insightful questions have arguably never been more critical.",['Andres Karjus'],,arXiv,2024,https://doi.org/10.48550/arXiv.2309.14379,Anomali
Graph Representation Learning Towards Patents Network Analysis,"Patent analysis has recently been recognized as a powerful technique for large companies worldwide to lend them insight into the age of competition among various industries. This technique is considered a shortcut for developing countries since it can significantly accelerate their technology development. Therefore, as an inevitable process, patent analysis can be utilized to monitor rival companies and diverse industries. This research employed a graph representation learning approach to create, analyze, and find similarities in the patent data registered in the Iranian Official Gazette. The patent records were scrapped and wrangled through the Iranian Official Gazette portal. Afterward, the key entities were extracted from the scrapped patents dataset to create the Iranian patents graph from scratch based on novel natural language processing and entity resolution techniques. Finally, thanks to the utilization of novel graph algorithms andtextminingmethods, we identified new areas of industry and research from Iranian patent data, which can be used extensively to prevent duplicate patents, familiarity with similar and connected inventions, Awareness of legal entities supporting patents and knowledge of researchers and linked stakeholders in a particular research field.","['Mohammad Heydari', 'Babak Teimourpour']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.13888,Anomali
Multiple Relations Classification using Imbalanced Predictions Adaptation,"The relation classification task assigns the proper semantic relation to a pair of subject and object entities; the task plays a crucial role in varioustextminingapplications, such as knowledge graph construction and entities interaction discovery in biomedicaltext. Current relation classification models employ additional procedures to identify multiple relations in a single sentence. Furthermore, they overlook the imbalanced predictions pattern. The pattern arises from the presence of a few valid relations that need positive labeling in a relatively large predefined relations set. We propose a multiple relations classification model that tackles these issues through a customized output architecture and by exploiting additional input features. Our findings suggest that handling the imbalanced predictions leads to significant improvements, even on a modest training design. The results demonstrate superiority performance on benchmark datasets commonly used in relation classification. To the best of our knowledge, this work is the first that recognizes the imbalanced predictions within the relation classification task.","['Sakher Khalil Alqaaidi', 'Elika Bozorgi', 'Krzysztof J. Kochut']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.13718,Anomali
Decoding the Alphabet Soup of Degrees in the United States Postsecondary Education System Through Hybrid Method: Database and Text Mining,"This paper proposes a model to predict the levels (e.g., Bachelor, Master, etc.) of postsecondary degree awards that have been ambiguously expressed in the student tracking reports of the National Student Clearinghouse (NSC). The model will be the hybrid of two modules. The first module interprets the relevant abbreviatory elements embedded in NSC reports by referring to a comprehensive database that we have made of nearly 950 abbreviations for degree titles used by American postsecondary educators. The second module is a combination of feature classification andtextminingmodeled with CNN-BiLSTM, which is preceded by several steps of heavy pre-processing. The model proposed in this paper was trained with four multi-label datasets of different grades of resolution and returned 97.83\% accuracy with the most sophisticated dataset. Such a thorough classification of degree levels will provide insights into the modeling patterns of student success and mobility. To date, such a classification strategy has not been attempted except using manual methods and simpletextparsing logic.","['Sahar Voghoei', 'James Byars', 'John A Miller', 'Khaled Rasheed', 'Hamid A Arabnia']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.13050,Anomali
Exploiting Modality-Specific Features For Multi-Modal Manipulation Detection And Grounding,"AI-synthesizedtextand images have gained significant attention, particularly due to the widespread dissemination of multi-modal manipulations on the internet, which has resulted in numerous negative impacts on society. Existing methods for multi-modal manipulation detection and grounding primarily focus on fusing vision-language features to make predictions, while overlooking the importance of modality-specific features, leading to sub-optimal results. In this paper, we construct a simple and novel transformer-based framework for multi-modal manipulation detection and grounding tasks. Our framework simultaneously explores modality-specific features while preserving the capability for multi-modal alignment. To achieve this, we introduce visual/language pre-trained encoders and dual-branch cross-attention (DCA) to extract and fuse modality-unique features. Furthermore, we design decoupled fine-grained classifiers (DFC) to enhance modality-specific featureminingand mitigate modality competition. Moreover, we propose an implicit manipulation query (IMQ) that adaptively aggregates global contextual cues within each modality using learnable queries, thereby improving the discovery of forged details. Extensive experiments on the $\rm DGM^4$ dataset demonstrate the superior performance of our proposed model compared to state-of-the-art approaches.","['Jiazhen Wang', 'Bin Liu', 'Changtao Miao', 'Zhiwei Zhao', 'Wanyi Zhuang', 'Qi Chu', 'Nenghai Yu']",,arXiv,2024,https://doi.org/10.48550/arXiv.2309.12657,Anomali
Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech,"Prosodic phrasing is crucial to the naturalness and intelligibility of end-to-endText-to-Speech (TTS). There exist both linguistic and emotional prosody in natural speech. As the study of prosodic phrasing has been linguistically motivated, prosodic phrasing for expressive emotion rendering has not been well studied. In this paper, we propose an emotion-aware prosodic phrasing model, termed \textit{EmoPP}, tominethe emotional cues of utterance accurately and predict appropriate phrase breaks. We first conduct objective observations on the ESD dataset to validate the strong correlation between emotion and prosodic phrasing. Then the objective and subjective evaluations show that the EmoPP outperforms all baselines and achieves remarkable performance in terms of emotion expressiveness. The audio samples and the code are available at \url{https://github.com/AI-S2-Lab/EmoPP}.","['Rui Liu', 'Bin Liu', 'Haizhou Li']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.11724,Anomali
Large Synthetic Data from the arXiv for OCR Post Correction of Historic Scientific Articles,"Scientific articles published prior to the ""age of digitization"" (~1997) require Optical Character Recognition (OCR) to transform scanned documents into machine-readabletext, a process that often produces errors. We develop a pipeline for the generation of a synthetic ground truth/OCR dataset to correct the OCR results of the astrophysics literature holdings of the NASA Astrophysics Data System (ADS). Byminingthe arXiv we create, to the authors' knowledge, the largest scientific synthetic ground truth/OCR post correction dataset of 203,354,393 character pairs. We provide baseline models trained with this dataset and find the mean improvement in character and word error rates of 7.71% and 18.82% for historical OCRtext, respectively. When used to classify parts of sentences as inline math, we find a classification F1 score of 77.82%. Interactive dashboards to explore the dataset are available online: https://readingtimemachine.github.io/projects/1-ocr-groundtruth-may2023, and data and code, within the limitations of our agreement with the arXiv, are hosted on GitHub: https://github.com/ReadingTimeMachine/ocr_post_correction.","['Jill P. Naiman', 'Morgan G. Cosillo', 'Peter K. G. Williams', 'Alyssa Goodman']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.11549,Anomali
Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning,"In recent years, the explosion of web videos makestext-video retrieval increasingly essential and popular for video filtering, recommendation, and search.Text-video retrieval aims to rank relevanttext/video higher than irrelevant ones. The core of this task is to precisely measure the cross-modal similarity betweentextsand videos. Recently, contrastive learning methods have shown promising results fortext-video retrieval, most of which focus on the construction of positive and negative pairs to learntextand video representations. Nevertheless, they do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity. To address these two issues, this paper improves contrastive learning using two novel techniques. First, to exploit hard examples for robust discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module (DMAE) tominehard negative pairs from textual and visual clues. By further introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively identify all these hard negatives and explicitly highlight their impacts in the training loss. Second, our work argues that triplet samples can better model fine-grained semantic similarity compared to pairwise samples. We thereby present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to construct partial order triplet samples by automatically generating fine-grained hard negatives for matchedtext-video pairs. The proposed TPM-CL designs an adaptive token masking strategy with cross-modal interaction to model subtle semantic differences. Extensive experiments demonstrate that the proposed approach outperforms existing methods on four widely-usedtext-video retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.","['Chen Jiang', 'Hong Liu', 'Xuzheng Yu', 'Qing Wang', 'Yuan Cheng', 'Jia Xu', 'Zhongyi Liu', 'Qingpei Guo', 'Wei Chu', 'Ming Yang', 'Yuan Qi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2309.11082,Anomali
Semi-automatic staging area for high-quality structured data extraction from scientific literature,"We propose a semi-automatic staging area for efficiently building an accurate database of experimental physical properties of superconductors from literature, called SuperCon2, to enrich the existing manually-built superconductor database SuperCon. Here we report our curation interface (SuperCon2 Interface) and a workflow managing the state transitions of each examined record, to validate the dataset of superconductors from PDF documents collected using Grobid-superconductors in a previous work. This curation workflow allows both automatic and manual operations, the former contains ``anomaly detection'' that scans new data identifying outliers, and a ``training data collector'' mechanism that collects training data examples based on manual corrections. Such training data collection policy is effective in improving the machine-learning models with a reduced number of examples. For manual operations, the interface (SuperCon2 interface) is developed to increase efficiency during manual correction by providing a smart interface and an enhanced PDF document viewer. We show that our interface significantly improves the curation quality by boosting precision and recall as compared with the traditional ``manual correction''. Our semi-automatic approach would provide a solution for achieving a reliable database withtext-dataminingof scientific documents.","['Luca Foppiano', 'Tomoya Mato', 'Kensei Terashima', 'Pedro Ortiz Suarez', 'Taku Tou', 'Chikako Sakai', 'Wei-Sheng Wang', 'Toshiyuki Amagasa', 'Yoshihiko Takano', 'Masashi Ishii']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.10923,Anomali
In Consideration of Indigenous Data Sovereignty: Data Mining as a Colonial Practice,"Dataminingreproduces colonialism, and Indigenous voices are being left out of the development of technology that relies on data, such as artificial intelligence. This research stresses the need for the inclusion of Indigenous Data Sovereignty and centers on the importance of Indigenous rights over their own data. Inclusion is necessary in order to integrate Indigenous knowledge into the design, development, and implementation of data-reliant technology. To support this hypothesis and address the problem, the CARE Principles for Indigenous Data Governance (Collective Benefit, Authority to Control, Responsibility, and Ethics) are applied. We cover how the colonial practices of dataminingdo not align with Indigenous convictions. The included case studies highlight connections to Indigenous rights in relation to the protection of data and environmental ecosystems, thus establishing how data governance can serve both the people and the Earth. By applying the CARE Principles to the issues that arise from dataminingand neocolonialism, our goal is to provide a framework that can be used in technological development. The theory is that this could reflect outwards to promote data sovereignty generally and create new relationships between people and data that are ethical as opposed to driven by speed and profit.","['Jennafer Shae Roberts', 'Laura N Montoya']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.10215,Anomali
AutoAM: An End-To-End Neural Model for Automatic and Universal Argument Mining,"Argumentminingis to analyze argument structure and extract important argument information from unstructuredtext. An argumentminingsystem can help people automatically gain causal and logical information behind thetext. As argumentative corpus gradually increases, like more people begin to argue and debate on social media, argumentminingfrom them is becoming increasingly critical. However, argumentminingis still a big challenge in natural language tasks due to its difficulty, and relative techniques are not mature. For example, research on non-tree argumentminingneeds to be done more. Most works just focus on extracting tree structure argument information. Moreover, current methods cannot accurately describe and capture argument relations and do not predict their types. In this paper, we propose a novel neural model called AutoAM to solve these problems. We first introduce the argument component attention mechanism in our model. It can capture the relevant information between argument components, so our model can better perform argumentmining. Our model is a universal end-to-end framework, which can analyze argument structure without constraints like tree structure and complete three subtasks of argumentminingin one model. The experiment results show that our model outperforms the existing works on several metrics in two public datasets.",['Lang Cao'],,arXiv,2023,https://doi.org/10.48550/arXiv.2309.09300,Anomali
Modeling the Evolutionary Trends in Corporate ESG Reporting: A Study based on Knowledge Management Model,"Environmental, social, and governance (ESG) reports are globally recognized as a keystone in sustainable enterprise development. However, current literature has not concluded the development of topics and trends in ESG contexts in the twenty-first century. Therefore, We selected 1114 ESG reports from firms in the technology industry to analyze the evolutionary trends of ESG topics bytextmining. We discovered the homogenization effect towards low environmental, medium governance, and high social features in the evolution. We also designed a strategic framework to look closer into the dynamic changes of firms' within-industry scores and across-domain importances. We found that companies are gradually converging towards the third quadrant, which indicates that firms contribute less to industrial outstanding and professional distinctiveness in ESG reporting. Firms choose to imitate ESG reports from each other to mitigate uncertainty and enhance behavioral legitimacy.","['Ziyuan Xia', 'Anchen Sun', 'Xiaodong Cai', 'Saixing Zeng']",,arXiv,2024,https://doi.org/10.48550/arXiv.2309.07001,Anomali
Synthetic Text Generation using Hypergraph Representations,"Generating synthetic variants of a document is often posed astext-to-texttransformation. We propose an alternate LLM based method that first decomposes a document into semantic frames and then generatestextusing this interim sparse format. The frames are modeled using a hypergraph, which allows perturbing the frame contents in a principled manner. Specifically, new hyperedges areminedthrough topological analysis and complex polyadic relationships including hierarchy and temporal dynamics are accommodated. We show that our solution generates documents that are diverse, coherent and vary in style, sentiment, format, composition and facts.","['Natraj Raman', 'Sameena Shah']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.06550,Anomali
Computational Approaches for Predicting Drug-Disease Associations: A Comprehensive Review,"In recent decades, traditional drug research and development have been facing challenges such as high cost, long timelines, and high risks. To address these issues, many computational approaches have been suggested for predicting the relationship between drugs and diseases through drug repositioning, aiming to reduce the cost, development cycle, and risks associated with developing new drugs. Researchers have explored different computational methods to predict drug-disease associations, including drug side effects-disease associations, drug-target associations, and miRNAdisease associations. In this comprehensive review, we focus on recent advances in predicting drug-disease association methods for drug repositioning. We first categorize these methods into several groups, including neural network-based algorithms, matrixbased algorithms, recommendation algorithms, link-based reasoning algorithms, andtextminingand semantic reasoning. Then, we compare the prediction performance of existing drug-disease association prediction algorithms. Lastly, we delve into the present challenges and future prospects concerning drug-disease associations.","['Chunyan Ao', 'Zhichao Xiao', 'Lixin Guan', 'Liang Yu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.06388,Anomali
Dual-Path Temporal Map Optimization for Make-up Temporal Video Grounding,"Make-up temporal video grounding (MTVG) aims to localize the target video segment which is semantically related to a sentence describing a make-up activity, given a long video. Compared with the general video grounding task, MTVG focuses on meticulous actions and changes on the face. The make-up instruction step, usually involving detailed differences in products and facial areas, is more fine-grained than general activities (e.g, cooking activity and furniture assembly). Thus, existing general approaches cannot locate the target activity effectually. More specifically, existing proposal generation modules are not yet fully developed in providing semantic cues for the more fine-grained make-up semantic comprehension. To tackle this issue, we propose an effective proposal-based framework named Dual-Path Temporal Map Optimization Network (DPTMO) to capture fine-grained multimodal semantic details of make-up activities. DPTMO extracts both query-agnostic and query-guided features to construct two proposal sets and uses specific evaluation methods for the two sets. Different from the commonly used single structure in previous methods, our dual-path structure canminemore semantic information in make-up videos and distinguish fine-grained actions well. These two candidate sets represent the cross-modal makeup video-textsimilarity and multi-modal fusion relationship, complementing each other. Each set corresponds to its respective optimization perspective, and their joint prediction enhances the accuracy of video timestamp prediction. Comprehensive experiments on the YouMakeup dataset demonstrate our proposed dual structure excels in fine-grained semantic comprehension.","['Jiaxiu Li', 'Kun Li', 'Jia Li', 'Guoliang Chen', 'Dan Guo', 'Meng Wang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.06176,Anomali
Balanced and Explainable Social Media Analysis for Public Health with Large Language Models,"As social media becomes increasingly popular, more and more public health activities emerge, which is worth noting for pandemic monitoring and government decision-making. Current techniques for public health analysis involve popular models such as BERT and large language models (LLMs). Although recent progress in LLMs has shown a strong ability to comprehend knowledge by being fine-tuned on specific domain datasets, the costs of training an in-domain LLM for every specific public health task are especially expensive. Furthermore, such kinds of in-domain datasets from social media are generally highly imbalanced, which will hinder the efficiency of LLMs tuning. To tackle these challenges, the data imbalance issue can be overcome by sophisticated data augmentation methods for social media datasets. In addition, the ability of the LLMs can be effectively utilised by prompting the model properly. In light of the above discussion, in this paper, a novel ALEX framework is proposed for social media analysis on public health. Specifically, an augmentation pipeline is developed to resolve the data imbalance issue. Furthermore, an LLMs explanation mechanism is proposed by prompting an LLM with the predicted results from BERT models. Extensive experiments conducted on three tasks at the Social MediaMiningfor Health 2023 (SMM4H) competition with the first ranking in two tasks demonstrate the superior performance of the proposed ALEX method. Our code has been released in https://github.com/YanJiangJerry/ALEX.","['Yan Jiang', 'Ruihong Qiu', 'Yi Zhang', 'Peng-Fei Zhang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.05951,Anomali
Duplicate Question Retrieval and Confirmation Time Prediction in Software Communities,"Community Question Answering (CQA) in different domains is growing at a large scale because of the availability of several platforms and huge shareable information among users. With the rapid growth of such online platforms, a massive amount of archived data makes it difficult for moderators to retrieve possible duplicates for a new question and identify and confirm existing question pairs as duplicates at the right time. This problem is even more critical in CQAs corresponding to large software systems like askubuntu where moderators need to be experts to comprehend something as a duplicate. Note that the prime challenge in such CQA platforms is that the moderators are themselves experts and are therefore usually extremely busy with their time being extraordinarily expensive. To facilitate the task of the moderators, in this work, we have tackled two significant issues for the askubuntu CQA platform: (1) retrieval of duplicate questions given a new question and (2) duplicate question confirmation time prediction. In the first task, we focus on retrieving duplicate questions from a question pool for a particular newly posted question. In the second task, we solve a regression problem to rank a pair of questions that could potentially take a long time to get confirmed as duplicates. For duplicate question retrieval, we propose a Siamese neural network based approach by exploiting bothtextand network-based features, which outperforms several state-of-the-art baseline techniques. Our method outperforms DupPredictor and DUPE by 5% and 7% respectively. For duplicate confirmation time prediction, we have used both the standard machine learning models and neural network along with thetextand graph-based features. We obtain Spearman's rank correlation of 0.20 and 0.213 (statistically significant) fortextand graph based features respectively.","['Rima Hazra', 'Debanjan Saha', 'Amruit Sahoo', 'Somnath Banerjee', 'Animesh Mukherjee']",,arXiv,2024,https://doi.org/10.48550/arXiv.2309.05035,Anomali
Multi-modal Extreme Classification,"This paper develops the MUFIN technique for extreme classification (XC) tasks with millions of labels where datapoints and labels are endowed with visual and textual descriptors. Applications of MUFIN to product-to-product recommendation and bid query prediction over several millions of products are presented. Contemporary multi-modal methods frequently rely on purely embedding-based methods. On the other hand, XC methods utilize classifier architectures to offer superior accuracies than embedding only methods but mostly focus ontext-based categorization tasks. MUFIN bridges this gap by reformulating multi-modal categorization as an XC problem with several millions of labels. This presents the twin challenges of developing multi-modal architectures that can offer embeddings sufficiently expressive to allow accurate categorization over millions of labels; and training and inference routines that scale logarithmically in the number of labels. MUFIN develops an architecture based on cross-modal attention and trains it in a modular fashion using pre-training and positive and negativemining. A novel product-to-product recommendation dataset MM-AmazonTitles-300K containing over 300K products was curated from publicly available amazon.com listings with each product endowed with a title and multiple images. On the all datasets MUFIN offered at least 3% higher accuracy than leadingtext-based, image-based and multi-modal techniques. Code for MUFIN is available at https://github.com/Extreme-classification/MUFIN","['Anshul Mittal', 'Kunal Dahiya', 'Shreya Malani', 'Janani Ramaswamy', 'Seba Kuruvilla', 'Jitendra Ajmera', 'Keng-hao Chang', 'Sumeet Agarwal', 'Purushottam Kar', 'Manik Varma']",Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2022,arXiv,2023,https://doi.org/10.48550/arXiv.2309.04961,Anomali
Weakly supervised learning for pattern classification in serial femtosecond crystallography,"Serial femtosecond crystallography at X-ray free electron laser facilities opens a new era for the determination of crystal structure. However, the data processing of those experiments is facing unprecedented challenge, because the total number of diffraction patterns needed to determinate a high-resolution structure is huge. Machine learning methods are very likely to play important roles in dealing with such a large volume of data. Convolutional neural networks have made a great success in the field of pattern classification, however, training of the networks need very large datasets with labels. Th is heavy dependence on labeled datasets will seriously restrict the application of networks, because it is very costly to annotate a large number of diffraction patterns. In this article we present our job on the classification of diffraction pattern by weakly supervised algorithms, with the aim of reducing as much as possible the size of the labeled dataset required for training. Our result shows that weakly supervised methods can significantly reduce the need for the number of labeled patterns while achieving comparable accuracy to fully supervised methods.","['Jianan Xie', 'Ji Liu', 'Chi Zhang', 'Xihui Chen', 'Ping Huai', 'Jie Zheng', 'Xiaofeng Zhang']","Opt. Express 31(20), 32909-32924 (2023)",arXiv,2023,https://doi.org/10.48550/arXiv.2309.04474,Anomali
ViCGCN: Graph Convolutional Network with Contextualized Language Models for Social Media Mining in Vietnamese,"Social media processing is a fundamental task in natural language processing with numerous applications. As Vietnamese social media and information science have grown rapidly, the necessity of information-basedminingon Vietnamese social media has become crucial. However, state-of-the-art research faces several significant drawbacks, including imbalanced data and noisy data on social media platforms. Imbalanced and noisy are two essential issues that need to be addressed in Vietnamese social mediatexts. Graph Convolutional Networks can address the problems of imbalanced and noisy data intextclassification on social media by taking advantage of the graph structure of the data. This study presents a novel approach based on contextualized language model (PhoBERT) and graph-based method (Graph Convolutional Networks). In particular, the proposed approach, ViCGCN, jointly trained the power of Contextualized embeddings with the ability of Graph Convolutional Networks, GCN, to capture more syntactic and semantic dependencies to address those drawbacks. Extensive experiments on various Vietnamese benchmark datasets were conducted to verify our approach. The observation shows that applying GCN to BERTology models as the final layer significantly improves performance. Moreover, the experiments demonstrate that ViCGCN outperforms 13 powerful baseline models, including BERTology models, fusion BERTology and GCN models, other baselines, and SOTA on three benchmark social media datasets. Our proposed ViCGCN approach demonstrates a significant improvement of up to 6.21%, 4.61%, and 2.63% over the best Contextualized Language Models, including multilingual and monolingual, on three benchmark datasets, UIT-VSMEC, UIT-ViCTSD, and UIT-VSFC, respectively. Additionally, our integrated model ViCGCN achieves the best performance compared to other BERTology integrated with GCN models.","['Chau-Thang Phan', 'Quoc-Nam Nguyen', 'Chi-Thanh Dang', 'Trong-Hop Do', 'Kiet Van Nguyen']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.02902,Anomali
Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts,"Many of the most commonly explored natural language processing (NLP) information extraction tasks can be thought of as evaluations of declarative knowledge, or fact-based information extraction. Procedural knowledge extraction, i.e., breaking down a described process into a series of steps, has received much less attention, perhaps in part due to the lack of structured datasets that capture the knowledge extraction process from end-to-end. To address this unmet need, we present FlaMBé (Flow annotations for Multiverse Biological entities), a collection of expert-curated datasets across a series of complementary tasks that capture procedural knowledge in biomedicaltexts. This dataset is inspired by the observation that one ubiquitous source of procedural knowledge that is described as unstructuredtextis within academic papers describing their methodology. The workflows annotated in FlaMBé are fromtextsin the burgeoning field of single cell research, a research area that has become notorious for the number of software tools and complexity of workflows used. Additionally, FlaMBé provides, to our knowledge, the largest manually curated named entity recognition (NER) and disambiguation (NED) datasets for tissue/cell type, a fundamental biological entity that is critical for knowledge extraction in the biomedical research domain. Beyond providing a valuable dataset to enable further development of NLP models for procedural knowledge extraction, automating the process of workflowminingalso has important implications for advancing reproducibility in biomedical research.","['Ruth Dannenfelser', 'Jeffrey Zhong', 'Ran Zhang', 'Vicky Yao']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.01812,Anomali
The Power of Patents: Leveraging Text Mining and Social Network Analysis to Forecast IoT Trends,"Technology has become an indispensable competitive tool as science and technology have progressed throughout history. Organizations can compete on an equal footing by implementing technology appropriately. Technology trends or technology lifecycles begin during the initiation phase. Finally, it reaches saturation after entering the maturity phase. As technology reaches saturation, it will be removed or replaced by another. This makes investing in technologies during this phase unjustifiable. Technology forecasting is a critical tool for research and development to determine the future direction of technology. Based on registered patents, this study examined the trends of IOT technologies. A total of 3697 patents related to the Internet of Things from the last six years of patenting have been gathered using lens.org for this purpose. The main people and companies were identified through the creation of the IOT patent registration cooperation network, and the main groups active in patent registration were identified by the community detection technique. The patents were then divided into six technology categories: Safety and Security, Information Services, Public Safety and Environment Monitoring, Collaborative Aware Systems, Smart Homes/Buildings, and Smart Grid. And their technical maturity was identified and examined using the Sigma Plot program. Based on the findings, information services technologies are in the saturation stage, while both smart homes/buildings, and smart grid technologies are in the saturation stage. Three technologies, Safety and Security, Public Safety and Environment Monitoring, and Collaborative Aware Systems are in the maturity stage.","['Mehrdad Maghsoudi', 'Reza Nourbakhsh', 'Mehrdad Agha Mohammadali Kermani', 'Rahim Khanizad']",,arXiv,2023,https://doi.org/10.48550/arXiv.2309.00707,Anomali
Insights Into the Nutritional Prevention of Macular Degeneration based on a Comparative Topic Modeling Approach,"Topic modeling andtextminingare subsets of Natural Language Processing (NLP) with relevance for conducting meta-analysis (MA) and systematic review (SR). For evidence synthesis, the above NLP methods are conventionally used for topic-specific literature searches or extracting values from reports to automate essential phases of SR and MA. Instead, this work proposes a comparative topic modeling approach to analyze reports of contradictory results on the same general research question. Specifically, the objective is to identify topics exhibiting distinct associations with significant results for an outcome of interest by ranking them according to their proportional occurrence in (and consistency of distribution across) reports of significant effects. The proposed method was tested on broad-scope studies addressing whether supplemental nutritional compounds significantly benefit macular degeneration (MD). Four of these were further supported in terms of effectiveness upon conducting a follow-up literature search for validation (omega-3 fatty acids, copper, zeaxanthin, and nitrates). The two not supported by the follow-up literature search (niacin and molybdenum) also had scores in the lowest range under the proposed scoring system, suggesting that the proposed methods score for a given topic may be a viable proxy for its degree of association with the outcome of interest and can be helpful in the search for potentially causal relationships. These results underpin the proposed methods potential to add specificity in understanding effects from broad-scope reports, elucidate topics of interest for future research, and guide evidence synthesis in a systematic and scalable way. All of this is accomplished while yielding valuable insights into the prevention of MD.",['Lucas Cassiel Jacaruso'],,arXiv,2023,https://doi.org/10.48550/arXiv.2309.00312,Anomali
Large Language Models as Data Preprocessors,"Large Language Models (LLMs), typified by OpenAI's GPT, have marked a significant advancement in artificial intelligence. Trained on vast amounts oftextdata, LLMs are capable of understanding and generating human-liketextacross a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in dataminingand analytics applications. Aiming at tabular data, we delve into the applicability of state-of-the-art LLMs such as GPT-4 and GPT-4o for a series of preprocessing tasks, including error detection, data imputation, schema matching, and entity matching. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the performance and efficiency of these models. The effectiveness of LLMs in data preprocessing is evaluated through an experimental study spanning a variety of public datasets. GPT-4 emerged as a standout, achieving 100\% accuracy or F1 score on 4 of these datasets, suggesting LLMs' immense potential in these tasks. Despite certain limitations, our study underscores the promise of LLMs in this domain and anticipates future developments to overcome current hurdles.","['Haochen Zhang', 'Yuyang Dong', 'Chuan Xiao', 'Masafumi Oyamada']",,arXiv,2024,https://doi.org/10.48550/arXiv.2308.16361,Anomali
CoVR-2: Automatic Data Construction for Composed Video Retrieval,"Composed Image Retrieval (CoIR) has recently gained popularity as a task that considers bothtextand image queries together, to search for relevant images in a database. Most CoIR approaches require manually annotated datasets, comprising image-text-image triplets, where thetextdescribes a modification from the query image to the target image. However, manual curation of CoIR triplets is expensive and prevents scalability. In this work, we instead propose a scalable automatic dataset creation methodology that generates triplets given video-caption pairs, while also expanding the scope of the task to include composed video retrieval (CoVR). To this end, weminepaired videos with a similar caption from a large database, and leverage a large language model to generate the corresponding modificationtext. Applying this methodology to the extensive WebVid2M collection, we automatically construct our WebVid-CoVR dataset, resulting in 1.6 million triplets. Moreover, we introduce a new benchmark for CoVR with a manually annotated evaluation set, along with baseline results. We further validate that our methodology is equally applicable to image-caption pairs, by generating 3.3 million CoIR training triplets using the Conceptual Captions dataset. Our model builds on BLIP-2 pretraining, adapting it to composed video (or image) retrieval, and incorporates an additional caption retrieval loss to exploit extra supervision beyond the triplet. We provide extensive ablations to analyze the design choices on our new CoVR benchmark. Our experiments also demonstrate that training a CoVR model on our datasets effectively transfers to CoIR, leading to improved state-of-the-art performance in the zero-shot setup on the CIRR, FashionIQ, and CIRCO benchmarks. Our code, datasets, and models are publicly available at https://imagine.enpc.fr/~ventural/covr/.","['Lucas Ventura', 'Antoine Yang', 'Cordelia Schmid', 'Gül Varol']",IEEE Transactions on Pattern Analysis and Machine Intelligence (2024),arXiv,2024,https://doi.org/10.1109/TPAMI.2024.3463799,Anomali
Biomedical Entity Linking with Triple-aware Pre-Training,"Linking biomedical entities is an essential aspect in biomedical natural language processing tasks, such astextminingand question answering. However, a difficulty of linking the biomedical entities using current large language models (LLM) trained on a general corpus is that biomedical entities are scarcely distributed intextsand therefore have been rarely seen during training by the LLM. At the same time, those LLMs are not aware of high level semantic connection between different biomedical entities, which are useful in identifying similar concepts in different textual contexts. To cope with aforementioned problems, some recent works focused on injecting knowledge graph information into LLMs. However, former methods either ignore the relational knowledge of the entities or lead to catastrophic forgetting. Therefore, we propose a novel framework to pre-train the powerful generative LLM by a corpus synthesized from a KG. In the evaluations we are unable to confirm the benefit of including synonym, description or relational information.","['Xi Yan', 'Cedric Möller', 'Ricardo Usbeck']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.14429,Anomali
Feature Extraction Using Deep Generative Models for Bangla Text Classification on a New Comprehensive Dataset,"The selection of features fortextclassification is a fundamental task intextminingand information retrieval. Despite being the sixth most widely spoken language in the world, Bangla has received little attention due to the scarcity oftextdatasets. In this research, we collected, annotated, and prepared a comprehensive dataset of 212,184 Bangla documents in seven different categories and made it publicly accessible. We implemented three deep learning generative models: LSTM variational autoencoder (LSTM VAE), auxiliary classifier generative adversarial network (AC-GAN), and adversarial autoencoder (AAE) to extracttextfeatures, although their applications are initially found in the field of computer vision. We utilized our dataset to train these three models and used the feature space obtained in the document classification task. We evaluated the performance of the classifiers and found that the adversarial autoencoder model produced the best feature space.","['Md. Rafi-Ur-Rashid', 'Sami Azam', 'Mirjam Jonkman']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.13545,Anomali
DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion,"This paper outlines the performance evaluation of a system for adverse drug event normalization, developed by the Data Science for Digital Health (DS4DH) group for the Social MediaMiningfor Health Applications (SMM4H) 2023 shared task 5. Shared task 5 targeted the normalization of adverse drug event mentions in Twitter to standard concepts of the Medical Dictionary for Regulatory Activities terminology. Our system hinges on a two-stage approach: BERT fine-tuning for entity recognition, followed by zero-shot normalization using sentence transformers and reciprocal-rank fusion. The approach yielded a precision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed the median performance in shared task 5 by 10% and demonstrated the highest performance among all participants. These results substantiate the effectiveness of our approach and its potential application for adverse drug event normalization in the realm of social mediatextmining.","['Anthony Yazdani', 'Hossein Rouhizadeh', 'David Vicente Alvarez', 'Douglas Teodoro']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.12877,Anomali
LKPNR: LLM and KG for Personalized News Recommendation Framework,"Accurately recommending candidate news articles to users is a basic challenge faced by personalized news recommendation systems. Traditional methods are usually difficult to grasp the complex semantic information in newstexts, resulting in unsatisfactory recommendation results. Besides, these traditional methods are more friendly to active users with rich historical behaviors. However, they can not effectively solve the ""long tail problem"" of inactive users. To address these issues, this research presents a novel general framework that combines Large Language Models (LLM) and Knowledge Graphs (KG) into semantic representations of traditional methods. In order to improve semantic understanding in complex newstexts, we use LLMs' powerfultextunderstanding ability to generate news representations containing rich semantic information. In addition, our method combines the information about news entities andmineshigh-order structural information through multiple hops in KG, thus alleviating the challenge of long tail distribution. Experimental results demonstrate that compared with various traditional models, the framework significantly improves the recommendation effect. The successful integration of LLM and KG in our framework has established a feasible path for achieving more accurate personalized recommendations in the news field. Our code is available at https://github.com/Xuan-ZW/LKPNR.","['Chen hao', 'Xie Runfeng', 'Cui Xiangyang', 'Yan Zhou', 'Wang Xin', 'Xuan Zhanwei', 'Zhang Kai']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.12028,Anomali
Progressive Feature Mining and External Knowledge-Assisted Text-Pedestrian Image Retrieval,"Text-Pedestrian Image Retrieval aims to use thetextdescribing pedestrian appearance to retrieve the corresponding pedestrian image. This task involves not only modality discrepancy, but also the challenge of the textual diversity of pedestrians with the same identity. At present, although existing research progress has been made intext-pedestrian image retrieval, these methods do not comprehensively consider the above-mentioned problems. Considering these, this paper proposes a progressive featureminingand external knowledge-assisted feature purification method. Specifically, we use a progressiveminingmode to enable the model tominediscriminative features from neglected information, thereby avoiding the loss of discriminative information and improving the expression ability of features. In addition, to further reduce the negative impact of modal discrepancy andtextdiversity on cross-modal matching, we propose to use other sample knowledge of the same modality, i.e., external knowledge to enhance identity-consistent features and weaken identity-inconsistent features. This process purifies features and alleviates the interference caused by textual diversity and negative sample correlation features of the same modal. Extensive experiments on three challenging datasets demonstrate the effectiveness and superiority of the proposed method, and the retrieval performance even surpasses that of the large-scale model-based method on large-scale datasets.","['Huafeng Li', 'Shedan Yang', 'Yafei Zhang', 'Dapeng Tao', 'Zhengtao Yu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.11994,Anomali
BELB: a Biomedical Entity Linking Benchmark,"Biomedical entity linking (BEL) is the task of grounding entity mentions to a knowledge base. It plays a vital role in information extraction pipelines for the life sciences literature. We review recent work in the field and find that, as the task is absent from existing benchmarks for biomedicaltextmining, different studies adopt different experimental setups making comparisons based on published numbers problematic. Furthermore, neural systems are tested primarily on instances linked to the broad coverage knowledge base UMLS, leaving their performance to more specialized ones, e.g. genes or variants, understudied. We therefore developed BELB, a Biomedical Entity Linking Benchmark, providing access in a unified format to 11 corpora linked to 7 knowledge bases and spanning six entity types: gene, disease, chemical, species, cell line and variant. BELB greatly reduces preprocessing overhead in testing BEL systems on multiple corpora offering a standardized testbed for reproducible experiments. Using BELB we perform an extensive evaluation of six rule-based entity-specific systems and three recent neural approaches leveraging pre-trained language models. Our results reveal a mixed picture showing that neural approaches fail to perform consistently across entity types, highlighting the need of further studies towards entity-agnostic models.","['Samuele Garda', 'Leon Weber-Genzel', 'Robert Martin', 'Ulf Leser']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.11537,Anomali
DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment,"Cross-modal garment synthesis and manipulation will significantly benefit the way fashion designers generate garments and modify their designs via flexible linguistic interfaces.Current approaches follow the generaltext-to-image paradigm andminecross-modal relations via simple cross-attention modules, neglecting the structural correspondence between visual and textual representations in the fashion design domain. In this work, we instead introduce DiffCloth, a diffusion-based pipeline for cross-modal garment synthesis and manipulation, which empowers diffusion models with flexible compositionality in the fashion domain by structurally aligning the cross-modal semantics. Specifically, we formulate the part-level cross-modal alignment as a bipartite matching problem between the linguistic Attribute-Phrases (AP) and the visual garment parts which are obtained via constituency parsing and semantic segmentation, respectively. To mitigate the issue of attribute confusion, we further propose a semantic-bundled cross-attention to preserve the spatial structure similarities between the attention maps of attribute adjectives and part nouns in each AP. Moreover, DiffCloth allows for manipulation of the generated results by simply replacing APs in thetextprompts. The manipulation-irrelevant regions are recognized by blended masks obtained from the bundled attention maps of the APs and kept unchanged. Extensive experiments on the CM-Fashion benchmark demonstrate that DiffCloth both yields state-of-the-art garment synthesis results by leveraging the inherent structural information and supports flexible manipulation with region consistency.","['Xujie Zhang', 'Binbin Yang', 'Michael C. Kampffmeyer', 'Wenqing Zhang', 'Shiyue Zhang', 'Guansong Lu', 'Liang Lin', 'Hang Xu', 'Xiaodan Liang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.11206,Anomali
Economic Policy Uncertainty: A Review on Applications and Measurement Methods with Focus on Text Mining Methods,"Economic Policy Uncertainty (EPU) represents the uncertainty realized by the investors during economic policy alterations. EPU is a critical indicator in economic studies to predict future investments, the unemployment rate, and recessions. EPU values can be estimated based on financial parameters directly or implied uncertainty indirectly using thetextminingmethods. Although EPU is a well-studied topic within the economy, the methods utilized to measure it are understudied. In this article, we define the EPU briefly and review the methods used to measure the EPU, and survey the areas influenced by the changes in EPU level. We divide the EPU measurement methods into three major groups with respect to their input data. Examples of each group of methods are enlisted, and the pros and cons of the groups are discussed. Among the EPU measures,textmining-based ones are dominantly studied. These methods measure the realized uncertainty by taking into account the uncertainty represented in the news and publicly available sources of financial information. Finally, we survey the research areas that rely on measuring the EPU index with the hope that studying the impacts of uncertainty would attract further attention of researchers from various research fields. In addition, we propose a list of future research approaches focusing on measuring EPU using textual material.","['Fatemeh Kaveh-Yazdy', 'Sajjad Zarifzadeh']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.10304,Anomali
How Do Java Developers Reuse StackOverflow Answers in Their GitHub Projects?,"StackOverflow (SO) is a widely used question-and-answer (Q\&A) website for software developers and computer scientists. GitHub is an online development platform used for storing, tracking, and collaborating on software projects. Prior work relates the informationminedfrom both platforms to link user accounts or compare developers' activities across platforms. However, not much work is done to characterize the SO answers reused by GitHub projects. For this paper, we did an empirical study byminingthe SO answers reused by Java projects available on GitHub. We created a hybrid approach of clone detection, keyword-based search, and manual inspection, to identify the answer(s) actually leveraged by developers. Based on the identified answers, we further studied topics of the discussion threads, answer characteristics (e.g., scores, ages, code lengths, andtextlengths), and developers' reuse practices.
  We observed that most reused answers offer programs to implement specific coding tasks. Among all analyzed SO discussion threads, the reused answers often have relatively higher scores, older ages, longer code, and longertextthan unused answers. In only 9% of scenarios (40/430), developers fully copied answer code for reuse. In the remaining scenarios, they reused partial code or created brand new code from scratch. Our study characterized 130 SO discussion threads referred to by Java developers in 357 GitHub projects. Our empirical findings can guide SO answerers to provide better answers, and shed lights on future research related to SO and GitHub.","['Juntong Chen', 'Yan Zhao', 'Na Meng']",,arXiv,2024,https://doi.org/10.48550/arXiv.2308.09573,Anomali
Characterizing Information Seeking Events in Health-Related Social Discourse,"Social media sites have become a popular platform for individuals to seek and share health information. Despite the progress in natural language processing for social mediamining, a gap remains in analyzing health-relatedtextson social discourse in the context of events. Event-driven analysis can offer insights into different facets of healthcare at an individual and collective level, including treatment options, misconceptions, knowledge gaps, etc. This paper presents a paradigm to characterize health-related information-seeking in social discourse through the lens of events. Events here are board categories defined with domain experts that capture the trajectory of the treatment/medication. To illustrate the value of this approach, we analyze Reddit posts regarding medications for Opioid Use Disorder (OUD), a critical global health concern. To the best of our knowledge, this is the first attempt to define event categories for characterizing information-seeking in OUD social discourse. Guided by domain experts, we develop TREAT-ISE, a novel multilabel treatment information-seeking event dataset to analyze online discourse on an event-based framework. This dataset contains Reddit posts on information-seeking events related to recovery from OUD, where each post is annotated based on the type of events. We also establish a strong performance benchmark (77.4% F1 score) for the task by employing several machine learning and deep learning classifiers. Finally, we thoroughly investigate the performance and errors of ChatGPT on this task, providing valuable insights into the LLM's capabilities and ongoing characterization efforts.","['Omar Sharif', 'Madhusudan Basak', 'Tanzia Parvin', 'Ava Scharfstein', 'Alphonso Bradham', 'Jacob T. Borodovsky', 'Sarah E. Lord', 'Sarah M. Preum']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.09156,Anomali
Event-Guided Procedure Planning from Instructional Videos with Text Supervision,"In this work, we focus on the task of procedure planning from instructional videos withtextsupervision, where a model aims to predict an action sequence to transform the initial visual state into the goal visual state. A critical challenge of this task is the large semantic gap between observed visual states and unobserved intermediate actions, which is ignored by previous works. Specifically, this semantic gap refers to that the contents in the observed visual states are semantically different from the elements of some actiontextlabels in a procedure. To bridge this semantic gap, we propose a novel event-guided paradigm, which first infers events from the observed states and then plans out actions based on both the states and predicted events. Our inspiration comes from that planning a procedure from an instructional video is to complete a specific event and a specific event usually involves specific actions. Based on the proposed paradigm, we contribute an Event-guided Prompting-based Procedure Planning (E3P) model, which encodes event information into the sequential modeling process to support procedure planning. To further consider the strong action associations within each event, our E3P adopts a mask-and-predict approach for relationmining, incorporating a probabilistic masking scheme for regularization. Extensive experiments on three datasets demonstrate the effectiveness of our proposed model.","['An-Lan Wang', 'Kun-Yu Lin', 'Jia-Run Du', 'Jingke Meng', 'Wei-Shi Zheng']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.08885,Anomali
Real-Time Construction Algorithm of Co-Occurrence Network Based on Inverted Index,"Co-occurrence networks are an important method in the field of natural language processing andtextminingfor discovering semantic relationships withintexts. However, the traditional traversal algorithm for constructing co-occurrence networks has high time complexity and space complexity when dealing with large-scaletextdata. In this paper, we propose an optimized algorithm based on inverted indexing and breadth-first search to improve the efficiency of co-occurrence network construction and reduce memory consumption. Firstly, the traditional traversal algorithm is analyzed, and its performance issues in constructing co-occurrence networks are identified. Then, the detailed implementation process of the optimized algorithm is presented. Subsequently, the CSL large-scale Chinese scientific literature dataset is used for experimental validation, comparing the performance of the traditional traversal algorithm and the optimized algorithm in terms of running time and memory usage. Finally, using non-parametric test methods, the optimized algorithm is proven to have significantly better performance than the traditional traversal algorithm. The research in this paper provides an effective method for the rapid construction of co-occurrence networks, contributing to the further development of the Information Organization fields.",['Jiahao Cheng'],,arXiv,2023,https://doi.org/10.48550/arXiv.2308.08756,Anomali
Ranking-aware Uncertainty for Text-guided Image Retrieval,"Text-guided image retrieval is to incorporate conditionaltextto better capture users' intent. Traditionally, the existing methods focus on minimizing the embedding distances between the source inputs and the targeted image, using the provided triplets $\langle$source image, sourcetext, target image$\rangle$. However, such triplet optimization may limit the learned retrieval model to capture more detailed ranking information, e.g., the triplets are one-to-one correspondences and they fail to account for many-to-many correspondences arising from semantic diversity in feedback languages and images. To capture more ranking information, we propose a novel ranking-aware uncertainty approach to model many-to-many correspondences by only using the provided triplets. We introduce uncertainty learning to learn the stochastic ranking list of features. Specifically, our approach mainly comprises three components: (1) In-sample uncertainty, which aims to capture semantic diversity using a Gaussian distribution derived from both combined and target features; (2) Cross-sample uncertainty, which furtherminesthe ranking information from other samples' distributions; and (3) Distribution regularization, which aligns the distributional representations of source inputs and targeted image. Compared to the existing state-of-the-art methods, our proposed method achieves significant results on two public datasets for composed image retrieval.","['Junyang Chen', 'Hanjiang Lai']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.08131,Anomali
BI-LAVA: Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis,"In the biomedical domain, taxonomies organize the acquisition modalities of scientific images in hierarchical structures. Such taxonomies leverage large sets of correct image labels and provide essential information about the importance of a scientific publication, which could then be used in biocuration tasks. However, the hierarchical nature of the labels, the overhead of processing images, the absence or incompleteness of labeled data, and the expertise required to label this type of data impede the creation of useful datasets for biocuration. From a multi-year collaboration with biocurators andtext-miningresearchers, we derive an iterative visual analytics and active learning strategy to address these challenges. We implement this strategy in a system called BI-LAVA Biocuration with Hierarchical Image Labeling through Active Learning and Visual Analysis. BI-LAVA leverages a small set of image labels, a hierarchical set of image classifiers, and active learning to help model builders deal with incomplete ground-truth labels, target a hierarchical taxonomy of image modalities, and classify a large pool of unlabeled images. BI-LAVA's front end uses custom encodings to represent data distributions, taxonomies, image projections, and neighborhoods of image thumbnails, which help model builders explore an unfamiliar image dataset and taxonomy and correct and generate labels. An evaluation with machine learning practitioners shows that our mixed human-machine approach successfully supports domain experts in understanding the characteristics of classes within the taxonomy, as well as validating and improving data quality in labeled and unlabeled collections.","['Juan Trelles', 'Andrew Wentzel', 'William Berrios', 'G. Elisabeta Marai']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.08003,Anomali
Computational reproducibility of Jupyter notebooks from biomedical publications,"Jupyter notebooks facilitate the bundling of executable code with its documentation and output in one interactive environment, and they represent a popular mechanism to document and share computational workflows. The reproducibility of computational aspects of research is a key component of scientific reproducibility but has not yet been assessed at scale for Jupyter notebooks associated with biomedical publications. We address computational reproducibility at two levels: First, using fully automated workflows, we analyzed the computational reproducibility of Jupyter notebooks related to publications indexed in PubMed Central. We identified such notebooks byminingthe articles fulltext, locating them on GitHub and re-running them in an environment as close to the original as possible. We documented reproduction success and exceptions and explored relationships between notebook reproducibility and variables related to the notebooks or publications. Second, this study represents a reproducibility attempt in and of itself, using essentially the same methodology twice on PubMed Central over two years. Out of 27271 notebooks from 2660 GitHub repositories associated with 3467 articles, 22578 notebooks were written in Python, including 15817 that had their dependencies declared in standard requirement files and that we attempted to re-run automatically. For 10388 of these, all declared dependencies could be installed successfully, and we re-ran them to assess reproducibility. Of these, 1203 notebooks ran through without any errors, including 879 that produced results identical to those reported in the original notebook and 324 for which our results differed from the originally reported ones. Running the other notebooks resulted in exceptions. We zoom in on common problems, highlight trends and discuss potential improvements to Jupyter-related workflows associated with biomedical publications.","['Sheeba Samuel', 'Daniel Mietchen']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.07333,Anomali
Multi-modal Multi-view Clustering based on Non-negative Matrix Factorization,"By combining related objects, unsupervised machine learning techniques aim to reveal the underlying patterns in a data set. Non-negative Matrix Factorization (NMF) is a dataminingtechnique that splits data matrices by imposing restrictions on the elements' non-negativity into two matrices: one representing the data partitions and the other to represent the cluster prototypes of the data set. This method has attracted a lot of attention and is used in a wide range of applications, includingtextmining, clustering, language modeling, music transcription, and neuroscience (gene separation). The interpretation of the generated matrices is made simpler by the absence of negative values. In this article, we propose a study on multi-modal clustering algorithms and present a novel method called multi-modal multi-view non-negative matrix factorization, in which we analyze the collaboration of several local NMF models. The experimental results show the value of the proposed approach, which was evaluated using a variety of data sets, and the obtained results are very promising compared to state of art methods.","['Yasser Khalafaoui', 'Nistor Grozavu', 'Basarab Matei', 'Laurent-Walter Goix']","2022 IEEE Symposium Series on Computational Intelligence (SSCI), Dec 2022, Singapore, Singapore. pp.1386-1391",arXiv,2023,https://doi.org/10.48550/arXiv.2308.04778,Anomali
Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach,"In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraudtexts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for thetextclassification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuable insights for industry practitioners, regulators, and researchers in the pursuit of more robust and effective fraud detection methodologies.",['Qiuru Li'],,arXiv,2023,https://doi.org/10.48550/arXiv.2308.03800,Anomali
MedMine: Examining Pre-trained Language Models on Medication Mining,"Automatic medicationminingfrom clinical and biomedicaltexthas become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medicationminingshared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or improve their overall accuracy by ensemble learning and data augmentation. MedMine is part of the M3 Initiative \url{https://github.com/HECTA-UoM/M3}","['Haifa Alrdahi', 'Lifeng Han', 'Hendrik Šuvalov', 'Goran Nenadic']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.03629,Anomali
Analysis of the Evolution of Advanced Transformer-Based Language Models: Experiments on Opinion Mining,"Opinionmining, also known as sentiment analysis, is a subfield of natural language processing (NLP) that focuses on identifying and extracting subjective information in textual material. This can include determining the overall sentiment of a piece oftext(e.g., positive or negative), as well as identifying specific emotions or opinions expressed in thetext, that involves the use of advanced machine and deep learning techniques. Recently, transformer-based language models make this task of human emotion analysis intuitive, thanks to the attention mechanism and parallel computation. These advantages make such models very powerful on linguistic tasks, unlike recurrent neural networks that spend a lot of time on sequential processing, making them prone to fail when it comes to processing longtext. The scope of our paper aims to study the behaviour of the cutting-edge Transformer-based language models on opinionminingand provide a high-level comparison between them to highlight their key particularities. Additionally, our comparative study shows leads and paves the way for production engineers regarding the approach to focus on and is useful for researchers as it provides guidelines for future research subjects.","['Nour Eddine Zekaoui', 'Siham Yousfi', 'Maryem Rhanoui', 'Mounia Mikram']","IAES Int. J. Artif. Intell. (IJ-AI) 12(4), 1995-2010 (Dec. 2023)",arXiv,2023,https://doi.org/10.48550/arXiv.2308.03235,Anomali
BioBERT Based SNP-traits Associations Extraction from Biomedical Literature,"Scientific literature contains a considerable amount of information that provides an excellent opportunity for developingtextminingmethods to extract biomedical relationships. An important type of information is the relationship between singular nucleotide polymorphisms (SNP) and traits. In this paper, we present a BioBERT-GRU method to identify SNP- traits associations. Based on the evaluation of our method on the SNPPhenA dataset, it is concluded that this new method performs better than previous machine learning and deep learning based methods. BioBERT-GRU achieved the result a precision of 0.883, recall of 0.882 and F1-score of 0.881.","['Mohammad Dehghani', 'Behrouz Bokharaeian', 'Zahra Yazdanparast']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.02569,Anomali
Industrial Memories: Exploring the Findings of Government Inquiries with Neural Word Embedding and Machine Learning,"We present atextminingsystem to support the exploration of large volumes oftextdetailing the findings of government inquiries. Despite their historical significance and potential societal impact, key findings of inquiries are often hidden within lengthy documents and remain inaccessible to the general public. We transform the findings of the Irish government's inquiry into industrial schools and through the use of word embedding,textclassification and visualisation, present an interactive web-based platform that enables the exploration of thetextto uncover new historical insights.","['Susan Leavy', 'Emilie Pine', 'Mark T Keane']","ECML PKDD 2018. Lecture Notes in Computer Science(), vol 11053. Springer, Cham",arXiv,2023,https://doi.org/10.48550/arXiv.2308.02556,Anomali
Chinese Financial Text Emotion Mining: GCGTS -- A Character Relationship-based Approach for Simultaneous Aspect-Opinion Pair Extraction,"Aspect-Opinion Pair Extraction (AOPE) from Chinese financialtextsis a specialized task in fine-grainedtextsentiment analysis. The main objective is to extract aspect terms and opinion terms simultaneously from a diverse range of financialtexts. Previous studies have mainly focused on developing grid annotation schemes within grid-based models to facilitate this extraction process. However, these methods often rely on character-level (token-level) feature encoding, which may overlook the logical relationships between Chinese characters within words. To address this limitation, we propose a novel method called Graph-based Character-level Grid Tagging Scheme (GCGTS). The GCGTS method explicitly incorporates syntactic structure using Graph Convolutional Networks (GCN) and unifies the encoding of characters within the same syntactic semantic unit (Chinese word level). Additionally, we introduce an image convolutional structure into the grid model to better capture the local relationships between characters within evaluation units. This innovative structure reduces the excessive reliance on pre-trained language models and emphasizes the modeling of structure and local relationships, thereby improving the performance of the model on Chinese financialtexts. Through comparative experiments with advanced models such as Synchronous Double-channel Recurrent Network (SDRN) and Grid Tagging Scheme (GTS), the proposed GCGTS model demonstrates significant improvements in performance.","['Qi Chen', 'Dexi Liu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.02113,Anomali
A short review of the main concerns in A.I. development and application within the public sector supported by NLP and TM,"Artificial Intelligence is not a new subject, and business, industry and public sectors have used it in different ways and contexts and considering multiple concerns. This work reviewed research papers published in ACM Digital Library and IEEE Xplore conference proceedings in the last two years supported by fundamental concepts of Natural Language Processing (NLP) andTextMining(TM). The objective was to capture insights regarding data privacy, ethics, interpretability, explainability, trustworthiness, and fairness in the public sector. The methodology has saved analysis time and could retrieve papers containing relevant information. The results showed that fairness was the most frequent concern. The least prominent topic was data privacy (although embedded in most articles), while the most prominent was trustworthiness. Finally, gathering helpful insights about those concerns regarding A.I. applications in the public sector was also possible.",['Carlos Ferreira'],,arXiv,2023,https://doi.org/10.48550/arXiv.2308.02042,Anomali
Trading and wealth evolution in the Proof of Stake protocol,"With the increasing adoption of the Proof of Stake (PoS) blockchain, it is timely to study the economy created by such blockchain. In this chapter, we will survey recent progress on the trading and wealth evolution in a cryptocurrency where the new coins are issued according to the PoS protocol. We first consider the wealth evolution in the PoS protocol assuming no trading, and focus on the problem of decentralisation. Next we consider each miner's trading incentive and strategy through the lens of optimal control, where the miner needs to trade off PoSminingand trading. Finally, we study the collective behavior of the miners in a PoS trading environment by a mean field model. We use both stochastic and analytic tools in our study. A list of open problems are also presented.",['Wenpin Tang'],,arXiv,2023,https://doi.org/10.48550/arXiv.2308.01803,Anomali
Supply chain emission estimation using large language models,"Large enterprises face a crucial imperative to achieve the Sustainable Development Goals (SDGs), especially goal 13, which focuses on combating climate change and its impacts. To mitigate the effects of climate change, reducing enterprise Scope 3 (supply chain emissions) is vital, as it accounts for more than 90\% of total emission inventories. However, tracking Scope 3 emissions proves challenging, as data must be collected from thousands of upstream and downstream suppliers.To address the above mentioned challenges, we propose a first-of-a-kind framework that uses domain-adapted NLP foundation models to estimate Scope 3 emissions, by utilizing financial transactions as a proxy for purchased goods and services. We compared the performance of the proposed framework with the state-of-arttextclassification models such as TF-IDF, word2Vec, and Zero shot learning. Our results show that the domain-adapted foundation model outperforms state-of-the-arttextminingtechniques and performs as well as a subject matter expert (SME). The proposed framework could accelerate the Scope 3 estimation at Enterprise scale and will help to take appropriate climate actions to achieve SDG 13.","['Ayush Jain', 'Manikandan Padmanaban', 'Jagabondhu Hazra', 'Shantanu Godbole', 'Kommy Weldemariam']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.01741,Anomali
Evaluating ChatGPT text-mining of clinical records for obesity monitoring,"Background: Veterinary clinical narratives remain a largely untapped resource for addressing complex diseases. Here we compare the ability of a large language model (ChatGPT) and a previously developed regular expression (RegexT) to identify overweight body condition scores (BCS) in veterinary narratives. Methods: BCS values were extracted from 4,415 anonymised clinical narratives using either RegexT or by appending the narrative to a prompt sent to ChatGPT coercing the model to return the BCS information. Data were manually reviewed for comparison. Results: The precision of RegexT was higher (100%, 95% CI 94.81-100%) than the ChatGPT (89.3%; 95% CI82.75-93.64%). However, the recall of ChatGPT (100%. 95% CI 96.18-100%) was considerably higher than that of RegexT (72.6%, 95% CI 63.92-79.94%). Limitations: Subtle prompt engineering is needed to improve ChatGPT output. Conclusions: Large language models create diverse opportunities and, whilst complex, present an intuitive interface to information but require careful implementation to avoid unpredictable errors.","['Ivo S. Fins', 'Heather Davies', 'Sean Farrell', 'Jose R. Torres', 'Gina Pinchbeck', 'Alan D. Radford', 'Peter-John Noble']",,arXiv,2023,https://doi.org/10.48550/arXiv.2308.01666,Anomali
A new mapping of technological interdependence,"How does technological interdependence affect innovation? We address this question by examining the influence of neighbors' innovativeness and the structure of the innovators' network on a sector's capacity to develop new technologies. We study these two dimensions of technological interdependence by applying novel methods oftextminingand network analysis to the documents of 6.5 million patents granted by the United States Patent and Trademark Office (USPTO) between 1976 and 2021. We find that, in the long run, the influence of network linkages is as important as that of neighbor innovativeness. In the short run, however, positive shocks to neighbor innovativeness yield relatively rapid effects, while the impact of shocks strengthening network linkages manifests with delay, even though lasts longer. Our analysis also highlights that patenttextcontains a wealth of information often not captured by traditional innovation metrics, such as patent citations.","['A. Fronzetti Colladon', 'B. Guardabascio', 'F. Venturini']","Research Policy 54(1), 105126 (2025)",arXiv,2024,https://doi.org/10.48550/arXiv.2308.00014,Anomali
UnScientify: Detecting Scientific Uncertainty in Scholarly Full Text,"This demo paper presents UnScientify, an interactive system designed to detect scientific uncertainty in scholarly fulltext. The system utilizes a weakly supervised technique that employs a fine-grained annotation scheme to identify verbally formulated uncertainty at the sentence level in scientifictexts. The pipeline for the system includes a combination of pattern matching, complex sentence checking, and authorial reference checking. Our approach automates labeling and annotation tasks for scientific uncertainty identification, taking into account different types of scientific uncertainty, that can serve various applications such as information retrieval,textmining, and scholarly document processing. Additionally, UnScientify provides interpretable results, aiding in the comprehension of identified instances of scientific uncertainty intext.","['Panggih Kusuma Ningrum', 'Philipp Mayr', 'Iana Atanassova']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.14236,Anomali
"Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection","Human-Object Interaction (HOI) detection is a challenging computer vision task that requires visual models to address the complex interactive relationship between humans and objects and predict HOI triplets. Despite the challenges posed by the numerous interaction combinations, they also offer opportunities for multimodal learning of visualtexts. In this paper, we present a systematic and unified framework (RmLR) that enhances HOI detection by incorporating structuredtextknowledge. Firstly, we qualitatively and quantitatively analyze the loss of interaction information in the two-stage HOI detector and propose a re-miningstrategy to generate more comprehensive visual representation.Secondly, we design more fine-grained sentence- and word-level alignment and knowledge transfer strategies to effectively address the many-to-many matching problem between multiple interactions and multiple texts.These strategies alleviate the matching confusion problem that arises when multiple interactions occur simultaneously, thereby improving the effectiveness of the alignment process. Finally, HOI reasoning by visual features augmented with textual knowledge substantially improves the understanding of interactions. Experimental results illustrate the effectiveness of our approach, where state-of-the-art performance is achieved on public benchmarks. We further analyze the effects of different components of our approach to provide insights into its efficacy.","['Yichao Cao', 'Qingfei Tang', 'Feng Yang', 'Xiu Su', 'Shan You', 'Xiaobo Lu', 'Chang Xu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.13529,Anomali
Text-oriented Modality Reinforcement Network for Multimodal Sentiment Analysis from Unaligned Multimodal Sequences,"Multimodal Sentiment Analysis (MSA) aims tominesentiment information fromtext, visual, and acoustic modalities. Previous works have focused on representation learning and feature fusion strategies. However, most of these efforts ignored the disparity in the semantic richness of different modalities and treated each modality in the same manner. That may lead to strong modalities being neglected and weak modalities being overvalued. Motivated by these observations, we propose aText-oriented Modality Reinforcement Network (TMRN), which focuses on the dominance of thetextmodality in MSA. More specifically, we design aText-Centered Cross-modal Attention (TCCA) module to make full interaction fortext/acoustic andtext/visual pairs, and aText-Gated Self-Attention (TGSA) module to guide the self-reinforcement of the other two modalities. Furthermore, we present an adaptive fusion mechanism to decide the proportion of different modalities involved in the fusion process. Finally, we combine the feature matrices into vectors to get the final representation for the downstream tasks. Experimental results show that our TMRN outperforms the state-of-the-art methods on two MSA benchmarks.","['Yuxuan Lei', 'Dingkang Yang', 'Mingcheng Li', 'Shunli Wang', 'Jiawei Chen', 'Lihua Zhang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.13205,Anomali
Schema-Driven Actionable Insight Generation and Smart Recommendation,"In natural language generation (NLG), insightminingis seen as a data-to-texttask, where data isminedfor interesting patterns and verbalised into 'insight' statements. An 'over-generate and rank' paradigm is intuitively used to generate such insights. The multidimensionality and subjectivity of this process make it challenging. This paper introduces a schema-driven method to generate actionable insights from data to drive growth and change. It also introduces a technique to rank the insights to align with user interests based on their feedback. We show preliminary qualitative results of the insights generated using our technique and demonstrate its ability to adapt to feedback.","['Allmin Susaiyah', 'Aki Härmä', 'Milan Petković']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.13176,Anomali
Opinion Mining Using Population-tuned Generative Language Models,"We present a novel method forminingopinions fromtextcollections using generative language models trained on data collected from different populations. We describe the basic definitions, methodology and a generic algorithm for opinion insightmining. We demonstrate the performance of our method in an experiment where a pre-trained generative model is fine-tuned using specifically tailored content with unnatural and fully annotated opinions. We show that our approach can learn and transfer the opinions to the semantic classes while maintaining the proportion of polarisation. Finally, we demonstrate the usage of an insightminingsystem to scale up the discovery of opinion insights from a realtextcorpus.","['Allmin Susaiyah', 'Abhinay Pandya', 'Aki Härmä']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.13173,Anomali
Explainable Topic-Enhanced Argument Mining from Heterogeneous Sources,"Given a controversial target such as ``nuclear energy'', argumentminingaims to identify the argumentativetextfrom heterogeneous sources. Current approaches focus on exploring better ways of integrating the target-associated semantic information with the argumentativetext. Despite their empirical successes, two issues remain unsolved: (i) a target is represented by a word or a phrase, which is insufficient to cover a diverse set of target-related subtopics; (ii) the sentence-level topic information within an argument, which we believe is crucial for argumentmining, is ignored. To tackle the above issues, we propose a novel explainable topic-enhanced argumentminingapproach. Specifically, with the use of the neural topic model and the language model, the target information is augmented by explainable topic representations. Moreover, the sentence-level topic information within the argument is captured by minimizing the distance between its latent topic distribution and its semantic representation through mutual learning. Experiments have been conducted on the benchmark dataset in both the in-target setting and the cross-target setting. Results demonstrate the superiority of the proposed model against the state-of-the-art baselines.","['Jiasheng Si', 'Yingjie Zhu', 'Xingyu Shi', 'Deyu Zhou', 'Yulan He']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.12131,Anomali
Meta-Transformer: A Unified Framework for Multimodal Learning,"Multimodal learning aims to build models that can process and relate information from multiple modalities. Despite years of development in this field, it still remains challenging to design a unified network for processing various modalities ($\textit{e.g.}$ natural language, 2D images, 3D point clouds, audio, video, time series, tabular data) due to the inherent gaps among them. In this work, we propose a framework, named Meta-Transformer, that leverages a $\textbf{frozen}$ encoder to perform multimodal perception without any paired multimodal training data. In Meta-Transformer, the raw input data from various modalities are mapped into a shared token space, allowing a subsequent encoder with frozen parameters to extract high-level semantic features of the input data. Composed of three main components: a unified data tokenizer, a modality-shared encoder, and task-specific heads for downstream tasks, Meta-Transformer is the first framework to perform unified learning across 12 modalities with unpaired data. Experiments on different benchmarks reveal that Meta-Transformer can handle a wide range of tasks including fundamental perception (text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral, and IMU), and datamining(graph, tabular, and time-series). Meta-Transformer indicates a promising future for developing unified multimodal intelligence with transformers. Code will be available at https://github.com/invictus717/MetaTransformer","['Yiyuan Zhang', 'Kaixiong Gong', 'Kaipeng Zhang', 'Hongsheng Li', 'Yu Qiao', 'Wanli Ouyang', 'Xiangyu Yue']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.10802,Anomali
Large Language Models can accomplish Business Process Management Tasks,"Business Process Management (BPM) aims to improve organizational activities and their outcomes by managing the underlying processes. To achieve this, it is often necessary to consider information from various sources, including unstructured textual documents. Therefore, researchers have developed several BPM-specific solutions that extract information from textual documents using Natural Language Processing techniques. These solutions are specific to their respective tasks and cannot accomplish multiple process-related problems as a general-purpose instrument. However, in light of the recent emergence of Large Language Models (LLMs) with remarkable reasoning capabilities, such a general-purpose instrument with multiple applications now appears attainable. In this paper, we illustrate how LLMs can accomplishtext-related BPM tasks by applying a specific LLM to three exemplary tasks:miningimperative process models from textual descriptions,miningdeclarative process models from textual descriptions, and assessing the suitability of process tasks from textual descriptions for robotic process automation. We show that, without extensive configuration or prompt engineering, LLMs perform comparably to or better than existing solutions and discuss implications for future BPM research as well as practical usage.","['Michael Grohs', 'Luka Abb', 'Nourhan Elsayed', 'Jana-Rebecca Rehse']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.09923,Anomali
ActionPrompt: Action-Guided 3D Human Pose Estimation With Text and Pose Prompting,"Recent 2D-to-3D human pose estimation (HPE) utilizes temporal consistency across sequences to alleviate the depth ambiguity problem but ignore the action related prior knowledge hidden in the pose sequence. In this paper, we propose a plug-and-play module named Action Prompt Module (APM) that effectivelyminesdifferent kinds of action clues for 3D HPE. The highlight is that, theminingscheme of APM can be widely adapted to different frameworks and bring consistent benefits. Specifically, we first present a novel Action-relatedTextPrompt module (ATP) that directly embeds action labels and transfers the rich language information in the label to the pose sequence. Besides, we further introduce Action-specific Pose Prompt module (APP) tominethe position-aware pose pattern of each action, and exploit the correlation between theminedpatterns and input pose sequence for further pose refinement. Experiments show that APM can improve the performance of most video-based 2D-to-3D HPE frameworks by a large margin.","['Hongwei Zheng', 'Han Li', 'Bowen Shi', 'Wenrui Dai', 'Botao Wan', 'Yu Sun', 'Min Guo', 'Hongkai Xiong']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.09026,Anomali
Exploring acceptance of autonomous vehicle policies using KeyBERT and SNA: Targeting engineering students,"This study aims to explore user acceptance of Autonomous Vehicle (AV) policies with improvedtext-miningmethods. Recently, South Korean policymakers have viewed Autonomous Driving Car (ADC) and Autonomous Driving Robot (ADR) as next-generation means of transportation that will reduce the cost of transporting passengers and goods. They support the construction of V2I and V2V communication infrastructures for ADC and recognize that ADR is equivalent to pedestrians to promote its deployment into sidewalks. To fill the gap where end-user acceptance of these policies is not well considered, this study applied twotext-miningmethods to the comments of graduate students in the fields of Industrial, Mechanical, and Electronics-Electrical-Computer. One is the Co-occurrence Network Analysis (CNA) based on TF-IWF and Dice coefficient, and the other is the Contextual Semantic Network Analysis (C-SNA) based on both KeyBERT, which extracts keywords that contextually represent the comments, and double cosine similarity. The reason for comparing these approaches is to balance interest not only in the implications for the AV policies but also in the need to apply qualitytextminingto this research domain. Significantly, the limitation of frequency-basedtextmining, which does not reflect textual context, and the trade-off of adjusting thresholds in Semantic Network Analysis (SNA) were considered. As the results of comparing the two approaches, the C-SNA provided the information necessary to understand users' voices using fewer nodes and features than the CNA. The users who pre-emptively understood the AV policies based on their engineering literacy and the giventextsrevealed potential risks of the AV accident policies. This study adds suggestions to manage these risks to support the successful deployment of AVs on public roads.","['Jinwoo Ha', 'Dongsoo Kim']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.09014,Anomali
Experimental verification of a coherence factorization law for quantum states,"As a quantum resource, quantum coherence plays an important role in modern physics. Many coherence measures and their relations with entanglement have been proposed, and the dynamics of entanglement has been experimentally studied. However, the knowledge of general results for coherence dynamics in open systems is limited. Here we propose a coherence factorization law, which describes the evolution of coherence passing through any noisy channels characterized by genuinely incoherent operations. We use photons to implement the quantum operations and experimentally verify the law for qubits and qutrits. Our work is a step toward the understanding of the evolution of coherence when the system interacts with the environment, and will boost the study of more general laws of coherence.","['Yi Zheng', 'Cheng-Jie Zhang', 'Zheng-Hao Liu', 'Jian-Wei Shao', 'Jin-Shi Xu', 'Chuan-Feng Li', 'Guang-Can Guo']","Photon. Res. 10, 2172 (2022)",arXiv,2023,https://doi.org/10.48550/arXiv.2307.08462,Anomali
Controllable Data Augmentation for Few-Shot Text Mining with Chain-of-Thought Attribute Manipulation,"Prompting large language models (LLMs) for data augmentation has recently become a common practice in few-shot NLP tasks. In this paper, we propose Chain-of-Thought Attribute Manipulation (CoTAM), a novel approach that generates new data from existing examples by only tweaking in the user-provided, task-specific attribute, e.g., sentiment polarity or topic in movie reviews. Instead of conventional latent representation controlling, we leverage the chain-of-thought prompting to directly edit thetextin three steps, (1) attribute decomposition, (2) manipulation proposal, and (3) sentence reconstruction. Extensive results on various tasks, such astext(pair) classification, aspect-based sentiment analysis, and conditionaltextgeneration, verify the superiority of CoTAM over other LLM-based augmentation methods with the same number of training examples for both fine-tuning and in-context learning. Remarkably, the 2D visualization of the augmented dataset using principal component analysis revealed a human-recognizable decision boundary that is likely hinted by the attribute manipulation, demonstrating the potential of our proposed approach.","['Letian Peng', 'Yuwei Zhang', 'Jingbo Shang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2307.07099,Anomali
GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest,"Visual instruction tuning large language model(LLM) on image-textpairs has achieved general-purpose vision-language abilities. However, the lack of region-textpairs limits their advancements to fine-grained multimodal understanding. In this paper, we propose spatial instruction tuning, which introduces the reference to the region-of-interest(RoI) in the instruction. Before sending to LLM, the reference is replaced by RoI features and interleaved with language embeddings as a sequence. Our model GPT4RoI, trained on 7 region-textpair datasets, brings an unprecedented interactive and conversational experience compared to previous image-level models. (1) Interaction beyond language: Users can interact with our model by both language and drawing bounding boxes to flexibly adjust the referring granularity. (2) Versatile multimodal abilities: A variety of attribute information within each RoI can beminedby GPT4RoI, e.g., color, shape, material, action, etc. Furthermore, it can reason about multiple RoIs based on common sense. On the Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin (the second place is 75.6%) and almost reaching human-level performance of 85.0%. The code and model can be found at https://github.com/jshilong/GPT4RoI.","['Shilong Zhang', 'Peize Sun', 'Shoufa Chen', 'Min Xiao', 'Wenqi Shao', 'Wenwei Zhang', 'Yu Liu', 'Kai Chen', 'Ping Luo']",,arXiv,2025,https://doi.org/10.48550/arXiv.2307.03601,Anomali
Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning,"Multimodal knowledge graphs (MKGs), which intuitively organize information in various modalities, can benefit multiple practical downstream tasks, such as recommendation systems, and visual question answering. However, most MKGs are still far from complete, which motivates the flourishing of MKG reasoning models. Recently, with the development of general artificial architectures, the pretrained transformer models have drawn increasing attention, especially for multimodal scenarios. However, the research of multimodal pretrained transformer (MPT) for knowledge graph reasoning (KGR) is still at an early stage. As the biggest difference between MKG and other multimodal data, the rich structural information underlying the MKG still cannot be fully leveraged in existing MPT models. Most of them only utilize the graph structure as a retrieval map for matching images andtextsconnected with the same entity. This manner hinders their reasoning performances. To this end, we propose the graph Structure Guided Multimodal Pretrained Transformer for knowledge graph reasoning, termed SGMPT. Specifically, the graph structure encoder is adopted for structural feature encoding. Then, a structure-guided fusion module with two different strategies, i.e., weighted summation and alignment constraint, is first designed to inject the structural information into both the textual and visual features. To the best of our knowledge, SGMPT is the first MPT model for multimodal KGR, whichminesthe structural information underlying the knowledge graph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that our SGMPT outperforms existing state-of-the-art models, and prove the effectiveness of the designed strategies.","['Ke Liang', 'Sihang Zhou', 'Yue Liu', 'Lingyuan Meng', 'Meng Liu', 'Xinwang Liu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.03591,Anomali
BiPhone: Modeling Inter Language Phonetic Influences in Text,"A large number of people are forced to use the Web in a language they have low literacy in due to technology asymmetries. Writtentextin the second language (L2) from such users often contains a large number of errors that are influenced by their native language (L1). We propose a method tominephoneme confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of L1 and L2. These confusions are then plugged into a generative model (Bi-Phone) for synthetically producing corrupted L2text. Through human evaluations, we show that Bi-Phone generates plausible corruptions that differ across L1s and also have widespread coverage on the Web. We also corrupt the popular language understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically Noised GLUE) and show that SoTA language understating models perform poorly. We also introduce a new phoneme prediction pre-training task which helps byte models to recover performance close to SuperGLUE. Finally, we also release the FunGLUE benchmark to promote further research in phonetically robust language models. To the best of our knowledge, FunGLUE is the first benchmark to introduce L1-L2 interactions intext.","['Abhirut Gupta', 'Ananya B. Sai', 'Richard Sproat', 'Yuri Vasilevski', 'James S. Ren', 'Ambarish Jash', 'Sukhdeep S. Sodhi', 'Aravindan Raghuveer']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.03322,Anomali
Multi-Task Learning Improves Performance In Deep Argument Mining Models,"The successful analysis of argumentative techniques from user-generatedtextis central to many downstream tasks such as political and market analysis. Recent argumentminingtools use state-of-the-art deep learning methods to extract and annotate argumentative techniques from various onlinetextcorpora, however each task is treated as separate and different bespoke models are fine-tuned for each dataset. We show that different argumentminingtasks share common semantic and logical structure by implementing a multi-task approach to argumentminingthat achieves better performance than state-of-the-art methods for the same problems. Our model builds a shared representation of the inputtextthat is common to all tasks and exploits similarities between tasks in order to further boost performance via parameter-sharing. Our results are important for argumentminingas they show that different tasks share substantial similarities and suggest a holistic approach to the extraction of argumentative techniques fromtext.","['Amirhossein Farzam', 'Shashank Shekhar', 'Isaac Mehlhaff', 'Marco Morucci']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.01401,Anomali
Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search,"Despite large-scale pre-trained language models have achieved striking results fortextclassificaion, recent work has raised concerns about the challenge of shortcut learning. In general, a keyword is regarded as a shortcut if it creates a superficial association with the label, resulting in a false prediction. Conversely, shortcut learning can be mitigated if the model relies on robust causal features that help produce sound predictions. To this end, many studies have explored post-hoc interpretable methods tomineshortcuts and causal features for robustness and generalization. However, most existing methods focus only on single word in a sentence and lack consideration of word-group, leading to wrong causal features. To solve this problem, we propose a new Word-Groupminingapproach, which captures the causal effect of any keyword combination and orders the combinations that most affect the prediction. Our approach bases on effective post-hoc analysis and beam search, which ensures theminingeffect and reduces the complexity. Then, we build a counterfactual augmentation method based on the multiple word-groups, and use an adaptive voting mechanism to learn the influence of different augmentated samples on the prediction results, so as to force the model to pay attention to effective causal features. We demonstrate the effectiveness of the proposed method by several tasks on 8 affective review datasets and 4 toxic language datasets, including cross-domaintextclassificaion,textattack and gender fairness test.","['Rui Song', 'Fausto Giunchiglia', 'Yingji Li', 'Hao Xu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.01214,Anomali
From Portfolio Optimization to Quantum Blockchain and Security: A Systematic Review of Quantum Computing in Finance,"In this paper, we provide an overview of the recent work in the quantum finance realm from various perspectives. The applications in consideration are Portfolio Optimization, Fraud Detection, and Monte Carlo methods for derivative pricing and risk calculation. Furthermore, we give a comprehensive overview of the applications of quantum computing in the field of blockchain technology which is a main concept in fintech. In that sense, we first introduce the general overview of blockchain with its main cryptographic primitives such as digital signature algorithms, hash functions, and random number generators as well as the security vulnerabilities of blockchain technologies after the merge of quantum computers considering Shor's quantum factoring and Grover's quantum search algorithms. We then discuss the privacy preserving quantum-resistant blockchain systems via threshold signatures, ring signatures, and zero-knowledge proof systems i.e. ZK-SNARKs in quantum resistant blockchains. After emphasizing the difference between the quantum-resistant blockchain and quantum-safe blockchain we mention the security countermeasures to take against the possible quantumized attacks aiming these systems. We finalize our discussion with quantum blockchain, efficient quantumminingand necessary infrastructures for constructing such systems based on quantum computing. This review has the intention to be a bridge to fill the gap between quantum computing and one of its most prominent application realms: Finance. We provide the state-of-the-art results in the intersection of finance and quantum technology for both industrial practitioners and academicians.","['Abha Naik', 'Esra Yeniaras', 'Gerhard Hellstern', 'Grishma Prasad', 'Sanjay Kumar Lalta Prasad Vishwakarma']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.01155,Anomali
"The Building Data Genome Directory -- An open, comprehensive data sharing platform for building performance research","The building sector plays a crucial role in the worldwide decarbonization effort, accounting for significant portions of energy consumption and environmental effects. However, the scarcity of open data sources is a continuous challenge for built environment researchers and practitioners. Although several efforts have been made to consolidate existing open datasets, no database currently offers a comprehensive collection of building data types with all subcategories and time granularities (e.g., year, month, and sub-hour). This paper presents the Building Data Genome Directory, an open data-sharing platform serving as a one-stop shop for the data necessary for vital categories of building energy research. The data directory is an online portal (http://buildingdatadirectory.org/) that allows filtering and discovering valuable datasets. The directory covers meter, building-level, and aggregated community-level data at the spatial scale and year-to-minute level at the temporal scale. The datasets were consolidated from a comprehensive exploration of sources, including governments, research institutes, and online energy dashboards. The results of this effort include the aggregation of 60 datasets pertaining to building energy ontologies, building energy models, building energy and water data, electric vehicle data, weather data, building information data,text-mining-based research data, image data of buildings, fault detection diagnosis data and occupant data. A crowdsourcing mechanism in the platform allows users to submit datasets they suggest for inclusion by filling out an online form. This directory can fuel research and applications on building energy efficiency, which is an essential step toward addressing the world's energy and environmental challenges.","['Xiaoyu Jin', 'Chun Fu', 'Hussain Kazmi', 'Atilla Balint', 'Ada Canaydin', 'Matias Quintana', 'Filip Biljecki', 'Fu Xiao', 'Clayton Miller']",J Phys Conf Ser. 2023;2600: 032003,arXiv,2023,https://doi.org/10.48550/arXiv.2307.00793,Anomali
Information Extraction in Domain and Generic Documents: Findings from Heuristic-based and Data-driven Approaches,"Information extraction (IE) plays very important role in natural language processing (NLP) and is fundamental to many NLP applications that used to extract structured information from unstructuredtextdata. Heuristic-based searching and data-driven learning are two main stream implementation approaches. However, no much attention has been paid to document genre and length influence on IE tasks. To fill the gap, in this study, we investigated the accuracy and generalization abilities of heuristic-based searching and data-driven to perform two IE tasks: named entity recognition (NER) and semantic role labeling (SRL) on domain-specific and generic documents with different length. We posited two hypotheses: first, short documents may yield better accuracy results compared to long documents; second, generic documents may exhibit superior extraction outcomes relative to domain-dependent documents due to training document genre limitations. Our findings reveals that no single method demonstrated overwhelming performance in both tasks. For named entity extraction, data-driven approaches outperformed symbolic methods in terms of accuracy, particularly in shorttexts. In the case of semantic roles extraction, we observed that heuristic-based searching method and data-driven based model with syntax representation surpassed the performance of pure data-driven approach which only consider semantic information. Additionally, we discovered that different semantic roles exhibited varying accuracy levels with the same method. This study offers valuable insights for downstreamtextminingtasks, such as NER and SRL, when addressing various document features and genres.","['Shiyu Yuan', 'Carlo Lipizzi']",,arXiv,2023,https://doi.org/10.48550/arXiv.2307.00130,Anomali
NCL++: Nested Collaborative Learning for Long-Tailed Visual Recognition,"Long-tailed visual recognition has received increasing attention in recent years. Due to the extremely imbalanced data distribution in long-tailed learning, the learning process shows great uncertainties. For example, the predictions of different experts on the same image vary remarkably despite the same training settings. To alleviate the uncertainty, we propose a Nested Collaborative Learning (NCL++) which tackles the long-tailed learning problem by a collaborative learning. To be specific, the collaborative learning consists of two folds, namely inter-expert collaborative learning (InterCL) and intra-expert collaborative learning (IntraCL). In-terCL learns multiple experts collaboratively and concurrently, aiming to transfer the knowledge among different experts. IntraCL is similar to InterCL, but it aims to conduct the collaborative learning on multiple augmented copies of the same image within the single expert. To achieve the collaborative learning in long-tailed learning, the balanced online distillation is proposed to force the consistent predictions among different experts and augmented copies, which reduces the learning uncertainties. Moreover, in order to improve the meticulous distinguishing ability on the confusing categories, we further propose a Hard CategoryMining(HCM), which selects the negative categories with high predicted scores as the hard categories. Then, the collaborative learning is formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial perspective. Extensive experiments manifest the superiority of our method with outperforming the state-of-the-art whether with using a single model or an ensemble. The code will be publicly released.","['Zichang Tan', 'Jun Li', 'Jinhao Du', 'Jun Wan', 'Zhen Lei', 'Guodong Guo']",,arXiv,2024,https://doi.org/10.48550/arXiv.2306.16709,Anomali
Image-based Communication on Social Coding Platforms,"Visual content in the form of images and videos has taken over general-purpose social networks in a variety of ways, streamlining and enriching online communications. We are interested to understand if and to what extent the use of images is popular and helpful in social coding platforms. Weminednine years of data from two popular software developers' platforms: the Mozilla issue tracking system, i.e., Bugzilla, and the most well-known platform for developers' Q/A, i.e., Stack Overflow. We further triangulated and extended ourminingresults by performing a survey with 168 software developers. We observed that, between 2013 and 2022, the number of posts containing image data on Bugzilla and Stack Overflow doubled. Furthermore, we found that sharing images makes other developers engage more and faster with the content. In the majority of cases in which an image is included in a developer's post, the information in that image is complementary to thetextprovided. Finally, our results showed that when an image is shared, understanding the content without the information in the image is unlikely for 86.9\% of the cases. Based on these observations, we discuss the importance of considering visual content when analyzing developers and designing automation tools.","['Maleknaz Nayebi', 'Bram Adams']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.15851,Anomali
DMNER: Biomedical Entity Recognition by Detection and Matching,"Biomedical named entity recognition (BNER) serves as the foundation for numerous biomedicaltextminingtasks. Unlike general NER, BNER require a comprehensive grasp of the domain, and incorporating external knowledge beyond training data poses a significant challenge. In this study, we propose a novel BNER framework called DMNER. By leveraging existing entity representation models SAPBERT, we tackle BNER as a two-step process: entity boundary detection and biomedical entity matching. DMNER exhibits applicability across multiple NER scenarios: 1) In supervised NER, we observe that DMNER effectively rectifies the output of baseline NER models, thereby further enhancing performance. 2) In distantly supervised NER, combining MRC and AutoNER as span boundary detectors enables DMNER to achieve satisfactory results. 3) For training NER by merging multiple datasets, we adopt a framework similar to DS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the training. Through extensive experiments conducted on 10 benchmark datasets, we demonstrate the versatility and effectiveness of DMNER.","['Junyi Bian', 'Rongze Jiang', 'Weiqi Zhai', 'Tianyang Huang', 'Hong Zhou', 'Shanfeng Zhu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.15736,Anomali
Hierarchical Dense Correlation Distillation for Few-Shot Segmentation-Extended Abstract,"Few-shot semantic segmentation (FSS) aims to form class-agnostic models segmenting unseen classes with only a handful of annotations. Previous methods limited to the semantic feature and prototype representation suffer from coarse segmentation granularity and train-set overfitting. In this work, we design Hierarchically Decoupled Matching Network (HDMNet)miningpixel-level support correlation based on the transformer architecture. The self-attention modules are used to assist in establishing hierarchical dense features, as a means to accomplish the cascade matching between query and support features. Moreover, we propose a matching module to reduce train-set overfitting and introduce correlation distillation leveraging semantic correspondence from coarse resolution to boost fine-grained segmentation. Our method performs decently in experiments. We achieve 50.0% mIoU on COCO dataset one-shot setting and 56.0% on five-shot segmentation, respectively. The code will be available on the project website. We hope our work can benefit broader industrial applications where novel classes with limited annotations are required to be decently identified.","['Bohao Peng', 'Zhuotao Tian', 'Xiaoyang Wu', 'Chengyao Wang', 'Shu Liu', 'Jingyong Su', 'Jiaya Jia']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.15278,Anomali
Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs,"Introduction: Microblogging websites have massed rich data sources for sentiment analysis and opinionmining. In this regard, sentiment classification has frequently proven inefficient because microblog posts typically lack syntactically consistent terms and representatives since users on these social networks do not like to write lengthy statements. Also, there are some limitations to low-resource languages. The Persian language has exceptional characteristics and demands unique annotated data and models for the sentiment analysis task, which are distinctive fromtextfeatures within the English dialect. Method: This paper first constructs a user opinion dataset called ITRC-Opinion in a collaborative environment and insource way. Our dataset contains 60,000 informal and colloquial Persiantextsfrom social microblogs such as Twitter and Instagram. Second, this study proposes a new architecture based on the convolutional neural network (CNN) model for more effective sentiment analysis of colloquialtextin social microblog posts. The constructed datasets are used to evaluate the presented architecture. Furthermore, some models, such as LSTM, CNN-RNN, BiLSTM, and BiGRU with different word embeddings, including Fasttext, Glove, and Word2vec, investigated our dataset and evaluated the results. Results: The results demonstrate the benefit of our dataset and the proposed model (72% accuracy), displaying meaningful improvement in sentiment classification performance.","['Mojtaba Mazoochi', 'Leila Rabiei', 'Farzaneh Rahmani', 'Zeinab Rajabi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2306.12679,Anomali
ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis,"We use prompt engineering to guide ChatGPT in the automation oftextminingof metal-organic frameworks (MOFs) synthesis conditions from diverse formats and styles of the scientific literature. This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging. Our approach involves the development of a workflow implementing three different processes fortextmining, programmed by ChatGPT itself. All of them enable parsing, searching, filtering, classification, summarization, and data unification with different tradeoffs between labor, speed, and accuracy. We deploy this system to extract 26,257 distinct synthesis parameters pertaining to approximately 800 MOFs sourced from peer-reviewed research articles. This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT intextmining, resulting in impressive precision, recall, and F1 scores of 90-99%. Furthermore, with the dataset built bytextmining, we constructed a machine-learning model with over 86% accuracy in predicting MOF experimental crystallization outcomes and preliminarily identifying important factors in MOF crystallization. We also developed a reliable data-grounded MOF chatbot to answer questions on chemical reactions and synthesis procedures. Given that the process of using ChatGPT reliablyminesand tabulates diverse MOF synthesis information in a unified format, while using only narrative language requiring no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be very useful across various other chemistry sub-disciplines.","['Zhiling Zheng', 'Oufan Zhang', 'Christian Borgs', 'Jennifer T. Chayes', 'Omar M. Yaghi']","J. Am. Chem. Soc. 2023, 145, 32, 18048-18062",arXiv,2023,https://doi.org/10.48550/arXiv.2306.11296,Anomali
Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction,"Multi-Modal Relation Extraction (MMRE) aims at identifying the relation between two entities intextsthat contain visual clues. Rich visual content is valuable for the MMRE task, but existing works cannot well model finer associations among different modalities, failing to capture the truly helpful visual information and thus limiting relation extraction performance. In this paper, we propose a novel MMRE framework to better capture the deeper correlations oftext, entity pair, and image/objects, so as tominemore helpful information for the task, termed as DGF-PT. We first propose a prompt-based autoregressive encoder, which builds the associations of intra-modal and inter-modal features related to the task, respectively by entity-oriented and object-oriented prefixes. To better integrate helpful visual information, we design a dual-gated fusion module to distinguish the importance of image/objects and further enrichtextrepresentations. In addition, a generative decoder is introduced with entity type restriction on relations, better filtering out candidates. Extensive experiments conducted on the benchmark dataset show that our approach achieves excellent performance compared to strong competitors, even in the few-shot situation.","['Qian Li', 'Shu Guo', 'Cheng Ji', 'Xutan Peng', 'Shiyao Cui', 'Jianxin Li']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.11020,Anomali
Efficient Generalized Temporal Pattern Mining in Big Time Series Using Mutual Information,"Big time series are increasingly available from an ever wider range of IoT-enabled sensors deployed in various environments. Significant insights can be gained byminingtemporal patterns from these time series. Temporal patternmining(TPM) extends traditional patternminingby adding event time intervals into extracted patterns, making them more expressive at the expense of increased time and space complexities. Besides frequent temporal patterns (FTPs), which occur frequently in the entire dataset, another useful type of temporal patterns are so-called rare temporal patterns (RTPs), which appear rarely but with high confidence.Miningrare temporal patterns yields additional challenges. For FTPmining, the temporal information and complex relations between events already create an exponential search space. For RTPmining, the support measure is set very low, leading to a further combinatorial explosion and potentially producing too many uninteresting patterns. Thus, there is a need for a generalized approach which canmineboth frequent and rare temporal patterns. This paper presents our Generalized Temporal PatternMiningfrom Time Series (GTPMfTS) approach with the following specific contributions: (1) The end-to-end GTPMfTS process taking time series as input and producing frequent/rare temporal patterns as output. (2) The efficient Generalized Temporal PatternMining(GTPM) algorithmminesfrequent and rare temporal patterns using efficient data structures for fast retrieval of events and patterns during theminingprocess, and employs effective pruning techniques for significantly fastermining. (3) An approximate version of GTPM that uses mutual information, a measure of data correlation, to prune unpromising time series from the search space.","['Van Long Ho', 'Nguyen Ho', 'Torben Bach Pedersen', 'Panagiotis Papapetrou']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.10994,Anomali
Persian Semantic Role Labeling Using Transfer Learning and BERT-Based Models,"Semantic role labeling (SRL) is the process of detecting the predicate-argument structure of each predicate in a sentence. SRL plays a crucial role as a pre-processing step in many NLP applications such as topic and concept extraction, question answering, summarization, machine translation, sentiment analysis, andtextmining. Recently, in many languages, unified SRL dragged lots of attention due to its outstanding performance, which is the result of overcoming the error propagation problem. However, regarding the Persian language, all previous works have focused on traditional methods of SRL leading to a drop in accuracy and imposing expensive feature extraction steps in terms of financial resources, time and energy consumption. In this work, we present an end-to-end SRL method that not only eliminates the need for feature extraction but also outperforms existing methods in facing new samples in practical situations. The proposed method does not employ any auxiliary features and shows more than 16 (83.16) percent improvement in accuracy against previous methods in similar circumstances.","['Saeideh Niksirat Aghdam', 'Sayyed Ali Hossayni', 'Erfan Khedersolh Sadeh', 'Nasim Khozouei', 'Behrouz Minaei Bidgoli']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.10339,Anomali
Rosetta Neurons: Mining the Common Units in a Model Zoo,"Do different neural networks, trained for various vision tasks, share some common representations? In this paper, we demonstrate the existence of common features we call ""Rosetta Neurons"" across a range of models with different architectures, different tasks (generative and discriminative), and different types of supervision (class-supervised,text-supervised, self-supervised). We present an algorithm formininga dictionary of Rosetta Neurons across several popular vision models: Class Supervised-ResNet50, DINO-ResNet50, DINO-ViT, MAE, CLIP-ResNet50, BigGAN, StyleGAN-2, StyleGAN-XL. Our findings suggest that certain visual concepts and structures are inherently embedded in the natural world and can be learned by different models regardless of the specific task or architecture, and without the use of semantic labels. We can visualize shared concepts directly due to generative models included in our analysis. The Rosetta Neurons facilitate model-to-model translation enabling various inversion-based manipulations, including cross-class alignments, shifting, zooming, and more, without the need for specialized training.","['Amil Dravid', 'Yossi Gandelsman', 'Alexei A. Efros', 'Assaf Shocher']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.09346,Anomali
MetricPrompt: Prompting Model as a Relevance Metric for Few-shot Text Classification,"Prompting methods have shown impressive performance in a variety oftextminingtasks and applications, especially few-shot ones. Despite the promising prospects, the performance of prompting model largely depends on the design of prompt template and verbalizer. In this work, we propose MetricPrompt, which eases verbalizer design difficulty by reformulating few-shottextclassification task intotextpair relevance estimation task. MetricPrompt adopts prompting model as the relevance metric, further bridging the gap between Pre-trained Language Model's (PLM) pre-training objective andtextclassification task, making possible PLM's smooth adaption. Taking a training sample and a query one simultaneously, MetricPrompt captures cross-sample relevance information for accurate relevance estimation. We conduct experiments on three widely usedtextclassification datasets across four few-shot settings. Results show that MetricPrompt outperforms manual verbalizer and other automatic verbalizer design methods across all few-shot settings, achieving new state-of-the-art (SOTA) performance.","['Hongyuan Dong', 'Weinan Zhang', 'Wanxiang Che']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.08892,Anomali
Curatr: A Platform for Semantic Analysis and Curation of Historical Literary Texts,"The increasing availability of digital collections of historical and contemporary literature presents a wealth of possibilities for new research in the humanities. The scale and diversity of such collections however, presents particular challenges in identifying and extracting relevant content. This paper presents Curatr, an online platform for the exploration and curation of literature with machine learning-supported semantic search, designed within the context of digital humanities scholarship. The platform provides atextminingworkflow that combines neural word embeddings with expert domain knowledge to enable the generation of thematic lexicons, allowing researches to curate relevant sub-corpora from a large corpus of 18th and 19th century digitisedtexts.","['Susan Leavy', 'Gerardine Meaney', 'Karen Wade', 'Derek Greene']","Metadata and Semantic Research (MTSR 2019), Communications in Computer and Information Science, vol 1057. Springer, Cham",arXiv,2023,https://doi.org/10.48550/arXiv.2306.08020,Anomali
MOFI: Learning Image Representations from Noisy Entity Annotated Images,"We present MOFI, Manifold OF Images, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: (i) pre-training data, and (ii) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-textpairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. It's a simple, cost-effective method that can scale to handle billions of web-minedimage-textpairs. Through this method, we have created Image-to-Entities (I2E), a new dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes like supervised pre-training, contrastive pre-training, and multi-task learning. For contrastive pre-training, we treat entity names as free-formtext, and further enrich them with entity descriptions. Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks, and multi-task training further improves the performance. The final MOFI model achieves 86.66% mAP on the challenging GPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19% from OpenAI's CLIP model. Further experiments on zero-shot and linear probe image classification also show that MOFI outperforms a CLIP model trained on the original image-textdata, demonstrating the effectiveness of the I2E dataset in learning strong image representations. We release our code and model weights at https://github.com/apple/ml-mofi.","['Wentao Wu', 'Aleksei Timofeev', 'Chen Chen', 'Bowen Zhang', 'Kun Duan', 'Shuangning Liu', 'Yantao Zheng', 'Jonathon Shlens', 'Xianzhi Du', 'Zhe Gan', 'Yinfei Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2306.07952,Anomali
Recurrent Attention Networks for Long-text Modeling,"Self-attention-based models have achieved remarkable progress in short-textmining. However, the quadratic computational complexities restrict their application in longtextprocessing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-textencoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.","['Xianming Li', 'Zongxi Li', 'Xiaotian Luo', 'Haoran Xie', 'Xing Lee', 'Yingbin Zhao', 'Fu Lee Wang', 'Qing Li']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.06843,Anomali
Modeling Structural Similarities between Documents for Coherence Assessment with Graph Convolutional Networks,"Coherence is an important aspect oftextquality, and various approaches have been applied to coherence modeling. However, existing methods solely focus on a single document's coherence patterns, ignoring the underlying correlation between documents. We investigate a GCN-based coherence model that is capable of capturing structural similarities between documents. Our model first creates a graph structure for each document, from where weminedifferent subgraph patterns. We then construct a heterogeneous graph for the training corpus, connecting documents based on their shared subgraphs. Finally, a GCN is applied to the heterogeneous graph to model the connectivity relationships. We evaluate our method on two tasks, assessing discourse coherence and automated essay scoring. Results show that our GCN-based model outperforms all baselines, achieving a new state-of-the-art on both tasks.","['Wei Liu', 'Xiyan Fu', 'Michael Strube']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.06472,Anomali
Environmental Considerations in the age of Space Exploration: the Conservation and Protection of Non-Earth Environments,"This document is an abbreviated version of the law review, led by Alexander Q. Gilbert, entitled: ""Major Federal Actions Significantly Affecting the Quality of the Space Environment: Applying NEPA to Federal and Federally Authorized Outer Space Activities."" Here, we discuss the future of the space environment, and how it is increasingly becoming a human environment with regard to continued robotic and human presence in orbit, planned and proposed robotic and human presence on bodies such as the Moon and Mars, planned spaceminingprojects, the increase use of low-Earth orbit for communications satellites, and other human uses of space. As such, we must evaluate and protect these environments just as we do on Earth. In order to prioritize mitigating threat of contamination, avoiding conflict, and promoting sustainability in space, all to ensure that actors maintain equal and safe access to space, we propose applying the National Environmental Policy Act, or NEPA, to space missions. We put forward three examples of environmental best practices for those involved in space missions to consider: adopting precautionary and communicative structure to before, during, and after missions taking place off-world, environmental impact statements, and transparency in tools that may impact the environment (including radioisotope power sources, plans in case of vehicle loss or loss of trajectory, and others). For additional discussion related to potential space applications of NEPA, NEPA's statutorytext, and NEPA's relation to space law and judicial precedent for space, we recommend reading the full law review.","['Monica R. Vidaurri', 'Alexander Q. Gilbert']",ENVIRONS 2019 44:2 pp 233-272,arXiv,2023,https://doi.org/10.48550/arXiv.2306.05594,Anomali
Advancing Italian Biomedical Information Extraction with Transformers-based Models: Methodological Insights and Multicenter Practical Application,"The introduction of computerized medical records in hospitals has reduced burdensome activities like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting data from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation by using automatedtext-miningpipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Transformers-based model. Moreover, we collected and leveraged three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a ""low-resource"" approach. This allowed us to establish methodological guidelines that pave the way for Natural Language Processing studies in less-resourced languages.","['Claudio Crema', 'Tommaso Mario Buonocore', 'Silvia Fostinelli', 'Enea Parimbelli', 'Federico Verde', 'Cira Fundarò', 'Marina Manera', 'Matteo Cotta Ramusino', 'Marco Capelli', 'Alfredo Costa', 'Giuliano Binetti', 'Riccardo Bellazzi', 'Alberto Redolfi']","Journal of Biomedical Informatics, Volume 148, 2023, 104557, ISSN 1532-0464",arXiv,2024,https://doi.org/10.48550/arXiv.2306.05323,Anomali
Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale,"Large language models (LLMs), such as ChatGPT and GPT-4, are gaining wide-spread real world use. Yet, these LLMs are closed source, and little is known about their performance in real-world use cases. In this paper, we apply and evaluate the combination of ChatGPT and GPT-4 for the real-world task ofmininginsights from atextcorpus in order to identify research challenges in the field of HCI. We extract 4,392 research challenges in over 100 topics from the 2023~CHI conference proceedings and visualize the research challenges for interactive exploration. We critically evaluate the LLMs on this practical task and conclude that the combination of ChatGPT and GPT-4 makes an excellent cost-efficient means for analyzing atextcorpus at scale. Cost-efficiency is key for flexibly prototyping research ideas and analyzingtextcorpora from different perspectives, with implications for applying LLMs formininginsights in academia and practice.","['Jonas Oppenlaender', 'Joonas Hämäläinen']",,arXiv,2024,https://doi.org/10.48550/arXiv.2306.05036,Anomali
COURIER: Contrastive User Intention Reconstruction for Large-Scale Visual Recommendation,"With the advancement of multimedia internet, the impact of visual characteristics on the decision of users to click or not within the online retail industry is increasingly significant. Thus, incorporating visual features is a promising direction for further performance improvements in click-through rate (CTR). However, experiments on our production system revealed that simply injecting the image embeddings trained with established pre-training methods only has marginal improvements. We believe that the main advantage of existing image feature pre-training methods lies in their effectiveness for cross-modal predictions. However, this differs significantly from the task of CTR prediction in recommendation systems. In recommendation systems, other modalities of information (such astext) can be directly used as features in downstream models. Even if the performance of cross-modal prediction tasks is excellent, it is challenging to provide significant information gain for the downstream models. We argue that a visual feature pre-training method tailored for recommendation is necessary for further improvements beyond existing modality features. To this end, we propose an effective user intention reconstruction module tominevisual features related to user interests from behavior histories, which constructs a many-to-one correspondence. We further propose a contrastive training method to learn the user intentions and prevent the collapse of embedding vectors. We conduct extensive experimental evaluations on public datasets and our production system to verify that our method can learn users' visual interests. Our method achieves $0.46\%$ improvement in offline AUC and $0.88\%$ improvement in Taobao GMV (Cross Merchandise Volume) with p-value$<$0.01.","['Jia-Qi Yang', 'Chenglei Dai', 'Dan OU', 'Dongshuai Li', 'Ju Huang', 'De-Chuan Zhan', 'Xiaoyi Zeng', 'Yang Yang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2306.05001,Anomali
Cross-Genre Argument Mining: Can Language Models Automatically Fill in Missing Discourse Markers?,"Available corpora for ArgumentMiningdiffer along several axes, and one of the key differences is the presence (or absence) of discourse markers to signal argumentative content. Exploring effective ways to use discourse markers has received wide attention in various discourse parsing tasks, from which it is well-known that discourse markers are strong indicators of discourse relations. To improve the robustness of ArgumentMiningsystems across different genres, we propose to automatically augment a giventextwith discourse markers such that all relations are explicitly signaled. Our analysis unveils that popular language models taken out-of-the-box fail on this task; however, when fine-tuned on a new heterogeneous dataset that we construct (including synthetic and real examples), they perform considerably better. We demonstrate the impact of our approach on an ArgumentMiningdownstream task, evaluated on different corpora, showing that language models can be trained to automatically fill in discourse markers across different corpora, improving the performance of a downstream model in some, but not all, cases. Our proposed approach can further be employed as an assistive tool for better discourse understanding.","['Gil Rocha', 'Henrique Lopes Cardoso', 'Jonas Belouadi', 'Steffen Eger']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.04314,Anomali
Analysis of the Fed's communication by using textual entailment model of Zero-Shot classification,"In this study, we analyze documents published by central banks usingtextminingtechniques and propose a method to evaluate the policy tone of central banks. Since the monetary policies of major central banks have a broad impact on financial market trends, the pricing of risky assets, and the real economy, market participants are attempting to more accurately capture changes in the outlook for central banks' future monetary policies. Since the published documents are also an important tool for the central bank to communicate with the market, they are meticulously elaborated on grammatical syntax and wording, and investors are urged to read more accurately about the central bank's policy stance. Sentiment analysis on central bank documents has long been carried out, but it has been difficult to interpret the meaning of the documents accurately and to explicitly capture even the intentional change in nuance. This study attempts to evaluate the implication of the zero-shottextclassification method for an unknown economic environment using the same model. We compare the tone of the statements, minutes, press conference transcripts of FOMC meetings, and the Fed officials' (chair, vice chair, and Governors) speeches. In addition, the minutes of the FOMC meetings were subjected to a phase analysis of changes in each policy stance since 1971.","['Yasuhiro Nakayama', 'Tomochika Sawaki']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.04277,Anomali
Graph-Aware Language Model Pre-Training on a Large Graph Corpus Can Help Multiple Graph Applications,"Model pre-training on largetextcorpora has been demonstrated effective for various downstream applications in the NLP domain. In the graphminingdomain, a similar analogy can be drawn for pre-training graph models on large graphs in the hope of benefiting downstream graph applications, which has also been explored by several recent studies. However, no existing study has ever investigated the pre-training oftextplus graph models on large heterogeneous graphs with abundant textual information (a.k.a. large graph corpora) and then fine-tuning the model on different related downstream applications with different graph schemas. To address this problem, we propose a framework of graph-aware language model pre-training (GALM) on a large graph corpus, which incorporates large language models and graph neural networks, and a variety of fine-tuning methods on downstream applications. We conduct extensive experiments on Amazon's real internal datasets and large public datasets. Comprehensive empirical results and in-depth analysis demonstrate the effectiveness of our proposed methods along with lessons learned.","['Han Xie', 'Da Zheng', 'Jun Ma', 'Houyu Zhang', 'Vassilis N. Ioannidis', 'Xiang Song', 'Qing Ping', 'Sheng Wang', 'Carl Yang', 'Yi Xu', 'Belinda Zeng', 'Trishul Chilimbi']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.02592,Anomali
Explaining Hate Speech Classification with Model Agnostic Methods,"There have been remarkable breakthroughs in Machine Learning and Artificial Intelligence, notably in the areas of Natural Language Processing and Deep Learning. Additionally, hate speech detection in dialogues has been gaining popularity among Natural Language Processing researchers with the increased use of social media. However, as evidenced by the recent trends, the need for the dimensions of explainability and interpretability in AI models has been deeply realised. Taking note of the factors above, the research goal of this paper is to bridge the gap between hate speech prediction and the explanations generated by the system to support its decision. This has been achieved by first predicting the classification of atextand then providing a posthoc, model agnostic and surrogate interpretability approach for explainability and to prevent model bias. The bidirectional transformer model BERT has been used for prediction because of its state of the art efficiency over other Machine Learning models. The model agnostic algorithm LIME generates explanations for the output of a trained classifier and predicts the features that influence the model decision. The predictions generated from the model were evaluated manually, and after thorough evaluation, we observed that the model performs efficiently in predicting and explaining its prediction. Lastly, we suggest further directions for the expansion of the provided research work.","['Durgesh Nandini', 'Ute Schmid']",,arXiv,2023,https://doi.org/10.48550/arXiv.2306.00021,Anomali
FakeSwarm: Improving Fake News Detection with Swarming Characteristics,"The proliferation of fake news poses a serious threat to society, as it can misinform and manipulate the public, erode trust in institutions, and undermine democratic processes. To address this issue, we present FakeSwarm, a fake news identification system that leverages the swarming characteristics of fake news. To extract the swarm behavior, we propose a novel concept of fake news swarming characteristics and design three types of swarm features, including principal component analysis, metric representation, and position encoding. We evaluate our system on a public dataset and demonstrate the effectiveness of incorporating swarm features in fake news identification, achieving an f1-score and accuracy of over 97% by combining all three types of swarm features. Furthermore, we design an online learning pipeline based on the hypothesis of the temporal distribution pattern of fake news emergence, validated on a topic with early emerging fake news and a shortage oftextsamples, showing that swarm features can significantly improve recall rates in such cases. Our work provides a new perspective and approach to fake news detection and highlights the importance of considering swarming characteristics in detecting fake news.","['Jun Wu', 'Xuesong Ye']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.19194,Anomali
Mapping ChatGPT in Mainstream Media to Unravel Jobs and Diversity Challenges: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis,"The exponential growth in user acquisition and popularity of OpenAIs ChatGPT, an artificial intelligence(AI) powered chatbot, was accompanied by widespread mainstream media coverage. This article presents a quantitative data analysis of the early trends and sentiments revealed by conductingtextminingand NLP methods onto a corpus of 10,902 mainstream news headlines related to the subject of ChatGPT and artificial intelligence, from the launch of ChatGPT in November 2022 to March 2023. The findings revealed in sentiment analysis, ChatGPT and artificial intelligence, were perceived more positively than negatively in the mainstream media. In regards to word frequency results, over sixty-five percent of the top frequency words were focused on Big Tech issues and actors while topics such as jobs, diversity, ethics, copyright, gender and women were poorly represented or completely absent and only accounted for six percent of the total corpus. This article is a critical analysis into the power structures and collusions between Big Tech and Big Media in their hegemonic exclusion of diversity and job challenges from mainstream media.",['Maya Karanouh'],,arXiv,2023,https://doi.org/10.48550/arXiv.2305.18340,Anomali
A Framework For Refining Text Classification and Object Recognition from Academic Articles,"With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Dataminingtechniques are generally employed to solve this issue. However, dataminingfor academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current dataminingmethods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified publication, we emphasize implementing specific methods for specific characteristics in academic articles. We have developed a novelTextBlock Refinement Framework (TBRF), a machine learning and rule-based scheme hybrid. We used the well-known ACL proceeding articles as experimental data for the validation experiment. The experiment shows that our approach achieved over 95% classification accuracy and 90% detection accuracy for tables and figures.","['Jinghong Li', 'Koichi Ota', 'Wen Gu', 'Shinobu Hasegawa']",,arXiv,2024,https://doi.org/10.48550/arXiv.2305.17401,Anomali
An Interactive Decision Support System for Analyzing Time Related Restrictions in Renaturation and Redevelopment Planning Projects,"The operation of open-cast ligniteminesis a large intervention in nature, making the areas uninhabitable even after closing themineswithout renaturation processes. Renaturation of these large areas requires a regional planning process which is tied to many conditions and restrictions, such as environmental protection laws. The related information is available only as unstructuredtextin a variety of documents. Associated temporal aspects and the geographical borders to these textual information have to be linked manually so far. This process is highly time-consuming, error-prone, and tedious. Therefore, the knowledge of experts is often used, but this does not necessarily include all the relevant information. In this paper, we present a system to support the experts in decision-making of urban planning, renaturation, and redevelopment projects. The system allows to plan new projects, while considering spatial and temporal restrictions extracted fromtextdocuments. With this, our presented system can also be used to verify compliance with certain legal regulations, such as nature conservation laws.","['Yves Annanias', 'Christofer Meinecke', 'Daniel Wiegreffe']",Workshop on Visualisation in Environmental Sciences (EnvirVis) 2023,arXiv,2023,https://doi.org/10.48550/arXiv.2305.16975,Anomali
Neural Natural Language Processing for Long Texts: A Survey on Classification and Summarization,"The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shortertexts, while the ever increasing size of documents uploaded online renders automated understanding of lengthytextsa critical issue. Relevant applications include automated Webmining, legal document review, medical records analysis, financial reports analysis, contract management, environmental impact assessment, news aggregation, etc. Despite the relatively recent development of efficient algorithms for analyzing long documents, practical tools in this field are currently flourishing. This article serves as an entry point into this dynamic domain and aims to achieve two objectives. First of all, it provides an introductory overview of the relevant neural building blocks, serving as a concise tutorial for the field. Secondly, it offers a brief examination of the current state-of-the-art in two key long document analysis tasks: document classification and document summarization. Sentiment analysis for longtextsis also covered, since it is typically treated as a particular case of document classification. Consequently, this article presents an introductory exploration of document-level analysis, addressing the primary challenges, concerns, and existing solutions. Finally, it offers a concise definition of ""longtext/document"", presents an original overarching taxonomy of common deep neural methods for long document analysis and lists publicly available annotated datasets that can facilitate further research in this area.","['Dimitrios Tsirmpas', 'Ioannis Gkionis', 'Georgios Th. Papadopoulos', 'Ioannis Mademlis']","Engineering Applications of Artificial Intelligence, Volume 133, Part C, 2024, 108231, ISSN 0952-1976",arXiv,2024,https://doi.org/10.48550/arXiv.2305.16259,Anomali
Connecting the Dots: What Graph-Based Text Representations Work Best for Text Classification Using Graph Neural Networks?,"Given the success of Graph Neural Networks (GNNs) for structure-aware machine learning, many studies have explored their use fortextclassification, but mostly in specific domains with limited data characteristics. Moreover, some strategies prior to GNNs relied on graphminingand classical machine learning, making it difficult to assess their effectiveness in modern settings. This work extensively investigates graph representation methods fortextclassification, identifying practical implications and open challenges. We compare different graph construction schemes using a variety of GNN architectures and setups across five datasets, encompassing short and long documents as well as unbalanced scenarios in diverse domains. Two Transformer-based large language models are also included to complement the study. The results show that i) although the effectiveness of graphs depends on the textual input features and domain, simple graph constructions perform better the longer the documents are, ii) graph representations are especially beneficial for longer documents, outperforming Transformer-based models, iii) graph methods are particularly efficient at solving the task.","['Margarita Bugueño', 'Gerard de Melo']","Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8943-8960",arXiv,2024,https://doi.org/10.48550/arXiv.2305.14578,Anomali
PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents,"Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: actionmining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrativetexts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.","['Simeng Sun', 'Yang Liu', 'Shuohang Wang', 'Chenguang Zhu', 'Mohit Iyyer']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.14564,Anomali
Process-To-Text: A Framework for the Quantitative Description of Processes in Natural Language,"In this paper we present the Process-To-Text(P2T) framework for the automatic generation of textual descriptive explanations of processes. P2T integrates three AI paradigms: processminingfor extracting temporal and structural information from a process, fuzzy linguistic protoforms for modelling uncertain terms, and natural language generation for building the explanations. A real use-case in the cardiology domain is presented, showing the potential of P2T for providing natural language explanations addressed to specialists.","['Yago Fontenla-Seco', 'Alberto Bugarín-Diz', 'Manuel Lama']",,arXiv,2023,this http URL,Anomali
Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality,"Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning, leading to state-of-the-art models for various downstream multimodal tasks. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graph-structured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed fromtextas a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images andtextthat aligns sentences of various complexities to the same image. Along with this, we propose novel negativeminingtechniques in the scene graph space for improving attribute binding and relation understanding. Through extensive experiments, we demonstrate the effectiveness of our approach that significantly improves attribute binding, relation understanding, systematic generalization, and productivity on multiple recently proposed benchmarks (For example, improvements upto $18\%$ for systematic generalization, $16.5\%$ for relation understanding over a strong baseline), while achieving similar or better performance than CLIP on various general multimodal tasks.","['Harman Singh', 'Pengchuan Zhang', 'Qifan Wang', 'Mengjiao Wang', 'Wenhan Xiong', 'Jingfei Du', 'Yu Chen']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.13812,Anomali
Towards Zero-shot Relation Extraction in Web Mining: A Multimodal Approach with Relative XML Path,"The rapid growth of web pages and the increasing complexity of their structure poses a challenge for webminingmodels. Webminingmodels are required to understand the semi-structured web pages, particularly when little is known about the subject or template of a new page. Current methods migrate language models to the webminingby embedding the XML source code into the transformer or encoding the rendered layout with graph neural networks. However, these approaches do not take into account the relationships betweentextnodes within and across pages. In this paper, we propose a new approach, ReXMiner, for zero-shot relation extraction in webmining. ReXMiner encodes the shortest relative paths in the Document Object Model (DOM) tree which is a more accurate and efficient signal for key-value pair extraction within a web page. It also incorporates the popularity of eachtextnode by counting the occurrence of the sametextnode across different web pages. We use the contrastive learning to address the issue of sparsity in relation extraction. Extensive experiments on public benchmarks show that our method, ReXMiner, outperforms the state-of-the-art baselines in the task of zero-shot relation extraction in webmining.","['Zilong Wang', 'Jingbo Shang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.13805,Anomali
The Grammar and Syntax Based Corpus Analysis Tool For The Ukrainian Language,"This paper provides an overview of atextminingtool the StyloMetrix developed initially for the Polish language and further extended for English and recently for Ukrainian. The StyloMetrix is built upon various metrics crafted manually by computational linguists and researchers from literary studies to analyze grammatical, stylistic, and syntactic patterns. The idea of constructing the statistical evaluation of syntactic and grammar features is straightforward and familiar for the languages like English, Spanish, German, and others; it is yet to be developed for low-resource languages like Ukrainian. We describe the StyloMetrix pipeline and provide some experiments with this tool for thetextclassification task. We also describe our package's main limitations and the metrics' evaluation procedure.","['Daria Stetsenko', 'Inez Okulska']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.13530,Anomali
WOT-Class: Weakly Supervised Open-world Text Classification,"State-of-the-art weakly supervisedtextclassification methods, while significantly reduced the required human supervision, still requires the supervision to cover all the classes of interest. This is never easy to meet in practice when human explore new, large corpora without complete pictures. In this paper, we work on a novel yet important problem of weakly supervised open-worldtextclassification, where supervision is only needed for a few examples from a few known classes and the machine should handle both known and unknown classes in test time. General open-world classification has been studied mostly using image classification; however, existing methods typically assume the availability of sufficient known-class supervision and strong unknown-class prior knowledge (e.g., the number and/or data distribution). We propose a novel framework WOT-Class that lifts those strong assumptions. Specifically, it follows an iterative process of (a) clusteringtextto new classes, (b)miningand ranking indicative words for each class, and (c) merging redundant classes by using the overlapped indicative words as a bridge. Extensive experiments on 7 populartextclassification datasets demonstrate that WOT-Class outperforms strong baselines consistently with a large margin, attaining 23.33% greater average absolute macro-F1 over existing approaches across all datasets. Such competent accuracy illuminates the practical potential of further reducing human effort fortextclassification.","['Tianle Wang', 'Zihan Wang', 'Weitang Liu', 'Jingbo Shang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.12401,Anomali
DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining,"Manytextminingmodels are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge nowadays is maintaining performance when we use a lightweight model with limited labelled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among distilled student cohorts to promote their SSL effectiveness. DisCo employs a novel co-training technique to optimize a cohort of multiple small student models by promoting knowledge sharing among students under diversified views: model views produced by different distillation strategies and data views produced by various input augmentations. We evaluate DisCo on both semi-supervisedtextclassification and extractive summarization tasks. Experimental results show that DisCo can produce student models that are 7.6 times smaller and 4.8 times faster in inference than the baseline PLMs while maintaining comparable performance. We also show that DisCo-generated student models outperform the similar-sized models elaborately tuned in distinct tasks.","['Weifeng Jiang', 'Qianren Mao', 'Chenghua Lin', 'Jianxin Li', 'Ting Deng', 'Weiyi Yang', 'Zheng Wang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.12074,Anomali
ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph,"The purpose of this work is to describe the Orkg-Leaderboard software designed to extract leaderboards defined as Task-Dataset-Metric tuples automatically from large collections of empirical research papers in Artificial Intelligence (AI). The software can support both the main workflows of scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the system is integrated with the Open Research Knowledge Graph (ORKG) platform, which fosters the machine-actionable publishing of scholarly findings. Thus the system output, when integrated within the ORKG's supported Semantic Web infrastructure of representing machine-actionable 'resources' on the Web, enables: 1) broadly, the integration of empirical results of researchers across the world, thus enabling transparency in empirical research with the potential to also being complete contingent on the underlying data source(s) of publications; and 2) specifically, enables researchers to track the progress in AI with an overview of the state-of-the-art (SOTA) across the most common AI tasks and their corresponding datasets via dynamic ORKG frontend views leveraging tables and visualization charts over the machine-actionable data. Our best model achieves performances above 90% F1 on the \textit{leaderboard} extraction task, thus proving Orkg-Leaderboards a practically viable tool for real-world usage. Going forward, in a sense, Orkg-Leaderboards transforms the leaderboard extraction task to an automated digitalization task, which has been, for a long time in the community, a crowdsourced endeavor.","['Salomon Kabongo', ""Jennifer D'Souza"", 'Sören Auer']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.11068,Anomali
BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,"Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between twotexts, which is the essence of information retrieval). In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM. Fully fine-grained expression and query-oriented saliency are two properties of the matching signal. Thus, in BERM, a single passage is segmented into multiple units and two unit-level requirements are proposed for representation as the constraint in training to obtain the effective matching signal. One is semantic unit balance and the other is essential matching unit extractability. Unit-level view and balanced semantics make representation express thetextin a fine-grained manner. Essential matching unit extractability makes passage representation sensitive to the given query to extract the pure matching information from the passage containing complex context. Experiments on BEIR show that our method can be effectively combined with different dense retrieval training methods (vanilla, hard negativesminingand knowledge distillation) to improve its generalization ability without any additional inference overhead and target domain data.","['Shicheng Xu', 'Liang Pang', 'Huawei Shen', 'Xueqi Cheng']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.11052,Anomali
OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding,"We introduce OpenShape, a method for learning multi-modal joint representations oftext, image, and point clouds. We adopt the commonly used multi-modal contrastive learning framework for representation alignment, but with a specific focus on scaling up 3D representations to enable open-world 3D shape understanding. To achieve this, we scale up training data by ensembling multiple 3D datasets and propose several strategies to automatically filter and enrich noisytextdescriptions. We also explore and compare strategies for scaling 3D backbone networks and introduce a novel hard negativeminingmodule for more efficient training. We evaluate OpenShape on zero-shot 3D classification benchmarks and demonstrate its superior capabilities for open-world recognition. Specifically, OpenShape achieves a zero-shot accuracy of 46.8% on the 1,156-category Objaverse-LVIS benchmark, compared to less than 10% for existing methods. OpenShape also achieves an accuracy of 85.3% on ModelNet40, outperforming previous zero-shot baseline methods by 20% and performing on par with some fully-supervised methods. Furthermore, we show that our learned embeddings encode a wide range of visual and semantic concepts (e.g., subcategories, color, shape, style) and facilitate fine-grainedtext-3D and image-3D interactions. Due to their alignment with CLIP embeddings, our learned shape representations can also be integrated with off-the-shelf CLIP-based models for various applications, such as point cloud captioning and point cloud-conditioned image generation.","['Minghua Liu', 'Ruoxi Shi', 'Kaiming Kuang', 'Yinhao Zhu', 'Xuanlin Li', 'Shizhong Han', 'Hong Cai', 'Fatih Porikli', 'Hao Su']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.10764,Anomali
CLIP-GCD: Simple Language Guided Generalized Category Discovery,"Generalized Category Discovery (GCD) requires a model to both classify known categories and cluster unknown categories in unlabeled data. Prior methods leveraged self-supervised pre-training combined with supervised fine-tuning on the labeled data, followed by simple clustering methods. In this paper, we posit that such methods are still prone to poor performance on out-of-distribution categories, and do not leverage a key ingredient: Semantic relationships between object categories. We therefore propose to leverage multi-modal (vision and language) models, in two complementary ways. First, we establish a strong baseline by replacing uni-modal features with CLIP, inspired by its zero-shot performance. Second, we propose a novel retrieval-based mechanism that leverages CLIP's aligned vision-language representations byminingtextdescriptions from atextcorpus for the labeled and unlabeled set. We specifically use the alignment between CLIP's visual encoding of the image and textual encoding of the corpus to retrieve top-k relevant pieces oftextand incorporate their embeddings to perform joint image+textsemi-supervised clustering. We perform rigorous experimentation and ablations (including on where to retrieve from, how much to retrieve, and how to combine information), and validate our results on several datasets including out-of-distribution domains, demonstrating state-of-art results.","['Rabah Ouldnoughi', 'Chia-Wen Kuo', 'Zsolt Kira']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.10420,Anomali
Fuzzy Temporal Protoforms for the Quantitative Description of Processes in Natural Language,"In this paper, we propose a series of fuzzy temporal protoforms in the framework of the automatic generation of quantitative and qualitative natural language descriptions of processes. The model includes temporal and causal information from processes and attributes, quantifies attributes in time during the process life-span and recalls causal relations and temporal distances between events, among other features. Through integrating processminingtechniques and fuzzy sets within the usual Data-to-Textarchitecture, our framework is able to extract relevant quantitative temporal as well as structural information from a process and describe it in natural language involving uncertain terms. A real use-case in the cardiology domain is presented, showing the potential of our model for providing natural language explanations addressed to domain experts.","['Yago Fontenla-Seco', 'Alberto Bugarín-Diz', 'Manuel Lama']",,arXiv,2023,this https URL,Anomali
Comparing Variation in Tokenizer Outputs Using a Series of Problematic and Challenging Biomedical Sentences,"Background & Objective: Biomedicaltextdata are increasingly available for research. Tokenization is an initial step in many biomedicaltextminingpipelines. Tokenization is the process of parsing an input biomedical sentence (represented as a digital character sequence) into a discrete set of word/token symbols, which convey focused semantic/syntactic meaning. The objective of this study is to explore variation in tokenizer outputs when applied across a series of challenging biomedical sentences.
  Method: Diaz [2015] introduce 24 challenging example biomedical sentences for comparing tokenizer performance. In this study, we descriptively explore variation in outputs of eight tokenizers applied to each example biomedical sentence. The tokenizers compared in this study are the NLTK white space tokenizer, the NLTK Penn Tree Bank tokenizer, Spacy and SciSpacy tokenizers, Stanza/Stanza-Craft tokenizers, the UDPipe tokenizer, and R-tokenizers.
  Results: For many examples, tokenizers performed similarly effectively; however, for certain examples, there were meaningful variation in returned outputs. The white space tokenizer often performed differently than other tokenizers. We observed performance similarities for tokenizers implementing rule-based systems (e.g. pattern matching and regular expressions) and tokenizers implementing neural architectures for token classification. Oftentimes, the challenging tokens resulting in the greatest variation in outputs, are those words which convey substantive and focused biomedical/clinical meaning (e.g. x-ray, IL-10, TCR/CD3, CD4+ CD8+, and (Ca2+)-regulated).
  Conclusion: When state-of-the-art, open-source tokenizers from Python and R were applied to a series of challenging biomedical example sentences, we observed subtle variation in the returned outputs.","['Christopher Meaney', 'Therese A Stukel', 'Peter C Austin', 'Michael Escobar']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.08787,Anomali
PLIP: Language-Image Pre-training for Person Representation Learning,"Language-image pre-training is an effective technique for learning powerful representations in general domains. However, when directly turning to person representation learning, these general pre-training methods suffer from unsatisfactory performance. The reason is that they neglect critical person-related characteristics, i.e., fine-grained attributes and identities. To address this issue, we propose a novel language-image pre-training framework for person representation learning, termed PLIP. Specifically, we elaborately design three pretext tasks: 1)Text-guided Image Colorization, aims to establish the correspondence between the person-related image regions and the fine-grained color-part textual phrases. 2) Image-guided Attributes Prediction, aims tominefine-grained attribute information of the person body in the image; and 3) Identity-based Vision-Language Contrast, aims to correlate the cross-modal representations at the identity level rather than the instance level. Moreover, to implement our pre-train framework, we construct a large-scale person dataset with image-textpairs named SYNTH-PEDES by automatically generating textual annotations. We pre-train PLIP on SYNTH-PEDES and evaluate our models by spanning downstream person-centric tasks. PLIP not only significantly improves existing methods on all these tasks, but also shows great ability in the zero-shot and domain generalization settings. The code, dataset and weights will be released at~\url{https://github.com/Zplusdragon/PLIP}","['Jialong Zuo', 'Jiahao Hong', 'Feng Zhang', 'Changqian Yu', 'Hanyu Zhou', 'Changxin Gao', 'Nong Sang', 'Jingdong Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2305.08386,Anomali
Hierarchical Aligned Multimodal Learning for NER on Tweet Posts,"Miningstructured knowledge from tweets using named entity recognition (NER) can be beneficial for many down stream applications such as recommendation and intention understanding. With tweet posts tending to be multimodal, multimodal named entity recognition (MNER) has attracted more attention. In this paper, we propose a novel approach, which can dynamically align the image andtextsequence and achieve the multi-level cross-modal learning to augment textual word representation for MNER improvement. To be specific, our framework can be split into three main stages: the first stage focuses on intra-modality representation learning to derive the implicit global and local knowledge of each modality, the second evaluates the relevance between thetextand its accompanying image and integrates different grained visual information based on the relevance, the third enforces semantic refinement via iterative cross-modal interactions and co-attention. We conduct experiments on two open datasets, and the results and detailed analysis demonstrate the advantage of our model.","['Peipei Liu', 'Hong Li', 'Yimo Ren', 'Jie Liu', 'Shuaizong Si', 'Hongsong Zhu', 'Limin Sun']",,arXiv,2024,https://doi.org/10.48550/arXiv.2305.08372,Anomali
Learner-Centered Analysis in Educational Metaverse Environments: Exploring Value Exchange Systems through Natural Interaction and Text Mining,"This paper explores the potential developments of self-directed learning in the metaverse in response to Education 4.0 and the Fourth Industrial Revolution. It highlights the importance of education keeping up with technological advancements and adopting learner-centered approaches. Additionally, it focuses on exploring value exchange systems through natural interaction,textmining, and analysis. The metaverse concept extends beyond extended reality (XR) technologies, encompassing digital avatars and shared ecological value. The role of educators in exploring new technologies and leveragingtext-miningtechniques to enhance learning efficiency is emphasized. The metaverse is presented as a platform for value exchange, necessitating meaningful and valuable content to attract users. Integrating virtual and real-world experiences within the metaverse offers practical applications and contributes to its essence. This paper sheds light on the metaverse's potential to create a learner-centered educational environment and adapt to the evolving landscape of Education 4.0. Its findings, supported bytextmininganalysis, contribute to understanding the metaverse's role in shaping education in the Fourth Industrial Revolution.",['Yun-Cheng Tsai'],,arXiv,2023,https://doi.org/10.48550/arXiv.2305.08326,Anomali
BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text,"The community of different types of microbes present in a biological niche plays a very important role in functioning of the system. The crosstalk or interactions among the different microbes contributes to the building blocks of such microbial community structures. Evidence reported in biomedicaltextserves as a reliable source for predicting such interactions. However, going through the vast and ever-increasing volume of biomedical literature is an intimidating and time consuming process. This necessitates development of automated methods capable of accurately extracting bacterial relations reported in biomedical literature. In this paper, we introduce a method for automated extraction of microbial interactions (specifically between bacteria) from biomedical literature along with ways of using transfer learning to improve its accuracy. We also describe a pipeline using which relations among specific bacteria groups can bemined. Additionally, we introduce the first publicly available dataset which can be used to develop bacterial interaction extraction methods.","['Krishanu Das Baksi', 'Vatsala Pokhrel', 'Kuntal Kumar Bhusan', 'Sharmila Mande']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.07468,Anomali
V2Meow: Meowing to the Visual Beat via Video-to-Music Generation,"Video-to-music generation demands both a temporally localized high-quality listening experience and globally aligned video-acoustic signatures. While recent music generation models excel at the former through advanced audio codecs, the exploration of video-acoustic signatures has been confined to specific visual scenarios. In contrast, our research confronts the challenge of learning globally aligned signatures between video and music directly from paired music and videos, without explicitly modeling domain-specific rhythmic or semantic relationships. We propose V2Meow, a video-to-music generation system capable of producing high-quality music audio for a diverse range of video input types using a multi-stage autoregressive model. Trained on 5k hours of music audio clips paired with video framesminedfrom in-the-wild music videos, V2Meow is competitive with previous domain-specific models when evaluated in a zero-shot manner. It synthesizes high-fidelity music audio waveforms solely by conditioning on pre-trained general-purpose visual features extracted from video frames, with optional style control viatextprompts. Through both qualitative and quantitative evaluations, we demonstrate that our model outperforms various existing music generation systems in terms of visual-audio correspondence and audio quality. Music samples are available at tinyurl.com/v2meow.","['Kun Su', 'Judith Yue Li', 'Qingqing Huang', 'Dima Kuzmin', 'Joonseok Lee', 'Chris Donahue', 'Fei Sha', 'Aren Jansen', 'Yu Wang', 'Mauro Verzetti', 'Timo I. Denk']",,arXiv,2024,https://doi.org/10.48550/arXiv.2305.06594,Anomali
Long-Tailed Question Answering in an Open World,"Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitlyminesknowledge from a large pre-trained language model (LM). Specifically, we organize our model through a pool of fine-grained components and dynamically combine these components for an input to facilitate knowledge sharing. A retrieve-then-rerank frame is further introduced to select in-context examples, which guild the LM to generatetextthat express knowledge for QA tasks. Moreover, a two-stage training approach is introduced to pre-train the framework by knowledge distillation (KD) from the LM and then jointly train the frame and a QA model through an adaptive mutual KD method. On a large-scale OLTQA dataset we curate from 43 existing QA datasets, our model consistently outperforms the state-of-the-art. We release the code and data at \url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/oltqa}.","['Yi Dai', 'Hao Lang', 'Yinhe Zheng', 'Fei Huang', 'Yongbin Li']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.06557,Anomali
PLM-GNN: A Webpage Classification Method based on Joint Pre-trained Language Model and Graph Neural Network,"The number of web pages is growing at an exponential rate, accumulating massive amounts of data on the web. It is one of the key processes to classify webpages in web informationmining. Some classical methods are based on manually building features of web pages and training classifiers based on machine learning or deep learning. However, building features manually requires specific domain knowledge and usually takes a long time to validate the validity of features. Considering webpages generated by the combination oftextand HTML Document Object Model(DOM) trees, we propose a representation and classification method based on a pre-trained language model and graph neural network, named PLM-GNN. It is based on the joint encoding oftextand HTML DOM trees in the web pages. It performs well on the KI-04 and SWDE datasets and on practical dataset AHS for the project of scholar's homepage crawling.","['Qiwei Lang', 'Jingbo Zhou', 'Haoyi Wang', 'Shiqi Lyu', 'Rui Zhang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.05378,Anomali
Linguistic More: Taking a Further Step toward Efficient and Accurate Scene Text Recognition,"Vision model have gained increasing attention due to their simplicity and efficiency in SceneTextRecognition (STR) task. However, due to lacking the perception of linguistic knowledge and information, recent vision models suffer from two problems: (1) the pure vision-based query results in attention drift, which usually causes poor recognition and is summarized as linguistic insensitive drift (LID) problem in this paper. (2) the visual feature is suboptimal for the recognition in some vision-missing cases (e.g. occlusion, etc.). To address these issues, we propose a $\textbf{L}$inguistic $\textbf{P}$erception $\textbf{V}$ision model (LPV), which explores the linguistic capability of vision model for accuratetextrecognition. To alleviate the LID problem, we introduce a Cascade Position Attention (CPA) mechanism that obtains high-quality and accurate attention maps through step-wise optimization and linguistic informationmining. Furthermore, a Global Linguistic Reconstruction Module (GLRM) is proposed to improve the representation of visual features by perceiving the linguistic information in the visual space, which gradually converts visual features into semantically rich ones during the cascade process. Different from previous methods, our method obtains SOTA results while keeping low complexity (92.4% accuracy with only 8.11M parameters). Code is available at https://github.com/CyrilSterling/LPV.","['Boqiang Zhang', 'Hongtao Xie', 'Yuxin Wang', 'Jianjun Xu', 'Yongdong Zhang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.05140,Anomali
Towards Understanding Machine Learning Testing in Practise,"Visualisations drive all aspects of the Machine Learning (ML) Development Cycle but remain a vastly untapped resource by the research community. ML testing is a highly interactive and cognitive process which demands a human-in-the-loop approach. Besides writing tests for the code base, bulk of the evaluation requires application of domain expertise to generate and interpret visualisations. To gain a deeper insight into the process of testing ML systems, we propose to study visualisations of ML pipelines byminingJupyter notebooks. We propose a two prong approach in conducting the analysis. First, gather general insights and trends using a qualitative study of a smaller sample of notebooks. And then use the knowledge gained from the qualitative study to design an empirical study using a larger sample of notebooks. Computational notebooks provide a rich source of information in three formats --text, code and images. We hope to utilise existing work in image analysis and Natural Language Processing fortextand code, to analyse the information present in notebooks. We hope to gain a new perspective into program comprehension and debugging in the context of ML testing.","['Arumoy Shome', 'Luis Cruz', 'Arie van Deursen']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.04988,Anomali
Cross-Modal Retrieval for Motion and Text via DopTriple Loss,"Cross-modal retrieval of image-textand video-textis a prominent research area in computer vision and natural language processing. However, there has been insufficient attention given to cross-modal retrieval between human motion andtext, despite its wide-ranging applicability. To address this gap, we utilize a concise yet effective dual-unimodal transformer encoder for tackling this task. Recognizing that overlapping atomic actions in different human motion sequences can lead to semantic conflicts between samples, we explore a novel triplet loss function called DropTriple Loss. This loss function discards false negative samples from the negative sample set and focuses onminingremaining genuinely hard negative samples for triplet training, thereby reducing violations they cause. We evaluate our model and approach on the HumanML3D and KIT Motion-Language datasets. On the latest HumanML3D dataset, we achieve a recall of 62.9% for motion retrieval and 71.5% fortextretrieval (both based on R@10). The source code for our approach is publicly available at https://github.com/eanson023/rehamot.","['Sheng Yan', 'Yang Liu', 'Haoqiang Wang', 'Xin Du', 'Mengyuan Liu', 'Hong Liu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.04195,Anomali
evaluating bert and parsbert for analyzing persian advertisement data,"This paper discusses the impact of the Internet on modern trading and the importance of data generated from these transactions for organizations to improve their marketing efforts. The paper uses the example of Divar, an online marketplace for buying and selling products and services in Iran, and presents a competition to predict the percentage of a car sales ad that would be published on the Divar website. Since the dataset provides a rich source of Persiantextdata, the authors use the Hazm library, a Python library designed for processing Persiantext, and two state-of-the-art language models, mBERT and ParsBERT, to analyze it. The paper's primary objective is to compare the performance of mBERT and ParsBERT on the Divar dataset. The authors provide some background on datamining, Persian language, and the two language models, examine the dataset's composition and statistical features, and provide details on their fine-tuning and training configurations for both approaches. They present the results of their analysis and highlight the strengths and weaknesses of the two language models when applied to Persiantextdata. The paper offers valuable insights into the challenges and opportunities of working with low-resource languages such as Persian and the potential of advanced language models like BERT for analyzing such data. The paper also explains the dataminingprocess, including steps such as data cleaning and normalization techniques. Finally, the paper discusses the types of machine learning problems, such as supervised, unsupervised, and reinforcement learning, and the pattern evaluation techniques, such as confusion matrix. Overall, the paper provides an informative overview of the use of language models and dataminingtechniques for analyzingtextdata in low-resource languages, using the example of the Divar dataset.","['Ali Mehrban', 'Pegah Ahadian']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.02426,Anomali
Causality-aware Concept Extraction based on Knowledge-guided Prompting,"Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used intext-based concept extraction (CE). However, PLMs tend tominethe co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens. As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt can effectively alleviate concept bias and improve the performance of PLM-based CE models.The code has been released on https://github.com/siyuyuan/KPCE.","['Siyu Yuan', 'Deqing Yang', 'Jinxi Liu', 'Shuyu Tian', 'Jiaqing Liang', 'Yanghua Xiao', 'Rui Xie']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.01876,Anomali
SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation,"Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging fromtextmining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which usetextminingand textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tuning it with an object detection model. We show that our pipeline sets a new benchmark in this context and performs at par with the existing methods and the supervised counterparts, if not outperforms. The code is made publicly available at: https://github.com/MaitySubhajit/SelfDocSeg","['Subhajit Maity', 'Sanket Biswas', 'Siladittya Manna', 'Ayan Banerjee', 'Josep Lladós', 'Saumik Bhattacharya', 'Umapada Pal']","ICDAR 2023 (International Conference on Document Analysis and Recognition) Lecture Notes in Computer Science, vol 14187, pp. 342-360. Springer Nature",arXiv,2023,https://doi.org/10.48550/arXiv.2305.00795,Anomali
Boosting Weakly-Supervised Temporal Action Localization with Text Information,"Due to the lack of temporal annotation, current Weakly-supervised Temporal Action Localization (WTAL) methods are generally stuck into over-complete or incomplete localization. In this paper, we aim to leverage thetextinformation to boost WTAL from two aspects, i.e., (a) the discriminative objective to enlarge the inter-class difference, thus reducing the over-complete; (b) the generative objective to enhance the intra-class integrity, thus finding more complete temporal boundaries. For the discriminative objective, we propose aText-SegmentMining(TSM) mechanism, which constructs atextdescription based on the action class label, and regards thetextas the query tomineall class-related segments. Without the temporal annotation of actions, TSM compares thetextquery with the entire videos across the dataset tominethe best matching segments while ignoring irrelevant ones. Due to the shared sub-actions in different categories of videos, merely applying TSM is too strict to neglect the semantic-related segments, which results in incomplete localization. We further introduce a generative objective named Video-textLanguage Completion (VLC), which focuses on all semantic-related segments from videos to complete thetextsentence. We achieve the state-of-the-art performance on THUMOS14 and ActivityNet1.3. Surprisingly, we also find our proposed method can be seamlessly applied to existing methods, and improve their performances with a clear margin. The code is available at https://github.com/lgzlIlIlI/Boosting-WTAL.","['Guozhang Li', 'De Cheng', 'Xinpeng Ding', 'Nannan Wang', 'Xiaoyu Wang', 'Xinbo Gao']",,arXiv,2023,https://doi.org/10.48550/arXiv.2305.00607,Anomali
Patent Mining by Extracting Functional Analysis Information Modelled As Graph Structure: A Patent Knowledge-base Collaborative Building Approach,"Patents provide a rich source of information about design innovations. Patentminingtechniques employ various technologies, such astextmining, machine learning, natural language processing, and ontology-building techniques. An automated graph data modelling method is proposed for extracting functional representations for building a semantic database of patents of mechanical designs. The method has several benefits: The schema-free characteristic of the proposed graph modelling enables the ontology it is based on to evolve and generalise to upper ontologies across technology domains and to specify lower ontologies to more specific domains. Graph modelling benefits from enhanced performance of deep queries across many levels of relationships and interactions and provides efficient storage. Graph modelling also enables visualisation libraries to use the graph data structure immediately, avoiding the need for graph extraction programs from relational databases. Patent/Design comparisons are computed by search queries using counting of overlaps of different levels and weights. This work has produced the PatMine SolidWorks Add-in \c{opyright}, which compares annotated CAD designs with patents and highlights overlapping design concepts. The patent annotation extracts its functional analysis, representing its structure as geometric feature interactions. Additional features such as full-textsearch and semantic search of the PatMine patents database are available, and graph analytic methods and machine learning algorithms are enabled and can be implemented as plug-ins in future work. Keywords: PatentMining; Semantic Analysis; Functional Analysis Diagrams; Graph Data Modelling; Visualisation; Similarity Scoring; Big Data Analytics; Machine Learning; Artificial Intelligence; Natural Language Processing","['Manal E. Helal', 'Mohammed E. Helal']",,arXiv,2024,https://doi.org/10.48550/arXiv.2305.00309,Anomali
The Emotions of the Crowd: Learning Image Sentiment from Tweets via Cross-modal Distillation,"Trends and opinionminingin social media increasingly focus on novel interactions involving visual media, like images and short videos, in addition totext. In this work, we tackle the problem of visual sentiment analysis of social media images -- specifically, the prediction of image sentiment polarity. While previous work relied on manually labeled training sets, we propose an automated approach for building sentiment polarity classifiers based on a cross-modal distillation paradigm; starting from scraped multimodal (text+ images) data, we train a student model on the visual modality based on the outputs of a textual teacher model that analyses the sentiment of the corresponding textual modality. We applied our method to randomly collected images crawled from Twitter over three months and produced, after automatic cleaning, a weakly-labeled dataset of $\sim$1.5 million images. Despite exploiting noisy labeled samples, our training pipeline produces classifiers showing strong generalization capabilities and outperforming the current state of the art on five manually labeled benchmarks for image sentiment polarity prediction.","['Alessio Serra', 'Fabio Carrara', 'Maurizio Tesconi', 'Fabrizio Falchi']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.14942,Anomali
Assessing Text Mining and Technical Analyses on Forecasting Financial Time Series,"Forecasting financial time series (FTS) is an essential field in finance and economics that anticipates market movements in financial markets. This paper investigates the accuracy oftextminingand technical analyses in forecasting financial time series. It focuses on the S&P500 stock market index during the pandemic, which tracks the performance of the largest publicly traded companies in the US. The study compares two methods of forecasting the future price of the S&P500:textmining, which uses NLP techniques to extract meaningful insights from financial news, and technical analysis, which uses historical price and volume data to make predictions. The study examines the advantages and limitations of both methods and analyze their performance in predicting the S&P500. The FinBERT model outperforms other models in terms of S&P500 price prediction, as evidenced by its lower RMSE value, and has the potential to revolutionize financial analysis and prediction using financial news data. Keywords: ARIMA, BERT, FinBERT, Forecasting Financial Time Series, GARCH, LSTM, Technical Analysis,TextMiningJEL classifications: G4, C8",['Ali Lashgari'],,arXiv,2023,https://doi.org/10.48550/arXiv.2304.14544,Anomali
Analyzing categorical time series with the R package ctsfeatures,"Time series data are ubiquitous nowadays. Whereas most of the literature on the topic deals with real-valued time series, categorical time series have received much less attention. However, the development of dataminingtechniques for this kind of data has substantially increased in recent years. The R package ctsfeatures offers users a set of useful tools for analyzing categorical time series. In particular, several functions allowing the extraction of well-known statistical features and the construction of illustrative graphs describing underlying temporal patterns are provided in the package. The output of some functions can be employed to perform traditional machine learning tasks including clustering, classification and outlier detection. The package also includes two datasets of biological sequences introduced in the literature for clustering purposes, as well as three interesting synthetic databases. In this work, the main characteristics of the package are described and its use is illustrated through various examples. Practitioners from a wide variety of fields could benefit from the valuable tools provided by ctsfeatures.","['Ángel López Oriona', 'José Antonio Vilar Fernández']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.12332,Anomali
Hierarchical Contrastive Learning Enhanced Heterogeneous Graph Neural Network,"Heterogeneous graph neural networks (HGNNs) as an emerging technique have shown superior capacity of dealing with heterogeneous information network (HIN). However, most HGNNs follow a semi-supervised learning manner, which notably limits their wide use in reality since labels are usually scarce in real applications. Recently, contrastive learning, a self-supervised method, becomes one of the most exciting learning paradigms and shows great potential when there are no labels. In this paper, we study the problem of self-supervised HGNNs and propose a novel co-contrastive learning mechanism for HGNNs, named HeCo. Different from traditional contrastive learning which only focuses on contrasting positive and negative samples, HeCo employs cross-view contrastive mechanism. Specifically, two views of a HIN (network schema and meta-path views) are proposed to learn node embeddings, so as to capture both of local and high-order structures simultaneously. Then the cross-view contrastive learning, as well as a view mask mechanism, is proposed, which is able to extract the positive and negative embeddings from two views. This enables the two views to collaboratively supervise each other and finally learn high-level node embeddings. Moreover, to further boost the performance of HeCo, two additional methods are designed to generate harder negative samples with high quality. Besides the invariant factors, view-specific factors complementally provide the diverse structure information between different nodes, which also should be contained into the final embeddings. Therefore, we need to further explore each view independently and propose a modified model, called HeCo++. Specifically, HeCo++ conducts hierarchical contrastive learning, including cross-view and intra-view contrasts, which aims to enhance theminingof respective structures.","['Nian Liu', 'Xiao Wang', 'Hui Han', 'Chuan Shi']",,arXiv,2024,https://doi.org/10.48550/arXiv.2304.12228,Anomali
A Reference Model for Collaborative Business Intelligence Virtual Assistants,"Collaborative Business Analysis (CBA) is a methodology that involves bringing together different stakeholders, including business users, analysts, and technical specialists, to collaboratively analyze data and gain insights into business operations. The primary objective of CBA is to encourage knowledge sharing and collaboration between the different groups involved in business analysis, as this can lead to a more comprehensive understanding of the data and better decision-making. CBA typically involves a range of activities, including data gathering and analysis, brainstorming, problem-solving, decision-making and knowledge sharing. These activities may take place through various channels, such as in-person meetings, virtual collaboration tools or online forums. This paper deals with virtual collaboration tools as an important part of Business Intelligence (BI) platform. Collaborative Business Intelligence (CBI) tools are becoming more user-friendly, accessible, and flexible, allowing users to customize their experience and adapt to their specific needs. The goal of a virtual assistant is to make data exploration more accessible to a wider range of users and to reduce the time and effort required for data analysis. It describes the unified business intelligence semantic model, coupled with a data warehouse and collaborative unit to employ dataminingtechnology. Moreover, we propose a virtual assistant for CBI and a reference model of virtual tools for CBI, which consists of three components: conversational, data exploration and recommendation agents. We believe that the allocation of these three functional tasks allows you to structure the CBI issue and apply relevant and productive models for human-like dialogue,text-to-command transferring, and recommendations simultaneously. The complex approach based on these three points gives the basis for virtual tool for collaboration. CBI encourages people, processes, and technology to enable everyone sharing and leveraging collective expertise, knowledge and data to gain valuable insights for making better decisions. This allows to respond more quickly and effectively to changes in the market or internal operations and improve the progress.","['Olga Cherednichenko', 'Fahad Muhammad', 'Jérôme Darmont', 'Cécile Favre']","6th International Conference on Computational Linguistics and Intelligent Systems (CoLInS 2022), Apr 2023, Kharkiv, Ukraine",arXiv,2023,https://doi.org/10.48550/arXiv.2304.10556,Anomali
NetGPT: Generative Pretrained Transformer for Network Traffic,"All data on the Internet are transferred by network traffic, thus accurately modeling network traffic can help improve network services quality and protect data privacy. Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as application classification, attack detection and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.
  To tackle these challenges, in this paper, we make the first attempt to provide a generative pretrained model NetGPT for both traffic understanding and generation tasks. We propose the multi-pattern network traffic modeling to construct unifiedtextinputs and support both traffic understanding and generation tasks. We further optimize the adaptation effect of the pretrained model to diversified tasks by shuffling header fields, segmenting packets in flows, and incorporating diverse task labels with prompts. With diverse traffic datasets from encrypted software, DNS, private industrial protocols and cryptocurrencymining, expensive experiments demonstrate the effectiveness of our NetGPT in a range of traffic understanding and generation tasks on traffic datasets, and outperform state-of-the-art baselines by a wide margin.","['Xuying Meng', 'Chungang Lin', 'Yequan Wang', 'Yujun Zhang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.09513,Anomali
EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text,"Background Medical research generates millions of publications and it is a great challenge for researchers to utilize this information in full since its scale and complexity greatly surpasses human reading capabilities. Automatedtextminingcan help extract and connect information spread across this large body of literature but this technology is not easily accessible to life scientists. Results Here, we developed an easy-to-use end-to-end pipeline for deep learning- and dictionary-based named entity recognition (NER) of typical entities found in medical research articles, including diseases, cells, chemicals, genes/proteins, and species. The pipeline can access and process large medical research article collections (PubMed, CORD-19) or rawtextand incorporates a series of deep learning models fine-tuned on the HUNER corpora collection. In addition, the pipeline can perform dictionary-based NER related to COVID-19 and other medical topics. Users can also load their own NER models and dictionaries to include additional entities. The output consists of publication-ready ranked lists and graphs of detected entities and files containing the annotatedtexts. An associated script allows rapid inspection of the results for specific entities of interest. As model use cases, the pipeline was deployed on two collections of autophagy-related abstracts from PubMed and on the CORD19 dataset, a collection of 764 398 research article abstracts related to COVID-19. Conclusions The NER pipeline we present is applicable in a variety of medical research settings and makes customizabletextminingaccessible to life scientists.","['Rafsan Ahmed', 'Petter Berntsson', 'Alexander Skafte', 'Salma Kazemi Rashed', 'Marcus Klang', 'Adam Barvesten', 'Ola Olde', 'William Lindholm', 'Antton Lamarca Arrizabalaga', 'Pierre Nugues', 'Sonja Aits']",,arXiv,2024,https://doi.org/10.48550/arXiv.2304.07805,Anomali
A Clustering Framework for Unsupervised and Semi-supervised New Intent Discovery,"New intent discovery is of great value to natural language processing, allowing for a better understanding of user needs and providing friendly services. However, most existing methods struggle to capture the complicated semantics of discretetextrepresentations when limited or no prior knowledge of labeled data is available. To tackle this problem, we propose a novel clustering framework, USNID, for unsupervised and semi-supervised new intent discovery, which has three key technologies. First, it fully utilizes unsupervised or semi-supervised data tomineshallow semantic similarity relations and provide well-initialized representations for clustering. Second, it designs a centroid-guided clustering mechanism to address the issue of cluster allocation inconsistency and provide high-quality self-supervised targets for representation learning. Third, it captures high-level semantics in unsupervised or semi-supervised data to discover fine-grained intent-wise clusters by optimizing both cluster-level and instance-level objectives. We also propose an effective method for estimating the cluster number in open-world scenarios without knowing the number of new intents beforehand. USNID performs exceptionally well on several benchmark intent datasets, achieving new state-of-the-art results in unsupervised and semi-supervised new intent discovery and demonstrating robust performance with different cluster numbers.","['Hanlei Zhang', 'Hua Xu', 'Xin Wang', 'Fei Long', 'Kai Gao']",IEEE Transactions on Knowledge and Data Engineering 2023,arXiv,2023,https://doi.org/10.48550/arXiv.2304.07699,Anomali
Zero-Shot Multi-Label Topic Inference with Sentence Encoders,"Sentence encoders have indeed been shown to achieve superior performances for many downstreamtext-miningtasks and, thus, claimed to be fairly general. Inspired by this, we performed a detailed study on how to leverage these sentence encoders for the ""zero-shot topic inference"" task, where the topics are defined/provided by the users in real-time. Extensive experiments on seven different datasets demonstrate that Sentence-BERT demonstrates superior generality compared to other encoders, while Universal Sentence Encoder can be preferred when efficiency is a top priority.","['Souvika Sarkar', 'Dongji Feng', 'Shubhra Kanti Karmaker Santu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.07382,Anomali
Delay Impact on Stubborn Mining Attack Severity in Imperfect Bitcoin Network,"Stubbornminingattack greatly downgrades Bitcoin throughput and also benefits malicious miners (attackers). This paper aims to quantify the impact of block receiving delay on stubbornminingattack severity in imperfect Bitcoin networks. We develop an analytic model and derive formulas of both relative revenue and system throughput, which are applied to study attack severity. Experiment results validate our analysis method and show that imperfect networks favor attackers. The quantitative analysis offers useful insight into stubbornminingattack and then helps the development of countermeasures.","['Haoran Zhu', 'Xiaolin Chang', 'Jelena Mišić', 'Vojislav B. Mišić']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.06963,Anomali
Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study,"Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly interested in whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of \emph{opinions}, \emph{sentiments}, and \emph{emotions} contained in thetext. Specifically, we evaluate it in three settings, including \emph{standard} evaluation, \emph{polarity shift} evaluation and \emph{open-domain} evaluation. We conduct an evaluation on 7 representative sentiment analysis tasks covering 17 benchmark datasets and compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on them. We also attempt several popular prompting techniques to elicit the ability further. Moreover, we conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.","['Zengzhi Wang', 'Qiming Xie', 'Yi Feng', 'Zixiang Ding', 'Zinong Yang', 'Rui Xia']",,arXiv,2024,https://doi.org/10.48550/arXiv.2304.04339,Anomali
FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain,"This paper introduces FrenchMedMCQA, the first publicly available Multiple-Choice Question Answering (MCQA) dataset in French for medical domain. It is composed of 3,105 questions taken from real exams of the French medical specialization diploma in pharmacy, mixing single and multiple answers. Each instance of the dataset contains an identifier, a question, five possible answers and their manual correction(s). We also propose first baseline models to automatically process this MCQA task in order to report on the current performances and to highlight the difficulty of the task. A detailed analysis of the results showed that it is necessary to have representations adapted to the medical domain or to the MCQA task: in our case, English specialized models yielded better results than generic French ones, even though FrenchMedMCQA is in French. Corpus, models and tools are available online.","['Yanis Labrak', 'Adrien Bazoge', 'Richard Dufour', 'Mickael Rouvier', 'Emmanuel Morin', 'Béatrice Daille', 'Pierre-Antoine Gourraud']",Proceedings of the 13th International Workshop on Health Text Mining and Information Analysis (LOUHI 2022),arXiv,2023,https://doi.org/10.48550/arXiv.2304.04280,Anomali
Do Subjectivity and Objectivity Always Agree? A Case Study with Stack Overflow Questions,"In Stack Overflow (SO), the quality of posts (i.e., questions and answers) is subjectively evaluated by users through a voting mechanism. The net votes (upvotes - downvotes) obtained by a post are often considered an approximation of its quality. However, about half of the questions that received working solutions got more downvotes than upvotes. Furthermore, about 18% of the accepted answers (i.e., verified solutions) also do not score the maximum votes. All these counter-intuitive findings cast doubts on the reliability of the evaluation mechanism employed at SO. Moreover, many users raise concerns against the evaluation, especially downvotes to their posts. Therefore, rigorous verification of the subjective evaluation is highly warranted to ensure a non-biased and reliable quality assessment mechanism. In this paper, we compare the subjective assessment of questions with their objective assessment using 2.5 million questions and tentextanalysis metrics. According to our investigation, four objective metrics agree with the subjective evaluation, two do not agree, one either agrees or disagrees, and the remaining three neither agree nor disagree with the subjective evaluation. We then develop machine learning models to classify the promoted and discouraged questions. Our models outperform the state-of-the-art models with a maximum of about 76% - 87% accuracy.","['Saikat Mondal', 'Mohammad Masudur Rahman', 'Chanchal K. Roy']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.03563,Anomali
Affect as a proxy for literary mood,"We propose to use affect as a proxy for mood in literarytexts. In this study, we explore the differences in computationally detecting tone versus detecting mood. Methodologically we utilize affective word embeddings to look at the affective distribution in differenttextsegments. We also present a simple yet efficient and effective method of enhancing emotion lexicons to take both semantic shift and the domain of thetextinto account producing real-world congruent results closely matching both contemporary and modern qualitative analyses.","['Emily Öhman', 'Riikka Rossi']","Journal of Data Mining & Digital Humanities, NLP4DH (August 13, 2023) jdmdh:11164",arXiv,2023,https://doi.org/10.48550/arXiv.2304.02894,Anomali
Specialty-Oriented Generalist Medical AI for Chest CT Screening,"Modern medical records include a vast amount of multimodal freetextclinical data and imaging data from radiology, cardiology, and digital pathology. Fullyminingsuch big data requires multitasking; otherwise, occult but important aspects may be overlooked, adversely affecting clinical management and population healthcare. Despite remarkable successes of AI in individual tasks with single-modal data, the progress in developing generalist medical AI remains relatively slow to combine multimodal data for multitasks because of the dual challenges of data curation and model architecture. The data challenge involves querying and curating multimodal structured and unstructuredtext, alphanumeric, and especially 3D tomographic scans on an individual patient level for real-time decisions and on a scale to estimate population health statistics. The model challenge demands a scalable and adaptable network architecture to integrate multimodal datasets for diverse clinical tasks. Here we propose the first-of-its-kind medical multimodal-multitask foundation model (M3FM) with application in lung cancer screening and related tasks. After we curated a comprehensive multimodal multitask dataset consisting of 49 clinical data types including 163,725 chest CT series and 17 medical tasks involved in LCS, we develop a multimodal question-answering framework as a unified training and inference strategy to synergize multimodal information and perform multiple tasks via free-textprompting. M3FM consistently outperforms the state-of-the-art single-modal task-specific models, identifies multimodal data elements informative for clinical tasks and flexibly adapts to new tasks with a small out-of-distribution dataset. As a specialty-oriented generalist medical AI model, M3FM paves the way for similar breakthroughs in other areas of medicine, closing the gap between specialists and the generalist.","['Chuang Niu', 'Qing Lyu', 'Christopher D. Carothers', 'Parisa Kaviani', 'Josh Tan', 'Pingkun Yan', 'Mannudeep K. Kalra', 'Christopher T. Whitlow', 'Ge Wang']",,arXiv,2024,https://doi.org/10.48550/arXiv.2304.02649,Anomali
FASTAGEDS: Fast Approximate Graph Entity Dependency Discovery,"This paper studies the discovery of approximate rules in property graphs. We propose a semantically meaningful measure of error formininggraph entity dependencies (GEDs) at almost hold, to tolerate errors and inconsistencies that exist in real-world graphs. We present a new characterisation of GED satisfaction, and devise a depth-first search strategy to traverse the search space of candidate rules efficiently. Further, we perform experiments to demonstrate the feasibility and scalability of our solution, FASTAGEDS, with three real-world graphs.","['Guangtong Zhou', 'Selasi Kwashie', 'Yidi Zhang', 'Michael Bewong', 'Vincent M. Nofong', 'Debo Cheng', 'Keqing He', 'Zaiwen Feng']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.02323,Anomali
Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data,"Scaling up weakly-supervised datasets has shown to be highly effective in the image-textdomain and has contributed to most of the recent state-of-the-art computer vision and multimodal neural networks. However, existing large-scale video-textdatasets andminingtechniques suffer from several limitations, such as the scarcity of aligned data, the lack of diversity in the data, and the difficulty of collecting aligned data. Currently popular video-textdataminingapproach via automatic speech recognition (ASR) used in HowTo100M provides low-quality captions that often do not refer to the video content. Otherminingapproaches do not provide proper language descriptions (video tags) and are biased toward short clips (alttext). In this work, we show how recent advances in image captioning allow us to pre-train high-quality video models without any parallel video-textdata. We pre-train several video captioning models that are based on an OPT language model and a TimeSformer visual backbone. We fine-tune these networks on several video captioning datasets. First, we demonstrate that image captioning pseudolabels work better for pre-training than the existing HowTo100M ASR captions. Second, we show that pre-training on both images and videos produces a significantly better network (+4 CIDER on MSR-VTT) than pre-training on a single modality. Our methods are complementary to the existing pre-training or dataminingapproaches and can be used in a variety of settings. Given the efficacy of the pseudolabeling method, we are planning to publicly release the generated captions.","['Vladislav Lialin', 'Stephen Rawls', 'David Chan', 'Shalini Ghosh', 'Anna Rumshisky', 'Wael Hamza']",2023 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),arXiv,2023,https://doi.org/10.48550/arXiv.2304.02080,Anomali
Multidimensional Perceptron for Efficient and Explainable Long Text Classification,"Because of the inevitable cost and complexity of transformer and pre-trained models, efficiency concerns are raised for longtextclassification. Meanwhile, in the highly sensitive domains, e.g., healthcare and legal long-textmining, potential model distrust, yet underrated and underexplored, may hatch vital apprehension. Existing methods generally segment the longtext, encode each piece with the pre-trained model, and use attention or RNNs to obtain longtextrepresentation for classification. In this work, we propose a simple but effective model, Segment-aWare multIdimensional PErceptron (SWIPE), to replace attention/RNNs in the above framework. Unlike prior efforts, SWIPE can effectively learn the label of the entiretextwith supervised training, while perceive the labels of the segments and estimate their contributions to the long-textlabeling in an unsupervised manner. As a general classifier, SWIPE can endorse different encoders, and it outperforms SOTA models in terms of classification accuracy and model efficiency. It is noteworthy that SWIPE achieves superior interpretability to transparentize longtextclassification results.","['Yexiang Wang', 'Yating Zhang', 'Xiaozhong Liu', 'Changlong Sun']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.01638,Anomali
Thematic context vector association based on event uncertainty for Twitter,"Keyword extraction is a crucial process intextmining. The extraction of keywords with respective contextual events in Twitter data is a big challenge. The challenging issues are mainly because of the informality in the language used. The use of misspelled words, acronyms, and ambiguous terms causes informality. The extraction of keywords with informal language in current systems is pattern based or event based. In this paper, contextual keywords are extracted using thematic events with the help of data association. The thematic context for events is identified using the uncertainty principle in the proposed system. The thematic contexts are weighed with the help of vectors called thematic context vectors which signifies the event as certain or uncertain. The system is tested on the Twitter COVID-19 dataset and proves to be effective. The system extracts event-specific thematic context vectors from the test dataset and ranks them. The extracted thematic context vectors are used for the clustering of contextual thematic vectors which improves the silhouette coefficient by 0.5% than state of art methods namely TF and TF-IDF. The thematic context vector can be used in other applications like Cyberbullying, sarcasm detection, figurative language detection, etc.","['Vaibhav Khatavkar', 'Swapnil Mane', 'Parag Kulkarni']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.01423,Anomali
Multi-Modal Representation Learning with Text-Driven Soft Masks,"We propose a visual-linguistic representation learning approach within a self-supervised learning framework by introducing a new operation, loss, and data augmentation strategy. First, we generate diverse features for the image-textmatching (ITM) task via soft-masking the regions in an image, which are most relevant to a certain word in the corresponding caption, instead of completely removing them. Since our framework relies only on image-caption pairs with no fine-grained annotations, we identify the relevant regions to each word by computing the word-conditional visual attention using multi-modal encoder. Second, we encourage the model to focus more on hard but diverse examples by proposing a focal loss for the image-textcontrastive learning (ITC) objective, which alleviates the inherent limitations of overfitting and bias issues. Last, we perform multi-modal data augmentations for self-supervised learning viaminingvarious examples by maskingtextsand rendering distortions on images. We show that the combination of these three innovations is effective for learning a pretrained model, leading to outstanding performance on multiple vision-language downstream tasks.","['Jaeyoo Park', 'Bohyung Han']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.00719,Anomali
Subject-driven Text-to-Image Generation via Apprenticeship Learning,"Recenttext-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-drivenText-to-Image generator that replaces subject-specific fine tuning with in-context learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by apprenticeship learning, where a single apprentice model is learned from data generated by a massive number of subject-specific expert models. Specifically, weminemillions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train a massive number of expert models, each specializing in a different subject. The apprentice model SuTI then learns to imitate the behavior of these fine-tuned experts. SuTI can generate high-quality and customized subject-specific images 20x faster than optimization-based SoTA methods. On the challenging DreamBench and DreamBench-v2, our human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth, especially on the subject andtextalignment aspects.","['Wenhu Chen', 'Hexiang Hu', 'Yandong Li', 'Nataniel Ruiz', 'Xuhui Jia', 'Ming-Wei Chang', 'William W. Cohen']",,arXiv,2023,https://doi.org/10.48550/arXiv.2304.00186,Anomali
Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text,"Detecting protein-protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. However, with the fast-paced growth of biomedical literature, there is a growing need for automated and accurate extraction of PPIs to facilitate scientific knowledge discovery. Pre-trained language models, such as generative pre-trained transformers (GPT) and bidirectional encoder representations from transformers (BERT), have shown promising results in natural language processing (NLP) tasks. We evaluated the performance of PPI identification of multiple GPT and BERT models using three manually curated gold-standard corpora: Learning Language in Logic (LLL) with 164 PPIs in 77 sentences, Human Protein Reference Database with 163 PPIs in 145 sentences, and Interaction Extraction Performance Assessment with 335 PPIs in 486 sentences. BERT-based models achieved the best overall performance, with BioBERT achieving the highest recall (91.95%) and F1-score (86.84%) and PubMedBERT achieving the highest precision (85.25%). Interestingly, despite not being explicitly trained for biomedicaltexts, GPT-4 achieved commendable performance, comparable to the top-performing BERT models. It achieved a precision of 88.37%, a recall of 85.14%, and an F1-score of 86.49% on the LLL dataset. These results suggest that GPT models can effectively detect PPIs fromtextdata, offering promising avenues for application in biomedical literaturemining. Further research could explore how these models might be fine-tuned for even more specialized tasks within the biomedical domain.","['Hasin Rehana', 'Nur Bengisu Çam', 'Mert Basmaci', 'Jie Zheng', 'Christianah Jemiyo', 'Yongqun He', 'Arzucan Özgür', 'Junguk Hur']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.17728,Anomali
TraVaG: Differentially Private Trace Variant Generation Using GANs,"Processminingis rapidly growing in the industry. Consequently, privacy concerns regarding sensitive and private information included in event data, used by processminingalgorithms, are becoming increasingly relevant. State-of-the-art research mainly focuses on providing privacy guarantees, e.g., differential privacy, for trace variants that are used by the main processminingtechniques, e.g., process discovery. However, privacy preservation techniques for releasing trace variants still do not fulfill all the requirements of industry-scale usage. Moreover, providing privacy guarantees when there exists a high rate of infrequent trace variants is still a challenge. In this paper, we introduce TraVaG as a new approach for releasing differentially private trace variants based on \text{Generative Adversarial Networks} (GANs) that provides industry-scale benefits and enhances the level of privacy guarantees when there exists a high ratio of infrequent variants. Moreover, TraVaG overcomes shortcomings of conventional privacy preservation techniques such as bounding the length of variants and introducing fake variants. Experimental results on real-life event data show that our approach outperforms state-of-the-art techniques in terms of privacy guarantees, plain data utility preservation, and result utility preservation.","['Majid Rafiei', 'Frederik Wangelik', 'Mahsa Pourbafrani', 'Wil M. P. van der Aalst']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.16704,Anomali
On Codex Prompt Engineering for OCL Generation: An Empirical Study,"The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to MOF models. Despite its potential to provide precision and conciseness to UML models, the unfamiliar syntax of OCL has hindered its adoption. Recent advancements in LLMs, such as GPT-3, have shown their capability in many NLP tasks, including semantic parsing andtextgeneration. Codex, a GPT-3 descendant, has been fine-tuned on publicly available code from GitHub and can generate code in many programming languages. We investigate the reliability of OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications and crafted a prompt template with slots to populate with UML information and the target task, using both zero- and few-shot learning methods. By measuring the syntactic validity and execution accuracy metrics of the generated OCL constraints, we found that enriching the prompts with UML information and enabling few-shot learning increases the reliability of the generated OCL constraints. Furthermore, the results reveal a close similarity based on sentence embedding between the generated OCL constraints and the human-written ones in the ground truth, implying a level of clarity and understandability in the generated OCL constraints by Codex.","['Seif Abukhalaf', 'Mohammad Hamdaqa', 'Foutse Khomh']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.16244,Anomali
"Informed Machine Learning, Centrality, CNN, Relevant Document Detection, Repatriation of Indigenous Human Remains","Among the pressing issues facing Australian and other First Nations peoples is the repatriation of the bodily remains of their ancestors, which are currently held in Western scientific institutions. The success of securing the return of these remains to their communities for reburial depends largely on locating information within scientific and other literature published between 1790 and 1970 documenting their theft, donation, sale, or exchange between institutions. This article reports on collaborative research by data scientists and social science researchers in the Research, Reconcile, Renew Network (RRR) to develop and applytextminingtechniques to identify this vital information. We describe our work to date on developing a machine learning-based solution to automate the process of finding and semantically analysing relevanttexts. Classification models, particularly deep learning-based models, are known to have low accuracy when trained with small amounts of labelled (i.e. relevant/non-relevant) documents. To improve the accuracy of our detection model, we explore the use of an Informed Neural Network (INN) model that describes documentary content using expert-informed contextual knowledge. Only a few labelled documents are used to provide specificity to the model, using conceptually related keywords identified by RRR experts in provenance research. The results confirm the value of using an INN network model for identifying relevant documents related to the investigation of the global commercial trade in Indigenous human remains. Empirical analysis suggests that this INN model can be generalized for use by other researchers in the social sciences and humanities who want to extract relevant information from large textual corpora.","['Md Abul Bashar', 'Richi Nayak', 'Gareth Knapman', 'Paul Turnbull', 'Cressida Fforde']",Social Science Computer Review (2023),arXiv,2023,https://doi.org/10.48550/arXiv.2303.14475,Anomali
ChatGPT as the Transportation Equity Information Source for Scientific Writing,"Transportation equity is an interdisciplinary agenda that requires both transportation and social inputs. Traditionally, transportation equity information are sources from public libraries, conferences, televisions, social media, among other. Artificial intelligence (AI) tools including advanced language models such as ChatGPT are becoming favorite information sources. However, their credibility has not been well explored. This study explored the content and usefulness of ChatGPT-generated information related to transportation equity. It utilized 152 papers retrieved through the Web of Science (WoS) repository. The prompt was crafted for ChatGPT to provide an abstract given the title of the paper. The ChatGPT-based abstracts were then compared to human-written abstracts using statistical tools and unsupervisedtextmining. The results indicate that a weak similarity between ChatGPT and human-written abstracts. On average, the human-written abstracts and ChatGPT generated abstracts were about 58% similar, with a maximum and minimum of 97% and 1.4%, respectively. The keywords from the abstracts of papers with over the mean similarity score were more likely to be similar whereas those from below the average score were less likely to be similar. Themes with high similarity scores include access, public transit, and policy, among others. Further, clear differences in the key pattern of clusters for high and low similarity score abstracts was observed. Contrarily, the findings from collocated keywords were inconclusive. The study findings suggest that ChatGPT has the potential to be a source of transportation equity information. However, currently, a great amount of attention is needed before a user can utilize materials from ChatGPT","['Boniphace Kutela', 'Shoujia Li', 'Subasish Das', 'Jinli Liu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.11158,Anomali
NoisyHate: Mining Online Human-Written Perturbations for Realistic Robustness Benchmarking of Content Moderation Models,"Onlinetextswith toxic content are a clear threat to the users on social media in particular and society in general. Although many platforms have adopted various measures (e.g., machine learning-based hate-speech detection systems) to diminish their effect, toxic content writers have also attempted to evade such measures by using cleverly modified toxic words, so-called human-writtentextperturbations. Therefore, to help build automatic detection tools to recognize those perturbations, prior methods have developed sophisticated techniques to generate diverse adversarial samples. However, we note that these ``algorithms""-generated perturbations do not necessarily capture all the traits of ``human""-written perturbations. Therefore, in this paper, we introduce a novel, high-quality dataset of human-written perturbations, named as NoisyHate, that was created from real-life perturbations that are both written and verified by human-in-the-loop. We show that perturbations in NoisyHate have different characteristics than prior algorithm-generated toxic datasets show, and thus can be in particular useful to help develop better toxic speech detection solutions. We thoroughly validate NoisyHate against state-of-the-art language models, such as BERT and RoBERTa, and black box APIs, such as Perspective API, on two tasks, such as perturbation normalization and understanding.","['Yiran Ye', 'Thai Le', 'Dongwon Lee']",,arXiv,2025,https://doi.org/10.48550/arXiv.2303.10430,Anomali
"Cognitive Semantic Communication Systems Driven by Knowledge Graph: Principle, Implementation, and Performance Evaluation","Semantic communication is envisioned as a promising technique to break through the Shannon limit. However, semantic inference and semantic error correction have not been well studied. Moreover, error correction methods of existing semantic communication frameworks are inexplicable and inflexible, which limits the achievable performance. In this paper, to tackle this issue, a knowledge graph is exploited to develop semantic communication systems. Two cognitive semantic communication frameworks are proposed for the single-user and multiple-user communication scenarios. Moreover, a simple, general, and interpretable semantic alignment algorithm for semantic information detection is proposed. Furthermore, an effective semantic correction algorithm is proposed byminingthe inference rule from the knowledge graph. Additionally, the pre-trained model is fine-tuned to recover semantic information. For the multi-user cognitive semantic communication system, a message recovery algorithm is proposed to distinguish messages of different users by matching the knowledge level between the source and the destination. Extensive simulation results conducted on a public dataset demonstrate that our proposed single-user and multi-user cognitive semantic communication systems are superior to benchmark communication systems in terms of the data compression rate and communication reliability. Finally, we present realistic single-user and multi-user cognitive semantic communication systems results by building a software-defined radio prototype system.","['Fuhui Zhou', 'Yihao Li', 'Ming Xu', 'Lu Yuan', 'Qihui Wu', 'Rose Qingyang Hu', 'Naofal Al-Dhahir']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.08546,Anomali
Mining False Positive Examples for Text-Based Person Re-identification,"Text-based person re-identification (ReID) aims to identify images of the targeted person from a large-scale person image database according to a given textual description. However, due to significant inter-modal gaps,text-based person ReID remains a challenging problem. Most existing methods generally rely heavily on the similarity contributed by matched word-region pairs, while neglecting mismatched word-region pairs which may play a decisive role. Accordingly, we propose tominefalse positive examples (MFPE) via a jointly optimized multi-branch architecture to handle this problem. MFPE contains three branches including a false positivemining(FPM) branch to highlight the role of mismatched word-region pairs. Besides, MFPE delicately designs a cross-relu loss to increase the gap of similarity scores between matched and mismatched word-region pairs. Extensive experiments on CUHK-PEDES demonstrate the superior effectiveness of MFPE. Our code is released at https://github.com/xx-adeline/MFPE.","['Wenhao Xu', 'Zhiyin Shao', 'Changxing Ding']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.08466,Anomali
German BERT Model for Legal Named Entity Recognition,"The use of BERT, one of the most popular language models, has led to improvements in many Natural Language Processing (NLP) tasks. One such task is Named Entity Recognition (NER) i.e. automatic identification of named entities such as location, person, organization, etc. from a giventext. It is also an important base step for many NLP tasks such as information extraction and argumentationmining. Even though there is much research done on NER using BERT and other popular language models, the same is not explored in detail when it comes to Legal NLP or Legal Tech. Legal NLP applies various NLP techniques such as sentence similarity or NER specifically on legal data. There are only a handful of models for NER tasks using BERT language models, however, none of these are aimed at legal documents in German. In this paper, we fine-tune a popular BERT language model trained on German data (German BERT) on a Legal Entity Recognition (LER) dataset. To make sure our model is not overfitting, we performed a stratified 10-fold cross-validation. The results we achieve by fine-tuning German BERT on the LER dataset outperform the BiLSTM-CRF+ model used by the authors of the same LER dataset. Finally, we make the model openly available via HuggingFace.","['Harshil Darji', 'Jelena Mitrović', 'Michael Granitzer']",Proceedings of the 15th International Conference on Agents and Artificial Intelligence - Volume 3: ICAART (2023) 723-728,arXiv,2023,https://doi.org/10.48550/arXiv.2303.05388,Anomali
Tucker Bilinear Attention Network for Multi-scale Remote Sensing Object Detection,"Object detection on VHR remote sensing images plays a vital role in applications such as urban planning, land resource management, and rescue missions. The large-scale variation of the remote-sensing targets is one of the main challenges in VHR remote-sensing object detection. Existing methods improve the detection accuracy of high-resolution remote sensing objects by improving the structure of feature pyramids and adopting different attention modules. However, for small targets, there still be seriously missed detections due to the loss of key detail features. There is still room for improvement in the way of multiscale feature fusion and balance. To address this issue, this paper proposes two novel modules: Guided Attention and Tucker Bilinear Attention, which are applied to the stages of early fusion and late fusion respectively. The former can effectively retain clean key detail features, and the latter can better balance features through semantic-level correlationmining. Based on two modules, we build a new multi-scale remote sensing object detection framework. No bells and whistles. The proposed method largely improves the average precisions of small objects and achieves the highest mean average precisions compared with 9 state-of-the-art methods on DOTA, DIOR, and NWPU VHR-10.Code and models are available at https://github.com/Shinichict/GTNet.","['Tao Chen', 'Ruirui Li', 'Jiafeng Fu', 'Daguang Jiang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.05329,Anomali
Does Synthetic Data Generation of LLMs Help Clinical Text Mining?,"Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinicaltextminingby examining its ability to extract structured information from unstructured healthcaretexts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score from 23.37% to 63.99% for the named entity recognition task and from 75.86% to 83.59% for the relation extraction task. Furthermore, generating data using ChatGPT can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. In summary, the proposed framework presents a promising solution to enhance the applicability of LLM models to clinicaltextmining.","['Ruixiang Tang', 'Xiaotian Han', 'Xiaoqian Jiang', 'Xia Hu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.04360,Anomali
On the Visualisation of Argumentation Graphs to Support Text Interpretation,"The recent evolution in Natural Language Processing (NLP) methods, in particular in the field of argumentationmining, has the potential to transform the way we interact withtext, supporting the interpretation and analysis of complex discourse and debates. Can a graphic visualisation of complex argumentation enable a more critical interpretation of the arguments? This study focuses on analysing the impact of argumentation graphs (AGs) compared with regulartextsfor supporting argument interpretation. We found that AGs outperformed the extrinsic metrics throughout most UEQ scales as well as the NASA-TLX workload in all the terms but not in temporal or physical demand. The AG model was liked by a more significant number of participants, despite the fact that both thetext-based and AG models yielded comparable outcomes in the critical interpretation in terms of working memory and altering participants decisions. The interpretation process involves reference to argumentation schemes (linked to critical questions (CQs)) in AGs. Interestingly, we found that the participants chose more CQs (using argument schemes in AGs) when they were less familiar with the argument topics, making AG schemes on some scales (relatively) supportive of the interpretation process. Therefore, AGs were considered to deliver a more critical approach to argument interpretation, especially with unfamiliar topics. Based on the 25 participants conducted in this study, it appears that AG has demonstrated an overall positive effect on the argument interpretation process.","['Hanadi Mardah', 'Oskar Wysocki', 'Markel Vigo', 'Andre Freitas']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.03235,Anomali
Mining both Commonality and Specificity from Multiple Documents for Multi-Document Summarization,"The multi-document summarization task requires the designed summarizer to generate a shorttextthat covers the important information of original documents and satisfies content diversity. This paper proposes a multi-document summarization approach based on hierarchical clustering of documents. It utilizes the constructed class tree of documents to extract both the sentences reflecting the commonality of all documents and the sentences reflecting the specificity of some subclasses of these documents for generating a summary, so as to satisfy the coverage and diversity requirements of multi-document summarization. Comparative experiments with different variant approaches on DUC'2002-2004 datasets prove the effectiveness ofminingboth the commonality and specificity of documents for multi-document summarization. Experiments on DUC'2004 and Multi-News datasets show that our approach achieves competitive performance compared to the state-of-the-art unsupervised and supervised approaches.",['Bing Ma'],,arXiv,2023,https://doi.org/10.48550/arXiv.2303.02677,Anomali
Continual Causal Inference with Incremental Observational Data,"The era of big data has witnessed an increasing availability of observational data from mobile and social networking, online advertising, webmining, healthcare, education, public policy, marketing campaigns, and so on, which facilitates the development of causal effect estimation. Although significant advances have been made to overcome the challenges in the academic area, such as missing counterfactual outcomes and selection bias, they only focus on source-specific and stationary observational data, which is unrealistic in most industrial applications. In this paper, we investigate a new industrial problem of causal effect estimation from incrementally available observational data and present three new evaluation criteria accordingly, including extensibility, adaptability, and accessibility. We propose a Continual Causal Effect Representation Learning method for estimating causal effects with observational data, which are incrementally available from non-stationary data distributions. Instead of having access to all seen observational data, our method only stores a limited subset of feature representations learned from previous data. Combining selective and balanced representation learning, feature representation distillation, and feature transformation, our method achieves the continual causal effect estimation for new data without compromising the estimation capability for original data. Extensive experiments demonstrate the significance of continual causal effect estimation and the effectiveness of our method.","['Zhixuan Chu', 'Ruopeng Li', 'Stephen Rathbun', 'Sheng Li']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.01775,Anomali
NLP Workbench: Efficient and Extensible Integration of State-of-the-art Text Mining Tools,"NLP Workbench is a web-based platform fortextminingthat allows non-expert users to obtain semantic understanding of large-scale corpora using state-of-the-arttextminingmodels. The platform is built upon latest pre-trained models and open source systems from academia that provide semantic analysis functionalities, including but not limited to entity linking, sentiment analysis, semantic parsing, and relation extraction. Its extensible design enables researchers and developers to smoothly replace an existing model or integrate a new one. To improve efficiency, we employ a microservice architecture that facilitates allocation of acceleration hardware and parallelization of computation. This paper presents the architecture of NLP Workbench and discusses the challenges we faced in designing it. We also discuss diverse use cases of NLP Workbench and the benefits of using it over other approaches. The platform is under active development, with its source code released under the MIT license. A website and a short video demonstrating our platform are also available.","['Peiran Yao', 'Matej Kosmajac', 'Abeer Waheed', 'Kostyantyn Guzhva', 'Natalie Hervieux', 'Denilson Barbosa']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.01410,Anomali
Selectively Hard Negative Mining for Alleviating Gradient Vanishing in Image-Text Matching,"Recently, a series of Image-TextMatching (ITM) methods achieve impressive performance. However, we observe that most existing ITM models suffer from gradients vanishing at the beginning of training, which makes these models prone to falling into local minima. Most ITM models adopt triplet loss with Hard Negativemining(HN) as the optimization objective. We find that optimizing an ITM model using only the hard negative samples can easily lead to gradient vanishing. In this paper, we derive the condition under which the gradient vanishes during training. When the difference between the positive pair similarity and the negative pair similarity is close to 0, the gradients on both the image andtextencoders will approach 0. To alleviate the gradient vanishing problem, we propose a Selectively Hard NegativeMining(SelHN) strategy, which chooses whether tominehard negative samples according to the gradient vanishing condition. SelHN can be plug-and-play applied to existing ITM models to give them better training behavior. To further ensure the back-propagation of gradients, we construct a Residual Visual Semantic Embedding model with SelHN, denoted as RVSE++. Extensive experiments on two ITM benchmarks demonstrate the strength of RVSE++, achieving state-of-the-art performance.","['Zheng Li', 'Caili Guo', 'Xin Wang', 'Zerun Feng', 'Zhongtian Du']",,arXiv,2023,https://doi.org/10.48550/arXiv.2303.00181,Anomali
Discovering Top-k Structural Hole Spanners in Dynamic Networks,"Structural Hole (SH) theory states that the node which acts as a connecting link among otherwise disconnected communities gets positional advantages in the network. These nodes are called Structural Hole Spanners (SHS). Numerous solutions are proposed to discover SHSs; however, most of the solutions are only applicable to static networks. Since real-world networks are dynamic networks; consequently, in this study, we aim to discover SHSs in dynamic networks. Discovering SHSs is an NP-hard problem, due to which, instead of discovering exact k SHSs, we adopt a greedy approach to discover Top-k SHSs. We first propose an efficient Tracking-SHS algorithm for updating SHSs in dynamic networks. Our algorithm reuses the information obtained during the initial runs of the static algorithm and avoids the recomputations for the nodes unaffected by the updates. Besides, motivated from the success of Graph Neural Networks (GNNs) on various graphminingproblems, we also design a Graph Neural Network-based model, GNN-SHS, to discover SHSs in dynamic networks, aiming to reduce the computational cost while achieving high accuracy. We provide a theoretical analysis of the Tracking-SHS algorithm, and our theoretical results prove that for a particular type of graphs, such as Preferential Attachment graphs [1], Tracking-SHS algorithm achieves 1.6 times of speedup compared with the static algorithm. We perform extensive experiments, and our results demonstrate that the Tracking-SHS algorithm attains a minimum of 3.24 times speedup over the static algorithm. Also, the proposed second model GNN-SHS is on an average 671.6 times faster than the Tracking-SHS algorithm.","['Diksha Goel', 'Hong Shen', 'Hui Tian', 'Mingyu Guo']",,arXiv,2024,https://doi.org/10.48550/arXiv.2302.13292,Anomali
A Text Mining Analysis of Data Protection Politics: The Case of Plenary Sessions of the European Parliament,"Data protection laws and policies have been studied extensively in recent years, but little is known about the parliamentary politics of data protection. This imitation applies even to the European Union (EU) that has taken the global lead in data protection and privacy regulation. For patching this notable gap in existing research, this paper explores the data protection questions raised by the Members of the European Parliament (MEPs) in the Parliament's plenary sessions and the answers given to these by the European Commission. Over a thousand of such questions and answers are covered in a period from 1995 to early 2023. Given computational analysis based ontextmining, the results indicate that (a) data protection has been actively debated in the Parliament during the past twenty years. No noticeable longitudinal trends are present; the debates have been relatively constant. As could be expected, (b) the specific data protection laws in the EU have frequently been referenced in these debates, which (c) do not seem to align along conventional political dimensions such as the left-right axis. Furthermore, (d) numerous distinct data protection topics have been debated by the parliamentarians, indicating that data protection politics in the EU go well-beyond the recently enacted regulations.",['Jukka Ruohonen'],,arXiv,2023,https://doi.org/10.48550/arXiv.2302.09939,Anomali
Time-to-event modeling of subreddits transitions to r/SuicideWatch,"Recent dataminingresearch has focused on the analysis of social mediatext, content and networks to identify suicide ideation online. However, there has been limited research on the temporal dynamics of users and suicide ideation. In this work, we use time-to-event modeling to identify which subreddits have a higher association with users transitioning to posting on r/suicidewatch. For this purpose we use a Cox proportional hazards model that takes as inputtextand subreddit network features and outputs a probability distribution for the time until a Reddit user posts on r/suicidewatch. In our analysis we find a number of statistically significant features that predict earlier transitions to r/suicidewatch. While some patterns match existing intuition, for example r/depression is positively associated with posting sooner on r/suicidewatch, others were more surprising (for example, the average time between a high risk post on r/Wishlist and a post on r/suicidewatch is 10.2 days). We then discuss these results as well as directions for future research.","['Xueying Liu', 'Shiaofen Fang', 'George Mohler', 'Joan Carlson', 'Yunyu Xiao']",,arXiv,2023,https://doi.org/10.48550/arXiv.2302.06030,Anomali
Lightweight Transformers for Clinical Natural Language Processing,"Specialised pre-trained language models are becoming more frequent in NLP since they can potentially outperform models trained on generictexts. BioBERT and BioClinicalBERT are two examples of such models that have shown promise in medical NLP tasks. Many of these models are overparametrised and resource-intensive, but thanks to techniques like Knowledge Distillation (KD), it is possible to create smaller versions that perform almost as well as their larger counterparts. In this work, we specifically focus on development of compact language models for processing clinicaltexts(i.e. progress notes, discharge summaries etc). We developed a number of efficient lightweight clinical transformers using knowledge distillation and continual learning, with the number of parameters ranging from 15 million to 65 million. These models performed comparably to larger models such as BioBERT and ClinicalBioBERT and significantly outperformed other compact models trained on general or biomedical data. Our extensive evaluation was done across several standard datasets and covered a wide range of clinicaltext-miningtasks, including Natural Language Inference, Relation Extraction, Named Entity Recognition, and Sequence Classification. To our knowledge, this is the first comprehensive study specifically focused on creating efficient and compact transformers for clinical NLP tasks. The models and code used in this study can be found on our Huggingface profile at https://huggingface.co/nlpie and Github page at https://github.com/nlpie-research/Lightweight-Clinical-Transformers, respectively, promoting reproducibility of our results.","['Omid Rohanian', 'Mohammadmahdi Nouriborji', 'Hannah Jauncey', 'Samaneh Kouchaki', 'ISARIC Clinical Characterisation Group', 'Lei Clifton', 'Laura Merson', 'David A. Clifton']",Nat. Lang. Eng. 30 (2024) 887-914,arXiv,2023,https://doi.org/10.48550/arXiv.2302.04725,Anomali
Real-Word Error Correction with Trigrams: Correcting Multiple Errors in a Sentence,"Spelling correction is a fundamental task inTextMining. In this study, we assess the real-word error correction model proposed by Mays, Damerau and Mercer and describe several drawbacks of the model. We propose a new variation which focuses on detecting and correcting multiple real-word errors in a sentence, by manipulating a Probabilistic Context-Free Grammar (PCFG) to discriminate between items in the search space. We test our approach on the Wall Street Journal corpus and show that it outperforms Hirst and Budanitsky's WordNet-based method and Wilcox-O'Hearn, Hirst, and Budanitsky's fixed windows size method.-O'Hearn, Hirst, and Budanitsky's fixed windows size method.",['Seyed MohammadSadegh Dashti'],Language Resources and Evaluation. 2018 Jun;52:485-502,arXiv,2023,https://doi.org/10.48550/arXiv.2302.04096,Anomali
Estimation of Gaussian Bi-Clusters with General Block-Diagonal Covariance Matrix and Applications,"Bi-clustering is a technique that allows for the simultaneous clustering of observations and features in a dataset. This technique is often used in bioinformatics,textmining, and time series analysis. An important advantage of biclustering algorithm is the ability to uncover multiple ``views'' (i.e., through rows and column groupings) in the data. Several Gaussian mixture model based biclustering approach currently exist in the literature. However, they impose severe restrictions on the structure of the covariance matrix. Here, we propose a Gaussian mixture model-based bi-clustering approach that provides a more flexible block-diagonal covariance structure. We show that the clustering accuracy of the proposed model is comparable to other known techniques but our approach provides a more flexible covariance structure and has substantially lower computational time. We demonstrate the application of the proposed model in bioinformatics and topic modelling.","['Anastasiia Livochka', 'Ryan Browne', 'Sanjeena Subedi']",,arXiv,2023,https://doi.org/10.48550/arXiv.2302.03849,Anomali
PhysFormer++: Facial Video-based Physiological Measurement with SlowFast Temporal Difference Transformer,"Remote photoplethysmography (rPPG), which aims at measuring heart activities and physiological signals from facial video without any contact, has great potential in many applications (e.g., remote healthcare and affective computing). Recent deep learning approaches focus onminingsubtle rPPG clues using convolutional neural networks with limited spatio-temporal receptive fields, which neglect the long-range spatio-temporal perception and interaction for rPPG modeling. In this paper, we propose two end-to-end video transformer based architectures, namely PhysFormer and PhysFormer++, to adaptively aggregate both local and global spatio-temporal features for rPPG representation enhancement. As key modules in PhysFormer, the temporal difference transformers first enhance the quasi-periodic rPPG features with temporal difference guided global attention, and then refine the local spatio-temporal representation against interference. To better exploit the temporal contextual and periodic rPPG clues, we also extend the PhysFormer to the two-pathway SlowFast based PhysFormer++ with temporal difference periodic and cross-attention transformers. Furthermore, we propose the label distribution learning and a curriculum learning inspired dynamic constraint in frequency domain, which provide elaborate supervisions for PhysFormer and PhysFormer++ and alleviate overfitting. Comprehensive experiments are performed on four benchmark datasets to show our superior performance on both intra- and cross-dataset testings. Unlike most transformer networks needed pretraining from large-scale datasets, the proposed PhysFormer family can be easily trained from scratch on rPPG datasets, which makes it promising as a novel transformer baseline for the rPPG community.","['Zitong Yu', 'Yuming Shen', 'Jingang Shi', 'Hengshuang Zhao', 'Yawen Cui', 'Jiehua Zhang', 'Philip Torr', 'Guoying Zhao']",,arXiv,2023,https://doi.org/10.48550/arXiv.2302.03548,Anomali
Precursor recommendation for inorganic synthesis by machine learning materials similarity from scientific literature,"Synthesis prediction is a key accelerator for the rapid design of advanced materials. However, determining synthesis variables such as the choice of precursor materials is challenging for inorganic materials because the sequence of reactions during heating is not well understood. In this work, we use a knowledge base of 29,900 solid-state synthesis recipes,text-minedfrom the scientific literature, to automatically learn which precursors to recommend for the synthesis of a novel target material. The data-driven approach learns chemical similarity of materials and refers the synthesis of a new target to precedent synthesis procedures of similar materials, mimicking human synthesis design. When proposing five precursor sets for each of 2,654 unseen test target materials, the recommendation strategy achieves a success rate of at least 82%. Our approach captures decades of heuristic synthesis data in a mathematical form, making it accessible for use in recommendation engines and autonomous laboratories.","['Tanjin He', 'Haoyan Huo', 'Christopher J. Bartel', 'Zheren Wang', 'Kevin Cruse', 'Gerbrand Ceder']","Sci. Adv. 9, eadg8180 (2023)",arXiv,2023,https://doi.org/10.48550/arXiv.2302.02303,Anomali
Bioformer: an efficient transformer language model for biomedical text mining,"Pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art performance in natural language processing (NLP) tasks. Recently, BERT has been adapted to the biomedical domain. Despite the effectiveness, these models have hundreds of millions of parameters and are computationally expensive when applied to large-scale NLP applications. We hypothesized that the number of parameters of the original BERT can be dramatically reduced with minor impact on performance. In this study, we present Bioformer, a compact BERT model for biomedicaltextmining. We pretrained two Bioformer models (named Bioformer8L and Bioformer16L) which reduced the model size by 60% compared to BERTBase. Bioformer uses a biomedical vocabulary and was pre-trained from scratch on PubMed abstracts and PubMed Central full-textarticles. We thoroughly evaluated the performance of Bioformer as well as existing biomedical BERT models including BioBERT and PubMedBERT on 15 benchmark datasets of four different biomedical NLP tasks: named entity recognition, relation extraction, question answering and document classification. The results show that with 60% fewer parameters, Bioformer16L is only 0.1% less accurate than PubMedBERT while Bioformer8L is 0.9% less accurate than PubMedBERT. Both Bioformer16L and Bioformer8L outperformed BioBERTBase-v1.1. In addition, Bioformer16L and Bioformer8L are 2-3 fold as fast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been successfully deployed to PubTator Central providing gene annotations over 35 million PubMed abstracts and 5 million PubMed Central full-textarticles. We make Bioformer publicly available via https://github.com/WGLab/bioformer, including pre-trained models, datasets, and instructions for downstream use.","['Li Fang', 'Qingyu Chen', 'Chih-Hsuan Wei', 'Zhiyong Lu', 'Kai Wang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2302.01588,Anomali
Curriculum-guided Abstractive Summarization for Mental Health Online Posts,"Automatically generating short summaries from users' online mental health posts could save counselors' reading time and reduce their fatigue so that they can provide timely responses to those seeking help for improving their mental state. Recent Transformers-based summarization models have presented a promising approach to abstractive summarization. They go beyond sentence selection and extractive strategies to deal with more complicated tasks such as novel word generation and sentence paraphrasing. Nonetheless, these models have a prominent shortcoming; their training strategy is not quite efficient, which restricts the model's performance. In this paper, we include a curriculum learning approach to reweigh the training samples, bringing about an efficient learning procedure. We apply our model on extreme summarization dataset of MentSum posts -- a dataset of mental health related posts from Reddit social media. Compared to the state-of-the-art model, our proposed method makes substantial gains in terms of Rouge and Bertscore evaluation metrics, yielding 3.5% (Rouge-1), 10.4% (Rouge-2), and 4.7% (Rouge-L), 1.5% (Bertscore) relative improvements.","['Sajad Sotudeh', 'Nazli Goharian', 'Hanieh Deilamsalehy', 'Franck Dernoncourt']",,arXiv,2023,https://doi.org/10.48550/arXiv.2302.00954,Anomali
Improving the Inference of Topic Models via Infinite Latent State Replications,"Intextmining, topic models are a type of probabilistic generative models for inferring latent semantic topics fromtextcorpus. One of the most popular inference approaches to topic models is perhaps collapsed Gibbs sampling (CGS), which typically samples one single topic label for each observed document-word pair. In this paper, we aim at improving the inference of CGS for topic models. We propose to leverage state augmentation technique by maximizing the number of topic samples to infinity, and then develop a new inference approach, called infinite latent state replication (ILR), to generate robust soft topic assignment for each given document-word pair. Experimental results on the publicly available datasets show that ILR outperforms CGS for inference of existing established topic models.","['Daniel Rugeles', 'Zhen Hai', 'Juan Felipe Carmona', 'Manoranjan Dash', 'Gao Cong']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.12974,Anomali
"Down the Rabbit Hole: Detecting Online Extremism, Radicalisation, and Politicised Hate Speech","Social media is a modern person's digital voice to project and engage with new ideas and mobilise communities $\unicode{x2013}$ a power shared with extremists. Given the societal risks of unvetted content-moderating algorithms for Extremism, Radicalisation, and Hate speech (ERH) detection, responsible software engineering must understand the who, what, when, where, and why such models are necessary to protect user safety and free expression. Hence, we propose and examine the unique research field of ERH contextminingto unify disjoint studies. Specifically, we evaluate the start-to-finish design process from socio-technical definition-building and dataset collection strategies to technical algorithm design and performance. Our 2015-2021 51-study Systematic Literature Review (SLR) provides the first cross-examination of textual, network, and visual approaches to detecting extremist affiliation, hateful content, and radicalisation towards groups and movements. We identify consensus-driven ERH definitions and propose solutions to existing ideological and geographic biases, particularly due to the lack of research in Oceania/Australasia. Our hybridised investigation on Natural Language Processing, Community Detection, and visual-textmodels demonstrates the dominating performance of textual transformer-based algorithms. We conclude with vital recommendations for ERH contextminingresearchers and propose an uptake roadmap with guidelines for researchers, industries, and governments to enable a safer cyberspace.","['Jarod Govers', 'Philip Feldman', 'Aaron Dant', 'Panos Patros']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.11579,Anomali
Theme-driven Keyphrase Extraction to Analyze Social Media Discourse,"Social media platforms are vital resources for sharing self-reported health experiences, offering rich data on various health topics. Despite advancements in Natural Language Processing (NLP) enabling large-scale social media data analysis, a gap remains in applying keyphrase extraction to health-related content. Keyphrase extraction is used to identify salient concepts in social media discourse without being constrained by predefined entity classes. This paper introduces a theme-driven keyphrase extraction framework tailored for social media, a pioneering approach designed to capture clinically relevant keyphrases from user-generated healthtexts. Themes are defined as broad categories determined by the objectives of the extraction task. We formulate this novel task of theme-driven keyphrase extraction and demonstrate its potential for efficientlyminingsocial mediatextfor the use case of treatment for opioid use disorder. This paper leverages qualitative and quantitative analysis to demonstrate the feasibility of extracting actionable insights from social media data and efficiently extracting keyphrases using minimally supervised NLP models. Our contributions include the development of a novel data collection and curation framework for theme-driven keyphrase extraction and the creation of MOUD-Keyphrase, the first dataset of its kind comprising human-annotated keyphrases from a Reddit community. We also identify the scope of minimally supervised NLP models to extract keyphrases from social media data efficiently. Lastly, we found that a large language model (ChatGPT) outperforms unsupervised keyphrase extraction models, and we evaluate its efficacy in this task.","['William Romano', 'Omar Sharif', 'Madhusudan Basak', 'Joseph Gatto', 'Sarah Preum']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.11508,Anomali
Understanding Finetuning for Factual Knowledge Extraction from Language Models,"Language models (LMs) pretrained on large corpora oftextfrom the web have been observed to contain large amounts of various types of knowledge about the world. This observation has led to a new and exciting paradigm in knowledge graph construction where, instead of manual curation ortextmining, one extracts knowledge from the parameters of an LM. Recently, it has been shown that finetuning LMs on a set of factual knowledge makes them produce better answers to queries from a different set, thus making finetuned LMs a good candidate for knowledge extraction and, consequently, knowledge graph construction. In this paper, we analyze finetuned LMs for factual knowledge extraction. We show that along with its previously known positive effects, finetuning also leads to a (potentially harmful) phenomenon which we call Frequency Shock, where at the test time the model over-predicts rare entities that appear in the training set and under-predicts common entities that do not appear in the training set enough times. We show that Frequency Shock leads to a degradation in the predictions of the model and beyond a point, the harm from Frequency Shock can even outweigh the positive effects of finetuning, making finetuning harmful overall. We then consider two solutions to remedy the identified negative effect: 1- model mixing and 2- mixture finetuning with the LM's pre-training task. The two solutions combined lead to significant improvements compared to vanilla finetuning.","['Mehran Kazemi', 'Sid Mittal', 'Deepak Ramachandran']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.11293,Anomali
Paraphrase Acquisition from Image Captions,"We propose to use image captions from the Web as a previously underutilized resource for paraphrases (i.e.,textswith the same ""message"") and to create and analyze a corresponding dataset. When an image is reused on the Web, an original caption is often assigned. We hypothesize that different captions for the same image naturally form a set of mutual paraphrases. To demonstrate the suitability of this idea, we analyze captions in the English Wikipedia, where editors frequently relabel the same image for different articles. The paper introduces the underlyingminingtechnology, the resulting Wikipedia-IPC dataset, and compares known paraphrase corpora with respect to their syntactic and semantic paraphrase similarity to our new resource. In this context, we introduce characteristic maps along the two similarity dimensions to identify the style of paraphrases coming from different sources. An annotation study demonstrates the high reliability of the algorithmically determined characteristic maps.","['Marcel Gohsen', 'Matthias Hagen', 'Martin Potthast', 'Benno Stein']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.11030,Anomali
NLP as a Lens for Causal Analysis and Perception Mining to Infer Mental Health on Social Media,"Interactions among humans on social media often convey intentions behind their actions, yielding a psychological language resource for Mental Health Analysis (MHA) of online users. The success of Computational Intelligence Techniques (CIT) for inferring mental illness from such social media resources points to NLP as a lens for causal analysis and perceptionmining. However, we argue that more consequential and explainable research is required for optimal impact on clinical psychology practice and personalized mental healthcare. To bridge this gap, we posit two significant dimensions: (1) Causal analysis to illustrate a cause and effect relationship in the user generatedtext; (2) Perceptionminingto infer psychological perspectives of social effects on online users intentions. Within the scope of Natural Language Processing (NLP), we further explore critical areas of inquiry associated with these two dimensions, specifically through recent advancements in discourse analysis. This position paper guides the community to explore solutions in this space and advance the state of practice in developing conversational agents for inferring mental health from social media. We advocate for a more explainable approach toward modeling computational psychology problems through the lens of language as we observe an increased number of research contributions in dataset and problem formulation for causal relation extraction and perception enhancements while inferring mental states.","['Muskan Garg', 'Chandni Saxena', 'Usman Naseem', 'Bonnie J Dorr']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.11004,Anomali
Cross-lingual Argument Mining in the Medical Domain,"Nowadays the medical domain is receiving more and more attention in applications involving Artificial Intelligence as clinicians decision-making is increasingly dependent on dealing with enormous amounts of unstructured textual data. In this context, ArgumentMining(AM) helps to meaningfully structure textual data by identifying the argumentative components in thetextand classifying the relations between them. However, as it is the case for man tasks in Natural Language Processing in general and in medicaltextprocessing in particular, the large majority of the work on computational argumentation has been focusing only on the English language. In this paper, we investigate several strategies to perform AM in medicaltextsfor a language such as Spanish, for which no annotated data is available. Our work shows that automatically translating and projecting annotations (data-transfer) from English to a given target language is an effective way to generate annotated data without costly manual intervention. Furthermore, and contrary to conclusions from previous work for other sequence labelling tasks, our experiments demonstrate that data-transfer outperforms methods based on the crosslingual transfer capabilities of multilingual pre-trained language models (model-transfer). Finally, we show how the automatically generated data in Spanish can also be used to improve results in the original English monolingual setting, providing thus a fully automatic data augmentation strategy.","['Anar Yeginbergen', 'Rodrigo Agerri']","Procesamiento del Lenguaje Natural vol 73, 2024",arXiv,2024,https://doi.org/10.48550/arXiv.2301.10527,Anomali
Performance-Preserving Event Log Sampling for Predictive Monitoring,"Predictive process monitoring is a subfield of processminingthat aims to estimate case or event features for running process instances. Such predictions are of significant interest to the process stakeholders. However, most of the state-of-the-art methods for predictive monitoring require the training of complex machine learning models, which is often inefficient. Moreover, most of these methods require a hyper-parameter optimization that requires several repetitions of the training process which is not feasible in many real-life applications. In this paper, we propose an instance selection procedure that allows sampling training process instances for prediction models. We show that our instance selection procedure allows for a significant increase of training speed for next activity and remaining time prediction methods while maintaining reliable levels of prediction accuracy.","['Mohammadreza Fani Sani', 'Mozhgan Vazifehdoostirani', 'Gyunam Park', 'Marco Pegoraro', 'Sebastiaan J. van Zelst', 'Wil M. P. van der Aalst']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.07624,Anomali
Cross-institution text mining to uncover clinical associations: a case study relating social factors and code status in intensive care medicine,"Objective:Textminingof clinical notes embedded in electronic medical records is increasingly used to extract patient characteristics otherwise not or only partly available, to assess their association with relevant health outcomes. As manual data labeling needed to developtextminingmodels is resource intensive, we investigated whether off-the-shelftextminingmodels developed at external institutions, together with limited within-institution labeled data, could be used to reliably extract study variables to conduct association studies.
  Materials and Methods: We developed multipletextminingmodels on different combinations of within-institution and external-institution data to extract social factors from discharge reports of intensive care patients. Subsequently, we assessed the associations between social factors and having a do-not-resuscitate/intubate code. Results: Important differences were found between associations based on manually labeled data compared totext-minedsocial factors in three out of five cases. Adopting external-institutiontextminingmodels using manually labeled within-institution data resulted in models with higher F1-scores, but not in meaningfully different associations.
  Discussion: Whiletextminingfacilitated scaling analyses to larger samples leading to discovering a larger number of associations, the estimates may be unreliable. Confirmation is needed with bettertextminingmodels, ideally on a larger manually labeled dataset.
  Conclusion: The currently usedtextminingmodels were not sufficiently accurate to be used reliably in an association study. Model adaptation using within-institution data did not improve the estimates. Further research is needed to set conditions for reliable use oftextminingin medical research.","['Madhumita Sushil', 'Atul J. Butte', 'Ewoud Schuit', 'Maarten van Smeden', 'Artuur M. Leeuwenberg']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.06570,Anomali
A data science and machine learning approach to continuous analysis of Shakespeare's plays,"The availability of quantitativetextanalysis methods has provided new ways of analyzing literature in a manner that was not available in the pre-information era. Here we apply comprehensive machine learning analysis to the work of William Shakespeare. The analysis shows clear changes in the style of writing over time, with the most significant changes in the sentence length, frequency of adjectives and adverbs, and the sentiments expressed in thetext. Applying machine learning to make a stylometric prediction of the year of the play shows a Pearson correlation of 0.71 between the actual and predicted year, indicating that Shakespeare's writing style as reflected by the quantitative measurements changed over time. Additionally, it shows that the stylometrics of some of the plays is more similar to plays written either before or after the year they were written. For instance, Romeo and Juliet is dated 1596, but is more similar in stylometrics to plays written by Shakespeare after 1600. The source code for the analysis is available for free download.","['Charles Swisher', 'Lior Shamir']","Journal of Data Mining & Digital Humanities, 2023 (July 13, 2023) jdmdh:10829",arXiv,2023,https://doi.org/10.48550/arXiv.2301.06024,Anomali
Interpretable and Scalable Graphical Models for Complex Spatio-temporal Processes,"This thesis focuses on data that has complex spatio-temporal structure and on probabilistic graphical models that learn the structure in an interpretable and scalable manner. We target two research areas of interest: Gaussian graphical models for tensor-variate data and summarization of complex time-varyingtextsusing topic models. This work advances the state-of-the-art in several directions. First, it introduces a new class of tensor-variate Gaussian graphical models via the Sylvester tensor equation. Second, it develops an optimization technique based on a fast-converging proximal alternating linearized minimization method, which scales tensor-variate Gaussian graphical model estimations to modern big-data settings. Third, it connects Kronecker-structured (inverse) covariance models with spatio-temporal partial differential equations (PDEs) and introduces a new framework for ensemble Kalman filtering that is capable of tracking chaotic physical systems. Fourth, it proposes a modular and interpretable framework for unsupervised and weakly-supervised probabilistic topic modeling of time-varying data that combines generative statistical models with computational geometric methods. Throughout, practical applications of the methodology are considered using real datasets. This includes brain-connectivity analysis using EEG data, space weather forecasting using solar imaging data, longitudinal analysis of public opinions using Twitter data, andminingof mental health related issues using TalkLife data. We show in each case that the graphical modeling framework introduced here leads to improved interpretability, accuracy, and scalability.",['Yu Wang'],,arXiv,2023,this https URL,Anomali
Scalable Batch Acquisition for Deep Bayesian Active Learning,"In deep active learning, it is especially important to choose multiple examples to markup at each step to work efficiently, especially on large datasets. At the same time, existing solutions to this problem in the Bayesian setup, such as BatchBALD, have significant limitations in selecting a large number of examples, associated with the exponential complexity of computing mutual information for joint random variables. We, therefore, present the Large BatchBALD algorithm, which gives a well-grounded approximation to the BatchBALD method that aims to achieve comparable quality while being more computationally efficient. We provide a complexity analysis of the algorithm, showing a reduction in computation time, especially for large batches. Furthermore, we present an extensive set of experimental results on image andtextdata, both on toy datasets and larger ones such as CIFAR-100.","['Aleksandr Rubashevskii', 'Daria Kotova', 'Maxim Panov']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.05490,Anomali
MaNLP@SMM4H22: BERT for Classification of Twitter Posts,"The reported work is our straightforward approach for the shared task Classification of tweets self-reporting age organized by the Social MediaMiningfor Health Applications (SMM4H) workshop. This literature describes the approach that was used to build a binary classification system, that classifies the tweets related to birthday posts into two classes namely, exact age(positive class) and non-exact age(negative class). We made two submissions with variations in the preprocessing oftextwhich yielded F1 scores of 0.80 and 0.81 when evaluated by the organizers.","['Keshav Kapur', 'Rajitha Harikrishnan']",,arXiv,2022,https://doi.org/10.48550/arXiv.2301.05395,Anomali
Investigating Conversational Search Behavior For Domain Exploration,"Conversational search has evolved as a new information retrieval paradigm, marking a shift from traditional search systems towards interactive dialogues with intelligent search agents. This change especially affects exploratory information-seeking contexts, where conversational search systems can guide the discovery of unfamiliar domains. In these scenarios, users find it often difficult to express their information goals due to insufficient background knowledge. Conversational interfaces can provide assistance by eliciting information needs and narrowing down the search space. However, due to the complexity of information-seeking behavior, the design of conversational interfaces for retrieving information remains a great challenge. Although prior work has employed user studies to empirically ground the system design, most existing studies are limited to well-defined search tasks or known domains, thus being less exploratory in nature. Therefore, we conducted a laboratory study to investigate open-ended search behavior for navigation through unknown information landscapes. The study comprised of 26 participants who were restricted in their search to atextchat interface. Based on the collected dialogue transcripts, we applied statistical analyses and processminingtechniques to uncover general information-seeking patterns across five different domains. We not only identify core dialogue acts and their interrelations that enable users to discover domain knowledge, but also derive design suggestions for conversational search systems.","['Phillip Schneider', 'Anum Afzal', 'Juraj Vladika', 'Daniel Braun', 'Florian Matthes']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.04098,Anomali
UnifySpeech: A Unified Framework for Zero-shot Text-to-Speech and Voice Conversion,"Text-to-speech (TTS) and voice conversion (VC) are two different tasks both aiming at generating high quality speaking voice according to different input modality. Due to their similarity, this paper proposes UnifySpeech, which brings TTS and VC into a unified framework for the first time. The model is based on the assumption that speech can be decoupled into three independent components: content information, speaker information, prosody information. Both TTS and VC can be regarded asminingthese three parts of information from the input and completing the reconstruction of speech. For TTS, the speech content information is derived from thetext, while in VC it's derived from the source speech, so all the remaining units are shared except for the speech content extraction module in the two tasks. We applied vector quantization and domain constrain to bridge the gap between the content domains of TTS and VC. Objective and subjective evaluation shows that by combining the two task, TTS obtains better speaker modeling ability while VC gets hold of impressive speech content decoupling capability.","['Haogeng Liu', 'Tao Wang', 'Ruibo Fu', 'Jiangyan Yi', 'Zhengqi Wen', 'Jianhua Tao']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.03801,Anomali
Mining Healthcare Procurement Data Using Text Mining and Natural Language Processing -- Reflection From An Industrial Project,"Whiletextminingand NLP research has been established for decades, there remain gaps in the literature that reports the use of these techniques in building real-world applications. For example, they typically look at single and sometimes simplified tasks, and do not discuss in-depth data heterogeneity and inconsistency that is common in real-world problems or their implication on the development of their methods. Also, few prior work has focused on the healthcare domain. In this work, we describe an industry project that developedtextminingand NLP solutions tominemillions of heterogeneous, multilingual procurement documents in the healthcare sector. We extract structured procurement contract data that is used to power a platform for dynamically assessing supplier risks. Our work makes unique contributions in a number of ways. First, we deal with highly heterogeneous, multilingual data and we document our approach to tackle these challenges. This is mainly based on a method that effectively uses domain knowledge and generalises to multipletextminingand NLP tasks and languages. Second, applying this method tominemillions of procurement documents, we develop the first structured procurement contract database that will help facilitate the tendering process. Second, Finally, we discuss lessons learned for practicaltextmining/NLP development, and make recommendations for future research and practice.","['Ziqi Zhang', 'Tomas Jasaitis', 'Richard Freeman', 'Rowida Alfrjani', 'Adam Funk']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.03458,Anomali
Cursive Caption Text Detection in Videos,"Textual content appearing in videos represents an interesting index for semantic retrieval of videos (from archives), generation of alerts (live streams) as well as high level applications like opinionminingand content summarization. One of the key components of such systems is the detection of textual content in video frames and the same makes the subject of our present study. This paper presents a robust technique for detection of textual content appearing in video frames. More specifically we targettextin cursive script taking Urdutextas a case study. Detection of textual regions in video frames is carried out by fine-tuning object detectors based on deep convolutional neural networks for the specific case oftextdetection. Since it is common to have videos with captiontextin multiple-scripts, cursivetextis distinguished from Latintextusing a script-identification module. Finally, detection and script identification are combined in a single end-to-end trainable system. Experiments on a comprehensive dataset of around 11,000 video frames report an F-measure of 0.91.","['Ali Mirza', 'Imran Siddiqi']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.03164,Anomali
Testing High-dimensional Multinomials with Applications to Text Analysis,"Motivated by applications intextminingand discrete distribution inference, we investigate the testing for equality of probability mass functions of $K$ groups of high-dimensional multinomial distributions. A test statistic, which is shown to have an asymptotic standard normal distribution under the null, is proposed. The optimal detection boundary is established, and the proposed test is shown to achieve this optimal detection boundary across the entire parameter space of interest. The proposed method is demonstrated in simulation studies and applied to analyze two real-world datasets to examine variation among consumer reviews of Amazon movies and diversity of statistical paper abstracts.","['T. Tony Cai', 'Zheng Tracy Ke', 'Paxton Turner']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.01381,Anomali
Understanding the main failure scenarios of subsea blowout preventers systems: An approach through Latent Semantic Analysis,"The blowout preventer (BOP) system is one of the most important well safety barriers during the drilling phase because it can prevent the development of blowout events. This paper investigates BOP system's main failures using an LSA-based methodology. A total of 1312 failure records from companies worldwide were collected from the International Association of Drilling Contractors' RAPID-S53 database. The database contains recordings of halted drilling operations due to BOP system's failures and component's function deviations. The main failure scenarios of the components annular preventer, shear rams preventer, compensated chamber solenoid valve, and hydraulic regulators were identified using the proposed methodology. The scenarios contained valuable information about corrective maintenance procedures, such as frequently observed failure modes, detection methods used, suspected causes, and corrective actions. The findings highlighted that the major failures of the components under consideration were leakages caused by damaged elastomeric seals. The majority of the failures were detected during function and pressure tests with the BOP system in the rig. This study provides an alternative safety analysis that contributes to understanding blowout preventer system's critical component failures by applying a methodology based on a well-establishedtextminingtechnique and analyzing failure records from an international database.","['Gustavo Jorge Martins de Aguiar', 'Ramon Baptista Narcizo', 'Rodolfo Cardoso', 'Iara Tammela', 'Edwin Benito Mitacc Meza', 'Danilo Colombo', 'Luiz Antônio de Oliveira Chaves', 'Jamile Eleutério Delesposte']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.00844,Anomali
ClusTop: An unsupervised and integrated text clustering and topic extraction framework,"Textclustering and topic extraction are two important tasks intextmining. Usually, these two tasks are performed separately. For topic extraction to facilitate clustering, we can first projecttextsinto a topic space and then perform a clustering algorithm to obtain clusters. To promote topic extraction by clustering, we can first obtain clusters with a clustering algorithm and then extract cluster-specific topics. However, this naive strategy ignores the fact thattextclustering and topic extraction are strongly correlated and follow a chicken-and-egg relationship. Performing them separately fails to make them mutually benefit each other to achieve the best overall performance. In this paper, we propose an unsupervisedtextclustering and topic extraction framework (ClusTop) which integratestextclustering and topic extraction into a unified framework and can achieve high-quality clustering result and extract topics from each cluster simultaneously. Our framework includes four components: enhanced language model training, dimensionality reduction, clustering and topic extraction, where the enhanced language model can be viewed as a bridge between clustering and topic extraction. On one hand, it providestextembeddings with a strong cluster structure which facilitates effectivetextclustering; on the other hand, it pays high attention on the topic related words for topic extraction because of its self-attention architecture. Moreover, the training of enhanced language model is unsupervised. Experiments on two datasets demonstrate the effectiveness of our framework and provide benchmarks for different model combinations in this framework.","['Zhongtao Chen', 'Chenghu Mi', 'Siwei Duo', 'Jingfei He', 'Yatong Zhou']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.00818,Anomali
IRT2: Inductive Linking and Ranking in Knowledge Graphs of Varying Scale,"We address the challenge of building domain-specific knowledge models for industrial use cases, where labelled data and taxonomic information is initially scarce. Our focus is on inductive link prediction models as a basis for practical tools that support knowledge engineers with exploringtextcollections and discovering and linking new (so-called open-world) entities to the knowledge graph. We argue that - though neural approaches totextmininghave yielded impressive results in the past years - current benchmarks do not reflect the typical challenges encountered in the industrial wild properly. Therefore, our first contribution is an open benchmark coined IRT2 (inductive reasoning withtext) that (1) covers knowledge graphs of varying sizes (including very small ones), (2) comes with incidental, low-qualitytextmentions, and (3) includes not only triple completion but also ranking, which is relevant for supporting experts with discovery tasks.
  We investigate two neural models for inductive link prediction, one based on end-to-end learning and one that learns from the knowledge graph andtextdata in separate steps. These models compete with a strong bag-of-words baseline. The results show a significant advance in performance for the neural approaches as soon as the available graph data decreases for linking. For ranking, the results are promising, and the neural approaches outperform the sparse retriever by a wide margin.","['Felix Hamann', 'Adrian Ulges', 'Maurice Falk']",,arXiv,2023,https://doi.org/10.48550/arXiv.2301.00716,Anomali
Logic Mill -- A Knowledge Navigation System,"Logic Mill is a scalable and openly accessible software system that identifies semantically similar documents within either one domain-specific corpus or multi-domain corpora. It uses advanced Natural Language Processing (NLP) techniques to generate numerical representations of documents. Currently it leverages a large pre-trained language model to generate these document representations. The system focuses on scientific publications and patent documents and contains more than 200 million documents. It is easily accessible via a simple Application Programming Interface (API) or via a web interface. Moreover, it is continuously being updated and can be extended totextcorpora from other domains. We see this system as a general-purpose tool for future research applications in the social sciences and other domains.","['Sebastian Erhardt', 'Mainak Ghosh', 'Erik Buunk', 'Michael E. Rose', 'Dietmar Harhoff']","Proceedings of the 5th Workshop on Patent Text Mining and Semantic Technologies (PatentSemTech 2024), Washington D.C., USA, July 28th, 2024",arXiv,2023,https://doi.org/10.48550/arXiv.2301.00200,Anomali
NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis,"With the development of natural language processing techniques(NLP), automatic diagnosis of eye diseases using ophthalmology electronic medical records (OEMR) has become possible. It aims to evaluate the condition of both eyes of a patient respectively, and we formulate it as a particular multi-label classification task in this paper. Although there are a few related studies in other diseases, automatic diagnosis of eye diseases exhibits unique characteristics. First, descriptions of both eyes are mixed up in OEMR documents, with both freetextand templated asymptomatic descriptions, resulting in sparsity and clutter of information. Second, OEMR documents contain multiple parts of descriptions and have long document lengths. Third, it is critical to provide explainability to the disease diagnosis model. To overcome those challenges, we present an effective automatic eye disease diagnosis framework, NEEDED. In this framework, a preprocessing module is integrated to improve the density and quality of information. Then, we design a hierarchical transformer structure for learning the contextualized representations of each sentence in the OEMR document. For the diagnosis part, we propose an attention-based predictor that enables traceable diagnosis by obtaining disease-specific information. Experiments on the real dataset and comparison with several baseline models show the advantage and explainability of our framework.","['Xu Ye', 'Meng Xiao', 'Zhiyuan Ning', 'Weiwei Dai', 'Wenjuan Cui', 'Yi Du', 'Yuanchun Zhou']",,arXiv,2023,https://doi.org/10.48550/arXiv.2212.13408,Anomali
"Explainable AI for Bioinformatics: Methods, Tools, and Applications","Artificial intelligence (AI) systems utilizing deep neural networks (DNNs) and machine learning (ML) algorithms are widely used for solving important problems in bioinformatics, biomedical informatics, and precision medicine. However, complex DNNs or ML models, which are often perceived as opaque and black-box, can make it difficult to understand the reasoning behind their decisions. This lack of transparency can be a challenge for both end-users and decision-makers, as well as AI developers. Additionally, in sensitive areas like healthcare, explainability and accountability are not only desirable but also legally required for AI systems that can have a significant impact on human lives. Fairness is another growing concern, as algorithmic decisions should not show bias or discrimination towards certain groups or individuals based on sensitive attributes. Explainable artificial intelligence (XAI) aims to overcome the opaqueness of black-box models and provide transparency in how AI systems make decisions. Interpretable ML models can explain how they make predictions and the factors that influence their outcomes. However, most state-of-the-art interpretable ML methods are domain-agnostic and evolved from fields like computer vision, automated reasoning, or statistics, making direct application to bioinformatics problems challenging without customization and domain-specific adaptation. In this paper, we discuss the importance of explainability in the context of bioinformatics, provide an overview of model-specific and model-agnostic interpretable ML methods and tools, and outline their potential caveats and drawbacks. Besides, we discuss how to customize existing interpretable ML methods for bioinformatics problems. Nevertheless, we demonstrate how XAI methods can improve transparency through case studies in bioimaging, cancer genomics, andtextmining.","['Md. Rezaul Karim', 'Tanhim Islam', 'Oya Beyan', 'Christoph Lange', 'Michael Cochez', 'Dietrich Rebholz-Schuhmann', 'Stefan Decker']",,arXiv,2023,https://doi.org/10.48550/arXiv.2212.13261,Anomali
Word Embedding Neural Networks to Advance Knee Osteoarthritis Research,"Osteoarthritis (OA) is the most prevalent chronic joint disease worldwide, where knee OA takes more than 80% of commonly affected joints. Knee OA is not a curable disease yet, and it affects large columns of patients, making it costly to patients and healthcare systems. Etiology, diagnosis, and treatment of knee OA might be argued by variability in its clinical and physical manifestations. Although knee OA carries a list of well-known terminology aiming to standardize the nomenclature of the diagnosis, prognosis, treatment, and clinical outcomes of the chronic joint disease, in practice there is a wide range of terminology associated with knee OA across different data sources, including but not limited to biomedical literature, clinical notes, healthcare literacy, and health-related social media. Among these data sources, the scientific articles published in the biomedical literature usually make a principled pipeline to study disease. Rapid yet, accuratetextminingon large-scale scientific literature may discover novel knowledge and terminology to better understand knee OA and to improve the quality of knee OA diagnosis, prevention, and treatment. The present works aim to utilize artificial neural network strategies to automatically extract vocabularies associated with knee OA diseases. Our finding indicates the feasibility of developing word embedding neural networks for autonomous keyword extraction and abstraction of knee OA.","['Soheyla Amirian', 'Husam Ghazaleh', 'Mehdi Assefi', 'Hilal Maradit Kremers', 'Hamid R. Arabnia', 'Johannes F. Plate', 'Ahmad P. Tafti']",,arXiv,2022,https://doi.org/10.48550/arXiv.2212.11933,Anomali
The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique,"The arrival of handwriting recognition technologies offers new possibilities for research in heritage studies. However, it is now necessary to reflect on the experiences and the practices developed by research teams. Our use of the Transkribus platform since 2018 has led us to search for the most significant ways to improve the performance of our handwrittentextrecognition (HTR) models which are made to transcribe French handwriting dating from the 17th century. This article therefore reports on the impacts of creating transcribing protocols, using the language model at full scale and determining the best way to use base models in order to help increase the performance of HTR models. Combining all of these elements can indeed increase the performance of a single model by more than 20% (reaching a Character Error Rate below 5%). This article also discusses some challenges regarding the collaborative nature of HTR platforms such as Transkribus and the way researchers can share their data generated in the process of creating or training handwrittentextrecognition models.","['Beatrice Couture', 'Farah Verret', 'Maxime Gohier', 'Dominique Deslandres']","Journal of Data Mining & Digital Humanities, Historical Documents and automatic text recognition (December 6, 2023) jdmdh:10542",arXiv,2023,https://doi.org/10.48550/arXiv.2212.11146,Anomali
AgAsk: An Agent to Help Answer Farmer's Questions From Scientific Documents,"Decisions in agriculture are increasingly data-driven; however, valuable agricultural knowledge is often locked away in free-textreports, manuals and journal articles. Specialised search systems are needed that canmineagricultural information to provide relevant answers to users' questions. This paper presents AgAsk -- an agent able to answer natural language agriculture questions byminingscientific documents.
  We carefully survey and analyse farmers' information needs. On the basis of these needs we release an information retrieval test collection comprising real questions, a large collection of scientific documents split in passages, and ground truth relevance assessments indicating which passages are relevant to each question.
  We implement and evaluate a number of information retrieval models to answer farmers questions, including two state-of-the-art neural ranking models. We show that neural rankers are highly effective at matching passages to questions in this context.
  Finally, we propose a deployment architecture for AgAsk that includes a client based on the Telegram messaging platform and retrieval model deployed on commodity hardware.
  The test collection we provide is intended to stimulate more research in methods to match natural language to answers in scientific documents. While the retrieval models were evaluated in the agriculture domain, they are generalisable and of interest to others working on similar problems.
  The test collection is available at: \url{https://github.com/ielab/agvaluate}.","['Bevan Koopman', 'Ahmed Mourad', 'Hang Li', 'Anton van der Vegt', 'Shengyao Zhuang', 'Simon Gibson', 'Yash Dang', 'David Lawrence', 'Guido Zuccon']",,arXiv,2022,https://doi.org/10.48550/arXiv.2212.10762,Anomali
Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval,"Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingualtextembeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in $N$ languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingualtextembeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitextmining, and cross-lingual question retrieval -- the last of which we introduce in this paper. Overall, our Variational Multilingual Source-Separation Transformer (VMSST) model outperforms both a strong contrastive and generative baseline on these tasks.","['John Wieting', 'Jonathan H. Clark', 'William W. Cohen', 'Graham Neubig', 'Taylor Berg-Kirkpatrick']",,arXiv,2023,https://doi.org/10.48550/arXiv.2212.10726,Anomali
Graph-based Semantical Extractive Text Analysis,"In the past few decades, there has been an explosion in the amount of available data produced from various sources with different topics. The availability of this enormous data necessitates us to adopt effective computational tools to explore the data. This leads to an intense growing interest in the research community to develop computational methods focused on processing thistextdata. A line of study focused on condensing thetextso that we are able to get a higher level of understanding in a shorter time. The two important tasks to do this are keyword extraction andtextsummarization. In keyword extraction, we are interested in finding the key important words from atext. This makes us familiar with the general topic of atext. Intextsummarization, we are interested in producing a short-lengthtextwhich includes important information about the document. The TextRank algorithm, an unsupervised learning method that is an extension of the PageRank (algorithm which is the base algorithm of Google search engine for searching pages and ranking them) has shown its efficacy in large-scaletextmining, especially fortextsummarization and keyword extraction. this algorithm can automatically extract the important parts of atext(keywords or sentences) and declare them as the result. However, this algorithm neglects the semantic similarity between the different parts. In this work, we improved the results of the TextRank algorithm by incorporating the semantic similarity between parts of thetext. Aside from keyword extraction andtextsummarization, we develop a topic clustering algorithm based on our framework which can be used individually or as a part of generating the summary to overcome coverage problems.",['Mina Samizadeh'],,arXiv,2022,https://doi.org/10.48550/arXiv.2212.09701,Anomali
SrTR: Self-reasoning Transformer with Visual-linguistic Knowledge for Scene Graph Generation,"Objects in a scene are not always related. The execution efficiency of the one-stage scene graph generation approaches are quite high, which infer the effective relation between entity pairs using sparse proposal sets and a few queries. However, they only focus on the relation between subject and object in triplet set subject entity, predicate entity, object entity, ignoring the relation between subject and predicate or predicate and object, and the model lacks self-reasoning ability. In addition, linguistic modality has been neglected in the one-stage method. It is necessary tominelinguistic modality knowledge to improve model reasoning ability. To address the above-mentioned shortcomings, a Self-reasoning Transformer with Visual-linguistic Knowledge (SrTR) is proposed to add flexible self-reasoning ability to the model. An encoder-decoder architecture is adopted in SrTR, and a self-reasoning decoder is developed to complete three inferences of the triplet set, s+o-p, s+p-o and p+o-s. Inspired by the large-scale pre-training image-textfoundation models, visual-linguistic prior knowledge is introduced and a visual-linguistic alignment strategy is designed to project visual representations into semantic spaces with prior knowledge to aid relational reasoning. Experiments on the Visual Genome dataset demonstrate the superiority and fast inference ability of the proposed method.","['Yuxiang Zhang', 'Zhenbo Liu', 'Shuai Wang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2212.09329,Anomali
Very Large Language Model as a Unified Methodology of Text Mining,"Textdataminingis the process of deriving essential information from languagetext. Typicaltextminingtasks includetextcategorization,textclustering, topic modeling, information extraction, andtextsummarization. Various data sets are collected and various algorithms are designed for the different types of tasks. In this paper, I present a blue sky idea that very large language model (VLLM) will become an effective unified methodology oftextmining. I discuss at least three advantages of this new methodology against conventional methods. Finally I discuss the challenges in the design and development of VLLM techniques fortextmining.",['Meng Jiang'],,arXiv,2022,https://doi.org/10.48550/arXiv.2212.09271,Anomali
Text2Struct: A Machine Learning Pipeline for Mining Structured Data from Text,"Many analysis and prediction tasks require the extraction of structured data from unstructuredtexts. However, an annotation scheme and a training dataset have not been available for training machine learning models tominestructured data fromtextwithout special templates and patterns. To solve it, this paper presents an end-to-end machine learning pipeline, Text2Struct, including atextannotation scheme, training data processing, and machine learning implementation. We formulated theminingproblem as the extraction of metrics and units associated with numerals in thetext. Text2Struct was trained and evaluated using an annotatedtextdataset collected from abstracts of medical publications regarding thrombectomy. In terms of prediction performance, a dice coefficient of 0.82 was achieved on the test dataset. By random sampling, most predicted relations between numerals and entities were well matched to the ground-truth annotations. These results show that Text2Struct is viable for theminingof structured data fromtextwithout special templates or patterns. It is anticipated to further improve the pipeline by expanding the dataset and investigating other machine learning models. A code demonstration can be found at: https://github.com/zcc861007/CourseProject","['Chaochao Zhou', 'Bo Yang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2212.09044,Anomali
NLIP: Noise-robust Language-Image Pre-training,"Large-scale cross-modal pre-training paradigms have recently shown ubiquitous success on a wide range of downstream tasks, e.g., zero-shot classification, retrieval and image captioning. However, their successes highly rely on the scale and quality of web-crawled data that naturally contain incomplete and noisy information (e.g., wrong or irrelevant content). Existing works either design manual rules to clean data or generate pseudo-targets as auxiliary signals for reducing noise impact, which do not explicitly tackle both the incorrect and incomplete challenges simultaneously. In this paper, to automatically mitigate the impact of noise by solelyminingover existing data, we propose a principled Noise-robust Language-Image Pre-training framework (NLIP) to stabilize pre-training via two schemes: noise-harmonization and noise-completion. First, in noise-harmonization scheme, NLIP estimates the noise probability of each pair according to the memorization effect of cross-modal transformers, then adopts noise-adaptive regularization to harmonize the cross-modal alignments with varying degrees. Second, in noise-completion scheme, to enrich the missing object information oftext, NLIP injects a concept-conditioned cross-modal decoder to obtain semantic-consistent synthetic captions to complete noisy ones, which uses the retrieved visual concepts (i.e., objects' names) for the corresponding image to guide captioning generation. By collaboratively optimizing noise-harmonization and noise-completion schemes, our NLIP can alleviate the common noise effects during image-textpre-training in a more efficient way. Extensive experiments show the significant performance improvements of our NLIP using only 26M data over existing pre-trained models (e.g., CLIP, FILIP and BLIP) on 12 zero-shot classification datasets, MSCOCO image captioning and zero-shot image-textretrieval tasks.","['Runhui Huang', 'Yanxin Long', 'Jianhua Han', 'Hang Xu', 'Xiwen Liang', 'Chunjing Xu', 'Xiaodan Liang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2212.07086,Anomali
Earthquake Impact Analysis Based on Text Mining and Social Media Analytics,"Earthquakes have a deep impact on wide areas, and emergency rescue operations may benefit from social media information about the scope and extent of the disaster. Therefore, this work presents atextminingbased approach to collect and analyze social media data for early earthquake impact analysis. First, disasterrelated microblogs are collected from the Sina microblog based on crawler technology. Then, after data cleaning a series of analyses are conducted including (1) the hot words analysis, (2) the trend of the number of microblogs, (3) the trend of public opinion sentiment, and (4) a keyword and rule-basedtextclassification for earthquake impact analysis. Finally, two recent earthquakes with the same magnitude and focal depth in China are analyzed to compare their impacts. The results show that the public opinion trend analysis and the trend of public opinion sentiment can estimate the earthquake's social impact at an early stage, which will be helpful to decision-making and rescue management.","['Zhe Zheng', 'Hong-Zheng Shi', 'Yu-Cheng Zhou', 'Xin-Zheng Lu', 'Jia-Rui Lin']",,arXiv,2022,https://doi.org/10.48550/arXiv.2212.06765,Anomali
Evaluating Airline Service Quality Through the Comprehensive Text-mining and TOPSIS-VIKOR-AISM Analysis,"Service quality rankings are pivotal for maintaining sustainability in the fiercely competitive airline industry. However, prior research in this domain has often fallen short in aspects of sample size, efficiency, and dependability. This study introduces refined insights into this area and establishes a comprehensive, yet highly elucidative, ranking framework. Initially, we employ Latent Semantic Analysis (LSA) to distill principal themes and sentiments from online reviews of 80 airlines. Subsequently, we utilize the SentiWordNet lexicon and the TextBlob package for conducting sentiment analysis based on these reviews. Following this, we construct a hierarchical structure using the computation of compromise solutions, employing an integrated Technique for Order Preference by Similarity to Ideal Solution, vis-à-vis Kriterijumska Optimizacija I Kompromisno Resenje-Adversarial Interpretive Structural Model (TOPSIS-VIKOR-AISM) methodology. Beyond aiding consumer decision-making and fostering airline growth, this study contributes novel viewpoints on evaluating the efficacy of airlines and other sectors.","['Haotian Xie', 'Yi Li', 'Yang Pu', 'Chen Zhang', 'Junlin Huang']",Journal of Air Transport Management 120 (2024) 102655,arXiv,2024,https://doi.org/10.48550/arXiv.2212.06332,Anomali
Effective Seed-Guided Topic Discovery by Integrating Multiple Types of Contexts,"Instead ofminingcoherent topics from a giventextcorpus in a completely unsupervised manner, seed-guided topic discovery methods leverage user-provided seed words to extract distinctive and coherent topics so that theminedtopics can better cater to the user's interest. To model the semantic correlation between words and seeds for discovering topic-indicative terms, existing seed-guided approaches utilize different types of context signals, such as document-level word co-occurrences, sliding window-based local contexts, and generic linguistic knowledge brought by pre-trained language models. In this work, we analyze and show empirically that each type of context information has its value and limitation in modeling word semantics under seed guidance, but combining three types of contexts (i.e., word embeddings learned from local contexts, pre-trained language model representations obtained from general-domain training, and topic-indicative sentences retrieved based on seed information) allows them to complement each other for discovering quality topics. We propose an iterative framework, SeedTopicMine, which jointly learns from the three types of contexts and gradually fuses their context signals via an ensemble ranking process. Under various sets of seeds and on multiple datasets, SeedTopicMine consistently yields more coherent and accurate topics than existing seed-guided topic discovery approaches.","['Yu Zhang', 'Yunyi Zhang', 'Martin Michalski', 'Yucheng Jiang', 'Yu Meng', 'Jiawei Han']",,arXiv,2023,https://doi.org/10.48550/arXiv.2212.06002,Anomali
Text Mining-Based Patent Analysis for Automated Rule Checking in AEC,"Automated rule checking (ARC), which is expected to promote the efficiency of the compliance checking process in the architecture, engineering, and construction (AEC) industry, is gaining increasing attention. Throwing light on the ARC application hotspots and forecasting its trends are useful to the related research and drive innovations. Therefore, this study takes the patents from the database of the Derwent Innovations Index database (DII) and China national knowledge infrastructure (CNKI) as data sources and then carried out a three-step analysis including (1) quantitative characteristics (i.e., annual distribution analysis) of patents, (2) identification of ARC topics using a latent Dirichlet allocation (LDA) and, (3) SNA-based co-occurrence analysis of ARC topics. The results show that the research hotspots and trends of Chinese and English patents are different. The contributions of this study have three aspects: (1) an approach to a comprehensive analysis of patents by integrating multipletextminingmethods (i.e., SNA and LDA) is introduced ; (2) the application hotspots and development trends of ARC are reviewed based on patent analysis; and (3) a signpost for technological development and innovation of ARC is provided.","['Zhe Zheng', 'Bo-Rui Kang', 'Qi-Tian Yuan', 'Yu-Cheng Zhou', 'Xin-Zheng Lu', 'Jia-Rui Lin']",,arXiv,2022,https://doi.org/10.48550/arXiv.2212.05891,Anomali
Robust convex biclustering with a tuning-free method,"Biclustering is widely used in different kinds of fields including gene information analysis,textmining, and recommendation system by effectively discovering the local correlation between samples and features. However, many biclustering algorithms will collapse when facing heavy-tailed data. In this paper, we propose a robust version of convex biclustering algorithm with Huber loss. Yet, the newly introduced robustification parameter brings an extra burden to selecting the optimal parameters. Therefore, we propose a tuning-free method for automatically selecting the optimal robustification parameter with high efficiency. The simulation study demonstrates the more fabulous performance of our proposed method than traditional biclustering methods when encountering heavy-tailed noise. A real-life biomedical application is also presented. The R package RcvxBiclustr is available at https://github.com/YifanChen3/RcvxBiclustr.","['Yifan Chen', 'Chunyin Lei', 'Chuanquan Li', 'Haiqiang Ma', 'Ningyuan Hu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2212.03122,Anomali
Interdisciplinary Discovery of Nanomaterials Based on Convolutional Neural Networks,"The material science literature contains up-to-date and comprehensive scientific knowledge of materials. However, their content is unstructured and diverse, resulting in a significant gap in providing sufficient information for material design and synthesis. To this end, we used natural language processing (NLP) and computer vision (CV) techniques based on convolutional neural networks (CNN) to discover valuable experimental-based information about nanomaterials and synthesis methods in energy-material-related publications. Our first system, TextMaster, extracts opinions fromtextsand classifies them into challenges and opportunities, achieving 94% and 92% accuracy, respectively. Our second system, GraphMaster, realizes data extraction of tables and figures from publications with 98.3\% classification accuracy and 4.3% data extraction mean square error. Our results show that these systems could assess the suitability of materials for a certain application by evaluation of synthesis insights and case analysis with detailed references. This work offers a fresh perspective onminingknowledge from scientific literature, providing a wide swatch to accelerate nanomaterial research through CNN.","['Tong Xie', 'Yuwei Wan', 'Weijian Li', 'Qingyuan Linghu', 'Shaozhou Wang', 'Yalun Cai', 'Han Liu', 'Chunyu Kit', 'Clara Grazian', 'Bram Hoex']",,arXiv,2022,https://doi.org/10.48550/arXiv.2212.02805,Anomali
Location-Aware Self-Supervised Transformers for Semantic Segmentation,"Pixel-level labels are particularly expensive to acquire. Hence, pretraining is a critical step to improve models on a task like semantic segmentation. However, prominent algorithms for pretraining neural networks use image-level objectives, e.g. image classification, image-textalignment a la CLIP, or self-supervised contrastive learning. These objectives do not model spatial information, which might be sub-optimal when finetuning on downstream tasks with spatial reasoning. In this work, we pretrain network with a location-aware (LOCA) self-supervised method which fosters the emergence of strong dense features. Specifically, we use both a patch-level clustering scheme tominedense pseudo-labels and a relative location prediction task to encourage learning about object parts and their spatial arrangements. Our experiments show that LOCA pretraining leads to representations that transfer competitively to challenging and diverse semantic segmentation datasets.","['Mathilde Caron', 'Neil Houlsby', 'Cordelia Schmid']",,arXiv,2023,https://doi.org/10.48550/arXiv.2212.02400,Anomali
Box2Mask: Box-supervised Instance Segmentation via Level-set Evolution,"In contrast to fully supervised methods using pixel-wise mask labels, box-supervised instance segmentation takes advantage of simple box annotations, which has recently attracted increasing research attention. This paper presents a novel single-shot instance segmentation approach, namely Box2Mask, which integrates the classical level-set evolution model into deep neural network learning to achieve accurate mask prediction with only bounding box supervision. Specifically, both the input image and its deep features are employed to evolve the level-set curves implicitly, and a local consistency module based on a pixel affinity kernel is used tominethe local context and spatial relations. Two types of single-stage frameworks, i.e., CNN-based and transformer-based frameworks, are developed to empower the level-set evolution for box-supervised instance segmentation, and each framework consists of three essential components: instance-aware decoder, box-level matching assignment and level-set evolution. By minimizing the level-set energy function, the mask map of each instance can be iteratively optimized within its bounding box annotation. The experimental results on five challenging testbeds, covering general scenes, remote sensing, medical and scenetextimages, demonstrate the outstanding performance of our proposed Box2Mask approach for box-supervised instance segmentation. In particular, with the Swin-Transformer large backbone, our Box2Mask obtains 42.4% mask AP on COCO, which is on par with the recently developed fully mask-supervised methods. The code is available at: https://github.com/LiWentomng/boxlevelset.","['Wentong Li', 'Wenyu Liu', 'Jianke Zhu', 'Miaomiao Cui', 'Risheng Yu', 'Xiansheng Hua', 'Lei Zhang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2212.01579,Anomali
Resolving Uncertain Case Identifiers in Interaction Logs: A User Study,"Modern software systems are able to record vast amounts of user actions, stored for later analysis. One of the main types of such user interaction data is click data: the digital trace of the actions of a user through the graphical elements of an application, website or software. While readily available, click data is often missing a case notion: an attribute linking events from user interactions to a specific process instance in the software. In this paper, we propose a neural network-based technique to determine a case notion for click data, thus enabling processminingand other process analysis techniques on user interaction data. We describe our method, show its scalability to datasets of large dimensions, and we validate its efficacy through a user study based on the segmented event log resulting from interaction data of a mobility sharing company. Interviews with domain experts in the company demonstrate that the case notion obtained by our method can lead to actionable process insights.","['Marco Pegoraro', 'Merih Seran Uysal', 'Tom-Hendrik Hülsmann', 'Wil M. P. van der Aalst']",,arXiv,2022,https://doi.org/10.48550/arXiv.2212.00009,Anomali
AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning,"Biomedical named entity recognition (BioNER) seeks to automatically recognize biomedical entities in natural languagetext, serving as a necessary foundation for downstreamtextminingtasks and applications such as information extraction and question answering. Manually labeling training data for the BioNER task is costly, however, due to the significant domain expertise required for accurate annotation. The resulting data scarcity causes current BioNER approaches to be prone to overfitting, to suffer from limited generalizability, and to address a single entity type at a time (e.g., gene or disease). We therefore propose a novel all-in-one (AIO) scheme that uses external data from existing annotated resources to enhance the accuracy and stability of BioNER models. We further present AIONER, a general-purpose BioNER tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and compares favorably to other state-of-the-art approaches such as multi-task learning. We further demonstrate the practical utility of AIONER in three independent tasks to recognize entity types not previously seen in training data, as well as the advantages of AIONER over existing methods for processing biomedicaltextat a large scale (e.g., the entire PubMed data).","['Ling Luo', 'Chih-Hsuan Wei', 'Po-Ting Lai', 'Robert Leaman', 'Qingyu Chen', 'Zhiyong Lu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2211.16944,Anomali
Gapped String Indexing in Subquadratic Space and Sublinear Query Time,"In Gapped String Indexing, the goal is to compactly represent a string $S$ of length $n$ such that for any query consisting of two strings $P_1$ and $P_2$, called patterns, and an integer interval $[α, β]$, called gap range, we can quickly find occurrences of $P_1$ and $P_2$ in $S$ with distance in $[α, β]$. Gapped String Indexing is a central problem in computational biology andtextminingand has thus received significant research interest, including parameterized and heuristic approaches. Despite this interest, the best-known time-space trade-offs for Gapped String Indexing are the straightforward $O(n)$ space and $O(n+occ)$ query time or $Ω(n^2)$ space and $\tilde{O}(|P_1| + |P_2| + occ)$ query time.
  We break through this barrier obtaining the first interesting trade-offs with polynomially subquadratic space and polynomially sublinear query time. In particular, we show that, for every $0\leq δ\leq 1$, there is a data structure for Gapped String Indexing with either $\tilde{O}(n^{2-δ/3})$ or $\tilde{O}(n^{3-2δ})$ space and $\tilde{O}(|P_1| + |P_2| + n^δ\cdot (occ+1))$ query time, where $occ$ is the number of reported occurrences.
  As a new tool towards obtaining our main result, we introduce the Shifted Set Intersection problem. We show that this problem is equivalent to the indexing variant of 3SUM (3SUM Indexing). Via a series of reductions, we obtain a solution to the Gapped String Indexing problem. Furthermore, we enhance our data structure for deciding Shifted Set Intersection, so that we can support the reporting variant of the problem. Via the obtained equivalence to 3SUM Indexing, we thus give new improved data structures for the reporting variant of 3SUM Indexing, and we show how this improves upon the state-of-the-art solution for Jumbled Indexing for any alphabet of constant size $σ>5$.","['Philip Bille', 'Inge Li Gørtz', 'Moshe Lewenstein', 'Solon P. Pissis', 'Eva Rotenberg', 'Teresa Anna Steiner']",,arXiv,2024,https://doi.org/10.48550/arXiv.2211.16860,Anomali
A Survey of Relevant Text Mining Technology,"Recent advances intextminingand natural language processing technology have enabled researchers to detect an authors identity or demographic characteristics, such as age and gender, in severaltextgenres by automatically analysing the variation of linguistic characteristics. However, applying such techniques in the wild, i.e., in both cybercriminal and regular online social media, differs from more general applications in that its defining characteristics are both domain and process dependent. This gives rise to a number of challenges of which contemporary research has only scratched the surface. More specifically, atextminingapproach applied on social media communications typically has no control over the dataset size, the number of available communications will vary across users. Hence, the system has to be robust towards limited data availability. Additionally, the quality of the data cannot be guaranteed. As a result, the approach needs to be tolerant to a certain degree of linguistic noise (for example, abbreviations, non-standard language use, spelling variations and errors). Finally, in the context of cybercriminal fora, it has to be robust towards deceptive or adversarial behaviour, i.e. offenders who attempt to hide their criminal intentions (obfuscation) or who assume a false digital persona (imitation), potentially using coded language.
  In this work we present a comprehensive survey that discusses the problems that have already been addressed in current literature and review potential solutions. Additionally, we highlight which areas need to be given more attention.","['Claudia Peersman', 'Matthew Edwards', 'Emma Williams', 'Awais Rashid']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.15784,Anomali
AI Knows Which Words Will Appear in Next Year's Korean CSAT,"Atext-mining-based word class categorization method and LSTM-based vocabulary pattern prediction method are introduced in this paper. A preprocessing method based on simpletextappearance frequency analysis is first described. This method was developed as a data screening tool but showed 4.35 ~ 6.21 times higher than previous works. An LSTM deep learning method is also suggested for vocabulary appearance pattern prediction method. AI performs a regression with various size of data window of previous exams to predict the probabilities of word appearance in the next exam. Predicted values of AI over various data windows are processed into a single score as a weighted sum, which we call an ""AI-Score"", which represents the probability of word appearance in next year's exam. Suggested method showed 100% accuracy at the range 100-score area and showed only 1.7% error of prediction in the section where the scores were over 60 points. All source codes are freely available at the authors' Git Hub repository. (https://github.com/needleworm/bigdata_voca)","['Byunghyun Ban', 'Jejong Lee', 'Hyeonmok Hwang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2211.15426,Anomali
Automating Systematic Literature Reviews with Natural Language Processing and Text Mining: a Systematic Literature Review,"Objectives: An SLR is presented focusing ontextminingbased automation of SLR creation. The present review identifies the objectives of the automation studies and the aspects of those steps that were automated. In so doing, the various ML techniques used, challenges, limitations and scope of further research are explained.
  Methods: Accessible published literature studies that primarily focus on automation of study selection, study quality assessment, data extraction and data synthesis portions of SLR. Twenty-nine studies were analyzed.
  Results: This review identifies the objectives of the automation studies, steps within the study selection, study quality assessment, data extraction and data synthesis portions that were automated, the various ML techniques used, challenges, limitations and scope of further research.
  Discussion: We describe uses of NLP/TM techniques to support increased automation of systematic literature reviews. This area has attracted increase attention in the last decade due to significant gaps in the applicability of TM to automate steps in the SLR process. There are significant gaps in the application of TM and related automation techniques in the areas of data extraction, monitoring, quality assessment and data synthesis. There is thus a need for continued progress in this area, and this is expected to ultimately significantly facilitate the construction of systematic literature reviews.","['Girish Sundaram', 'Daniel Berleant']",,arXiv,2023,https://doi.org/10.48550/arXiv.2211.15397,Anomali
"Method for Determining the Similarity of Text Documents for the Kazakh language, Taking Into Account Synonyms: Extension to TF-IDF","The task of determining the similarity oftextdocuments has received considerable attention in many areas such as Information Retrieval,TextMining, Natural Language Processing (NLP) and Computational Linguistics. Transferring data to numeric vectors is a complex task where algorithms such as tokenization, stopword filtering, stemming, and weighting of terms are used. The term frequency - inverse document frequency (TF-IDF) is the most widely used term weighting method to facilitate the search for relevant documents. To improve the weighting of terms, a large number of TF-IDF extensions are made. In this paper, another extension of the TF-IDF method is proposed where synonyms are taken into account. The effectiveness of the method is confirmed by experiments on functions such as Cosine, Dice and Jaccard to measure the similarity oftextdocuments for the Kazakh language.",['Bakhyt Bakiyev'],,arXiv,2022,https://doi.org/10.48550/arXiv.2211.12364,Anomali
A Large-Scale Dataset for Biomedical Keyphrase Generation,"Keyphrase generation is the task consisting in generating a set of words or phrases that highlight the main topics of a document. There are few datasets for keyphrase generation in the biomedical domain and they do not meet the expectations in terms of size for training generative models. In this paper, we introduce kp-biomed, the first large-scale biomedical keyphrase generation dataset with more than 5M documents collected from PubMed abstracts. We train and release several generative models and conduct a series of experiments showing that using large scale datasets improves significantly the performances for present and absent keyphrase generation. The dataset is available under CC-BY-NC v4.0 license at https://huggingface.co/ datasets/taln-ls2n/kpbiomed.","['Mael Houbre', 'Florian Boudin', 'Beatrice Daille']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.12124,Anomali
Fast Multiplex Graph Association Rules for Link Prediction,"Multiplex networks allow us to study a variety of complex systems where nodes connect to each other in multiple ways, for example friend, family, and co-worker relations in social networks. Link prediction is the branch of network analysis allowing us to forecast the future status of a network: which new connections are the most likely to appear in the future? In multiplex link prediction we also ask: of which type? Because this last question is unanswerable with classical link prediction, here we investigate the use of graph association rules to inform multiplex link prediction. We derive such rules by identifying all frequent patterns in a network via multiplex graphmining, and then score each unobserved link's likelihood by finding the occurrences of each rule in the original network. Association rules add new abilities to multiplex link prediction: to predict new node arrivals, to consider higher order structures with four or more nodes, and to be memory efficient. We improve over previous work by creating a framework that is also efficient in terms of runtime, which enables an increase in prediction performance. This increase in efficiency allows us to improve a case study on a signed multiplex network, showing how graph association rules can provide valuable insights to extend social balance theory.","['Michele Coscia', 'Christian Borgelt', 'Michael Szell']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.12094,Anomali
Explainable Model-specific Algorithm Selection for Multi-Label Classification,"Multi-label classification (MLC) is an ML task of predictive modeling in which a data instance can simultaneously belong to multiple classes. MLC is increasingly gaining interest in different application domains such astextmining, computer vision, and bioinformatics. Several MLC algorithms have been proposed in the literature, resulting in a meta-optimization problem that the user needs to address: which MLC approach to select for a given dataset? To address this algorithm selection problem, we investigate in this work the quality of an automated approach that uses characteristics of the datasets - so-called features - and a trained algorithm selector to choose which algorithm to apply for a given task. For our empirical evaluation, we use a portfolio of 38 datasets. We consider eight MLC algorithms, whose quality we evaluate using six different performance metrics. We show that our automated algorithm selector outperforms any of the single MLC algorithms, and this is for all evaluated performance measures. Our selection approach is explainable, a characteristic that we exploit to investigate which meta-features have the largest influence on the decisions made by the algorithm selector. Finally, we also quantify the importance of the most significant meta-features for various domains.","['Ana Kostovska', 'Carola Doerr', 'Sašo Džeroski', 'Dragi Kocev', 'Panče Panov', 'Tome Eftimov']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.11227,Anomali
Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material,"Midrash collections are complex rabbinic works that consist oftextin multiple languages, which evolved through long processes of unstable oral and written transmission. Determining the origin of a given passage in such a compilation is not always straightforward and is often a matter of dispute among scholars, yet it is essential for scholars' understanding of the passage and its relationship to othertextsin the rabbinic corpus. To help solve this problem, we propose a system for classification of rabbinic literature based on its style, leveraging recent advances in natural language processing for Hebrewtexts. Additionally, we demonstrate how this method can be applied to uncover lost material from a specific midrash genre, Tan\d{h}uma-Yelammedenu, that has been preserved in later anthologies.","['Shlomo Tannor', 'Nachum Dershowitz', 'Moshe Lavee']","Journal of Data Mining & Digital Humanities, NLP4DH (August 13, 2023) jdmdh:11375",arXiv,2023,https://doi.org/10.48550/arXiv.2211.09710,Anomali
Absolute frequency measurement of the $6s^2~^1S_0 \to 6s6p~^3P_1$ $F=3/2\to F'=5/2$ $^{\text{201}}$Hg transition with background-free saturation spectroscopy,"We report the development of a method for eliminating background-induced systematic shifts affecting precise measurements of saturation absorption signals. With this technique, we measured the absolute frequency of the$6s^2~^1\text{S}_0 \to 6s6p~^3\text{P}_1$transition in$^{201a}\text{Hg}$($F=3/2\to F'=5/2$) to be $1181541111051(83)$~kHz. The measurement was referenced with an optical frequency comb synchronized to the frequency of the local representation of the UTC. This specific atomic line is situated on the steep slope of the Doppler background at room temperature, which results in frequency systematic shift. We determined the dependence of this shift on the properties of both the spectral line and the background of the measured signal.","['Adam Linek', 'Piotr Morzyński', 'Marcin Witkowski']","Opt. Express 30, 44103-44117, (2022)",arXiv,2022,https://doi.org/10.48550/arXiv.2211.09197,Anomali
Coronavirus statistics causes emotional bias: a social media text mining perspective,"While COVID-19 has impacted humans for a long time, people search the web for pandemic-related information, causing anxiety. From a theoretic perspective, previous studies have confirmed that the number of COVID-19 cases can cause negative emotions, but how statistics of different dimensions, such as the number of imported cases, the number of local cases, and the number of government-designated lockdown zones, stimulate people's emotions requires detailed understanding. In order to obtain the views of people on COVID-19, this paper first proposes a deep learning model which classifiestextsrelated to the pandemic fromtextdata with place labels. Next, it conducts a sentiment analysis based on multi-task learning. Finally, it carries out a fixed-effect panel regression with outputs of the sentiment analysis. The performance of the algorithm shows a promising result. The empirical study demonstrates while the number of local cases is positively associated with risk perception, the number of imported cases is negatively associated with confidence levels, which explains why citizens tend to ascribe the protracted pandemic to foreign factors. Besides, this study finds that previous pandemic hits cities recover slowly from the suffering, while local governments' spending on healthcare can improve the situation. Our study illustrates the reasons for risk perception and confidence based on different sources of statistical information due to cognitive bias. It complements the knowledge related to epidemic information. It also contributes to a framework that combines sentiment analysis using advanced deep learning technology with the empirical regression method.","['Linjiang Guo', 'Zijian Feng', 'Yuxue Chi', 'Mingzhu Wang', 'Yijun Liu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.08644,Anomali
Persian Emotion Detection using ParsBERT and Imbalanced Data Handling Approaches,"Emotion recognition is one of the machine learning applications which can be done usingtext, speech, or image data gathered from social media spaces. Detecting emotion can help us in different fields, including opinionmining. With the spread of social media, different platforms like Twitter have become data sources, and the language used in these platforms is informal, making the emotion detection task difficult. EmoPars and ArmanEmo are two new human-labeled emotion datasets for the Persian language. These datasets, especially EmoPars, are suffering from inequality between several samples between two classes. In this paper, we evaluate EmoPars and compare them with ArmanEmo. Throughout this analysis, we use data augmentation techniques, data re-sampling, and class-weights with Transformer-based Pretrained Language Models(PLMs) to handle the imbalance problem of these datasets. Moreover, feature selection is used to enhance the models' performance by emphasizing thetext'sspecific features. In addition, we provide a new policy for selecting data from EmoPars, which selects the high-confidence samples; as a result, the model does not see samples that do not have specific emotion during training. Our model reaches a Macro-averaged F1-score of 0.81 and 0.76 on ArmanEmo and EmoPars, respectively, which are new state-of-the-art results in these benchmarks.","['Amirhossein Abaskohi', 'Nazanin Sabri', 'Behnam Bahrak']",ACM Transactions on Asian and Low-Resource Language Information Processing 2022,arXiv,2022,https://doi.org/10.48550/arXiv.2211.08029,Anomali
Auto-outlier Fusion Technique for Chest X-ray classification with Multi-head Attention Mechanism,"A chest X-ray is one of the most widely available radiological examinations for diagnosing and detecting various lung illnesses. The National Institutes of Health (NIH) provides an extensive database, ChestX-ray8 and ChestXray14, to help establish a deep learning community for analysing and predicting lung diseases. ChestX-ray14 consists of 112,120 frontal-view X-ray images of 30,805 distinct patients withtext-minedfourteen disease image labels, where each image has multiple labels and has been utilised in numerous research in the past. To our current knowledge, no previous study has investigated outliers and multi-label impact for a single X-ray image during the preprocessing stage. The effect of outliers is mitigated in this paper by our proposed auto-outlier fusion technique. The image label is regenerated by concentrating on a particular factor in one image. The final cleaned dataset will be used to compare the mechanisms of multi-head self-attention and multi-head attention with generalised max-pooling.","['Yuru Jing', 'Zixuan Li']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.08006,Anomali
Conversational Pattern Mining using Motif Detection,"The subject of conversationalmininghas become of great interest recently due to the explosion of social and other online media. Supplementing this explosion oftextis the advancement in pre-trained language models which have helped us to leverage these sources of information. An interesting domain to analyse is conversations in terms of complexity and value. Complexity arises due to the fact that a conversation can be asynchronous and can involve multiple parties. It is also computationally intensive to process. We use unsupervised methods in our work in order to develop a conversational patternminingtechnique which does not require time consuming, knowledge demanding and resource intensive labelling exercises. The task of identifying repeating patterns in sequences is well researched in the Bioinformatics field. In our work, we adapt this to the field of Natural Language Processing and make several extensions to a motif detection algorithm. In order to demonstrate the application of the algorithm on a dynamic, real world data set; we extract motifs from an open-source film script data source. We run an exploratory investigation into the types of motifs we are able tomine.","['Nicolle Garber', 'Vukosi Marivate']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.06846,Anomali
Speech-to-Speech Translation For A Real-world Unwritten Language,"We study speech-to-speech translation (S2ST) that translates speech from one language into another language and focuses on building systems to support languages without standardtextwriting systems. We use English-Taiwanese Hokkien as a case study, and present an end-to-end solution from training data collection, modeling choices to benchmark dataset release. First, we present efforts on creating human annotated data, automaticallyminingdata from large unlabeled speech datasets, and adopting pseudo-labeling to produce weakly supervised data. On the modeling, we take advantage of recent advances in applying self-supervised discrete representations as target for prediction in S2ST and show the effectiveness of leveraging additionaltextsupervision from Mandarin, a language similar to Hokkien, in model training. Finally, we release an S2ST benchmark set to facilitate future research in this field. The demo can be found at https://huggingface.co/spaces/facebook/Hokkien_Translation .","['Peng-Jen Chen', 'Kevin Tran', 'Yilin Yang', 'Jingfei Du', 'Justine Kao', 'Yu-An Chung', 'Paden Tomasello', 'Paul-Ambroise Duquenne', 'Holger Schwenk', 'Hongyu Gong', 'Hirofumi Inaguma', 'Sravya Popuri', 'Changhan Wang', 'Juan Pino', 'Wei-Ning Hsu', 'Ann Lee']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.06474,Anomali
GeoAI for Knowledge Graph Construction: Identifying Causality Between Cascading Events to Support Environmental Resilience Research,"Knowledge graph technology is considered a powerful and semantically enabled solution to link entities, allowing users to derive new knowledge by reasoning data according to various types of reasoning rules. However, in building such a knowledge graph, events modeling, such as that of disasters, is often limited to single, isolated events. The linkages among cascading events are often missing in existing knowledge graphs. This paper introduces our GeoAI (Geospatial Artificial Intelligence) solutions to identify causality among events, in particular, disaster events, based on a set of spatially and temporally-enabled semantic rules. Through a use case of causal disaster events modeling, we demonstrated how these defined rules, including theme-based identification of correlated events, spatiotemporal co-occurrence constraint, andtextminingof event metadata, enable the automatic extraction of causal relationships between different events. Our solution enriches the event knowledge base and allows for the exploration of linked cascading events in large knowledge graphs, therefore empowering knowledge query and discovery.","['Yuanyuan Tian', 'Wenwen Li']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.06011,Anomali
DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis,"The rapid development of aspect-based sentiment analysis (ABSA) within recent decades shows great potential for real-world society. The current ABSA works, however, are mostly limited to the scenario of a singletextpiece, leaving the study in dialogue contexts unexplored. To bridge the gap between fine-grained sentiment analysis and conversational opinionmining, in this work, we introduce a novel task of conversational aspect-based sentiment quadruple analysis, namely DiaASQ, aiming to detect the quadruple of target-aspect-opinion-sentiment in a dialogue. We manually construct a large-scale high-quality DiaASQ dataset in both Chinese and English languages. We deliberately develop a neural model to benchmark the task, which advances in effectively performing end-to-end quadruple prediction, and manages to incorporate rich dialogue-specific and discourse feature representations for better cross-utterance quadruple extraction. We hope the new benchmark will spur more advancements in the sentiment analysis community.","['Bobo Li', 'Hao Fei', 'Fei Li', 'Yuhan Wu', 'Jinsong Zhang', 'Shengqiong Wu', 'Jingye Li', 'Yijiang Liu', 'Lizi Liao', 'Tat-Seng Chua', 'Donghong Ji']",,arXiv,2023,https://doi.org/10.48550/arXiv.2211.05705,Anomali
Extracting and Pre-Processing Event Logs,"Event data is the basis for all processmininganalysis. Most processminingtechniques assume their input to be an event log. However, event data is rarely recorded in an event log format, but has to be extracted from raw data. Event log extraction itself is an act of modeling as the analyst has to consciously choose which features of the raw data are used for describing which behavior of which entities. Being aware of these choices and subtle but important differences in concepts such as trace, case, activity, event, table, and log is crucial for mastering advanced processmininganalyses.
  Thistextprovides fundamental concepts and formalizations and discusses design decisions in event log extraction from a raw event table and for event log pre-processing. It is intended as study material for an advanced lecture in a processminingcourse.",['Dirk Fahland'],,arXiv,2022,https://doi.org/10.48550/arXiv.2211.04338,Anomali
Arabic Text Mining,"The rapid growth of the internet has increased the number of onlinetexts. This led to the rapid growth of the number of onlinetextsin the Arabic language. The enormous amount oftextmust be organized into classes to make the analysis process andtextretrieval easier.Textclassification is, therefore, a key component oftextmining. There are numerous systems and approaches for categorizing literature in English, European (French, German, Spanish), and Asian (Chinese, Japanese). In contrast, there are relatively few studies on categorizing Arabic literature due to the difficulty of the Arabic language. In this work, a brief explanation of key ideas relevant to Arabictextminingare introduced then a new classification system for the Arabic language is presented using light stemming and Classifier Naïve Bayesian (CNB).Textsfrom two classes: politics and sports, are included in our corpus. Sometextsare added to the system, and the system correctly classified them, demonstrating the effectiveness of the system.","['Sumaia Mohammed AL-Ghuribi', 'Shahrul Azman Mohd Noah']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.02772,Anomali
Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia,"Online encyclopedias, such as Wikipedia, have been well-developed and researched in the last two decades. One can find any attributes or other information of a wiki item on a wiki page edited by a community of volunteers. However, the traditionaltext, images and tables can hardly express some aspects of an wiki item. For example, when we talk about ``Shiba Inu'', one may care more about ``How to feed it'' or ``How to train it not to protect its food''. Currently, short-video platforms have become a hallmark in the online world. Whether you're on TikTok, Instagram, Kuaishou, or YouTube Shorts, short-video apps have changed how we consume and create content today. Except for producing short videos for entertainment, we can find more and more authors sharing insightful knowledge widely across all walks of life. These short videos, which we call knowledge videos, can easily express any aspects (e.g. hair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and they can be systematically analyzed and organized like an online encyclopedia. In this paper, we propose Kuaipedia, a large-scale multi-modal encyclopedia consisting of items, aspects, and short videos lined to them, which was extracted from billions of videos of Kuaishou (Kwai), a well-known short-video platform in China. We first collected items from multiple sources andmineduser-centered aspects from millions of users' queries to build an item-aspect tree. Then we propose a new task called ``multi-modal item-aspect linking'' as an expansion of ``entity linking'' to link short videos into item-aspect pairs and build the whole short-video encyclopedia. Intrinsic evaluations show that our encyclopedia is of large scale and highly accurate. We also conduct sufficient extrinsic experiments to show how Kuaipedia can help fundamental applications such as entity typing and entity linking.","['Haojie Pan', 'Zepeng Zhai', 'Yuzhou Zhang', 'Ruiji Fu', 'Ming Liu', 'Yangqiu Song', 'Zhongyuan Wang', 'Bing Qin']",,arXiv,2023,https://doi.org/10.48550/arXiv.2211.00732,Anomali
Leveraging Graph-based Cross-modal Information Fusion for Neural Sign Language Translation,"Sign Language (SL), as the mother tongue of the deaf community, is a special visual language that most hearing people cannot understand. In recent years, neural Sign Language Translation (SLT), as a possible way for bridging communication gap between the deaf and the hearing people, has attracted widespread academic attention. We found that the current mainstream end-to-end neural SLT models, which tries to learning language knowledge in a weakly supervised manner, could notmineenough semantic information under the condition of low data resources. Therefore, we propose to introduce additional word-level semantic knowledge of sign language linguistics to assist in improving current end-to-end neural SLT models. Concretely, we propose a novel neural SLT model with multi-modal feature fusion based on the dynamic graph, in which the cross-modal information, i.e.textand video, is first assembled as a dynamic graph according to their correlation, and then the graph is processed by a multi-modal graph encoder to generate the multi-modal embeddings for further usage in the subsequent neural translation models. To the best of our knowledge, we are the first to introduce graph neural networks, for fusing multi-modal information, into neural sign language translation models. Moreover, we conducted experiments on a publicly available popular SLT dataset RWTH-PHOENIX-Weather-2014T. and the quantitative experiments show that our method can improve the model.","['Jiangbin Zheng', 'Siyuan Li', 'Cheng Tan', 'Chong Wu', 'Yidong Chen', 'Stan Z. Li']",,arXiv,2022,https://doi.org/10.48550/arXiv.2211.00526,Anomali
Automated Code Extraction from Discussion Board Text Dataset,"This study introduces and investigates the capabilities of three differenttextminingapproaches, namely Latent Semantic Analysis, Latent Dirichlet Analysis, and Clustering Word Vectors, for automating code extraction from a relatively small discussion board dataset. We compare the outputs of each algorithm with a previous dataset that was manually coded by two human raters. The results show that even with a relatively small dataset, automated approaches can be an asset to course instructors by extracting some of the discussion codes, which can be used in Epistemic Network Analysis.","['Sina Mahdipour Saravani', 'Sadaf Ghaffari', 'Yanye Luther', 'James Folkestad', 'Marcia Moraes']",,arXiv,2023,https://doi.org/10.48550/arXiv.2210.17495,Anomali
Mining Word Boundaries in Speech as Naturally Annotated Word Segmentation Data,"Inspired by early research on exploring naturally annotated data for Chinese word segmentation (CWS), and also by recent research on integration of speech andtextprocessing, this work for the first time proposes tomineword boundaries from parallel speech/textdata. First we collect parallel speech/textdata from two Internet sources that are related with CWS data used in our experiments. Then, we obtain character-level alignments and design simple heuristic rules for determining word boundaries according to pause duration between adjacent characters. Finally, we present an effective complete-then-train strategy that can better utilize extra naturally annotated data for model training. Experiments demonstrate our approach can significantly boost CWS performance in both cross-domain and low-resource scenarios.","['Lei Zhang', 'Zhenghua Li', 'Shilin Zhou', 'Chen Gong', 'Zhefeng Wang', 'Baoxing Huai', 'Min Zhang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2210.17122,Anomali
Incorporating Crowdsourced Annotator Distributions into Ensemble Modeling to Improve Classification Trustworthiness for Ancient Greek Papyri,"Performing classification on noisy, crowdsourced image datasets can prove challenging even for the best neural networks. Two issues which complicate the problem on such datasets are class imbalance and ground-truth uncertainty in labeling. The AL-ALL and AL-PUB datasets - consisting of tightly cropped, individual characters from images of ancient Greek papyri - are strongly affected by both issues. The application of ensemble modeling to such datasets can help identify images where the ground-truth is questionable and quantify the trustworthiness of those samples. As such, we apply stacked generalization consisting of nearly identical ResNets with different loss functions: one utilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence (KLD). Both networks use labels drawn from a crowd-sourced consensus. This consensus is derived from a Normalized Distribution of Annotations (NDA) based on all annotations for a given character in the dataset. For the second network, the KLD is calculated with respect to the NDA. For our ensemble model, we apply a k-nearest neighbors model to the outputs of the CXE and KLD networks. Individually, the ResNet models have approximately 93% accuracy, while the ensemble model achieves an accuracy of > 95%, increasing the classification trustworthiness. We also perform an analysis of the Shannon entropy of the various models' output distributions to measure classification uncertainty. Our results suggest that entropy is useful for predicting model misclassifications.","['Graham West', 'Matthew I. Swindall', 'Ben Keener', 'Timothy Player', 'Alex C. Williams', 'James H. Brusuelas', 'John F. Wallin']","Journal of Data Mining & Digital Humanities, Historical Documents and automatic text recognition, Digital humanities in languages (February 7, 2024) jdmdh:10297",arXiv,2024,https://doi.org/10.48550/arXiv.2210.16380,Anomali
Can Current Explainability Help Provide References in Clinical Notes to Support Humans Annotate Medical Codes?,"The medical codes prediction problem from clinical notes has received substantial interest in the NLP community, and several recent studies have shown the state-of-the-art (SOTA) code prediction results of full-fledged deep learning-based methods. However, most previous SOTA works based on deep learning are still in early stages in terms of providing textual references and explanations of the predicted codes, despite the fact that this level of explainability of the prediction outcomes is critical to gaining trust from professional medical coders. This raises the important question of how well current explainability methods apply to advanced neural network models such as transformers to predict correct codes and present references in clinical notes that support code prediction. First, we present an explainable Read, Attend, and Code (xRAC) framework and assess two approaches, attention score-based xRAC-ATTN and model-agnostic knowledge-distillation-based xRAC-KD, through simplified but thorough human-grounded evaluations with SOTA transformer-based model, RAC. We find that the supporting evidencetexthighlighted by xRAC-ATTN is of higher quality than xRAC-KD whereas xRAC-KD has potential advantages in production deployment scenarios. More importantly, we show for the first time that, given the current state of explainability methodologies, using the SOTA medical codes prediction system still requires the expertise and competencies of professional coders, even though its prediction accuracy is superior to that of human coders. This, we believe, is a very meaningful step toward developing explainable and accurate machine learning systems for fully autonomous medical code prediction from clinical notes.","['Byung-Hak Kim', 'Zhongfen Deng', 'Philip S. Yu', 'Varun Ganapathi']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.15882,Anomali
Timed Alignments with Mixed Moves,"The subject of this paper is to study conformance checking for timed models, that is, process models that consider both the sequence of events in a process as well as the timestamps at which each event is recorded. Time-aware processminingis a growing subfield of research, and as tools that seek to discover timing related properties in processes develop, so does the need for conformance checking techniques that can tackle time constraints and provide insightful quality measures for time-aware process models. In particular, one of the most useful conformance artefacts is the alignment, that is, finding the minimal changes necessary to correct a new observation to conform to a process model.  This paper follows a previous one, where we have set our problem of timed alignment. In the present paper, we solve the case where the metrics used to compare timed processes allows mixed moves, i.e. an error on the timestamp of an event may or may not have propagated to its successors, and provide linear time algorithms for distance computation and alignment on models with sequential causal processes.","['Neha Rino', 'Thomas Chatain']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.15209,Anomali
"Don't Prompt, Search! Mining-based Zero-Shot Learning with Language Models","Masked language models like BERT can performtextclassification in a zero-shot fashion by reformulating downstream tasks astextinfilling. However, this approach is highly sensitive to the template used to prompt the model, yet practitioners are blind when designing them in strict zero-shot settings. In this paper, we propose an alternativemining-based approach for zero-shot learning. Instead of prompting language models, we use regular expressions tominelabeled examples from unlabeled corpora, which can optionally be filtered through prompting, and used to finetune a pretrained model. Our method is more flexible and interpretable than prompting, and outperforms it on a wide range of tasks when using comparable templates. Our results suggest that the success of prompting can partly be explained by the model being exposed to similar examples during pretraining, which can be directly retrieved through regular expressions.","['Mozes van de Kar', 'Mengzhou Xia', 'Danqi Chen', 'Mikel Artetxe']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.14803,Anomali
Enhancing Product Safety in E-Commerce with NLP,"Ensuring safety of the products offered to the customers is of paramount importance to any e- commerce platform. Despite stringent quality and safety checking of products listed on these platforms, occasionally customers might receive a product that can pose a safety issue arising out of its use. In this paper, we present an innovative mechanism of how a large scale multinational e-commerce platform, Zalando, uses Natural Language Processing techniques to assist timely investigation of the potentially unsafe productsmineddirectly from customer written claims in unstructured plaintext. We systematically describe the types of safety issues that concern Zalando customers. We demonstrate how we map this core business problem into a supervisedtextclassification problem with highly imbalanced, noisy, multilingual data in a AI-in-the-loop setup with a focus on Key Performance Indicator (KPI) driven evaluation. Finally, we present detailed ablation studies to show a comprehensive comparison between different classification techniques. We conclude the work with how this NLP model was deployed.","['Kishaloy Halder', 'Josip Krapac', 'Dmitry Goryunov', 'Anthony Brew', 'Matti Lyra', 'Alsida Dizdari', 'William Gillett', 'Adrien Renahy', 'Sinan Tang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.14363,Anomali
"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering","We introduce RoMQA, the first benchmark for robust, multi-evidence, multi-answer question answering (QA). RoMQA contains clusters of questions that are derived from related constraintsminedfrom the Wikidata knowledge graph. RoMQA evaluates robustness of QA models to varying constraints by measuring worst-case performance within each question cluster. Compared to prior QA datasets, RoMQA has more human-written questions that require reasoning over more evidencetextand have, on average, many more correct answers. In addition, human annotators rate RoMQA questions as more natural or likely to be asked by people. We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is challenging: zero-shot and few-shot models perform similarly to naive baselines, while supervised retrieval methods perform well below gold evidence upper bounds. Moreover, existing models are not robust to variations in question constraints, but can be made more robust by tuning on clusters of related questions. Our results show that RoMQA is a challenging benchmark for large language models, and provides a quantifiable test to build more robust QA methods.","['Victor Zhong', 'Weijia Shi', 'Wen-tau Yih', 'Luke Zettlemoyer']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.14353,Anomali
Toward an Intelligent Tutoring System for Argument Mining in Legal Texts,"We propose an adaptive environment (CABINET) to support caselaw analysis (identifying key argument elements) based on a novel cognitive computing framework that carefully matches various machine learning (ML) capabilities to the proficiency of a user. CABINET supports law students in their learning as well as professionals in their work. The results of our experiments focused on the feasibility of the proposed framework are promising. We show that the system is capable of identifying a potential error in the analysis with very low false positives rate (2.0-3.5%), as well as of predicting the key argument element type (e.g., an issue or a holding) with a reasonably high F1-score (0.74).","['Hannes Westermann', 'Jaromir Savelka', 'Vern R. Walker', 'Kevin D. Ashley', 'Karim Benyekhlef']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.13635,Anomali
Multimodal Model with Text and Drug Embeddings for Adverse Drug Reaction Classification,"In this paper, we focus on the classification of tweets as sources of potential signals for adverse drug effects (ADEs) or drug reactions (ADRs). Following the intuition thattextand drug structure representations are complementary, we introduce a multimodal model with two components. These components are state-of-the-art BERT-based models for language understanding and molecular property prediction. Experiments were carried out on multilingual benchmarks of the Social MediaMiningfor Health Research and Applications (#SMM4H) initiative. Our models obtained state-of-the-art results of 0.61 F1 and 0.57 F1 on #SMM4H 2021 Shared Tasks 1a and 2 in English and Russian, respectively. On the classification of French tweets from SMM4H 2020 Task 1, our approach pushes the state of the art by an absolute gain of 8% F1. Our experiments show that the molecular information obtained from neural networks is more beneficial for ADE classification than traditional molecular descriptors. The source code for our models is freely available at https://github.com/Andoree/smm4h_2021_classification.","['Andrey Sakhovskiy', 'Elena Tutubalina']","Journal of Biomedical Informatics, Volume 135, 2022, 104182, ISSN 1532-0464",arXiv,2022,https://doi.org/10.48550/arXiv.2210.13238,Anomali
Dissecting Deep Metric Learning Losses for Image-Text Retrieval,"Visual-Semantic Embedding (VSE) is a prevalent approach in image-textretrieval by learning a joint embedding space between the image and language modalities where semantic similarities would be preserved. The triplet loss with hard-negativemininghas become the de-facto objective for most VSE methods. Inspired by recent progress in deep metric learning (DML) in the image domain which gives rise to new loss functions that outperform triplet loss, in this paper, we revisit the problem of finding better objectives for VSE in image-textmatching. Despite some attempts in designing losses based on gradient movement, most DML losses are defined empirically in the embedding space. Instead of directly applying these loss functions which may lead to sub-optimal gradient updates in model parameters, in this paper we present a novel Gradient-based Objective AnaLysis framework, or \textit{GOAL}, to systematically analyze the combinations and reweighting of the gradients in existing DML functions. With the help of this analysis framework, we further propose a new family of objectives in the gradient space exploring different gradient combinations. In the event that the gradients are not integrable to a valid loss function, we implement our proposed objectives such that they would directly operate in the gradient space instead of on the losses in the embedding space. Comprehensive experiments have demonstrated that our novel objectives have consistently improved performance over baselines across different visual/textfeatures and model frameworks. We also showed the generalizability of the GOAL framework by extending it to other models using triplet family losses including vision-language model with heavy cross-modal interactions and have achieved state-of-the-art results on the image-textretrieval tasks on COCO and Flick30K.","['Hong Xuan', 'Xi Chen']",WACV2023,arXiv,2022,https://doi.org/10.48550/arXiv.2210.13188,Anomali
Full-Text Argumentation Mining on Scientific Publications,"Scholarly ArgumentationMining(SAM) has recently gained attention due to its potential to help scholars with the rapid growth of published scientific literature. It comprises two subtasks: argumentative discourse unit recognition (ADUR) and argumentative relation extraction (ARE), both of which are challenging since they require e.g. the integration of domain knowledge, the detection of implicit statements, and the disambiguation of argument structure. While previous work focused on dataset construction and baseline methods for specific document sections, such as abstract or results, full-textscholarly argumentationmininghas seen little progress. In this work, we introduce a sequential pipeline model combining ADUR and ARE for full-textSAM, and provide a first analysis of the performance of pretrained language models (PLMs) on both subtasks. We establish a new SotA for ADUR on the Sci-Arg corpus, outperforming the previous best reported result by a large margin (+7% F1). We also present the first results for ARE, and thus for the full AM pipeline, on this benchmark dataset. Our detailed error analysis reveals that non-contiguous ADUs as well as the interpretation of discourse connectors pose major challenges and that data annotation needs to be more consistent.","['Arne Binder', 'Bhuvanesh Verma', 'Leonhard Hennig']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.13084,Anomali
Feature selection intelligent algorithm with mutual information and steepest ascent strategy,"Remote sensing is a higher technology to produce knowledge for dataminingapplications. In principle hyperspectral images (HSIs) is a remote sensing tool that provides precise classification of regions. The HSI contains more than a hundred of images of the ground truth (GT) map. Some images are carrying relevant information, but others describe redundant information, or they are affected by atmospheric noise. The aim is to reduce dimensionality of HSI. Many studies use mutual information (MI) or normalised forms of MI to select appropriate bands. In this paper we design an algorithm based also on MI, and we combine MI with steepest ascent algorithm, to improve a symmetric uncertainty coefficient-based strategy to select relevant bands for classification of HSI. This algorithm is a feature selection tool and a wrapper strategy. We perform our study on HSI AVIRIS 92AV3C. This is an artificial intelligent system to control redundancy; we had to clear the difference of the result's algorithm and the human decision, and this can be viewed as case study which human decision is perhaps different to an intelligent algorithm. Index Terms - Hyperspectral images, Classification, Fea-ture selection, Mutual Information, Redundancy, Steepest Ascent. Artificial Intelligence","['Elkebir Sarhrouni', 'Ahmed Hammouch', 'Driss Aboutajdine']","Int. J. Advanced Intelligence Paradigms, Vol. 5, No. 4, 2013 - http://www.scopus.com/inward/record.url?eid=2-s2.0-84890828902&partnerID=MN8TOARS",arXiv,2022,https://doi.org/10.48550/arXiv.2210.12296,Anomali
Image-Text Retrieval with Binary and Continuous Label Supervision,"Most image-textretrieval work adopts binary labels indicating whether a pair of image andtextmatches or not. Such a binary indicator covers only a limited subset of image-textsemantic relations, which is insufficient to represent relevance degrees between images andtextsdescribed by continuous labels such as image captions. The visual-semantic embedding space obtained by learning binary labels is incoherent and cannot fully characterize the relevance degrees. In addition to the use of binary labels, this paper further incorporates continuous pseudo labels (generally approximated bytextsimilarity between captions) to indicate the relevance degrees. To learn a coherent embedding space, we propose an image-textretrieval framework with Binary and Continuous Label Supervision (BCLS), where binary labels are used to guide the retrieval model to learn limited binary correlations, and continuous labels are complementary to the learning of image-textsemantic relations. For the learning of binary labels, we improve the common Triplet ranking loss with Soft Negativemining(Triplet-SN) to improve convergence. For the learning of continuous labels, we design Kendall ranking loss inspired by Kendall rank correlation coefficient (Kendall), which improves the correlation between the similarity scores predicted by the retrieval model and the continuous labels. To mitigate the noise introduced by the continuous pseudo labels, we further design Sliding Window sampling and Hard Sampleminingstrategy (SW-HS) to alleviate the impact of noise and reduce the complexity of our framework to the same order of magnitude as the triplet ranking loss. Extensive experiments on two image-textretrieval benchmarks demonstrate that our method can improve the performance of state-of-the-art image-textretrieval models.","['Zheng Li', 'Caili Guo', 'Zerun Feng', 'Jenq-Neng Hwang', 'Ying Jin', 'Yufeng Zhang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.11319,Anomali
Machine and Deep Learning Methods with Manual and Automatic Labelling for News Classification in Bangla Language,"Research in Natural Language Processing (NLP) has increasingly become important due to applications such astextclassification,textmining, sentiment analysis, POS tagging, named entity recognition, textual entailment, and many others. This paper introduces several machine and deep learning methods with manual and automatic labelling for news classification in the Bangla language. We implemented several machine (ML) and deep learning (DL) algorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient Descent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit (GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and FastText word embedding models. We develop automatic labelling methods using Latent Dirichlet Allocation (LDA) and investigate the performance of single-label and multi-label article classification methods. To investigate performance, we developed from scratch Potrika, the largest and the most extensive dataset for news classification in the Bangla language, comprising 185.51 million words and 12.57 million sentences contained in 664,880 news articles in eight distinct categories, curated from six popular online news portals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83% achieve the highest accuracy for manually-labelled data. For the automatic labelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy for single-label and multi-label data, respectively. The methods developed in this paper are expected to advance research in Bangla and other languages.","['Istiak Ahmad', 'Fahad AlQurashi', 'Rashid Mehmood']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.10903,Anomali
Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective,"Two interlocking research questions of growing interest and importance in privacy research are Authorship Attribution (AA) and Authorship Obfuscation (AO). Given an artifact, especially atextt in question, an AA solution aims to accurately attribute t to its true author out of many candidate authors while an AO solution aims to modify t to hide its true authorship. Traditionally, the notion of authorship and its accompanying privacy concern is only toward human authors. However, in recent years, due to the explosive advancements in NeuralTextGeneration (NTG) techniques in NLP, capable of synthesizing human-quality open-endedtexts(so-called ""neuraltexts""), one has to now consider authorships by humans, machines, or their combination. Due to the implications and potential threats of neuraltextswhen used maliciously, it has become critical to understand the limitations of traditional AA/AO solutions and develop novel AA/AO solutions in dealing with neuraltexts. In this survey, therefore, we make a comprehensive review of recent literature on the attribution and obfuscation of neuraltextauthorship from a DataMiningperspective, and share our view on their limitations and promising research directions.","['Adaku Uchendu', 'Thai Le', 'Dongwon Lee']",,arXiv,2023,https://doi.org/10.48550/arXiv.2210.10488,Anomali
BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining,"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study ontextgeneration further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms. Code is available at https://github.com/microsoft/BioGPT.","['Renqian Luo', 'Liai Sun', 'Yingce Xia', 'Tao Qin', 'Sheng Zhang', 'Hoifung Poon', 'Tie-Yan Liu']","Briefings in Bioinformatics, 2022;, bbac409",arXiv,2023,https://doi.org/10.48550/arXiv.2210.10341,Anomali
CLIP-Driven Fine-grained Text-Image Person Re-identification,"TIReID aims to retrieve the image corresponding to the giventextquery from a pool of candidate images. Existing methods employ prior knowledge from single-modality pre-training to facilitate learning, but lack multi-modal correspondences. Besides, due to the substantial gap between modalities, existing methods embed the original modal features into the same latent space for cross-modal alignment. However, feature embedding may lead to intra-modal information distortion. Recently, CLIP has attracted extensive attention from researchers due to its powerful semantic concept learning capacity and rich multi-modal knowledge, which can help us solve the above problems. Accordingly, in the paper, we propose a CLIP-driven Fine-grained information excavation framework (CFine) to fully utilize the powerful knowledge of CLIP for TIReID. To transfer the multi-modal knowledge effectively, we perform fine-grained information excavation tomineintra-modal discriminative clues and inter-modal correspondences. Specifically, we first design a multi-grained global feature learning module to fullymineintra-modal discriminative local information, which can emphasize identity-related discriminative clues by enhancing the interactions between global image (text) and informative local patches (words). Secondly, cross-grained feature refinement (CFR) and fine-grained correspondence discovery (FCD) modules are proposed to establish the cross-grained and fine-grained interactions between modalities, which can filter out non-modality-shared image patches/words andminecross-modal correspondences from coarse to fine. CFR and FCD are removed during inference to save computational costs. Note that the above process is performed in the original modality space without further feature embedding. Extensive experiments on multiple benchmarks demonstrate the superior performance of our method on TIReID.","['Shuanglin Yan', 'Neng Dong', 'Liyan Zhang', 'Jinhui Tang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.10276,Anomali
Multi-granularity Argument Mining in Legal Texts,"In this paper, we explore legal argumentminingusing multiple levels of granularity. Argumentmininghas usually been conceptualized as a sentence classification problem. In this work, we conceptualize argumentminingas a token-level (i.e., word-level) classification problem. We use a Longformer model to classify the tokens. Results show that token-leveltextclassification identifies certain legal argument elements more accurately than sentence-leveltextclassification. Token-level classification also provides greater flexibility to analyze legaltextsand to gain more insight into what the model focuses on when processing a large amount of input data.","['Huihui Xu', 'Kevin Ashley']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.09472,Anomali
Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints,"Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not.
  Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedicaltextmining. The code used in the experiments are going to be made available at https://github.com/omidrohanian/bottleneck-adapters.","['Omid Rohanian', 'Hannah Jauncey', 'Mohammadmahdi Nouriborji', 'Vinod Kumar Chauhan', 'Bronner P. Gonçalves', 'Christiana Kartsonaki', 'ISARIC Clinical Characterisation Group', 'Laura Merson', 'David Clifton']",,arXiv,2023,https://doi.org/10.48550/arXiv.2210.09440,Anomali
Privacy of federated QR decomposition using additive secure multiparty computation,"Federated learning (FL) is a privacy-aware dataminingstrategy keeping the private data on the owners' machine and thereby confidential. The clients compute local models and send them to an aggregator which computes a global model. In hybrid FL, the local parameters are additionally masked using secure aggregation, such that only the global aggregated statistics become available in cleartext, not the client specific updates. Federated QR decomposition has not been studied extensively in the context of cross-silo federated learning. In this article, we investigate the suitability of three QR decomposition algorithms for cross-silo FL and suggest a privacy-aware QR decomposition scheme based on the Gram-Schmidt algorithm which does not blatantly leak raw data. We apply the algorithm to compute linear regression in a federated manner.","['Anne Hartebrodt', 'Richard Röttger']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.06163,Anomali
ERNIE-Layout: Layout Knowledge Enhanced Pre-training for Visually-rich Document Understanding,"Recent years have witnessed the rise and success of pre-training techniques in visually-rich document understanding. However, most existing methods lack the systematicminingand utilization of layout-centered knowledge, leading to sub-optimal performances. In this paper, we propose ERNIE-Layout, a novel document pre-training solution with layout knowledge enhancement in the whole workflow, to learn better representations that combine the features fromtext, layout, and image. Specifically, we first rearrange input sequences in the serialization stage, and then present a correlative pre-training task, reading order prediction, to learn the proper reading order of documents. To improve the layout awareness of the model, we integrate a spatial-aware disentangled attention into the multi-modal transformer and a replaced regions prediction task into the pre-training phase. Experimental results show that ERNIE-Layout achieves superior performance on various downstream tasks, setting new state-of-the-art on key information extraction, document image classification, and document question answering datasets. The code and models are publicly available at http://github.com/PaddlePaddle/PaddleNLP/tree/develop/model_zoo/ernie-layout.","['Qiming Peng', 'Yinxu Pan', 'Wenjin Wang', 'Bin Luo', 'Zhenyu Zhang', 'Zhengjie Huang', 'Teng Hu', 'Weichong Yin', 'Yongfeng Chen', 'Yin Zhang', 'Shikun Feng', 'Yu Sun', 'Hao Tian', 'Hua Wu', 'Haifeng Wang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.06155,Anomali
Adversarial Contrastive Learning for Evidence-aware Fake News Detection with Graph Neural Networks,"The prevalence and perniciousness of fake news have been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on attention mechanisms. Despite their effectiveness, they still suffer from three weaknesses. Firstly, sequential models fail to integrate the relevant information that is scattered far apart in evidences. Secondly, they underestimate much redundant information in evidences may be useless or harmful. Thirdly, insufficient data utilization limits the separability and reliability of representations captured by the model. To solve these problems, we propose a unified Graph-based sEmantic structureminingframework with ConTRAstive Learning, namely GETRAL in short. Specifically, we first model claims and evidences as graph-structured data to capture the long-distance semantic dependency. Consequently, we reduce information redundancy by performing graph structure learning. Then the fine-grained semantic representations are fed into the claim-evidence interaction module for predictions. Finally, an adversarial contrastive learning module is applied to make full use of data and strengthen representation learning. Comprehensive experiments have demonstrated the superiority of GETRAL over the state-of-the-arts and validated the efficacy of semanticminingwith graph structure and contrastive learning.","['Junfei Wu', 'Weizhi Xu', 'Qiang Liu', 'Shu Wu', 'Liang Wang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.05498,Anomali
Bridging CLIP and StyleGAN through Latent Alignment for Image Editing,"Text-driven image manipulation is developed since the vision-language model (CLIP) has been proposed. Previous work has adopted CLIP to design atext-image consistency-based objective to address this issue. However, these methods require either test-time optimization or image feature cluster analysis for single-mode manipulation direction. In this paper, we manage to achieve inference-time optimization-free diverse manipulation directionminingby bridging CLIP and StyleGAN through Latent Alignment (CSLA). More specifically, our efforts consist of three parts: 1) a data-free training strategy to train latent mappers to bridge the latent space of CLIP and StyleGAN; 2) for more precise mapping, temporal relative consistency is proposed to address the knowledge distribution bias problem among different latent spaces; 3) to refine the mapped latent in s space, adaptive style mixing is also proposed. With this mapping scheme, we can achieve GAN inversion,text-to-image generation andtext-driven image manipulation. Qualitative and quantitative comparisons are made to demonstrate the effectiveness of our method.","['Wanfeng Zheng', 'Qiang Li', 'Xiaoyan Guo', 'Pengfei Wan', 'Zhongyuan Wang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2210.04506,Anomali
Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP,"Open-vocabulary semantic segmentation aims to segment an image into semantic regions according totextdescriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their correspondingtextdescriptions. We collect training data byminingan existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, we utilize the ""blank"" areas in masked images using a method we dub mask prompt tuning. Experiments demonstrate mask prompt tuning brings significant improvement without modifying any weights of CLIP, and it can further improve a fully finetuned model. In particular, when trained on COCO and evaluated on ADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the previous state-of-the-art. For the first time, open-vocabulary generalist models match the performance of supervised specialist models in 2017 without dataset-specific adaptations.","['Feng Liang', 'Bichen Wu', 'Xiaoliang Dai', 'Kunpeng Li', 'Yinan Zhao', 'Hang Zhang', 'Peizhao Zhang', 'Peter Vajda', 'Diana Marculescu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2210.04150,Anomali
Reasoning about Complex Networks: A Logic Programming Approach,"Reasoning about complex networks has in recent years become an important topic of study due to its many applications: the adoption of commercial products, spread of disease, the diffusion of an idea, etc. In this paper, we present the MANCaLog language, a formalism based on logic programming that satisfies a set of desiderata proposed in previous work as recommendations for the development of approaches to reasoning in complex networks. To the best of our knowledge, this is the first formalism that satisfies all such criteria. We first focus on algorithms for finding minimal models (on which multi-attribute analysis can be done), and then on how this formalism can be applied in certain real world scenarios. Towards this end, we study the problem of deciding group membership in social networks: given a social network and a set of groups where group membership of only some of the individuals in the network is known, we wish to determine a degree of membership for the remaining group-individual pairs. We develop a prototype implementation that we use to obtain experimental results on two real world datasets, including a current social network of criminal gangs in a major U.S.\ city. We then show how the assignment of degree of membership to nodes in this case allows for a better understanding of the criminal gang problem when combined with other social networkminingtechniques -- including detection of sub-groups and identification of core group members -- which would not be possible without further identification of additional group members.","['Paulo Shakarian', 'Gerardo I. Simari', 'Devon Callahan']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.15067,Anomali
Perturbations and Subpopulations for Testing Robustness in Token-Based Argument Unit Recognition,Argument Unit Recognition and Classification aims at identifying argument units fromtextand classifying them as pro or against. One of the design choices that need to be made when developing systems for this task is what the unit of classification should be: segments of tokens or full sentences. Previous research suggests that fine-tuning language models on the token-level yields more robust results for classifying sentences compared to training on sentences directly. We reproduce the study that originally made this claim and further investigate what exactly token-based systems learned better compared to sentence-based ones. We develop systematic tests for analysing the behavioural differences between the token-based and the sentence-based system. Our results show that token-based models are generally more robust than sentence-based models both on manually perturbed examples and on specific subpopulations of the data.,"['Jonathan Kamp', 'Lisa Beinborn', 'Antske Fokkens']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.14780,Anomali
Knowledge-Aware Bayesian Deep Topic Model,"We propose a Bayesian generative model for incorporating prior domain knowledge into hierarchical topic modeling. Although embedded topic models (ETMs) and its variants have gained promising performance intextanalysis, they mainly focus onminingword co-occurrence patterns, ignoring potentially easy-to-obtain prior topic hierarchies that could help enhance topic coherence. While several knowledge-based topic models have recently been proposed, they are either only applicable to shallow hierarchies or sensitive to the quality of the provided prior knowledge. To this end, we develop a novel deep ETM that jointly models the documents and the given prior knowledge by embedding the words and topics into the same space. Guided by the provided knowledge, the proposed model tends to discover topic hierarchies that are organized into interpretable taxonomies. Besides, with a technique for adapting a given graph, our extended version allows the provided prior topic structure to be finetuned to match the target corpus. Extensive experiments show that our proposed model efficiently integrates the prior knowledge and improves both hierarchical topic discovery and document representation.","['Dongsheng Wang', 'Yishi Xu', 'Miaoge Li', 'Zhibin Duan', 'Chaojie Wang', 'Bo Chen', 'Mingyuan Zhou']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.14228,Anomali
Unified Loss of Pair Similarity Optimization for Vision-Language Retrieval,"There are two popular loss functions used for vision-language retrieval, i.e., triplet loss and contrastive learning loss, both of them essentially minimize the difference between the similarities of negative pairs and positive pairs. More specifically, Triplet loss with Hard Negativemining(Triplet-HN), which is widely used in existing retrieval models to improve the discriminative ability, is easy to fall into local minima in training. On the other hand, Vision-Language Contrastive learning loss (VLC), which is widely used in the vision-language pre-training, has been shown to achieve significant performance gains on vision-language retrieval, but the performance of fine-tuning with VLC on small datasets is not satisfactory. This paper proposes a unified loss of pair similarity optimization for vision-language retrieval, providing a powerful tool for understanding existing loss functions. Our unified loss includes the hard sampleminingstrategy of VLC and introduces the margin used by the triplet loss for better similarity separation. It is shown that both Triplet-HN and VLC are special forms of our unified loss. Compared with the Triplet-HN, our unified loss has a fast convergence speed. Compared with the VLC, our unified loss is more discriminative and can provide better generalization in downstream fine-tuning tasks. Experiments on image-textand video-textretrieval benchmarks show that our unified loss can significantly improve the performance of the state-of-the-art retrieval models.","['Zheng Li', 'Caili Guo', 'Xin Wang', 'Zerun Feng', 'Jenq-Neng Hwang', 'Zhongtian Du']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.13869,Anomali
Text-Adaptive Multiple Visual Prototype Matching for Video-Text Retrieval,"Cross-modal retrieval between videos andtextshas gained increasing research interest due to the rapid emergence of videos on the web. Generally, a video contains rich instance and event information and the querytextonly describes a part of the information. Thus, a video can correspond to multiple differenttextdescriptions and queries. We call this phenomenon the ``Video-TextCorrespondence Ambiguity'' problem. Current techniques mostly concentrate onmininglocal or multi-level alignment between contents of a video andtext(\textit{e.g.}, object to entity and action to verb). It is difficult for these methods to alleviate the video-textcorrespondence ambiguity by describing a video using only one single feature, which is required to be matched with multiple differenttextfeatures at the same time. To address this problem, we propose aText-Adaptive Multiple Visual Prototype Matching model, which automatically captures multiple prototypes to describe a video by adaptive aggregation of video token features. Given a querytext, the similarity is determined by the most similar prototype to find correspondence in the video, which is termedtext-adaptive matching. To learn diverse prototypes for representing the rich information in videos, we propose a variance loss to encourage different prototypes to attend to different contents of the video. Our method outperforms state-of-the-art methods on four public video retrieval datasets.","['Chengzhi Lin', 'Ancong Wu', 'Junwei Liang', 'Jun Zhang', 'Wenhang Ge', 'Wei-Shi Zheng', 'Chunhua Shen']",NIPS2022,arXiv,2022,https://doi.org/10.48550/arXiv.2209.13307,Anomali
SMTCE: A Social Media Text Classification Evaluation Benchmark and BERTology Models for Vietnamese,"Textclassification is a typical natural language processing or computational linguistics task with various interesting applications. As the number of users on social media platforms increases, data acceleration promotes emerging studies on Social MediaTextClassification (SMTC) or social mediatextminingon these valuable resources. In contrast to English, Vietnamese, one of the low-resource languages, is still not concentrated on and exploited thoroughly. Inspired by the success of the GLUE, we introduce the Social MediaTextClassification Evaluation (SMTCE) benchmark, as a collection of datasets and models across a diverse set of SMTC tasks. With the proposed benchmark, we implement and analyze the effectiveness of a variety of multilingual BERT-based models (mBERT, XLM-R, and DistilmBERT) and monolingual BERT-based models (PhoBERT, viBERT, vELECTRA, and viBERT4news) for tasks in the SMTCE benchmark. Monolingual models outperform multilingual models and achieve state-of-the-art results on alltextclassification tasks. It provides an objective assessment of multilingual and monolingual BERT-based models on the benchmark, which will benefit future studies about BERTology in the Vietnamese language.","['Luan Thanh Nguyen', 'Kiet Van Nguyen', 'Ngan Luu-Thuy Nguyen']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.10482,Anomali
The language and social behavior of innovators,"Innovators are creative people who can conjure the ground-breaking ideas that represent the main engine of innovative organizations. Past research has extensively investigated who innovators are and how they behave in work-related activities. In this paper, we suggest that it is necessary to analyze how innovators behave in other contexts, such as in informal communication spaces, where knowledge is shared without formal structure, rules, and work obligations. Drawing on communication and network theory, we analyze about 38,000 posts available in the intranet forum of a large multinational company. From this, we explain how innovators differ from other employees in terms of social network behavior and language characteristics. Throughtextmining, we find that innovators write more, use a more complex language, introduce new concepts/ideas, and use positive but factual-based language. Understanding how innovators behave and communicate can support the decision-making processes of managers who want to foster innovation.","['A. Fronzetti Colladon', 'L. Toschi', 'E. Ughetto', 'F. Greco']","Journal of Business Research 154, 113317 (2023)",arXiv,2022,https://doi.org/10.48550/arXiv.2209.09511,Anomali
HAPI: A Large-scale Longitudinal Dataset of Commercial ML API Predictions,"Commercial ML APIs offered by providers such as Google, Amazon and Microsoft have dramatically simplified ML adoption in many applications. Numerous companies and academics pay to use ML APIs for tasks such as object detection, OCR and sentiment analysis. Different ML APIs tackling the same task can have very heterogeneous performance. Moreover, the ML models underlying the APIs also evolve over time. As ML APIs rapidly become a valuable marketplace and a widespread way to consume machine learning, it is critical to systematically study and compare different APIs with each other and to characterize how APIs change over time. However, this topic is currently underexplored due to the lack of data. In this paper, we present HAPI (History of APIs), a longitudinal dataset of 1,761,417 instances of commercial ML API applications (involving APIs from Amazon, Google, IBM, Microsoft and other providers) across diverse tasks including image tagging, speech recognition andtextminingfrom 2020 to 2022. Each instance consists of a query input for an API (e.g., an image ortext) along with the API's output prediction/annotation and confidence scores. HAPI is the first large-scale dataset of ML API usages and is a unique resource for studying ML-as-a-service (MLaaS). As examples of the types of analyses that HAPI enables, we show that ML APIs' performance change substantially over time--several APIs' accuracies dropped on specific benchmark datasets. Even when the API's aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. Such changes can substantially impact the entire analytics pipelines that use some ML API as a component. We further use HAPI to study commercial APIs' performance disparities across demographic subgroups over time. HAPI can stimulate more research in the growing field of MLaaS.","['Lingjiao Chen', 'Zhihua Jin', 'Sabri Eyuboglu', 'Christopher Ré', 'Matei Zaharia', 'James Zou']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.08443,Anomali
ChemNLP: A Natural Language Processing based Library for Materials Chemistry Text Data,"In this work, we present the ChemNLP library that can be used for 1) curating open access datasets for materials and chemistry literature, developing and comparing traditional machine learning, transformers and graph neural network models for 2) classifying and clusteringtexts, 3) named entity recognition for large-scaletext-mining, 4) abstractive summarization for generating titles of articles from abstracts, 5)textgeneration for suggesting abstracts from titles, 6) integration with density functional theory dataset for identifying potential candidate materials such as superconductors, and 7) web-interface development fortextand reference query. We primarily use the publicly available arXiv and Pubchem datasets but the tools can be used for other datasets as well. Moreover, as new models are developed, they can be easily integrated in the library. ChemNLP is available at the websites: https://github.com/usnistgov/chemnlp and https://jarvis.nist.gov/jarvischemnlp.","['Kamal Choudhary', 'Mathew L. Kelley']",,arXiv,2023,https://doi.org/10.48550/arXiv.2209.08203,Anomali
Entity-based Claim Representation Improves Fact-Checking of Medical Content in Tweets,"False medical information on social media poses harm to people's health. While the need for biomedical fact-checking has been recognized in recent years, user-generated medical content has received comparably little attention. At the same time, models for othertextgenres might not be reusable, because the claims they have been trained with are substantially different. For instance, claims in the SciFact dataset are short and focused: ""Side effects associated with antidepressants increases risk of stroke"". In contrast, social media holds naturally-occurring claims, often embedded in additional context: ""`If you take antidepressants like SSRIs, you could be at risk of a condition called serotonin syndrome' Serotonin syndrome nearly killed me in 2010. Had symptoms of stroke and seizure."" This showcases the mismatch between real-world medical claims and the input that existing fact-checking systems expect. To make user-generated content checkable by existing models, we propose to reformulate the social-media input in such a way that the resulting claim mimics the claim characteristics in established datasets. To accomplish this, our method condenses the claim with the help of relational entity information and either compiles the claim out of an entity-relation-entity triple or extracts the shortest phrase that contains these elements. We show that the reformulated input improves the performance of various fact-checking models as opposed to checking the tweettextin its entirety.","['Amelie Wührl', 'Roman Klinger']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.07834,Anomali
Model Inversion Attacks against Graph Neural Networks,"Many dataminingtasks rely on graphs to model relational structures among individuals (nodes). Since relational data are often sensitive, there is an urgent need to evaluate the privacy risks in graph data. One famous privacy attack against data analysis models is the model inversion attack, which aims to infer sensitive data in the training dataset and leads to great privacy concerns. Despite its success in grid-like domains, directly applying model inversion attacks on non-grid domains such as graph leads to poor attack performance. This is mainly due to the failure to consider the unique properties of graphs. To bridge this gap, we conduct a systematic study on model inversion attacks against Graph Neural Networks (GNNs), one of the state-of-the-art graph analysis tools in this paper. Firstly, in the white-box setting where the attacker has full access to the target GNN model, we present GraphMI to infer the private training graph data. Specifically, in GraphMI, a projected gradient module is proposed to tackle the discreteness of graph edges and preserve the sparsity and smoothness of graph features; a graph auto-encoder module is used to efficiently exploit graph topology, node attributes, and target model parameters for edge inference; a random sampling module can finally sample discrete edges. Furthermore, in the hard-label black-box setting where the attacker can only query the GNN API and receive the classification results, we propose two methods based on gradient estimation and reinforcement learning (RL-GraphMI). Our experimental results show that such defenses are not sufficiently effective and call for more advanced defenses against privacy attacks.","['Zaixi Zhang', 'Qi Liu', 'Zhenya Huang', 'Hao Wang', 'Chee-Kong Lee', 'Enhong Chen']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.07807,Anomali
On the State of the Art in Authorship Attribution and Authorship Verification,"Despite decades of research on authorship attribution (AA) and authorship verification (AV), inconsistent dataset splits/filtering and mismatched evaluation methods make it difficult to assess the state of the art. In this paper, we present a survey of the fields, resolve points of confusion, introduce Valla that standardizes and benchmarks AA/AV datasets and metrics, provide a large-scale empirical evaluation, and provide apples-to-apples comparisons between existing methods. We evaluate eight promising methods on fifteen datasets (including distribution-shifted challenge sets) and introduce a new large-scale dataset based ontextsarchived by Project Gutenberg. Surprisingly, we find that a traditional Ngram-based model performs best on 5 (of 7) AA tasks, achieving an average macro-accuracy of $76.50\%$ (compared to $66.71\%$ for a BERT-based model). However, on the two AA datasets with the greatest number of words per author, as well as on the AV datasets, BERT-based models perform best. While AV methods are easily applied to AA, they are seldom included as baselines in AA papers. We show that through the application of hard-negativemining, AV methods are competitive alternatives to AA methods. Valla and all experiment code can be found here: https://github.com/JacobTyo/Valla","['Jacob Tyo', 'Bhuwan Dhingra', 'Zachary C. Lipton']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.06869,Anomali
ImageArg: A Multi-modal Tweet Dataset for Image Persuasiveness Mining,"The growing interest in developing corpora of persuasivetextshas promoted applications in automated systems, e.g., debating and essay scoring systems; however, there is little prior workminingimage persuasiveness from an argumentative perspective. To expand persuasivenessmininginto a multi-modal realm, we present a multi-modal dataset, ImageArg, consisting of annotations of image persuasiveness in tweets. The annotations are based on a persuasion taxonomy we developed to explore image functionalities and the means of persuasion. We benchmark image persuasiveness tasks on ImageArg using widely-used multi-modal learning methods. The experimental results show that our dataset offers a useful resource for this rich and challenging topic, and there is ample room for modeling improvement.","['Zhexiong Liu', 'Meiqi Guo', 'Yue Dai', 'Diane Litman']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.06416,Anomali
Pitfalls and Guidelines for Using Time-Based Git Data,"Many software engineering research papers rely on time-based data (e.g., commit timestamps, issue report creation/update/close dates, release dates). Like most real-world data however, time-based data is often dirty. To date, there are no studies that quantify how frequently such data is used by the software engineering research community, or investigate sources of and quantify how often such data is dirty. Depending on the research task and method used, including such dirty data could affect the research results. This paper presents an extended survey of papers that utilize time-based data, published in theMiningSoftware Repositories (MSR) conference series. Out of the 754 technical track and data papers published in MSR 2004--2021, we saw at least 290 (38%) papers utilized time-based data. We also observed that most time-based data used in research papers comes in the form of Git commits, often from GitHub. Based on those results, we then used the Boa and Software Heritage infrastructures to help identify and quantify several sources of dirty Git timestamp data. Finally we provide guidelines/best practices for researchers utilizing time-based data from Git repositories.","['Samuel W. Flint', 'Jigyasa Chauhan', 'Robert Dyer']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.04511,Anomali
MIntRec: A New Dataset for Multimodal Intent Recognition,"Multimodal intent recognition is a significant task for understanding human language in real-world multimodal scenes. Most existing intent recognition methods have limitations in leveraging the multimodal information due to the restrictions of the benchmark datasets with onlytextinformation. This paper introduces a novel dataset for multimodal intent recognition (MIntRec) to address this issue. It formulates coarse-grained and fine-grained intent taxonomies based on the data collected from the TV series Superstore. The dataset consists of 2,224 high-quality samples withtext, video, and audio modalities and has multimodal annotations among twenty intent categories. Furthermore, we provide annotated bounding boxes of speakers in each video segment and achieve an automatic process for speaker annotation. MIntRec is helpful for researchers tominerelationships between different modalities to enhance the capability of intent recognition. We extract features from each modality and model cross-modal interactions by adapting three powerful multimodal fusion methods to build baselines. Extensive experiments show that employing the non-verbal modalities achieves substantial improvements compared with thetext-only modality, demonstrating the effectiveness of using multimodal information for intent recognition. The gap between the best-performing methods and humans indicates the challenge and importance of this task for the community. The full dataset and codes are available for use at https://github.com/thuiar/MIntRec.","['Hanlei Zhang', 'Hua Xu', 'Xin Wang', 'Qianrui Zhou', 'Shaojie Zhao', 'Jiayan Teng']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.04355,Anomali
5q032e@SMM4H'22: Transformer-based classification of premise in tweets related to COVID-19,"Automation of social network data assessment is one of the classic challenges of natural language processing. During the COVID-19 pandemic,miningpeople's stances from public messages have become crucial regarding understanding attitudes towards health orders. In this paper, the authors propose the predictive model based on transformer architecture to classify the presence of premise in Twittertexts. This work is completed as part of the Social MediaMiningfor Health (SMM4H) Workshop 2022. We explored modern transformer-based classifiers in order to construct the pipeline efficiently capturing tweets semantics. Our experiments on a Twitter dataset showed that RoBERTa is superior to the other transformer models in the case of the premise prediction task. The model achieved competitive performance with respect to ROC AUC value 0.807, and 0.7648 for the F1 score.","['Vadim Porvatov', 'Natalia Semenova']","Mining for Health Applications, Workshop & Shared Task (SMM4H 2022) (p. 108)",arXiv,2023,https://doi.org/10.48550/arXiv.2209.03851,Anomali
Language-aware Domain Generalization Network for Cross-Scene Hyperspectral Image Classification,"Textinformation including extensive prior knowledge about land cover classes has been ignored in hyperspectral image classification (HSI) tasks. It is necessary to explore the effectiveness of linguistic mode in assisting HSI classification. In addition, the large-scale pre-training image-textfoundation models have demonstrated great performance in a variety of downstream applications, including zero-shot transfer. However, most domain generalization methods have never addressedmininglinguistic modal knowledge to improve the generalization performance of model. To compensate for the inadequacies listed above, a Language-aware Domain Generalization Network (LDGnet) is proposed to learn cross-domain invariant representation from cross-domain shared prior knowledge. The proposed method only trains on the source domain (SD) and then transfers the model to the target domain (TD). The dual-stream architecture including image encoder andtextencoder is used to extract visual and linguistic features, in which coarse-grained and fine-grainedtextrepresentations are designed to extract two levels of linguistic features. Furthermore, linguistic features are used as cross-domain shared semantic space, and visual-linguistic alignment is completed by supervised contrastive learning in semantic space. Extensive experiments on three datasets demonstrate the superiority of the proposed method when compared with state-of-the-art techniques.","['Yuxiang Zhang', 'Mengmeng Zhang', 'Wei Li', 'Shuai Wang', 'Ran Tao']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.02700,Anomali
Spatiotemporal statistics of the turbulent piston-removed phase and Zernike coefficients for two distinct beams,"In the context of adaptive optics for astronomy, one can rely on the statistics of the turbulent phase to assess a part of the system's performance. Temporal statistics with one source and spatial statistics with two sources are well-known and are widely used for classical adaptive optics systems. A more general framework, including both spatial and temporal statistics, can be useful for the analysis of the existing systems and to support the design of the future ones. In this paper, we propose an expression of the temporal cross power spectral densities of the turbulent phases in two distinct beams, that is from two different sources to two different apertures. We either consider the phase as it is, without piston, or as its decomposition on Zernike modes. The general formulas allow to cover a wide variety of configurations, from single-aperture to interferometric telescopes equipped with adaptive optics, with the possibility to consider apertures of different sizes and/or sources at a finite distance. The presented approach should lead to similar results with respect to existing methods in the Fourier domain, but it is focused on temporal frequencies rather than spatial ones, which might be convenient for some aspects such as control optimization. To illustrate this framework with a simple application, we demonstrate that the wavefront residual due to the anisoplanatism error in a single-conjugated adaptive optics system is overestimated when it is computed from covariances without taking into account the temporal filtering of the adaptive optics loop. We also show this overestimation in the case of a small-baseline interferometer, for which the two beams are significantly correlated.","['Cédric Plantet', 'Giulia Carlà', 'Guido Agapito', 'Lorenzo Busoni']","J. Opt. Soc. Am. A 39, 17-27 (2022)",arXiv,2022,https://doi.org/10.48550/arXiv.2209.00931,Anomali
Structure-Preserving Graph Representation Learning,"Though graph representation learning (GRL) has made significant progress, it is still a challenge to extract and embed the rich topological structure and feature information in an adequate way. Most existing methods focus on local structure and fail to fully incorporate the global topological structure. To this end, we propose a novel Structure-Preserving Graph Representation Learning (SPGRL) method, to fully capture the structure information of graphs. Specifically, to reduce the uncertainty and misinformation of the original graph, we construct a feature graph as a complementary view via k-Nearest Neighbor method. The feature graph can be used to contrast at node-level to capture the local relation. Besides, we retain the global topological structure information by maximizing the mutual information (MI) of the whole graph and feature embeddings, which is theoretically reduced to exchanging the feature embeddings of the feature and the original graphs to reconstruct themselves. Extensive experiments show that our method has quite superior performance on semi-supervised node classification task and excellent robustness under noise perturbation on graph structure or node features.","['Ruiyi Fang', 'Liangjian Wen', 'Zhao Kang', 'Jianzhuang Liu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2209.00793,Anomali
Compound Figure Separation of Biomedical Images: Mining Large Datasets for Self-supervised Learning,"With the rapid development of self-supervised learning (e.g., contrastive learning), the importance of having large-scale images (even without annotations) for training a more generalizable AI model has been widely recognized in medical image analysis. However, collecting large-scale task-specific unannotated data at scale can be challenging for individual labs. Existing online resources, such as digital books, publications, and search engines, provide a new resource for obtaining large-scale images. However, published images in healthcare (e.g., radiology and pathology) consist of a considerable amount of compound figures with subplots. In order to extract and separate compound figures into usable individual images for downstream learning, we propose a simple compound figure separation (SimCFS) framework without using the traditionally required detection bounding box annotations, with a new loss function and a hard case simulation. Our technical contribution is four-fold: (1) we introduce a simulation-based training framework that minimizes the need for resource extensive bounding box annotations; (2) we propose a new side loss that is optimized for compound figure separation; (3) we propose an intra-class image augmentation method to simulate hard cases; and (4) to the best of our knowledge, this is the first study that evaluates the efficacy of leveraging self-supervised learning with compound image separation. From the results, the proposed SimCFS achieved state-of-the-art performance on the ImageCLEF 2016 Compound Figure Separation Database. The pretrained self-supervised learning model using large-scaleminedfigures improved the accuracy of downstream image classification tasks with a contrastive learning algorithm. The source code of SimCFS is made publicly available at https://github.com/hrlblab/ImageSeperation.","['Tianyuan Yao', 'Chang Qu', 'Jun Long', 'Quan Liu', 'Ruining Deng', 'Yuanhan Tian', 'Jiachen Xu', 'Aadarsh Jha', 'Zuhayr Asad', 'Shunxing Bao', 'Mengyang Zhao', 'Agnes B. Fogo', 'Bennett A. Landman', 'Haichun Yang', 'Catie Chang', 'Yuankai Huo']",Machine.Learning.for.Biomedical.Imaging. 1 (2022),arXiv,2022,https://doi.org/10.48550/arXiv.2208.14357,Anomali
"Multi-dimensional Racism Classification during COVID-19: Stigmatization, Offensiveness, Blame, and Exclusion","Transcending the binary categorization of racisttexts, our study takes cues from social science theories to develop a multi-dimensional model for racism detection, namely stigmatization, offensiveness, blame, and exclusion. With the aid of BERT and topic modeling, this categorical detection enables insights into the underlying subtlety of racist discussion on digital platforms during COVID-19. Our study contributes to enriching the scholarly discussion on deviant racist behaviours on social media. First, a stage-wise analysis is applied to capture the dynamics of the topic changes across the early stages of COVID-19 which transformed from a domestic epidemic to an international public health emergency and later to a global pandemic. Furthermore, mapping this trend enables a more accurate prediction of public opinion evolvement concerning racism in the offline world, and meanwhile, the enactment of specified intervention strategies to combat the upsurge of racism during the global public health crisis like COVID-19. In addition, this interdisciplinary research also points out a direction for future studies on social network analysis andmining. Integration of social science perspectives into the development of computational methods provides insights into more accurate data detection and analytics.","['Xin Pei', 'Deval Mehta']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.13318,Anomali
Effectiveness of Mining Audio and Text Pairs from Public Data for Improving ASR Systems for Low-Resource Languages,"End-to-end (E2E) models have become the default choice for state-of-the-art speech recognition systems. Such models are trained on large amounts of labelled data, which are often not available for low-resource languages. Techniques such as self-supervised learning and transfer learning hold promise, but have not yet been effective in training accurate models. On the other hand, collecting labelled datasets on a diverse set of domains and speakers is very expensive. In this work, we demonstrate an inexpensive and effective alternative to these approaches by ``mining''textand audio pairs for Indian languages from public sources, specifically from the public archives of All India Radio. As a key component, we adapt the Needleman-Wunsch algorithm to align sentences with corresponding audio segments given a long audio and a PDF of its transcript, while being robust to errors due to OCR, extraneoustext, and non-transcribed speech. We thus create Shrutilipi, a dataset which contains over 6,400 hours of labelled audio across 12 Indian languages totalling to 4.95M sentences. On average, Shrutilipi results in a 2.3x increase over publicly available labelled data. We establish the quality of Shrutilipi with 21 human evaluators across the 12 languages. We also establish the diversity of Shrutilipi in terms of represented regions, speakers, and mentioned named entities. Significantly, we show that adding Shrutilipi to the training set of Wav2Vec models leads to an average decrease in WER of 5.8\% for 7 languages on the IndicSUPERB benchmark. For Hindi, which has the most benchmarks (7), the average WER falls from 18.8% to 13.5%. This improvement extends to efficient models: We show a 2.3% drop in WER for a Conformer model (10x smaller than Wav2Vec). Finally, we demonstrate the diversity of Shrutilipi by showing that the model trained with it is more robust to noisy input.","['Kaushal Santosh Bhogale', 'Abhigyan Raman', 'Tahir Javed', 'Sumanth Doddapaneni', 'Anoop Kunchukuttan', 'Pratyush Kumar', 'Mitesh M. Khapra']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.12666,Anomali
Repairing Activity Start Times to Improve Business Process Simulation,"Business Process Simulation (BPS) is a common technique to estimate the impact of business process changes, e.g. what would be the cycle time of a process if the number of traces increases? The starting point of BPS is a business process model annotated with simulation parameters (a BPS model). Several studies have proposed methods to automatically discover BPS models from event logs -- extracted from enterprise information systems -- via processminingtechniques. These approaches model the processing time of each activity based on the start and end timestamps recorded in the event log. In practice, however, it is common that the recorded start times do not precisely reflect the actual start of the activities. For example, a resource starts working on an activity, but its start time is not recorded until she/he interacts with the system. If not corrected, these situations induce waiting times in which the resource is considered to be free, while she/he is actually working. To address this limitation, this article proposes a technique to identify the waiting time previous to each activity instance in which the resource is actually working on them, and repair their start time so that they reflect the actual processing time. The idea of the proposed technique is that, as far as simulation is concerned, an activity instance may start once it is enabled and the corresponding resource is available. Accordingly, for each activity instance, the proposed technique estimates the activity enablement and the resource availability time based on the information available in the event log, and repairs the start time to include the non-recorded processing time. An empirical evaluation involving eight real-life event logs shows that the proposed approach leads to BPS models that closely reflect the temporal dynamics of the process.","['David Chapela-Campa', 'Marlon Dumas']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.12224,Anomali
Learning Branched Fusion and Orthogonal Projection for Face-Voice Association,"Recent years have seen an increased interest in establishing association between faces and voices of celebrities leveraging audio-visual information from YouTube. Prior works adopt metric learning methods to learn an embedding space that is amenable for associated matching and verification tasks. Albeit showing some progress, such formulations are, however, restrictive due to dependency on distance-dependent margin parameter, poor run-time training complexity, and reliance on carefully crafted negativeminingprocedures. In this work, we hypothesize that an enriched representation coupled with an effective yet efficient supervision is important towards realizing a discriminative joint embedding space for face-voice association tasks. To this end, we propose a light-weight, plug-and-play mechanism that exploits the complementary cues in both modalities to form enriched fused embeddings and clusters them based on their identity labels via orthogonality constraints. We coin our proposed mechanism as fusion and orthogonal projection (FOP) and instantiate in a two-stream network. The overall resulting framework is evaluated on VoxCeleb1 and MAV-Celeb datasets with a multitude of tasks, including cross-modal verification and matching. Results reveal that our method performs favourably against the current state-of-the-art methods and our proposed formulation of supervision is more effective and efficient than the ones employed by the contemporary methods. In addition, we leverage cross-modal verification and matching tasks to analyze the impact of multiple languages on face-voice association. Code is available: \url{https://github.com/msaadsaeed/FOP}","['Muhammad Saad Saeed', 'Shah Nawaz', 'Muhammad Haris Khan', 'Sajid Javed', 'Muhammad Haroon Yousaf', 'Alessio Del Bue']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.10238,Anomali
Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes,"Semantic textual similarity (STS) in the clinical domain helps improve diagnostic efficiency and produce concisetextsfor downstream dataminingtasks. However, given the high degree of domain knowledge involved in clinictext, it remains challenging for general language models to infer implicit medical relationships behind clinical sentences and output similarities correctly. In this paper, we present a graph-augmented cyclic learning framework for similarity estimation in the clinical domain. The framework can be conveniently implemented on a state-of-art backbone language model, and improve its performance by leveraging domain knowledge through co-training with an auxiliary graph convolution network (GCN) based network. We report the success of introducing domain knowledge in GCN and the co-training framework by improving the Bio-clinical BERT baseline by 16.3% and 27.9%, respectively.","['Can Zheng', 'Yanshan Wang', 'Xiaowei Jia']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.09437,Anomali
End-to-end Clinical Event Extraction from Chinese Electronic Health Record,"Event extraction is an important work of medicaltextprocessing. According to the complex characteristics of medicaltextannotation, we use the end-to-end event extraction model to enhance the output formatting information of events. Through pre training and fine-tuning, we can extract the attributes of the four dimensions of medicaltext: anatomical position, subject word, description word and occurrence state. On the test set, the accuracy rate was 0.4511, the recall rate was 0.3928, and the F1 value was 0.42. The method of this model is simple, and it has won the second place in the task ofminingclinical discovery events (task2) in the Chinese electronic medical record of the seventh China health information processing Conference (chip2021).","['Wei Feng', 'Ruochen Huang', 'Yun Yu', 'Huiting Sun', 'Yun Liu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.09354,Anomali
UniCausal: Unified Benchmark and Repository for Causal Text Mining,"Current causaltextminingdatasets vary in objectives, data coverage, and annotation schemes. These inconsistent efforts prevent modeling capabilities and fair comparisons of model performance. Furthermore, few datasets include cause-effect span annotations, which are needed for end-to-end causal relation extraction. To address these issues, we propose UniCausal, a unified benchmark for causaltextminingacross three tasks: (I) Causal Sequence Classification, (II) Cause-Effect Span Detection and (III) Causal Pair Classification. We consolidated and aligned annotations of six high quality, mainly human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165 examples for each task respectively. Since the definition of causality can be subjective, our framework was designed to allow researchers to work on some or all datasets and tasks. To create an initial benchmark, we fine-tuned BERT pre-trained language models to each task, achieving 70.10% Binary F1, 52.42% Macro F1, and 84.68% Binary F1 scores respectively.","['Fiona Anting Tan', 'Xinyu Zuo', 'See-Kiong Ng']",,arXiv,2023,https://doi.org/10.48550/arXiv.2208.09163,Anomali
"See Finer, See More: Implicit Modality Alignment for Text-based Person Retrieval","Text-based person retrieval aims to find the query person based on a textual description. The key is to learn a common latent space mapping between visual-textual modalities. To achieve this goal, existing works employ segmentation to obtain explicitly cross-modal alignments or utilize attention to explore salient alignments. These methods have two shortcomings: 1) Labeling cross-modal alignments are time-consuming. 2) Attention methods can explore salient cross-modal alignments but may ignore some subtle and valuable pairs. To relieve these issues, we introduce an Implicit Visual-Textual (IVT) framework fortext-based person retrieval. Different from previous models, IVT utilizes a single network to learn representation for both modalities, which contributes to the visual-textual interaction. To explore the fine-grained alignment, we further propose two implicit semantic alignment paradigms: multi-level alignment (MLA) and bidirectional mask modeling (BMM). The MLA module explores finer matching at sentence, phrase, and word levels, while the BMM module aims tomine\textbf{more} semantic alignments between visual and textual modalities. Extensive experiments are carried out to evaluate the proposed IVT on public datasets, i.e., CUHK-PEDES, RSTPReID, and ICFG-PEDES. Even without explicit body part alignment, our approach still achieves state-of-the-art performance. Code is available at: https://github.com/TencentYoutuResearch/PersonRetrieval-IVT.","['Xiujun Shu', 'Wei Wen', 'Haoqian Wu', 'Keyu Chen', 'Yiran Song', 'Ruizhi Qiao', 'Bo Ren', 'Xiao Wang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.08608,Anomali
Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology,"Oversight AI is an emerging concept in radiology where the AI forms a symbiosis with radiologists by continuously supporting radiologists in their decision-making. Recent advances in vision-language models sheds a light on the long-standing problems of the oversight AI by the understanding both visual and textual concepts and their semantic correspondences. However, there have been limited successes in the application of vision-language models in the medical domain, as the current vision-language models and learning strategies for photographic images and captions call for the web-scale data corpus of image andtextpairs which was not often feasible in the medical domain. To address this, here we present a model dubbed Medical Cross-attention Vision-Language model (Medical X-VL), leveraging the key components to be tailored for the medical domain. Our medical X-VL model is based on the following components: self-supervised uni-modal models in medical domain and fusion encoder to bridge them, momentum distillation, sentence-wise contrastive learning for medical reports, and the sentence similarity-adjusted hard negativemining. We experimentally demonstrated that our model enables various zero-shot tasks for oversight AI, ranging from the zero-shot classification to zero-shot error correction. Our model outperformed the current state-of-the-art models in two different medical image database, suggesting the novel clinical usage of our oversight AI model for monitoring human errors. Our method was especially successful in the data-limited setting, which is frequently encountered in the clinics, suggesting the potential widespread applicability in medical domain.","['Sangjoon Park', 'Eun Sun Lee', 'Kyung Sook Shin', 'Jeong Eun Lee', 'Jong Chul Ye']",,arXiv,2023,https://doi.org/10.48550/arXiv.2208.05140,Anomali
E2EG: End-to-End Node Classification Using Graph Topology and Text-based Node Attributes,"Node classification utilizingtext-based node attributes has many real-world applications, ranging from prediction of paper topics in academic citation graphs to classification of user characteristics in social media networks. State-of-the-art node classification frameworks, such as GIANT, use a two-stage pipeline: first embedding thetextattributes of graph nodes then feeding the resulting embeddings into a node classification model. In this paper, we eliminate these two stages and develop an end-to-end node classification model that builds upon GIANT, called End-to-End-GIANT (E2EG). The tandem utilization of a main and an auxiliary classification objectives in our approach results in a more robust model, enabling the BERT backbone to be switched out for a distilled encoder with a 25% - 40% reduction in the number of parameters. Moreover, the model's end-to-end nature increases ease of use, as it avoids the need of chaining multiple models for node classification. Compared to a GIANT+MLP baseline on the ogbn-arxiv and ogbn-products datasets, E2EG obtains slightly better accuracy in the transductive setting (+0.5%), while reducing model training time by up to 40%. Our model is also applicable in the inductive setting, outperforming GIANT+MLP by up to +2.23%.","['Tu Anh Dinh', 'Jeroen den Boef', 'Joran Cornelisse', 'Paul Groth']",,arXiv,2023,https://doi.org/10.48550/arXiv.2208.04609,Anomali
GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training,"Most of the currently existing vision and language pre-training (VLP) methods have mainly focused on how to extract and align vision andtextfeatures. In contrast to the mainstream VLP methods, we highlight that two routinely applied steps during pre-training have crucial impact on the performance of the pre-trained model: in-batch hard negative sampling for image-textmatching (ITM) and assigning the large masking probability for the masked language modeling (MLM). After empirically showing the unexpected effectiveness of above two steps, we systematically devise our GRIT-VLP, which adaptively samples mini-batches for more effectiveminingof hard negative samples for ITM while maintaining the computational cost for pre-training. Our method consists of three components: 1) GRouped mIni-baTch sampling (GRIT) strategy that collects similar examples in a mini-batch, 2) ITC consistency loss for improving theminingability, and 3) enlarged masking probability for MLM. Consequently, we show our GRIT-VLP achieves a new state-of-the-art performance on various downstream tasks with much less computational cost. Furthermore, we demonstrate that our model is essentially in par with ALBEF, the previous state-of-the-art, only with one-third of training epochs on the same training data. Code is available at https://github.com/jaeseokbyun/GRIT-VLP.","['Jaeseok Byun', 'Taebaek Hwang', 'Jianlong Fu', 'Taesup Moon']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.04060,Anomali
CSSAM:Code Search via Attention Matching of Code Semantics and Structures,"Despite the continuous efforts in improving both the effectiveness and efficiency of code search, two issues remained unsolved. First, programming languages have inherent strong structural linkages, and featureminingof code astextform would omit the structural information contained inside it. Second, there is a potential semantic relationship between code and query, it is challenging to align code andtextacross sequences so that vectors are spatially consistent during similarity matching. To tackle both issues, in this paper, a code search model named CSSAM (Code Semantics and Structures Attention Matching) is proposed. By introducing semantic and structural matching mechanisms, CSSAM effectively extracts and fuses multidimensional code features. Specifically, the cross and residual layer was developed to facilitate high-latitude spatial alignment of code and query at the token level. By leveraging the residual interaction, a matching module is designed to preserve more code semantics and descriptive features, that enhances the adhesion between the code and its corresponding querytext. Besides, to improve the model's comprehension of the code's inherent structure, a code representation structure named CSRG (Code Semantic Representation Graph) is proposed for jointly representing abstract syntax tree nodes and the data flow of the codes. According to the experimental results on two publicly available datasets containing 540k and 330k code segments, CSSAM significantly outperforms the baselines in terms of achieving the highest SR@1/5/10, MRR, and NDCG@50 on both datasets respectively. Moreover, the ablation study is conducted to quantitatively measure the impact of each key component of CSSAM on the efficiency and effectiveness of code search, which offers the insights into the improvement of advanced code search solutions.","['Yi Hu', 'Bo Cai', 'Yaoxiang Yu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.03922,Anomali
"Hybrid Multimodal Feature Extraction, Mining and Fusion for Sentiment Analysis","In this paper, we present our solutions for the Multimodal Sentiment Analysis Challenge (MuSe) 2022, which includes MuSe-Humor, MuSe-Reaction and MuSe-Stress Sub-challenges. The MuSe 2022 focuses on humor detection, emotional reactions and multimodal emotional stress utilizing different modalities and data sets. In our work, different kinds of multimodal features are extracted, including acoustic, visual,textand biological features. These features are fused by TEMMA and GRU with self-attention mechanism frameworks. In this paper, 1) several new audio features, facial expression features and paragraph-leveltextembeddings are extracted for accuracy improvement. 2) we substantially improve the accuracy and reliability of multimodal sentiment prediction byminingand blending the multimodal features. 3) effective data augmentation strategies are applied in model training to alleviate the problem of sample imbalance and prevent the model from learning biased subject characters. For the MuSe-Humor sub-challenge, our model obtains the AUC score of 0.8932. For the MuSe-Reaction sub-challenge, the Pearson's Correlations Coefficient of our approach on the test set is 0.3879, which outperforms all other participants. For the MuSe-Stress sub-challenge, our approach outperforms the baseline in both arousal and valence on the test dataset, reaching a final combined result of 0.5151.","['Jia Li', 'Ziyang Zhang', 'Junjie Lang', 'Yueqi Jiang', 'Liuwei An', 'Peng Zou', 'Yangyang Xu', 'Sheng Gao', 'Jie Lin', 'Chunxiao Fan', 'Xiao Sun', 'Meng Wang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.03051,Anomali
Twitter Attribute Classification with Q-Learning on Bitcoin Price Prediction,"Aspiring to achieve an accurate Bitcoin price prediction based on people's opinions on Twitter usually requires millions of tweets, using differenttextminingtechniques (preprocessing, tokenization, stemming, stop word removal), and developing a machine learning model to perform the prediction. These attempts lead to the employment of a significant amount of computer power, central processing unit (CPU) utilization, random-access memory (RAM) usage, and time. To address this issue, in this paper, we consider a classification of tweet attributes that effects on price changes and computer resource usage levels while obtaining an accurate price prediction. To classify tweet attributes having a high effect on price movement, we collect all Bitcoin-related tweets posted in a certain period and divide them into four categories based on the following tweet attributes: $(i)$ the number of followers of the tweet poster, $(ii)$ the number of comments on the tweet, $(iii)$ the number of likes, and $(iv)$ the number of retweets. We separately train and test by using the Q-learning model with the above four categorized sets of tweets and find the best accurate prediction among them. Especially, we design several reward functions to improve the prediction accuracy of the Q-leaning. We compare our approach with a classic approach where all Bitcoin-related tweets are used as input data for the model, by analyzing the CPU workloads, RAM usage, memory, time, and prediction accuracy. The results show that tweets posted by users with the most followers have the most influence on a future price, and their utilization leads to spending 80\% less time, 88.8\% less CPU consumption, and 12.5\% more accurate predictions compared with the classic approach.","['Sattarov Otabek', 'Jaeyoung Choi']",,arXiv,2023,https://doi.org/10.48550/arXiv.2208.02610,Anomali
Evaluating and improving social awareness of energy communities through semantic network analysis of online news,"The implementation of energy communities represents a cross-disciplinary phenomenon that has the potential to support the energy transition while fostering citizens' participation throughout the energy system and their exploitation of renewables. An important role is played by online information sources in engaging people in this process and increasing their awareness of associated benefits. In this view, this work analyses online news data on energy communities to understand people's awareness and the media importance of this topic. We use the Semantic Brand Score (SBS) indicator as an innovative measure of semantic importance, combining social network analysis andtextminingmethods. Results show different importance trends for energy communities and other energy and society-related topics, also allowing the identification of their connections. Our approach gives evidence to information gaps and possible actions that could be taken to promote a low-carbon energy transition.","['C. Piselli', 'A. Fronzetti Colladon', 'L. Segneri', 'A. L. Pisello']","Renewable and Sustainable Energy Reviews 167, 112792 (2022)",arXiv,2022,https://doi.org/10.48550/arXiv.2208.01892,Anomali
Joint Learning-based Causal Relation Extraction from Biomedical Literature,"Causal relation extraction of biomedical entities is one of the most complex tasks in biomedicaltextmining, which involves two kinds of information: entity relations and entity functions. One feasible approach is to take relation extraction and function detection as two independent sub-tasks. However, this separate learning method ignores the intrinsic correlation between them and leads to unsatisfactory performance. In this paper, we propose a joint learning model, which combines entity relation extraction and entity function detection to exploit their commonality and capture their inter-relationship, so as to improve the performance of biomedical causal relation extraction. Meanwhile, during the model training stage, different function types in the loss function are assigned different weights. Specifically, the penalty coefficient for negative function instances increases to effectively improve the precision of function detection. Experimental results on the BioCreative-V Track 4 corpus show that our joint learning model outperforms the separate models in BEL statement extraction, achieving the F1 scores of 58.4% and 37.3% on the test set in Stage 2 and Stage 1 evaluations, respectively. This demonstrates that our joint learning system reaches the state-of-the-art performance in Stage 2 compared with other systems.","['Dongling Li', 'Pengchao Wu', 'Yuehu Dong', 'Jinghang Gu', 'Longhua Qian', 'Guodong Zhou']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.01316,Anomali
Large-Scale Product Retrieval with Weakly Supervised Representation Learning,"Large-scale weakly supervised product retrieval is a practically useful yet computationally challenging problem. This paper introduces a novel solution for the eBay Visual Search Challenge (eProduct) held at the Ninth Workshop on Fine-Grained Visual Categorisation workshop (FGVC9) of CVPR 2022. This competition presents two challenges: (a) E-commerce is a drastically fine-grained domain including many products with subtle visual differences; (b) A lacking of target instance-level labels for model training, with only coarse category labels and product titles available. To overcome these obstacles, we formulate a strong solution by a set of dedicated designs: (a) Instead of usingtexttraining data directly, weminethousands of pseudo-attributes from product titles and use them as the ground truths for multi-label classification. (b) We incorporate several strong backbones with advanced training recipes for more discriminative representation learning. (c) We further introduce a number of post-processing techniques including whitening, re-ranking and model ensemble for retrieval enhancement. By achieving 71.53% MAR, our solution ""Involution King"" achieves the second position on the leaderboard.","['Xiao Han', 'Kam Woh Ng', 'Sauradip Nag', 'Zhiyu Qu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.00955,Anomali
Data Collection and Analysis of French Dialects,"This paper discusses creating and analysing a new dataset for dataminingandtextanalytics research, contributing to a joint Leeds University research project for the Corpus of National Dialects. This report investigates machine learning classifiers to classify samples of French dialecttextacross various French-speaking countries. Following the steps of the CRISP-DM methodology, this report explores the data collection process, data quality issues and data conversion fortextanalysis. Finally, after applying suitable dataminingtechniques, the evaluation methods, best overall features and classifiers and conclusions are discussed.","['Omar Shaur Choudhry', 'Paul Omara Odida', 'Joshua Reiner', 'Keiron Appleyard', 'Danielle Kushnir', 'William Toon']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.00752,Anomali
Effects of dilute coal char particle suspensions on propagating methane detonation wave,"Methane/coal dust hybrid explosion is one of the common hazards in process andminingindustries. In this study, methane detonation propagation in dilute coal char particle suspensions is studied based on Eulerian-Lagrangian method. The effects of char combustion on methane detonation dynamics are focused on. The results show that propagation of the methane detonation wave in coal particle suspensions are considerably affected by particle concentration and size. Detonation extinction occurs when the coal particle size is small and concentration is high. The averaged lead shock speed generally decreases with increased particle concentration and decreased particle size. Mean structure and interphase coupling of hybrid detonation are analysed, based on the gas and particle quantities. It is found that char combustion proceeds in the subsonic region behind the detonation wave and heat release is relatively distributed compared to that from gas phase reaction. The mass and energy transfer rates increase rapidly to the maximum near the reaction front in the induction zone. Moreover, for 1 μm particles, if the particle concentration is beyond a threshold value, detonation re-initiation occurs after it is quenched at the beginning of the coal dust suspensions. This is caused by hot spots from the shock focusing along the reaction front in a decoupled detonation and these shocks are generated from char combustion behind the lead shock.","['Jingtai Shi', 'Pikai Zhang', 'Yong Xu', 'Wanxing Ren', 'Huangwei Zhang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2208.00370,Anomali
A Data-driven Latent Semantic Analysis for Automatic Text Summarization using LDA Topic Modelling,"With the advent and popularity of big dataminingand hugetextanalysis in modern times, automatedtextsummarization became prominent for extracting and retrieving important information from documents. This research investigates aspects of automatictextsummarization from the perspectives of single and multiple documents. Summarization is a task of condensing hugetextarticles into short, summarized versions. Thetextis reduced in size for summarization purpose but preserving key vital information and retaining the meaning of the original document. This study presents the Latent Dirichlet Allocation (LDA) approach used to perform topic modelling from summarised medical science journal articles with topics related to genes and diseases. In this study, PyLDAvis web-based interactive visualization tool was used to visualise the selected topics. The visualisation provides an overarching view of the main topics while allowing and attributing deep meaning to the prevalence individual topic. This study presents a novel approach to summarization of single and multiple documents. The results suggest the terms ranked purely by considering their probability of the topic prevalence within the processed document using extractive summarization technique. PyLDAvis visualization describes the flexibility of exploring the terms of the topics' association to the fitted LDA model. The topic modelling result shows prevalence within topics 1 and 2. This association reveals that there is similarity between the terms in topic 1 and 2 in this study. The efficacy of the LDA and the extractive summarization methods were measured using Latent Semantic Analysis (LSA) and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics to evaluate the reliability and validity of the model.","['Daniel F. O. Onah', 'Elaine L. L. Pang', 'Mahmoud El-Haj']",,arXiv,2023,https://doi.org/10.48550/arXiv.2207.14687,Anomali
Knowledge-Driven Mechanistic Enrichment of the Preeclampsia Ignorome,"Preeclampsia is a leading cause of maternal and fetal morbidity and mortality. Currently, the only definitive treatment of preeclampsia is delivery of the placenta, which is central to the pathogenesis of the disease. Transcriptional profiling of human placenta from pregnancies complicated by preeclampsia has been extensively performed to identify differentially expressed genes (DEGs). The decisions to investigate DEGs experimentally are biased by many factors, causing many DEGs to remain uninvestigated. A set of DEGs which are associated with a disease experimentally, but which have no known association to the disease in the literature are known as the ignorome. Preeclampsia has an extensive body of scientific literature, a large pool of DEG data, and only one definitive treatment. Tools facilitating knowledge-based analyses, which are capable of combining disparate data from many sources in order to suggest underlying mechanisms of action, may be a valuable resource to support discovery and improve our understanding of this disease. In this work we demonstrate how a biomedical knowledge graph (KG) can be used to identify novel preeclampsia molecular mechanisms. Existing open source biomedical resources and publicly available high-throughput transcriptional profiling data were used to identify and annotate the function of currently uninvestigated preeclampsia-associated DEGs. Experimentally investigated genes associated with preeclampsia were identified from PubMed abstracts usingtext-miningmethodologies. The relative complement of thetext-mined- and meta-analysis-derived lists were identified as the uninvestigated preeclampsia-associated DEGs (n=445), i.e., the preeclampsia ignorome. Using the KG to investigate relevant DEGs revealed 53 novel clinically relevant and biologically actionable mechanistic associations.","['Tiffany J. Callahan', 'Adrianne L. Stefanski', 'Jin-Dong Kim', 'William A. Baumgartner Jr.', 'Jordan M. Wyrwa', 'Lawrence E. Hunter']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.14294,Anomali
An Explainable Decision Support System for Predictive Process Analytics,"Predictive Process Analytics is becoming an essential aid for organizations, providing online operational support of their processes. However, process stakeholders need to be provided with an explanation of the reasons why a given process execution is predicted to behave in a certain way. Otherwise, they will be unlikely to trust the predictive monitoring technology and, hence, adopt it. This paper proposes a predictive analytics framework that is also equipped with explanation capabilities based on the game theory of Shapley Values. The framework has been implemented in the IBM ProcessMiningsuite and commercialized for business users. The framework has been tested on real-life event data to assess the quality of the predictions and the corresponding evaluations. In particular, a user evaluation has been performed in order to understand if the explanations provided by the system were intelligible to process stakeholders.","['Riccardo Galanti', 'Massimiliano de Leoni', 'Merylin Monaro', 'Nicolò Navarin', 'Alan Marazzi', 'Brigida Di Stasi', 'Stéphanie Maldera']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.12782,Anomali
A Reference Data Model for Process-Related User Interaction Logs,"User interaction (UI) logs are high-resolution event logs that record low-level activities performed by a user during the execution of a task in an information system. Each event in a UI log corresponds to a single interaction between the user and the interface, such as clicking a button or entering a string into atextfield. UI logs are used for purposes like taskminingor robotic process automation (RPA), but each study and tool relies on a different conceptualization and implementation of the elements and attributes that constitute user interactions. This lack of standardization makes it difficult to integrate UI logs from different sources and to combine tools for UI data collection with downstream analytics or automation solutions. To address this, we propose a universally applicable reference data model for process-related UI logs. Based on a review of scientific literature and industry solutions, this model includes the core attributes of UI logs, but remains flexible with regard to the scope, level of abstraction, and case notion. We provide an implementation of the model as an extension to the XES interchange standard for event logs and demonstrate its practical applicability in a real-life RPA scenario.","['Luka Abb', 'Jana-Rebecca Rehse']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.12054,Anomali
Explored An Effective Methodology for Fine-Grained Snake Recognition,"Fine-Grained Visual Classification (FGVC) is a longstanding and fundamental problem in computer vision and pattern recognition, and underpins a diverse set of real-world applications. This paper describes our contribution at SnakeCLEF2022 with FGVC. Firstly, we design a strong multimodal backbone to utilize various meta-information to assist in fine-grained identification. Secondly, we provide new loss functions to solve the long tail distribution with dataset. Then, in order to take full advantage of unlabeled datasets, we use self-supervised learning and supervised learning joint training to provide pre-trained model. Moreover, some effective data process tricks also are considered in our experiments. Last but not least, fine-tuned in downstream task with hardmining, ensambled kinds of model performance. Extensive experiments demonstrate that our method can effectively improve the performance of fine-grained recognition. Our method can achieve a macro f1 score 92.7% and 89.4% on private and public dataset, respectively, which is the 1st place among the participators on private leaderboard.","['Yong Huang', 'Aderon Huang', 'Wei Zhu', 'Yanming Fang', 'Jinghua Feng']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.11637,Anomali
Patent Search Using Triplet Networks Based Fine-Tuned SciBERT,"In this paper, we propose a novel method for the prior-art search task. We fine-tune SciBERT transformer model using Triplet Network approach, allowing us to represent each patent with a fixed-size vector. This also enables us to conduct efficient vector similarity computations to rank patents in query time. In our experiments, we show that our proposed method outperforms baseline methods.","['Utku Umur Acikalin', 'Mucahid Kutlu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.11497,Anomali
You Actually Look Twice At it (YALTAi): using an object detection approach instead of region segmentation within the Kraken engine,"Layout Analysis (the identification of zones and their classification) is the first step along line segmentation in Optical Character Recognition and similar tasks. The ability of identifying main body oftextfrom marginaltextor running titles makes the difference between extracting the work fulltextof a digitized book and noisy outputs. We show that most segmenters focus on pixel classification and that polygonization of this output has not been used as a target for the latest competition on historical document (ICDAR 2017 and onwards), despite being the focus in the early 2010s. We propose to shift, for efficiency, the task from a pixel classification-based polygonization to an object detection using isothetic rectangles. We compare the output of Kraken and YOLOv5 in terms of segmentation and show that the later severely outperforms the first on small datasets (1110 samples and below). We release two datasets for training and evaluation on historical documents as well as a new package, YALTAi, which injects YOLOv5 in the segmentation pipeline of Kraken 4.1.",['Thibault Clérice'],,arXiv,2024,https://doi.org/10.48550/arXiv.2207.11230,Anomali
Learning from what we know: How to perform vulnerability prediction using noisy historical data,"Vulnerability prediction refers to the problem of identifying system components that are most likely to be vulnerable. Typically, this problem is tackled by training binary classifiers on historical data. Unfortunately, recent research has shown that such approaches underperform due to the following two reasons: a) the imbalanced nature of the problem, and b) the inherently noisy historical data, i.e., most vulnerabilities are discovered much later than they are introduced. This misleads classifiers as they learn to recognize actual vulnerable components as non-vulnerable. To tackle these issues, we propose TROVON, a technique that learns from known vulnerable components rather than from vulnerable and non-vulnerable components, as typically performed. We perform this by contrasting the known vulnerable, and their respective fixed components. This way, TROVON manages to learn from the things we know, i.e., vulnerabilities, hence reducing the effects of noisy and unbalanced data. We evaluate TROVON by comparing it with existing techniques on three security-critical open source systems, i.e., Linux Kernel, OpenSSL, and Wireshark, with historical vulnerabilities that have been reported in the National Vulnerability Database (NVD). Our evaluation demonstrates that the prediction capability of TROVON significantly outperforms existing vulnerability prediction techniques such as Software Metrics, Imports, Function Calls,TextMining, Devign, LSTM, and LSTM-RF with an improvement of 40.84% in Matthews Correlation Coefficient (MCC) score under Clean Training Data Settings, and an improvement of 35.52% under Realistic Training Data Settings.","['Aayush Garg', 'Renzo Degiovanni', 'Matthieu Jimenez', 'Maxime Cordy', 'Mike Papadakis', 'Yves LeTraon']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.11018,Anomali
Urdu Speech and Text Based Sentiment Analyzer,"Discovering what other people think has always been a key aspect of our information-gathering strategy. People can now actively utilize information technology to seek out and comprehend the ideas of others, thanks to the increased availability and popularity of opinion-rich resources such as online review sites and personal blogs. Because of its crucial function in understanding people's opinions, sentiment analysis (SA) is a crucial task. Existing research, on the other hand, is primarily focused on the English language, with just a small amount of study devoted to low-resource languages. For sentiment analysis, this work presented a new multi-class Urdu dataset based on user evaluations. The tweeter website was used to get Urdu dataset. Our proposed dataset includes 10,000 reviews that have been carefully classified into two categories by human experts: positive, negative. The primary purpose of this research is to construct a manually annotated dataset for Urdu sentiment analysis and to establish the baseline result. Five different lexicon- and rule-based algorithms including Naivebayes, Stanza, Textblob, Vader, and Flair are employed and the experimental results show that Flair with an accuracy of 70% outperforms other tested algorithms.","['Waqar Ahmad', 'Maryam Edalati']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.09163,Anomali
Research Trends and Applications of Data Augmentation Algorithms,"In the Machine Learning research community, there is a consensus regarding the relationship between model complexity and the required amount of data and computation power. In real world applications, these computational requirements are not always available, motivating research on regularization methods. In addition, current and past research have shown that simpler classification algorithms can reach state-of-the-art performance on computer vision tasks given a robust method to artificially augment the training dataset. Because of this, data augmentation techniques became a popular research topic in recent years. However, existing data augmentation methods are generally less transferable than other regularization methods. In this paper we identify the main areas of application of data augmentation algorithms, the types of algorithms used, significant research trends, their progression over time and research gaps in data augmentation literature. To do this, the related literature was collected through the Scopus database. Its analysis was done following network science,textminingand exploratory analysis approaches. We expect readers to understand the potential of data augmentation, as well as identify future research directions and open questions within data augmentation research.","['Joao Fonseca', 'Fernando Bacao']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.08817,Anomali
A transportable clock laser system with an instability of $1.6 \times 10^{-16}$,"We present a transportable ultra-stable clock laser system based on a Fabry-Pérot cavity with crystalline Al$_{0.92}$Ga$_{0.08}$As/GaAs mirror coatings, fused silica (FS) mirror substrates and a 20~cm-long ultra-low expansion (ULE\textsuperscript{\textregistered}) glass spacer with a predicted thermal noise floor of $\mathrm{mod}\,σ_\mathrm{y} = 7 \times 10^{-17}$ in modified Allan deviation at one second averaging time. The cavity has a cylindrical shape and is mounted at ten points. Its measured sensitivity of the fractional frequency to acceleration for the three Cartesian directions are $2(1) \times 10^{-12}$/(ms$^{-2}$), $3(3) \times 10^{-12}$/(ms$^{-2}$) and $3(1) \times 10^{-12}$/(ms$^{-2}$), which belong to the lowest acceleration sensitivities published for transportable systems. The laser system's instability reaches down to $\mathrm{mod}\,σ_\mathrm{y} = 1.6 \times 10^{-16}$","['Sofia Herbers', 'Sebastian Häfner', 'Sören Dörscher', 'Tim Lücke', 'Uwe Sterr', 'Christian Lisdat']","Opt. Lett. 47, pp. 5441-5444 (2022)",arXiv,2022,https://doi.org/10.48550/arXiv.2207.08679,Anomali
Transcribing Medieval Manuscripts for Machine Learning,"This article focuses on the transcription of medieval manuscripts. Whereas problems of transcription have long interested medievalists, few workable options in the era of printed editions were available besides normalisation. The automation of this process, known as handwrittentextrecognition (HTR), has made new kinds of digitaltextcreation possible, but also has foregrounded the necessity of theorising transcription in our scholarly practices. We reflect here on different notions of transcription against the backdrop of changingtexttechnologies. Moreover, drawing on our own research on medieval Latin Bibles, we present general guidelines for customizing transcription schemes, arguing that they must be designed with specific research questions and scholarly end use in mind. Since we are particularly interested in the scribal contribution to the production of codices, our transcription guidelines aim to capture abbreviations and orthographic variation between different textual witnesses for downstream machine learning tasks. In the final section of the article, we discuss a few examples of how the HTR-created transcriptions allow us to address new questions at scale in medieval manuscripts, such as textual variance across witnesses, the prediction of a change in scribal hands within a single manuscript as well as the profiling of individual and regional scribal characteristics.","['Estelle Guéville', 'David Joseph Wrisley']","Journal of Data Mining & Digital Humanities, On the Way to the Future of Digital Manuscript Studies (July 2, 2024) jdmdh:9805",arXiv,2023,https://doi.org/10.48550/arXiv.2207.07726,Anomali
OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text,"In recent years, creating and managing knowledge bases have become crucial to the retail product and enterprise domains. We present an automatic knowledge base construction system thatminesdata from documents. This system can generate training data during the training process without human intervention. Therefore, it is domain-agnostic trainable using only the target domaintextcorpus and a pre-defined knowledge base. This system is called OASYS and is the first system built with the Korean language in mind. In addition, we also have constructed a new human-annotated benchmark dataset of the Korean Wikipedia corpus paired with a Korean DBpedia to aid system evaluation. The system performance results on human-annotated benchmark test dataset are meaningful and show that the generated knowledge base from OASYS trained on only auto-generated data is useful. We provide both a human-annotated test dataset and an auto-generated dataset.","['Minsang Kim', 'Sang-hyun Je', 'Eunjoo Park']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.07597,Anomali
GrabQC: Graph based Query Contextualization for automated ICD coding,"Automated medical coding is a process of codifying clinical notes to appropriate diagnosis and procedure codes automatically from the standard taxonomies such as ICD (International Classification of Diseases) and CPT (Current Procedure Terminology). The manual coding process involves the identification of entities from the clinical notes followed by querying a commercial or non-commercial medical codes Information Retrieval (IR) system that follows the Centre for Medicare and Medicaid Services (CMS) guidelines. We propose to automate this manual process by automatically constructing a query for the IR system using the entities auto-extracted from the clinical notes. We propose \textbf{GrabQC}, a \textbf{Gra}ph \textbf{b}ased \textbf{Q}uery \textbf{C}ontextualization method that automatically extracts queries from the clinicaltext, contextualizes the queries using a Graph Neural Network (GNN) model and obtains the ICD Codes using an external IR system. We also propose a method for labelling the dataset for training the model. We perform experiments on two datasets of clinicaltextin three different setups to assert the effectiveness of our approach. The experimental results show that our proposed method is better than the compared baselines in all three settings.","['Jeshuren Chelladurai', 'Sudarsun Santhiappan', 'Balaraman Ravindran']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.06802,Anomali
Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa,"Textsentiment analysis, also known as opinionmining, is research on the calculation of people's views, evaluations, attitude and emotions expressed by entities.Textsentiment analysis can be divided intotext-level sentiment analysis, sen-tence-level sentiment analysis and aspect-level sentiment analysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the field of sentiment analysis, which aims to predict the polarity of aspects. The research of pre-training neural model has significantly improved the performance of many natural language processing tasks. In recent years, pre training model (PTM) has been applied in ABSA. Therefore, there has been a question, which is whether PTMs contain sufficient syntactic information for ABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced BERT with disentangled attention) to solve Aspect-Based Sentiment Analysis problem. DeBERTa is a kind of neural language model based on transformer, which uses self-supervised learning to pre-train on a large number of originaltextcorpora. Based on the Local Context Focus (LCF) mechanism, by integrating DeBERTa model, we purpose a multi-task learning model for aspect-based sentiment analysis. The experiments result on the most commonly used the laptop and restaurant datasets of SemEval-2014 and the ACL twitter dataset show that LCF mechanism with DeBERTa has significant improvement.","['Tianyu Zhao', 'Junping Du', 'Zhe Xue', 'Ang Li', 'Zeli Guan']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.02424,Anomali
A Tutorial on the Spectral Theory of Markov Chains,"Markov chains are a class of probabilistic models that have achieved widespread application in the quantitative sciences. This is in part due to their versatility, but is compounded by the ease with which they can be probed analytically. This tutorial provides an in-depth introduction to Markov chains, and explores their connection to graphs and random walks. We utilize tools from linear algebra and graph theory to describe the transition matrices of different types of Markov chains, with a particular focus on exploring properties of the eigenvalues and eigenvectors corresponding to these matrices. The results presented are relevant to a number of methods in machine learning and datamining, which we describe at various stages. Rather than being a novel academic study in its own right, thistextpresents a collection of known results, together with some new concepts. Moreover, the tutorial focuses on offering intuition to readers rather than formal understanding, and only assumes basic exposure to concepts from linear algebra and probability theory. It is therefore accessible to students and researchers from a wide variety of disciplines.","['Eddie Seabrook', 'Laurenz Wiskott']",Neural Computation (2023) 35 (11): 1713-1796,arXiv,2022,https://doi.org/10.48550/arXiv.2207.02296,Anomali
Keyword Extraction in Scientific Documents,"The scientific publication output grows exponentially. Therefore, it is increasingly challenging to keep track of trends and changes. Understanding scientific documents is an important step in downstream tasks such as knowledge graph building,textmining, and discipline classification. In this workshop, we provide a better understanding of keyword and keyphrase extraction from the abstract of scientific publications.","['Susie Xi Rao', 'Piriyakorn Piriyatamwong', 'Parijat Ghoshal', 'Sara Nasirian', 'Emmanuel de Salis', 'Sandra Mitrović', 'Michael Wechner', 'Vanya Brucker', 'Peter Egger', 'Ce Zhang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.01888,Anomali
GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot Learning,"Generalized Zero-Shot Learning (GZSL) aims to recognize images from both the seen and unseen classes by transferring semantic knowledge from seen to unseen classes. It is a promising solution to take the advantage of generative models to hallucinate realistic unseen samples based on the knowledge learned from the seen classes. However, due to the generation shifts, the synthesized samples by most existing methods may drift from the real distribution of the unseen data. To address this issue, we propose a novel flow-based generative framework that consists of multiple conditional affine coupling layers for learning unseen data generation. Specifically, we discover and address three potential problems that trigger the generation shifts, i.e., semantic inconsistency, variance collapse, and structure disorder. First, to enhance the reflection of the semantic information in the generated samples, we explicitly embed the semantic information into the transformation in each conditional affine coupling layer. Second, to recover the intrinsic variance of the real unseen features, we introduce a boundary sampleminingstrategy with entropy maximization to discover more difficult visual variants of semantic prototypes and hereby adjust the decision boundary of the classifiers. Third, a relative positioning strategy is proposed to revise the attribute embeddings, guiding them to fully preserve the inter-class geometric structure and further avoid structure disorder in the semantic space. Extensive experimental results on four GZSL benchmark datasets demonstrate that GSMFlow achieves the state-of-the-art performance on GZSL.","['Zhi Chen', 'Yadan Luo', 'Sen Wang', 'Jingjing Li', 'Zi Huang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.01798,Anomali
Enhancing Automated Software Traceability by Transfer Learning from Open-World Data,"Software requirements traceability is a critical component of the software engineering process, enabling activities such as requirements validation, compliance verification, and safety assurance. However, the cost and effort of manually creating a complete set of trace links across natural language artifacts such as requirements, design, and test-cases can be prohibitively expensive. Researchers have therefore proposed automated link-generation solutions primarily based on information-retrieval (IR) techniques; however, these solutions have failed to deliver the accuracy needed for full adoption in industrial projects. Improvements can be achieved using deep-learning traceability models; however, their efficacy is impeded by the limited size and availability of project-level artifacts and links to serve as training data. In this paper, we address this problem by proposing and evaluating several deep-learning approaches fortext-to-texttraceability. Our method, named NLTrace, explores three transfer learning strategies that use datasetsminedfrom open world platforms. Through pretraining Language Models (LMs) and leveraging adjacent tracing tasks, we demonstrate that NLTrace can significantly improve the performance of LM based trace models when training links are available. In such scenarios NLTrace outperforms the best performing classical IR method with an 188% improvement in F2 score and 94.01% in Mean Average Precision (MAP). It also outperforms the general LM based trace model by 7% and 23% for F2 and MAP respectively. In addition, NLTrace can adapt to low-resource tracing scenarios where other LM models can not. The knowledge learned from adjacent tasks enables NLTrace to outperform VSM models by 28% F2 on generation challenges when presented with a small number of training examples.","['Jinfeng Lin', 'Amrit Poudel', 'Wenhao Yu', 'Qingkai Zeng', 'Meng Jiang', 'Jane Cleland-Huang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.01084,Anomali
FE${}^\textbf{ANN}$ $-$ An efficient data-driven multiscale approach based on physics-constrained neural networks and automated data mining,"Herein, we present a new data-driven multiscale framework called FE${}^\text{ANN}$which is based on two main keystones: the usage of physics-constrained artificial neural networks (ANNs) as macroscopic surrogate models and an autonomous dataminingprocess. Our approach allows the efficient simulation of materials with complex underlying microstructures which reveal an overall anisotropic and nonlinear behavior on the macroscale. Thereby, we restrict ourselves to finite strain hyperelasticity problems for now. By using a set of problem specific invariants as the input of the ANN and the Helmholtz free energy density as the output, several physical principles, e.g., objectivity, material symmetry, compatibility with the balance of angular momentum and thermodynamic consistency are fulfilled a priori. The necessary data for the training of the ANN-based surrogate model, i.e., macroscopic deformations and corresponding stresses, are collected via computational homogenization of representative volume elements (RVEs). Thereby, the core feature of the approach is given by a completely autonomousminingof the required data set within an overall loop. In each iteration of the loop, new data are generated by gathering the macroscopic deformation states from the macroscopic finite element (FE) simulation and a subsequently sorting by using the anisotropy class of the considered material. Finally, all unknown deformations are prescribed in the RVE simulation to get the corresponding stresses and thus to extend the data set. The proposed framework consequently allows to reduce the number of time-consuming microscale simulations to a minimum. It is exemplarily applied to several descriptive examples, where a fiber reinforced composite with a highly nonlinear Ogden-type behavior of the individual components is considered.","['Karl A. Kalina', 'Lennart Linden', 'Jörg Brummund', 'Markus Kästner']","Computational Mechanics 71, 827-851 (2023)",arXiv,2022,https://doi.org/10.48550/arXiv.2207.01045,Anomali
Mining Tourism Experience on Twitter: A case study,"With the increase of digital data and social network platforms the impact of social media science in driving company decision related to product/service features and customer care operations is becoming more crucial. In particular, platform such as Twitter where people can share experience about almost everything can drastically impact the reputation and offering of a company as well as of a place or tourism site.Textminingtools are researched and proposed in literature in order to gain value and perform trend topics and sentiment analysis on Twitter. As data are the fuels for these models, the ""right"" ones, i.e the domain-related ones makes a difference on their accuracy. In this paper, we describe a pipeline of \textit{DataOps / MLOps} operations performed over a tourism related Twitter dataset in order to comprehend tourism motivation and interest. The gained knowledge can be exploit, by the travel/hospitality industry in order to develop data-driven strategic service, and by travelers which can consume relevant information about tourist destination.","['Davide Stirparo', 'Beatrice Penna', 'Mohammad Kazemi', 'Ariona Shashaj']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.00816,Anomali
Is this bug severe? A text-cum-graph based model for bug severity prediction,"Repositories of large software systems have become commonplace. This massive expansion has resulted in the emergence of various problems in these software platforms including identification of (i) bug-prone packages, (ii) critical bugs, and (iii) severity of bugs. One of the important goals would be tominethese bugs and recommend them to the developers to resolve them. The first step to this is that one has to accurately detect the extent of severity of the bugs. In this paper, we take up this task of predicting the severity of bugs in the near future. Contextualized neural models built on thetextdescription of a bug and the user comments about the bug help to achieve reasonably good performance. Further information on how the bugs are related to each other in terms of the ways they affect packages can be summarised in the form of a graph and used along with thetextto get additional benefits.","['Rima Hazra', 'Arpit Dwivedi', 'Animesh Mukherjee']",,arXiv,2022,https://doi.org/10.48550/arXiv.2207.00623,Anomali
BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models,"It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing ortextmining, are often limited to a small predefined set of relations due to manual cost or restrictions intextcorpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore mechanism for improved efficiency and accuracy. We deploy the approach to harvest KGs of over 400 new relations from different LMs. Extensive human and automatic evaluations show our approach manages to extract diverse accurate knowledge, including tuples of complex relations (e.g., ""A is capable of but not good at B""). The resulting KGs as a symbolic interpretation of the source LMs also reveal new insights into the LMs' knowledge capacities.","['Shibo Hao', 'Bowen Tan', 'Kaiwen Tang', 'Bin Ni', 'Xiyan Shao', 'Hengzhe Zhang', 'Eric P. Xing', 'Zhiting Hu']",,arXiv,2023,https://doi.org/10.48550/arXiv.2206.14268,Anomali
Principal Phrase Mining,"Extracting frequent words from a collection oftextsis commonly performed in many subjects. However, as useful as it is to obtain a collection of commonly occurring words fromtexts, there is a need for more specific information to be obtained fromtextsin the form of most commonly occurring phrases. Despite this need, extracting frequent phrases is not commonly done due to inherent complications, the most significant being double-counting. Double-counting occurs when words or phrases are counted when they appear inside longer phrases that themselves are also counted, resulting in a selection of mostly meaningless phrases that are frequent only because they occur inside frequent super phrases. Several papers have been written on phraseminingthat describe solutions to this issue; however, they either require a list of so-called quality phrases to be available to the extracting process, or they require human interaction to identify those quality phrases during the process. We present here a method that eliminates double-counting via a unique rectification process that does not require lists of quality phrases. In the context of a set oftexts, we define a principal phrase as a phrase that does not cross punctuation marks, does not start with a stop word, with the exception of the stop words ""not"" and ""no"", does not end with a stop word, is frequent within thosetextswithout being double counted, and is meaningful to the user. Our method identifies such principal phrases independently without human input, and enables their extraction from anytextswithin a reasonable amount of time.","['Ellie Small', 'Javier Cabrera']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.13748,Anomali
Video Activity Localisation with Uncertainties in Temporal Boundary,"Current methods for video activity localisation over time assume implicitly that activity temporal boundaries labelled for model training are determined and precise. However, in unscripted natural videos, different activities mostly transit smoothly, so that it is intrinsically ambiguous to determine in labelling precisely when an activity starts and ends over time. Such uncertainties in temporal labelling are currently ignored in model training, resulting in learning mis-matched video-textcorrelation with poor generalisation in test. In this work, we solve this problem by introducing Elastic Moment Bounding (EMB) to accommodate flexible and adaptive activity temporal boundaries towards modelling universally interpretable video-textcorrelation with tolerance to underlying temporal uncertainties in pre-fixed annotations. Specifically, we construct elastic boundaries adaptively byminingand discovering frame-wise temporal endpoints that can maximise the alignment between video segments and query sentences. To enable both more accurate matching (segment content attention) and more robust localisation (segment elastic boundaries), we optimise the selection of frame-wise endpoints subject to segment-wise contents by a novel Guided Attention mechanism. Extensive experiments on three video activity localisation benchmarks demonstrate compellingly the EMB's advantages over existing methods without modelling uncertainty.","['Jiabo Huang', 'Hailin Jin', 'Shaogang Gong', 'Yang Liu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.12923,Anomali
Protoformer: Embedding Prototypes for Transformers,"Transformers have been widely applied intextclassification. Unfortunately, real-world data contain anomalies and noisy labels that cause challenges for state-of-art Transformers. This paper proposes Protoformer, a novel self-learning framework for Transformers that can leverage problematic samples fortextclassification. Protoformer features a selection mechanism for embedding samples that allows us to efficiently extract and utilize anomalies prototypes and difficult class prototypes. We demonstrated such capabilities on datasets with diverse textual structures (e.g., Twitter, IMDB, ArXiv). We also applied the framework to several models. The results indicate that Protoformer can improve current Transformers in various empirical settings.","['Ashkan Farhangi', 'Ning Sui', 'Nan Hua', 'Haiyan Bai', 'Arthur Huang', 'Zhishan Guo']","Advances in Knowledge Discovery and Data Mining: 26th Pacific-Asia Conference, PAKDD 2022",arXiv,2022,https://doi.org/10.48550/arXiv.2206.12710,Anomali
Mining Error Templates for Grammatical Error Correction,"Some grammatical error correction (GEC) systems incorporate hand-crafted rules and achieve positive results. However, manually defining rules is time-consuming and laborious. In view of this, we propose a method tomineerror templates for GEC automatically. An error template is a regular expression aiming at identifyingtexterrors. We use the web crawler to acquire such error templates from the Internet. For each template, we further select the corresponding corrective action by using the language model perplexity as a criterion. We have accumulated 1,119 error templates for Chinese GEC based on this method. Experimental results on the newly proposed CTC-2021 Chinese GEC benchmark show that combing our error templates can effectively improve the performance of a strong GEC system, especially on two error types with very little training data. Our error templates are available at \url{https://github.com/HillZhang1999/gec_error_template}.","['Yue Zhang', 'Haochen Jiang', 'Zuyi Bao', 'Bo Zhang', 'Chen Li', 'Zhenghua Li']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.11569,Anomali
Depth-based clustering analysis of directional data,"A new depth-based clustering procedure for directional data is proposed. Such method is fully non-parametric and has the advantages to be flexible and applicable even in high dimensions when a suitable notion of depth is adopted. The introduced technique is evaluated through an extensive simulation study. In addition, a real data example intextminingis given to explain its effectiveness in comparison with other existing directional clustering algorithms.","['Giuseppe Pandolfo', ""Antonio D'ambrosio""]",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.10447,Anomali
RuArg-2022: Argument Mining Evaluation,"Argumentation analysis is a field of computational linguistics that studies methods for extracting arguments fromtextsand the relationships between them, as well as building argumentation structure oftexts. This paper is a report of the organizers on the first competition of argumentation analysis systems dealing with Russian languagetextswithin the framework of the Dialogue conference. During the competition, the participants were offered two tasks: stance detection and argument classification. A corpus containing 9,550 sentences (comments on social media posts) on three topics related to the COVID-19 pandemic (vaccination, quarantine, and wearing masks) was prepared, annotated, and used for training and testing. The system that won the first place in both tasks used the NLI (Natural Language Inference) variant of the BERT architecture, automatic translation into English to apply a specialized BERT model, retrained on Twitter posts discussing COVID-19, as well as additional masking of target entities. This system showed the following results: for the stance detection task an F1-score of 0.6968, for the argument classification task an F1-score of 0.7404. We hope that the prepared dataset and baselines will help to foster further research on argumentminingfor the Russian language.","['Evgeny Kotelnikov', 'Natalia Loukachevitch', 'Irina Nikishina', 'Alexander Panchenko']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.09249,Anomali
Intelligent Blockchain-based Edge Computing via Deep Reinforcement Learning: Solutions and Challenges,"The convergence of mobile edge computing (MEC) and blockchain is transforming the current computing services in wireless Internet-of-Things networks, by enabling task offloading with security enhancement based on blockchainmining. Yet the existing approaches for these enabling technologies are isolated, providing only tailored solutions for specific services and scenarios. To fill this gap, we propose a novel cooperative task offloading and blockchainmining(TOBM) scheme for a blockchain-based MEC system, where each edge device not only handles computation tasks but also deals with blockminingfor improving system utility. To address the latency issues caused by the blockchain operation in MEC, we develop a new Proof-of-Reputation consensus mechanism based on a lightweight block verification strategy. To accommodate the highly dynamic environment and high-dimensional system state space, we apply a novel distributed deep reinforcement learning-based approach by using a multi-agent deep deterministic policy gradient algorithm. Experimental results demonstrate the superior performance of the proposed TOBM scheme in terms of enhanced system reward, improved offloading utility with lower blockchainmininglatency, and better system utility, compared to the existing cooperative and non-cooperative schemes. The paper concludes with key technical challenges and possible directions for future blockchain-based MEC research.","['Dinh C. Nguyen', 'Van-Dinh Nguyen', 'Ming Ding', 'Symeon Chatzinotas', 'Pubudu N. Pathirana', 'Aruna Seneviratne', 'Octavia Dobre', 'Albert Y. Zomaya']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.09009,Anomali
From Bi-Level to One-Level: A Framework for Structural Attacks to Graph Anomaly Detection,"The success of graph neural networks stimulates the prosperity of graphminingand the corresponding downstream tasks including graph anomaly detection (GAD). However, it has been explored that those graphminingmethods are vulnerable to structural manipulations on relational data. That is, the attacker can maliciously perturb the graph structures to assist the target nodes to evade anomaly detection. In this paper, we explore the structural vulnerability of two typical GAD systems: unsupervised FeXtra-based GAD and supervised GCN-based GAD. Specifically, structural poisoning attacks against GAD are formulated as complex bi-level optimization problems. Our first major contribution is then to transform the bi-level problem into one-level leveraging different regression methods. Furthermore, we propose a new way of utilizing gradient information to optimize the one-level optimization problem in the discrete domain. Comprehensive experiments demonstrate the effectiveness of our proposed attack algorithm BinarizedAttack.","['Yulin Zhu', 'Yuni Lai', 'Kaifa Zhao', 'Xiapu Luo', 'Mingquan Yuan', 'Jun Wu', 'Jian Ren', 'Kai Zhou']",,arXiv,2023,https://doi.org/10.48550/arXiv.2206.08260,Anomali
Born for Auto-Tagging: Faster and better with new objective functions,"Keyword extraction is a task oftextmining. It is applied to increase search volume in SEO and ads. Implemented in auto-tagging, it makes tagging on a mass scale of online articles and photos efficiently and accurately. BAT is invented for auto-tagging which served as awoo's AI marketing platform (AMP). awoo AMP not only provides service as a customized recommender system but also increases the converting rate in E-commerce. The strength of BAT converges faster and better than other SOTA models, as its 4-layer structure achieves the best F scores at 50 epochs. In other words, it performs better than other models which require deeper layers at 100 epochs. To generate rich and clean tags, awoo creates new objective functions to maintain similar ${\rm F_1}$ scores with cross-entropy while enhancing ${\rm F_2}$ scores simultaneously. To assure the even better performance of F scores awoo revamps the learning rate strategy proposed by Transformer \cite{Transformer} to increase ${\rm F_1}$ and ${\rm F_2}$ scores at the same time.","['Chiung-ju Liu', 'Huang-Ting Shieh']",,arXiv,2023,https://doi.org/10.48550/arXiv.2206.07264,Anomali
TeKo: Text-Rich Graph Neural Networks with External Knowledge,"Graph Neural Networks (GNNs) have gained great popularity in tackling various analytical tasks on graph-structured data (i.e., networks). Typical GNNs and their variants follow a message-passing manner that obtains network representations by the feature propagation process along network topology, which however ignore the rich textual semantics (e.g., local word-sequence) that exist in many real-world networks. Existing methods fortext-rich networks integrate textual semantics by mainly utilizing internal information such as topics or phrases/words, which often suffer from an inability to comprehensivelyminethetextsemantics, limiting the reciprocal guidance between network structure andtextsemantics. To address these problems, we propose a noveltext-rich graph neural network with external knowledge (TeKo), in order to take full advantage of both structural and textual information withintext-rich networks. Specifically, we first present a flexible heterogeneous semantic network that incorporates high-quality entities and interactions among documents and entities. We then introduce two types of external knowledge, that is, structured triplets and unstructured entity description, to gain a deeper insight into textual semantics. We further design a reciprocal convolutional mechanism for the constructed heterogeneous semantic network, enabling network structure and textual semantics to collaboratively enhance each other and learn high-level network representations. Extensive experimental results on four publictext-rich networks as well as a large-scale e-commerce searching dataset illustrate the superior performance of TeKo over state-of-the-art baselines.","['Zhizhi Yu', 'Di Jin', 'Jianguo Wei', 'Ziyang Liu', 'Yue Shang', 'Yun Xiao', 'Jiawei Han', 'Lingfei Wu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.07253,Anomali
Electron-phonon coupling strength from ab initio frozen-phonon approach,"We propose a fast method for high-throughput screening of potential superconducting materials. The method is based on calculating metallic screening of zone-center phonon modes, which provides an accurate estimate for the electron-phonon coupling strength. This method is complementary to the recently proposed Rigid Muffin Tin (RMT) method, which amounts to integrating the electron-phonon coupling over the entire Brillouin zone (as opposed to the zone center), but in a relatively inferior approximation. We illustrate the use of this method by applying it to MgB$_\text{2}$, where the high-temperature superconductivity is known to be driven largely by the zone-center modes, and compare it to a sister compound AlB$_\text{2}$. We further illustrate the usage of this descriptor by screening a large number of binary hydrides, for which accurate first-principle calculations of electron-phonon coupling have been recently published. Together with the RMT descriptor, this method opens a way to perform initial high-throughput screening in search of conventional superconductors via machine learning or datamining.","['Yang Sun', 'Feng Zhang', 'Cai-Zhuang Wang', 'Kai-Ming Ho', 'Igor I. Mazin', 'Vladimir Antropov']","Phys. Rev. Materials 6, 074801 (2022)",arXiv,2022,https://doi.org/10.48550/arXiv.2206.06503,Anomali
Sentiment analysis on electricity twitter posts,"In today's world, everyone is expressive in some way, and the focus of this project is on people's opinions about rising electricity prices in United Kingdom and India using data from Twitter, a micro-blogging platform on which people post messages, known as tweets. Because many people's incomes are not good and they have to pay so many taxes and bills, maintaining a home has become a disputed issue these days. Despite the fact that Government offered subsidy schemes to compensate people electricity bills but it is not welcomed by people. In this project, the aim is to perform sentiment analysis on people's expressions and opinions expressed on Twitter. In order to grasp the electricity prices opinion, it is necessary to carry out sentiment analysis for the government and consumers in energy market. Furthermore,textpresent on these medias are unstructured in nature, so to process them we firstly need to pre-process the data. There are so many feature extraction techniques such as Bag of Words, TF-IDF (Term Frequency-Inverse Document Frequency), word embedding, NLP based features like word count. In this project, we analysed the impact of feature TF-IDF word level on electricity bills dataset of sentiment analysis. We found that by using TF-IDF word level performance of sentiment analysis is 3-4 higher than using N-gram features. Analysis is done using four classification algorithms including Naive Bayes, Decision Tree, Random Forest, and Logistic Regression and considering F-Score, Accuracy, Precision, and Recall performance parameters.","['Pardeep Kaur', 'Maryam Edalati']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.05042,Anomali
SsciBERT: A Pre-trained Language Model for Social Science Texts,"The academic literature of social sciences records human civilization and studies human social problems. With its large-scale growth, the ways to quickly find existing research on relevant issues have become an urgent demand for researchers. Previous studies, such as SciBERT, have shown that pre-training using domain-specifictextscan improve the performance of natural language processing tasks. However, the pre-trained language model for social sciences is not available so far. In light of this, the present research proposes a pre-trained model based on the abstracts published in the Social Science Citation Index (SSCI) journals. The models, which are available on GitHub (https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent performance on discipline classification, abstract structure-function recognition, and named entity recognition tasks with the social sciences literature.","['Si Shen', 'Jiangfeng Liu', 'Litao Lin', 'Ying Huang', 'Lin Zhang', 'Chang Liu', 'Yutong Feng', 'Dongbo Wang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.04510,Anomali
Graph Attention Multi-Layer Perceptron,"Graph neural networks (GNNs) have achieved great success in many graph-based applications. However, the enormous size and high sparsity level of graphs hinder their applications under industrial scenarios. Although some scalable GNNs are proposed for large-scale graphs, they adopt a fixed $K$-hop neighborhood for each node, thus facing the over-smoothing issue when adopting large propagation depths for nodes within sparse regions. To tackle the above issue, we propose a new GNN architecture -- Graph Attention Multi-Layer Perceptron (GAMLP), which can capture the underlying correlations between different scales of graph knowledge. We have deployed GAMLP in Tencent with the Angel platform, and we further evaluate GAMLP on both real-world datasets and large-scale industrial datasets. Extensive experiments on these 14 graph datasets demonstrate that GAMLP achieves state-of-the-art performance while enjoying high scalability and efficiency. Specifically, it outperforms GAT by 1.3\% regarding predictive accuracy on our large-scale Tencent Video dataset while achieving up to $50\times$ training speedup. Besides, it ranks top-1 on both the leaderboards of the largest homogeneous and heterogeneous graph (i.e., ogbn-papers100M and ogbn-mag) of Open Graph Benchmark.","['Wentao Zhang', 'Ziqi Yin', 'Zeang Sheng', 'Yang Li', 'Wen Ouyang', 'Xiaosen Li', 'Yangyu Tao', 'Zhi Yang', 'Bin Cui']","In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022",arXiv,2022,https://doi.org/10.48550/arXiv.2206.04355,Anomali
Unsupervised Key Event Detection from Massive Text Corpora,"Automated event detection from news corpora is a crucial task towardsminingfast-evolving structured knowledge. As real-world events have different granularities, from the top-level themes to key events and then to event mentions corresponding to concrete actions, there are generally two lines of research: (1) theme detection identifies from a news corpus major themes (e.g., ""2019 Hong Kong Protests"" vs. ""2020 U.S. Presidential Election"") that have very distinct semantics; and (2) action extraction extracts from one document mention-level actions (e.g., ""the police hit the left arm of the protester"") that are too fine-grained for comprehending the event. In this paper, we propose a new task, key event detection at the intermediate level, aiming to detect from a news corpus key events (e.g., ""HK Airport Protest on Aug. 12-14""), each happening at a particular time/location and focusing on the same topic. This task can bridge event understanding and structuring and is inherently challenging because of the thematic and temporal closeness of key events and the scarcity of labeled data due to the fast-evolving nature of news articles. To address these challenges, we develop an unsupervised key event detection framework, EvMine, that (1) extracts temporally frequent peak phrases using a novel ttf-itf score, (2) merges peak phrases into event-indicative feature sets by detecting communities from our designed peak phrase graph that captures document co-occurrences, semantic similarities, and temporal closeness signals, and (3) iteratively retrieves documents related to each key event by training a classifier with automatically generated pseudo labels from the event-indicative feature sets and refining the detected key events using the retrieved documents. Extensive experiments and case studies show EvMine outperforms all the baseline methods and its ablations on two real-world news corpora.","['Yunyi Zhang', 'Fang Guo', 'Jiaming Shen', 'Jiawei Han']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.04153,Anomali
Sentiment Analysis of Online Travel Reviews Based on Capsule Network and Sentiment Lexicon,"With the development of online travel services, it has great application prospects to timelymineusers' evaluation emotions for travel services and use them as indicators to guide the improvement of online travel service quality. In this paper, we study thetextsentiment classification of online travel reviews based on social media online comments and propose the SCCL model based on capsule network and sentiment lexicon. SCCL model aims at the lack of consideration of local features and emotional semantic features of thetextin the language model that can efficiently extracttextcontext features like BERT and GRU. Then make the following improvements to their shortcomings. On the one hand, based on BERT-BiGRU, the capsule network is introduced to extract local features while retaining good context features. On the other hand, the sentiment lexicon is introduced to extract the emotional sequence of thetextto provide richer emotional semantic features for the model. To enhance the universality of the sentiment lexicon, the improved SO-PMI algorithm based on TF-IDF is used to expand the lexicon, so that the lexicon can also perform well in the field of online travel reviews.","['Jia Wang', 'Junping Du', 'Yingxia Shao', 'Ang Li']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.02160,Anomali
Egocentric Video-Language Pretraining,"Video-Language Pretraining (VLP), which aims to learn transferable representation to advance a wide range of video-textdownstream tasks, has recently received increasing attention. Best performing works rely on large-scale, 3rd-person video-textdatasets, such as HowTo100M. In this work, we exploit the recently released Ego4D dataset to pioneer Egocentric VLP along three directions. (i) We create EgoClip, a 1st-person video-textpretraining dataset comprising 3.8M clip-textpairs well-chosen from Ego4D, covering a large variety of human daily activities. (ii) We propose a novel pretraining objective, dubbed EgoNCE, which adapts video-textcontrastive learning to the egocentric domain byminingegocentric-aware positive and negative samples. (iii) We introduce EgoMCQ, a development benchmark that is close to EgoClip and hence can support effective validation and fast exploration of our design decisions in EgoClip and EgoNCE. Furthermore, we demonstrate strong performance on five egocentric downstream tasks across three datasets: video-textretrieval on EPIC-KITCHENS-100; action recognition on Charades-Ego; natural language query, moment query, and object state change classification on Ego4D challenge benchmarks. The dataset and code are available at https://github.com/showlab/EgoVLP.","['Kevin Qinghong Lin', 'Alex Jinpeng Wang', 'Mattia Soldan', 'Michael Wray', 'Rui Yan', 'Eric Zhongcong Xu', 'Difei Gao', 'Rongcheng Tu', 'Wenzhe Zhao', 'Weijie Kong', 'Chengfei Cai', 'Hongfa Wang', 'Dima Damen', 'Bernard Ghanem', 'Wei Liu', 'Mike Zheng Shou']",,arXiv,2022,https://doi.org/10.48550/arXiv.2206.01670,Anomali
Photoionization cross sections of ultracold $^{88}$Sr in $^1$P$_1$ and $^3$S$_1$ states at 390 nm and the resulting blue-detuned magic wavelength optical lattice clock constraints,"We present the measurements of the photoionisation cross sections of the excited $^1$P$_1$ and $^3$S$_1$ states of ultracold $^{88}$Sr atoms at 389.889 nm wavelength, which is the magic wavelength of the $^{1}$S$_{0}$-${}^{3}$P${}_{0}$ clock transition. The photoionisation cross section of the $^1$P$_1$ state is determined from the measured ionisation rates of $^{88}$Sr in the magneto-optical trap in the $^1$P$_1$ state to be 2.20(50)$\times$10$^{-20}$ m$^2$, while the photoionisation cross section of $^{88}$Sr in the $^3$S$_1$ state is inferred from the photoionisation-induced reduction in the number of atoms transferred through the$^3\text{S}_1$state in an operating optical lattice clock to be $1.38(66)\times$10$^{-18}$ m$^2$. Furthermore, the resulting limitations of employing a blue-detuned magic wavelength optical lattice in strontium optical lattice clocks are evaluated. We estimated photoionisation induced loss rates of atoms at 389.889 nm wavelength under typical experimental conditions and made several suggestions on how to mitigate these losses. In particular, the large photoionisation induced losses for the $^3$S$_1$ state would make the use of the $^3$S$_1$ state in the optical cycle in a blue-detuned optical lattice unfeasible and would instead require the less commonly used $^3$D$_{1,2}$ states during the detection part of the optical clock cycle.","['Marcin Witkowski', 'Sławomir Bilicki', 'Marcin Bober', 'Domagoj Kovačić', 'Vijay Singh', 'Ara Tonoyan', 'Michał Zawada']","Opt. Express 30, 21423-21438 (2022)",arXiv,2022,https://doi.org/10.48550/arXiv.2206.00733,Anomali
Computational Reproducibility Within Prognostics and Health Management,"Scientific research frequently involves the use of computational tools and methods. Providing thorough documentation, open-source code, and data -- the creation of reproducible computational research -- helps others understand a researcher's work. Here, we explore computational reproducibility, broadly, and from within the field of prognostics and health management (PHM). The adoption of reproducible computational research practices remains low across scientific disciplines and within PHM. Ourtextminingof more than 300 articles, from publications engaged in PHM research, showed that fewer than 1% of researchers made their code and data available to others. Although challenges remain, there are also clear opportunities, and benefits, for engaging in reproducible computational research. Highlighting an opportunity, we introduce an open-source software tool, called PyPHM, to assist PHM researchers in accessing and preprocessing common industrial datasets.","['Tim von Hahn', 'Chris K. Mechefske']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.15489,Anomali
X-SCITLDR: Cross-Lingual Extreme Summarization of Scholarly Documents,"The number of scientific publications nowadays is rapidly increasing, causing information overload for researchers and making it hard for scholars to keep up to date with current trends and lines of work. Consequently, recent work on applyingtextminingtechnologies for scholarly publications has investigated the application of automatictextsummarization technologies, including extreme summarization, for this domain. However, previous work has concentrated only on monolingual settings, primarily in English. In this paper, we fill this research gap and present an abstractive cross-lingual summarization dataset for four different languages in the scholarly domain, which enables us to train and evaluate models that process English papers and generate summaries in German, Italian, Chinese and Japanese. We present our new X-SCITLDR dataset for multilingual summarization and thoroughly benchmark different models based on a state-of-the-art multilingual pre-trained model, including a two-stage `summarize and translate' approach and a direct cross-lingual model. We additionally explore the benefits of intermediate-stage training using English monolingual summarization and machine translation as intermediate tasks and analyze performance in zero- and few-shot scenarios.","['Sotaro Takeshita', 'Tommaso Green', 'Niklas Friedrich', 'Kai Eckert', 'Simone Paolo Ponzetto']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.15051,Anomali
A multimedia recommendation model based on collaborative graph,"As one of the main solutions to the information overload problem, recommender systems are widely used in daily life. In the recent emerging micro-video recommendation scenario, micro-videos contain rich multimedia information, involvingtext, image, video and other multimodal data, and these rich multimodal information conceals users' deep interest in the items. Most of the current recommendation algorithms based on multimodal data use multimodal information to expand the information on the item side, but ignore the different preferences of users for different modal information, and lack the fine-grainedminingof the internal connection of multimodal information. To investigate the problems in the micro-video recommendr system mentioned above, we design a hybrid recommendation model based on multimodal information, introduces multimodal information and user-side auxiliary information in the network structure, fully explores the deep interest of users, measures the importance of each dimension of user and item feature representation in the scoring prediction task, makes the application of graph neural network in the recommendation system is improved by using an attention mechanism to fuse the multi-layer state output information, allowing the shallow structural features provided by the intermediate layer to better participate in the prediction task. The recommendation accuracy is improved compared with the traditional recommendation algorithm on different data sets, and the feasibility and effectiveness of our model is verified.","['Breda Lim', 'Shubhi Bansal', 'Ahmed Buru', 'Kayla Manthey']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.14931,Anomali
COVID-19 Literature Mining and Retrieval using Text Mining Approaches,"The novel coronavirus disease (COVID-19) began in Wuhan, China, in late 2019 and to date has infected over 148M people worldwide, resulting in 3.12M deaths. On March 10, 2020, the World Health Organisation (WHO) declared it as a global pandemic. Many academicians and researchers started to publish papers describing the latest discoveries on covid-19. The large influx of publications made it hard for other researchers to go through a large amount of data and find the appropriate one that helps their research. So, the proposed model attempts to extract relavent titles from the large corpus of research publications which makes the job easy for the researchers. Allen Institute for AI released the CORD-19 dataset, which consists of 2,00,000 journal articles related to coronavirus-related research publications from PubMed's PMC, WHO (World Health Organization), bioRxiv, and medRxiv pre-prints. Along with this document corpus, they have also provided a topics dataset named topics-rnd3 consisting of a list of topics. Each topic has three types of representations like query, question, and narrative. These Datasets are made open for research, and also they released a TREC-COVID competition on Kaggle. Using these topics like queries, our goal is to find out the relevant documents in the CORD-19 dataset. In this research, relevant documents should be recognized for the posed topics in topics-rnd3 data set. The proposed model uses Natural Language Processing(NLP) techniques like Bag-of-Words, Average Word-2-Vec, Average BERT Base model and Tf-Idf weighted Word2Vec model to fabricate vectors for query, question, narrative, and combinations of them. Similarly, fabricate vectors for titles in the CORD-19 dataset. After fabricating vectors, cosine similarity is used for finding similarities between every two vectors. Cosine similarity helps us to find relevant documents for the given topic.","['Sanku Satya Uday', 'Satti Thanuja Pavani', 'T. Jaya Lakshmi', 'Rohit Chivukula']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.14781,Anomali
Multimodal Fake News Detection via CLIP-Guided Learning,"Multimodal fake news detection has attracted many research interests in social forensics. Many existing approaches introduce tailored attention mechanisms to guide the fusion of unimodal features. However, how the similarity of these features is calculated and how it will affect the decision-making process in FND are still open questions. Besides, the potential of pretrained multi-modal feature learning models in fake news detection has not been well exploited. This paper proposes a FND-CLIP framework, i.e., a multimodal Fake News Detection network based on Contrastive Language-Image Pretraining (CLIP). Given a targeted multimodal news, we extract the deep representations from the image andtextusing a ResNet-based encoder, a BERT-based encoder and two pair-wise CLIP encoders. The multimodal feature is a concatenation of the CLIP-generated features weighted by the standardized cross-modal similarity of the two modalities. The extracted features are further processed for redundancy reduction before feeding them into the final classifier. We introduce a modality-wise attention module to adaptively reweight and aggregate the features. We have conducted extensive experiments on typical fake news datasets. The results indicate that the proposed framework has a better capability inminingcrucial features for fake news detection. The proposed FND-CLIP can achieve better performances than previous works, i.e., 0.7\%, 6.8\% and 1.3\% improvements in overall accuracy on Weibo, Politifact and Gossipcop, respectively. Besides, we justify that CLIP-based learning can allow better flexibility on multimodal feature selection.","['Yangming Zhou', 'Qichao Ying', 'Zhenxing Qian', 'Sheng Li', 'Xinpeng Zhang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.14304,Anomali
Toward practical weak measurement wavefront sensing: spatial resolution and achromatism,"The weak measurement wavefront sensor detects the phase gradient of light like the Shack-Hartmann sensor does. However, the use of one thin birefringent crystal to displace light beams results in a wavelength-dependent phase difference between the two polarization components, which limits the practical application. Using a Savart plate which consists of two such crystals can compensate for the phase difference and realize achromatic wavefront sensing when combined with an achromatic retarder. We discuss the spatial resolution of the sensor and experimentally reconstruct a wavefront modulated by a pattern. Then we obtain the Zernike coefficients with three different wavelengths before and after modulation. Our work makes this new wavefront sensor more applicable to actual tasks like biomedical imaging.","['Yi Zheng', 'Mu Yang', 'Zheng-Hao Liu', 'Jin-Shi Xu', 'Chuan-Feng Li', 'Guang-Can Guo']","Opt. Lett. 47, 2734-2737 (2022)",arXiv,2022,https://doi.org/10.48550/arXiv.2205.13373,Anomali
Evidential Temporal-aware Graph-based Social Event Detection via Dempster-Shafer Theory,"The rising popularity of online social network services has attracted lots of research onminingsocial media data, especially onminingsocial events. Social event detection, due to its wide applications, has now become a trivial task. State-of-the-art approaches exploiting Graph Neural Networks (GNNs) usually follow a two-step strategy: 1) constructingtextgraphs based on various views (\textit{co-user}, \textit{co-entities} and \textit{co-hashtags}); and 2) learning a unifiedtextrepresentation by a specific GNN model. Generally, the results heavily rely on the quality of the constructed graphs and the specific message passing scheme. However, existing methods have deficiencies in both aspects: 1) They fail to recognize the noisy information induced by unreliable views. 2) Temporal information which works as a vital indicator of events is neglected in most works. To this end, we propose ETGNN, a novel Evidential Temporal-aware Graph Neural Network. Specifically, we construct view-specific graphs whose nodes are thetextsand edges are determined by several types of shared elements respectively. To incorporate temporal information into the message passing scheme, we introduce a novel temporal-aware aggregator which assigns weights to neighbours according to an adaptive time exponential decay formula. Considering the view-specific uncertainty, the representations of all views are converted into mass functions through evidential deep learning (EDL) neural networks, and further combined via Dempster-Shafer theory (DST) to make the final detection. Experimental results on three real-world datasets demonstrate the effectiveness of ETGNN in accuracy, reliability and robustness in social event detection.","['Jiaqian Ren', 'Lei Jiang', 'Hao Peng', 'Zhiwei Liu', 'Jia Wu', 'Philip S. Yu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.12179,Anomali
Adaptive Fairness-Aware Online Meta-Learning for Changing Environments,"The fairness-aware online learning framework has arisen as a powerful tool for the continual lifelong learning setting. The goal for the learner is to sequentially learn new tasks where they come one after another over time and the learner ensures the statistic parity of the new coming task across different protected sub-populations (e.g. race and gender). A major drawback of existing methods is that they make heavy use of the i.i.d assumption for data and hence provide static regret analysis for the framework. However, low static regret cannot imply a good performance in changing environments where tasks are sampled from heterogeneous distributions. To address the fairness-aware online learning problem in changing environments, in this paper, we first construct a novel regret metric FairSAR by adding long-term fairness constraints onto a strongly adapted loss regret. Furthermore, to determine a good model parameter at each round, we propose a novel adaptive fairness-aware online meta-learning algorithm, namely FairSAOML, which is able to adapt to changing environments in both bias control and model precision. The problem is formulated in the form of a bi-level convex-concave optimization with respect to the model's primal and dual parameters that are associated with the model's accuracy and fairness, respectively. The theoretic analysis provides sub-linear upper bounds for both loss regret and violation of cumulative fairness constraints. Our experimental evaluation on different real-world datasets with settings of changing environments suggests that the proposed FairSAOML significantly outperforms alternatives based on the best prior online learning approaches.","['Chen Zhao', 'Feng Mi', 'Xintao Wu', 'Kai Jiang', 'Latifur Khan', 'Feng Chen']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.11264,Anomali
Retrieval-Augmented Multilingual Keyphrase Generation with Retriever-Generator Iterative Training,"Keyphrase generation is the task of automatically predicting keyphrases given a piece of longtext. Despite its recent flourishing, keyphrase generation on non-English languages haven't been vastly investigated. In this paper, we call attention to a new setting named multilingual keyphrase generation and we contribute two new datasets, EcommerceMKP and AcademicMKP, covering six languages. Technically, we propose a retrieval-augmented method for multilingual keyphrase generation to mitigate the data shortage problem in non-English languages. The retrieval-augmented model leverages keyphrase annotations in English datasets to facilitate generating keyphrases in low-resource languages. Given a non-English passage, a cross-lingual dense passage retrieval module finds relevant English passages. Then the associated English keyphrases serve as external knowledge for keyphrase generation in the current language. Moreover, we develop a retriever-generator iterative training algorithm tominepseudo parallel passage pairs to strengthen the cross-lingual passage retriever. Comprehensive experiments and ablations show that the proposed approach outperforms all baselines.","['Yifan Gao', 'Qingyu Yin', 'Zheng Li', 'Rui Meng', 'Tong Zhao', 'Bing Yin', 'Irwin King', 'Michael R. Lyu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.10471,Anomali
Towards a Holistic View on Argument Quality Prediction,"Argumentation is one of society's foundational pillars, and, sparked by advances in NLP and the vast availability oftextdata, automatedminingof arguments receives increasing attention. A decisive property of arguments is their strength or quality. While there are works on the automated estimation of argument strength, their scope is narrow: they focus on isolated datasets and neglect the interactions with related argumentminingtasks, such as argument identification, evidence detection, or emotional appeal. In this work, we close this gap by approaching argument quality estimation from multiple different angles: Grounded on rich results from thorough empirical evaluations, we assess the generalization capabilities of argument quality estimation across diverse domains, the interplay with related argumentminingtasks, and the impact of emotions on perceived argument strength. We find that generalization depends on a sufficient representation of different domains in the training part. In zero-shot transfer and multi-task experiments, we reveal that argument quality is among the more challenging tasks but can improve others. Finally, we show that emotions play a minor role in argument quality than is often assumed.","['Michael Fromm', 'Max Berrendorf', 'Johanna Reiml', 'Isabelle Mayerhofer', 'Siddharth Bhargava', 'Evgeniy Faerman', 'Thomas Seidl']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.09803,Anomali
CTL* model checking for data-aware dynamic systems with arithmetic,"The analysis of complex dynamic systems is a core research topic in formal methods and AI, and combined modelling of systems with data has gained increasing importance in applications such as business process management. In addition, processminingtechniques are nowadays used to automaticallymineprocess models from event data, often without correctness guarantees. Thus verification techniques for linear and branching time properties are needed to ensure desired behavior. Here we consider data-aware dynamic systems with arithmetic (DDSAs), which constitute a concise but expressive formalism of transition systems with linear arithmetic guards. We present a CTL* model checking procedure for DDSAs that relies on a finite-state abstraction by means of a set of formulas that capture variable configurations. Linear-time verification was shown to be decidable in specific classes of DDSAs where the constraint language or the control flow are suitably confined. We investigate several of these restrictions for the case of CTL*, with both positive and negative results: CTL* verification is proven decidable for monotonicity and integer periodicity constraint systems, but undecidable for feedback free and bounded lookback systems. To demonstrate the feasibility of our approach, we implemented it in the SMT-based prototype ada, showing that many practical business process models can be effectively analyzed.","['Paolo Felli', 'Marco Montali', 'Sarah Winkler']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.08976,Anomali
Heri-Graphs: A Workflow of Creating Datasets for Multi-modal Machine Learning on Graphs of Heritage Values and Attributes with Social Media,"Values (why to conserve) and Attributes (what to conserve) are essential concepts of cultural heritage. Recent studies have been using social media to map values and attributes conveyed by public to cultural heritage. However, it is rare to connect heterogeneous modalities of images,texts, geo-locations, timestamps, and social network structures tominethe semantic and structural characteristics therein. This study presents a methodological workflow for constructing such multi-modal datasets using posts and images on Flickr for graph-based machine learning (ML) tasks concerning heritage values and attributes. After data pre-processing using state-of-the-art ML models, the multi-modal information of visual contents and textual semantics are modelled as node features and labels, while their social relationships and spatiotemporal contexts are modelled as links in Multi-Graphs. The workflow is tested in three cities containing UNESCO World Heritage properties - Amsterdam, Suzhou, and Venice, which yielded datasets with high consistency for semi-supervised learning tasks. The entire process is formally described with mathematical notations, ready to be applied in provisional tasks both as ML problems with technical relevance and as urban/heritage study questions with societal interests. This study could also benefit the understanding and mapping of heritage values and attributes for future research in global cases, aiming at inclusive heritage management practices.","['Nan Bai', 'Pirouz Nourian', 'Renqian Luo', 'Ana Pereira Roders']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.07545,Anomali
PathologyBERT -- Pre-trained Vs. A New Transformer Language Model for Pathology Domain,"Pathologytextminingis a challenging task given the reporting variability and constant new findings in cancer sub-type definitions. However, successfultextminingof a large pathology database can play a critical role to advance 'big data' cancer research like similarity-based treatment selection, case identification, prognostication, surveillance, clinical trial screening, risk stratification, and many others. While there is a growing interest in developing language models for more specific clinical domains, no pathology-specific language space exist to support the rapid data-miningdevelopment in pathology space. In literature, a few approaches fine-tuned general transformer models on specialized corpora while maintaining the original tokenizer, but in fields requiring specialized terminology, these models often fail to perform adequately. We propose PathologyBERT - a pre-trained masked language model which was trained on 347,173 histopathology specimen reports and publicly released in the Huggingface repository. Our comprehensive experiments demonstrate that pre-training of transformer model on pathology corpora yields performance improvements on Natural Language Understanding (NLU) and Breast Cancer Diagnose Classification when compared to nonspecific language models.","['Thiago Santos', 'Amara Tariq', 'Susmita Das', 'Kavyasree Vayalpati', 'Geoffrey H. Smith', 'Hari Trivedi', 'Imon Banerjee']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.06885,Anomali
Twitter-Based Gender Recognition Using Transformers,"Social media contains useful information about people and the society that could help advance research in many different areas (e.g. by applying opinionmining, emotion/sentiment analysis, and statistical analysis) such as business and finance, health, socio-economic inequality and gender vulnerability. User demographics provide rich information that could help study the subject further. However, user demographics such as gender are considered private and are not freely available. In this study, we propose a model based on transformers to predict the user's gender from their images and tweets. We fine-tune a model based on Vision Transformers (ViT) to stratify female and male images. Next, we fine-tune another model based on Bidirectional Encoders Representations from Transformers (BERT) to recognize the user's gender by their tweets. This is highly beneficial, because not all users provide an image that indicates their gender. The gender of such users could be detected form their tweets. The combination model improves the accuracy of image andtextclassification models by 6.98% and 4.43%, respectively. This shows that the image andtextclassification models are capable of complementing each other by providing additional information to one another. We apply our method to the PAN-2018 dataset, and obtain an accuracy of 85.52%.","['Zahra Movahedi Nia', 'Ali Ahmadi', 'Bruce Mellado', 'Jianhong Wu', 'James Orbinski', 'Ali Agary', 'Jude Dzevela Kong']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.06801,Anomali
Research on the correlation between text emotion mining and stock market based on deep learning,"This paper discusses how to crawl the data of financial forums such as stock bar, and conduct emotional analysis combined with the in-depth learning model. This paper will use the Bert model to train the financial corpus and predict the Shenzhen stock index. Through the comparative study of the maximal information coefficient (MIC), it is found that the emotional characteristics obtained by applying the BERT model to the financial corpus can be reflected in the fluctuation of the stock market, which is conducive to effectively improve the prediction accuracy. At the same time, this paper combines in-depth learning with financialtextsto further explore the impact mechanism of investor sentiment on the stock market through in-depth learning, which will help the national regulatory authorities and policy departments to formulate more reasonable policies and guidelines for maintaining the stability of the stock market.",['Chenrui Zhang'],,arXiv,2022,https://doi.org/10.48550/arXiv.2205.06675,Anomali
Probabilistic and Non-Deterministic Event Data in Process Mining: Embedding Uncertainty in Process Analysis Techniques,"Processminingis a subfield of process science that analyzes event data collected in databases called event logs. Recently, novel types of event data have become of interest due to the wide industrial application of processmininganalyses. In this paper, we examine uncertain event data. Such data contain meta-attributes describing the amount of imprecision tied with attributes recorded in an event log. We provide examples of uncertain event data, present the state of the art in regard of uncertainty in processmining, and illustrate open challenges related to this research direction.",['Marco Pegoraro'],,arXiv,2022,https://doi.org/10.48550/arXiv.2205.04827,Anomali
Deep learning based Chinese text sentiment mining and stock market correlation research,"We explore how to crawl financial forum data such as stock bars and combine them with deep learning models for sentiment analysis. In this paper, we will use the BERT model to train against the financial corpus and predict the SZSE Component Index, and find that applying the BERT model to the financial corpus through the maximum information coefficient comparison study. The obtained sentiment features will be able to reflect the fluctuations in the stock market and help to improve the prediction accuracy effectively. Meanwhile, this paper combines deep learning with financialtext, in further exploring the mechanism of investor sentiment on stock market through deep learning method, which will be beneficial for national regulators and policy departments to develop more reasonable policy guidelines for maintaining the stability of stock market.",['Chenrui Zhang'],,arXiv,2022,https://doi.org/10.48550/arXiv.2205.04743,Anomali
Assigning Species Information to Corresponding Genes by a Sequence Labeling Framework,"The automatic assignment of species information to the corresponding genes in a research article is a critically important step in the gene normalization task, whereby a gene mention is normalized and linked to a database record or identifier by atext-miningalgorithm. Existing methods typically rely on heuristic rules based on gene and species co-occurrence in the article, but their accuracy is suboptimal. We therefore developed a high-performance method, using a novel deep learning-based framework, to classify whether there is a relation between a gene and a species. Instead of the traditional binary classification framework in which all possible pairs of genes and species in the same article are evaluated, we treat the problem as a sequence-labeling task such that only a fraction of the pairs needs to be considered. Our benchmarking results show that our approach obtains significantly higher performance compared to that of the rule-based baseline method for the species assignment task (from 65.8% to 81.3% in accuracy). The source code and data for species assignment are freely available at https://github.com/ncbi/SpeciesAssignment.","['Ling Luo', 'Chih-Hsuan Wei', 'Po-Ting Lai', 'Qingyu Chen', 'Rezarta Islamaj Doğan', 'Zhiyong Lu']","Database, Volume 2022, 2022, baac090",arXiv,2022,https://doi.org/10.48550/arXiv.2205.03853,Anomali
AKI-BERT: a Pre-trained Clinical Language Model for Early Prediction of Acute Kidney Injury,"Acute kidney injury (AKI) is a common clinical syndrome characterized by a sudden episode of kidney failure or kidney damage within a few hours or a few days. Accurate early prediction of AKI for patients in ICU who are more likely than others to have AKI can enable timely interventions, and reduce the complications of AKI. Much of the clinical information relevant to AKI is captured in clinical notes that are largely unstructuredtextand requires advanced natural language processing (NLP) for useful information extraction. On the other hand, pre-trained contextual language models such as Bidirectional Encoder Representations from Transformers (BERT) have improved performances for many NLP tasks in general domain recently. However, few have explored BERT on disease-specific medical domain tasks such as AKI early prediction. In this paper, we try to apply BERT to specific diseases and present an AKI domain-specific pre-trained language model based on BERT (AKI-BERT) that could be used tominethe clinical notes for early prediction of AKI. AKI-BERT is a BERT model pre-trained on the clinical notes of patients having risks for AKI. Our experiments on Medical Information Mart for Intensive Care III (MIMIC-III) dataset demonstrate that AKI-BERT can yield performance improvements for early AKI prediction, thus expanding the utility of the BERT model from general clinical domain to disease-specific domain.","['Chengsheng Mao', 'Liang Yao', 'Yuan Luo']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.03695,Anomali
Using virtual edges to extract keywords from texts modeled as complex networks,"Detecting keywords intextsis important for manytextminingapplications. Graph-based methods have been commonly used to automatically find the key concepts intexts, however, relevant information provided by embeddings has not been widely used to enrich the graph structure. Here we modeledtextsco-occurrence networks, where nodes are words and edges are established either by contextual or semantical similarity. We compared two embedding approaches -- Word2vec and BERT -- to check whether edges created via word embeddings can improve the quality of the keyword extraction method. We found that, in fact, the use of virtual edges can improve the discriminability of co-occurrence networks. The best performance was obtained when we considered low percentages of addition of virtual (embedding) edges. A comparative analysis of structural and dynamical network metrics revealed the degree, PageRank, and accessibility are the metrics displaying the best performance in the model enriched with virtual edges.","['Jorge A. V. Tohalino', 'Thiago C. Silva', 'Diego R. Amancio']",,arXiv,2022,https://doi.org/10.48550/arXiv.2205.02172,Anomali
Towards Understanding the Skill Gap in Cybersecurity,"Given the ongoing ""arms race"" in cybersecurity, the shortage of skilled professionals in this field is one of the strongest in computer science. The currently unmet staffing demand in cybersecurity is estimated at over 3 million jobs worldwide. Furthermore, the qualifications of the existing workforce are largely believed to be insufficient. We attempt to gain deeper insights into the nature of the current skill gap in cybersecurity. To this end, we correlate data from job ads and academic curricula using two kinds of skill characterizations: manual definitions from established skill frameworks as well as ""skill topics"" automatically derived bytextminingtools. Our analysis shows a strong agreement between these two analysis techniques and reveals a substantial undersupply in several crucial skill categories, e.g., software and application security, security management, requirements engineering, compliance, and certification. Based on the results of our analysis, we provide recommendations for future curricula development in cybersecurity so as to decrease the identified skill gaps.","['Francois Goupil', 'Pavel Laskov', 'Irdin Pekaric', 'Michael Felderer', 'Alexander Dürr', 'Frederic Thiesse']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.13793,Anomali
The Causal News Corpus: Annotating Causal Relations in Event Sentences from News,"Despite the importance of understanding causality, corpora addressing causal relations are limited. There is a discrepancy between existing annotation guidelines of event causality and conventional causality corpora that focus more on linguistics. Many guidelines restrict themselves to include only explicit relations or clause-based arguments. Therefore, we propose an annotation schema for event causality that addresses these concerns. We annotated 3,559 event sentences from protest event news with labels on whether it contains causal relations or not. Our corpus is known as the Causal News Corpus (CNC). A neural network built upon a state-of-the-art pre-trained language model performed well with 81.20% F1 score on test set, and 83.46% in 5-folds cross-validation. CNC is transferable across two external corpora: CausalTimeBank (CTB) and Penn Discourse Treebank (PDTB). Leveraging each of these external datasets for training, we achieved up to approximately 64% F1 on the CNC test set without additional fine-tuning. CNC also served as an effective training and pre-training dataset for the two external corpora. Lastly, we demonstrate the difficulty of our task to the layman in a crowd-sourced annotation exercise. Our annotated corpus is publicly available, providing a valuable resource for causaltextminingresearchers.","['Fiona Anting Tan', 'Ali Hürriyetoğlu', 'Tommaso Caselli', 'Nelleke Oostdijk', 'Tadashi Nomoto', 'Hansi Hettiarachchi', 'Iqra Ameer', 'Onur Uca', 'Farhana Ferdousi Liza', 'Tiancheng Hu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.11714,Anomali
Financial data analysis application via multi-strategy text processing,"Maintaining financial system stability is critical to economic development, and early identification of risks and opportunities is essential. The financial industry contains a wide variety of data, such as financial statements, customer information, stock trading data, news, etc. Massive heterogeneous data calls for intelligent algorithms for machines to process and understand. This paper mainly focuses on the stock trading data and news about China A-share companies. We present a financial data analysis application, Financial Quotient Porter, designed to combine textual and numerical data by using a multi-strategy dataminingapproach. Additionally, we present our efforts and plans in deep learning financialtextprocessing application scenarios using natural language processing (NLP) and knowledge graph (KG) technologies. Based on KG technology, risks and opportunities can be identified from heterogeneous data. NLP technology can be used to extract entities, relations, and events from unstructuredtext, and analyze market sentiment. Experimental results show market sentiments towards a company and an industry, as well as news-level associations between companies.",['Hongyin Zhu'],,arXiv,2022,https://doi.org/10.48550/arXiv.2204.11394,Anomali
EPiDA: An Easy Plug-in Data Augmentation Framework for High Performance Text Classification,"Recent works have empirically shown the effectiveness of data augmentation (DA) in NLP tasks, especially for those suffering from data scarcity. Intuitively, given the size of generated data, their diversity and quality are crucial to the performance of targeted tasks. However, to the best of our knowledge, most existing methods consider only either the diversity or the quality of augmented data, thus cannot fullyminethe potential of DA for NLP. In this paper, we present an easy and plug-in data augmentation framework EPiDA to support effectivetextclassification. EPiDA employs two mechanisms: relative entropy maximization (REM) and conditional entropy minimization (CEM) to control data generation, where REM is designed to enhance the diversity of augmented data while CEM is exploited to ensure their semantic consistency. EPiDA can support efficient and continuous data generation for effective classifier training. Extensive experiments show that EPiDA outperforms existing SOTA methods in most cases, though not using any agent networks or pre-trained generation networks, and it works well with various DA algorithms and classification models. Code is available at https://github.com/zhaominyiz/EPiDA.","['Minyi Zhao', 'Lu Zhang', 'Yi Xu', 'Jiandong Ding', 'Jihong Guan', 'Shuigeng Zhou']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.11205,Anomali
Global Mapping of Gene/Protein Interactions in PubMed Abstracts: A Framework and an Experiment with P53 Interactions,"Gene/protein interactions provide critical information for a thorough understanding of cellular processes. Recently, considerable interest and effort has been focused on the construction and analysis of genome-wide gene networks. The large body of biomedical literature is an important source of gene/protein interaction information. Recent advances intextminingtools have made it possible to automatically extract such documented interactions from free-textliterature. In this paper, we propose a comprehensive framework for constructing and analyzing large-scale gene functional networks based on the gene/protein interactions extracted from biomedical literature repositories usingtextminingtools. Our proposed framework consists of analyses of the network topology, network topology-gene function relationship, and temporal network evolution to distill valuable information embedded in the gene functional interactions in literature. We demonstrate the application of the proposed framework using a testbed of P53-related PubMed abstracts, which shows that literature-based P53 networks exhibit small-world and scale-free properties. We also found that high degree genes in the literature-based networks have a high probability of appearing in the manually curated database and genes in the same pathway tend to form local clusters in our literature-based networks. Temporal analysis showed that genes interacting with many other genes tend to be involved in a large number of newly discovered interactions.","['Xin Li', 'Hsinchun Chen', 'Zan Huang', 'Hua Su', 'Jesse D. Martinez']","Journal of biomedical informatics, 2007",arXiv,2022,https://doi.org/10.48550/arXiv.2204.10476,Anomali
"Text-mined dataset of gold nanoparticle synthesis procedures, morphologies, and size entities","Gold nanoparticles are highly desired for a range of technological applications due to their tunable properties, which are dictated by the size and shape of the constituent particles. Many heuristic methods for controlling the morphological characteristics of gold nanoparticles are well known. However, the underlying mechanisms controlling their size and shape remain poorly understood, partly due to the immense range of possible combinations of synthesis parameters. Data-driven methods can offer insight to help guide understanding of these underlying mechanisms, so long as sufficient synthesis data are available. To facilitate dataminingin this direction, we have constructed and made publicly available a dataset of codified gold nanoparticle synthesis protocols and outcomes extracted directly from the nanoparticle materials science literature using natural language processing andtext-miningtechniques. This dataset contains 5,154 data records, each representing a single gold nanoparticle synthesis article, filtered from a database of 4,973,165 publications. Each record contains codified synthesis protocols and extracted morphological information from a total of 7,608 experimental and 12,519 characterization paragraphs.","['Kevin Cruse', 'Amalie Trewartha', 'Sanghoon Lee', 'Zheren Wang', 'Haoyan Huo', 'Tanjin He', 'Olga Kononova', 'Anubhav Jain', 'Gerbrand Ceder']",,arXiv,2022,this https URL,Anomali
Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on Twitter (BEAR),"Textminingand information extraction for the medical domain has focused on scientifictextgenerated by researchers. However, their direct access to individual patient experiences or patient-doctor interactions can be limited. Information provided on social media, e.g., by patients and their relatives, complements the knowledge in scientifictext. It reflects the patient's journey and their subjective perspective on the process of developing symptoms, being diagnosed and offered a treatment, being cured or learning to live with a medical condition. The value of this type of data is therefore twofold: Firstly, it offers direct access to people's perspectives. Secondly, it might cover information that is not available elsewhere, including self-treatment or self-diagnoses. Named entity recognition and relation extraction are methods to structure information that is available in unstructuredtext. However, existing medical social media corpora focused on a comparably small set of entities and relations and particular domains, rather than putting the patient into the center of analyses. With this paper we contribute a corpus with a rich set of annotation layers following the motivation to uncover and model patients' journeys and experiences in more detail. We label 14 entity classes (incl. environmental factors, diagnostics, biochemical processes, patients' quality-of-life descriptions, pathogens, medical conditions, and treatments) and 20 relation classes (e.g., prevents, influences, interactions, causes) most of which have not been considered before for social media data. The publicly available dataset consists of 2,100 tweets with approx. 6,000 entity and 3,000 relation annotations. In a corpus analysis we find that over 80 % of documents contain relevant entities. Over 50 % of tweets express relations which we consider essential for uncovering patients' narratives about their journeys.","['Amelie Wührl', 'Roman Klinger']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.09952,Anomali
Multi-label classification for biomedical literature: an overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations,"The COVID-19 pandemic has been severely impacting global society since December 2019. Massive research has been undertaken to understand the characteristics of the virus and design vaccines and drugs. The related findings have been reported in biomedical literature at a rate of about 10,000 articles on COVID-19 per month. Such rapid growth significantly challenges manual curation and interpretation. For instance, LitCovid is a literature database of COVID-19-related articles in PubMed, which has accumulated more than 200,000 articles with millions of accesses each month by users worldwide. One primary curation task is to assign up to eight topics (e.g., Diagnosis and Treatment) to the articles in LitCovid. Despite the continuing advances in biomedicaltextminingmethods, few have been dedicated to topic annotations in COVID-19 literature. To close the gap, we organized the BioCreative LitCovid track to call for a community effort to tackle automated topic annotation for COVID-19 literature. The BioCreative LitCovid dataset, consisting of over 30,000 articles with manually reviewed topics, was created for training and testing. It is one of the largest multilabel classification datasets in biomedical scientific literature. 19 teams worldwide participated and made 80 submissions in total. Most teams used hybrid systems based on transformers. The highest performing submissions achieved 0.8875, 0.9181, and 0.9394 for macro F1-score, micro F1-score, and instance-based F1-score, respectively. The level of participation and results demonstrate a successful track and help close the gap between dataset curation and method development. The dataset is publicly available via https://ftp.ncbi.nlm.nih.gov/pub/lu/LitCovid/biocreative/ for benchmarking and further development.","['Qingyu Chen', 'Alexis Allot', 'Robert Leaman', 'Rezarta Islamaj Doğan', 'Jingcheng Du', 'Li Fang', 'Kai Wang', 'Shuo Xu', 'Yuefu Zhang', 'Parsa Bagherzadeh', 'Sabine Bergler', 'Aakash Bhatnagar', 'Nidhir Bhavsar', 'Yung-Chun Chang', 'Sheng-Jie Lin', 'Wentai Tang', 'Hongtong Zhang', 'Ilija Tavchioski', 'Senja Pollak', 'Shubo Tian', 'Jinfeng Zhang', 'Yulia Otmakhova', 'Antonio Jimeno Yepes', 'Hang Dong', 'Honghan Wu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.09781,Anomali
Research on Domain Information Mining and Theme Evolution of Scientific Papers,"In recent years, with the increase of social investment in scientific research, the number of research results in various fields has increased significantly. Cross-disciplinary research results have gradually become an emerging frontier research direction. There is a certain dependence between a large number of research results. It is difficult to effectively analyze today's scientific research results when looking at a single research field in isolation. How to effectively use the huge number of scientific papers to help researchers becomes a challenge. This paper introduces the research status at home and abroad in terms of domain informationminingand topic evolution law of scientific and technological papers from three aspects: the semantic feature representation learning of scientific and technological papers, the field informationminingof scientific and technological papers, and theminingand prediction of research topic evolution rules of scientific and technological papers.","['Changwei Zheng', 'Zhe Xue', 'Meiyu Liang', 'Feifei Kou', 'Zeli Guan']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.08476,Anomali
Machine-learning rationalization and prediction of solid-state synthesis conditions,"There currently exist no quantitative methods to determine the appropriate conditions for solid-state synthesis. This not only hinders the experimental realization of novel materials but also complicates the interpretation and understanding of solid-state reaction mechanisms. Here, we demonstrate a machine-learning approach that predicts synthesis conditions using large solid-state synthesis datasetstext-minedfrom scientific journal articles. Using feature importance ranking analysis, we discovered that optimal heating temperatures have strong correlations with the stability of precursor materials quantified using melting points and formation energies ($ΔG_f$, $ΔH_f$). In contrast, features derived from the thermodynamics of synthesis-related reactions did not directly correlate to the chosen heating temperatures. This correlation between optimal solid-state heating temperature and precursor stability extends Tamman's rule from intermetallics to oxide systems, suggesting the importance of reaction kinetics in determining synthesis conditions. Heating times are shown to be strongly correlated with the chosen experimental procedures and instrument setups, which may be indicative of human bias in the dataset. Using these predictive features, we constructed machine-learning models with good performance and general applicability to predict the conditions required to synthesize diverse chemical systems. Codes and data used in this work can be found at: https://github.com/CederGroupHub/s4.","['Haoyan Huo', 'Christopher J. Bartel', 'Tanjin He', 'Amalie Trewartha', 'Alexander Dunn', 'Bin Ouyang', 'Anubhav Jain', 'Gerbrand Ceder']","Chemistry of Materials, 2022",arXiv,2022,https://doi.org/10.48550/arXiv.2204.08151,Anomali
Non-Parallel Text Style Transfer with Self-Parallel Supervision,"The performance of existingtextstyle transfer models is severely limited by the non-parallel datasets on which the models are trained. In non-parallel datasets, no direct mapping exists between sentences of the source and target style; the style transfer models thus only receive weak supervision of the target sentences during training, which often leads the model to discard too much style-independent information, or utterly fail to transfer the style. In this work, we propose LaMer, a noveltextstyle transfer framework based on large-scale language models. LaMer firstminesthe roughly parallel expressions in the non-parallel datasets with scene graphs, and then employs MLE training, followed by imitation learning refinement, to leverage the intrinsic parallelism within the data. On two benchmark tasks (sentiment & formality transfer) and a newly proposed challenging task (political stance transfer), our model achieves qualitative advances in transfer accuracy, content preservation, and fluency. Further empirical and human evaluations demonstrate that our model not only makes training more efficient, but also generates more readable and diverse expressions than previous models.","['Ruibo Liu', 'Chongyang Gao', 'Chenyan Jia', 'Guangxuan Xu', 'Soroush Vosoughi']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.08123,Anomali
A New Dynamic Algorithm for Densest Subhypergraphs,"Computing a dense subgraph is a fundamental problem in graphmining, with a diverse set of applications ranging from electronic commerce to community detection in social networks. In many of these applications, the underlying context is better modelled as a weighted hypergraph that keeps evolving with time.
  This motivates the problem of maintaining the densest subhypergraph of a weighted hypergraph in a {\em dynamic setting}, where the input keeps changing via a sequence of updates (hyperedge insertions/deletions). Previously, the only known algorithm for this problem was due to Hu et al. [HWC17]. This algorithm worked only on unweighted hypergraphs, and had an approximation ratio of $(1+ε)r^2$ and an update time of$O(\text{poly} (r, \log n))$, where $r$ denotes the maximum rank of the input across all the updates.
  We obtain a new algorithm for this problem, which works even when the input hypergraph is weighted. Our algorithm has a significantly improved (near-optimal) approximation ratio of $(1+ε)$ that is independent of $r$, and a similar update time of$O(\text{poly} (r, \log n))$. It is the first $(1+ε)$-approximation algorithm even for the special case of weighted simple graphs.
  To complement our theoretical analysis, we perform experiments with our dynamic algorithm on large-scale, real-world data-sets. Our algorithm significantly outperforms the state of the art [HWC17] both in terms of accuracy and efficiency.","['Suman K. Bera', 'Sayan Bhattacharya', 'Jayesh Choudhari', 'Prantar Ghosh']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.08106,Anomali
Accurate Portraits of Scientific Resources and Knowledge Service Components,"With the advent of the cloud computing era, the cost of creating, capturing and managing information has gradually decreased. The amount of data in the Internet is also showing explosive growth, and more and more scientific and technological resources are uploaded to the network. Different from news and social media data ubiquitous in the Internet, the main body of scientific and technological resources is composed of academic-style resources or entities such as papers, patents, authors, and research institutions. There is a rich relationship network between resources, from which a large amount of cutting-edge scientific and technological information can bemined. There are a large number of management and classification standards for existing scientific and technological resources, but these standards are difficult to completely cover all entities and associations of scientific and technological resources, and cannot accurately extract important information contained in scientific and technological resources. How to construct a complete and accurate representation of scientific and technological resources from structured and unstructured reports andtextsin the network, and how to tap the potential value of scientific and technological resources is an urgent problem. The solution is to construct accurate portraits of scientific and technological resources in combination with knowledge graph related technologies.","['Yue Wang', 'Zhe Xue', 'Ang Li']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.04883,Anomali
BioRED: A Rich Biomedical Relation Extraction Dataset,"Automated relation extraction (RE) from biomedical literature is critical for many downstreamtextminingapplications in both research and real-world settings. However, most existing benchmarking datasets for bio-medical RE only focus on relations of a single type (e.g., protein-protein interactions) at the sentence level, greatly limiting the development of RE systems in biomedicine. In this work, we first review commonly used named entity recognition (NER) and RE datasets. Then we present BioRED, a first-of-its-kind biomedical RE corpus with multiple entity types (e.g., gene/protein, disease, chemical) and relation pairs (e.g., gene-disease; chemical-chemical) at the document level, on a set of 600 PubMed abstracts. Further, we label each relation as describing either a novel finding or previously known background knowledge, enabling automated algorithms to differentiate between novel and background information. We assess the utility of BioRED by benchmarking several existing state-of-the-art methods, including BERT-based models, on the NER and RE tasks. Our results show that while existing approaches can reach high performance on the NER task (F-score of 89.3%), there is much room for improvement for the RE task, especially when extracting novel relations (F-score of 47.7%). Our experiments also demonstrate that such a rich dataset can successfully facilitate the development of more accurate, efficient, and robust RE systems for biomedicine. The BioRED dataset and annotation guideline are freely available at https://ftp.ncbi.nlm.nih.gov/pub/lu/BioRED/.","['Ling Luo', 'Po-Ting Lai', 'Chih-Hsuan Wei', 'Cecilia N Arighi', 'Zhiyong Lu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.04263,Anomali
RuBioRoBERTa: a pre-trained biomedical language model for Russian language biomedical text mining,"This paper presents several BERT-based models for Russian language biomedicaltextmining(RuBioBERT, RuBioRoBERTa). The models are pre-trained on a corpus of freely availabletextsin the Russian biomedical domain. With this pre-training, our models demonstrate state-of-the-art results on RuMedBench - Russian medical language understanding benchmark that covers a diverse set of tasks, includingtextclassification, question answering, natural language inference, and named entity recognition.","['Alexander Yalunin', 'Alexander Nesterov', 'Dmitriy Umerenkov']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.03951,Anomali
CrudeOilNews: An Annotated Crude Oil News Corpus for Event Extraction,"In this paper, we present CrudeOilNews, a corpus of English Crude Oil news for event extraction. It is the first of its kind for Commodity News and serve to contribute towards resource building for economic and financialtextmining. This paper describes the data collection process, the annotation methodology and the event typology used in producing the corpus. Firstly, a seed set of 175 news articles were manually annotated, of which a subset of 25 news were used as the adjudicated reference test set for inter-annotator and system evaluation. Agreement was generally substantial and annotator performance was adequate, indicating that the annotation scheme produces consistent event annotations of high quality. Subsequently the dataset is expanded through (1) data augmentation and (2) Human-in-the-loop active learning. The resulting corpus has 425 news articles with approximately 11k events annotated. As part of active learning process, the corpus was used to train basic event extraction models for machine labeling, the resulting models also serve as a validation or as a pilot study demonstrating the use of the corpus in machine learning purposes. The annotated corpus is made available for academic research purpose at https://github.com/meisin/CrudeOilNews-Corpus.","['Meisin Lee', 'Lay-Ki Soon', 'Eu-Gene Siew', 'Ly Fie Sugianto']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.03871,Anomali
tmVar 3.0: an improved variant concept recognition and normalization tool,"Previous studies have shown that automatedtext-miningtools are becoming increasingly important for successfully unlocking variant information in scientific literature at large scale. Despite multiple attempts in the past, existing tools are still of limited recognition scope and precision. We propose tmVar 3.0: an improved variant recognition and normalization tool. Compared to its predecessors, tmVar 3.0 is able to recognize a wide spectrum of variant related entities (e.g., allele and copy number variants), and to group different variant mentions belonging to the same concept in an article for improved accuracy. Moreover, tmVar3 provides additional variant normalization options such as allele-specific identifiers from the ClinGen Allele Registry. tmVar3 exhibits a state-of-the-art performance with over 90% accuracy in F-measure in variant recognition and normalization, when evaluated on three independent benchmarking datasets. tmVar3 is freely available for download. We have also processed the entire PubMed and PMC with tmVar3 and released its annotations on our FTP. Availability: ftp://ftp.ncbi.nlm.nih.gov/pub/lu/tmVar3","['Chih-Hsuan Wei', 'Alexis Allot', 'Kevin Riehle', 'Aleksandar Milosavljevic', 'Zhiyong Lu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.03637,Anomali
Learning Audio-Video Modalities from Image Captions,"A major challenge intext-video andtext-audio retrieval is the lack of large-scale training data. This is unlike image-captioning, where datasets are in the order of millions of samples. To close this gap we propose a new videominingpipeline which involves transferring captions from image captioning datasets to video clips with no additional manual effort. Using this pipeline, we create a new large-scale, weakly labelled audio-video captioning dataset consisting of millions of paired clips and captions. We show that training a multimodal transformed based model on this data achieves competitive performance on video retrieval and video captioning, matching or even outperforming HowTo100M pretraining with 20x fewer clips. We also show that ourminedclips are suitable fortext-audio pretraining, and achieve state of the art results for the task of audio retrieval.","['Arsha Nagrani', 'Paul Hongsuck Seo', 'Bryan Seybold', 'Anja Hauth', 'Santiago Manen', 'Chen Sun', 'Cordelia Schmid']",,arXiv,2022,https://doi.org/10.48550/arXiv.2204.00679,Anomali
A Theoretical Approach for Structuring and Analysing Knowledge Provenance for Visual Analytics,"The primary goal of Visual Analytics (VA) is to enable user-guided knowledge generation. Theoretical VA works to explain how the different aspects of a VA tool bring forth new insights through user interactivity, which itself can be captured through tracking methods for reproduction or evaluation. However, the process of automatically capturing the user's thought process, such as intent and insights, and associating it with user's interaction events are largely ignored. Also, two forms of interactivity capture are typically ambiguous and intermixed: the temporal aspect, which indicates sequences of events, and the atemporal aspect, which explains the workflow as sequences of states within a state-space. In this work, we propose Visual Analytics Knowledge Graph (VAKG), a conceptual framework that brings VA modeling theory to practice through a novel Set-Theory formalization of knowledge modeling. By extracting such a model from a VA tool, VAKG structures a 4-way temporal knowledge graph that describes user behavior and its associated knowledge gain process. Such knowledge graphs can be populated manually or automatically during user analysis sessions, which can then be analyzed using graph analysis methods. VAKG is demonstrated by modeling and collecting Tableau and visualtext-miningworkflows, where comparative user satisfaction, tool efficacy, and overall workflow shortcomings can be extracted from the knowledge graph.","['Leonardo Christino', 'Sima Rezaeipourfarsangi', 'Evangelos Milios', 'Fernando V. Paulovich']",,arXiv,2023,https://doi.org/10.48550/arXiv.2204.00585,Anomali
A Large-scale Dataset of (Open Source) License Text Variants,"We introduce a large-scale dataset of the completetextsof free/open source software (FOSS) license variants. To assemble it we have collected from the Software Heritage archive-the largest publicly available archive of FOSS source code with accompanying development history-all versions of files whose names are commonly used to convey licensing terms to software users and developers.The dataset consists of 6.5 million unique license files that can be used to conduct empirical studies on open source licensing, training of automated license classifiers, natural language processing (NLP) analyses of legaltexts, as well as historical and phylogenetic studies on FOSS licensing. Additional metadata about shipped license files are also provided, making the dataset ready to use in various contexts; they include: file length measures, detected MIME type, detected SPDX license (using ScanCode), example origin (e.g., GitHub repository), oldest public commit in which the license appeared.The dataset is released as open data as an archive file containing all deduplicated license files, plus several portable CSV files for metadata, referencing files via cryptographic checksums.",['Stefano Zacchiroli'],,arXiv,2022,https://doi.org/10.48550/arXiv.2204.00256,Anomali
Biclustering Algorithms Based on Metaheuristics: A Review,"Biclustering is an unsupervised machine learning technique that simultaneously clusters rows and columns in a data matrix. Biclustering has emerged as an important approach and plays an essential role in various applications such as bioinformatics,textmining, and pattern recognition. However, finding significant biclusters is an NP-hard problem that can be formulated as an optimization problem. Therefore, different metaheuristics have been applied to biclustering problems because of their exploratory capability of solving complex optimization problems in reasonable computation time. Although various surveys on biclustering have been proposed, there is a lack of a comprehensive survey on the biclustering problem using metaheuristics. This chapter will present a survey of metaheuristics approaches to address the biclustering problem. The review focuses on the underlying optimization methods and their main search components: representation, objective function, and variation operators. A specific discussion on single versus multi-objective approaches is presented. Finally, some emerging research directions are presented.","['Adan Jose-Garcia', 'Julie Jacques', 'Vincent Sobanski', 'Clarisse Dhaenens']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.16241,Anomali
bitsa_nlp@LT-EDI-ACL2022: Leveraging Pretrained Language Models for Detecting Homophobia and Transphobia in Social Media Comments,"Online social networks are ubiquitous and user-friendly. Nevertheless, it is vital to detect and moderate offensive content to maintain decency and empathy. However,miningsocial mediatextsis a complex task since users don't adhere to any fixed patterns. Comments can be written in any combination of languages and many of them may be low-resource.
  In this paper, we present our system for the LT-EDI shared task on detecting homophobia and transphobia in social media comments. We experiment with a number of monolingual and multilingual transformer based models such as mBERT along with a data augmentation technique for tackling class imbalance. Such pretrained large models have recently shown tremendous success on a variety of benchmark tasks in natural language processing. We observe their performance on a carefully annotated, real life dataset of YouTube comments in English as well as Tamil.
  Our submission achieved ranks 9, 6 and 3 with a macro-averaged F1-score of 0.42, 0.64 and 0.58 in the English, Tamil and Tamil-English subtasks respectively. The code for the system has been open sourced.","['Vitthal Bhandari', 'Poonam Goyal']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.14267,Anomali
Knowledge Mining with Scene Text for Fine-Grained Recognition,"Recently, the semantics of scenetexthas been proven to be essential in fine-grained image classification. However, the existing methods mainly exploit the literal meaning of scenetextfor fine-grained recognition, which might be irrelevant when it is not significantly related to objects/scenes. We propose an end-to-end trainable network thatminesimplicit contextual knowledge behind scenetextimage and enhance the semantics and correlation to fine-tune the image representation. Unlike the existing methods, our model integrates three modalities: visual feature extraction,textsemantics extraction, and correlating background knowledge to fine-grained image classification. Specifically, we employ KnowBert to retrieve relevant knowledge for semantic representation and combine it with image features for fine-grained classification. Experiments on two benchmark datasets, Con-Text, and Drink Bottle, show that our method outperforms the state-of-the-art by 3.72\% mAP and 5.39\% mAP, respectively. To further validate the effectiveness of the proposed method, we create a new dataset on crowd activity recognition for the evaluation. The source code and new dataset of this work are available at https://github.com/lanfeng4659/KnowledgeMiningWithSceneText.","['Hao Wang', 'Junchao Liao', 'Tianheng Cheng', 'Zewen Gao', 'Hao Liu', 'Bo Ren', 'Xiang Bai', 'Wenyu Liu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.14215,Anomali
Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?,"Identifying argument components from unstructuredtextsand predicting the relationships expressed among them are two primary steps of argumentmining. The intrinsic complexity of these tasks demands powerful learning models. While pretrained Transformer-based Language Models (LM) have been shown to provide state-of-the-art results over different NLP tasks, the scarcity of manually annotated data and the highly domain-dependent nature of argumentation restrict the capabilities of such models. In this work, we propose a novel transfer learning strategy to overcome these challenges. We utilize argumentation-rich social discussions from the ChangeMyView subreddit as a source of unsupervised, argumentative discourse-aware knowledge by finetuning pretrained LMs on a selectively masked language modeling task. Furthermore, we introduce a novel prompt-based strategy for inter-component relation prediction that compliments our proposed finetuning method while leveraging on the discourse context. Exhaustive experiments show the generalization capability of our method on these two tasks over within-domain as well as out-of-domain datasets, outperforming several existing and employed strong baselines.","['Subhabrata Dutta', 'Jeevesh Juneja', 'Dipankar Das', 'Tanmoy Chakraborty']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.12881,Anomali
CLIP meets GamePhysics: Towards bug identification in gameplay videos using zero-shot transfer learning,"Gameplay videos contain rich information about how players interact with the game and how the game responds. Sharing gameplay videos on social media platforms, such as Reddit, has become a common practice for many players. Often, players will share gameplay videos that showcase video game bugs. Such gameplay videos are software artifacts that can be utilized for game testing, as they provide insight for bug analysis. Although large repositories of gameplay videos exist, parsing andminingthem in an effective and structured fashion has still remained a big challenge. In this paper, we propose a search method that accepts any Englishtextquery as input to retrieve relevant videos from large repositories of gameplay videos. Our approach does not rely on any external information (such as video metadata); it works solely based on the content of the video. By leveraging the zero-shot transfer capabilities of the Contrastive Language-Image Pre-Training (CLIP) model, our approach does not require any data labeling or training. To evaluate our approach, we present the $\texttt{GamePhysics}$ dataset consisting of 26,954 videos from 1,873 games, that were collected from the GamePhysics section on the Reddit website. Our approach shows promising results in our extensive analysis of simple queries, compound queries, and bug queries, indicating that our approach is useful for object and event detection in gameplay videos. An example application of our approach is as a gameplay video search engine to aid in reproducing video game bugs. Please visit the following link for the code and the data: https://asgaardlab.github.io/CLIPxGamePhysics/","['Mohammad Reza Taesiri', 'Finlay Macklon', 'Cor-Paul Bezemer']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.11096,Anomali
Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering,"Miningaligned natural language (NL) and programming language (PL) pairs is a critical task to NL-PL understanding. Existing methods applied specialized hand-crafted features or separately-trained models for each PL. However, they usually suffered from low transferability across multiple PLs, especially for niche PLs with less annotated data. Fortunately, a Stack Overflow answer post is essentially a sequence oftextand code blocks and its global textual context can provide PL-agnostic supplementary information. In this paper, we propose a Sequence Labeling based Question Answering (SLQA) method tomineNL-PL pairs in a PL-agnostic manner. In particular, we propose to apply the BIO tagging scheme instead of the conventional binary scheme tominethe code solutions which are often composed of multiple blocks of a post. Experiments on current single-PL single-block benchmarks and a manually-labeled cross-PL multi-block benchmark prove the effectiveness and transferability of SLQA. We further present a parallel NL-PL corpus named Lang2Code automaticallyminedwith SLQA, which contains about 1.4M pairs on 6 PLs. Under statistical analysis and downstream evaluation, we demonstrate that Lang2Code is a large-scale high-quality data resource for further NL-PL research.","['Changran Hu', 'Akshara Reddi Methukupalli', 'Yutong Zhou', 'Chen Wu', 'Yubo Chen']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.10744,Anomali
An Exploratory Study on Refactoring Documentation in Issues Handling,"Understanding the practice of refactoring documentation is of paramount importance in academia and industry. Issue tracking systems are used by most software projects enabling developers, quality assurance, managers, and users to submit feature requests and other tasks such as bug fixing and code review. Although recent studies explored how to document refactoring in commit messages, little is known about how developers describe their refactoring needs in issues. In this study, we aim at exploring developer-reported refactoring changes in issues to better understand what developers consider to be problematic in their code and how they handle it. Our approach relies ontextmining45,477 refactoring-related issues and identifying refactoring patterns from a diverse corpus of 77 Java projects by investigating issues associated with 15,833 refactoring operations and developers' explicit refactoring intention. Our results show that (1) developers mostly use move refactoring related terms/phrases to target refactoring-related issues; and (2) developers tend to explicitly mention the improvement of specific quality attributes and focus on duplicate code removal. We envision our findings enabling tool builders to support developers with automated documentation of refactoring changes in issues.","['Eman Abdullah AlOmar', 'Anthony Peruma', 'Mohamed Wiem Mkaouer', 'Christian D. Newman', 'Ali Ouni']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.10221,Anomali
Equitable Ability Estimation in Neurodivergent Student Populations with Zero-Inflated Learner Models,"At present, the educational dataminingcommunity lacks many tools needed for ensuring equitable ability estimation for Neurodivergent (ND) learners. On one hand, most learner models are susceptible to under-estimating ND ability since confounding contexts cannot be held accountable (eg consider dyslexia andtext-heavy assessments), and on the other, few (if any) existing datasets are suited for appraising model and data bias in ND contexts. In this paper we attempt to model the relationships between context (delivery and response types) and performance of ND students with zero-inflated learner models. This approach facilitates simulation of several expected ND behavioural traits, provides equitable ability estimates across all student groups from generated datasets, increases interpretability confidence, and can significantly increase the quality of learning opportunities for ND students. Our approach consistently out-performs baselines in our experiments and can also be applied to many other learner modelling frameworks.","['Niall Twomey', 'Sarah McMullan', 'Anat Elhalal', 'Rafael Poyiadzi', 'Luis Vaquero']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.10170,Anomali
Learning video retrieval models with relevance-aware online mining,"Due to the amount of videos and related captions uploaded every hour, deep learning-based solutions for cross-modal video retrieval are attracting more and more attention. A typical approach consists in learning a jointtext-video embedding space, where the similarity of a video and its associated caption is maximized, whereas a lower similarity is enforced with all the other captions, called negatives. This approach assumes that only the video and caption pairs in the dataset are valid, but different captions - positives - may also describe its visual contents, hence some of them may be wrongly penalized. To address this shortcoming, we propose the Relevance-Aware Negatives and Positivesmining(RANP) which, based on the semantics of the negatives, improves their selection while also increasing the similarity of other valid positives. We explore the influence of these techniques on two video-textdatasets: EPIC-Kitchens-100 and MSR-VTT. By using the proposed techniques, we achieve considerable improvements in terms of nDCG and mAP, leading to state-of-the-art results, e.g. +5.3% nDCG and +3.0% mAP on EPIC-Kitchens-100. We share code and pretrained models at \url{https://github.com/aranciokov/ranp}.","['Alex Falcon', 'Giuseppe Serra', 'Oswald Lanz']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.08688,Anomali
Neural Topic Modeling with Deep Mutual Information Estimation,"The emerging neural topic models make topic modeling more easily adaptable and extendable in unsupervisedtextmining. However, the existing neural topic models is difficult to retain representative information of the documents within the learnt topic representation. In this paper, we propose a neural topic model which incorporates deep mutual information estimation, i.e., Neural Topic Modeling with Deep Mutual Information Estimation(NTM-DMIE). NTM-DMIE is a neural network method for topic learning which maximizes the mutual information between the input documents and their latent topic representation. To learn robust topic representation, we incorporate the discriminator to discriminate negative examples and positive examples via adversarial learning. Moreover, we use both global and local mutual information to preserve the rich information of the input documents in the topic representation. We evaluate NTM-DMIE on several metrics, including accuracy oftextclustering, with topic representation, topic uniqueness and topic coherence. Compared to the existing methods, the experimental results show that NTM-DMIE can outperform in all the metrics on the four datasets.","['Kang Xu', 'Xiaoqiu Lu', 'Yuan-fang Li', 'Tongtong Wu', 'Guilin Qi', 'Ning Ye', 'Dong Wang', 'Zheng Zhou']",,arXiv,2022,https://doi.org/10.48550/arXiv.2203.06298,Anomali
Enhancing Legal Argument Mining with Domain Pre-training and Neural Networks,"The contextual word embedding model, BERT, has proved its ability on downstream tasks with limited quantities of annotated data. BERT and its variants help to reduce the burden of complex annotation work in many interdisciplinary research areas, for example, legal argumentminingin digital humanities. Argumentminingaims to developtextanalysis tools that can automatically retrieve arguments and identify relationships between argumentation clauses. Since argumentation is one of the key aspects of case law, argumentminingtools for legaltextsare applicable to both academic and non-academic legal research. Domain-specific BERT variants (pre-trained with corpora from a particular background) have also achieved strong performance in many tasks. To our knowledge, previous machine learning studies of argumentminingon judicial case law still heavily rely on statistical models. In this paper, we provide a broad study of both classic and contextual embedding models and their performance on practical case law from the European Court of Human Rights (ECHR). During our study, we also explore a number of neural networks when being combined with different embeddings. Our experiments provide a comprehensive overview of a variety of approaches to the legal argumentminingtask. We conclude that domain pre-trained transformer models have great potential in this area, although traditional embeddings can also achieve strong performance when combined with additional neural network layers.","['Gechuan Zhang', 'Paul Nulty', 'David Lillis']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.13457,Anomali
A Systematic Literature Review about Idea Mining: The Use of Machine-driven Analytics to Generate Ideas,"Idea generation is the core activity of innovation. Digital data sources, which are sources of innovation, such as patents, publications, social media, websites, etc., are increasingly growing at unprecedented volume. Manual idea generation is time-consuming and is affected by the subjectivity of the individuals involved. Therefore, the use machine-driven data analytics techniques to analyze data to generate ideas and support idea generation by serving users is useful. The objective of this study is to study state-of the-art machine-driven analytics for idea generation and data sources, hence the result of this study will generally server as a guideline for choosing techniques and data sources. A systematic literature review is conducted to identify relevant scholarly literature from IEEE, Scopus, Web of Science and Google Scholar. We selected a total of 71 articles and analyzed them thematically. The results of this study indicate that idea generation through machine-driven analytics appliestextmining, information retrieval (IR), artificial intelligence (AI), deep learning, machine learning, statistical techniques, natural language processing (NLP), NLP-based morphological analysis, network analysis, and bibliometric to support idea generation. The results include a list of techniques and procedures in idea generation through machine-driven idea analytics. Additionally, characterization and heuristics used in idea generation are summarized. For the future, tools designed to generate ideas could be explored.","['Workneh Y. Ayele', 'Gustaf Juell-Skielse']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.12826,Anomali
Mining Naturally-occurring Corrections and Paraphrases from Wikipedia's Revision History,"Naturally-occurring instances of linguistic phenomena are important both for training and for evaluating automatic processes ontext. When available in large quantities, they also prove interesting material for linguistic studies. In this article, we present a new resource built from Wikipedia's revision history, called WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), which contains numerous editings by human contributors, including various corrections and rewritings. We discuss the main motivations for building such a resource, describe how it was built and present initial applications on French.","['Aurélien Max', 'Guillaume Wisniewski']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.12575,Anomali
A new LDA formulation with covariates,"The Latent Dirichlet Allocation (LDA) model is a popular method for creating mixed-membership clusters. Despite having been originally developed fortextanalysis, LDA has been used for a wide range of other applications. We propose a new formulation for the LDA model which incorporates covariates. In this model, a negative binomial regression is embedded within LDA, enabling straight-forward interpretation of the regression coefficients and the analysis of the quantity of cluster-specific elements in each sampling units (instead of the analysis being focused on modeling the proportion of each cluster, as in Structural Topic Models). We use slice sampling within a Gibbs sampling algorithm to estimate model parameters. We rely on simulations to show how our algorithm is able to successfully retrieve the true parameter values and the ability to make predictions for the abundance matrix using the information given by the covariates. The model is illustrated using real data sets from three different areas:text-miningof Coronavirus articles, analysis of grocery shopping baskets, and ecology of tree species on Barro Colorado Island (Panama). This model allows the identification of mixed-membership clusters in discrete data and provides inference on the relationship between covariates and the abundance of these clusters.","['Gilson Shimizu', 'Rafael Izbicki', 'Denis Valle']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.11527,Anomali
Deep Graph Learning for Anomalous Citation Detection,"Anomaly detection is one of the most active research areas in various critical domains, such as healthcare, fintech, and public security. However, little attention has been paid to scholarly data, i.e., anomaly detection in a citation network. Citation is considered as one of the most crucial metrics to evaluate the impact of scientific research, which may be gamed in multiple ways. Therefore, anomaly detection in citation networks is of significant importance to identify manipulation and inflation of citations. To address this open issue, we propose a novel deep graph learning model, namely GLAD (Graph Learning for Anomaly Detection), to identify anomalies in citation networks. GLAD incorporatestextsemanticminingto network representation learning by adding both node attributes and link attributes via graph neural networks. It exploits not only the relevance of citation contents but also hidden relationships between papers. Within the GLAD framework, we propose an algorithm called CPU (Citation PUrpose) to discover the purpose of citation based on citationtexts. The performance of GLAD is validated through a simulated anomalous citation dataset. Experimental results demonstrate the effectiveness of GLAD on the anomalous citation detection task.","['Jiaying Liu', 'Feng Xia', 'Xu Feng', 'Jing Ren', 'Huan Liu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.11360,Anomali
SemEval 2022 Task 12: Symlink- Linking Mathematical Symbols to their Descriptions,"Given the increasing number of livestreaming videos, automatic speech recognition and post-processing for livestreaming video transcripts are crucial for efficient data management as well as knowledgemining. A key step in this process is punctuation restoration which restores fundamentaltextstructures such as phrase and sentence boundaries from the video transcripts. This work presents a new human-annotated corpus, called BehancePR, for punctuation restoration in livestreaming video transcripts. Our experiments on BehancePR demonstrate the challenges of punctuation restoration for this domain. Furthermore, we show that popular natural language processing toolkits are incapable of detecting sentence boundary on non-punctuated transcripts of livestreaming videos, calling for more research effort to develop robust models for this area.","['Viet Dac Lai', 'Amir Pouran Ben Veyseh', 'Franck Dernoncourt', 'Thien Huu Nguyen']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.09695,Anomali
XAI in the context of Predictive Process Monitoring: Too much to Reveal,"Predictive Process Monitoring (PPM) has been integrated into processminingtools as a value-adding task. PPM provides useful predictions on the further execution of the running business processes. To this end, machine learning-based techniques are widely employed in the context of PPM. In order to gain stakeholders trust and advocacy of PPM predictions, eXplainable Artificial Intelligence (XAI) methods are employed in order to compensate for the lack of transparency of most efficient predictive models. Even when employed under the same settings regarding data, preprocessing techniques, and ML models, explanations generated by multiple XAI methods differ profoundly. A comparison is missing to distinguish XAI characteristics or underlying conditions that are deterministic to an explanation. To address this gap, we provide a framework to enable studying the effect of different PPM-related settings and ML model-related choices on characteristics and expressiveness of resulting explanations. In addition, we compare how different explainability methods characteristics can shape resulting explanations and enable reflecting underlying model reasoning process","['Ghada Elkhawaga', 'Mervat Abuelkheir', 'Manfred Reichert']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.08265,Anomali
Documentation based Semantic-Aware Log Parsing,"With the recent advances of deep learning techniques, there are rapidly growing interests in applying machine learning to log data. As a fundamental part of log analytics, accurate log parsing that transforms raw logs to structured events is critical for subsequent machine learning and dataminingtasks. Previous approaches either analyze the source code for parsing or are data-driven such astextclustering. They largely neglect to exploit another widely available and valuable resource, software documentation that provides detailed explanations for the messages, to improve accuracy. In this paper, we propose an approach and system framework to use documentation knowledge for log parsing. With parameter value identification, it not only can improve the parsing accuracy for documented messages but also for undocumented messages. In addition, it can discover the linkages between event templates that are established by sharing parameters and indicate the correlation of the event context.","['Lei Yu', 'Tian Wu', 'Jiaqi Li', 'Patrick Chan', 'Hong Min', 'Fanjing Meng']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.07169,Anomali
An Adaptive Deep Clustering Pipeline to Inform Text Labeling at Scale,"Miningthe latent intentions from large volumes of natural language inputs is a key step to help data analysts design and refine Intelligent Virtual Assistants (IVAs) for customer service and sales support. We created a flexible and scalable clustering pipeline within the Verint Intent Manager (VIM) that integrates the fine-tuning of language models, a high performing k-NN library and community detection techniques to help analysts quickly surface and organize relevant user intentions from conversationaltexts. The fine-tuning step is necessary because pre-trained language models cannot encodetextsto efficiently surface particular clustering structures when the targettextsare from an unseen domain or the clustering task is not topic detection. We describe the pipeline and demonstrate its performance and ability to scale on three real-worldtextminingtasks. As deployed in the VIM application, this clustering pipeline produces high quality results, improving the performance of data analysts and reducing the time it takes to surface intentions from customer service data, thereby reducing the time it takes to build and deploy IVAs in new domains.","['Xinyu Chen', 'Ian Beaver']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.01211,Anomali
A Semi-Supervised Deep Clustering Pipeline for Mining Intentions From Texts,"Miningthe latent intentions from large volumes of natural language inputs is a key step to help data analysts design and refine Intelligent Virtual Assistants (IVAs) for customer service. To aid data analysts in this task we present Verint Intent Manager (VIM), an analysis platform that combines unsupervised and semi-supervised approaches to help analysts quickly surface and organize relevant user intentions from conversationaltexts. For the initial exploration of data we make use of a novel unsupervised and semi-supervised pipeline that integrates the fine-tuning of high performing language models, a distributed k-NN graph building method and community detection techniques forminingthe intentions and topics fromtexts. The fine-tuning step is necessary because pre-trained language models cannot encodetextsto efficiently surface particular clustering structures when the targettextsare from an unseen domain or the clustering task is not topic detection. For flexibility we deploy two clustering approaches: where the number of clusters must be specified and where the number of clusters is detected automatically with comparable clustering quality but at the expense of additional computation time. We describe the application and deployment and demonstrate its performance using BERT on threetextminingtasks. Our experiments show that BERT begins to produce better task-aware representations using a labeled subset as small as 0.5% of the task data. The clustering quality exceeds the state-of-the-art results when BERT is fine-tuned with labeled subsets of only 2.5% of the task data. As deployed in the VIM application, this flexible clustering pipeline produces high quality results, improving the performance of data analysts and reducing the time it takes to surface intentions from customer service data, thereby reducing the time it takes to build and deploy IVAs in new domains.","['Xinyu Chen', 'Ian Beaver']",,arXiv,2022,https://doi.org/10.48550/arXiv.2202.00802,Anomali
Boosting Entity Mention Detection for Targetted Twitter Streams with Global Contextual Embeddings,"Microblogging sites, like Twitter, have emerged as ubiquitous sources of information. Two important tasks related to the automatic extraction and analysis of information in Microblogs are Entity Mention Detection (EMD) and Entity Detection (ED). The state-of-the-art EMD systems aim to model the non-literary nature of microblogtextby training upon offline static datasets. They extract a combination of surface-level features -- orthographic, lexical, and semantic -- from individual messages for noisytextmodeling and entity extraction. But given the constantly evolving nature of microblog streams, detecting all entity mentions from such varying yet limited context of short messages remains a difficult problem. To this end, we propose a framework named EMD Globalizer, better suited for the execution of EMD learners on microblog streams. It deviates from the processing of isolated microblog messages by existing EMD systems, where learned knowledge from the immediate context of a message is used to suggest entities. After an initial extraction of entity candidates by an EMD system, the proposed framework leverages occurrenceminingto find additional candidate mentions that are missed during this first detection. Aggregating the local contextual representations of these mentions, a global embedding is drawn from the collective context of an entity candidate within a stream. The global embeddings are then utilized to separate entities within the candidates from false positives. All mentions of said entities from the stream are produced in the framework's final outputs. Our experiments show that EMD Globalizer can enhance the effectiveness of all existing EMD systems that we tested (on average by 25.61%) with a small additional computational overhead.","['Satadisha Saha Bhowmick', 'Eduard C. Dragut', 'Weiyi Meng']",,arXiv,2022,https://doi.org/10.48550/arXiv.2201.11885,Anomali
Learning Deep Semantic Model for Code Search using CodeSearchNet Corpus,"Semantic code search is the task of retrieving relevant code snippet given a natural language query. Different from typical information retrieval tasks, code search requires to bridge the semantic gap between the programming language and natural language, for better describing intrinsic concepts and semantics. Recently, deep neural network for code search has been a hot research topic. Typical methods for neural code search first represent the code snippet and querytextas separate embeddings, and then use vector distance (e.g. dot-product or cosine) to calculate the semantic similarity between them. There exist many different ways for aggregating the variable length of code or query tokens into a learnable embedding, including bi-encoder, cross-encoder, and poly-encoder. The goal of the query encoder and code encoder is to produce embeddings that are close with each other for a related pair of query and the corresponding desired code snippet, in which the choice and design of encoder is very significant.
  In this paper, we propose a novel deep semantic model which makes use of the utilities of not only the multi-modal sources, but also feature extractors such as self-attention, the aggregated vectors, combination of the intermediate representations. We apply the proposed model to tackle the CodeSearchNet challenge about semantic code search. We align cross-lingual embedding for multi-modality learning with large batches and hard examplemining, and combine different learned representations for better enhancing the representation learning. Our model is trained on CodeSearchNet corpus and evaluated on the held-out data, the final model achieves 0.384 NDCG and won the first place in this benchmark. Models and code are available at https://github.com/overwindows/SemanticCodeSearch.git.","['Chen Wu', 'Ming Yan']",,arXiv,2022,https://doi.org/10.48550/arXiv.2201.11313,Anomali
Personality Type Based on Myers-Briggs Type Indicator with Text Posting Style by using Traditional and Deep Learning,"The term personality may be expressed in terms of the individual differences in characteristics pattern of thinking, feeling, and behavior. This work presents several machine learning techniques including Naive Bayes, Support Vector Machines, and Recurrent Neural Networks to predict people personality fromtextbased on Myers-Briggs Type Indicator (MBTI). Furthermore, this project applies CRISP-DM, which stands for Cross-Industry Standard Process for DataMining, to guide the learning process. Since, CRISP-DM is kind of iterative development, we have adopted it with agile methodology, which is a rapid iterative software development method, in order to reduce the development cycle to be minimal.","['Sakdipat Ontoum', 'Jonathan H. Chan']",,arXiv,2022,https://doi.org/10.48550/arXiv.2201.08717,Anomali
Data-to-Value: An Evaluation-First Methodology for Natural Language Projects,"Big data, i.e. collecting, storing and processing of data at scale, has recently been possible due to the arrival of clusters of commodity computers powered by application-level distributed parallel operating systems like HDFS/Hadoop/Spark, and such infrastructures have revolutionized dataminingat scale. For dataminingproject to succeed more consistently, some methodologies were developed (e.g. CRISP-DM, SEMMA, KDD), but these do not account for (1) very large scales of processing, (2) dealing with textual (unstructured) data (i.e. Natural Language Processing (NLP, ""textanalytics""), and (3) non-technical considerations (e.g. legal, ethical, project managerial aspects).
  To address these shortcomings, a new methodology, called ""Data to Value"" (D2V), is introduced, which is guided by a detailed catalog of questions in order to avoid a disconnect of big datatextanalytics project team with the topic when facing rather abstract box-and-arrow diagrams commonly associated with methodologies.",['Jochen L. Leidner'],,arXiv,2022,https://doi.org/10.48550/arXiv.2201.07725,Anomali
Sectioning of Biomedical Abstracts: A Sequence of Sequence Classification Task,"Rapid growth of the biomedical literature has led to many advances in the biomedicaltextminingfield. Among the vast amount of information, biomedical article abstracts are the easily accessible sources. However, the number of the structured abstracts, describing the rhetorical sections with one of Background, Objective, Method, Result and Conclusion categories is still not considerable. Exploration of valuable information in the biomedical abstracts can be expedited with the improvements in the sequential sentence classification task. Deep learning based models has great performance/potential in achieving significant results in this task. However, they can often be overly complex and overfit to specific data. In this project, we study a state-of-the-art deep learning model, which we called SSN-4 model here. We investigate different components of the SSN-4 model to study the trade-off between the performance and complexity. We explore how well this model generalizes to a new data set beyond Randomized Controlled Trials (RCT) dataset. We address the question that whether word embeddings can be adjusted to the task to improve the performance. Furthermore, we develop a second model that addresses the confusion pairs in the first model. Results show that SSN-4 model does not appear to generalize well beyond RCT dataset.","['Mehmet Efruz Karabulut', 'K. Vijay-Shanker']",,arXiv,2022,https://doi.org/10.48550/arXiv.2201.07112,Anomali
Unveiling music genre structure through common-interest communities,"Using a dataset of more than 90,000 metal music reviews written by over 9,000 users in a period of 15 years, we analyse the genre structure of metal music with the aid of reviewtextinformation. We model the relationships between genres using a user-oriented network, based on the written reviews. We then perform community detection and employ a network ""averaging"" method to obtain stable genre clusters, in order to analyse the structures of clusters both locally within each cluster and globally over the entire network. In addition to identifying the clusters, we use Dependency Parsing and modified Term Frequency - Inverse Document Frequency to extract significant and unique features of each cluster. These structures and reviewtextinformation can allow us to understand how music audience (fans) perceive similar and different genres, and also assist in classifying different genres which share common-interest user communities, offering a more objective way in grouping music genres. Furthermore, the classification can also help recommendation engines provide more targeted suggestions of music, and potentially help musicians to select genre labels for their music, and design music to better cater to preferences of their audiences based on previous reviews.","['Zhiheng Jiang', 'Hoai Nguyen Huynh']","Social Network Analysis and Mining, 12:35 (2022)",arXiv,2022,https://doi.org/10.48550/arXiv.2201.06842,Anomali
Attention over Self-attention:Intention-aware Re-ranking with Dynamic Transformer Encoders for Recommendation,"Re-ranking models refine item recommendation lists generated by the prior global ranking model, which have demonstrated their effectiveness in improving the recommendation quality. However, most existing re-ranking solutions only learn from implicit feedback with a shared prediction model, which regrettably ignore inter-item relationships under diverse user intentions. In this paper, we propose a novel Intention-aware Re-ranking Model with Dynamic Transformer Encoder (RAISE), aiming to perform user-specific prediction for each individual user based on her intentions. Specifically, we first propose tominelatent user intentions fromtextreviews with an intention discovering module (IDM). By differentiating the importance of review information with a co-attention network, the latent user intention can be explicitly modeled for each user-item pair. We then introduce a dynamic transformer encoder (DTE) to capture user-specific inter-item relationships among item candidates by seamlessly accommodating the learned latent user intentions via IDM. As such, one can not only achieve more personalized recommendations but also obtain corresponding explanations by constructing RAISE upon existing recommendation engines. Empirical study on four public datasets shows the superiority of our proposed RAISE, with up to 13.95%, 9.60%, and 13.03% relative improvements evaluated by Precision@5, MAP@5, and NDCG@5 respectively.","['Zhuoyi Lin', 'Sheng Zang', 'Rundong Wang', 'Zhu Sun', 'J. Senthilnath', 'Chi Xu', 'Chee-Keong Kwoh']",,arXiv,2023,https://doi.org/10.48550/arXiv.2201.05333,Anomali
Multiple Genome Analytics Framework: The Case of All SARS-CoV-2 Complete Variants,"Pattern detection and string matching are fundamental problems in computer science and the accelerated expansion of bioinformatics and computational biology have made them a core topic for both disciplines. The SARS-CoV-2 pandemic has made such problems more demanding with hundreds or thousands of new genome variants discovered every week, because of constant mutations, and there is a desperate need for fast and accurate analyses. The requirement for computational tools for genomic analyses, such as sequence alignment, is very important, although, in most cases the resources and computational power required are enormous. The presented Multiple Genome Analytics Framework combines data structures and algorithms, specifically built fortextminingand pattern detection, that can help to efficiently address several computational biology and bioinformatics problems concurrently with minimal resources. A single execution of advanced algorithms, with space and time complexity O(nlogn), is enough to acquire knowledge on all repeated patterns that exist in multiple genome sequences and this information can be used from other meta-algorithms for further meta-analyses. The potential of the proposed framework is demonstrated with the analysis of more than 300,000 SARS-CoV-2 genome sequences and the detection of all repeated patterns with length up to 60 nucleotides in these sequences. These results have been used to provide answers to questions such as common patterns among all variants, sequence alignment, palindromes and tandem repeats detection, different organism genome comparisons, polymerase chain reaction primers detection, etc.",['Konstantinos Xylogiannopoulos'],,arXiv,2022,https://doi.org/10.48550/arXiv.2201.05198,Anomali
Promoting and countering misinformation during Australia's 2019-2020 bushfires: A case study of polarisation,"During Australia's unprecedented bushfires in 2019-2020, misinformation blaming arson resurfaced on Twitter using #ArsonEmergency. The extent to which bots were responsible for disseminating and amplifying this misinformation has received scrutiny in the media and academic research. Here we study Twitter communities spreading this misinformation during the population-level event, and investigate the role of online communities and bots. Our in-depth investigation of the dynamics of the discussion uses a phased approach -- before and after reporting of bots promoting the hashtag was broadcast by the mainstream media. Though we did not find many bots, the most bot-like accounts were social bots, which present as genuine humans. Further, we distilled meaningful quantitative differences between two polarised communities in the Twitter discussion, resulting in the following insights. First, Supporters of the arson narrative promoted misinformation by engaging others directly with replies and mentions using hashtags and links to external sources. In response, Opposers retweeted fact-based articles and official information. Second, Supporters were embedded throughout their interaction networks, but Opposers obtained high centrality more efficiently despite their peripheral positions. By the last phase, Opposers and unaffiliated accounts appeared to coordinate, potentially reaching a broader audience. Finally, unaffiliated accounts shared the same URLs as Opposers over Supporters by a ratio of 9:1 in the last phase, having shared mostly Supporter URLs in the first phase. This foiled Supporters' efforts, highlighting the value of exposing misinformation campaigns. We speculate that the communication strategies observed here could be discoverable in other misinformation-related discussions and could inform counter-strategies.","['Derek Weber', 'Lucia Falzon', 'Lewis Mitchell', 'Mehwish Nasim']",,arXiv,2022,https://doi.org/10.1007/978-3-030-61841-4_11,Anomali
Differentially Private Release of Event Logs for Process Mining,"The applicability of processminingtechniques hinges on the availability of event logs capturing the execution of a business process. In some use cases, particularly those involving customer-facing processes, these event logs may contain private information. Data protection regulations restrict the use of such event logs for analysis purposes. One way of circumventing these restrictions is to anonymize the event log to the extent that no individual can be singled out using the anonymized log. This article addresses the problem of anonymizing an event log in order to guarantee that, upon release of the anonymized log, the probability that an attacker may single out any individual represented in the original log does not increase by more than a threshold. The article proposes a differentially private release mechanism, which samples the cases in the log and adds noise to the timestamps to the extent required to achieve the above privacy guarantee. The article reports on an empirical comparison of the proposed approach against the state-of-the-art approaches using 14 real-life event logs in terms of data utility loss and computational efficiency.","['Gamal Elkoumy', 'Alisa Pankova', 'Marlon Dumas']",,arXiv,2022,https://doi.org/10.48550/arXiv.2201.03010,Anomali
Large-scale protein-protein post-translational modification extraction with distant supervision and confidence calibrated BioBERT,"Protein-protein interactions (PPIs) are critical to normal cellular function and are related to many disease pathways. However, only 4% of PPIs are annotated with PTMs in biological knowledge databases such as IntAct, mainly performed through manual curation, which is neither time nor cost-effective. We use the IntAct PPI database to create a distant supervised dataset annotated with interacting protein pairs, their corresponding PTM type, and associated abstracts from the PubMed database. We train an ensemble of BioBERT models - dubbed PPI-BioBERT-x10 to improve confidence calibration. We extend the use of ensemble average confidence approach with confidence variation to counteract the effects of class imbalance to extract high confidence predictions. The PPI-BioBERT-x10 model evaluated on the test set resulted in a modest F1-micro 41.3 (P =5 8.1, R = 32.1). However, by combining high confidence and low variation to identify high quality predictions, tuning the predictions for precision, we retained 19% of the test predictions with 100% precision. We evaluated PPI-BioBERT-x10 on 18 million PubMed abstracts and extracted 1.6 million (546507 unique PTM-PPI triplets) PTM-PPI predictions, and filter ~ 5700 (4584 unique) high confidence predictions. Of the 5700, human evaluation on a small randomly sampled subset shows that the precision drops to 33.7% despite confidence calibration and highlights the challenges of generalisability beyond the test set even with confidence calibration. We circumvent the problem by only including predictions associated with multiple papers, improving the precision to 58.8%. In this work, we highlight the benefits and challenges of deep learning-basedtextminingin practice, and the need for increased emphasis on confidence calibration to facilitate human curation efforts.","['Aparna Elangovan', 'Yuan Li', 'Douglas E. V. Pires', 'Melissa J. Davis', 'Karin Verspoor']","BMC Bioinformatics 23, 4 (2022)",arXiv,2022,https://doi.org/10.48550/arXiv.2201.02229,Anomali
"An Opinion Mining of Text in COVID-19 Issues along with Comparative Study in ML, BERT & RNN","The global world is crossing a pandemic situation where this is a catastrophic outbreak of Respiratory Syndrome recognized as COVID-19. This is a global threat all over the 212 countries that people every day meet with mighty situations. On the contrary, thousands of infected people live rich in mountains. Mental health is also affected by this worldwide coronavirus situation. Due to this situation online sources made a communicative place that common people shares their opinion in any agenda. Such as affected news related positive and negative, financial issues, country and family crisis, lack of import and export earning system etc. different kinds of circumstances are recent trendy news in anywhere. Thus, vast amounts oftextare produced within moments therefore, in subcontinent areas the same as situation in other countries and peoples opinion oftextand situation also same but the language is different. This article has proposed some specific inputs along with Banglatextcomments from individual sources which can assure the goal of illustration that machine learning outcome capable of building an assistive system. Opinionminingassistive system can be impactful in all language preferences possible. To the best of our knowledge, the article predicted the Bangla inputtexton COVID-19 issues proposed ML algorithms and deep learning models analysis also check the future reachability with a comparative analysis. Comparative analysis states a report ontextprediction accuracy is 91% along with ML algorithms and 79% along with Deep Learning Models.","['Md. Mahadi Hasan Sany', 'Mumenunnesa Keya', 'Sharun Akter Khushbu', 'Akm Shahariar Azad Rabby', 'Abu Kaisar Mohammad Masum']","3rd International Conference on Deep Learning, Artificial Intelligence and Robotics, (ICDLAIR) 2021",arXiv,2022,https://doi.org/10.48550/arXiv.2201.02119,Anomali
Mining Adverse Drug Reactions from Unstructured Mediums at Scale,"Adverse drug reactions / events (ADR/ADE) have a major impact on patient health and health care costs. Detecting ADR's as early as possible and sharing them with regulators, pharma companies, and healthcare providers can prevent morbidity and save many lives. While most ADR's are not reported via formal channels, they are often documented in a variety of unstructured conversations such as social media posts by patients, customer support call transcripts, or CRM notes of meetings between healthcare providers and pharma sales reps. In this paper, we propose a natural language processing (NLP) solution that detects ADR's in such unstructured free-textconversations, which improves on previous work in three ways. First, a new Named Entity Recognition (NER) model obtains new state-of-the-art accuracy for ADR and Drug entity extraction on the ADE, CADEC, and SMM4H benchmark datasets (91.75%, 78.76%, and 83.41% F1 scores respectively). Second, two new Relation Extraction (RE) models are introduced - one based on BioBERT while the other utilizing crafted features over a Fully Connected Neural Network (FCNN) - are shown to perform on par with existing state-of-the-art models, and outperform them when trained with a supplementary clinician-annotated RE dataset. Third, a newtextclassification model, for deciding if a conversation includes an ADR, obtains new state-of-the-art accuracy on the CADEC dataset (86.69% F1 score). The complete solution is implemented as a unified NLP pipeline in a production-grade library built on top of Apache Spark, making it natively scalable and able to process millions of batch or streaming records on commodity clusters.","['Hasham Ul Haq', 'Veysel Kocaman', 'David Talby']",,arXiv,2022,https://doi.org/10.48550/arXiv.2201.01405,Anomali
Improved Topic modeling in Twitter through Community Pooling,"Social networks play a fundamental role in propagation of information and news. Characterizing the content of the messages becomes vital for different tasks, like breaking news detection, personalized message recommendation, fake users detection, information flow characterization and others. However, Twitter posts are short and often less coherent than othertextdocuments, which makes it challenging to applytextminingalgorithms to these datasets efficiently. Tweet-pooling (aggregating tweets into longer documents) has been shown to improve automatic topic decomposition, but the performance achieved in this task varies depending on the pooling method.
  In this paper, we propose a new pooling scheme for topic modeling in Twitter, which groups tweets whose authors belong to the same community (group of users who mainly interact with each other but not with other groups) on a user interaction graph. We present a complete evaluation of this methodology, state of the art schemes and previous pooling models in terms of the cluster quality, document retrieval tasks performance and supervised machine learning classification score. Results show that our Community polling method outperformed other methods on the majority of metrics in two heterogeneous datasets, while also reducing the running time. This is useful when dealing with big amounts of noisy and short user-generated social mediatexts. Overall, our findings contribute to an improved methodology for identifying the latent topics in a Twitter dataset, without the need of modifying the basic machinery of a topic decomposition model.","['Federico Albanese', 'Esteban Feuerstein']",,arXiv,2021,https://doi.org/10.48550/arXiv.2201.00690,Anomali
Understanding Public Opinion on Using Hydroxychloroquine for COVID-19 Treatment via Social Media,"Hydroxychloroquine (HCQ) is used to prevent or treat malaria caused by mosquito bites. Recently, the drug has been suggested to treat COVID-19, but that has not been supported by scientific evidence. The information regarding the drug efficacy has flooded social networks, posting potential threats to the community by perverting their perceptions of the drug efficacy. This paper studies the reactions of social network users on the recommendation of using HCQ for COVID-19 treatment by analyzing the reaction patterns and sentiment of the tweets. We collected 164,016 tweets from February to December 2020 and used atextminingapproach to identify social reaction patterns and opinion change over time. Our descriptive analysis identified an irregularity of the users' reaction patterns associated tightly with the social and news feeds on the development of HCQ and COVID-19 treatment. The study linked the tweets and Google search frequencies to reveal the viewpoints of local communities on the use of HCQ for COVID-19 treatment across different states. Further, our tweet sentiment analysis reveals that public opinion changed significantly over time regarding the recommendation of using HCQ for COVID-19 treatment. The data showed that high support in the early dates but it significantly declined in October. Finally, using the manual classification of 4,850 tweets by humans as our benchmark, our sentiment analysis showed that the Google Cloud Natural Language algorithm outperformed the Valence Aware Dictionary and sEntiment Reasoner in classifying tweets, especially in the sarcastic tweet group.","['Thuy T. Do', 'Du Nguyen', 'Anh Le', 'Anh Nguyen', 'Dong Nguyen', 'Nga Hoang', 'Uyen Le', 'Tuan Tran']",,arXiv,2022,https://doi.org/10.48550/arXiv.2201.00237,Anomali
Temporal Attention Augmented Transformer Hawkes Process,"In recent years,miningthe knowledge from asynchronous sequences by Hawkes process is a subject worthy of continued attention, and Hawkes processes based on the neural network have gradually become the most hotly researched fields, especially based on the recurrence neural network (RNN). However, these models still contain some inherent shortcomings of RNN, such as vanishing and exploding gradient and long-term dependency problems. Meanwhile, Transformer based on self-attention has achieved great success in sequential modeling liketextprocessing and speech recognition. Although the Transformer Hawkes process (THP) has gained huge performance improvement, THPs do not effectively utilize the temporal information in the asynchronous events, for these asynchronous sequences, the event occurrence instants are as important as the types of events, while conventional THPs simply convert temporal information into position encoding and add them as the input of transformer. With this in mind, we come up with a new kind of Transformer-based Hawkes process model, Temporal Attention Augmented Transformer Hawkes Process (TAA-THP), we modify the traditional dot-product attention structure, and introduce the temporal encoding into attention structure. We conduct numerous experiments on a wide range of synthetic and real-life datasets to validate the performance of our proposed TAA-THP model, significantly improvement compared with existing baseline models on the different measurements is achieved, including log-likelihood on the test dataset, and prediction accuracies of event types and occurrence times. In addition, through the ablation studies, we vividly demonstrate the merit of introducing additional temporal attention by comparing the performance of the model with and without temporal attention.","['Lu-ning Zhang', 'Jian-wei Liu', 'Zhi-yan Song', 'Xin Zuo']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.14472,Anomali
Mining and Classifying Privacy and Data Protection Requirements in Issue Reports,"Digital and physical footprints are a trail of user activities collected over the use of software applications and systems. As software becomes ubiquitous, protecting user privacy has become challenging. With the increase of user privacy awareness and advent of privacy regulations and policies, there is an emerging need to implement software systems that enhance the protection of personal data processing. However, existing data protection and privacy regulations provide key principles in high-level, making it difficult for software engineers to design and implement privacy-aware systems. In this paper, we develop a taxonomy that provides a comprehensive set of privacy requirements based on four well-established personal data protection regulations and privacy frameworks, the General Data Protection Regulation (GDPR), ISO/IEC 29100, Thailand Personal Data Protection Act (Thailand PDPA) and Asia-Pacific Economic Cooperation (APEC) privacy framework. These requirements are extracted, refined and classified into a level that can be used to map with issue reports. We have also performed a study on how two large open-source software projects (Google Chrome and Moodle) address the privacy requirements in our taxonomy throughminingtheir issue reports. The paper discusses how the collected issues were classified, and presents the findings and insights generated from our study.Miningand classifying privacy requirements in issue reports can help organisations be aware of their state of compliance by identifying privacy requirements that have not been addressed in their software projects. The taxonomy can also trace back to regulations, standards and frameworks that the software projects have not complied with based on the identified privacy requirements.","['Pattaraporn Sangaroonsilp', 'Hoa Khanh Dam', 'Morakot Choetkiertikul', 'Chaiyong Ragkhitwetsagul', 'Aditya Ghose']",,arXiv,2022,https://doi.org/10.48550/arXiv.2112.13994,Anomali
Deeper Clinical Document Understanding Using Relation Extraction,"The surging amount of biomedical literature & digital clinical records presents a growing need fortextminingtechniques that can not only identify but also semantically relate entities in unstructured data. In this paper we propose atextminingframework comprising of Named Entity Recognition (NER) and Relation Extraction (RE) models, which expands on previous work in three main ways. First, we introduce two new RE model architectures -- an accuracy-optimized one based on BioBERT and a speed-optimized one utilizing crafted features over a Fully Connected Neural Network (FCNN). Second, we evaluate both models on public benchmark datasets and obtain new state-of-the-art F1 scores on the 2012 i2b2 Clinical Temporal Relations challenge (F1 of 73.6, +1.2% over the previous SOTA), the 2010 i2b2 Clinical Relations challenge (F1 of 69.1, +1.2%), the 2019 Phenotype-Gene Relations dataset (F1 of 87.9, +8.5%), the 2012 Adverse Drug Events Drug-Reaction dataset (F1 of 90.0, +6.3%), and the 2018 n2c2 Posology Relations dataset (F1 of 96.7, +0.6%). Third, we show two practical applications of this framework -- for building a biomedical knowledge graph and for improving the accuracy of mapping entities to clinical codes. The system is built using the Spark NLP library which provides a production-grade, natively scalable, hardware-optimized, trainable & tunable NLP framework.","['Hasham Ul Haq', 'Veysel Kocaman', 'David Talby']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.13259,Anomali
Interpretable Knowledge Tracing: Simple and Efficient Student Modeling with Causal Relations,"Intelligent Tutoring Systems have become critically important in future learning environments. Knowledge Tracing (KT) is a crucial part of that system. It is about inferring the skill mastery of students and predicting their performance to adjust the curriculum accordingly. Deep Learning-based KT models have shown significant predictive performance compared with traditional models. However, it is difficult to extract psychologically meaningful explanations from the tens of thousands of parameters in neural networks, that would relate to cognitive theory. There are several ways to achieve high accuracy in student performance prediction but diagnostic and prognostic reasoning is more critical in learning sciences. Since KT problem has few observable features (problem ID and student's correctness at each practice), we extract meaningful latent features from students' response data by using machine learning and dataminingtechniques. In this work, we present Interpretable Knowledge Tracing (IKT), a simple model that relies on three meaningful latent features: individual skill mastery, ability profile (learning transfer across skills), and problem difficulty. IKT's prediction of future student performance is made using a Tree-Augmented Naive Bayes Classifier (TAN), therefore its predictions are easier to explain than deep learning-based student models. IKT also shows better student performance prediction than deep learning-based student models without requiring a huge amount of parameters. We conduct ablation studies on each feature to examine their contribution to student performance prediction. Thus, IKT has great potential for providing adaptive and personalized instructions with causal reasoning in real-world educational systems.","['Sein Minn', 'Jill-Jenn Vie', 'Koh Takeuchi', 'Hisashi Kashima', 'Feida Zhu']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.11209,Anomali
Natural language processing to identify lupus nephritis phenotype in electronic health records,"Systemic lupus erythematosus (SLE) is a rare autoimmune disorder characterized by an unpredictable course of flares and remission with diverse manifestations. Lupus nephritis, one of the major disease manifestations of SLE for organ damage and mortality, is a key component of lupus classification criteria. Accurately identifying lupus nephritis in electronic health records (EHRs) would therefore benefit large cohort observational studies and clinical trials where characterization of the patient population is critical for recruitment, study design, and analysis. Lupus nephritis can be recognized through procedure codes and structured data, such as laboratory tests. However, other critical information documenting lupus nephritis, such as histologic reports from kidney biopsies and prior medical history narratives, require sophisticatedtextprocessing tomineinformation from pathology reports and clinical notes. In this study, we developed algorithms to identify lupus nephritis with and without natural language processing (NLP) using EHR data. We developed four algorithms: a rule-based algorithm using only structured data (baseline algorithm) and three algorithms using different NLP models. The three NLP models are based on regularized logistic regression and use different sets of features including positive mention of concept unique identifiers (CUIs), number of appearances of CUIs, and a mixture of three components respectively. The baseline algorithm and the best performed NLP algorithm were external validated on a dataset from Vanderbilt University Medical Center (VUMC). Our best performing NLP model incorporating features from both structured data, regular expression concepts, and mapped CUIs improved F measure in both the NMEDW (0.41 vs 0.79) and VUMC (0.62 vs 0.96) datasets compared to the baseline lupus nephritis algorithm.","['Yu Deng', 'Jennifer A. Pacheco', 'Anh Chung', 'Chengsheng Mao', 'Joshua C. Smith', 'Juan Zhao', 'Wei-Qi Wei', 'April Barnado', 'Chunhua Weng', 'Cong Liu', 'Adam Cordon', 'Jingzhi Yu', 'Yacob Tedla', 'Abel Kho', 'Rosalind Ramsey-Goldman', 'Theresa Walunas', 'Yuan Luo']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.10821,Anomali
Blockchain Mining with Multiple Selfish Miners,"This paper studies a fundamental problem regarding the security of blockchain PoW consensus on how the existence of multiple misbehaving miners influences the profitability of selfishmining. Each selfish miner (or attacker interchangeably) maintains a private chain and makes it public opportunistically for acquiring more rewards incommensurate to his Hash power. We first establish a general Markov chain model to characterize the state transition of public and private chains for Basic SelfishMining(BSM), and derive the stationary profitable threshold of Hash power in closed-form. It reduces from 25% for a single attacker to below 21.48% for two symmetric attackers theoretically, and further reduces to around 10% with eight symmetric attackers experimentally. We next explore the profitable threshold when one of the attackers performs strategicminingbased on Partially Observable Markov Decision Process (POMDP) that only half of the attributes pertinent to aminingstate are observable to him. An online algorithm is presented to compute the nearly optimal policy efficiently despite the large state space and high dimensional belief space. The strategic attackerminesselfishly and more agilely than BSM attacker when his Hash power is relatively high, andmineshonestly otherwise, thus leading to a much lower profitable threshold. Last, we formulate a simple model of absoluteminingrevenue that yields an interesting observation: selfishminingis never profitable at the first difficulty adjustment period, but replying on the reimbursement of stationary selfishmininggains in the future periods. The delay till being profitable of an attacker increases with the decrease of his Hash power, making blockchain miners more cautious on performing selfishmining.","['Qianlan Bai', 'Yuedong Xu', 'Nianyi Liu', 'Xin Wang']",,arXiv,2023,https://doi.org/10.48550/arXiv.2112.10454,Anomali
DegreEmbed: incorporating entity embedding into logic rule learning for knowledge graph reasoning,"Knowledge graphs (KGs), as structured representations of real world facts, are intelligent databases incorporating human knowledge that can help machine imitate the way of human problem solving. However, KGs are usually huge and there are inevitably missing facts in KGs, thus undermining applications such as question answering and recommender systems that are based on knowledge graph reasoning. Link prediction for knowledge graphs is the task aiming to complete missing facts by reasoning based on the existing knowledge. Two main streams of research are widely studied: one learns low-dimensional embeddings for entities and relations that can explore latent patterns, and the other gains good interpretability bymininglogical rules. Unfortunately, the heterogeneity of modern KGs that involve entities and relations of various types is not well considered in the previous studies. In this paper, we propose DegreEmbed, a model that combines embedding-based learning and logic ruleminingfor inferring on KGs. Specifically, we study the problem of predicting missing links in heterogeneous KGs from the perspective of the degree of nodes. Experimentally, we demonstrate that our DegreEmbed model outperforms the state-of-the-art methods on real world datasets and the rulesminedby our model are of high quality and interpretability.","['Haotian Li', 'Hongri Liu', 'Yao Wang', 'Guodong Xin', 'Yuliang Wei']",,arXiv,2023,https://doi.org/10.48550/arXiv.2112.09933,Anomali
Improving Ethical Outcomes with Machine-in-the-Loop: Broadening Human Understanding of Data Annotations,"We introduce a machine-in-the-loop pipeline that aims to address root causes of unwanted bias in natural language based supervised machine learning tasks in the education domain. Learning from the experiences of students is foundational for education researchers, and academic administrators. 21st-century skills learned from experience are becoming a core part of college and career readiness as well as the hiring process in the new knowledge economy. Minoritized students demonstrate these skills in their daily lives, but documenting, assessing, and validating these skills is a huge problem for educational institutions. As an equity focused online platform, LivedX translates minoritized students' lived experiences into the 21st century skills, issues micro-credentials, and creates personal 21st century skills portfolio. To automate the micro credentialminingfrom the natural languagetextsreceived from the students' submitted essays, we employed a bag-of-word model to construct a multi-output classifier. Despite our goal, our model initially exacerbated disparate impact on minoritized students. We used a machine-in-the-loop model development pipeline to address the problem and refine the aforementioned model to ensure fairness in its prediction.","['Ashis Kumer Biswas', 'Geeta Verma', 'Justin Otto Barber']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.09738,Anomali
Inorganic Synthesis Reaction Condition Prediction with Generative Machine Learning,"Data-driven synthesis planning with machine learning is a key step in the design and discovery of novel inorganic compounds with desirable properties. Inorganic materials synthesis is often guided by chemists' prior knowledge and experience, built upon experimental trial-and-error that is both time and resource consuming. Recent developments in natural language processing (NLP) have enabled large-scaletextminingof scientific literature, providing open source databases of synthesis information of synthesized compounds, material precursors, and reaction conditions (temperatures, times). In this work, we employ a conditional variational autoencoder (CVAE) to predict suitable inorganic reaction conditions for the crucial inorganic synthesis steps of calcination and sintering. We find that the CVAE model is capable of learning subtle differences in target material composition, precursor compound identities, and choice of synthesis route (solid-state, sol-gel) that are present in the inorganic synthesis space. Moreover, the CVAE can generalize well to unseen chemical entities and shows promise for predicting reaction conditions for previously unsynthesized compounds of interest.","['Christopher Karpovich', 'Zach Jensen', 'Vineeth Venugopal', 'Elsa Olivetti']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.09612,Anomali
Provenance-aware Discovery of Functional Dependencies on Integrated Views,"The automatic discovery of functional dependencies(FDs) has been widely studied as one of the hardest problems in data profiling. Existing approaches have focused on making the FD computation efficient while inspecting single relations at a time. In this paper, for the first time we address the problem of inferring FDs for multiple relations as they occur in integrated views by solely using the functional dependencies of the base relations of the view itself. To this purpose, we leverage logical inference and selectiveminingand show that we can discover most of the exact FDs from the base relations and avoid the full computation of the FDs for the integrated view itself, while at the same time preserving the lineage of FDs of base relations. We propose algorithms to speedup the inferred FD discovery process andmineFDs on-the-fly only from necessary data partitions. We present InFine(INferred FunctIoNal dEpendency), an end-to-end solution to discover inferred FDs on integrated views by leveraging provenance information of base relations. Our experiments on a range of real-world and synthetic datasets demonstrate the benefits of our method over existing FD discovery methods that need to rerun the discovery process on the view from scratch and cannot exploit lineage information on the FDs. We show that InFine outperforms traditional methods necessitating the full integrated view computation by one to two order of magnitude in terms of runtime. It is also the most memory efficient method while preserving FD provenance information using mainly inference from base table with negligible execution time.","['Ugo Comignani', 'Laure Berti-Équille', 'Noël Novelli', 'Angela Bonifati']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.09011,Anomali
Text Mining Through Label Induction Grouping Algorithm Based Method,"The main focus of information retrieval methods is to provide accurate and efficient results which are cost-effective too. LINGO (Label Induction Grouping Algorithm) is a clustering algorithm that aims to provide search results in form of quality clusters but also has a few limitations. In this paper, our focus is based on achieving results that are more meaningful and improving the overall performance of the algorithm. LINGO works on two main steps; Cluster Label Induction by using Latent Semantic Indexing technique (LSI) and Cluster content discovery by using the Vector Space Model (VSM). As LINGO uses VSM in cluster content discovery, our task is to replace VSM with LSI for cluster content discovery and to analyze the feasibility of using LSI with Okapi BM25. The next task is to compare the results of a modified method with the LINGO original method. The research is applied to five differenttext-based data sets to get more reliable results for every method. Research results show that LINGO produces 40-50% better results when using LSI for content Discovery. From theoretical evidence using Okapi BM25 for scoring method in LSI (LSI+Okapi BM25) for cluster content discovery instead of VSM, also results in better clusters generation in terms of scalability and performance when compares to both VSM and LSI's Results.","['Gulshan Saleem', 'Nisar Ahmed', 'Usman Qamar']",Science International 28.1 (2016),arXiv,2021,https://doi.org/10.48550/arXiv.2112.08486,Anomali
Textless Speech-to-Speech Translation on Real Data,"We present a textless speech-to-speech translation (S2ST) system that can translate speech from one language into another language and can be built without the need of anytextdata. Different from existing work in the literature, we tackle the challenge in modeling multi-speaker target speech and train the systems with real-world S2ST data. The key to our approach is a self-supervised unit-based speech normalization technique, which finetunes a pre-trained speech encoder with paired audios from multiple speakers and a single reference speaker to reduce the variations due to accents, while preserving the lexical content. With only 10 minutes of paired data for speech normalization, we obtain on average 3.2 BLEU gain when training the S2ST model on the VoxPopuli S2ST dataset, compared to a baseline trained on un-normalized speech target. We also incorporate automaticallyminedS2ST data and show an additional 2.0 BLEU gain. To our knowledge, we are the first to establish a textless S2ST technique that can be trained with real-world data and works for multiple language pairs. Audio samples are available at https://facebookresearch.github.io/speech_translation/textless_s2st_real_data/index.html .","['Ann Lee', 'Hongyu Gong', 'Paul-Ambroise Duquenne', 'Holger Schwenk', 'Peng-Jen Chen', 'Changhan Wang', 'Sravya Popuri', 'Yossi Adi', 'Juan Pino', 'Jiatao Gu', 'Wei-Ning Hsu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2112.08352,Anomali
TopNet: Learning from Neural Topic Model to Generate Long Stories,"Long story generation (LSG) is one of the coveted goals in natural language processing. Different from mosttextgeneration tasks, LSG requires to output a long story of rich content based on a much shortertextinput, and often suffers from information sparsity. In this paper, we propose \emph{TopNet} to alleviate this problem, by leveraging the recent advances in neural topic modeling to obtain high-quality skeleton words to complement the short input. In particular, instead of directly generating a story, we first learn to map the shorttextinput to a low-dimensional topic distribution (which is pre-assigned by a topic model). Based on this latent topic distribution, we can use the reconstruction decoder of the topic model to sample a sequence of inter-related words as a skeleton for the story. Experiments on two benchmark datasets show that our proposed framework is highly effective in skeleton word selection and significantly outperforms the state-of-the-art models in both automatic evaluation and human evaluation.","['Yazheng Yang', 'Boyuan Pan', 'Deng Cai', 'Huan Sun']","Yang, Yazheng, Boyuan Pan, Deng Cai, and Huan Sun. ""TopNet: Learning from Neural Topic Model to Generate Long Stories."" In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, pp. 1997-2005. 2021",arXiv,2021,https://doi.org/10.48550/arXiv.2112.07259,Anomali
UCD-CS at TREC 2021 Incident Streams Track,"In recent years, the task ofminingimportant information from social media posts during crises has become a focus of research for the purposes of assisting emergency response (ES). The TREC Incident Streams (IS) track is a research challenge organised for this purpose. The track asks participating systems to both classify a stream of crisis-related tweets into humanitarian aid related information types and estimate their importance regarding criticality. The former refers to a multi-label information type classification task and the latter refers to a priority estimation task. In this paper, we report on the participation of the University College Dublin School of Computer Science (UCD-CS) in TREC-IS 2021. We explored a variety of approaches, including simple machine learning algorithms, multi-task learning techniques,textaugmentation, and ensemble approaches. The official evaluation results indicate that our runs achieve the highest scores in many metrics. To aid reproducibility, our code is publicly available at https://github.com/wangcongcong123/crisis-mtl.","['Congcong Wang', 'David Lillis']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.03737,Anomali
Disability and Library Services: Global Research Trend,"The research on differently abled persons, and their use of library is getting global attention in recent years. The field has shown a modest, continuous but wide-scale growth. This research paper aimed at capturing the dynamics of the field using various bibliometrics andtextminingtools. The bibliographic data of journal articles published in the field were collected from the Web of Science (WoS) database. The records were collected form the year 1991 to 2021 and analysed to observed the trends of literature growth, core journals, institutes from where most of the literature is being published, prominent keywords and so on. The results show that there is a significant growth of publications since the year 2000. The trends shows that the research in these areas is mostly emerging from developed countries. The developing countries should also pay more attention to do research in this area because differently abled peoples need in developed countries may vary with respect to developed countries.",['Swapan Kumar Patra'],,arXiv,2021,https://doi.org/10.48550/arXiv.2112.03580,Anomali
Unsupervised Law Article Mining based on Deep Pre-Trained Language Representation Models with Application to the Italian Civil Code,"Modeling law search and retrieval as prediction problems has recently emerged as a predominant approach in law intelligence. Focusing on the law article retrieval task, we present a deep learning framework named LamBERTa, which is designed for civil-law codes, and specifically trained on the Italian civil code. To our knowledge, this is the first study proposing an advanced approach to law article prediction for the Italian legal system based on a BERT (Bidirectional Encoder Representations from Transformers) learning framework, which has recently attracted increased attention among deep learning approaches, showing outstanding effectiveness in several natural language processing and learning tasks. We define LamBERTa models by fine-tuning an Italian pre-trained BERT on the Italian civil code or its portions, for law article retrieval as a classification task. One key aspect of our LamBERTa framework is that we conceived it to address an extreme classification scenario, which is characterized by a high number of classes, the few-shot learning problem, and the lack of test query benchmarks for Italian legal prediction tasks. To solve such issues, we define different methods for the unsupervised labeling of the law articles, which can in principle be applied to any law article code system. We provide insights into the explainability and interpretability of our LamBERTa models, and we present an extensive experimental analysis over query sets of different type, for single-label as well as multi-label evaluation tasks. Empirical evidence has shown the effectiveness of LamBERTa, and also its superiority against widely used deep-learningtextclassifiers and a few-shot learner conceived for an attribute-aware prediction task.","['Andrea Tagarelli', 'Andrea Simeri']","This article was published with the \textit{Artificial Intelligence and Law} journal, Springer Nature, on 15 September 2021",arXiv,2021,https://doi.org/10.48550/arXiv.2112.03033,Anomali
Multi-Class and Automated Tweet Categorization,"Twitter is among the most prevalent social media platform being used by millions of people all over the world. It is used to express ideas and opinions about political, social, business, sports, health, religion, and various other categories. The study reported here aims to detect the tweet category from itstext. It becomes quite challenging whentextconsists of 140 characters only, with full of noise. The tweet is categorized under 12 specified categories usingTextMiningor Natural Language Processing (NLP), and Machine Learning (ML) techniques. It is observed that a huge number of trending topics are provided by Twitter but it is really challenging to find out that what these trending topics are all about. Therefore, it is extremely crucial to automatically categorize the tweets into general categories for plenty of information extraction tasks. A large dataset is constructed by combining two different nature of datasets having varying levels of category identification complexities. It is annotated by experts under proper guidelines for increased quality and high agreement values. It makes the proposed model quite robust. Various types of ML algorithms were used to train and evaluate the proposed model. These models have explored over three datasets separately. It is explored that the nature of the dataset is highly non-linear therefore complex or non-linear models perform better. The best ensemble model named, Gradient Boosting achieved an AUC score of 85\%. That is much better than the other related studies conducted.",['Khubaib Ahmed Qureshi'],,arXiv,2021,https://doi.org/10.48550/arXiv.2112.03005,Anomali
CU-UD: text-mining drug and chemical-protein interactions with ensembles of BERT-based models,"Identifying the relations between chemicals and proteins is an importanttextminingtask. BioCreative VII track 1 DrugProt task aims to promote the development and evaluation of systems that can automatically detect relations between chemical compounds/drugs and genes/proteins in PubMed abstracts. In this paper, we describe our submission, which is an ensemble system, including multiple BERT-based language models. We combine the outputs of individual models using majority voting and multilayer perceptron. Our system obtained 0.7708 in precision and 0.7770 in recall, for an F1 score of 0.7739, demonstrating the effectiveness of using ensembles of BERT-based language models for automatically detecting relations between chemicals and proteins. Our code is available at https://github.com/bionlplab/drugprot_bcvii.","['Mehmet Efruz Karabulut', 'K. Vijay-Shanker', 'Yifan Peng']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.03004,Anomali
A PubMedBERT-based Classifier with Data Augmentation Strategy for Detecting Medication Mentions in Tweets,"As a major social media platform, Twitter publishes a large number of user-generatedtext(tweets) on a daily basis.Miningsuch data can be used to address important social, public health, and emergency management issues that are infeasible through other means. An essential step in manytextminingpipelines is named entity recognition (NER), which presents some special challenges for tweet data. Among them are nonstandard expressions, extreme imbalanced classes, and lack of context information, etc. The track 3 of BioCreative challenge VII (BC7) was organized to evaluate methods for detecting medication mentions in tweets. In this paper, we report our work on BC7 track 3, where we explored a PubMedBERT-based classifier trained with a combination of multiple data augmentation approaches. Our method achieved an F1 score of 0.762, which is substantially higher than the mean of all submissions (0.696).","['Qing Han', 'Shubo Tian', 'Jinfeng Zhang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.02998,Anomali
Multi-View Active Learning for Short Text Classification in User-Generated Data,"Mininguser-generated data often suffers from the lack of enough labeled data, short document lengths, and the informal user language. In this paper, we propose a novel active learning model to overcome these obstacles in the tasks tailored for query phrases--e.g., detecting positive reports of natural disasters. Our model has three novelties: 1) It is the first approach to employ multi-view active learning in this domain. 2) It uses the Parzen-Rosenblatt window method to integrate the representativeness measure into multi-view active learning. 3) It employs a query-by-committee strategy, based on the agreement between predictors, to address the usually noisy language of the documents in this domain. We evaluate our model in four publicly available Twitter datasets with distinctly different applications. We also compare our model with a wide range of baselines including those with multiple classifiers. The experiments testify that our model is highly consistent and outperforms existing models.","['Payam Karisani', 'Negin Karisani', 'Li Xiong']",,arXiv,2022,https://doi.org/10.48550/arXiv.2112.02611,Anomali
On the Documentation of Refactoring Types,"Commit messages are the atomic level of software documentation. They provide a natural language description of the code change and its purpose. Messages are critical for software maintenance and program comprehension. Unlike documenting feature updates and bug fixes, little is known about how developers document their refactoring activities. Developers can perform multiple refactoring operations, including moving methods, extracting classes, for various reasons. Yet, there is no systematic study that analyzes the extent to which the documentation of refactoring accurately describes the refactoring operations performed at the source code level. Therefore, this paper challenges the ability of refactoring documentation to adequately predict the refactoring types, performed at the commit level. Our analysis relies on thetextminingof commit messages to extract the corresponding features that better represent each class. The extraction oftextpatterns, specific to each refactoring allows the design of a model that verifies the consistency of these patterns with their corresponding refactoring. Such verification process can be achieved via automatically predicting the method-level type of refactoring being applied, namely Extract Method, Inline Method, Move Method, Pull-up Method, Push-down Method, and Rename Method. We compared various classifiers, and a baseline keyword-based approach, in terms of their prediction performance, using a dataset of 5,004 commits. Our main findings show that the complexity of refactoring type prediction varies from one type to another. Rename method and Extract method were found to be the best documented refactoring activities, while Pull-up Method and Push-down Method were the hardest to be identified via textual descriptions. Such findings bring the attention of developers to the necessity of paying more attention to the documentation of these types.","['Eman Abdullah AlOmar', 'Jiaqian Liu', 'Kenneth Addo', 'Mohamed Wiem Mkaouer', 'Christian Newman', 'Ali Ouni', 'Zhe Yu']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.01581,Anomali
Using word embedding for environmental violation analysis: Evidence from Pennsylvania unconventional oil and gas compliance reports,"With the booming of the unconventional oil and gas industry, its inevitable damage to the environment and human health has attracted public attention. We appliedtextminingon a total 6057 the type of Environmental Health and Safety compliance reports from 2008 to 2018 lunched by the Department of Environmental Protection in Pennsylvania, USA, to discover the intern mechanism of environmental violations.","['Dan Bi', 'Ju-e Guo', 'Erlong Zhao', 'Shaolong Sun', 'Shouyang Wang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.01224,Anomali
Large-Scale Data Mining of Rapid Residue Detection Assay Data From HTML and PDF Documents: Improving Data Access and Visualization for Veterinarians,"Extra-label drug use in food animal medicine is authorized by the US Animal Medicinal Drug Use Clarification Act (AMDUCA), and estimated withdrawal intervals are based on published scientific pharmacokinetic data. Occasionally there is a paucity of scientific data on which to base a withdrawal interval or a large number of animals being treated, driving the need to test for drug residues. Rapid assay commercial farm-side tests are essential for monitoring drug residues in animal products to protect human health. Active ingredients, sensitivity, matrices, and species that have been evaluated for commercial rapid assay tests are typically reported on manufacturers' websites or in PDF documents that are available to consumers but may require a special access request. Additionally, this information is not always correlated with FDA-approved tolerances. Furthermore, parameter changes for these tests can be very challenging to regularly identify, especially those listed on websites or in documents that are not publicly available. Therefore, artificial intelligence plays a critical role in efficiently extracting the data and ensure current information. Extracting tables from PDF and HTML documents has been investigated both by academia and commercial tool builders. Research intextminingof such documents has become a widespread yet challenging arena in implementing natural language programming. However, techniques of extracting tables are still in their infancy and being investigated and improved by researchers. In this study, we developed and evaluated a data-miningmethod for automatically extracting rapid assay data from electronic documents. Our automatic electronic data extraction method includes a software package module, a developed pattern recognition tool, and a dataminingengine. Assay details were provided by several commercial entities that produce these rapid drug residue assay","['Majid Jaberi-Douraki', 'Soudabeh Taghian Dinani', 'Nuwan Indika Millagaha Gedara', 'Xuan Xu', 'Emily Richards', 'Fiona Maunsell', 'Nader Zad', 'Lisa Ann Tell']","8, 2021, 13",arXiv,2021,https://doi.org/10.48550/arXiv.2112.00962,Anomali
"NLP Research and Resources at DaSciM, Ecole Polytechnique","DaSciM (Data Science andMining) part of LIX at Ecole Polytechnique, established in 2013 and since then producing research results in the area of large scale data analysis via methods of machine and deep learning. The group has been specifically active in the area of NLP andtextminingwith interesting results at methodological and resources level. Here follow our different contributions of interest to the AFIA community.","['Hadi Abdine', 'Yanzhu Guo', 'Moussa Kamal Eddine', 'Giannis Nikolentzos', 'Stamatis Outsios', 'Guokan Shang', 'Christos Xypolopoulos', 'Michalis Vazirgiannis']",,arXiv,2021,https://doi.org/10.48550/arXiv.2112.00566,Anomali
Efficient Big Text Data Clustering Algorithms using Hadoop and Spark,"Document clustering is a traditional, efficient and yet quite effective,textminingtechnique when we need to get a better insight of the documents of a collection that could be grouped together. The K-Means algorithm and the Hierarchical Agglomerative Clustering (HAC) algorithm are two of the most known and commonly used clustering algorithms; the former due to its low time cost and the latter due to its accuracy. However, even the use of K-Means intextclustering over large-scale collections can lead to unacceptable time costs. In this paper we first address some of the most valuable approaches for document clustering over such 'big data' (large-scale) collections. We then present two very promising alternatives: (a) a variation of an existing K-Means-based fast clustering technique (known as BigKClustering - BKC) so that it can be applied in document clustering, and (b) a hybrid clustering approach based on a customized version of the Buckshot algorithm, which first applies a hierarchical clustering procedure on a sample of the input dataset and then it uses the results as the initial centers for a K-Means based assignment of the rest of the documents, with very few iterations. We also give highly efficient adaptations of the proposed techniques in the MapReduce model which are then experimentally tested using Apache Hadoop and Spark over a real cluster environment. As it comes out of the experiments, they both lead to acceptable clustering quality as well as to significant time improvements (compared to K-Means - especially the Buckshot-based algorithm), thus constituting very promising alternatives for big document collections.","['Sergios Gerakidis', 'Sofia Megarchioti', 'Basilis Mamalis']","International Journal of Computer Applications (IJCA), Vol. 174, No. 15, pp. 13-21, 2021",arXiv,2021,https://doi.org/10.48550/arXiv.2112.00200,Anomali
Text Mining Drug/Chemical-Protein Interactions using an Ensemble of BERT and T5 Based Models,"In Track-1 of the BioCreative VII Challenge participants are asked to identify interactions between drugs/chemicals and proteins. In-context named entity annotations for each drug/chemical and protein are provided and one of fourteen different interactions must be automatically predicted. For this relation extraction task, we attempt both a BERT-based sentence classification approach, and a more noveltext-to-textapproach using a T5 model. We find that larger BERT-based models perform better in general, with our BioMegatron-based model achieving the highest scores across all metrics, achieving 0.74 F1 score. Though our novel T5text-to-textmethod did not perform as well as most of our BERT-based models, it outperformed those trained on similar data, showing promising results, achieving 0.65 F1 score. We believe atext-to-textapproach to relation extraction has some competitive advantages and there is a lot of room for research advancement.","['Virginia Adams', 'Hoo-Chang Shin', 'Carol Anderson', 'Bo Liu', 'Anas Abidin']",,arXiv,2021,https://doi.org/10.48550/arXiv.2111.15617,Anomali
A New Multifractal-based Deep Learning Model for Text Mining,"In this world full of uncertainty, where the fabric of existence weaves patterns of complexity, multifractal emerges as beacons of insight, illuminating them. As we delve into the realm oftextminingthat underpins various natural language processing applications and powers a range of intelligent services, we recognize that behind the veil oftextlies a manifestation of human thought and cognition, intricately intertwined with the complexities. Building upon the foundation of perceivingtextas a complex system, this study embarks on a journey to unravel the hidden treasures within, armed with the proposed multifractal method that deciphers the multifractal attributes embedded within thetextlandscape. This endeavor culminates in the birth of our novel model, which also harnesses the power of the proposed activation function to facilitate nonlinear information transmission within its neural network architecture. The success on experiments anchored in real-world technical reports covering the extraction of technical term and classification of hazard events, stands as a testament to our endeavors. This research venture not only expands our understanding oftextminingbut also opens new horizons for knowledge discovery across various domains.","['Zhenhua Wang', 'Ming Ren', 'Dong Gao']",,arXiv,2023,https://doi.org/10.48550/arXiv.2111.13861,Anomali
A novel knowledge graph development for industry design: A case study on indirect coal liquefaction process,"Hazard and operability analysis (HAZOP) is a remarkable representative in industrial safety engineering. However, a great storehouse of industrial safety knowledge (ISK) in HAZOP reports has not been thoroughly exploited. In order to reuse and unlock the value of ISK and optimize HAZOP, we have developed a novel knowledge graph for industrial safety (ISKG) with HAZOP as the carrier through bridging data science and engineering design. Specifically, firstly, considering that the knowledge contained in HAZOP reports of different processes in industry is not the same, we creatively develope a general ISK standardization framework, it provides a practical scheme for integrating HAZOP reports from various processes and uniformly representing the ISK with diverse expressions. Secondly, we conceive a novel and reliable information extraction model based on deep learning combined with data science, it can effectivelymineISK from HAZOP reports, which alleviates the obstacle of ISK extraction caused by the particularity of HAZOPtext. Finally, we build ISK triples and store them in the Neo4j graph database. We take indirect coal liquefaction process as a case study to develop ISKG, and its oriented applications can optimize HAZOP andminethe potential of ISK, which is of great significance to improve the security of the system and enhance prevention awareness for people. ISKG containing the ISK standardization framework and the information extraction model sets an example of the interaction between data science and engineering design, which can enlighten other researchers and extend the perspectives of industrial safety.","['Zhenhua Wang', 'Beike Zhang', 'Dong Gao']",,arXiv,2022,https://doi.org/10.48550/arXiv.2111.13854,Anomali
Interpretable and Fair Boolean Rule Sets via Column Generation,"This paper considers the learning of Boolean rules in disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. We also consider the fairness setting and extend the formulation to include explicit constraints on two different measures of classification parity: equality of opportunity and equalized odds. Column generation (CG) is used to efficiently search over an exponential number of candidate rules without the need for heuristic rulemining. To handle large data sets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 data sets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurate. Compared to other fair and interpretable classifiers, our method is able to find rule sets that meet stricter notions of fairness with a modest trade-off in accuracy.","['Connor Lawless', 'Sanjeeb Dash', 'Oktay Gunluk', 'Dennis Wei']","Journal of Machine Learning Research 2023 Volume 24, Number 229, Pages 1-50",arXiv,2023,https://doi.org/10.48550/arXiv.2111.08466,Anomali
DeepXML: A Deep Extreme Multi-Label Learning Framework Applied to Short Text Documents,"Scalability and accuracy are well recognized challenges in deep extreme multi-label learning where the objective is to train architectures for automatically annotating a data point with the most relevant subset of labels from an extremely large label set. This paper develops the DeepXML framework that addresses these challenges by decomposing the deep extreme multi-label task into four simpler sub-tasks each of which can be trained accurately and efficiently. Choosing different components for the four sub-tasks allows DeepXML to generate a family of algorithms with varying trade-offs between accuracy and scalability. In particular, DeepXML yields the Astec algorithm that could be 2-12% more accurate and 5-30x faster to train than leading deep extreme classifiers on publically available shorttextdatasets. Astec could also efficiently train on Bing shorttextdatasets containing up to 62 million labels while making predictions for billions of users and data points per day on commodity hardware. This allowed Astec to be deployed on the Bing search engine for a number of shorttextapplications ranging from matching user queries to advertiser bid phrases to showing personalized ads where it yielded significant gains in click-through-rates, coverage, revenue and other online metrics over state-of-the-art techniques currently in production. DeepXML's code is available at https://github.com/Extreme-classification/deepxml","['Kunal Dahiya', 'Deepak Saini', 'Anshul Mittal', 'Ankush Shaw', 'Kushal Dave', 'Akshay Soni', 'Himanshu Jain', 'Sumeet Agarwal', 'Manik Varma']",Web Search and Data Mining 2021,arXiv,2021,https://doi.org/10.48550/arXiv.2111.06685,Anomali
American Hate Crime Trends Prediction with Event Extraction,"Social media platforms may provide potential space for discourses that contain hate speech, and even worse, can act as a propagation mechanism for hate crimes. The FBI's Uniform Crime Reporting (UCR) Program collects hate crime data and releases statistic report yearly. These statistics provide information in determining national hate crime trends. The statistics can also provide valuable holistic and strategic insight for law enforcement agencies or justify lawmakers for specific legislation. However, the reports are mostly released next year and lag behind many immediate needs. Recent research mainly focuses on hate speech detection in social mediatextor empirical studies on the impact of a confirmed crime. This paper proposes a framework that first utilizestextminingtechniques to extract hate crime events from New York Times news, then uses the results to facilitate predicting American national-level and state-level hate crime trends. Experimental results show that our method can significantly enhance the prediction performance compared with time series or regression methods without event-related factors. Our framework broadens the methods of national-level and state-level hate crime trends prediction.","['Songqiao Han', 'Hailiang Huang', 'Jiangwei Liu', 'Shengsheng Xiao']",,arXiv,2021,https://doi.org/10.48550/arXiv.2111.04951,Anomali
"Towards an integrated platform for characterizing laser-driven, isochorically-heated plasmas with 1-$μ$m spatial resolution","Warm dense matter is a region of phase space that is of high interest to multiple scientific communities ranging from astrophysics to inertial confinement fusion. Further understanding of the conditions and properties of this complex state of matter necessitates experimental benchmarking of the current theoretical models. Benchmarking of transport properties like conductivity and diffusivity has been scarce because they are small and slow processes that require micron-level resolution to see. We discuss development of a radiography platform designed to allow for measurement of these properties at large laser facilities such as the OMEGA Laser.
  \c{opyright} 2022 Optica Publishing Group. Users may use, reuse, and build upon the article, or use the article fortextor datamining, so long as such uses are for non-commercial purposes and appropriate attribution is maintained. All other rights are reserved.","['Cameron H Allen', 'Matthew Oliver', 'Laurent Divol', 'Otto L Landen', 'Yuan Ping', 'Markus Schoelmerich', 'Russell Wallace', 'Robert Earley', 'Wolfgang Theobald', 'Thomas G White', 'Tilo Doeppner']",,arXiv,2022,https://doi.org/10.48550/arXiv.2111.04799,Anomali
A Semi-automatic Data Extraction System for Heterogeneous Data Sources: A Case Study from Cotton Industry,"With the recent developments in digitisation, there are increasing number of documents available online. There are several information extraction tools that are available to extract information from digitised documents. However, identifying precise answers to a given query is often a challenging task especially if the data source where the relevant information resides is unknown. This situation becomes more complex when the data source is available in multiple formats such as PDF, table and html. In this paper, we propose a novel data extraction system to discover relevant and focused information from diverse unstructured data sources based ontextminingapproaches. We perform a qualitative analysis to evaluate the proposed system and its suitability and adaptability using cotton industry.","['Richi Nayak', 'Thirunavukarasu Balasubramaniam', 'Sangeetha Kutty', 'Sachindra Banduthilaka', 'Erin Peterson']",,arXiv,2021,https://doi.org/10.48550/arXiv.2111.03579,Anomali
Supervised Advantage Actor-Critic for Recommender Systems,"Casting session-based or sequential recommendation as reinforcement learning (RL) through reward signals is a promising research direction towards recommender systems (RS) that maximize cumulative profits. However, the direct use of RL algorithms in the RS setting is impractical due to challenges like off-policy training, huge action spaces and lack of sufficient reward signals. Recent RL approaches for RS attempt to tackle these challenges by combining RL and (self-)supervised sequential learning, but still suffer from certain limitations. For example, the estimation of Q-values tends to be biased toward positive values due to the lack of negative reward signals. Moreover, the Q-values also depend heavily on the specific timestamp of a sequence.
  To address the above problems, we propose negative sampling strategy for training the RL component and combine it with supervised sequential learning. We call this method Supervised Negative Q-learning (SNQN). Based on sampled (negative) actions (items), we can calculate the ""advantage"" of a positive action over the average case, which can be further utilized as a normalized weight for learning the supervised sequential part. This leads to another learning framework: Supervised Advantage Actor-Critic (SA2C). We instantiate SNQN and SA2C with four state-of-the-art sequential recommendation models and conduct experiments on two real-world datasets. Experimental results show that the proposed approaches achieve significantly better performance than state-of-the-art supervised methods and existing self-supervised RL methods . Code will be open-sourced.","['Xin Xin', 'Alexandros Karatzoglou', 'Ioannis Arapakis', 'Joemon M. Jose']",,arXiv,2021,https://doi.org/10.48550/arXiv.2111.03474,Anomali
Cross talk compensation in multimode continuous-variable entanglement distribution,"Two-mode squeezed states are scalable and robust entanglement resources for continuous-variable and hybrid quantum information protocols at a distance. We consider the effect of a linear cross talk in the multimode distribution of two-mode squeezed states propagating through parallel similar channels. First, to reduce degradation of the distributed Gaussian entanglement, we show that the initial two-mode squeezing entering the channel should be optimized already in the presence of a small cross talk. Second, we suggest simultaneous optimization of relative phase between the modes and their linear coupling on a receiver side prior to the use of entanglement, which can fully compensate the cross talk once the channel transmittance is the same for all the modes. For the realistic channels with similar transmittance values for either of the modes, the cross talk can be still largely compensated. This method relying on the mode interference overcomes an alternative method of entanglement localization in one pair of modes using measurement on another pair and feed-forward control. Our theoretical results pave the way to more efficient use of multimode continuous-variable photonic entanglement in scalable quantum networks with cross talk.","['Olena Kovalenko', 'Vladyslav C. Usenko', 'Radim Filip']","Optics Express, Vol. 29, Issue 15, pp. 24083-24101 (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2111.00948,Anomali
Calling to CNN-LSTM for Rumor Detection: A Deep Multi-channel Model for Message Veracity Classification in Microblogs,"Reputed by their low-cost, easy-access, real-time and valuable information, social media also wildly spread unverified or fake news. Rumors can notably cause severe damage on individuals and the society. Therefore, rumor detection on social media has recently attracted tremendous attention. Most rumor detection approaches focus on rumor feature analysis and social features, i.e., metadata in social media. Unfortunately, these features are data-specific and may not always be available, e.g., when the rumor has just popped up and not yet propagated. In contrast, post contents (including images or videos) play an important role and can indicate the diffusion purpose of a rumor. Furthermore, rumor classification is also closely related to opinionminingand sentiment analysis. Yet, to the best of our knowledge, exploiting images and sentiments is little investigated.Considering the available multimodal features from microblogs, notably, we propose in this paper an end-to-end model called deepMONITOR that is based on deep neural networks and allows quite accurate automated rumor verification, by utilizing all three characteristics: post textual and image contents, as well as sentiment. deepMONITOR concatenates image features with the jointtextand sentiment features to produce a reliable, fused classification. We conduct extensive experiments on two large-scale, real-world datasets. The results show that deepMONITOR achieves a higher accuracy than state-of-the-art methods.","['Abderrazek Azri', 'Cécile Favre', 'Nouria Harbi', 'Jérôme Darmont', 'Camille Noûs']","Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2021), Sep 2021, Bilbao, Spain. pp.497-513",arXiv,2021,https://doi.org/10.48550/arXiv.2110.15727,Anomali
Paperswithtopic: Topic Identification from Paper Title Only,"The deep learning field is growing rapidly as witnessed by the exponential growth of papers submitted to journals, conferences, and pre-print servers. To cope with the sheer number of papers, severaltextminingtools from natural language processing (NLP) have been proposed that enable researchers to keep track of recent findings. In this context, our paper makes two main contributions: first, we collected and annotated a dataset of papers paired by title and sub-field from the field of artificial intelligence (AI), and, second, we present results on how to predict a paper's AI sub-field from a given paper title only. Importantly, for the latter, short-textclassification task we compare several algorithms from conventional machine learning all the way up to recent, larger transformer architectures. Finally, for the transformer models, we also present gradient-based, attention visualizations to further explain the model's classification process. All code can be found at \url{https://github.com/1pha/paperswithtopic}","['Daehyun Cho', 'Christian Wallraven']",,arXiv,2022,https://doi.org/10.48550/arXiv.2110.15721,Anomali
BiC-Net: Learning Efficient Spatio-Temporal Relation for Text-Video Retrieval,"The task oftext-video retrieval aims to understand the correspondence between language and vision, has gained increasing attention in recent years. Previous studies either adopt off-the-shelf 2D/3D-CNN and then use average/max pooling to directly capture spatial features with aggregated temporal information as global video embeddings, or introduce graph-based models and expert knowledge to learn local spatial-temporal relations. However, the existing methods have two limitations: 1) The global video representations learn video temporal information in a simple average/max pooling manner and do not fully explore the temporal information between every two frames. 2) The graph-based local video representations are handcrafted, it depends heavily on expert knowledge and empirical feedback, which may not be able to effectivelyminethe higher-level fine-grained visual relations. These limitations result in their inability to distinguish videos with the same visual components but with different relations. To solve this problem, we propose a novel cross-modal retrieval framework, Bi-Branch Complementary Network (BiC-Net), which modifies transformer architecture to effectively bridgetext-video modalities in a complementary manner via combining local spatial-temporal relation and global temporal information. Specifically, local video representations are encoded using multiple transformer blocks and additional residual blocks to learn spatio-temporal relation features, calling the module a Spatio-Temporal Residual transformer (SRT). Meanwhile, Global video representations are encoded using a multi-layer transformer block to learn global temporal features. Finally, we align the spatio-temporal relation and global temporal features with thetextfeature on two embedding spaces for cross-modaltext-video retrieval.","['Ning Han', 'Jingjing Chen', 'Chuhao Shi', 'Yawen Zeng', 'Guangyi Xiao', 'Hao Chen']",,arXiv,2022,https://doi.org/10.48550/arXiv.2110.15609,Anomali
TopicNet: Semantic Graph-Guided Topic Discovery,"Existing deep hierarchical topic models are able to extract semantically meaningful topics from atextcorpus in an unsupervised manner and automatically organize them into a topic hierarchy. However, it is unclear how to incorporate prior beliefs such as knowledge graph to guide the learning of the topic hierarchy. To address this issue, we introduce TopicNet as a deep hierarchical topic model that can inject prior structural knowledge as an inductive bias to influence learning. TopicNet represents each topic as a Gaussian-distributed embedding vector, projects the topics of all layers into a shared embedding space, and explores both the symmetric and asymmetric similarities between Gaussian embedding vectors to incorporate prior semantic hierarchies. With an auto-encoding variational inference network, the model parameters are optimized by minimizing the evidence lower bound and a regularization term via stochastic gradient descent. Experiments on widely used benchmarks show that TopicNet outperforms related deep topic models on discovering deeper interpretable topics andminingbetter document~representations.","['Zhibin Duan', 'Yishi Xu', 'Bo Chen', 'Dongsheng Wang', 'Chaojie Wang', 'Mingyuan Zhou']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.14286,Anomali
Diachronic Text Mining Investigation of Therapeutic Candidates for COVID-19,"Diachronictextmininghas frequently been applied to long-term linguistic surveys of word meaning and usage shifts over time. In this paper we apply short-term diachronictextminingto a rapidly growing corpus of scientific publications on COVID-19 captured in the CORD-19 dataset in order to identify co-occurrences and analyze the behavior of potential candidate treatments. We used a data set associated with a COVID-19 drug re-purposing study from Oak Ridge National Laboratory. This study identified existing candidate coronavirus treatments, including drugs and approved compounds, which had been analyzed and ranked according to their potential for blocking the ability of the SARS-COV-2 virus to invade human cells. We investigated the occurrence of these candidates in temporal instances of the CORD-19 corpus. We found that at least 25% of the identified terms occurred in temporal instances of the corpus to the extent that their frequency and contextual dynamics could be evaluated. We identified three classes of behaviors: those where frequency and contextual shifts were small and positively correlated; those where there was no correlation between frequency and contextual changes; and those where there was a negative correlation between frequency and contextual shift. We speculate that the latter two patterns are indicative that a target candidate therapeutics is undergoing active evaluation. The patterns we detected demonstrate the potential benefits of using diachronictextminingtechniques with a large dynamictextcorpus to track drug-repurposing activities across international clinical and laboratory settings.","['James Powell', 'Kari Sentz']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.13971,Anomali
As long as you talk about me: The importance of family firm brands and the contingent role of family-firm identity,"This study explores the role of external audiences in determining the importance of family firm brands and the relationship with firm performance. Drawing ontextminingand social network analysis techniques, and considering the brand prevalence, diversity, and connectivity dimensions, we use the semantic brand score to measure the importance the media give to family firm brands. The analysis of a sample of 52,555 news articles published in 2017 about 63 Italian entrepreneurial families reveals that brand importance is positively associated with family firm revenues, and this relationship is stronger when there is identity match between the family and the firm. This study advances current literature by offering a rich and multifaceted perspective on how external audiences perceptions of the brand shape family firm performance.","['P. Rovelli', 'C. Benedetti', 'A. Fronzetti Colladon', 'A. De Massis']","Journal of Business Research 139, 692-700 (2022)",arXiv,2021,https://doi.org/10.48550/arXiv.2110.13815,Anomali
Open Rule Induction,"Rules have a number of desirable properties. It is easy to understand, infer new knowledge, and communicate with other inference systems. One weakness of the previous rule induction systems is that they only find rules within a knowledge base (KB) and therefore cannot generalize to more open and complex real-world rules. Recently, the language model (LM)-based rule generation are proposed to enhance the expressive power of the rules. In this paper, we revisit the differences between KB-based rule induction and LM-based rule generation. We argue that, while KB-based methods inducted rules by discovering data commonalities, the current LM-based methods are ""learning rules from rules"". This limits these methods to only produce ""canned"" rules whose patterns are constrained by the annotated rules, while discarding the rich expressive power of LMs for freetext.
  Therefore, in this paper, we propose the open rule induction problem, which aims to induce open rules utilizing the knowledge in LMs. Besides, we propose the Orion (\underline{o}pen \underline{r}ule \underline{i}nducti\underline{on}) system to automaticallymineopen rules from LMs without supervision of annotated rules. We conducted extensive experiments to verify the quality and quantity of the inducted open rules. Surprisingly, when applying the open rules in downstream tasks (i.e. relation extraction), these automatically inducted rules even outperformed the manually annotated rules.","['Wanyun Cui', 'Xingran Chen']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.13577,Anomali
RDD-Eclat: Approaches to Parallelize Eclat Algorithm on Spark RDD Framework (Extended Version),"Frequent itemsetmining(FIM) is a highly computational and data intensive algorithm. Therefore, parallel and distributed FIM algorithms have been designed to process large volume of data in a reduced time. Recently, a number of FIM algorithms have been designed on Hadoop MapReduce, a distributed big data processing framework. But, due to heavy disk I/O, MapReduce is found to be inefficient for the highly iterative FIM algorithms. Therefore, Spark, a more efficient distributed data processing framework, has been developed with in-memory computation and resilient distributed dataset (RDD) features to support the iterative algorithms. On this framework, Apriori and FP-Growth based FIM algorithms have been designed on the Spark RDD framework, but Eclat-based algorithm has not been explored yet. In this paper, RDD-Eclat, a parallel Eclat algorithm on the Spark RDD framework is proposed with its five variants. The proposed algorithms are evaluated on the various benchmark datasets, and the experimental results show that RDD-Eclat outperforms the Spark-based Apriori by many times. Also, the experimental results show the scalability of the proposed algorithms on increasing the number of cores and size of the dataset.","['Pankaj Singh', 'Sudhakar Singh', 'P K Mishra', 'Rakhi Garg']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.12012,Anomali
Deep learning-based NLP Data Pipeline for EHR Scanned Document Information Extraction,"Scanned documents in electronic health records (EHR) have been a challenge for decades, and are expected to stay in the foreseeable future. Current approaches for processing often include image preprocessing, optical character recognition (OCR), andtextmining. However, there is limited work that evaluates the choice of image preprocessing methods, the selection of NLP models, and the role of document layout. The impact of each element remains unknown. We evaluated this method on a use case of two key indicators for sleep apnea, Apnea hypopnea index (AHI) and oxygen saturation (SaO2) values, from scanned sleep study reports. Our data that included 955 manually annotated reports was secondarily utilized from a previous study in the University of Texas Medical Branch. We performed image preprocessing: gray-scaling followed by 1 iteration of dilating and erode, and 20% contrast increasing. The OCR was implemented with the Tesseract OCR engine. A total of seven Bag-of-Words models (Logistic Regression, Ridge Regression, Lasso Regression, Support Vector Machine, k-Nearest Neighbor, Naïve Bayes, and Random Forest) and three deep learning-based models (BiLSTM, BERT, and Clinical BERT) were evaluated. We also evaluated the combinations of image preprocessing methods (gray-scaling, dilate & erode, increased contrast by 20%, increased contrast by 60%), and two deep learning architectures (with and without structured input that provides document layout information). Our proposed method using Clinical BERT reached an AUROC of 0.9743 and document accuracy of 94.76% for AHI, and an AUROC of 0.9523, and document accuracy of 91.61% for SaO2. We demonstrated the proper use of image preprocessing and document layout could be beneficial to scanned document processing.","['Enshuo Hsu', 'Ioannis Malagaris', 'Yong-Fang Kuo', 'Rizwana Sultana', 'Kirk Roberts']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.11864,Anomali
Overview of the 2021 Key Point Analysis Shared Task,We describe the 2021 Key Point Analysis (KPA-2021) shared task on key point analysis that we organized as a part of the 8th Workshop on ArgumentMining(ArgMining 2021) at EMNLP 2021. We outline various approaches and discuss the results of the shared task. We expect the task and the findings reported in this paper to be relevant for researchers working ontextsummarization and argumentmining.,"['Roni Friedman', 'Lena Dankin', 'Yufang Hou', 'Ranit Aharonov', 'Yoav Katz', 'Noam Slonim']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.10577,Anomali
SocialVisTUM: An Interactive Visualization Toolkit for Correlated Neural Topic Models on Social Media Opinion Mining,"Recent research in opinionminingproposed word embedding-based topic modeling methods that provide superior coherence compared to traditional topic modeling. In this paper, we demonstrate how these methods can be used to display correlated topic models on social mediatextsusing SocialVisTUM, our proposed interactive visualization toolkit. It displays a graph with topics as nodes and their correlations as edges. Further details are displayed interactively to support the exploration of largetextcollections, e.g., representative words and sentences of topics, topic and sentiment distributions, hierarchical topic clustering, and customizable, predefined topic labels. The toolkit optimizes automatically on custom data for optimal coherence. We show a working instance of the toolkit on data crawled from English social media discussions about organic food consumption. The visualization confirms findings of a qualitative consumer research study. SocialVisTUM and its training procedures are accessible online.","['Gerhard Johann Hagerer', 'Martin Kirchhoff', 'Hannah Danner', 'Robert Pesch', 'Mainak Ghosh', 'Archishman Roy', 'Jiaxi Zhao', 'Georg Groh']",RANLP-2021,arXiv,2023,https://doi.org/10.48550/arXiv.2110.10575,Anomali
Ensembling Graph Predictions for AMR Parsing,"In many machine learning tasks, models are trained to predict structure data such as graphs. For example, in natural language processing, it is very common to parsetextsinto dependency trees or abstract meaning representation (AMR) graphs. On the other hand, ensemble methods combine predictions from multiple models to create a new one that is more robust and accurate than individual predictions. In the literature, there are many ensembling techniques proposed for classification or regression problems, however, ensemble graph prediction has not been studied thoroughly. In this work, we formalize this problem asminingthe largest graph that is the most supported by a collection of graph predictions. As the problem is NP-Hard, we propose an efficient heuristic algorithm to approximate the optimal solution. To validate our approach, we carried out experiments in AMR parsing problems. The experimental results demonstrate that the proposed approach can combine the strength of state-of-the-art AMR parsers to create new predictions that are more accurate than any individual models in five standard benchmark datasets.","['Hoang Thanh Lam', 'Gabriele Picco', 'Yufang Hou', 'Young-Suk Lee', 'Lam M. Nguyen', 'Dzung T. Phan', 'Vanessa López', 'Ramon Fernandez Astudillo']",,arXiv,2022,https://doi.org/10.48550/arXiv.2110.09131,Anomali
Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research,"AI is widely thought to be poised to transform business, yet current perceptions of the scope of this transformation may be myopic. Recent progress in natural language processing involving transformer language models (TLMs) offers a potential avenue for AI-driven business and societal transformation that is beyond the scope of what most currently foresee. We review this recent progress as well as recent literature utilizingtextminingin top IS journals to develop an outline for how future IS research can benefit from these new techniques. Our review of existing IS literature reveals that suboptimaltextminingtechniques are prevalent and that the more advanced TLMs could be applied to enhance and increase IS research involvingtextdata, and to enable new IS research topics, thus creating more value for the research community. This is possible because these techniques make it easier to develop very powerful custom systems and their performance is superior to existing methods for a wide range of tasks and applications. Further, multilingual language models make possible higher qualitytextanalytics for research in multiple languages. We also identify new avenues for IS research, like language user interfaces, that may offer even greater potential for future IS research.","['Ross Gruetzemacher', 'David Paradice']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.08975,Anomali
Dropping diversity of products of large US firms: Models and measures,"It is widely assumed that in our lifetimes the products available in the global economy have become more diverse. This assumption is difficult to investigate directly, however, because it is difficult to collect the necessary data about every product in an economy each year. We solve this problem byminingpublicly available textual descriptions of the products of every large US firms each year from 1997 to 2017. Although many aspects of economic productivity have been steadily rising during this period, ourtext-based measurements show that the diversity of the products of at least large US firms has steadily declined. This downward trend is visible using a variety of product diversity metrics, including some that depend on a measurement of the similarity of the products of every single pair of firms. The current state of the art in comprehensive and detailed firm-similarity measurements is a Boolean word vector model due to Hoberg and Phillips. We measure diversity using firm-similarities from this Boolean model and two more sophisticated variants, and we consistently observe a significant dropping trend in product diversity. These results make it possible to frame and start to test specific hypotheses for explaining the dropping product diversity trend.","['Ananthan Nambiar', 'Tobias Rubel', 'James McCaull', 'Jon deVries', 'Mark Bedau']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.08367,Anomali
Unsupervised Text Mining of COVID-19 Records,"Since the beginning of coronavirus, the disease has spread worldwide and drastically changed many aspects of the human's lifestyle. Twitter as a powerful tool can help researchers measure public health in response to COVID-19. According to the high volume of data production on social networks, automatedtextminingapproaches can help search, read and summarize helpful information. This paper preprocessed the existing medical dataset regarding COVID-19 named CORD-19 and annotated the dataset for supervised classification tasks. At this time of the COVID-19 pandemic, we made a preprocessed dataset for the research community. This may contribute towards finding new solutions for some social interventions that COVID-19 has made. The preprocessed version of the mentioned dataset is publicly available through Github.",['Mohamad Zamini'],,arXiv,2021,https://doi.org/10.48550/arXiv.2110.07357,Anomali
Rethinking Self-Supervision Objectives for Generalizable Coherence Modeling,"Given the claims of improvedtextgeneration quality across various pre-trained neural models, we consider the coherence evaluation of machine generatedtextto be one of the principal applications of coherence models that needs to be investigated. Prior work in neural coherence modeling has primarily focused on devising new architectures for solving the permuted document task. We instead use a basic model architecture and show significant improvements over state of the art within the same training regime. We then design a harder self-supervision objective by increasing the ratio of negative samples within a contrastive learning setup, and enhance the model further through automatic hard negativeminingcoupled with a large global negative queue encoded by a momentum encoder. We show empirically that increasing the density of negative samples improves the basic model, and using a global negative queue further improves and stabilizes the model while training with hard negative samples. We evaluate the coherence model on task-independent test sets that resemble real-world applications and show significant improvements in coherence evaluations of downstream tasks.","['Prathyusha Jwalapuram', 'Shafiq Joty', 'Xiang Lin']",,arXiv,2022,https://doi.org/10.48550/arXiv.2110.07198,Anomali
TAG: Toward Accurate Social Media Content Tagging with a Concept Graph,"Although conceptualization has been widely studied in semantics and knowledge representation, it is still challenging to find the most accurate concept phrases to characterize the main idea of atextsnippet on the fast-growing social media. This is partly attributed to the fact that most knowledge bases contain general terms of the world, such as trees and cars, which do not have the defining power or are not interesting enough to social media app users. Another reason is that the intricacy of natural language allows the use of tense, negation and grammar to change the logic or emphasis of language, thus conveying completely different meanings. In this paper, we present TAG, a high-quality concept matching dataset consisting of 10,000 labeled pairs of fine-grained concepts and web-styled natural language sentences,minedfrom the open-domain social media. The concepts we consider represent the trending interests of online users. Associated with TAG is a concept graph of these fine-grained concepts and entities to provide the structural context information. We evaluate a wide range of popular neuraltextmatching models as well as pre-trained language models on TAG, and point out their insufficiency to tag social media content with the most appropriate concept. We further propose a novel graph-graph matching method that demonstrates superior abstraction and generalization performance by better utilizing both the structural context in the concept graph and logic interactions between semantic units in the sentence via syntactic dependency parsing. We open-source both the TAG dataset and the proposed methods to facilitate further research.","['Jiuding Yang', 'Weidong Guo', 'Bang Liu', 'Yakun Yu', 'Chaoyue Wang', 'Jinwen Luo', 'Linglong Kong', 'Di Niu', 'Zhen Wen']",,arXiv,2022,https://doi.org/10.48550/arXiv.2110.06892,Anomali
Field Extraction from Forms with Unlabeled Data,"We propose a novel framework to conduct field extraction from forms with unlabeled data. To bootstrap the training process, we develop a rule-based method forminingnoisy pseudo-labels from unlabeled forms. Using the supervisory signal from the pseudo-labels, we extract a discriminative token representation from a transformer-based model by modeling the interaction betweentextin the form. To prevent the model from overfitting to label noise, we introduce a refinement module based on a progressive pseudo-label ensemble. Experimental results demonstrate the effectiveness of our framework.","['Mingfei Gao', 'Zeyuan Chen', 'Nikhil Naik', 'Kazuma Hashimoto', 'Caiming Xiong', 'Ran Xu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2110.04282,Anomali
Learning Topic Models: Identifiability and Finite-Sample Analysis,"Topic models provide a usefultext-miningtool for learning, extracting, and discovering latent structures in largetextcorpora. Although a plethora of methods have been proposed for topic modeling, lacking in the literature is a formal theoretical investigation of the statistical identifiability and accuracy of latent topic estimation. In this paper, we propose a maximum likelihood estimator (MLE) of latent topics based on a specific integrated likelihood that is naturally connected to the concept, in computational geometry, of volume minimization. Our theory introduces a new set of geometric conditions for topic model identifiability, conditions that are weaker than conventional separability conditions, which typically rely on the existence of pure topic documents or of anchor words. Weaker conditions allow a wider and thus potentially more fruitful investigation. We conduct finite-sample error analysis for the proposed estimator and discuss connections between our results and those of previous investigations. We conclude with empirical studies employing both simulated and real datasets.","['Yinyin Chen', 'Shishuang He', 'Yun Yang', 'Feng Liang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2110.04232,Anomali
A Fast Randomized Algorithm for Massive Text Normalization,"Many popular machine learning techniques in natural language processing and dataminingrely heavily on high-qualitytextsources. However real-worldtextdatasets contain a significant amount of spelling errors and improperly punctuated variants where the performance of these models would quickly deteriorate. Moreover, real-world, web-scale datasets contain hundreds of millions or even billions of lines oftext, where the existingtextcleaning tools are prohibitively expensive to execute over and may require an overhead to learn the corrections. In this paper, we present FLAN, a scalable randomized algorithm to clean and canonicalize massivetextdata. Our algorithm relies on the Jaccard similarity between words to suggest correction results. We efficiently handle the pairwise word-to-word comparisons via Locality Sensitive Hashing (LSH). We also propose a novel stabilization process to address the issue of hash collisions between dissimilar words, which is a consequence of the randomized nature of LSH and is exacerbated by the massive scale of real-world datasets. Compared with existing approaches, our method is more efficient, both asymptotically and in empirical evaluations, and does not rely on additional features, such as lexical/phonetic similarity or word embedding features. In addition, FLAN does not require any annotated data or supervised learning. We further theoretically show the robustness of our algorithm with upper bounds on the false positive and false negative rates of corrections. Our experimental results on real-world datasets demonstrate the efficiency and efficacy of FLAN.","['Nan Jiang', 'Chen Luo', 'Vihan Lakshman', 'Yesh Dattatreya', 'Yexiang Xue']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.03024,Anomali
Application of the interactive Leipzig Corpus Miner as a generic research platform for the use in the social sciences,"This article introduces to the interactive Leipzig Corpus Miner (iLCM) - a newly released, open-source software to perform automatic content analysis. Since the iLCM is based on the R-programming language, its generictextminingprocedures provided via a user-friendly graphical user interface (GUI) can easily be extended using the integrated IDE RStudio-Server or numerous other interfaces in the tool. Furthermore, the iLCM offers various possibilities to use quantitative and qualitative research approaches in combination. Some of these possibilities will be presented in more detail in the following.","['Christian Kahmann', 'Andreas Niekler', 'Gregor Wiedemann']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.02708,Anomali
Rerunning OCR: A Machine Learning Approach to Quality Assessment and Enhancement Prediction,"Iterating with new and improved OCR solutions enforces decision making when it comes to targeting the right candidates for reprocessing. This especially applies when the underlying data collection is of considerable size and rather diverse in terms of fonts, languages, periods of publication and consequently OCR quality. This article captures the efforts of the National Library of Luxembourg to support those targeting decisions. They are crucial in order to guarantee low computational overhead and reduced quality degradation risks, combined with a more quantifiable OCR improvement. In particular, this work explains the methodology of the library with respect totextblock level quality assessment. Through extension of this technique, a regression model, that is able to take into account the enhancement potential of a new OCR engine, is also presented. They both mark promising approaches, especially for cultural institutions dealing with historical data of lower quality.","['Pit Schneider', 'Yves Maurer']","Journal of Data Mining & Digital Humanities, 2022, Digital humanities in languages (November 30, 2022) jdmdh:8561",arXiv,2022,https://doi.org/10.48550/arXiv.2110.01661,Anomali
Project Debater APIs: Decomposing the AI Grand Challenge,"Project Debater was revealed in 2019 as the first AI system that can debate human experts on complex topics. Engaging in a live debate requires a diverse set of skills, and Project Debater has been developed accordingly as a collection of components, each designed to perform a specific subtask. Project Debater APIs provide access to many of these capabilities, as well as to more recently developed ones. This diverse set of web services, publicly available for academic use, includes core NLP services, argumentminingand analysis capabilities, and higher-level services for content summarization. We describe these APIs and their performance, and demonstrate how they can be used for building practical solutions. In particular, we will focus on Key Point Analysis, a novel technology that identifies the main points and their prevalence in a collection oftextssuch as survey responses and user reviews.","['Roy Bar-Haim', 'Yoav Kantor', 'Elad Venezian', 'Yoav Katz', 'Noam Slonim']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.01029,Anomali
FiLMing Multimodal Sarcasm Detection with Attention,"Sarcasm detection identifies natural language expressions whose intended meaning is different from what is implied by its surface meaning. It finds applications in many NLP tasks such as opinionmining, sentiment analysis, etc. Today, social media has given rise to an abundant amount of multimodal data where users express their opinions throughtextand images. Our paper aims to leverage multimodal data to improve the performance of the existing systems for sarcasm detection. So far, various approaches have been proposed that usestextand image modality and a fusion of both. We propose a novel architecture that uses the RoBERTa model with a co-attention layer on top to incorporate context incongruity between inputtextand image attributes. Further, we integrate feature-wise affine transformation by conditioning the input image through FiLMed ResNet blocks with the textual features using the GRU network to capture the multimodal information. The output from both the models and the CLS token from RoBERTa is concatenated and used for the final prediction. Our results demonstrate that our proposed model outperforms the existing state-of-the-art method by 6.14% F1 score on the public Twitter multimodal sarcasm detection dataset.","['Sundesh Gupta', 'Aditya Shah', 'Miten Shah', 'Laribok Syiemlieh', 'Chandresh Maurya']",,arXiv,2021,https://doi.org/10.48550/arXiv.2110.00416,Anomali
Learner to learner fuzzy profiles similarity using a hybrid interaction analysis grid,"The analysis of remote discussions is not yet at the same level as the face-to-face ones. The present paper aspires twofold. On the one hand, it attempts to establish a suitable environment of interaction and collaboration among learners by using the speech acts via a semi structured synchronous communication tool. On the other, it aims to define behavioral profiles and interpersonal skills hybrid grid by matching the BALES' IPA and PLETY's analysis system. By applying the fuzzy logic, we formalize human reasoning and, thus, giving very appreciable flexibility to the reasoning that use it, which makes it possible to take into account imprecisions and uncertainties. In addition, the educational dataminingtechniques are used to optimize the mapping of behaviors to learner's profile, with similarity-based clustering, using Eros and PCA measures. In order to show the validity of our system, we performed an experiment on real-world data. The results show, among others: (1) the usefulness of fuzzy logic to properly translate the profiletextdescriptions into a mathematical format, (2) an irregularity in the behavior of the learners, (3) the correlation between the profiles, (4) the superiority of Eros method to the PCA factor in precision.","['Chabane Khentout', 'Khadidja Harbouche', 'Mahieddine Djoudi']","Revue des Sciences et Technologies de l'Information - S{é}rie ISI : Ing{é}nierie des Syst{è}mes d'Information, Lavoisier, 2021, 26 (4), pp.375-386",arXiv,2021,https://doi.org/10.48550/arXiv.2110.00247,Anomali
Combining Transformers with Natural Language Explanations,"Many NLP applications require models to be interpretable. However, many successful neural architectures, including transformers, still lack effective interpretation methods. A possible solution could rely on building explanations from domain knowledge, which is often available as plain, natural languagetext. We thus propose an extension to transformer models that makes use of external memories to store natural language explanations and use them to explain classification outputs. We conduct an experimental evaluation on two domains, legaltextanalysis and argumentmining, to show that our approach can produce relevant explanations while retaining or even improving classification performance.","['Federico Ruggeri', 'Marco Lippi', 'Paolo Torroni']",,arXiv,2024,https://doi.org/10.48550/arXiv.2110.00125,Anomali
MatSciBERT: A Materials Domain Language Model for Text Mining and Information Extraction,"An overwhelmingly large amount of knowledge in the materials domain is generated and stored astextpublished in peer-reviewed scientific literature. Recent developments in natural language processing, such as bidirectional encoder representations from transformers (BERT) models, provide promising tools to extract information from thesetexts. However, direct application of these models in the materials domain may yield suboptimal results as the models themselves may not be trained on notations and jargon that are specific to the domain. Here, we present a materials-aware language model, namely, MatSciBERT, which is trained on a large corpus of scientific literature published in the materials domain. We further evaluate the performance of MatSciBERT on three downstream tasks, namely, abstract classification, named entity recognition, and relation extraction, on different materials datasets. We show that MatSciBERT outperforms SciBERT, a language model trained on science corpus, on all the tasks. Further, we discuss some of the applications of MatSciBERT in the materials domain for extracting information, which can, in turn, contribute to materials discovery or optimization. Finally, to make the work accessible to the larger materials community, we make the pretrained and finetuned weights and the models of MatSciBERT freely accessible.","['Tanishq Gupta', 'Mohd Zaki', 'N. M. Anoop Krishnan', 'Mausam']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.15290,Anomali
Multilingual Counter Narrative Type Classification,"The growing interest in employing counter narratives for hatred intervention brings with it a focus on dataset creation and automation strategies. In this scenario, learning to recognize counter narrative types from naturaltextis expected to be useful for applications such as hate speech countering, where operators from non-governmental organizations are supposed to answer to hate with several and diverse arguments that can beminedfrom online sources. This paper presents the first multilingual work on counter narrative type classification, evaluating SoTA pre-trained language models in monolingual, multilingual and cross-lingual settings. When considering a fine-grained annotation of counter narrative classes, we report strong baseline classification results for the majority of the counter narrative types, especially if we translate every language to English before cross-lingual prediction. This suggests that knowledge about counter narratives can be successfully transferred across languages.","['Yi-Ling Chung', 'Marco Guerini', 'Rodrigo Agerri']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.13664,Anomali
Automated Mining of Leaderboards for Empirical AI Research,"With the rapid growth of research publications, empowering scientists to keep oversight over the scientific progress is of paramount importance. In this regard, the Leaderboards facet of information organization provides an overview on the state-of-the-art by aggregating empirical results from various studies addressing the same research challenge. Crowdsourcing efforts like PapersWithCode among others are devoted to the construction of Leaderboards predominantly for various subdomains in Artificial Intelligence. Leaderboards provide machine-readable scholarly knowledge that has proven to be directly useful for scientists to keep track of research progress. The construction of Leaderboards could be greatly expedited with automatedtextmining.
  This study presents a comprehensive approach for generating Leaderboards for knowledge-graph-based scholarly information organization. Specifically, we investigate the problem of automated Leaderboard construction using state-of-the-art transformer models, viz. Bert, SciBert, and XLNet. Our analysis reveals an optimal approach that significantly outperforms existing baselines for the task with evaluation scores above 90% in F1. This, in turn, offers new state-of-the-art results for Leaderboard extraction. As a result, a vast share of empirical AI research can be organized in the next-generation digital libraries as knowledge graphs.","['Salomon Kabongo', ""Jennifer D'Souza"", 'Sören Auer']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.13089,Anomali
Multi-Task and Multi-Corpora Training Strategies to Enhance Argumentative Sentence Linking Performance,"Argumentative structure prediction aims to establish links between textual units and label the relationship between them, forming a structured representation for a given inputtext. The former task, linking, has been identified by earlier works as particularly challenging, as it requires finding the most appropriate structure out of a very large search space of possible link combinations. In this paper, we improve a state-of-the-art linking model by using multi-task and multi-corpora training strategies. Our auxiliary tasks help the model to learn the role of each sentence in the argumentative structure. Combining multi-corpora training with a selective sampling strategy increases the training data size while ensuring that the model still learns the desired target distribution well. Experiments on essays written by English-as-a-foreign-language learners show that both strategies significantly improve the model's performance; for instance, we observe a 15.8% increase in the F1-macro for individual link predictions.","['Jan Wira Gotama Putra', 'Simone Teufel', 'Takenobu Tokunaga']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.13067,Anomali
Entity Linking Meets Deep Learning: Techniques and Solutions,"Entity linking (EL) is the process of linking entity mentions appearing in webtextwith their corresponding entities in a knowledge base. EL plays an important role in the fields of knowledge engineering and datamining, underlying a variety of downstream applications such as knowledge base population, content analysis, relation extraction, and question answering. In recent years, deep learning (DL), which has achieved tremendous success in various domains, has also been leveraged in EL methods to surpass traditional machine learning based methods and yield the state-of-the-art performance. In this survey, we present a comprehensive review and analysis of existing DL based EL methods. First of all, we propose a new taxonomy, which organizes existing DL based EL methods using three axes: embedding, feature, and algorithm. Then we systematically survey the representative EL methods along the three axes of the taxonomy. Later, we introduce ten commonly used EL data sets and give a quantitative performance analysis of DL based EL methods over these data sets. Finally, we discuss the remaining limitations of existing methods and highlight some promising future directions.","['Wei Shen', 'Yuhan Li', 'Yinan Liu', 'Jiawei Han', 'Jianyong Wang', 'Xiaojie Yuan']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.12520,Anomali
GERNERMED -- An Open German Medical NER Model,"The current state of adoption of well-structured electronic health records and integration of digital methods for storing medical patient data in structured formats can often considered as inferior compared to the use of traditional, unstructuredtextbased patient data documentation. Dataminingin the field of medical data analysis often needs to rely solely on processing of unstructured data to retrieve relevant data. In natural language processing (NLP), statistical models have been shown successful in various tasks like part-of-speech tagging, relation extraction (RE) and named entity recognition (NER). In this work, we present GERNERMED, the first open, neural NLP model for NER tasks dedicated to detect medical entity types in Germantextdata. Here, we avoid the conflicting goals of protection of sensitive patient data from training data extraction and the publication of the statistical model weights by training our model on a custom dataset that was translated from publicly available datasets in foreign language by a pretrained neural machine translation model. The sample code and the statistical model is available at: https://github.com/frankkramer-lab/GERNERMED","['Johann Frei', 'Frank Kramer']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.12104,Anomali
An Algorithm for Generating Gap-Fill Multiple Choice Questions of an Expert System,"This research is aimed to propose an artificial intelligence algorithm comprising an ontology-based design,textmining, and natural language processing for automatically generating gap-fill multiple choice questions (MCQs). The simulation of this research demonstrated an application of the algorithm in generating gap-fill MCQs about software testing. The simulation results revealed that by using 103 online documents as inputs, the algorithm could automatically produce more than 16 thousand valid gap-fill MCQs covering a variety of topics in the software testing domain. Finally, in the discussion section of this paper we suggest how the proposed algorithm should be applied to produce gap-fill MCQs being collected in a question pool used by a knowledge expert system.","['Pornpat Sirithumgul', 'Pimpaka Prasertsilp', 'Lorne Olfman']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.11421,Anomali
Named Entity Recognition and Classification on Historical Documents: A Survey,"After decades of massive digitisation, an unprecedented amount of historical documents is available in digital format, along with their machine-readabletexts. While this represents a major step forward with respect to preservation and accessibility, it also opens up new opportunities in terms of contentminingand the next fundamental challenge is to develop appropriate technologies to efficiently search, retrieve and explore information from this 'big data of the past'. Among semantic indexing opportunities, the recognition and classification of named entities are in great demand among humanities scholars. Yet, named entity recognition (NER) systems are heavily challenged with diverse, historical and noisy inputs. In this survey, we present the array of challenges posed by historical documents to NER, inventory existing resources, describe the main approaches deployed so far, and identify key priorities for future developments.","['Maud Ehrmann', 'Ahmed Hamdi', 'Elvys Linhares Pontes', 'Matteo Romanello', 'Antoine Doucet']",ACM Computing Surveys 56-2 (2023) 1-47,arXiv,2021,https://doi.org/10.48550/arXiv.2109.11406,Anomali
Memory-Efficient Convex Optimization for Self-Dictionary Separable Nonnegative Matrix Factorization: A Frank-Wolfe Approach,"Nonnegative matrix factorization (NMF) often relies on the separability condition for tractable algorithm design. Separability-based NMF is mainly handled by two types of approaches, namely, greedy pursuit and convex programming. A notable convex NMF formulation is the so-called self-dictionary multiple measurement vectors (SD-MMV), which can work without knowing the matrix rank a priori, and is arguably more resilient to error propagation relative to greedy pursuit. However, convex SD-MMV renders a large memory cost that scales quadratically with the problem size. This memory challenge has been around for a decade, and a major obstacle for applying convex SD-MMV to big data analytics. This work proposes a memory-efficient algorithm for convex SD-MMV. Our algorithm capitalizes on the special update rules of a classic algorithm from the 1950s, namely, the Frank-Wolfe (FW) algorithm. It is shown that, under reasonable conditions, the FW algorithm solves the noisy SD-MMV problem with a memory cost that grows linearly with the amount of data. To handle noisier scenarios, a smoothed group sparsity regularizer is proposed to improve robustness while maintaining the low memory footprint with guarantees. The proposed approach presents the first linear memory complexity algorithmic framework for convex SD-MMV based NMF. The method is tested over a couple of unsupervised learning tasks, i.e.,textminingand community detection, to showcase its effectiveness and memory efficiency.","['Tri Nguyen', 'Xiao Fu', 'Ruiyuan Wu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2109.11135,Anomali
InvBERT: Reconstructing Text from Contextualized Word Embeddings by inverting the BERT pipeline,"Digital Humanities and Computational Literary Studies applytextminingmethods to investigate literature. Such automated approaches enable quantitative studies on large corpora which would not be feasible by manual inspection alone. However, due to copyright restrictions, the availability of relevant digitized literary works is limited. DerivedTextFormats (DTFs) have been proposed as a solution. Here, textual materials are transformed in such a way that copyright-critical features are removed, but that the use of certain analytical methods remains possible. Contextualized word embeddings produced by transformer-encoders (like BERT) are promising candidates for DTFs because they allow for state-of-the-art performance on various analytical tasks and, at first sight, do not disclose the originaltext. However, in this paper we demonstrate that under certain conditions the reconstruction of the original copyrightedtextbecomes feasible and its publication in the form of contextualized token representations is not safe. Our attempts to invert BERT suggest, that publishing the encoder as a black box together with the contextualized embeddings is critical, since it allows to generate data to train a decoder with a reconstruction accuracy sufficient to violate copyright laws.","['Kai Kugler', 'Simon Münker', 'Johannes Höhmann', 'Achim Rettinger']",,arXiv,2022,https://doi.org/10.48550/arXiv.2109.10104,Anomali
Efficient Measuring of Readability to Improve Documents Accessibility for Arabic Language Learners,"This paper presents an approach based on supervised machine learning methods to build a classifier that can identifytextcomplexity in order to present Arabic language learners withtextssuitable to their levels. The approach is based on machine learning classification methods to discriminate between the different levels of difficulty in reading and understanding atext. Several models were trained on a large corpusminedfrom online Arabic websites and manually annotated. The model uses both Count and TF-IDF representations and applies five machine learning algorithms; Multinomial Naive Bayes, Bernoulli Naive Bayes, Logistic Regression, Support Vector Machine and Random Forest, using unigrams and bigrams features. With the goal of extracting thetextcomplexity, the problem is usually addressed by formulating the level identification as a classification task. Experimental results showed that n-gram features could be indicative of the reading level of atextand could substantially improve performance, and showed that SVM and Multinomial Naive Bayes are the most accurate in predicting the complexity level. Best results were achieved using TF-IDF Vectors trained by a combination of word-based unigrams and bigrams with an overall accuracy of 87.14% over four classes of complexity.","['Sadik Bessou', 'Ghozlane Chenni']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.08648,Anomali
MOFSimplify: Machine Learning Models with Extracted Stability Data of Three Thousand Metal-Organic Frameworks,"We report a workflow and the output of a natural language processing (NLP)-based procedure tominethe extant metal-organic framework (MOF) literature describing structurally characterized MOFs and their solvent removal and thermal stabilities. We obtain over 2,000 solvent removal stability measures fromtextminingand 3,000 thermal decomposition temperatures from thermogravimetric analysis data. We assess the validity of our NLP methods and the accuracy of our extracted data by comparing to a hand-labeled subset. Machine learning (ML, i.e. artificial neural network) models trained on this data using graph- and pore-geometry-based representations enable prediction of stability on new MOFs with quantified uncertainty. Our web interface, MOFSimplify, provides users access to our curated data and enables them to harness that data for predictions on new MOFs. MOFSimplify also encourages community feedback on existing data and on ML model predictions for community-based active learning for improved MOF stability models.","['A. Nandy', 'G. Terrones', 'N. Arunachalam', 'C. Duan', 'D. W. Kastner', 'H. J. Kulik']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.08098,Anomali
InceptionXML: A Lightweight Framework with Synchronized Negative Sampling for Short Text Extreme Classification,"Automatic annotation of short-textdata to a large number of target labels, referred to as ShortTextExtreme Classification, has found numerous applications including prediction of related searches and product recommendation. In this paper, we propose a convolutional architecture InceptionXML which is light-weight, yet powerful, and robust to the inherent lack of word-order in short-textqueries encountered in search and recommendation. We demonstrate the efficacy of applying convolutions by recasting the operation along the embedding dimension instead of the word dimension as applied in conventional CNNs fortextclassification. Towards scaling our model to datasets with millions of labels, we also propose SyncXML pipeline which improves upon the shortcomings of the recently proposed dynamic hard-negativeminingtechnique for label short-listing by synchronizing the label-shortlister and extreme classifier. SyncXML not only reduces the inference time to half but is also an order of magnitude smaller than state-of-the-art Astec in terms of model size. Through a comprehensive empirical comparison, we show that not only can InceptionXML outperform existing approaches on benchmark datasets but also the transformer baselines requiring only 2% FLOPs. The code for InceptionXML is available at https://github.com/xmc-aalto/inceptionxml.","['Siddhant Kharbanda', 'Atmadeep Banerjee', 'Devaansh Gupta', 'Akash Palrecha', 'Rohit Babbar']",,arXiv,2024,https://doi.org/10.48550/arXiv.2109.07319,Anomali
PETGEN: Personalized Text Generation Attack on Deep Sequence Embedding-based Classification Models,"What should a malicious user write next to fool a detection model? Identifying malicious users is critical to ensure the safety and integrity of internet platforms. Several deep learning-based detection models have been created. However, malicious users can evade deep detection models by manipulating their behavior, rendering these models of little use. The vulnerability of such deep detection models against adversarial attacks is unknown. Here we create a novel adversarial attack model against deep user sequence embedding based classification models, which use the sequence of user posts to generate user embeddings and detect malicious users. In the attack, the adversary generates a new post to fool the classifier. We propose a novel end-to-end PersonalizedTextGeneration Attack model, called PETGEN, that simultaneously reduces the efficacy of the detection model and generates posts that have several key desirable properties. Specifically, PETGEN generates posts that are personalized to the user's writing style, have knowledge about a given target context, are aware of the user's historical posts on the target context, and encapsulate the user's recent topical interests. We conduct extensive experiments on two real-world datasets (Yelp and Wikipedia, both with ground-truth of malicious users) to show that PETGEN significantly reduces the performance of popular deep user sequence embedding-based classification models. PETGEN outperforms five attack baselines in terms oftextquality and attack efficacy in both white-box and black-box classifier settings. Overall, this work paves the path towards the next generation of adversary-aware sequence classification models.","['Bing He', 'Mustaque Ahamad', 'Srijan Kumar']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.06777,Anomali
Euphemistic Phrase Detection by Masked Language Model,"It is a well-known approach for fringe groups and organizations to use euphemisms -- ordinary-sounding and innocent-looking words with a secret meaning -- to conceal what they are discussing. For instance, drug dealers often use ""pot"" for marijuana and ""avocado"" for heroin. From a social media content moderation perspective, though recent advances in NLP have enabled the automatic detection of such single-word euphemisms, no existing work is capable of automatically detecting multi-word euphemisms, such as ""blue dream"" (marijuana) and ""black tar"" (heroin). Our paper tackles the problem of euphemistic phrase detection without human effort for the first time, as far as we are aware. We first perform phraseminingon a rawtextcorpus (e.g., social media posts) to extract quality phrases. Then, we utilize word embedding similarities to select a set of euphemistic phrase candidates. Finally, we rank those candidates by a masked language model -- SpanBERT. Compared to strong baselines, we report 20-50% higher detection accuracies using our algorithm for detecting euphemistic phrases.","['Wanzheng Zhu', 'Suma Bhat']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.04666,Anomali
IMG2SMI: Translating Molecular Structure Images to Simplified Molecular-input Line-entry System,"Like many scientific fields, new chemistry literature has grown at a staggering pace, with thousands of papers released every month. A large portion of chemistry literature focuses on new molecules and reactions between molecules. Most vital information is conveyed through 2-D images of molecules, representing the underlying molecules or reactions described. In order to ensure reproducible and machine-readable molecule representations,text-based molecule descriptors like SMILES and SELFIES were created. Thesetext-based molecule representations provide molecule generation but are unfortunately rarely present in published literature. In the absence of molecule descriptors, the generation of molecule descriptors from the 2-D images present in the literature is necessary to understand chemistry literature at scale. Successful methods such as Optical Structure Recognition Application (OSRA), and ChemSchematicResolver are able to extract the locations of molecules structures in chemistry papers and infer molecular descriptions and reactions. While effective, existing systems expect chemists to correct outputs, making them unsuitable for unsupervised large-scale datamining. Leveraging the task formulation of image captioning introduced by DECIMER, we introduce IMG2SMI, a model which leverages Deep Residual Networks for image feature extraction and an encoder-decoder Transformer layers for molecule description generation. Unlike previous Neural Network-based systems, IMG2SMI builds around the task of molecule description generation, which enables IMG2SMI to outperform OSRA-based systems by 163% in molecule similarity prediction as measured by the molecular MACCS Fingerprint Tanimoto Similarity. Additionally, to facilitate further research on this task, we release a new molecule prediction dataset. including 81 million molecules for molecule description generation","['Daniel Campos', 'Heng Ji']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.04202,Anomali
Knowledge mining of unstructured information: application to cyber-domain,"Information on cyber-related crimes, incidents, and conflicts is abundantly available in numerous open online sources. However, processing the large volumes and streams of data is a challenging task for the analysts and experts, and entails the need for newer methods and techniques. In this article we present and implement a novel knowledge graph and knowledgeminingframework for extracting the relevant information from free-formtextabout incidents in the cyberdomain. The framework includes a machine learning based pipeline for generating graphs of organizations, countries, industries, products and attackers with a non-technical cyber-ontology. The extracted knowledge graph is utilized to estimate the incidence of cyberattacks on a given graph configuration. We use publicly available collections of real cyber-incident reports to test the efficacy of our methods. The knowledge extraction is found to be sufficiently accurate, and the graph-based threat estimation demonstrates a level of correlation with the actual records of attacks. In practical use, an analyst utilizing the presented framework can infer additional information from the current cyber-landscape in terms of risk to various entities and propagation of the risk heuristic between industries and countries.","['Tuomas Takko', 'Kunal Bhattacharya', 'Martti Lehto', 'Pertti Jalasvirta', 'Aapo Cederberg', 'Kimmo Kaski']",,arXiv,2022,https://doi.org/10.48550/arXiv.2109.03848,Anomali
Recommend for a Reason: Unlocking the Power of Unsupervised Aspect-Sentiment Co-Extraction,"Compliments and concerns in reviews are valuable for understanding users' shopping interests and their opinions with respect to specific aspects of certain items. Existing review-based recommenders favor large and complex language encoders that can only learn latent and uninterpretabletextrepresentations. They lack explicit user attention and item property modeling, which however could provide valuable information beyond the ability to recommend items. Therefore, we propose a tightly coupled two-stage approach, including an Aspect-Sentiment Pair Extractor (ASPE) and an Attention-Property-aware Rating Estimator (APRE). Unsupervised ASPEminesAspect-Sentiment pairs (AS-pairs) and APRE predicts ratings using AS-pairs as concrete aspect-level evidence. Extensive experiments on seven real-world Amazon Review Datasets demonstrate that ASPE can effectively extract AS-pairs which enable APRE to deliver superior accuracy over the leading baselines.","['Zeyu Li', 'Wei Cheng', 'Reema Kshetramade', 'John Houser', 'Haifeng Chen', 'Wei Wang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.03821,Anomali
ArGoT: A Glossary of Terms extracted from the arXiv,"We introduce ArGoT, a data set of mathematical terms extracted from  the articles hosted on the arXiv website.  A term is any mathematical concept defined in an article. Using labels in the article's source code and examples from other popular math websites, wemineall the terms in the arXiv data and compile a comprehensive vocabulary of mathematical terms. Each term can be then organized in a dependency graph by using the term's definitions and the arXiv's metadata. Using both hyperbolic and standard word embeddings, we demonstrate how  this structure is reflected in thetext'svector representation and how they capture relations of entailment in mathematical concepts. This data set is part of an ongoing effort to align natural mathematicaltextwith existing Interactive Theorem Prover  Libraries (ITPs) of formally verified statements.",['Luis Berlioz'],"EPTCS 342, 2021, pp. 14-21",arXiv,2021,https://doi.org/10.48550/arXiv.2109.02801,Anomali
Effective user intent mining with unsupervised word representation models and topic modelling,"Understanding the intent behind chat between customers and customer service agents has become a crucial problem nowadays due to an exponential increase in the use of the Internet by people from different cultures and educational backgrounds. More importantly, the explosion of e-commerce has led to a significant increase intextconversation between customers and agents. In this paper, we propose an approach to dataminingthe conversation intents behind the textual data. Using the customer service data set, we train unsupervisedtextrepresentation models, and then develop an intent mapping model which would rank the predefined intents base on cosine similarity between sentences and intents. Topic-modeling techniques are used to define intents and domain experts are also involved to interpret topic modelling results. With this approach, we can get a good understanding of the user intentions behind the unlabelled customer service textual data.",['Bencheng Wei'],,arXiv,2021,https://doi.org/10.48550/arXiv.2109.01765,Anomali
Towards Explaining STEM Document Classification using Mathematical Entity Linking,"Document subject classification is essential for structuring (digital) libraries and allowing readers to search within a specific field. Currently, the classification is typically made by human domain experts. Semi-supervised Machine Learning algorithms can support them by exploiting the labeled data to predict subject classes for unclassified new documents. However, while humans partly do, machines mostly do not explain the reasons for their decisions. Recently, explainable AI research to address the problem of Machine Learning decisions being a black box has increasingly gained interest. Explainer models have already been applied to the classification of natural languagetexts, such as legal or medical documents. Documents from Science, Technology, Engineering, and Mathematics (STEM) disciplines are more difficult to analyze, since they contain both textual and mathematical formula content. In this paper, we present first advances towards STEM document classification explainability using classical and mathematical Entity Linking. We examine relationships between textual and mathematical subject classes and entities,mininga collection of documents from the arXiv preprint repository (NTCIR and zbMATH dataset). The results indicate that mathematical entities have the potential to provide high explainability as they are a crucial part of a STEM document.","['Philipp Scharpf', 'Moritz Schubotz', 'Bela Gipp']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.00954,Anomali
ConQX: Semantic Expansion of Spoken Queries for Intent Detection based on Conditioned Text Generation,"Intent detection of spoken queries is a challenging task due to their noisy structure and short length. To provide additional information regarding the query and enhance the performance of intent detection, we propose a method for semantic expansion of spoken queries, called ConQX, which utilizes thetextgeneration ability of an auto-regressive language model, GPT-2. To avoid off-topictextgeneration, we condition the input query to a structured context with promptmining. We then apply zero-shot, one-shot, and few-shot learning. We lastly use the expanded queries to fine-tune BERT and RoBERTa for intent detection. The experimental results show that the performance of intent detection can be improved by our semantic expansion method.","['Eyup Halit Yilmaz', 'Cagri Toraman']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.00729,Anomali
Discourse Analysis of Covid-19 in Persian Twitter Social Networks Using Graph Mining and Natural Language Processing,"One of the new scientific ways of understanding discourse dynamics is analyzing the public data of social networks. This research's aim is Post-structuralist Discourse Analysis (PDA) of Covid-19 phenomenon (inspired by Laclau and Mouffe's Discourse Theory) by using Intelligent DataMiningfor Persian Society. The examined big data is five million tweets from 160,000 users of the Persian Twitter network to compare two discourses. Besides analyzing the tweettextsindividually, a social network graph database has been created based on retweets relationships. We use the VoteRank algorithm to introduce and rank people whose posts become word of mouth, provided that the total information spreading scope is maximized over the network. These users are also clustered according to their word usage pattern (the Gaussian Mixture Model is used). The constructed discourse of influential spreaders is compared to the most active users. This analysis is done based on Covid-related posts over eight episodes. Also, by relying on the statistical content analysis and polarity of tweet words, discourse analysis is done for the whole mentioned subpopulations, especially for the top individuals. The most important result of this research is that the Twitter subjects' discourse construction is government-based rather than community-based. The analyzed Iranian society does not consider itself responsible for the Covid-19 wicked problem, does not believe in participation, and expects the government to solve all problems. The most active and most influential users' similarity is that political, national, and critical discourse construction is the predominant one. In addition to the advantages of its research methodology, it is necessary to pay attention to the study's limitations. Suggestion for future encounters of Iranian society with similar crises is given.","['Omid Shokrollahi', 'Niloofar Hashemi', 'Mohammad Dehghani']",,arXiv,2021,https://doi.org/10.48550/arXiv.2109.00298,Anomali
Mining Insights on Metal-Organic Framework Synthesis from Scientific Literature Texts,"Identifying optimal synthesis conditions for metal-organic frameworks (MOFs) is a major challenge that can serve as a bottleneck for new materials discovery and development. Trial-and-error approach that relies on a chemist's intuition and knowledge has limitations in efficiency due to the large MOF synthesis space. To this end, 47,187 number of MOF were dataminedusing our in-house developed code to extract their synthesis information in 28,565 MOF papers. The joint machine learning/rule-based algorithm yields an average F1 score of 90.3 % across different synthesis parameters (i.e. metal precursors, organic precursors, solvents, temperature, time, composition). From this data set, a PU learning algorithm was developed to predict synthesis of a given MOF material using synthesis conditions as inputs, and this algorithm successfully predicted successful synthesis in 83.1 % of the synthesized data in the test set. Finally, our model correctly predicted three amorphous MOFs (with their representative experimental synthesis condition) as having low synthesizability scores while the counterpart crystalline MOFs showed high synthesizability scores. Our results show that big data extracted from thetextsof MOF papers can be used to rationally predict synthesis conditions for these materials, which can accelerate the speed in which new MOFs are synthesized.","['Hyunsoo Park', 'Yeonghun Kang', 'Wonyoung Choe', 'Jihan Kim']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.13590,Anomali
Using GAN-based models to sentimental analysis on imbalanced datasets in education domain,"While the whole world is still struggling with the COVID-19 pandemic, online learning and home office become more common. Many schools transfer their courses teaching to the online classroom. Therefore, it is significant tominethe students' feedback and opinions from their reviews towards studies so that both schools and teachers can know where they need to improve. This paper trains machine learning and deep learning models using both balanced and imbalanced datasets for sentiment classification. Two SOTA category-awaretextgeneration GAN models: CatGAN and SentiGAN, are utilized to synthesizetextused to balance the highly imbalanced dataset. Results on three datasets with different imbalance degree from distinct domains show that when using generatedtextto balance the dataset, the F1-score of machine learning and deep learning model on sentiment classification increases 2.79% ~ 9.28%. Also, the results indicate that the average growth degree for CR100k is higher than CR23k, the average growth degree for deep learning is more increased than machine learning algorithms, and the average growth degree for more complex deep learning models is more increased than simpler deep learning models in experiments.","['Ru Yang', 'Maryam Edalati']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.12061,Anomali
Web Image Context Extraction with Graph Neural Networks and Sentence Embeddings on the DOM tree,"Web Image Context Extraction (WICE) consists in obtaining the textual information describing an image using the content of the surrounding webpage. A common preprocessing step before performing WICE is to render the content of the webpage. When done at a large scale (e.g., for search engine indexation), it may become very computationally costly (up to several seconds per page). To avoid this cost, we introduce a novel WICE approach that combines Graph Neural Networks (GNNs) and Natural Language Processing models. Our method relies on a graph model containing both node types andtextas features. The model is fed through several blocks of GNNs to extract the textual context. Since no labeled WICE dataset with ground truth exists, we train and evaluate the GNNs on a proxy task that consists in finding the semantically closesttextto the image caption. We then interpret importance weights to find the most relevanttextnodes and define them as the image context. Thanks to GNNs, our model is able to encode both structural and semantic information from the webpage. We show that our approach gives promising results to help address the large-scale WICE problem using only HTML data.","['Chen Dang', 'Hicham Randrianarivo', ""Raphaël Fournier-S'Niehotta"", 'Nicolas Audebert']","GEM: Graph Embedding and Mining - ECML/PKDD Workshops, Sep 2021, Bilbao, Spain",arXiv,2021,https://doi.org/10.48550/arXiv.2108.11629,Anomali
Hybrid Multisource Feature Fusion for the Text Clustering,"Thetextclustering technique is an unsupervisedtextminingmethod which are used to partition a huge amount oftextdocuments into groups. It has been reported thattextclustering algorithms are hard to achieve better performance than supervised methods and their clustering performance is highly dependent on the pickedtextfeatures. Currently, there are many different types oftextfeature generation algorithms, each of which extractstextfeatures from some specific aspects, such as VSM and distributed word embedding, thus seeking a new way of obtaining features as complete as possible from the corpus is the key to enhance the clustering effects. In this paper, we present a hybrid multisource feature fusion (HMFF) framework comprising three components, feature representation of multimodel, mutual similarity matrices and feature fusion, in which we construct mutual similarity matrices for each feature source and fuse discriminative features from mutual similarity matrices by reducing dimensionality to generate HMFF features, then k-means clustering algorithm could be configured to partition input samples into groups. The experimental tests show our HMFF framework outperforms other recently published algorithms on 7 of 11 public benchmark datasets and has the leading performance on the rest 4 benchmark datasets as well. At last, we compare HMFF framework with those competitors on a COVID-19 dataset from the wild with the unknown cluster count, which shows the clusters generated by HMFF framework partition those similar samples much closer.","['Jiaxuan Chen', 'Shenglin Gui']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.10926,Anomali
SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining,"Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand intext. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbors of linked-entity.In SMedBERT, the mention-neighbor hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighboring entity structure. Apart from knowledge integration as external features, we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts oftextmentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations. Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks. It also improves the performance of other tasks such as question answering, question matching and natural language inference.","['Taolin Zhang', 'Zerui Cai', 'Chengyu Wang', 'Minghui Qiu', 'Bite Yang', 'Xiaofeng He']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.08983,Anomali
DESYR: Definition and Syntactic Representation Based Claim Detection on the Web,"The formulation of a claim rests at the core of argumentmining. To demarcate between a claim and a non-claim is arduous for both humans and machines, owing to latent linguistic variance between the two and the inadequacy of extensive definition-based formalization. Furthermore, the increase in the usage of online social media has resulted in an explosion of unsolicited information on the web presented as informaltext. To account for the aforementioned, in this paper, we proposed DESYR. It is a framework that intends on annulling the said issues for informal web-basedtextby leveraging a combination of hierarchical representation learning (dependency-inspired Poincare embedding), definition-based alignment, and feature projection. We do away with fine-tuning computer-heavy language models in favor of fabricating a more domain-centric but lighter approach. Experimental results indicate that DESYR builds upon the state-of-the-art system across four benchmark claim datasets, most of which were constructed with informaltexts. We see an increase of 3 claim-F1 points on the LESA-Twitter dataset, an increase of 1 claim-F1 point and 9 macro-F1 points on the Online Comments(OC) dataset, an increase of 24 claim-F1 points and 17 macro-F1 points on the Web Discourse(WD) dataset, and an increase of 8 claim-F1 points and 5 macro-F1 points on the MicroTexts(MT) dataset. We also perform an extensive analysis of the results. We make a 100-D pre-trained version of our Poincare-variant along with the source code.","['Megha Sundriyal', 'Parantak Singh', 'Md Shad Akhtar', 'Shubhashis Sengupta', 'Tanmoy Chakraborty']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.08759,Anomali
Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study,"An independent ethical assessment of an artificial intelligence system is an impartial examination of the system's development, deployment, and use in alignment with ethical values. System-level qualitative frameworks that describe high-level requirements and component-level quantitative metrics that measure individual ethical dimensions have been developed over the past few years. However, there exists a gap between the two, which hinders the execution of independent ethical assessments in practice. This study bridges this gap and designs a holistic independent ethical assessment process for atextclassification model with a special focus on the task of hate speech detection. The assessment is further augmented with protected attributesminingand counterfactual-based analysis to enhance bias assessment. It covers assessments of technical performance, data bias, embedding bias, classification bias, and interpretability. The proposed process is demonstrated through an assessment of a deep hate speech detection model.","['Amitoj Singh', 'Jingshu Chen', 'Lihao Zhang', 'Amin Rasekh', 'Ilana Golbin', 'Anand Rao']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.07627,Anomali
A Dataset for Answering Time-Sensitive Questions,"Time is an important dimension in our physical world. Lots of facts can evolve with respect to time. For example, the U.S. President might change every four years. Therefore, it is important to consider the time dimension and empower the existing QA models to reason over time. However, the existing QA datasets contain rather few time-sensitive questions, hence not suitable for diagnosing or benchmarking the model's temporal reasoning capability. In order to promote research in this direction, we propose to construct a time-sensitive QA dataset. The dataset is constructed by 1)miningtime-evolving facts from WikiData and aligning them to their corresponding Wikipedia page, 2) employing crowd workers to verify and calibrate these noisy facts, 3) generating question-answer pairs based on the annotated time-sensitive facts. Our dataset poses challenges in the aspect of both temporal understanding and temporal reasoning. We evaluate different SoTA long-document QA systems like BigBird and FiD on our dataset. The best-performing model FiD can only achieve 46\% accuracy, still far behind the human performance of 87\%. We demonstrate that these models are still lacking the ability to perform consistent temporal reasoning. Therefore, we believe that our dataset could serve as a benchmark to develop NLP models more sensitive to temporal shifts. The dataset and code are released in~\url{https://github.com/wenhuchen/Time-Sensitive-QA}.","['Wenhu Chen', 'Xinyi Wang', 'William Yang Wang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.06314,Anomali
TextBenDS: a generic Textual data Benchmark for Distributed Systems,"Extracting top-k keywords and documents using weighting schemes are popular techniques employed intextminingand machine learning for different analysis and retrieval tasks. The weights are usually computed in the data preprocessing step, as they are costly to update and keep track of all the modifications performed on the dataset. Furthermore, computation errors are introduced when analyzing only subsets of the dataset. Therefore, in a Big Data context, it is crucial to lower the runtime of computing weighting schemes, without hindering the analysis process and the accuracy of the machine learning algorithms. To address this requirement for the task of top-k keywords and documents, it is customary to design benchmarks that compare weighting schemes within various configurations of distributed frameworks and database management systems. Thus, we propose a generic document-oriented benchmark for storing textual data and constructing weighting schemes (TextBenDS). Our benchmark offers a generic data model designed with a multidimensional approach for storingtextdocuments. We also propose using aggregation queries with various complexities and selectivities for constructing term weighting schemes, that are utilized in extracting top-k keywords and documents. We evaluate the computing performance of the queries on several distributed environments set within the Apache Hadoop ecosystem. Our experimental results provide interesting insights. As an example, MongoDB proves to have the best overall performance, while Spark's execution time remains almost the same, regardless of the weighting schemes.","['Ciprian-Octavian Truica', 'Elena Apostol', 'Jérôme Darmont', 'Ira Assent']","Information Systems Frontiers, Springer Verlag, 2021, Breakthroughs on Cross-Cutting Data Management, Data Analytics and Applied Data Science, 23, pp.81-100",arXiv,2021,https://doi.org/10.48550/arXiv.2108.05689,Anomali
TrUMAn: Trope Understanding in Movies and Animations,"Understanding and comprehending video content is crucial for many real-world applications such as search and recommendation systems. While recent progress of deep learning has boosted performance on various tasks using visual cues, deep cognition to reason intentions, motivation, or causality remains challenging. Existing datasets that aim to examine video reasoning capability focus on visual signals such as actions, objects, relations, or could be answered utilizingtextbias. Observing this, we propose a novel task, along with a new dataset: Trope Understanding in Movies and Animations (TrUMAn), with 2423 videos associated with 132 tropes, intending to evaluate and develop learning systems beyond visual signals. Tropes are frequently used storytelling devices for creative works. By coping with the trope understanding task and enabling the deep cognition skills of machines, dataminingapplications and algorithms could be taken to the next level. To tackle the challenging TrUMAn dataset, we present a Trope Understanding and Storytelling (TrUSt) with a new Conceptual Storyteller module, which guides the video encoder by performing video storytelling on a latent space. Experimental results demonstrate that state-of-the-art learning systems on existing tasks reach only 12.01% of accuracy with raw input signals. Also, even in the oracle case with human-annotated descriptions, BERT contextual embedding achieves at most 28% of accuracy. Our proposed TrUSt boosts the model performance and reaches 13.94% performance. We also provide detailed analysis to pave the way for future research. TrUMAn is publicly available at:https://www.cmlab.csie.ntu.edu.tw/project/trope","['Hung-Ting Su', 'Po-Wei Shen', 'Bing-Chen Tsai', 'Wen-Feng Cheng', 'Ke-Jyun Wang', 'Winston H. Hsu']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.04542,Anomali
A Bayesian Nonparametric Estimation of Mutual Information,"Mutual information is a widely-used information theoretic measure to quantify the amount of association between variables. It is used extensively in many applications such as image registration, diagnosis of failures in electrical machines, pattern recognition, dataminingand tests of independence. The main goal of this paper is to provide an efficient estimator of the mutual information based on the approach of Al Labadi et. al. (2021). The estimator is explored through various examples and is compared to its frequentist counterpart due to Berrett et al. (2019). The results show the good performance of the procedure by having a smaller mean squared error.","['Luai Al-Labadi', 'Forough Fazeli-Asl', 'Zahra Saberi']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.03780,Anomali
Tell me a story about yourself: The words of shopping experience and self-satisfaction,"In this paper we investigate the verbal expression of shopping experience obtained by a sample of customers asked to freely verbalize how they felt when entering a store. Using novel tools ofTextMiningand Social Network Analysis, we analyzed the interviews to understand the connection between the emotions aroused during the shopping experience, satisfaction and the way participants link these concepts to self-satisfaction and self-identity. The results show a prominent role of emotions in the discourse about the shopping experience before purchasing and an inward-looking connection to the self. Our results also suggest that modern retail environment should enhance the hedonic shopping experience in terms of fun, fantasy, moods, and emotions.","['L Petruzzellis', 'A Fronzetti Colladon', 'M Visentin', 'J. -C. Chebat']","Journal of Retailing and Consumer Services 63, 102703 (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2108.03016,Anomali
Differentially Private n-gram Extraction,"We revisit the problem of $n$-gram extraction in the differential privacy setting. In this problem, given a corpus of privatetextdata, the goal is to release as many $n$-grams as possible while preserving user level privacy. Extracting $n$-grams is a fundamental subroutine in many NLP applications such as sentence completion, response generation for emails etc. The problem also arises in other applications such as sequencemining, and is a generalization of recently studied differentially private set union (DPSU). In this paper, we develop a new differentially private algorithm for this problem which, in our experiments, significantly outperforms the state-of-the-art. Our improvements stem from combining recent advances in DPSU, privacy accounting, and new heuristics for pruning in the tree-based approach initiated by Chen et al. (2012).","['Kunho Kim', 'Sivakanth Gopi', 'Janardhan Kulkarni', 'Sergey Yekhanin']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.02831,Anomali
A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python,"In the age of big data and machine learning, at a time when the techniques and methods of software development are evolving rapidly, a problem has arisen: programmers can no longer detect all the security flaws and vulnerabilities in their code manually. To overcome this problem, developers can now rely on automatic techniques, like machine learning based prediction models, to detect such issues. An inherent property of such approaches is that they work with numeric vectors (i.e., feature vectors) as inputs. Therefore, one needs to transform the source code into such feature vectors, often referred to as code embedding. A popular approach for code embedding is to adapt natural language processing techniques, liketextrepresentation, to automatically derive the necessary features from the source code. However, the suitability and comparison of differenttextrepresentation techniques for solving Software Engineering (SE) problems is rarely studied systematically. In this paper, we present a comparative study on three populartextrepresentation methods, word2vec, fastText, and BERT applied to the SE task of detecting vulnerabilities in Python code. Using a dataminingapproach, we collected a large volume of Python source code in both vulnerable and fixed forms that we embedded with word2vec, fastText, and BERT to vectors and used a Long Short-Term Memory network to train on them. Using the same LSTM architecture, we could compare the efficiency of the different embeddings in deriving meaningful feature vectors. Our findings show that all thetextrepresentation methods are suitable for code representation in this particular task, but the BERT model is the most promising as it is the least time consuming and the LSTM model based on it achieved the best overall accuracy(93.8%) in predicting Python source code vulnerabilities.","['Amirreza Bagheri', 'Péter Hegedűs']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.02044,Anomali
Medical Literature Mining and Retrieval in a Conversational Setting,"The Covid-19 pandemic has caused a spur in the medical research literature. With new research advances in understanding the virus, there is a need for robusttextminingtools which can process, extract and present answers from the literature in a concise and consumable way. With a DialoGPT based multi-turn conversation generation module, and BM-25 \& neural embeddings based ensemble information retrieval module, in this paper we present a conversational system, which can retrieve and answer coronavirus-related queries from the rich medical literature, and present it in a conversational setting with the user. We further perform experiments to compare neural embedding-based document retrieval and the traditional BM25 retrieval algorithm and report the results.","['Souvik Das', 'Sougata Saha', 'Rohini K. Srihari']",,arXiv,2021,https://doi.org/10.48550/arXiv.2108.01436,Anomali
"Text Mining Undergraduate Engineering Programs' Applications: the Role of Gender, Nationality, and Socio-economic Status","Women, visible minorities, and other socially disadvantaged groups continue to be underrepresented in STEM education. Understanding students' motivations for pursuing a STEM major, and the roles gender, nationality, parental education attainment, and socio-economic background play in shaping students' motivations can support the design of more effective recruitment efforts towards these groups. In this paper, we propose and develop a noveltextminingapproach incorporating the Latent Dirichlet Allocation and word embeddings to analyze applicants' motivational factors for choosing an engineering program. We apply the proposed method to a dataset of 43,645 applications to the engineering school of a large Canadian university. We then investigate the relationship between applicants' gender, nationality, and family income and educational attainment, and their stated motivations for applying to their engineering program of choice. We find that interest in technology and the desire to make social impact are the two most powerful motivators for applicants. Additionally, while we find significant motivational differences related to applicants' nationality and family socio-economic status, gender has the strongest and the most robust impact on students' motivations for studying engineering.","['Bo Lin', 'Bissan Ghaddar', 'Ada Hurst']",,arXiv,2022,https://doi.org/10.48550/arXiv.2107.14034,Anomali
Impacts Towards a comprehensive assessment of the book impact by integrating multiple evaluation sources,"The surge in the number of books published makes the manual evaluation methods difficult to efficiently evaluate books. The use of books' citations and alternative evaluation metrics can assist manual evaluation and reduce the cost of evaluation. However, most existing evaluation research was based on a single evaluation source with coarse-grained analysis, which may obtain incomprehensive or one-sided evaluation results of book impact. Meanwhile, relying on a single resource for book assessment may lead to the risk that the evaluation results cannot be obtained due to the lack of the evaluation data, especially for newly published books. Hence, this paper measured book impact based on an evaluation system constructed by integrating multiple evaluation sources. Specifically, we conducted finer-grainedminingon the multiple evaluation sources, including books' internal evaluation resources and external evaluation resources. Various technologies (e.g. topic extraction, sentiment analysis,textclassification) were used to extract corresponding evaluation metrics from the internal and external evaluation resources. Then, Expert evaluation combined with analytic hierarchy process was used to integrate the evaluation metrics and construct a book impact evaluation system. Finally, the reliability of the evaluation system was verified by comparing with the results of expert evaluation, detailed and diversified evaluation results were then obtained. The experimental results reveal that differential evaluation resources can measure the books' impacts from different dimensions, and the integration of multiple evaluation data can assess books more comprehensively. Meanwhile, the book impact evaluation system can provide personalized evaluation results according to the users' evaluation purposes. In addition, the disciplinary differences should be considered for assessing books' impacts.","['Qingqing Zhou', 'Chengzhi Zhang']","Journal of Informetrics, 2021. 15(3): 101162",arXiv,2021,https://doi.org/10.48550/arXiv.2107.10434,Anomali
Checkovid: A COVID-19 misinformation detection system on Twitter using network and content mining perspectives,"During the COVID-19 pandemic, social media platforms were ideal for communicating due to social isolation and quarantine. Also, it was the primary source of misinformation dissemination on a large scale, referred to as the infodemic. Therefore, automatic debunking misinformation is a crucial problem. To tackle this problem, we present two COVID-19 related misinformation datasets on Twitter and propose a misinformation detection system comprising network-based and content-based processes based on machine learning algorithms and NLP techniques. In the network-based process, we focus on social properties, network characteristics, and users. On the other hand, we classify misinformation using the content of the tweets directly in the content-based process, which containstextclassification models (paragraph-level and sentence-level) and similarity models. The evaluation results on the network-based process show the best results for the artificial neural network model with an F1 score of 88.68%. In the content-based process, our novel similarity models, which obtained an F1 score of 90.26%, show an improvement in the misinformation classification results compared to the network-based models. In addition, in thetextclassification models, the best result was achieved using the stacking ensemble-learning model by obtaining an F1 score of 95.18%. Furthermore, we test our content-based models on the Constraint@AAAI2021 dataset, and by getting an F1 score of 94.38%, we improve the baseline results. Finally, we develop a fact-checking website called Checkovid that uses each process to detect misinformative and informative claims in the domain of COVID-19 from different perspectives.","['Sajad Dadgar', 'Mehdi Ghatee']",,arXiv,2021,https://doi.org/10.48550/arXiv.2107.09768,Anomali
Stock Movement Prediction with Financial News using Contextualized Embedding from BERT,"News events can greatly influence equity markets. In this paper, we are interested in predicting the short-term movement of stock prices after financial news events using only the headlines of the news. To achieve this goal, we introduce a newtextminingmethod called Fine-Tuned Contextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with previous approaches which use static vector representations of the news (static embedding), our model uses contextualized vector representations of the headlines (contextualized embeddings) generated from Bidirectional Encoder Representations from Transformers (BERT). Our model obtains the state-of-the-art result on this stock movement prediction task. It shows significant improvement compared with other baseline models, in both accuracy and trading simulations. Through various trading simulations based on millions of headlines from Bloomberg News, we demonstrate the ability of this model in real scenarios.",['Qinkai Chen'],,arXiv,2021,https://doi.org/10.48550/arXiv.2107.08721,Anomali
Duplicated Code Pattern Mining in Visual Programming Languages,"Visual Programming Languages (VPLs), coupled with the high-level abstractions that are commonplace in visual programming environments, enable users with less technical knowledge to become proficient programmers. However, the lower skill floor required by VPLs also entails that programmers are more likely to not adhere to best practices of software development, producing systems with high technical debt, and thus poor maintainability. Duplicated code is one important example of such technical debt. In fact, we observed that the amount of duplication in the OutSystems VPL code bases can reach as high as $39\%$.
  Duplicated code detection intext-based programming languages is still an active area of research with important implications regarding software maintainability and evolution. However, to the best of our knowledge, the literature on duplicated code detection for VPLs is very limited. We propose a novel and scalable duplicated code patternminingalgorithm that leverages the visual structure of VPLs in order to not only detect duplicated code, but also highlight duplicated code patterns that explain the reported duplication. The performance of the proposed approach is evaluated on a wide range of real-world mobile and web applications developed using OutSystems.","['Miguel Terra-Neves', 'João Nadkarni', 'Miguel Ventura', 'Pedro Resende', 'Hugo Veiga', 'António Alegria']",,arXiv,2021,https://doi.org/10.48550/arXiv.2107.07212,Anomali
Robust Learning for Text Classification with Multi-source Noise Simulation and Hard Example Mining,"Many real-world applications involve the use of Optical Character Recognition (OCR) engines to transform handwritten images into transcripts on which downstream Natural Language Processing (NLP) models are applied. In this process, OCR engines may introduce errors and inputs to downstream NLP models become noisy. Despite that pre-trained models achieve state-of-the-art performance in many NLP benchmarks, we prove that they are not robust to noisytextsgenerated by real OCR engines. This greatly limits the application of NLP models in real-world scenarios. In order to improve model performance on noisy OCR transcripts, it is natural to train the NLP model on labelled noisytexts. However, in most cases there are only labelled cleantexts. Since there is no handwritten pictures corresponding to thetext, it is impossible to directly use the recognition model to obtain noisy labelled data. Human resources can be employed to copytextsand take pictures, but it is extremely expensive considering the size of data for model training. Consequently, we are interested in making NLP models intrinsically robust to OCR errors in a low resource manner. We propose a novel robust training framework which 1) employs simple but effective methods to directly simulate natural OCR noises from cleantextsand 2) iterativelyminesthe hard examples from a large number of simulated samples for optimal performance. 3) To make our model learn noise-invariant representations, a stability loss is employed. Experiments on three real-world datasets show that the proposed framework boosts the robustness of pre-trained models by a large margin. We believe that this work can greatly promote the application of NLP models in actual scenarios, although the algorithm we use is simple and straightforward. We make our codes and three datasets publicly available\footnote{https://github.com/tal-ai/Robust-learning-MSSHEM}.","['Guowei Xu', 'Wenbiao Ding', 'Weiping Fu', 'Zhongqin Wu', 'Zitao Liu']",,arXiv,2021,https://doi.org/10.48550/arXiv.2107.07113,Anomali
"Design, simulation and characterization of integrated photonic spectrographs for Astronomy I: Generation-I AWG devices based on canonical layouts","We present an experimental study on our first generation of custom-developed arrayed waveguide gratings (AWG) on silica platform for spectroscopic applications in near-infrared astronomy. We provide a comprehensive description of the design, numerical simulation and characterization of several AWG devices aimed at spectral resolving powers of 15,000 - 60,000 in the astronomical H-band. We evaluate the spectral characteristics of the fabricated devices in terms of insertion loss and estimated spectral resolving power and compare the results with numerical simulations. We estimate resolving powers of up to 18,900 from the output channel 3-dB transmission bandwidth. Based on the first characterization results, we select two candidate AWGs for further processing by removal of the output waveguide array and polishing the output facet to optical quality with the goal of integration as the primary diffractive element in a cross-dispersed spectrograph. We further study the imaging properties of the processed AWGs with regards to spectral resolution in direct imaging mode, geometry-related defocus aberration, and polarization sensitivity of the spectral image. We identify phase error control, birefringence control, and aberration suppression as the three key areas of future research and development in the field of high-resolution AWG-based spectroscopy in astronomy.","['Andreas Stoll', 'Kalaga V. Madhav', 'Martin M. Roth']","Optics Express Vol. 29, Issue 16, pp. 24947-24971 (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2107.06342,Anomali
GitQ- Towards Using Badges as Visual Cues for GitHub Projects,"GitHub hosts millions of software repositories, facilitating developers to contribute to many projects in multiple ways. Most of the information about the repositories istext-based in the form of stars, forks, commits, and so on. However, developers willing to contribute to projects on GitHub often find it challenging to select appropriate projects to contribute to or reuse due to the large number of repositories present on GitHub. Further, obtaining this required information often becomes a tedious process, as one has to carefullymineinformation hidden inside the repository. To alleviate the effort intensiveminingprocedures, researchers have proposed npm-badges to outline information relating to build status of a project. However, these badges are static and limit their usage to package dependency and build details. Adding visual cues such as badges to the repositories might reduce the search space for developers. Hence, we present GitQ, to automatically augment GitHub repositories with badges representing information about source code and project maintenance. Presenting GitQ as a browser plugin to GitHub could make it easily accessible to developers using GitHub. GitQ is evaluated with 15 developers based on the UTAUT model to understand developer perception towards its usefulness. We observed that 11 out of 15 developers perceived GitQ to be useful in identifying the right set of repositories using visual cues such as generated by GitQ. The source code and tool are available for download on GitHub at https://github.com/gitq-for-github/plugin, and the demo can be found at https://youtu.be/c0yohmIat3A.","['Akhila Sri Manasa Venigalla', 'Kowndinya Boyalakunta', 'Sridhar Chimalakonda']",,arXiv,2022,https://doi.org/10.48550/arXiv.2107.03761,Anomali
AdaSpeech 3: Adaptive Text to Speech for Spontaneous Style,"While recenttextto speech (TTS) models perform very well in synthesizing reading-style (e.g., audiobook) speech, it is still challenging to synthesize spontaneous-style speech (e.g., podcast or conversation), mainly because of two reasons: 1) the lack of training data for spontaneous speech; 2) the difficulty in modeling the filled pauses (um and uh) and diverse rhythms in spontaneous speech. In this paper, we develop AdaSpeech 3, an adaptive TTS system that fine-tunes a well-trained reading-style TTS model for spontaneous-style speech. Specifically, 1) to insert filled pauses (FP) in thetextsequence appropriately, we introduce an FP predictor to the TTS model; 2) to model the varying rhythms, we introduce a duration predictor based on mixture of experts (MoE), which contains three experts responsible for the generation of fast, medium and slow speech respectively, and fine-tune it as well as the pitch predictor for rhythm adaptation; 3) to adapt to other speaker timbre, we fine-tune some parameters in the decoder with few speech data. To address the challenge of lack of training data, weminea spontaneous speech dataset to support our research this work and facilitate future research on spontaneous TTS. Experiments show that AdaSpeech 3 synthesizes speech with natural FP and rhythms in spontaneous styles, and achieves much better MOS and SMOS scores than previous adaptive TTS systems.","['Yuzi Yan', 'Xu Tan', 'Bohan Li', 'Guangyan Zhang', 'Tao Qin', 'Sheng Zhao', 'Yuan Shen', 'Wei-Qiang Zhang', 'Tie-Yan Liu']",,arXiv,2021,https://doi.org/10.48550/arXiv.2107.02530,Anomali
Contradiction Detection in Persian Text,"Detection of semantic contradictory sentences is one of the most challenging and fundamental issues for NLP applications such as recognition of textual entailments. Contradiction in this study includes different types of semantic confrontation, such as conflict and antonymy. Due to lack of sufficient data to apply precise machine learning and specifically deep learning methods to Persian and other low resource languages, rule-based approaches that can function similarly to these systems will be of a great interest. Also recently, emergence of new methods such as transfer learning, has opened up the possibility of deep learning for low-resource languages. Considering two above points, in this study, along with a simple rule-base baseline, a novel rule-base system for identifying semantic contradiction along with a Bert base deep contradiction detection system for Persiantextshave been introduced. The rule base system has used frequent ruleminingmethod to extract appropriate contradiction rules using a development set. Extracted rules are tested for different categories of contradictory sentences. In this system the maximum f-measure among contradiction categories is obtained for negation about 90% and the average F-measure of system for all classes is about 76% which outperforms other algorithms on Persiantexts. On the other hand, because of medium performance of rule base system for some categories of contradiction, we use a Bert base deep learning system using our translated dataset; with average F-measure of 73. Our hybrid system has f-measure of about 80.","['Zeinab Rahimi', 'Mehrnoush ShamsFard']",,arXiv,2021,https://doi.org/10.48550/arXiv.2107.01987,Anomali
"Audio-Oriented Multimodal Machine Comprehension: Task, Dataset and Model","While Machine Comprehension (MC) has attracted extensive research interests in recent years, existing approaches mainly belong to the category of Machine Reading Comprehension task whichminestextual inputs (paragraphs and questions) to predict the answers (choices ortextspans). However, there are a lot of MC tasks that accept audio input in addition to the textual input, e.g. English listening comprehension test. In this paper, we target the problem of Audio-Oriented Multimodal Machine Comprehension, and its goal is to answer questions based on the given audio and textual information. To solve this problem, we propose a Dynamic Inter- and Intra-modality Attention (DIIA) model to effectively fuse the two modalities (audio and textual). DIIA can work as an independent component and thus be easily integrated into existing MC models. Moreover, we further develop a Multimodal Knowledge Distillation (MKD) module to enable our multimodal MC model to accurately predict the answers based only on either thetextor the audio. As a result, the proposed approach can handle various tasks including: Audio-Oriented Multimodal Machine Comprehension, Machine Reading Comprehension and Machine Listening Comprehension, in a single model, making fair comparisons possible between our model and the existing unimodal MC models. Experimental results and analysis prove the effectiveness of the proposed approaches. First, the proposed DIIA boosts the baseline models by up to 21.08% in terms of accuracy; Second, under the unimodal scenarios, the MKD module allows our multimodal MC model to significantly outperform the unimodal models by up to 18.87%, which are trained and tested with only audio or textual data.","['Zhiqi Huang', 'Fenglin Liu', 'Xian Wu', 'Shen Ge', 'Helin Wang', 'Wei Fan', 'Yuexian Zou']",,arXiv,2021,https://doi.org/10.48550/arXiv.2107.01571,Anomali
Exploiting Cross-Session Information for Session-based Recommendation with Graph Neural Networks,"Different from the traditional recommender system, the session-based recommender system introduces the concept of the session, i.e., a sequence of interactions between a user and multiple items within a period, to preserve the user's recent interest. The existing work on the session-based recommender system mainly relies onminingsequential patterns within individual sessions, which are not expressive enough to capture more complicated dependency relationships among items. In addition, it does not consider the cross-session information due to the anonymity of the session data, where the linkage between different sessions is prevented. In this paper, we solve these problems with the graph neural networks technique. First, each session is represented as a graph rather than a linear sequence structure, based on which a novel Full Graph Neural Network (FGNN) is proposed to learn complicated item dependency. To exploit and incorporate cross-session information in the individual session's representation learning, we further construct a Broadly Connected Session (BCS) graph to link different sessions and a novel Mask-Readout function to improve session embedding based on the BCS graph. Extensive experiments have been conducted on two e-commerce benchmark datasets, i.e., Yoochoose and Diginetica, and the experimental results demonstrate the superiority of our proposal through comparisons with state-of-the-art session-based recommender models.","['Ruihong Qiu', 'Zi Huang', 'Jingjing Li', 'Hongzhi Yin']",,arXiv,2021,https://doi.org/10.48550/arXiv.2107.00852,Anomali
Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction,"The recent advancement of pre-trained Transformer models has propelled the development of effectivetextminingmodels across various biomedical tasks. However, these models are primarily learned on the textual data and often lack the domain knowledge of the entities to capture the context beyond the sentence. In this study, we introduced a novel framework that enables the model to learn multi-omnics biological information about entities (proteins) with the help of additional multi-modal cues like molecular structure. Towards this, rather developing modality-specific architectures, we devise a generalized and optimized graph based multi-modal learning mechanism that utilizes the GraphBERT model to encode the textual and molecular structure information and exploit the underlying features of various modalities to enable end-to-end learning. We evaluated our proposed method on ProteinProtein Interaction task from the biomedical corpus, where our proposed generalized approach is observed to be benefited by the additional domain-specific modality.","['Sriram Pingali', 'Shweta Yadav', 'Pratik Dutta', 'Sriparna Saha']",,arXiv,2021,https://doi.org/10.48550/arXiv.2107.00596,Anomali
SATDBailiff- Mining and Tracking Self-Admitted Technical Debt,"Self-Admitted Technical Debt (SATD) is a metaphorical concept to describe the self-documented addition of technical debt to a software project in the form of source code comments. SATD can linger in projects and degrade source-code quality, but it can also be more visible than unintentionally added or undocumented technical debt. Understanding the implications of adding SATD to a software project is important because developers can benefit from a better understanding of the quality trade-offs they are making. However, empirical studies, analyzing the survivability and removal of SATD comments, are challenged by potential code changes or SATD comment updates that may interfere with properly tracking their appearance, existence, and removal. In this paper, we propose SATDBailiff, a tool that uses an existing state-of-the-art SATD detection tool, to identify SATD in method comments, then properly track their lifespan. SATDBailiff is given as input links to open source projects, and its output is a list of all identified SATDs, and for each detected SATD, SATDBailiff reports all its associated changes, including any updates to itstext, all the way to reporting its removal. The goal of SATDBailiff is to aid researchers and practitioners in better tracking SATDs instances and providing them with a reliable tool that can be easily extended. SATDBailiff was validated using a dataset of previously detected and manually validated SATD instances.
  SATDBailiff is publicly available as an open-source, along with the manual analysis of SATD instances associated with its validation, on the project website","['Eman Abdullah AlOmar', 'Ben Christians', 'Mihal Busho', 'Ahmed Hamad AlKhalid', 'Ali Ouni', 'Christian Newman', 'Mohamed Wiem Mkaouer']",,arXiv,2021,https://doi.org/10.48550/arXiv.2107.00073,Anomali
Classification of Consumer Belief Statements From Social Media,"Social media offer plenty of information to perform market research in order to meet the requirements of customers. One way how this research is conducted is that a domain expert gathers and categorizes user-generated content into a complex and fine-grained class structure. In many of such cases, little data meets complex annotations. It is not yet fully understood how this can be leveraged successfully for classification. We examine the classification accuracy of expert labels when used with a) many fine-grained classes and b) few abstract classes. For scenario b) we compare abstract class labels given by the domain expert as baseline and by automatic hierarchical clustering. We compare this to another baseline where the entire class structure is given by a completely unsupervised clustering approach. By doing so, this work can serve as an example of how complex expert annotations are potentially beneficial and can be utilized in the most optimal way for opinionminingin highly specific domains. By exploring across a range of techniques and experiments, we find that automated class abstraction approaches in particular the unsupervised approach performs remarkably well against domain expert baseline ontextclassification tasks. This has the potential to inspire opinionminingapplications in order to support market researchers in practice and to inspire fine-grained automated content analysis on a large scale.","['Gerhard Johann Hagerer', 'Wenbin Le', 'Hannah Danner', 'Georg Groh']",,arXiv,2023,https://doi.org/10.48550/arXiv.2106.15498,Anomali
Text mining and sentiment analysis of COVID-19 tweets,"The human severe acute respiratory syndrome coronavirus 2 (SARS-Cov-2), causing the COVID-19 disease, has continued to spread all over the world. It menacingly affects not only public health and global economics but also mental health and mood. While the impact of the COVID-19 pandemic has been widely studied, relatively fewer discussions about the sentimental reaction of the population have been available. In this article, we scrape COVID-19 related tweets on the microblogging platform, Twitter, and examine the tweets from Feb~24, 2020 to Oct~14, 2020 in four Canadian cities (Toronto, Montreal, Vancouver, and Calgary) and four U.S. cities (New York, Los Angeles, Chicago, and Seattle). Applying the Vader and NRC approaches, we evaluate the sentiment intensity scores and visualize the information over different periods of the pandemic. Sentiment scores for the tweets concerning three anti-epidemic measures, masks, vaccine, and lockdown, are computed for comparisons. The results of four Canadian cities are compared with four cities in the United States. We study the causal relationships between the infected cases, the tweet activities, and the sentiment scores of COVID-19 related tweets, by integrating the echo state network method with convergent cross-mapping. Our analysis shows that public sentiments regarding COVID-19 vary in different time periods and locations. In general, people have a positive mood about COVID-19 and masks, but negative in the topics of vaccine and lockdown. The causal inference shows that the sentiment influences people's activities on Twitter, which is also correlated to the daily number of infections.","['Qihuang Zhang', 'Grace Y. Yi', 'Li-Pang Chen', 'Wenqing He']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.15354,Anomali
A new system for evaluating brand importance: A use case from the fashion industry,"Today brand managers and marketing specialists can leverage huge amount of data to reveal patterns and trends in consumer perceptions, monitoring positive or negative associations of brands with respect to desired topics. In this study, we apply the Semantic Brand Score (SBS) indicator to assess brand importance in the fashion industry. To this purpose, we measure and visualizetextdata using the SBS Business Intelligence App (SBS BI), which relies on methods and tools oftextminingand social network analysis. We collected and analyzed about 206,000 tweets that mentioned the fashion brands Fendi, Gucci and Prada, during the period from March 5 to March 12, 2021. From the analysis of the three SBS dimensions - prevalence, diversity and connectivity - we found that Gucci dominated the discourse, with high values of SBS. We use this case study as an example to present a new system for evaluating brand importance and image, through the analysis of (big) textual data.","['A. Fronzetti Colladon', 'F. Grippa', 'L. Segneri']","13th ACM Web Science Conference 2021 (WebSci '21 Companion) (pp. 132-136). ACM, New York, NY, USA (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2106.14657,Anomali
Traditional Machine Learning and Deep Learning Models for Argumentation Mining in Russian Texts,"Argumentationminingis a field of computational linguistics that is devoted to extracting fromtextsand classifying arguments and relations between them, as well as constructing an argumentative structure. A significant obstacle to research in this area for the Russian language is the lack of annotated Russian-languagetextcorpora. This article explores the possibility of improving the quality of argumentationminingusing the extension of the Russian-language version of the Argumentative Microtext Corpus (ArgMicro) based on the machine translation of the Persuasive Essays Corpus (PersEssays). To make it possible to use these two corpora combined, we propose a Joint Argument Annotation Scheme based on the schemes used in ArgMicro and PersEssays. We solve the problem of classifying argumentative discourse units (ADUs) into two classes - ""pro"" (""for"") and ""opp"" (""against"") using traditional machine learning techniques (SVM, Bagging and XGBoost) and a deep neural network (BERT model). An ensemble of XGBoost and BERT models was proposed, which showed the highest performance of ADUs classification for both corpora.","['Irina Fishcheva', 'Valeriya Goloviznina', 'Evgeny Kotelnikov']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.14438,Anomali
Pairing Conceptual Modeling with Machine Learning,"Both conceptual modeling and machine learning have long been recognized as important areas of research. With the increasing emphasis on digitizing and processing large amounts of data for business and other applications, it would be helpful to consider how these areas of research can complement each other. To understand how they can be paired, we provide an overview of machine learning foundations and development cycle. We then examine how conceptual modeling can be applied to machine learning and propose a framework for incorporating conceptual modeling into data science projects. The framework is illustrated by applying it to a healthcare application. For the inverse pairing, machine learning can impact conceptual modeling throughtextand rulemining, as well as knowledge graphs. The pairing of conceptual modeling and machine learning in this this way should help lay the foundations for future research.","['Wolfgang Maass', 'Veda C. Storey']",Data & Knowledge Engineering (2021),arXiv,2021,https://doi.org/10.48550/arXiv.2106.14251,Anomali
From Symbols to Embeddings: A Tale of Two Representations in Computational Social Science,"Computational Social Science (CSS), aiming at utilizing computational methods to address social science problems, is a recent emerging and fast-developing field. The study of CSS is data-driven and significantly benefits from the availability of online user-generated contents and social networks, which contain richtextand network data for investigation. However, these large-scale and multi-modal data also present researchers with a great challenge: how to represent data effectively tominethe meanings we want in CSS? To explore the answer, we give a thorough review of data representations in CSS for bothtextand network. Specifically, we summarize existing representations into two schemes, namely symbol-based and embedding-based representations, and introduce a series of typical methods for each scheme. Afterwards, we present the applications of the above representations based on the investigation of more than 400 research articles from 6 top venues involved with CSS. From the statistics of these applications, we unearth the strength of each kind of representations and discover the tendency that embedding-based representations are emerging and obtaining increasing attention over the last decade. Finally, we discuss several key challenges and open issues for future directions. This survey aims to provide a deeper understanding and more advisable applications of data representations for CSS researchers.","['Huimin Chen', 'Cheng Yang', 'Xuanming Zhang', 'Zhiyuan Liu', 'Maosong Sun', 'Jianbin Jin']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.14198,Anomali
A Cascade Dual-Decoder Model for Joint Entity and Relation Extraction,"In knowledge graph construction, a challenging issue is how to extract complex (e.g., overlapping) entities and relationships from a small amount of unstructured historical data. The traditional pipeline methods are to divide the extraction into two separate subtasks, which misses the potential interaction between the two subtasks and may lead to error propagation. In this work, we propose an effective cascade dual-decoder method to extract overlapping relational triples, which includes atext-specific relation decoder and a relation-corresponded entity decoder. Our approach is straightforward and it includes atext-specific relation decoder and a relation-corresponded entity decoder. Thetext-specific relation decoder detects relations from a sentence at thetextlevel. That is, it does this according to the semantic information of the whole sentence. For each extracted relation, which is with trainable embedding, the relation-corresponded entity decoder detects the corresponding head and tail entities using a span-based tagging scheme. In this way, the overlapping triple problem can be tackled naturally. We conducted experiments on a real-world open-pitminedataset and two public datasets to verify the method's generalizability. The experimental results demonstrate the effectiveness and competitiveness of our proposed method and achieve better F1 scores under strict evaluation metrics. Our implementation is available at https://github.com/prastunlp/DualDec.","['Jian Cheng', 'Tian Zhang', 'Shuang Zhang', 'Huimin Ren', 'Guo Yu', 'Xiliang Zhang', 'Shangce Gao', 'Lianbo Ma']",,arXiv,2024,https://doi.org/10.48550/arXiv.2106.14163,Anomali
Digital libraries: textual analysis for a systematic review and meta-analysis,"Purpose: We seek to explore the realm of literature about digital libraries. We specifically seek to ascertain how interest in this subject has evolved, its impact, the most productive journals and countries, the number of occurrences of digital libraries, the relationships and dynamics of the main concepts mentioned, and the dynamics of metadata formats.Methods: We extracted corpora from the Google Scholar and Microsoft Academic Search bibliographic databases. We analyzed the named entities and concepts contained within these corpora with the help oftextminingtechnologies, CorTexT in particular.Results: While the number of publications on the subject of digital libraries is increasing, their average number of citations is decreasing. China, the United States and India are the most productive countries on the subject. Literature about conservation and national libraries has gradually been replaced by literature about open access, university libraries and the relationship with users. Internet Archive is the most cited digital library in literature and continues to grow. Dublin Core is the most talked about metadata format, however the subject of metadata formats is declining in the corpus today.Conclusion: Digital libraries now seem to be reaching the age of maturity.","['Mathieu Andro', 'Marc Maisonneuve']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.13469,Anomali
ABCD: A Graph Framework to Convert Complex Sentences to a Covering Set of Simple Sentences,"Atomic clauses are fundamentaltextunits for understanding complex sentences. Identifying the atomic sentences within complex sentences is important for applications such as summarization, argumentmining, discourse analysis, discourse parsing, and question answering. Previous work mainly relies on rule-based methods dependent on parsing. We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task. Our neural model learns to Accept, Break, Copy or Drop elements of a graph that combines word adjacency and grammatical dependencies. The full processing pipeline includes modules for graph construction, graph editing, and sentence generation from the output graph. We introduce DeSSE, a new dataset designed to train and evaluate complex sentence decomposition, and MinWiki, a subset of MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on MinWiki. On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis.","['Yanjun Gao', 'Ting-hao Huang', 'Rebecca J. Passonneau']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.12027,Anomali
The True Role of Active Communicators: An Empirical Study of Jazz Core Developers,"Context: Interest in software engineering (SE) methodologies and tools has been complemented in recent years by research efforts oriented towards understanding the human processes involved in software development. This shift has been imperative given reports of inadequately performing teams and the consequent growing emphasis on individuals and team relations in contemporary SE methods. Objective: While software repositories have frequently been studied with a view to explaining such human processes, research has tended to use primarily quantitative analysis approaches. There is concern, however, that such approaches can provide only a partial picture of the software process. Given the way human behavior is nuanced within psychological and social contexts, it has been asserted that a full understanding may only be achieved through deeper contextual enquiries. Method: We have followed such an approach and have applied datamining, SNA, psycholinguistic analysis and directed content analysis (CA) to study the way core developers at IBM Rational Jazz contribute their social and intellectual capital, and have compared the attitudes, interactions and activities of these members to those of their less active counterparts. Results: Among our results, we uncovered that Jazz's core developers worked across multiple roles, and were crucial to their teams' organizational, intra-personal and inter-personal processes. Additionally, although these individuals were highly task- and achievement-focused, they were also largely responsible for maintaining positive team atmosphere, and for providing context awareness in support of their colleagues. Conclusion: Our results suggest that high-performing distributed agile teams rely on both individual and collective efforts, as well as organizational environments that promote informal and organic work structures.(Abridged)","['Sherlock A. Licorish', 'Stephen G. MacDonell']","Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering (EASE2013). Porto de Galinhas, Brazil, ACM Press, pp.228-239",arXiv,2021,https://doi.org/10.48550/arXiv.2106.10616,Anomali
Conclusion Stability for Natural Language Based Mining of Design Discussions,"Developer discussions range from in-person hallway chats to comment chains on bug reports. Being able to identify discussions that touch on software design would be helpful in documentation and refactoring software. Designminingis the application of machine learning techniques to correctly label a given discussion artifact, such as a pull request, as pertaining (or not) to design. In this paper we demonstrate a simple example of how designminingworks. We then show how conclusion stability is poor on different artifact types and different projects. We show two techniques -- augmentation and context specificity -- that greatly improve the conclusion stability and cross-project relevance of designmining. Our new approach achieves AUC of 0.88 on within dataset classification and 0.80 on the cross-dataset classification task.","['Alvi Mahadi', 'Neil A. Ernst', 'Karan Tongay']",,arXiv,2021,this https URL,Anomali
pysentimiento: A Python Toolkit for Opinion Mining and Social NLP tasks,"In recent years, the extraction of opinions and information from user-generatedtexthas attracted a lot of interest, largely due to the unprecedented volume of content in Social Media. However, social researchers face some issues in adopting cutting-edge tools for these tasks, as they are usually behind commercial APIs, unavailable for other languages than English, or very complex to use for non-experts. To address these issues, we present pysentimiento, a comprehensive multilingual Python toolkit designed for opinionminingand other Social NLP tasks. This open-source library brings state-of-the-art models for Spanish, English, Italian, and Portuguese in an easy-to-use Python library, allowing researchers to leverage these techniques. We present a comprehensive assessment of performance for several pre-trained language models across a variety of tasks, languages, and datasets, including an evaluation of fairness in the results.","['Juan Manuel Pérez', 'Mariela Rajngewerc', 'Juan Carlos Giudici', 'Damián A. Furman', 'Franco Luque', 'Laura Alonso Alemany', 'María Vanina Martínez']",,arXiv,2024,https://doi.org/10.48550/arXiv.2106.09462,Anomali
EPICURE Ensemble Pretrained Models for Extracting Cancer Mutations from Literature,"To interpret the genetic profile present in a patient sample, it is necessary to know which mutations have important roles in the development of the corresponding cancer type. Named entity recognition is a core step in thetextminingpipeline which facilitatesminingvaluable cancer information from the scientific literature. However, due to the scarcity of related datasets, previous NER attempts in this domain either suffer from low performance when deep learning based models are deployed, or they apply feature based machine learning models or rule based models to tackle this problem, which requires intensive efforts from domain experts, and limit the model generalization capability. In this paper, we propose EPICURE, an ensemble pre trained model equipped with a conditional random field pattern layer and a span prediction pattern layer to extract cancer mutations fromtext. We also adopt a data augmentation strategy to expand our training set from multiple datasets. Experimental results on three benchmark datasets show competitive results compared to the baseline models.","['Jiarun Cao', 'Elke M van Veen', 'Niels Peek', 'Andrew G Renehan', 'Sophia Ananiadou']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.07722,Anomali
Named Entity Normalization Model Using Edge Weight Updating Neural Network: Assimilation Between Knowledge-Driven Graph and Data-Driven Graph,"Discriminating the matched named entity pairs or identifying the entities' canonical forms are critical intextminingtasks. More precise named entity normalization intextminingwill benefit other subsequenttextanalytic applications. We built the named entity normalization model with a novel Edge Weight Updating Neural Network. Our proposed model when tested on four different datasets achieved state-of-the-art results. We, next, verify our model's performance on NCBI Disease, BC5CDR Disease, and BC5CDR Chemical databases, which are widely used named entity normalization datasets in the bioinformatics field. We also tested our model with our own financial named entity normalization dataset to validate the efficacy for more general applications. Using the constructed dataset, we differentiate named entity pairs. Our model achieved the highest named entity normalization performances in terms of various evaluation metrics.","['Sung Hwan Jeon', 'Sungzoon Cho']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.07549,Anomali
Network-based Topic Interaction Map for Big Data Mining of COVID-19 Biomedical Literature,"Since the emergence of the worldwide pandemic of COVID-19, relevant research has been published at a dazzling pace, which yields an abundant amount of big data in biomedical literature. Due to the high volum of relevant literature, it is practically impossible to follow up the research manually. Topic modeling is a well-known unsupervised learning that aims to reveal latent topics fromtextdata. In this paper, we propose a novel analytical framework for estimating topic interactions and effective visualization to improve topics' relationships. We first estimate topic-word distributions using the biterm topic model and estimate the topics' interaction based on the word distribution using the latent space item response model. We mapped these latent topics onto networks to visualize relationships among the topics. Moreover, in the proposed approach, we developed a score that is helpful in selecting meaningful words that characterize the topic. We figure out how topics are related by looking at how their relationships change. We do this with a ""trajectory plot"" that is made with different levels of word richness. These findings provide a thoroughlyminedand intuitive representation of relationships between topics related to a specific research area. The application of this proposed framework to the PubMed literature demonstrates utility of our approach in understanding of the topic composition related to COVID-19 studies in the stage of its emergence.","['Yeseul Jeon', 'Dongjun Chung', 'Jina Park', 'Ick Hoon Jin']",,arXiv,2022,https://doi.org/10.48550/arXiv.2106.07374,Anomali
BIOPAK Flasher: Epidemic disease monitoring and detection in Pakistan using text mining,"Infectious disease outbreak has a significant impact on morbidity, mortality and can cause economic instability of many countries. As global trade is growing, goods and individuals are expected to travel across the border, an infected epidemic area carrier can pose a great danger to his hostile. If a disease outbreak is recognized promptly, then commercial products and travelers (traders/visitors) will be effectively vaccinated, and therefore the disease stopped. Early detection of outbreaks plays an important role here, and beware of the rapid implementation of control measures by citizens, public health organizations, and government. Many indicators have valuable information, such as online news sources (RSS) and social media sources (Twitter, Facebook) that can be used, but are unstructured and bulky, to extract information about disease outbreaks. Few early warning outbreak systems exist with some limitation of linguistic (Urdu) and covering areas (Pakistan). In Pakistan, few channels are published the outbreak news in Urdu or English. The aim is to procure information from Pakistan's English and Urdu news channels and then investigate process, integrate, and visualize the disease epidemic. Urdu ontology is not existed before to match extracted diseases, so we also build that ontology of disease.","['Muhammad Nasir', 'Maheen Bakhtyar', 'Junaid Baber', 'Sadia Lakho', 'Bilal Ahmed', 'Waheed Noor']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.06720,Anomali
CommAID: Visual Analytics for Communication Analysis through Interactive Dynamics Modeling,"Communication consists of both meta-information as well as content. Currently, the automated analysis of such data often focuses either on the network aspects via social network analysis or on the content, utilizing methods fromtext-mining. However, the first category of approaches does not leverage the rich content information, while the latter ignores the conversation environment and the temporal evolution, as evident in the meta-information. In contradiction to communication research, which stresses the importance of a holistic approach, both aspects are rarely applied simultaneously, and consequently, their combination has not yet received enough attention in automated analysis systems. In this work, we aim to address this challenge by discussing the difficulties and design decisions of such a path as well as contribute CommAID, a blueprint for a holistic strategy to communication analysis. It features an integrated visual analytics design to analyze communication networks through dynamics modeling, semantic pattern retrieval, and a user-adaptable and problem-specific machine learning-based retrieval system. An interactive multi-level matrix-based visualization facilitates a focused analysis of both network and content using inline visuals supporting cross-checks and reducing context switches. We evaluate our approach in both a case study and through formative evaluation with eight law enforcement experts using a real-world communication corpus. Results show that our solution surpasses existing techniques in terms of integration level and applicability. With this contribution, we aim to pave the path for a more holistic approach to communication analysis.","['Maximilian T. Fischer', 'Daniel Seebacher', 'Rita Sevastjanova', 'Daniel A. Keim', 'Mennatallah El-Assady']","Computer Graphics Forum, 40(3), 2021",arXiv,2021,https://doi.org/10.48550/arXiv.2106.06334,Anomali
Automatic Construction of Context-Aware Sentiment Lexicon in the Financial Domain Using Direction-Dependent Words,"Increasing attention has been drawn to the sentiment analysis of financial documents. The most popular examples of such documents include analyst reports and economic news, the analysis of which is frequently used to capture the trends in market sentiments. On the other hand, the significance of the role sentiment analysis plays in the financial domain has given rise to the efforts to construct a financial domain-specific sentiment lexicon. Sentiment lexicons lend a hand for solving varioustextminingtasks, such as unsupervised classification oftextdata, while alleviating the arduous human labor required for manual labeling. One of the challenges in the construction of an effective sentiment lexicon is that the semantic orientation of a word may change depending on the context in which it appears. For instance, the word ``profit"" usually conveys positive sentiments; however, when the word is juxtaposed with another word ``decrease,"" the sentiment associated with the phrase ``profit decreases"" now becomes negative. Hence, the sentiment of a given word may shift as one begins to consider the context surrounding the word. In this paper, we address this issue by incorporating context when building sentiment lexicon from a given corpus. Specifically, we construct a lexicon named Senti-DD for the Sentiment lexicon composed of Direction-Dependent words, which expresses each term a pair of a directional word and a direction-dependent word. Experiment results show that higher classification performance is achieved with Senti-DD, proving the effectiveness of our method for automatically constructing a context-aware sentiment lexicon in the financial domain.","['Jihye Park', 'Hye Jin Lee', 'Sungzoon Cho']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.05723,Anomali
Case Studies on using Natural Language Processing Techniques in Customer Relationship Management Software,"How can atextcorpus stored in a customer relationship management (CRM) database be used for dataminingand segmentation? In order to answer this question we inherited the state of the art methods commonly used in natural language processing (NLP) literature, such as word embeddings, and deep learning literature, such as recurrent neural networks (RNN). We used thetextnotes from a CRM system which are taken by customer representatives of an internet ads consultancy agency between years 2009 and 2020. We trained word embeddings by using the correspondingtextcorpus and showed that these word embeddings can not only be used directly for dataminingbut also be used in RNN architectures, which are deep learning frameworks built with long short term memory (LSTM) units, for more comprehensive segmentation objectives. The results prove that structuredtextdata in a CRM can be used tomineout very valuable information and any CRM can be equipped with useful NLP features once the problem definitions are properly built and the solution methods are conveniently implemented.",['Şükrü Ozan'],Journal of Intelligent Information Systems volume 56 233-253 2021,arXiv,2021,https://doi.org/10.48550/arXiv.2106.05160,Anomali
Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation,"Answering a programming question using only its title is difficult as salient contextual information is omitted. Based on this observation, we present a corpus of over 40,000 StackOverflow questiontextsto be used in conjunction with their corresponding intents from the CoNaLa dataset (Yin et al., 2018). Using both the intent and question body, we use BART to establish a baseline BLEU score of 34.35 for this new task. We find further improvements of $2.8\%$ by combining theminedCoNaLa data with the labeled data to achieve a 35.32 BLEU score. We evaluate prior state-of-the-art CoNaLa models with this additional data and find that our proposed method of using the body andmineddata beats the BLEU score of the prior state-of-the-art by $71.96\%$. Finally, we perform ablations to demonstrate that BART is an unsupervised multimodal learner and examine its extractive behavior. The code and data can be found https://github.com/gabeorlanski/stackoverflow-encourages-cheating.","['Gabriel Orlanski', 'Alex Gittens']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.04447,Anomali
Defining definition: a Text mining Approach to Define Innovative Technological Fields,"One of the first task of an innovative project is delineating the scope of the project itself or of the product/service to be developed. A wrong scope definition can determine (in the worst case) project failure. A good scope definition become even more relevant in technological intensive innovation projects, nowadays characterized by a highly dynamic multidisciplinary, turbulent and uncertain environment. In these cases, the boundaries of the project are not easily detectable and it is difficult to decide what it is in-scope and out-of-scope. The present work proposes a tool for the scope delineation process, that automatically define an innovative technological field or a new technology. The tool is based onTextMiningalgorithm that exploits Elsevier's Scopus abstracts in order to the extract relevant data to define a technological scope. The automatic definition tool is then applied on four case studies: Artificial Intelligence and Data Science. The results show how the tool can provide many crucial information in the definition process of a technological field. In particular for the target technological field (or technology), it provides the definition and other elements related to the target.","['Vito Giordano', 'Filippo Chiarello', 'Elena Cervelli']",R&D MANAGEMENT CONFERENCE 2019 - DATA SCIENCE FOR INNOVATION R&D MANAGEMENT CONFERENCE 2019 - DATA SCIENCE FOR INNOVATION R&D Management Conference 2021 - Data Science for Innovatopm,arXiv,2021,https://doi.org/10.48550/arXiv.2106.04210,Anomali
A Case Study of Spanish Text Transformations for Twitter Sentiment Analysis,"Sentiment analysis is atextminingtask that determines the polarity of a giventext, i.e., its positiveness or negativeness. Recently, it has received a lot of attention given the interest in opinionminingin micro-blogging platforms. These new forms of textual expressions present new challenges to analyzetextgiven the use of slang, orthographic and grammatical errors, among others. Along with these challenges, a practical sentiment classifier should be able to handle efficiently large workloads.
  The aim of this research is to identify whichtexttransformations (lemmatization, stemming, entity removal, among others), tokenizers (e.g., words $n$-grams), and tokens weighting schemes impact the most the accuracy of a classifier (Support Vector Machine) trained on two Spanish corpus. The methodology used is to exhaustively analyze all the combinations of thetexttransformations and their respective parameters to find out which characteristics the best performing classifiers have in common. Furthermore, among the differenttexttransformations studied, we introduce a novel approach based on the combination of word based $n$-grams and character based $q$-grams. The results show that this novel combination of words and characters produces a classifier that outperforms the traditional word based combination by $11.17\%$ and $5.62\%$ on the INEGI and TASS'15 dataset, respectively.","['Eric S. Tellez', 'Sabino Miranda-Jiménez', 'Mario Graff', 'Daniela Moctezuma', 'Oscar S. Siodia', 'Elio A. Villaseñor']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.02009,Anomali
AliCG: Fine-grained and Evolvable Conceptual Graph Construction for Semantic Search at Alibaba,"Conceptual graphs, which is a particular type of Knowledge Graphs, play an essential role in semantic search. Prior conceptual graph construction approaches typically extract high-frequent, coarse-grained, and time-invariant concepts from formaltexts. In real applications, however, it is necessary to extract less-frequent, fine-grained, and time-varying conceptual knowledge and build taxonomy in an evolving manner. In this paper, we introduce an approach to implementing and deploying the conceptual graph at Alibaba. Specifically, We propose a framework called AliCG which is capable of a) extracting fine-grained concepts by a novel bootstrapping with alignment consensus approach, b)mininglong-tail concepts with a novel low-resource phraseminingapproach, c) updating the graph dynamically via a concept distribution estimation method based on implicit and explicit user behaviors. We have deployed the framework at Alibaba UC Browser. Extensive offline evaluation as well as online A/B testing demonstrate the efficacy of our approach.","['Ningyu Zhang', 'Qianghuai Jia', 'Shumin Deng', 'Xiang Chen', 'Hongbin Ye', 'Hui Chen', 'Huaixiao Tou', 'Gang Huang', 'Zhao Wang', 'Nengwei Hua', 'Huajun Chen']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.01686,Anomali
Corporate core values and social responsibility: What really matters to whom,"This study uses an innovative measure, the Semantic Brand Score, to assess the interest of stakeholders in different company core values. Among others, we focus on corporate social responsibility (CSR) core value statements, and on the attention they receive from five categories of stakeholders (customers, company communication teams, employees, associations and media). Combining big data methods and tools of Social Network Analysis andTextMining, we analyzed about 58,000 Italian tweets and found that different stakeholders have different prevailing interests. CSR gets much less attention than expected. Core values related to customers and employees are in the foreground.","['M. A. Barchiesi', 'A. Fronzetti Colladon']","Technological Forecasting and Social Change 170, 120907 (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2106.01644,Anomali
People's Attitudes Toward Automated Vehicle and Transit Integration: Case Study of Small Urban Areas,"Previous surveys of public attitudes toward automated vehicle (AV) and transit integration primarily took place in large urban areas. AV-transit integration also has a great potential in small urban areas. A survey of public attitudes towards AV-transit integration was carried out in two small urban areas in Wisconsin, United States. A total of 266 finished responses were analyzed usingtextmining, factor analysis, and regression analysis. Results showed that respondents knew about AVs and driving assistance technologies. Respondents welcome AV-transit integration but were unsure about its potential impacts. Technology-savvy respondents were more positive but had more concerns about AV-transit integration than others. Respondents who enjoyed driving were not necessarily against transit, as they were more positive about AV-transit integration and were more willing to use automated buses than those who did not enjoy driving as much. Transit users were more positive toward AV-transit integration than non-transit users.","['Yu Song', 'Madhav V. Chitturi', 'Chris McCahill', 'David A. Noyce']","Transportation Planning and Technology, 2021, 44(5), pp.1-21",arXiv,2021,https://doi.org/10.48550/arXiv.2106.01407,Anomali
LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations,"This work aims to tackle the challenging heterogeneous graph encoding problem in thetext-to-SQL task. Previous methods are typically node-centric and merely utilize different weight matrices to parameterize edge types, which 1) ignore the rich semantics embedded in the topological structure of edges, and 2) fail to distinguish local and non-local relations for each node. To this end, we propose a Line Graph EnhancedText-to-SQL (LGESQL) model tominethe underlying relational features without constructing meta-paths. By virtue of the line graph, messages propagate more efficiently through not only connections between nodes, but also the topology of directed edges. Furthermore, both local and non-local relations are integrated distinctively during the graph iteration. We also design an auxiliary task called graph pruning to improve the discriminative capability of the encoder. Our framework achieves state-of-the-art results (62.8% with Glove, 72.0% with Electra) on the cross-domaintext-to-SQL benchmark Spider at the time of writing.","['Ruisheng Cao', 'Lu Chen', 'Zhi Chen', 'Yanbin Zhao', 'Su Zhu', 'Kai Yu']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.01093,Anomali
ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining,"While online conversations can cover a vast amount of information in many different formats, abstractivetextsummarization has primarily focused on modeling solely news articles. This research gap is due, in part, to the lack of standardized datasets for summarizing online discussions. To address this gap, we design annotation protocols motivated by an issues--viewpoints--assertions framework to crowdsource four new datasets on diverse online conversation forms of news comments, discussion forums, community question answering forums, and email threads. We benchmark state-of-the-art models on our datasets and analyze characteristics associated with the data. To create a comprehensive benchmark, we also evaluate these models on widely-used conversation summarization datasets to establish strong baselines in this domain. Furthermore, we incorporate argumentminingthrough graph construction to directly model the issues, viewpoints, and assertions present in a conversation and filter noisy input, showing comparable or improved results according to automatic and human evaluations.","['Alexander R. Fabbri', 'Faiaz Rahman', 'Imad Rizvi', 'Borui Wang', 'Haoran Li', 'Yashar Mehdad', 'Dragomir Radev']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.00829,Anomali
NewsEmbed: Modeling News through Pre-trained Document Representations,"Effectively modelingtext-rich fresh content such as news articles at document-level is a challenging problem. To ensure a content-based model generalize well to a broad range of applications, it is critical to have a training dataset that is large beyond the scale of human labels while achieving desired quality. In this work, we address those two challenges by proposing a novel approach tominesemantically-relevant fresh documents, and their topic labels, with little human supervision. Meanwhile, we design a multitask model called NewsEmbed that alternatively trains a contrastive learning with a multi-label classification to derive a universal document encoder. We show that the proposed approach can provide billions of high quality organic training examples and can be naturally extended to multilingual setting wheretextsin different languages are encoded in the same semantic space. We experimentally demonstrate NewsEmbed's competitive performance across multiple natural language understanding tasks, both supervised and unsupervised.","['Jialu Liu', 'Tianqi Liu', 'Cong Yu']",,arXiv,2021,https://doi.org/10.48550/arXiv.2106.00590,Anomali
Corpus-Based Paraphrase Detection Experiments and Review,"Paraphrase detection is important for a number of applications, including plagiarism detection, authorship attribution, question answering,textsummarization,textminingin general, etc. In this paper, we give a performance overview of various types of corpus-based models, especially deep learning (DL) models, with the task of paraphrase detection. We report the results of eight models (LSI, TF-IDF, Word2Vec, Doc2Vec, GloVe, FastText, ELMO, and USE) evaluated on three different public available corpora: Microsoft Research Paraphrase Corpus, Clough and Stevenson and Webis Crowd Paraphrase Corpus 2011. Through a great number of experiments, we decided on the most appropriate approaches fortextpre-processing: hyper-parameters, sub-model selection-where they exist (e.g., Skipgram vs. CBOW), distance measures, and semantic similarity/paraphrase detection threshold. Our findings and those of other researchers who have used deep learning models show that DL models are very competitive with traditional state-of-the-art approaches and have potential that should be further developed.","['Tedo Vrbanec', 'Ana Mestrovic']","In Information (Switzerland) (Vol. 11, Issue 5, p. 241). 2020, MDPI AG",arXiv,2021,https://doi.org/10.48550/arXiv.2106.00145,Anomali
UCPhrase: Unsupervised Context-aware Quality Phrase Tagging,"Identifying and understanding quality phrases from context is a fundamental task intextmining. The most challenging part of this task arguably lies in uncommon, emerging, and domain-specific phrases. The infrequent nature of these phrases significantly hurts the performance of phraseminingmethods that rely on sufficient phrase occurrences in the input corpus. Context-aware tagging models, though not restricted by frequency, heavily rely on domain experts for either massive sentence-level gold labels or handcrafted gazetteers. In this work, we propose UCPhrase, a novel unsupervised context-aware quality phrase tagger. Specifically, we induce high-quality phrase spans as silver labels from consistently co-occurring word sequences within each document. Compared with typical context-agnostic distant supervision based on existing knowledge bases (KBs), our silver labels root deeply in the input domain and context, thus having unique advantages in preserving contextual completeness and capturing emerging, out-of-KB phrases. Training a conventional neural tagger based on silver labels usually faces the risk of overfitting phrase surface names. Alternatively, we observe that the contextualized attention maps generated from a transformer-based neural language model effectively reveal the connections between words in a surface-agnostic way. Therefore, we pair such attention maps with the silver labels to train a lightweight span prediction model, which can be applied to new input to recognize (unseen) quality phrases regardless of their surface names or frequency. Thorough experiments on various tasks and datasets, including corpus-level phrase ranking, document-level keyphrase extraction, and sentence-level phrase tagging, demonstrate the superiority of our design over state-of-the-art pre-trained, unsupervised, and distantly supervised methods.","['Xiaotao Gu', 'Zihan Wang', 'Zhenyu Bi', 'Yu Meng', 'Liyuan Liu', 'Jiawei Han', 'Jingbo Shang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2105.14078,Anomali
Climate Action During COVID-19 Recovery and Beyond: A Twitter Text Mining Study,"The Coronavirus pandemic created a global crisis that prompted immediate large-scale action, including economic shutdowns and mobility restrictions. These actions have had devastating effects on the economy, but some positive effects on the environment. As the world recovers from the pandemic, we ask the following question: What is the public attitude towards climate action during COVID-19 recovery and beyond? We answer this question by analyzing discussions on the Twitter social media platform. We find that most discussions support climate action and point out lessons learned during pandemic response that can shape future climate policy, although skeptics continue to have a presence. Additionally, concerns arise in the context of climate action during the pandemic, such as mitigating the risk of COVID-19 transmission on public transit.","['Mohammad S. Parsa', 'Lukasz Golab', 'Srinivasan Keshav']",,arXiv,2021,https://doi.org/10.48550/arXiv.2105.12190,Anomali
Big data and big values: When companies need to rethink themselves,"In order to face the complexity of business environments and detect priorities while triggering contingency strategies, we propose a new methodological approach that combinestextmining, social network and big data analytics, with the assessment of stakeholders' attitudes towards company core values. This approach was applied in a case study where we considered the Twitter discourse about core values in Italy. We collected more than 94,000 tweets related to the core values of the firms listed in Fortune's ranking of the World's Most Admired Companies (2013-2017). For the Italian scenario, we found three predominant core values orientations (Customers, Employees and Excellence) - which should be at the basis of any business strategy - and three latent ones (Economic-Financial Growth, Citizenship and Social Responsibility), which need periodic attention. Our contribution is mostly methodological and extends the research ontextminingand on online big data analytics applied in complex business contexts.","['M. A. Barchiesi', 'A. Fronzetti Colladon']","Journal of Business Research 129, 714-722 (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2105.12048,Anomali
Assessing perceived organizational leadership styles through twitter text mining,"We propose atextclassification tool based on support vector machines for the assessment of organizational leadership styles, as appearing to Twitter users. We collected Twitter data over 51 days, related to the first 30 Italian organizations in the 2015 ranking of Forbes Global 2000-out of which we selected the five with the most relevant volumes of tweets. We analyzed the communication of the company leaders, together with the dialogue among the stakeholders of each company, to understand the association with perceived leadership styles and dimensions. To assess leadership profiles, we referred to the 10-factor model developed by Barchiesi and La Bella in 2007. We maintain the distinctiveness of the approach we propose, as it allows a rapid assessment of the perceived leadership capabilities of an enterprise, as they emerge from its social media interactions. It can also be used to show how companies respond and manage their communication when specific events take place, and to assess their stakeholder's reactions.","['A. La Bella', 'A. Fronzetti Colladon', 'E. Battistoni', 'S. Castellan', 'M. Francucci']","Journal of the Association for Information Science and Technology 61(1), 21-31 (2018)",arXiv,2021,https://doi.org/10.48550/arXiv.2105.11276,Anomali
Editorial introduction: The power of words and networks,"According to Freud ""words were originally magic and to this day words have retained much of their ancient magical power"". By words, behaviors are transformed and problems are solved. The way we use words reveals our intentions, goals and values. Novel tools fortextanalysis help understand the magical power of words. This power is multiplied, if it is combined with the study of social networks, i.e. with the analysis of relationships among social units. This special issue of the International Journal of Information Management, entitled ""Combining Social Network Analysis andTextMining: from Theory to Practice"", includes heterogeneous and innovative research at the nexus oftextminingand social network analysis. It aims to enrich work at the intersection of these fields, which still lags behind in theoretical, empirical, and methodological foundations. The nine articles accepted for inclusion in this special issue all present methods and tools that have business applications. They are summarized in this editorial introduction.","['A. Fronzetti Colladon', 'P. Gloor', 'D. F. Iezzi']","International Journal of Information Management 51, 102031 (2020)",arXiv,2021,https://doi.org/10.48550/arXiv.2105.11263,Anomali
A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning,"We curated WikiPII, an automatically labeled dataset composed of Wikipedia biography pages, annotated for personal information extraction. Although automatic annotation can lead to a high degree of label noise, it is an inexpensive process and can generate large volumes of annotated documents. We trained a BERT-based NER model with WikiPII and showed that with an adequately large training dataset, the model can significantly decrease the cost of manual information extraction, despite the high level of label noise. In a similar approach, organizations can leveragetextminingtechniques to create customized annotated datasets from their historical data without sharing the raw data for human annotation. Also, we explore collaborative training of NER models through federated learning when the annotation is noisy. Our results suggest that depending on the level of trust to the ML operator and the volume of the available data, distributed training can be an effective way of training a personal information identifier in a privacy-preserved manner. Research material is available at https://github.com/ratmcu/wikipiifed.","['Rajitha Hathurusinghe', 'Isar Nejadgholi', 'Miodrag Bolic']",,arXiv,2021,https://doi.org/10.48550/arXiv.2105.09198,Anomali
"The Online Pivot: Lessons Learned from Teaching a Text and Data Mining Course in Lockdown, Enhancing online Teaching with Pair Programming and Digital Badges","In this paper we provide an account of how we ported atextand dataminingcourse online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students' learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds.","['Beatrice Alex', 'Clare Llewellyn', 'Pawel Michal Orzechowski', 'Maria Boutchkova']",,arXiv,2021,https://doi.org/10.48550/arXiv.2105.07847,Anomali
Identifying Biased Users in Online Social Networks to Enhance the Accuracy of Sentiment Analysis: A User Behavior-Based Approach,"The development of an automatic way to extract user opinions about products, movies, and foods from online social network (OSN) interactions is among the main interests of sentiment analysis and opinionminingstudies. Existing approaches in the sentiment analysis domain mostly do not discriminate the sentences of different types of users, even though some users are always negative and some are always positive. Thus, finding a way to identify these two types of user is significant because their attitudes can change the analysis of user reviews of businesses and products. Due to the complexity of natural language processing, puretextminingmethods may lead to misunderstandings about the exact nature of the sentiments expressed in reviewtext. In this study, we propose a neural network classifier to predict the presence of biased users on the basis of users' psychological behaviors. The identification of the psychological behaviors of users allows us to find overly positive and overly negative users and to categorize these users' attitudes regardless of the content of their reviewtexts. The experiment result indicates that the biased users can be predicted based on user behavior at an accuracy rate of 89%, 67% and 81% for three different datasets.",['Amin Mahmoudi'],,arXiv,2021,https://doi.org/10.48550/arXiv.2105.05950,Anomali
Forecasting election results by studying brand importance in online news,"This study uses the semantic brand score, a novel measure of brand importance in big textual data, to forecast elections based on online news. About 35,000 online news articles were transformed into networks of co-occurring words and analyzed by combining methods and tools from social network analysis andtextmining. Forecasts made for four voting events in Italy provided consistent results across different voting systems: a general election, a referendum, and a municipal election in two rounds. This work contributes to the research on electoral forecasting by focusing on predictions based on online big data; it offers new perspectives regarding the textual analysis of online news through a methodology which is relatively fast and easy to apply. This study also suggests the existence of a link between the brand importance of political candidates and parties and electoral results.",['A. Fronzetti Colladon'],"International Journal of Forecasting 36(2), 414-427 (2020)",arXiv,2021,https://doi.org/10.48550/arXiv.2105.05762,Anomali
Supporting Land Reuse of Former Open Pit Mining Sites using Text Classification and Active Learning,"Open pitminesleft many regions worldwide inhospitable or uninhabitable. To put these regions back into use, entire stretches of land must be renaturalized. For the sustainable subsequent use or transfer to a new primary use, many contaminated sites and soil information have to be permanently managed. In most cases, this information is available in the form of expert reports in unstructured data collections or file folders, which in the best case are digitized. Due to size and complexity of the data, it is difficult for a single person to have an overview of this data in order to be able to make reliable statements. This is one of the most important obstacles to the rapid transfer of these areas to after-use. An information-based approach to this issue supports fulfilling several Sustainable Development Goals regarding environment issues, health and climate action. We use a stack of Optical Character Recognition,TextClassification, Active Learning and Geographic Information System Visualization to effectivelymineand visualize this information. Subsequently, we link the extracted information to geographic coordinates and visualize them using a Geographic Information System. Active Learning plays a vital role because our dataset provides no training data. In total, we process nine categories and actively learn their representation in our dataset. We evaluate the OCR, Active Learning andTextClassification separately to report the performance of the system. Active Learning andtextclassification results are twofold: Whereas our categories about restrictions work sufficient ($>$.85 F1), the seven topic-oriented categories were complicated for human coders and hence the results achieved mediocre evaluation scores ($<$.70 F1).","['Christopher Schröder', 'Kim Bürgl', 'Yves Annanias', 'Andreas Niekler', 'Lydia Müller', 'Daniel Wiegreffe', 'Christian Bender', 'Christoph Mengs', 'Gerik Scheuermann', 'Gerhard Heyer']","Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021",arXiv,2022,https://doi.org/10.48550/arXiv.2105.05557,Anomali
A Text Extraction-Based Smart Knowledge Graph Composition for Integrating Lessons Learned during the Microchip Design,"The production of microchips is a complex and thus well documented process. Therefore, available textual data about the production can be overwhelming in terms of quantity. This affects the visibility and retrieval of a certain piece of information when it is most needed. In this paper, we propose a dynamic approach to interlink the information extracted from multisource production-relevant documents through the creation of a knowledge graph. This graph is constructed in order to support searchability and enhance user's access to large-scale production information.Textminingmethods are firstly utilized to extract data from multiple documentation sources. Document relations are thenminedand extracted for the composition of the knowledge graph. Graph search functionality is then supported with a recommendation use-case to enhance users' access to information that is related to the initial documents. The proposed approach is tailored to and tested on microchip design-relevant documents. It enhances the visibility and findability of previous design-failure-cases during the process of a new chip design.","['H. Abu-Rasheed', 'C. Weber', 'J. Zenkert', 'P. Czerner', 'R. Krumm', 'M. Fathi']","In: Arai K., Kapoor S., Bhatia R. (eds) Intelligent Systems and Applications. IntelliSys 2020. Advances in Intelligent Systems and Computing, vol 1251. Springer, Cham",arXiv,2021,https://doi.org/10.48550/arXiv.2105.05076,Anomali
Forecasting consumer confidence through semantic network analysis of online news,"This research studies the impact of online news on social and economic consumer perceptions through semantic network analysis. Using over 1.8 million online articles on Italian media covering four years, we calculate the semantic importance of specific economic-related keywords to see if words appearing in the articles could anticipate consumers' judgments about the economic situation and the Consumer Confidence Index. We use an innovative approach to analyze big textual data, combining methods and tools oftextminingand social network analysis. Results show a strong predictive power for the judgments about the current households and national situation. Our indicator offers a complementary approach to estimating consumer confidence, lessening the limitations of traditional survey-based methods.","['A. Fronzetti Colladon', 'F. Grippa', 'B. Guardabascio', 'G. Costante', 'F. Ravazzolo']","Scientific Reports 13, 11785 (2023)",arXiv,2023,https://doi.org/10.48550/arXiv.2105.04900,Anomali
Measuring Economic Policy Uncertainty Using an Unsupervised Word Embedding-based Method,"Economic Policy Uncertainty (EPU) is a critical indicator in economic studies, while it can be used to forecast a recession. Under higher levels of uncertainty, firms' owners cut their investment, which leads to a longer post-recession recovery. EPU index is computed by counting news articles containing pre-defined keywords related to policy-making and economy and convey uncertainty. Unfortunately, this method is sensitive to the original keyword set, its richness, and the news coverage. Thus, reproducing its results for different countries is challenging. In this paper, we propose an unsupervisedtextminingmethod that uses word-embedding representation space to select relevant keywords. This method is not strictly sensitive to the semantic similarity threshold applied to the word embedding vectors and does not require a pre-defined dictionary. Our experiments using a massive repository of Persian news show that the EPU series computed by the proposed method precisely follows major events affecting Iran's economy and is compatible with the World Uncertainty Index (WUI) of Iran.","['Fatemeh Kaveh-Yazdy', 'Sajjad Zarifzadeh']",,arXiv,2021,https://doi.org/10.48550/arXiv.2105.04631,Anomali
Text similarity analysis for evaluation of descriptive answers,"Keeping in mind the necessity of intelligent system in educational sector, this paper proposes atextanalysis based automated approach for automatic evaluation of the descriptive answers in an examination. In particular, the research focuses on the use of intelligent concepts of Natural Language Processing and DataMiningfor computer aided examination evaluation system. The paper present an architecture for fair evaluation of answer sheet. In this architecture, the examiner creates a sample answer sheet for given sets of question. By using the concept oftextsummarization,textsemantics and keywords summarization, the final score for each answer is calculated. Thetextsimilarity model is based on Siamese Manhattan LSTM (MaLSTM). The results of this research were compared to manually graded assignments and other existing system. This approach was found to be very efficient in order to be implemented in an institution or in an university.","['Vedant Bahel', 'Achamma Thomas']",,arXiv,2021,https://doi.org/10.48550/arXiv.2105.02935,Anomali
MineGAN++: Mining Generative Models for Efficient Knowledge Transfer to Limited Data Domains,"GANs largely increases the potential impact of generative models. Therefore, we propose a novel knowledge transfer method for generative models based onminingthe knowledge that is most beneficial to a specific target domain, either from a single or multiple pretrained GANs. This is done using a miner network that identifies which part of the generative distribution of each pretrained GAN outputs samples closest to the target domain.Miningeffectively steers GAN sampling towards suitable regions of the latent space, which facilitates the posterior finetuning and avoids pathologies of other methods, such as mode collapse and lack of flexibility. Furthermore, to prevent overfitting on small target domains, we introduce sparse subnetwork selection, that restricts the set of trainable neurons to those that are relevant for the target dataset. We perform comprehensive experiments on several challenging datasets using various GAN architectures (BigGAN, Progressive GAN, and StyleGAN) and show that the proposed method, called MineGAN, effectively transfers knowledge to domains with few target images, outperforming existing methods. In addition, MineGAN can successfully transfer knowledge from multiple pretrained GANs.","['Yaxing Wang', 'Abel Gonzalez-Garcia', 'Chenshen Wu', 'Luis Herranz', 'Fahad Shahbaz Khan', 'Shangling Jui', 'Joost van de Weijer']",,arXiv,2023,https://doi.org/10.48550/arXiv.2104.13742,Anomali
Research Communities in cyber security: A Comprehensive Literature Review,"In order to provide a coherent overview of cyber security research, the Scopus academic abstract and citation database wasminedto create a citation graph of 98,373 authors active in the field between 1949 and early 2020. The Louvain community detection algorithm was applied to the graph in order to identify existing research communities. The analysis discovered twelve top-level communities: access control, authentication, biometrics, cryptography (I & II), cyber-physical systems, information hiding, intrusion detection, malwares, quantum cryptography, sensor networks, and usable security. These top-level communities were in turn composed of a total of 80 sub-communities. The analysis results are presented for each community in descriptivetext, sub-community graphs, and tables with, for example, the most-cited papers and authors. A comparison between the detected communities and topical areas defined by other related work, is also presented, demonstrating a greater researcher emphasis on cryptography, quantum cryptography, information hiding and biometrics, at the expense of laws and regulation, risk management and governance, and security software lifecycle.","['Sotirios Katsikeas', 'Pontus Johnson', 'Mathias Ekstedt', 'Robert Lagerström']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.13196,Anomali
MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding,"Multi-modal reasoning systems rely on a pre-trained object detector to extract regions of interest from the image. However, this crucial module is typically used as a black box, trained independently of the downstream task and on a fixed vocabulary of objects and attributes. This makes it challenging for such systems to capture the long tail of visual concepts expressed in free formtext. In this paper we propose MDETR, an end-to-end modulated detector that detects objects in an image conditioned on a rawtextquery, like a caption or a question. We use a transformer-based architecture to reason jointly overtextand image by fusing the two modalities at an early stage of the model. We pre-train the network on 1.3Mtext-image pairs,minedfrom pre-existing multi-modal datasets having explicit alignment between phrases intextand objects in the image. We then fine-tune on several downstream tasks such as phrase grounding, referring expression comprehension and segmentation, achieving state-of-the-art results on popular benchmarks. We also investigate the utility of our model as an object detector on a given label set when fine-tuned in a few-shot setting. We show that our pre-training approach provides a way to handle the long tail of object categories which have very few labelled instances. Our approach can be easily extended for visual question answering, achieving competitive performance on GQA and CLEVR. The code and models are available at https://github.com/ashkamath/mdetr.","['Aishwarya Kamath', 'Mannat Singh', 'Yann LeCun', 'Gabriel Synnaeve', 'Ishan Misra', 'Nicolas Carion']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.12763,Anomali
Breast Mass Detection with Faster R-CNN: On the Feasibility of Learning from Noisy Annotations,"In this work we study the impact of noise on the training of object detection networks for the medical domain, and how it can be mitigated by improving the training procedure. Annotating large medical datasets for training data-hungry deep learning models is expensive and time consuming. Leveraging information that is already collected in clinical practice, in the form oftextreports, bookmarks or lesion measurements would substantially reduce this cost. Obtaining precise lesion bounding boxes through automaticminingprocedures, however, is difficult. We provide here a quantitative evaluation of the effect of bounding box coordinate noise on the performance of Faster R-CNN object detection networks for breast mass detection. Varying degrees of noise are simulated by randomly modifying the bounding boxes: in our experiments, bounding boxes could be enlarged up to six times the original size. The noise is injected in the CBIS-DDSM collection, a well curated public mammography dataset for which accurate lesion location is available. We show how, due to an imperfect matching between the ground truth and the network bounding box proposals, the noise is propagated during training and reduces the ability of the network to correctly classify lesions from background. When using the standard Intersection over Union criterion, the area under the FROC curve decreases by up to 9%. A novel matching criterion is proposed to improve tolerance to noise.","['Sina Famouri', 'Lia Morra', 'Leonardo Mangia', 'Fabrizio Lamberti']","IEEE Access, 2021",arXiv,2021,https://doi.org/10.48550/arXiv.2104.12218,Anomali
Tablext: A Combined Neural Network And Heuristic Based Table Extractor,"A significant portion of the data available today is found within tables. Therefore, it is necessary to use automated table extraction to obtain thorough results when data-mining. Today's popular state-of-the-art methods for table extraction struggle to adequately extract tables with machine-readabletextand structural data. To make matters worse, many tables do not have machine-readable data, such as tables saved as images, making most extraction methods completely ineffective. In order to address these issues, a novel, general format table extractor tool, Tablext, is proposed. This tool uses a combination of computer vision techniques and machine learning methods to efficiently and effectively identify and extract data from tables. Tablext begins by using a custom Convolutional Neural Network (CNN) to identify and separate all potential tables. The identification process is optimized by combining the custom CNN with the YOLO object detection network. Then, the high-level structure of each table is identified with computer vision methods. This high-level, structural meta-data is used by another CNN to identify exact cell locations. As a final step, Optical Characters Recognition (OCR) is performed on every individual cell to extract their content without needing machine-readabletext. This multi-stage algorithm allows for the neural networks to focus on completing complex tasks, while letting image processing methods efficiently complete the simpler ones. This leads to the proposed approach to be general-purpose enough to handle a large batch of tables regardless of their internal encodings or their layout complexity. Additionally, it becomes accurate enough to outperform competing state-of-the-art table extractors on the ICDAR 2013 table dataset.","['Zach Colter', 'Morteza Fayazi', 'Zineb Benameur-El', 'Serafina Kamp', 'Shuyan Yu', 'Ronald Dreslinski']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.11287,Anomali
Interval Probabilistic Fuzzy WordNet,"WordNet lexical-database groups English words into sets of synonyms called ""synsets."" Synsets are utilized for several applications in the field oftext-mining. However, they were also open to criticism because although, in reality, not all the members of a synset represent the meaning of that synset with the same degree, in practice, they are considered as members of the synset, identically. Thus, the fuzzy version of synsets, called fuzzy-synsets (or fuzzy word-sense classes) were proposed and studied. In this study, we discuss why (type-1) fuzzy synsets (T1 F-synsets) do not properly model the membership uncertainty, and propose an upgraded version of fuzzy synsets in which membership degrees of word-senses are represented by intervals, similar to what in Interval Type 2 Fuzzy Sets (IT2 FS) and discuss that IT2 FS theoretical framework is insufficient for analysis and design of such synsets, and propose a new concept, called Interval Probabilistic Fuzzy (IPF) sets. Then we present an algorithm for constructing the IPF synsets in any language, given a corpus and a word-sense-disambiguation system. Utilizing our algorithm and the open-American-online-corpus (OANC) and UKB word-sense-disambiguation, we constructed and published the IPF synsets of WordNet for English language.","['Yousef Alizadeh-Q', 'Behrouz Minaei-Bidgoli', 'Sayyed-Ali Hossayni', 'Mohammad-R Akbarzadeh-T', 'Diego Reforgiato Recupero', 'Mohammad-Reza Rajati', 'Aldo Gangemi']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.10660,Anomali
End-to-end Biomedical Entity Linking with Span-based Dictionary Matching,"Disease name recognition and normalization, which is generally called biomedical entity linking, is a fundamental process in biomedicaltextmining. Recently, neural joint learning of both tasks has been proposed to utilize the mutual benefits. While this approach achieves high performance, disease concepts that do not appear in the training dataset cannot be accurately predicted. This study introduces a novel end-to-end approach that combines span representations with dictionary-matching features to address this problem. Our model handles unseen concepts by referring to a dictionary while maintaining the performance of neural network-based models, in an end-to-end fashion. Experiments using two major datasets demonstrate that our model achieved competitive results with strong baselines, especially for unseen concepts during training.","['Shogo Ujiie', 'Hayate Iso', 'Shuntaro Yada', 'Shoko Wakamiya', 'Eiji Aramaki']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.10493,Anomali
Seed Word Selection for Weakly-Supervised Text Classification with Unsupervised Error Estimation,"Weakly-supervisedtextclassification aims to inducetextclassifiers from only a few user-provided seed words. The vast majority of previous work assumes high-quality seed words are given. However, the expert-annotated seed words are sometimes non-trivial to come up with. Furthermore, in the weakly-supervised learning setting, we do not have any labeled document to measure the seed words' efficacy, making the seed word selection process ""a walk in the dark"". In this work, we remove the need for expert-curated seed words by firstmining(noisy) candidate seed words associated with the category names. We then train interim models with individual candidate seed words. Lastly, we estimate the interim models' error rate in an unsupervised manner. The seed words that yield the lowest estimated error rates are added to the final seed word set. A comprehensive evaluation of six binary classification tasks on four popular datasets demonstrates that the proposed method outperforms a baseline using only category name seed words and obtained comparable performance as a counterpart using expert-annotated seed words.","['Yiping Jin', 'Akshay Bhatia', 'Dittaya Wanvarie']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.09765,Anomali
AI supported Topic Modeling using KNIME-Workflows,Topic modeling algorithms traditionally model topics as list of weighted terms. These topic models can be used effectively to classifytextsor to supporttextminingtasks such astextsummarization or fact extraction. The general procedure relies on statistical analysis of term frequencies. The focus of this work is on the implementation of the knowledge-based topic modelling services in a KNIME workflow. A brief description and evaluation of the DBPedia-based enrichment approach and the comparative evaluation of enriched topic models will be outlined based on our previous work. DBpedia-Spotlight is used to identify entities in the inputtextand information from DBpedia is used to extend these entities. We provide a workflow developed in KNIME implementing this approach and perform a result comparison of topic modeling supported by knowledge base information to traditional LDA. This topic modeling approach allows semantic interpretation both by algorithms and by humans.,"['Jamal Al Qundus', 'Silvio Peikert', 'Adrian Paschke']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.09428,Anomali
Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis,"Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with structured data, whileminingcausal relationship among factors from unstructured data, liketext, has been less examined, but is of great importance, especially in the legal domain.
  In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the framework on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the causal knowledge contained in GCI can be effectively injected into powerful neural networks for better performance and interpretability.","['Xiao Liu', 'Da Yin', 'Yansong Feng', 'Yuting Wu', 'Dongyan Zhao']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.09420,Anomali
User Behavior Discovery in the COVID-19 Era through the Sentiment Analysis of User Tweet Texts,"The coronavirus disease (COVID-19) outbreak was declared a pandemic in March 2020 and since then it has had a significant effect on all aspects of life. Although we live in an information era, we do not have accurate information about this disease. Online social networks (OSNs) play a vital role in society, especially people who do not have trust in the government would tend to have more confidence in the evidence that is formed by social networks. The advantages of OSNs in the COVID-19 era are clear. For instance, social media enables people to connect with each other without the need for real-world face-to-face social interaction. Social media networks also act as a collective intelligence in the absence of world leadership. Therefore, in this study, considering the phenomenon of information diffusion in OSNs, we focus on the effects of COVID-19 on user sentiment and show the user behavior trend during the early months of the pandemic throughminingand analyzing OSN data. Moreover, we propose a data-driven model to demonstrate how user sentiment changes over a period of time and how OSNs help us to obtain information on user behavior that is very important for the accurate prediction of future behavior. For this purpose, this study uses tweettextsabout COVID-19 and the related network structure to extract significant features, and then presents a model attempting to provide a more comprehensive real picture of current and future conditions.","['Amin Mahmoudi', 'Victoria Wai-lan Yeung', 'Eric W. K. See-To']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.08867,Anomali
"""Wikily"" Supervised Neural Translation Tailored to Cross-Lingual Tasks","We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language. We show that first sentences and titles of linked Wikipedia pages, as well as cross-lingual image captions, are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings forminingparalleltextfrom Wikipedia. Our final model achieves high BLEU scores that are close to or sometimes higher than strong supervised baselines in low-resource languages; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh. Moreover, we tailor our wikily supervised translation models to unsupervised image captioning, and cross-lingual dependency parser transfer. In image captioning, we train a multi-tasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a translated version of the English captioning data, using our wikily-supervised translation models. Our captioning results on Arabic are slightly better than that of its supervised model. In dependency parsing, we translate a large amount of monolingualtext, and use it as artificial training data in an annotation projection framework. We show that our model outperforms recent work on cross-lingual transfer of dependency parsers.","['Mohammad Sadegh Rasooli', 'Chris Callison-Burch', 'Derry Tanti Wijaya']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.08384,Anomali
Bilingual alignment transfers to multilingual alignment for unsupervised parallel text mining,"This work presents methods for learning cross-lingual sentence representations using paired or unpaired bilingualtexts. We hypothesize that the cross-lingual alignment strategy is transferable, and therefore a model trained to align only two languages can encode multilingually more aligned representations. We thus introduce dual-pivot transfer: training on one language pair and evaluating on other pairs. To study this theory, we design unsupervised models trained on unpaired sentences and single-pair supervised models trained on bitexts, both based on the unsupervised language model XLM-R with its parameters frozen. The experiments evaluate the models as universal sentence encoders on the task of unsupervised bitextminingon two datasets, where the unsupervised model reaches the state of the art of unsupervised retrieval, and the alternative single-pair supervised model approaches the performance of multilingually supervised models. The results suggest that bilingual training techniques as proposed can be applied to get sentence representations with multilingual alignment.","['Chih-chan Tien', 'Shane Steinert-Threlkeld']",,arXiv,2022,https://doi.org/10.48550/arXiv.2104.07642,Anomali
Natural Language Understanding with Privacy-Preserving BERT,"Privacy preservation remains a key challenge in dataminingand Natural Language Understanding (NLU). Previous research shows that the inputtextor eventextembeddings can leak private information. This concern motivates our research on effective privacy preservation approaches for pretrained Language Models (LMs). We investigate the privacy and utility implications of applying dx-privacy, a variant of Local Differential Privacy, to BERT fine-tuning in NLU applications. More importantly, we further propose privacy-adaptive LM pretraining methods and show that our approach can boost the utility of BERT dramatically while retaining the same level of privacy protection. We also quantify the level of privacy preservation and provide guidance on privacy configuration. Our experiments and findings lay the groundwork for future explorations of privacy-preserving NLU with pretrained LMs.","['Chen Qu', 'Weize Kong', 'Liu Yang', 'Mingyang Zhang', 'Michael Bendersky', 'Marc Najork']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.07504,Anomali
Empowering News Recommendation with Pre-trained Language Models,"Personalized news recommendation is an essential technique for online news services. News articles usually contain rich textual content, and accurate news modeling is important for personalized news recommendation. Existing news recommendation methods mainly model newstextsbased on traditionaltextmodeling methods, which is not optimal forminingthe deep semantic information in newstexts. Pre-trained language models (PLMs) are powerful for natural language understanding, which has the potential for better news modeling. However, there is no public report that show PLMs have been applied to news recommendation. In this paper, we report our work on exploiting pre-trained language models to empower news recommendation. Offline experimental results on both monolingual and multilingual news recommendation datasets show that leveraging PLMs for news modeling can effectively improve the performance of news recommendation. Our PLM-empowered news recommendation models have been deployed to the Microsoft News platform, and achieved significant gains in terms of both click and pageview in both English-speaking and global markets.","['Chuhan Wu', 'Fangzhao Wu', 'Tao Qi', 'Yongfeng Huang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.07413,Anomali
Community-Based Fact-Checking on Twitter's Birdwatch Platform,"Misinformation undermines the credibility of social media and poses significant threats to modern societies. As a countermeasure, Twitter has recently introduced ""Birdwatch,"" a community-driven approach to address misinformation on Twitter. On Birdwatch, users can identify tweets they believe are misleading, write notes that provide context to the tweet and rate the quality of other users' notes. In this work, we empirically analyze how users interact with this new feature. For this purpose, we collect {all} Birdwatch notes and ratings between the introduction of the feature in early 2021 and end of July 2021. We then map each Birdwatch note to the fact-checked tweet using Twitter's historical API. In addition, we usetextminingmethods to extract content characteristics from thetextexplanations in the Birdwatch notes (e.g., sentiment). Our empirical analysis yields the following main findings: (i) users more frequently file Birdwatch notes for misleading than not misleading tweets. These misleading tweets are primarily reported because of factual errors, lack of important context, or because they treat unverified claims as facts. (ii) Birdwatch notes are more helpful to other users if they link to trustworthy sources and if they embed a more positive sentiment. (iii) The social influence of the author of the source tweet is associated with differences in the level of user consensus. For influential users with many followers, Birdwatch notes yield a lower level of consensus among users and community-created fact checks are more likely to be seen as being incorrect and argumentative. Altogether, our findings can help social media platforms to formulate guidelines for users on how to write more helpful fact checks. At the same time, our analysis suggests that community-based fact-checking faces challenges regarding opinion speculation and polarization among the user base.",['Nicolas Pröllochs'],,arXiv,2021,https://doi.org/10.48550/arXiv.2104.07175,Anomali
Identity Inference on Blockchain using Graph Neural Network,"The anonymity of blockchain has accelerated the growth of illegal activities and criminal behaviors on cryptocurrency platforms. Although decentralization is one of the typical characteristics of blockchain, we urgently call for effective regulation to detect these illegal behaviors to ensure the safety and stability of user transactions. Identity inference, which aims to make a preliminary inference about account identity, plays a significant role in blockchain security. As a common tool, graphminingtechnique can effectively represent the interactive information between accounts and be used for identity inference. However, existing methods cannot balance scalability and end-to-end architecture, resulting high computational consumption and weak feature representation. In this paper, we present a novel approach to analyze user's behavior from the perspective of the transaction subgraph, which naturally transforms the identity inference task into a graph classification pattern and effectively avoids computation in large-scale graph. Furthermore, we propose a generic end-to-end graph neural network model, named$\text{I}^2 \text{BGNN}$, which can accept subgraph as input and learn a function mapping the transaction subgraph pattern to account identity, achieving de-anonymization. Extensive experiments on EOSG and ETHG datasets demonstrate that the proposed method achieve the state-of-the-art performance in identity inference.","['Jie Shen', 'Jiajun Zhou', 'Yunyi Xie', 'Shanqing Yu', 'Qi Xuan']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.06559,Anomali
Breaking Community Boundary: Comparing Academic and Social Communication Preferences regarding Global Pandemics,"The global spread of COVID-19 has caused pandemics to be widely discussed. This is evident in the large number of scientific articles and the amount of user-generated content on social media. This paper aims to compare academic communication and social communication about the pandemic from the perspective of communication preference differences. It aims to provide information for the ongoing research on global pandemics, thereby eliminating knowledge barriers and information inequalities between the academic and the social communities. First, we collected the fulltextand the metadata of pandemic-related articles and Twitter data mentioning the articles. Second, we extracted and analyzed the topics and sentiment tendencies of the articles and related tweets. Finally, we conducted pandemic-related differential analysis on the academic community and the social community. Weminedthe resulting data to generate pandemic communication preferences (e.g., information needs, attitude tendencies) of researchers and the public, respectively. The research results from 50,338 articles and 927,266 corresponding tweets mentioning the articles revealed communication differences about global pandemics between the academic and the social communities regarding the consistency of research recognition and the preferences for particular research topics. The analysis of large-scale pandemic-related tweets also confirmed the communication preference differences between the two communities.","['Qingqing Zhou', 'Chengzhi Zhang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.05409,Anomali
Results and Insights from Diagnostic Questions: The NeurIPS 2020 Education Challenge,"This competition concerns educational diagnostic questions, which are pedagogically effective, multiple-choice questions (MCQs) whose distractors embody misconceptions. With a large and ever-increasing number of such questions, it becomes overwhelming for teachers to know which questions are the best ones to use for their students. We thus seek to answer the following question: how can we use data on hundreds of millions of answers to MCQs to drive automatic personalized learning in large-scale learning scenarios where manual personalization is infeasible? Success in using MCQ data at scale helps build more intelligent, personalized learning platforms that ultimately improve the quality of education en masse. To this end, we introduce a new, large-scale, real-world dataset and formulate 4 dataminingtasks on MCQs that mimic real learning scenarios and target various aspects of the above question in a competition setting at NeurIPS 2020. We report on our NeurIPS competition in which nearly 400 teams submitted approximately 4000 submissions, with encouragingly diverse and effective approaches to each of our tasks.","['Zichao Wang', 'Angus Lamb', 'Evgeny Saveliev', 'Pashmina Cameron', 'Yordan Zaykov', 'Jose Miguel Hernandez-Lobato', 'Richard E. Turner', 'Richard G. Baraniuk', 'Craig Barton', 'Simon Peyton Jones', 'Simon Woodhead', 'Cheng Zhang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.04034,Anomali
Who Should Go First? A Self-Supervised Concept Sorting Model for Improving Taxonomy Expansion,"Taxonomies have been widely used in various machine learning andtextminingsystems to organize knowledge and facilitate downstream tasks. One critical challenge is that, as data and business scope grow in real applications, existing taxonomies need to be expanded to incorporate new concepts. Previous works on taxonomy expansion process the new concepts independently and simultaneously, ignoring the potential relationships among them and the appropriate order of inserting operations. However, in reality, the new concepts tend to be mutually correlated and form local hypernym-hyponym structures. In such a scenario, ignoring the dependencies of new concepts and the order of insertion may trigger error propagation. For example, existing taxonomy expansion systems may insert hyponyms to existing taxonomies before their hypernym, leading to sub-optimal expanded taxonomies. To complement existing taxonomy expansion systems, we propose TaxoOrder, a novel self-supervised framework that simultaneously discovers the local hypernym-hyponym structure among new concepts and decides the order of insertion. TaxoOrder can be directly plugged into any taxonomy expansion system and improve the quality of expanded taxonomies. Experiments on the real-world dataset validate the effectiveness of TaxoOrder to enhance taxonomy expansion systems, leading to better-resulting taxonomies with comparison to baselines under various evaluation metrics.","['Xiangchen Song', 'Jiaming Shen', 'Jieyu Zhang', 'Jiawei Han']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.03682,Anomali
"Comparative analysis of the government plans of the Peruvian presidential candidates, SDO(UN) and State Policies of the National Agreement based on NLP","The analysis of government proposal during elections from political parties is vital to choose the next authorities in any city or country. In this paper, we use atextminingapproach to analyze the documents and provide an easy visualization to support an easy analysis. Besides, a comparison with a national plan based on sustainable development objectives of UN(United Nations) from 2030 Agenda is perfomed using Natural Language techniques.","['Honorio Apaza Alanoca', 'Josimar Chire', 'Jimy Oblitas']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.01765,Anomali
Topic Scaling: A Joint Document Scaling -- Topic Model Approach To Learn Time-Specific Topics,"This paper proposes a new methodology to study sequential corpora by implementing a two-stage algorithm that learns time-based topics with respect to a scale of document positions and introduces the concept of Topic Scaling which ranks learned topics within the same document scale. The first stage ranks documents using Wordfish, a Poisson-based document scaling method, to estimate document positions that serve, in the second stage, as a dependent variable to learn relevant topics via a supervised Latent Dirichlet Allocation. This novelty brings two innovations intextminingas it explains document positions, whose scale is a latent variable, and ranks the inferred topics on the document scale to match their occurrences within the corpus and track their evolution. Tested on the U.S. State Of The Union two-party addresses, this inductive approach reveals that each party dominates one end of the learned scale with interchangeable transitions that follow the parties' term of office. Besides a demonstrated high accuracy in predicting in-sample documents' positions from topic scores, this method reveals further hidden topics that differentiate similar documents by increasing the number of learned topics to unfold potential nested hierarchical topic structures. Compared to other popular topic models, Topic Scaling learns topics with respect to document similarities without specifying a time frequency to learn topic evolution, thus capturing broader topic patterns than dynamic topic models and yielding more interpretable outputs than a plain latent Dirichlet allocation.","['Sami Diaf', 'Ulrich Fritsche']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.01117,Anomali
Streaming Social Event Detection and Evolution Discovery in Heterogeneous Information Networks,"Events are happening in real-world and real-time, which can be planned and organized for occasions, such as social gatherings, festival celebrations, influential meetings or sports activities. Social media platforms generate a lot of real-timetextinformation regarding public events with different topics. However,miningsocial events is challenging because events typically exhibit heterogeneous texture and metadata are often ambiguous. In this paper, we first design a novel event-based meta-schema to characterize the semantic relatedness of social events and then build an event-based heterogeneous information network (HIN) integrating information from external knowledge base. Second, we propose a novel Pairwise Popularity Graph Convolutional Network, named as PP-GCN, based on weighted meta-path instance similarity and textual semantic representation as inputs, to perform fine-grained social event categorization and learn the optimal weights of meta-paths in different tasks. Third, we propose a streaming social event detection and evolution discovery framework for HINs based on meta-path similarity search, historical information about meta-paths, and heterogeneous DBSCAN clustering method. Comprehensive experiments on real-world streaming socialtextdata are conducted to compare various social event detection and evolution discovery algorithms. Experimental results demonstrate that our proposed framework outperforms other alternative social event detection and evolution discovery techniques.","['Hao Peng', 'Jianxin Li', 'Yangqiu Song', 'Renyu Yang', 'Rajiv Ranjan', 'Philip S. Yu', 'Lifang He']",,arXiv,2021,https://doi.org/10.48550/arXiv.2104.00853,Anomali
No Keyword is an Island: In search of covert associations,"This paper describes how corpus-assisted discourse analysis based on keyword (KW) identification and interpretation can benefit from employing Market basket analysis (MBA) after KW extraction. MBA is a dataminingtechnique used originally in marketing that can reveal consistent associations between items in a shopping cart, but also between keywords in a corpus of manytexts. By identifying recurring associations between KWs we can compensate for the lack of wider context which is a major issue impeding the interpretation of isolated KWs (esp. when analyzing large data). To showcase the advantages of MBA in ""re-contextualizing"" keywords within the discourse, a pilot study on the topic of migration was conducted contrasting anti-system and center-right Czech internet media. was conducted. The results show that MBA is useful in identifying the dominant strategy of anti-system news portals: to weave in a confounding ideological undercurrent and connect the concept of migrants to a multitude of other topics (i.e., flooding the discourse).","['Václav Cvrček', 'Masako Ueda Fidler']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.17114,Anomali
Mining DEV for social and technical insights about software development,"Software developers are social creatures: they communicate, collaborate, and promote their work in a variety of channels. Twitter, GitHub, Stack Overflow, and other platforms offer developers opportunities to network and exchange ideas. Researchers analyze content on these sites to learn about trends and topics in software engineering. However, insightminedfrom thetextof Stack Overflow questions or GitHub issues is highly focused on detailed and technical aspects of software development. In this paper, we present a relatively new online community for software developers called DEV. On DEV users write long-form posts about their experiences, preferences, and working life in software, zooming out from specific issues and files to reflect on broader topics. About 50,000 users have posted over 140,000 articles related to software development. In this work, we describe the content of posts on DEV using a topic model, showing that developers discuss a rich variety and mixture of social and technical aspects of software development. We show that developers use DEV to promote themselves and their work: 83% link their profiles to their GitHub profiles and 56% to their Twitter profiles. 14% of users pin specific GitHub repos in their profiles. We argue that DEV is emerging as an important hub for software developers, and a valuable source of insight for researchers to complement data from platforms like GitHub and Stack Overflow.","['Maria Papoutsoglou', 'Johannes Wachs', 'Georgia M. Kapitsaki']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.17054,Anomali
Text Classification Using Hybrid Machine Learning Algorithms on Big Data,"Recently, there are unprecedented data growth originating from different online platforms which contribute to big data in terms of volume, velocity, variety and veracity (4Vs). Given this nature of big data which is unstructured, performing analytics to extract meaningful information is currently a great challenge to big data analytics. Collecting and analyzing unstructured textual data allows decision makers to study the escalation of comments/posts on our social media platforms. Hence, there is need for automatic big data analysis to overcome the noise and the non-reliability of these unstructured dataset from the digital media platforms. However, current machine learning algorithms used are performance driven focusing on the classification/prediction accuracy based on known properties learned from the training samples. With the learning task in a large dataset, most machine learning models are known to require high computational cost which eventually leads to computational complexity. In this work, two supervised machine learning algorithms are combined withtextminingtechniques to produce a hybrid model which consists of Naïve Bayes and support vector machines (SVM). This is to increase the efficiency and accuracy of the results obtained and also to reduce the computational cost and complexity. The system also provides an open platform where a group of persons with a common interest can share their comments/messages and these comments classified automatically as legal or illegal. This improves the quality of conversation among users. The hybrid model was developed using WEKA tools and Java programming language. The result shows that the hybrid model gave 96.76% accuracy as against the 61.45% and 69.21% of the Naïve Bayes and SVM models respectively.","['D. C. Asogwa', 'S. O. Anigbogu', 'I. E. Onyenwe', 'F. A. Sani']","International Journal of Trend in Research and Development, Volume 6(5), ISSN: 2394-9333, 2019",arXiv,2021,https://doi.org/10.48550/arXiv.2103.16624,Anomali
Text Mining of Stocktwits Data for Predicting Stock Prices,"Stock price prediction can be made more efficient by considering the price fluctuations and understanding the sentiments of people. A limited number of models understand financial jargon or have labelled datasets concerning stock price change. To overcome this challenge, we introduced FinALBERT, an ALBERT based model trained to handle financial domaintextclassification tasks by labelling Stocktwitstextdata based on stock price change. We collected Stocktwits data for over ten years for 25 different companies, including the major five FAANG (Facebook, Amazon, Apple, Netflix, Google). These datasets were labelled with three labelling techniques based on stock price changes. Our proposed model FinALBERT is fine-tuned with these labels to achieve optimal results. We experimented with the labelled dataset by training it on traditional machine learning, BERT, and FinBERT models, which helped us understand how these labels behaved with different model architectures. Our labelling method competitive advantage is that it can help analyse the historical data effectively, and the mathematical function can be easily customised to predict stock movement.","['Mukul Jaggi', 'Priyanka Mandal', 'Shreya Narang', 'Usman Naseem', 'Matloob Khushi']","Appl. Syst. Innov. 2021, 4, 13",arXiv,2021,https://doi.org/10.48550/arXiv.2103.16388,Anomali
A Multi-View Framework to Detect Redundant Activity Labels for More Representative Event Logs in Process Mining,"Processminingaims to gain knowledge of business processes via the discovery of process models from event logs generated by information systems. The insights revealed from processminingheavily rely on the quality of the event logs. Activities extracted from different data sources or the free-textnature within the same system may lead to inconsistent labels. Such inconsistency would then lead to redundancy in activity labels, which refer to labels that have different syntax but share the same behaviours. Redundant activity labels could introduce unnecessary complexities to the event logs. The identifications of these labels from data-driven process discovery are difficult and rely heavily on human intervention. Neither existing process discovery algorithms nor event data preprocessing techniques can solve such redundancy efficiently. In this paper, we propose a multi-view approach to automatically detect redundant activity labels using not only context-aware features such as control--flow relations and attribute values but also semantic features from the event logs. Our evaluation of several publicly available datasets and a real-life case study demonstrate that our approach can efficiently detect redundant activity labels even with low-occurrence frequencies. The proposed approach can add value to the preprocessing step to generate more representative event logs.","['Qifan Chen', 'Yang Lu', 'Charmaine S. Tam', 'Simon K. Poon']",,arXiv,2022,https://doi.org/10.48550/arXiv.2103.16061,Anomali
Memory Enhanced Embedding Learning for Cross-Modal Video-Text Retrieval,"Cross-modal video-textretrieval, a challenging task in the field of vision and language, aims at retrieving corresponding instance giving sample from either modality. Existing approaches for this task all focus on how to design encoding model through a hard negative ranking loss, leaving two key problems unaddressed during this procedure. First, in the training stage, only a mini-batch of instance pairs is available in each iteration. Therefore, this kind of hard negatives is locallyminedinside a mini-batch while ignoring the global negative samples among the dataset. Second, there are manytextdescriptions for one video and eachtextonly describes certain local features of a video. Previous works for this task did not consider to fuse the multiplytextscorresponding to a video during the training. In this paper, to solve the above two problems, we propose a novel memory enhanced embedding learning (MEEL) method for videotext retrieval. To be specific, we construct two kinds of memory banks respectively: cross-modal memory module andtextcenter memory module. The cross-modal memory module is employed to record the instance embeddings of all the datasets for global negativemining. To avoid the fast evolving of the embedding in the memory bank during training, we utilize a momentum encoder to update the features by a moving-averaging strategy. Thetextcenter memory module is designed to record the center information of the multiple textual instances corresponding to a video, and aims at bridging these textual instances together. Extensive experimental results on two challenging benchmarks, i.e., MSR-VTT and VATEX, demonstrate the effectiveness of the proposed method.","['Rui Zhao', 'Kecheng Zheng', 'Zheng-Jun Zha', 'Hongtao Xie', 'Jiebo Luo']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.15686,Anomali
Augmenting Text Mining Approaches with Social Network Analysis to Understand the Complex Relationships among Users' Requests: a Case Study of the Android Operating System,"Textminingapproaches are being used increasingly for business analytics. In particular, such approaches are now central to understanding users' feedback regarding systems delivered via online application distribution platforms such as Google Play. In such settings, large volumes of reviews of potentially numerous apps and systems means that it is infeasible to use manual mechanisms to extract insights and knowledge that could inform product improvement. In this context of identifying software system improvement options,textminingtechniques are used to reveal the features that are mentioned most often as being in need of correction (e.g., GPS), and topics that are associated with features perceived as being defective (e.g., inaccuracy of GPS). Other approaches may supplement such techniques to provide further insights for online communities and solution providers. In this work we augmenttextminingapproaches with social network analysis to demonstrate the utility of using multiple techniques. Our outcomes suggest thattextminingapproaches may indeed be supplemented with other methods to deliver a broader range of insights.","['Chan Won Lee', 'Sherlock A. Licorish', 'Bastin Tony Roy Savarimuthu', 'Stephen G. MacDonell']","Proceedings of the 49th Hawaii International Conference on System Sciences (HICSS2016). Koloa HI, USA, AIS, pp.1144-1153",arXiv,2021,https://doi.org/10.48550/arXiv.2103.14761,Anomali
Predicting the future success of scientific publications through social network and semantic analysis,"Citations acknowledge the impact a scientific publication has on subsequent work. At the same time, deciding how and when to cite a paper, is also heavily influenced by social factors. In this work, we conduct an empirical analysis based on a dataset of 2010-2012 global publications in chemical engineering. We use social network analysis andtextminingto measure publication attributes and understand which variables can better help predicting their future success. Controlling for intrinsic quality of a publication and for the number of authors in the byline, we are able to predict scholarly impact of a paper in terms of citations received six years after publication with almost 80 percent accuracy. Results suggest that, all other things being equal, it is better to co-publish with rotating co-authors and write the papers' abstract using more positive words, and a more complex, thus more informative, language. Publications that result from the collaboration of different social groups also attract more citations.","['Andrea Fronzetti Colladon', ""Ciriaco Andrea D'Angelo"", 'Peter A. Gloor']","Scientometrics, 124(1), 357-377(2020)",arXiv,2021,https://doi.org/10.48550/arXiv.2103.14556,Anomali
Promoting Fairness through Hyperparameter Optimization,"Considerable research effort has been guided towards algorithmic fairness but real-world adoption of bias reduction techniques is still scarce. Existing methods are either metric- or model-specific, require access to sensitive attributes at inference time, or carry high development or deployment costs. This work explores the unfairness that emerges when optimizing ML models solely for predictive performance, and how to mitigate it with a simple and easily deployed intervention: fairness-aware hyperparameter optimization (HO). We propose and evaluate fairness-aware variants of three popular HO algorithms: Fair Random Search, Fair TPE, and Fairband. We validate our approach on a real-world bank account opening fraud case-study, as well as on three datasets from the fairness literature. Results show that, without extra training cost, it is feasible to find models with 111% mean fairness increase and just 6% decrease in performance when compared with fairness-blind HO.","['André F. Cruz', 'Pedro Saleiro', 'Catarina Belém', 'Carlos Soares', 'Pedro Bizarro']",2021 IEEE International Conference on Data Mining (ICDM),arXiv,2021,https://doi.org/10.48550/arXiv.2103.12715,Anomali
HSEarch: semantic search system for workplace accident reports,"Semantic search engines, which integrate the output oftextmining(TM) methods, can significantly increase the ease and efficiency of finding relevant documents and locating important information within them. We present a novel search engine for the construction industry, HSEarch (http://www.nactem.ac.uk/hse/), which uses TM methods to provide semantically-enhanced, faceted search over a repository of workplace accident reports. Compared to previous TM-driven search engines for the construction industry, HSEarch provides a more interactive means for users to explore the contents of the repository, to review documents more systematically and to locate relevant knowledge within them.","['Emrah Inan', 'Paul Thompson', 'Tim Yates', 'Sophia Ananiadou']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.12420,Anomali
Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets,"With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, web-minedtextdatasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usabletext, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.","['Julia Kreutzer', 'Isaac Caswell', 'Lisa Wang', 'Ahsan Wahab', 'Daan van Esch', 'Nasanbayar Ulzii-Orshikh', 'Allahsera Tapo', 'Nishant Subramani', 'Artem Sokolov', 'Claytone Sikasote', 'Monang Setyawan', 'Supheakmungkol Sarin', 'Sokhar Samb', 'Benoît Sagot', 'Clara Rivera', 'Annette Rios', 'Isabel Papadimitriou', 'Salomey Osei', 'Pedro Ortiz Suarez', 'Iroro Orife', 'Kelechi Ogueji', 'Andre Niyongabo Rubungo', 'Toan Q. Nguyen', 'Mathias Müller', 'André Müller']",Transactions of the Association for Computational Linguistics (2022) 10: 50-72,arXiv,2022,https://doi.org/10.48550/arXiv.2103.12028,Anomali
"Common Sense Knowledge, Ontology and Text Mining for Implicit Requirements","The ability of a system to meet its requirements is a strong determinant of success. Thus effective requirements specification is crucial. Explicit Requirements are well-defined needs for a system to execute. IMplicit Requirements (IMRs) are assumed needs that a system is expected to fulfill though not elicited during requirements gathering. Studies have shown that a major factor in the failure of software systems is the presence of unhandled IMRs. Since relevance of IMRs is important for efficient system functionality, there are methods developed to aid the identification and management of IMRs. In this paper, we emphasize that Common Sense Knowledge, in the field of Knowledge Representation in AI, would be useful to automatically identify and manage IMRs. This paper is aimed at identifying the sources of IMRs and also proposing an automated support tool for managing IMRs within an organizational context. Since this is found to be a present gap in practice, our work makes a contribution here. We propose a novel approach for identifying and managing IMRs based on combining three core technologies: common sense knowledge,textminingand ontology. We claim that discovery and handling of unknown and non-elicited requirements would reduce risks and costs in software development.","['Onyeka Emebo', 'Aparna S. Varde', 'Olawande Daramola']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.11302,Anomali
Which contributions count? Analysis of attribution in open source,"Open source software projects usually acknowledge contributions withtextfiles, websites, and other idiosyncratic methods. These data sources are hard tomine, which is why contributorship is most frequently measured through changes to repositories, such as commits, pushes, or patches. Recently, some open source projects have taken to recording contributor actions with standardized systems; this opens up a unique opportunity to understand how community-generated notions of contributorship map onto codebases as the measure of contribution. Here, we characterize contributor acknowledgment models in open source by analyzing thousands of projects that use a model called All Contributors to acknowledge diverse contributions like outreach, finance, infrastructure, and community management. We analyze the life cycle of projects through this model's lens and contrast its representation of contributorship with the picture given by other methods of acknowledgment, including GitHub's top committers indicator and contributions derived from actions taken on the platform. We find that community-generated systems of contribution acknowledgment make work like idea generation or bug finding more visible, which generates a more extensive picture of collaboration. Further, we find that models requiring explicit attribution lead to more clearly defined boundaries around what is and what is not a contribution.","['Jean-Gabriel Young', 'Amanda Casari', 'Katie McLaughlin', 'Milo Z. Trujillo', 'Laurent Hébert-Dufresne', 'James P. Bagrow']","2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), pp. 242-253 (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2103.11007,Anomali
Code Word Detection in Fraud Investigations using a Deep-Learning Approach,"In modern litigation, fraud investigators often face an overwhelming number of documents that must be reviewed throughout a matter. In the majority of legal cases, fraud investigators do not know beforehand, exactly what they are looking for, nor where to find it. In addition, fraudsters may use deception to hide their behaviour and intentions by using code words. Effectively, this means fraud investigators are looking for a needle in the haystack without knowing what the needle looks like.
  As part of a larger research program, we use a framework to expedite the investigation process applyingtext-miningand machine learning techniques. We structure this framework using three well-known methods in fraud investigations: (i) the fraud triangle (ii) the golden (""W"") investigation questions, and (iii) the analysis of competing hypotheses. With this framework, it is possible to automatically organize investigative data, so it is easier for investigators to find answers to typical investigative questions.
  In this research, we focus on one of the components of this framework: the identification of the usage of code words by fraudsters. Here for, a novel (annotated) synthetic data set is created containing such code words, hidden in normal email communication. Subsequently, a range of machine learning techniques are employed to detect such code words. We show that the state-of-the-art BERT model significantly outperforms other methods on this task. With this result, we demonstrate that deep neural language models can reliably (F1 score of 0.9) be applied in fraud investigations for the detection of code words.","['Youri van der Zee', 'Jan C. Scholtes', 'Marcel Westerhoud', 'Julien Rossi']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.09606,Anomali
Combining Morphological and Histogram based Text Line Segmentation in the OCR Context,"Textline segmentation is one of the pre-stages of modern optical character recognition systems. The algorithmic approach proposed by this paper has been designed for this exact purpose. Its main characteristic is the combination of two different techniques, morphological image operations and horizontal histogram projections. The method was developed to be applied on a historic data collection that commonly features quality issues, such as degraded paper, blurredtext, or presence of noise. For that reason, the segmenter in question could be of particular interest for cultural institutions, that want access to robust line bounding boxes for a given historic document. Because of the promising segmentation results that are joined by low computational cost, the algorithm was incorporated into the OCR pipeline of the National Library of Luxembourg, in the context of the initiative of reprocessing their historic newspaper collection. The general contribution of this paper is to outline the approach and to evaluate the gains in terms of accuracy and speed, comparing it to the segmentation algorithm bundled with the used open source OCR software.",['Pit Schneider'],"Journal of Data Mining & Digital Humanities, 2021, HistoInformatics (November 4, 2021) jdmdh:7277",arXiv,2021,https://doi.org/10.48550/arXiv.2103.08922,Anomali
A Weakly Supervised Approach for Classifying Stance in Twitter Replies,"Conversations on social media (SM) are increasingly being used to investigate social issues on the web, such as online harassment and rumor spread. For such issues, a common thread of research uses adversarial reactions, e.g., replies pointing out factual inaccuracies in rumors. Though adversarial reactions are prevalent in online conversations, inferring those adverse views (or stance) from thetextin replies is difficult and requires complex natural language processing (NLP) models. Moreover, conventional NLP models for stanceminingneed labeled data for supervised learning. Getting labeled conversations can itself be challenging as conversations can be on any topic, and topics change over time. These challenges make learning the stance a difficult NLP problem.
  In this research, we first create a new stance dataset comprised of three different topics by labeling both users' opinions on the topics (as in pro/con) and users' stance while replying to others' posts (as in favor/oppose). As we find limitations with supervised approaches, we propose a weakly-supervised approach to predict the stance in Twitter replies. Our novel method allows using a smaller number of hashtags to generate weak labels for Twitter replies. Compared to supervised learning, our method improves the mean F1-macro by 8\% on the hand-labeled dataset without using any hand-labeled examples in the training set. We further show the applicability of our proposed method on COVID 19 related conversations on Twitter.","['Sumeet Kumar', 'Ramon Villa Cox', 'Matthew Babcock', 'Kathleen M. Carley']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.07098,Anomali
"""Sharks are not the threat humans are"": Argument Component Segmentation in School Student Essays","Argumentminingis often addressed by a pipeline method where segmentation oftextinto argumentative units is conducted first and proceeded by an argument component identification task. In this research, we apply a token-level classification to identify claim and premise tokens from a new corpus of argumentative essays written by middle school students. To this end, we compare a variety of state-of-the-art models such as discrete features and deep learning architectures (e.g., BiLSTM networks and BERT-based architectures) to identify the argument components. We demonstrate that a BERT-based multi-task learning architecture (i.e., token and sentence level classification) adaptively pretrained on a relevant unlabeled dataset obtains the best results","['Tariq Alhindi', 'Debanjan Ghosh']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.04518,Anomali
So you want to be a Super Researcher?,"Publishing original scientific research is inherent to the work of a researcher. However, the pressure to maintain productivity and scientific impact can lead to research group publishing excessively, negatively affecting the mental health of a researcher. Ph.D. students and early career researchers are particularly susceptible to this pressure due to the inherent vulnerability of their positions. At present, there are no resources that concisely summarise the publication culture of a research group to help the researcher make an informed decision before joining. In this article, we present the 'Super Researcher' app, an R Shiny application(app) with a user-friendly interface. Usingtext-miningmethodology to extract publicly available author data from Scopus, this pilot app has four fundamental functions to provide snapshot information that will help researchers grasp the publication culture of a research group within minutes. The 'Super Researcher' app provides information on: 1) institution data, 2) author's publication, 3) co-author network plots and 4) publication journals. The 'Super Researcher' app is built on R shiny which provides an interactive interface to users. This app utilizes the Big Data framework Apache Spark tominerelevant information from a huge author information database. The author's information is stored and manipulated using both SQL(SQLite) and NoSQL(HBase) databases. Hbase is used for local data storage and manipulation while SQLite feeds data to the R Shiny interface. In this paper, we introduce these functionalities and illustrate how this information can help guide a researcher to select a new Principle Investigator (PI) with better compatibility in terms of publication attitude using a case study.
  Available: https://researchmind.co.uk/super-researcher/","['Sanjay Rathee', 'Sheah Lin Lee']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.03351,Anomali
Straggler Mitigation through Unequal Error Protection for Distributed Approximate Matrix Multiplication,"Large-scale machine learning and dataminingmethods routinely distribute computations across multiple agents to parallelize processing. The time required for the computations at the agents is affected by the availability of local resources and/or poor channel conditions giving rise to the ""straggler problem"". As a remedy to this problem, we employ Unequal Error Protection (UEP) codes to obtain an approximation of the matrix product in the distributed computation setting to provide higher protection for the blocks with higher effect on the final result. We characterize the performance of the proposed approach from a theoretical perspective by bounding the expected reconstruction error for matrices with uncorrelated entries. We also apply the proposed coding strategy to the computation of the back-propagation step in the training of a Deep Neural Network (DNN) for an image classification task in the evaluation of the gradients. Our numerical experiments show that it is indeed possible to obtain significant improvements in the overall time required to achieve the DNN training convergence by producing approximation of matrix products using UEP codes in the presence of stragglers.","['Busra Tegin', 'Eduin. E. Hernandez', 'Stefano Rini', 'Tolga M. Duman']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.02928,Anomali
OAG-BERT: Towards A Unified Backbone Language Model For Academic Knowledge Services,"Academic knowledge services have substantially facilitated the development of the science enterprise by providing a plenitude of efficient research tools. However, many applications highly depend on ad-hoc models and expensive human labeling to understand scientific contents, hindering deployments into real products. To build a unified backbone language model for different knowledge-intensive academic applications, we pre-train an academic language model OAG-BERT that integrates both the heterogeneous entity knowledge and scientific corpora in the Open Academic Graph (OAG) -- the largest public academic graph to date. In OAG-BERT, we develop strategies for pre-trainingtextand entity data along with zero-shot inference techniques. In OAG-BERT, we develop strategies for pre-trainingtextand entity data along with zero-shot inference techniques. Its zero-shot capability furthers the path to mitigate the need of expensive annotations. OAG-BERT has been deployed for real-world applications, such as the reviewer recommendation function for National Nature Science Foundation of China (NSFC) -- one of the largest funding agencies in China -- and paper tagging in AMiner. All codes and pre-trained models are available via the CogDL toolkit.","['Xiao Liu', 'Da Yin', 'Jingnan Zheng', 'Xingjian Zhang', 'Peng Zhang', 'Hongxia Yang', 'Yuxiao Dong', 'Jie Tang']","In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2022). Association for Computing Machinery, New York, NY, USA, 3418-3428",arXiv,2022,https://doi.org/10.48550/arXiv.2103.02410,Anomali
The Healthy States of America: Creating a Health Taxonomy with Social Media,"Since the uptake of social media, researchers haveminedonline discussions to track the outbreak and evolution of specific diseases or chronic conditions such as influenza or depression. To broaden the set of diseases under study, we developed a Deep Learning tool for Natural Language Processing that extracts mentions of virtually any medical condition or disease from unstructured social mediatext. With that tool at hand, we processed Reddit and Twitter posts, analyzed the clusters of the two resulting co-occurrence networks of conditions, and discovered that they correspond to well-defined categories of medical conditions. This resulted in the creation of the first comprehensive taxonomy of medical conditions automatically derived from online discussions. We validated the structure of our taxonomy against the official International Statistical Classification of Diseases and Related Health Problems (ICD-11), finding matches of our clusters with 20 official categories, out of 22. Based on the mentions of our taxonomy's sub-categories on Reddit posts geo-referenced in the U.S., we were then able to compute disease-specific health scores. As opposed to counts of disease mentions or counts with no knowledge of our taxonomy's structure, we found that our disease-specific health scores are causally linked with the officially reported prevalence of 18 conditions.","['Sanja Scepanovic', 'Luca Maria Aiello', 'Ke Zhou', 'Sagar Joglekar', 'Daniele Quercia']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.01169,Anomali
CHAMP: Characterizing Undesired App Behaviors from User Comments based on Market Policies,"Millions of mobile apps have been available through various app markets. Although most app markets have enforced a number of automated or even manual mechanisms to vet each app before it is released to the market, thousands of low-quality apps still exist in different markets, some of which violate the explicitly specified market policies.In order to identify these violations accurately and timely, we resort to user comments, which can form an immediate feedback for app market maintainers, to identify undesired behaviors that violate market policies, including security-related user concerns. Specifically, we present the first large-scale study to detect and characterize the correlations between user comments and market policies. First, we propose CHAMP, an approach that adoptstextminingand natural language processing (NLP) techniques to extract semantic rules through a semi-automated process, and classifies comments into 26 pre-defined types of undesired behaviors that violate market policies. Our evaluation on real-world user comments shows that it achieves both high precision and recall ($>0.9$) in classifying comments for undesired behaviors. Then, we curate a large-scale comment dataset (over 3 million user comments) from apps in Google Play and 8 popular alternative Android app markets, and apply CHAMP to understand the characteristics of undesired behavior comments in the wild. The results confirm our speculation that user comments can be used to pinpoint suspicious apps that violate policies declared by app markets. The study also reveals that policy violations are widespread in many app markets despite their extensive vetting efforts. CHAMP can be a \textit{whistle blower} that assigns policy-violation scores and identifies most informative comments for apps.","['Yangyu Hu', 'Haoyu Wang', 'Tiantong Ji', 'Xusheng Xiao', 'Xiapu Luo', 'Peng Gao', 'Yao Guo']",,arXiv,2021,https://doi.org/10.48550/arXiv.2103.00712,Anomali
A Survey on Stance Detection for Mis- and Disinformation Identification,"Understanding attitudes expressed intexts, also known as stance detection, plays an important role in systems for detecting false information online, be it misinformation (unintentionally false) or disinformation (intentionally false information). Stance detection has been framed in different ways, including (a) as a component of fact-checking, rumour detection, and detecting previously fact-checked claims, or (b) as a task in its own right. While there have been prior efforts to contrast stance detection with other related tasks such as argumentationminingand sentiment analysis, there is no existing survey on examining the relationship between stance detection and mis- and disinformation detection. Here, we aim to bridge this gap by reviewing and analysing existing work in this area, with mis- and disinformation in focus, and discussing lessons learnt and future challenges.","['Momchil Hardalov', 'Arnav Arora', 'Preslav Nakov', 'Isabelle Augenstein']",,arXiv,2022,https://doi.org/10.48550/arXiv.2103.00242,Anomali
OneStop QAMaker: Extract Question-Answer Pairs from Text in a One-Stop Approach,"Large-scale question-answer (QA) pairs are critical for advancing research areas like machine reading comprehension and question answering. To construct QA pairs from documents requires determining how to ask a question and what is the corresponding answer. Existing methods for QA pair generation usually follow a pipeline approach. Namely, they first choose the most likely candidate answer span and then generate the answer-specific question. This pipeline approach, however, is undesired inminingthe most appropriate QA pairs from documents since it ignores the connection between question generation and answer extraction, which may lead to incompatible QA pair generation, i.e., the selected answer span is inappropriate for question generation. However, for human annotators, we take the whole QA pair into account and consider the compatibility between question and answer. Inspired by such motivation, instead of the conventional pipeline approach, we propose a model named OneStop generate QA pairs from documents in a one-stop approach. Specifically, questions and their corresponding answer span is extracted simultaneously and the process of question generation and answer extraction mutually affect each other. Additionally, OneStop is much more efficient to be trained and deployed in industrial scenarios since it involves only one model to solve the complex QA generation task. We conduct comprehensive experiments on three large-scale machine reading comprehension datasets: SQuAD, NewsQA, and DuReader. The experimental results demonstrate that our OneStop model outperforms the baselines significantly regarding the quality of generated questions, quality of generated question-answer pairs, and model efficiency.","['Shaobo Cui', 'Xintong Bao', 'Xinxing Zu', 'Yangyang Guo', 'Zhongzhou Zhao', 'Ji Zhang', 'Haiqing Chen']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.12128,Anomali
10 W injection-locked single-frequency continuous-wave titanium:sapphire laser,"High-power tunable lasers with good longitudinal and transverse modes are fundamental tools for exploring quantum physics. Here we report a high-power continuous-wave injection-locked titanium:sapphire laser with a low-loss cavity configuration, where only a laser crystal was installed in the laser cavity. Although the transverse mode was affected by a thermal lens formed in the laser crystal, the focal length of the thermal lens could be shifted via the temperature of the laser crystal holder or the pump power. As a result, we found a condition that 10 W single-frequency oscillation with a good transverse mode and a slope efficiency of 51% were achieved.","['Tetsushi Takano', 'Hisashi Ogawa', 'Chiaki Ohae', 'Masayuki Katsuragawa']","Optics Express Vol. 29, Issue 5, pp. 6927-6934 (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2102.10725,Anomali
Combining Text Mining and Visualization Techniques to Study Teams' Behavioral Processes,"There is growing interest inminingsoftware repository data to understand, and predict, various aspects of team processes. In particular,textminingand natural-language processing (NLP) techniques have supported such efforts. Visualization may also supplementtextminingto reveal unique multi-dimensional insights into software teams' behavioral processes. We demonstrate the utility of combining these approaches in this study. Future application of these methods to the study of teams' behavioral processes offers promise for both research and practice.","['Sherlock A. Licorish', 'Stephen G. MacDonell']","Proceedings of the 4th IEEE Workshop on Mining Unstructured Data (MUD2014). Victoria BC, Canada, IEEE Computer Society Press, pp.16-20",arXiv,2021,https://doi.org/10.48550/arXiv.2102.09710,Anomali
A Core of E-Commerce Customer Experience based on Conversational Data using Network Text Methodology,"E-commerce provides an efficient and effective way to exchange goods between sellers and customers. E-commerce has been a popular method for doing business, because of its simplicity of having commerce activity transparently available, including customer voice and opinion about their own experience. Those experiences can be a great benefit to understand customer experience comprehensively, both for sellers and future customers. This paper applies to e-commerces and customers in Indonesia. Many Indonesian customers expressed their voice to open social network services such as Twitter and Facebook, where a large proportion of data is in the form of conversational data. By understanding customer behavior through open social network service, we can have descriptions about the e-commerce services level in Indonesia. Thus, it is related to the government's effort to improve the Indonesian digital economy ecosystem. A method for finding core topics in large-scale internet unstructuredtextdata is needed, where the method should be fast but sufficiently accurate. Processing large-scale data is not a straightforward job, it often needs special skills of people and complex software and hardware computer system. We propose a fast methodology oftextminingmethods based on frequently appeared words and their word association to form networktextmethodology. This method is adapted from Social Network Analysis by the model relationships between words instead of actors.","['Andry Alamsyah', 'Nurlisa Laksmiani', 'Lies Anisa Rahimi']","International Journal of Business, 2018, 23(3)",arXiv,2021,https://doi.org/10.48550/arXiv.2102.09107,Anomali
I Want This Product but Different : Multimodal Retrieval with Synthetic Query Expansion,"This paper addresses the problem of media retrieval using a multimodal query (a query which combines visual input with additional semantic information in natural language feedback). We propose a SynthTriplet GAN framework which resolves this task by expanding the multimodal query with a synthetically generated image that captures semantic information from both image andtextinput. We introduce a novel tripletminingmethod that uses a synthetic image as an anchor to directly optimize for embedding distances of generated and target images. We demonstrate that apart from the added value of retrieval illustration with synthetic image with the focus on customization and user feedback, the proposed method greatly surpasses other multimodal generation methods and achieves state of the art results in the multimodal retrieval task. We also show that in contrast to other retrieval methods, our method provides explainable embeddings.","['Ivona Tautkute', 'Tomasz Trzcinski']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.08871,Anomali
The economic dependency of the Bitcoin security,"We study to what extent the Bitcoin blockchain security permanently depends on the underlying distribution of cryptocurrency market outcomes. We use daily blockchain and Bitcoin data for 2014-2019 and employ the ARDL approach. We test three equilibrium hypotheses: (i) sensitivity of the Bitcoin blockchain tominingreward; (ii) security outcomes of the Bitcoin blockchain and the proof-of-work cost; and (iii) the speed of adjustment of the Bitcoin blockchain security to deviations from the equilibrium path. Our results suggest that the Bitcoin price andminingrewards are intrinsically linked to Bitcoin security outcomes. The Bitcoin blockchain security's dependency onminingcosts is geographically differenced - it is more significant for the globalminingleader China than for other world regions. After input or output price shocks, the Bitcoin blockchain security reverts to its equilibrium security level.","['Pavel Ciaian', ""d'Artis Kancs"", 'Miroslava Rajcaniova']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.08378,Anomali
Real-time tracking of COVID-19 and coronavirus research updates through text mining,"The novel coronavirus (SARS-CoV-2) which causes COVID-19 is an ongoing pandemic. There are ongoing studies with up to hundreds of publications uploaded to databases daily. We are exploring the use-case of artificial intelligence and natural language processing in order to efficiently sort through these publications. We demonstrate that clinical trial information, preclinical studies, and a general topic model can be used astextminingdata intelligence tools for scientists all over the world to use as a resource for their own research. To evaluate our method, several metrics are used to measure the information extraction and clustering results. In addition, we demonstrate that our workflow not only have a use-case for COVID-19, but for other disease areas as well. Overall, our system aims to allow scientists to more efficiently research coronavirus. Our automatically updating modules are available on our information portal at https://ghddi-ailab.github.io/Targeting2019-nCoV/ for public viewing.","['Yutong Jin', 'Jie Li', 'Xinyu Wang', 'Peiyao Li', 'Jinjiang Guo', 'Junfeng Wu', 'Dawei Leng', 'Lurong Pan']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.07640,Anomali
Accelerating COVID-19 research with graph mining and transformer-based learning,"In 2020, the White House released the, ""Call to Action to the Tech Community on New Machine Readable COVID-19 Dataset,"" wherein artificial intelligence experts are asked to collect data and developtextminingtechniques that can help the science community answer high-priority scientific questions related to COVID-19. The Allen Institute for AI and collaborators announced the availability of a rapidly growing open dataset of publications, the COVID-19 Open Research Dataset (CORD-19). As the pace of research accelerates, biomedical scientists struggle to stay current. To expedite their investigations, scientists leverage hypothesis generation systems, which can automatically inspect published papers to discover novel implicit connections. We present an automated general purpose hypothesis generation systems AGATHA-C and AGATHA-GP for COVID-19 research. The systems are based on graph-miningand the transformer model. The systems are massively validated using retrospective information rediscovery and proactive analysis involving human-in-the-loop expert analysis. Both systems achieve high-quality predictions across domains (in some domains up to 0.97% ROC AUC) in fast computational time and are released to the broad scientific community to accelerate biomedical research. In addition, by performing the domain expert curated study, we show that the systems are able to discover on-going research findings such as the relationship between COVID-19 and oxytocin hormone.","['Ilya Tyagin', 'Ankit Kulshrestha', 'Justin Sybrandt', 'Krish Matta', 'Michael Shtutman', 'Ilya Safro']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.07631,Anomali
Pulse of the Pandemic: Iterative Topic Filtering for Clinical Information Extraction from Social Media,"The rapid evolution of the COVID-19 pandemic has underscored the need to quickly disseminate the latest clinical knowledge during a public-health emergency. One surprisingly effective platform for healthcare professionals (HCPs) to share knowledge and experiences from the front lines has been social media (for example, the ""#medtwitter"" community on Twitter). However, identifying clinically-relevant content in social media without manual labeling is a challenge because of the sheer volume of irrelevant data. We present an unsupervised, iterative approach tomineclinically relevant information from social media data, which begins by heuristically filtering for HCP-authoredtextsand incorporates topic modeling and concept extraction with MetaMap. This approach identifies granular topics and tweets with high clinical relevance from a set of about 52 million COVID-19-related tweets from January to mid-June 2020. We also show that because the technique does not require manual labeling, it can be used to identify emerging topics on a week-to-week basis. Our method can aid in future public-health emergencies by facilitating knowledge transfer among healthcare workers in a rapidly-changing information environment, and by providing an efficient and unsupervised way of highlighting potential areas for clinical research.","['Julia Wu', 'Venkatesh Sivaraman', 'Dheekshita Kumar', 'Juan M. Banda', 'David Sontag']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.06836,Anomali
Leveraging Artificial Intelligence to Analyze Citizens' Opinions on Urban Green Space,"Continued population growth and urbanization is shifting research to consider the quality of urban green space over the quantity of these parks, woods, and wetlands. The quality of urban green space has been hitherto measured by expert assessments, including in-situ observations, surveys, and remote sensing analyses. Location data platforms, such as TripAdvisor, can provide people's opinion on many destinations and experiences, including UGS. This paper leverages Artificial Intelligence techniques for opinionminingandtextclassification using such platform's reviews as a novel approach to urban green space quality assessments. Natural Language Processing is used to analyze contextual information given supervised scores of words by implementing computational analysis. Such an application can support local authorities and stakeholders in their understanding of and justification for future investments in urban green space.","['Mohammadhossein Ghahramani', 'Nadina J. Galle', 'Fabio Duarte', 'Carlo Ratti', 'Francesco Pilla']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.06659,Anomali
A Visual Analysis Approach to Update Systematic Reviews,"Context: In order to preserve the value of Systematic Reviews (SRs), they should be frequently updated considering new evidence that has been produced since the completion of the previous version of the reviews. However, the update of an SR is a time consuming, manual task. Thus, many SRs have not been updated as they should be and, therefore, they are currently outdated. Objective: The main contribution of this paper is to support the update of SRs. Method: We propose USR-VTM, an approach based on VisualTextMining(VTM) techniques, to support selection of new evidence in the form of primary studies. We then present a tool, named Revis, which supports our approach. Finally, we evaluate our approach through a comparison of outcomes achieved using USR-VTM versus the traditional (manual) approach. Results: Our results show that USR-VTM increases the number of studies correctly included compared to the traditional approach. Conclusions: USR-VTM effectively supports the update of SRs.","['Katia Romero Felizardo', 'Elisa Yumi Nakagawa', 'Stephen G. MacDonell', 'José Carlos Maldonado']","Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering (EASE2014). London, UK, ACM Press, pp.1-10",arXiv,2021,https://doi.org/10.48550/arXiv.2102.06345,Anomali
Relating IS Developers' Attitudes to Engagement,"Increasing effort is being directed to understanding the personality profiles of highly engaged information systems (IS) developers and the impact of such profiles on development outcomes. However, there has been a lesser degree of attention paid to studying attitudes at a fine-grained level, and relating such attitudes to developers' in-process activities, in spite of the fact that social motivation theory notes the importance of such a relationship in general group work. We have therefore applied linguistic analysis,textminingand visualization, and statistical analysis techniques to artefacts developed by 474 developers to study these issues. Our results indicate that our sample of IS developers conveyed a range of attitudes while working to deliver systems features, and those practitioners who communicated the most were also the most engaged. Additionally, of eight linguistic dimensions considered, expressions regarding work and achievement, as well as insightful attitudes, were most closely related to developers' engagement. Accordingly, team diversity and the provision of active support for outcome-driven developers may contribute positively to maintaining team balance and performance.","['Sherlock A. Licorish', 'Stephen G. MacDonell']","Proceedings of the 25th Australasian Conference on Information Systems (ACIS2014). Auckland, New Zealand, AIS, pp.1-10",arXiv,2021,https://doi.org/10.48550/arXiv.2102.06318,Anomali
Information Extraction From Co-Occurring Similar Entities,"Knowledge about entities and their interrelations is a crucial factor of success for tasks like question answering ortextsummarization. Publicly available knowledge graphs like Wikidata or DBpedia are, however, far from being complete. In this paper, we explore how information extracted from similar entities that co-occur in structures like tables or lists can help to increase the coverage of such knowledge graphs. In contrast to existing approaches, we do not focus on relationships within a listing (e.g., between two entities in a table row) but on the relationship between a listing's subject entities and the context of the listing. To that end, we propose a descriptive ruleminingapproach that uses distant supervision to derive rules for these relationships based on a listing's context. Extracted from a suitable data corpus, the rules can be used to extend a knowledge graph with novel entities and assertions. In our experiments we demonstrate that the approach is able to extract up to 3M novel entities and 30M additional assertions from listings in Wikipedia. We find that the extracted information is of high quality and thus suitable to extend Wikipedia-based knowledge graphs like DBpedia, YAGO, and CaLiGraph. For the case of DBpedia, this would result in an increase of covered entities by roughly 50%.","['Nicolas Heist', 'Heiko Paulheim']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.05444,Anomali
Driver2vec: Driver Identification from Automotive Data,"With increasing focus on privacy protection, alternative methods to identify vehicle operator without the use of biometric identifiers have gained traction for automotive data analysis. The wide variety of sensors installed on modern vehicles enable autonomous driving, reduce accidents and improve vehicle handling. On the other hand, the data these sensors collect reflect drivers' habit. Drivers' use of turn indicators, following distance, rate of acceleration, etc. can be transformed to an embedding that is representative of their behavior and identity. In this paper, we develop a deep learning architecture (Driver2vec) to map a short interval of driving data into an embedding space that represents the driver's behavior to assist in driver identification. We develop a custom model that leverages performance gains of temporal convolutional networks, embedding separation power of triplet loss and classification accuracy of gradient boosting decision trees. Trained on a dataset of 51 drivers provided by Nervtech, Driver2vec is able to accurately identify the driver from a short 10-second interval of sensor data, achieving an average pairwise driver identification accuracy of 83.1% from this 10-second interval, which is remarkably higher than performance obtained in previous studies. We then analyzed performance of Driver2vec to show that its performance is consistent across scenarios and that modeling choices are sound.","['Jingbo Yang', 'Ruge Zhao', 'Meixian Zhu', 'David Hallac', 'Jaka Sodnik', 'Jure Leskovec']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.05234,Anomali
A Text Mining Discovery of Similarities and Dissimilarities Among Sacred Scriptures,"The careful examination of sacredtextsgives valuable insights into human psychology, different ideas regarding the organization of societies as well as into terms like truth and God. To improve and deepen our understanding of sacredtexts, their comparison, and their separation is crucial. For this purpose, we use our data set has nine sacred scriptures. This work deals with the separation of the Quran, the Asian scriptures Tao-Te-Ching, the Buddhism, the Yogasutras, and the Upanishads as well as the four books from the Bible, namely the Book of Proverbs, the Book of Ecclesiastes, the Book of Ecclesiasticus, and the Book of Wisdom. These scriptures are analyzed based on the natural language processing NLP creating the mathematical representation of the corpus in terms of frequencies called document term matrix (DTM). After this analysis, machine learning methods like supervised and unsupervised learning are applied to perform classification. Here we use the Multinomial Naive Bayes (MNB), the Super Vector Machine (SVM), the Random Forest (RF), and the K-nearest Neighbors (KNN). We obtain that among these methods MNB is able to predict the class of a sacredtextwith an accuracy of about 85.84 %.","['Younous Mofenjou Peuriekeu', 'Victoire Djimna Noyum', 'Cyrille Feudjio', 'Alkan Goktug', 'Ernest Fokoue']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.04421,Anomali
Points2Vec: Unsupervised Object-level Feature Learning from Point Clouds,"Unsupervised representation learning techniques, such as learning word embeddings, have had a significant impact on the field of natural language processing. Similar representation learning techniques have not yet become commonplace in the context of 3D vision. This, despite the fact that the physical 3D spaces have a similar semantic structure to bodies oftext: words are surrounded by words that are semantically related, just like objects are surrounded by other objects that are similar in concept and usage.
  In this work, we exploit this structure in learning semantically meaningful low dimensional vector representations of objects. We learn these vector representations bymininga dataset of scanned 3D spaces using an unsupervised algorithm. We represent objects as point clouds, a flexible and general representation for 3D data, which we encode into a vector representation. We show that using our method to include context increases the ability of a clustering algorithm to distinguish different semantic classes from each other. Furthermore, we show that our algorithm produces continuous and meaningful object embeddings through interpolation experiments.","['Joël Bachmann', 'Kenneth Blomqvist', 'Julian Förster', 'Roland Siegwart']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.04136,Anomali
Using Visual Text Mining to Support the Study Selection Activity in Systematic Literature Reviews,"Background: A systematic literature review (SLR) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process used to select primary studies can be arduous, time consuming, and must often be conducted manually. Objective: We propose a novel approach, known as 'Systematic Literature Review based on VisualTextMining' or simply SLR-VTM, to support the primary study selection activity using visualtextmining(VTM) techniques. Method: We conducted a case study to compare the performance and effectiveness of four doctoral students in selecting primary studies manually and using the SLR-VTM approach. To enable the comparison, we also developed a VTM tool that implemented our approach. We hypothesized that students using SLR-VTM would present improved selection performance and effectiveness. Results: Our results show that incorporating VTM in the SLR study selection activity reduced the time spent in this activity and also increased the number of studies correctly included. Conclusions: Our pilot case study presents promising results suggesting that the use of VTM may indeed be beneficial during the study selection activity when performing an SLR.","['Katia Romero Felizardo', 'Norsaremah Salleh', 'Rafael M. Martins', 'Emília Mendes', 'Stephen G. MacDonell', 'José Carlos Maldonado']",Proceedings of the 5th International Symposium on Empirical Software Engineering and Measurement (ESEM2011),arXiv,2021,https://doi.org/10.48550/arXiv.2102.02934,Anomali
"HMC, an Algorithms in Data Mining, the Functional Analysis approach","The main purpose of this paper is to facilitate the communication between the Analytic, Probabilistic and Algorithmic communities.
  We present a proof of convergence of the Hamiltonian (Hybrid) Monte Carlo algorithm from the point of view of the
  Dynamical Systems, where the evolving objects are densities of probability distributions and the tool are derived from the Functional Analysis.","['Soumyadip Ghosh', 'Yingdong Lu', 'Tomasz Nowicki']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.02691,Anomali
Commonsense Knowledge Mining from Term Definitions,"Commonsense knowledge has proven to be beneficial to a variety of application areas, including question answering and natural language understanding. Previous work explored collecting commonsense knowledge triples automatically fromtextto increase the coverage of current commonsense knowledge graphs. We investigate a few machine learning approaches tominingcommonsense knowledge triples using dictionary term definitions as inputs and provide some initial evaluation of the results. We start from extracting candidate triples using part-of-speech tag patterns fromtext, and then compare the performance of three existing models for triple scoring. Our experiments show that term definitions contain some valid and novel commonsense knowledge triples for some semantic relations, and also indicate some challenges with using existing triple scoring models.","['Zhicheng Liang', 'Deborah L. McGuinness']",,arXiv,2021,https://doi.org/10.48550/arXiv.2102.00651,Anomali
Monolithic Thulium Fiber Laser with 567 W output power at 1970 nm,"We report on a monolithic thulium fiber laser with 567 W output power at 1970 nm, which is the highest power reported so far directly from a thulium oscillator. This is achieved by optimization of the splice parameters for the active fiber (minimizing signal light in the fiber cladding) and direct water cooling. Dual transverse mode operation is visible from the optical spectrum and can also be deduced from the measured beam quality of M^2 = 2.6. \c{opyright} 2016 Optical Society of America under Open Access Publishing Agreement. Users may use, reuse, and build upon the article, or use the article fortextor datamining, so long as such uses are for non-commercial purposes and appropriate attribution is maintained. All other rights are reserved.","['Till Walbaum', 'Matthias Heinzig', 'Thomas Schreiber', 'Ramona Eberhardt', 'Andreas Tünnermann']","Opt. Lett. 41, 2632-2635 (2016)",arXiv,2021,https://doi.org/10.48550/arXiv.2101.12587,Anomali
The Arabic Citation Index -- Toward a better understanding of Arab scientific literature,"The Arabic Citation Index (ARCI) was launched in 2020. This article provides an overview of the scientific literature contained in this new database and explores its possible usage in research evaluation. As of May 2022, ARCI had indexed 138,283 scientific publications published between 2015 and 2020. ARCI's coverage is characterised by using the metadata available in scientific publications. First, I investigate the distributions of the indexed literature at various levels (research domains, countries, languages, open access). Articles make up nearly all the documents indexed with a share of 99% of ARCI. The Arts & Humanities and Social Sciences fields have the highest concentration of publications. Most indexed journals are published in Egypt, Algeria, Iraq, Jordan, and Saudi Arabia. About 8% of publications in ARCI are published in languages other than Arabic. Second, I use an unsupervised machine learning model, LDA (Latent Dirichlet Allocation), and thetextminingalgorithm of VOSviewer to uncover the main topics in ARCI. These methods provide a better understanding of ARCI's thematic structure. Next, I discuss how ARCI can complement global standards in the context of a more inclusive research evaluation. Finally, I suggest a few research opportunities after discussing the findings of this study.",['Jamal El-Ouahi'],Quantitative Science Studies-2023,arXiv,2023,https://doi.org/10.48550/arXiv.2101.12177,Anomali
LESA: Linguistic Encapsulation and Semantic Amalgamation Based Generalised Claim Detection from Online Content,"The conceptualization of a claim lies at the core of argumentmining. The segregation of claims is complex, owing to the divergence in textual syntax and context across different distributions. Another pressing issue is the unavailability of labeled unstructuredtextfor experimentation. In this paper, we propose LESA, a framework which aims at advancing headfirst into expunging the former issue by assembling a source-independent generalized model that captures syntactic features through part-of-speech and dependency embeddings, as well as contextual features through a fine-tuned language model. We resolve the latter issue by annotating a Twitter dataset which aims at providing a testing ground on a large unstructured dataset. Experimental results show that LESA improves upon the state-of-the-art performance across six benchmark claim datasets by an average of 3 claim-F1 points for in-domain experiments and by 2 claim-F1 points for general-domain experiments. On our dataset too, LESA outperforms existing baselines by 1 claim-F1 point on the in-domain experiments and 2 claim-F1 points on the general-domain experiments. We also release comprehensive data annotation guidelines compiled during the annotation phase (which was missing in the current literature).","['Shreya Gupta', 'Parantak Singh', 'Megha Sundriyal', 'Md Shad Akhtar', 'Tanmoy Chakraborty']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.11891,Anomali
SkillNER: Mining and Mapping Soft Skills from any Text,"In today's digital world, there is an increasing focus on soft skills. On the one hand, they facilitate innovation at companies, but on the other, they are unlikely to be automated soon. Researchers struggle with accurately approaching quantitatively the study of soft skills due to the lack of data-driven methods to retrieve them. This limits the possibility for psychologists and HR managers to understand the relation between humans and digitalisation. This paper presents SkillNER, a novel data-driven method for automatically extracting soft skills fromtext. It is a named entity recognition (NER) system trained with a support vector machine (SVM) on a corpus of more than 5000 scientific papers. We developed this system by measuring the performance of our approach against different training models and validating the results together with a team of psychologists. Finally, SkillNER was tested in a real-world case study using the job descriptions of ESCO (European Skill/Competence Qualification and Occupation) as textual source. The system enabled the detection of communities of job profiles based on their shared soft skills and communities of soft skills based on their shared job profiles. This case study demonstrates that the tool can automatically retrieve soft skills from a large corpus in an efficient way, proving useful for firms, institutions, and workers. The tool is open and available online to foster quantitative methods for the study of soft skills.","['Silvia Fareri', 'Nicola Melluso', 'Filippo Chiarello', 'Gualtiero Fantoni']",Expert Systems With Applications 184 (2021) 115544,arXiv,2021,https://doi.org/10.48550/arXiv.2101.11431,Anomali
Inheritance-guided Hierarchical Assignment for Clinical Automatic Diagnosis,"Clinical diagnosis, which aims to assign diagnosis codes for a patient based on the clinical note, plays an essential role in clinical decision-making. Considering that manual diagnosis could be error-prone and time-consuming, many intelligent approaches based on clinicaltextmininghave been proposed to perform automatic diagnosis. However, these methods may not achieve satisfactory results due to the following challenges. First, most of the diagnosis codes are rare, and the distribution is extremely unbalanced. Second, existing methods are challenging to capture the correlation between diagnosis codes. Third, the lengthy clinical note leads to the excessive dispersion of key information related to codes. To tackle these challenges, we propose a novel framework to combine the inheritance-guided hierarchical assignment and co-occurrence graph propagation for clinical automatic diagnosis. Specifically, we propose a hierarchical joint prediction strategy to address the challenge of unbalanced codes distribution. Then, we utilize graph convolutional neural networks to obtain the correlation and semantic representations of medical ontology. Furthermore, we introduce multi attention mechanisms to extract crucial information. Finally, extensive experiments on MIMIC-III dataset clearly validate the effectiveness of our method.","['Yichao Du', 'Pengfei Luo', 'Xudong Hong', 'Tong Xu', 'Zhe Zhang', 'Chao Ren', 'Yi Zheng', 'Enhong Chen']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.11374,Anomali
Graphonomy: Universal Image Parsing via Graph Reasoning and Transfer,"Prior highly-tuned image parsing models are usually studied in a certain domain with a specific set of semantic labels and can hardly be adapted into other scenarios (e.g., sharing discrepant label granularity) without extensive re-training. Learning a single universal parsing model by unifying label annotations from different domains or at various levels of granularity is a crucial but rarely addressed topic. This poses many fundamental learning challenges, e.g., discovering underlying semantic structures among different label granularity ormininglabel correlation across relevant tasks. To address these challenges, we propose a graph reasoning and transfer learning framework, named ""Graphonomy"", which incorporates human knowledge and label taxonomy into the intermediate graph representation learning beyond local convolutions. In particular, Graphonomy learns the global and structured semantic coherency in multiple domains via semantic-aware graph reasoning and transfer, enforcing the mutual benefits of the parsing across domains (e.g., different datasets or co-related tasks). The Graphonomy includes two iterated modules: Intra-Graph Reasoning and Inter-Graph Transfer modules. The former extracts the semantic graph in each domain to improve the feature representation learning by propagating information with the graph; the latter exploits the dependencies among the graphs from different domains for bidirectional knowledge transfer. We apply Graphonomy to two relevant but different image understanding research topics: human parsing and panoptic segmentation, and show Graphonomy can handle both of them well via a standard pipeline against current state-of-the-art approaches. Moreover, some extra benefit of our framework is demonstrated, e.g., generating the human parsing at various levels of granularity by unifying annotations across different datasets.","['Liang Lin', 'Yiming Gao', 'Ke Gong', 'Meng Wang', 'Xiaodan Liang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.10620,Anomali
pdfPapers: shell-script utilities for frequency-based multi-word phrase extraction from PDF documents,"Biomedical research is intensive in processing information in the previously published papers. This motivated a lot of efforts to provide tools fortextminingand information extraction from PDF documents over the past decade. The *nix (Unix/Linux) operating systems offer many tools for working withtextfiles, however, very few such tools are available for processing the contents of PDF files. This paper reports our effort to develop shell script utilities for *nix systems with the core functionality focused on viewing and searching multiple PDF documents combining logical and regular expressions, and enabling more reliabletextextraction from PDF documents with subsequent manipulation of the resulting blocks oftext. Furthermore, a procedure for extracting the most frequently occurring multi-word phrases was devised and then demonstrated on several scientific papers in life sciences. Our experiments revealed that the procedure is surprisingly robust to deficiencies intextextraction and the actual scoring function used to rank the phrases in terms of their importance or relevance. The keyword relevance is strongly context dependent, the word stemming did not provide any recognizable advantage, and the stop-words should only be removed from the beginning and the end of phrases. In addition, the developed utilities were used to convert the list of acronyms and the index from a PDF e-book into a large list of biochemical terms which can be exploited in othertextminingtasks. All shell scripts and data files are available in a public repository named \pp\ on the Github. The key lesson learned in this work is that semi-automated methods combining the power of algorithms with the capabilities of research experience are the most promising for improving the research efficiency.",['Pavel Loskot'],,arXiv,2021,https://doi.org/10.48550/arXiv.2101.10554,Anomali
Randomized Deep Structured Prediction for Discourse-Level Processing,"Expressivetextencoders such as RNNs and Transformer Networks have been at the center of NLP models in recent work. Most of the effort has focused on sentence-level tasks, capturing the dependencies between words in a single sentence, or pairs of sentences. However, certain tasks, such as argumentationmining, require accounting for longertextsand complicated structural dependencies between them. Deep structured prediction is a general framework to combine the complementary strengths of expressive neural encoders and structured inference for highly structured domains. Nevertheless, when the need arises to go beyond sentences, most work relies on combining the output scores of independently trained classifiers. One of the main reasons for this is that constrained inference comes at a high computational cost. In this paper, we explore the use of randomized inference to alleviate this concern and show that we can efficiently leverage deep structured prediction and expressive neural encoders for a set of tasks involving complicated argumentative structures.","['Manuel Widmoser', 'Maria Leonor Pacheco', 'Jean Honorio', 'Dan Goldwasser']","Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2021",arXiv,2021,https://doi.org/10.48550/arXiv.2101.10435,Anomali
Writers Gonna Wait: The Effectiveness of Notifications to Initiate Aversive Action in Writing Procrastination,"This paper evaluates the use of notifications to reduce aversive-task-procrastination by helping initiate action. Specifically, we focus on aversion to graded writing tasks. We evaluate software designs commonly used by behavior change applications, such as goal setting and action support systems. We conduct a two-phase control trial experiment with 21 college students tasked to write two 3000-word writing assignments (14 students fully completed the experiment). Participants use a customizedtexteditor designed to continuously collect writing behavior. The results from the study reveal that notifications have minimal effect in encouraging users to get started. They can also increase negative effects on participants. Other techniques, such as eliminating distraction and showing simple writing statistics, yield higher satisfaction among participants as they complete the writing task. Furthermore, the incorporation oftextminingdecreases aversion to the task and helps participants overcome writer's block. Finally, we discuss lessons learned from our evaluation that help quantify the difficulty of behavior change for writing procrastination, with emphasis on goals for the HCI community.","['Chatchai Wangwiwattana', 'Sunjoli Aggarwal', 'Eric C. Larson']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.10191,Anomali
EGFI: Drug-Drug Interaction Extraction and Generation with Fusion of Enriched Entity and Sentence Information,"The rapid growth in literature accumulates diverse and yet comprehensive biomedical knowledge hidden to beminedsuch as drug interactions. However, it is difficult to extract the heterogeneous knowledge to retrieve or even discover the latest and novel knowledge in an efficient manner. To address such a problem, we propose EGFI for extracting and consolidating drug interactions from large-scale medical literaturetextdata. Specifically, EGFI consists of two parts: classification and generation. In the classification part, EGFI encompasses the language model BioBERT which has been comprehensively pre-trained on biomedical corpus. In particular, we propose the multi-head attention mechanism and pack BiGRU to fuse multiple semantic information for rigorous context modeling. In the generation part, EGFI utilizes another pre-trained language model BioGPT-2 where the generation sentences are selected based on filtering rules. We evaluated the classification part on ""DDIs 2013"" dataset and ""DTIs"" dataset, achieving the FI score of 0.842 and 0.720 respectively. Moreover, we applied the classification part to distinguish high-quality generated sentences and verified with the exiting growth truth to confirm the filtered sentences. The generated sentences that are not recorded in DrugBank and DDIs 2013 dataset also demonstrate the potential of EGFI to identify novel drug relationships.","['Lei Huang', 'Jiecong Lin', 'Xiangtao Li', 'Linqi Song', 'Ka-Chun Wong']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.09914,Anomali
Drug and Disease Interpretation Learning with Biomedical Entity Representation Transformer,"Concept normalization in free-formtextsis a crucial step in everytext-miningpipeline. Neural architectures based on Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art results in the biomedical domain. In the context of drug discovery and development, clinical trials are necessary to establish the efficacy and safety of drugs. We investigate the effectiveness of transferring concept normalization from the general biomedical domain to the clinical trials domain in a zero-shot setting with an absence of labeled data. We propose a simple and effective two-stage neural approach based on fine-tuned BERT architectures. In the first stage, we train a metric learning model that optimizes relative similarity of mentions and concepts via triplet loss. The model is trained on available labeled corpora of scientific abstracts to obtain vector embeddings of concept names and entity mentions fromtexts. In the second stage, we find the closest concept name representation in an embedding space to a given clinical mention. We evaluated several models, including state-of-the-art architectures, on a dataset of abstracts and a real-world dataset of trial records with interventions and conditions mapped to drug and disease terminologies. Extensive experiments validate the effectiveness of our approach in knowledge transfer from the scientific literature to clinical trials.","['Zulfat Miftahutdinov', 'Artur Kadurin', 'Roman Kudrin', 'Elena Tutubalina']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.09311,Anomali
LonelyText: A Short Messaging Based Classification of Loneliness,Loneliness does not only have emotional implications on a person but also on his/her well-being. The study of loneliness has been challenging and largely inconclusive in findings because of the several factors that might correlate to the phenomenon. We present one approach to predicting this event by discovering patterns of language associated with loneliness. Our results show insights and promising directions forminingtextfrom instant messaging to predict loneliness.,"['Mawulolo K. Ameko', 'Sonia Baee', 'Laura E. Barnes']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.09138,Anomali
Knowledge Graph for Microdata of Statistics Netherlands,"Statistics Netherlands (CBS) hosted a huge amount of data not only on the statistical level but also on the individual level. With the development of data science technologies, more and more researchers request to conduct their research by using high-quality individual data from CBS (called CBS Microdata) or combining them with other data sources. Making great use of these data for research and scientific purposes can tremendously benefit the whole society. However, CBS Microdata has been collected and maintained in different ways by different departments in and out of CBS. The representation, quality, metadata of datasets are not sufficiently harmonized. The project converts the descriptions of all CBS microdata sets into one knowledge graph with comprehensive metadata in Dutch and English usingtextminingand semantic web technologies. Researchers can easily query the metadata, explore the relations among multiple datasets, and find the needed variables. For example, if a researcher searches a dataset about ""Age at Death"" in the Health and Well-being category, all information related to this dataset will appear including keywords and variable names. ""Age at Death"" dataset has a keyword - ""Death"". This keyword will lead to other datasets such as ""Date of Death"". ""Cause of Death"", ""Production statistics Health and welfare"" from Population, Business categories, and Health and well-being categories. This will tremendously save time and costs for the data requester but also data maintainers.",['Chang Sun'],,arXiv,2021,https://doi.org/10.48550/arXiv.2101.07622,Anomali
Profiling Software Developers with Process Mining and N-Gram Language Models,"Context: Profiling developers is challenging since many factors, such as their skills, experience, development environment and behaviors, may influence a detailed analysis and the delivery of coherent interpretations.
  Objective: We aim at profiling software developers byminingtheir software development process. To do so, we performed a controlled experiment where, in the realm of a Python programming contest, a group of developers had the same well-defined set of requirements specifications and a well-defined sprint schedule. Events were collected from the PyCharm IDE, and from the Mooshak automatic jury where subjects checked-in their code.
  Method: We used n-gram language models andtextminingto characterize developers' profiles, and processminingalgorithms to discover their overall workflows and extract the correspondent metrics for further evaluation.
  Results: Findings show that we can clearly characterize with a coherent rationale most developers, and distinguish the top performers from the ones with more challenging behaviors. This approach may lead ultimately to the creation of a catalog of software development process smells.
  Conclusions: The profile of a developer provides a software project manager a clue for the selection of appropriate tasks he/she should be assigned. With the increasing usage of low and no-code platforms, where coding is automatically generated from an upper abstraction layer,miningdeveloper's actions in the development platforms is a promising approach to early detect not only behaviors but also assess project complexity and model effort.","['João Caldeira', 'Fernando Brito e Abreu', 'Jorge Cardoso', 'Ricardo Ribeiro', 'Claudia Werner']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.06733,Anomali
"Big Data Generated by Connected and Automated Vehicles for Safety Monitoring, Assessment and Improvement, Final Report (Year 3)","This report focuses on safety aspects of connected and automated vehicles (CAVs). The fundamental question to be answered is how can CAVs improve road users' safety? Using advanced dataminingand thematictextanalytics tools, the goal is to systematically synthesize studies related to Big Data for safety monitoring and improvement. Within this domain, the report systematically compares Big Data initiatives related to transportation initiatives nationally and internationally and provides insights regarding the evolution of Big Data science applications related to CAVs and new challenges. The objectives addressed are: 1-Creating a database of Big Data efforts by acquiring reports, white papers, and journal publications; 2-Applyingtextanalytics tools to extract key concepts, and spot patterns and trends in Big Data initiatives; 3-Understanding the evolution of CAV Big Data in the context of safety by quantifying granular taxonomies and modeling entity relations among contents in CAV Big Data research initiatives, and 4-Developing a foundation for exploring new approaches to tracking and analyzing CAV Big Data and related innovations. The study synthesizes and derives high-quality information from innovative research activities undertaken by various research entities through Big Data initiatives. The results can provide a conceptual foundation for developing new approaches for guiding and tracking the safety implications of Big Data and related innovations.","['Asad J. Khattak', 'Iman Mahdinia', 'Sevin Mohammadi', 'Amin Mohammadnazar', 'Behram Wali']",STC-2019-M4.UTK,arXiv,2021,https://doi.org/10.48550/arXiv.2101.06106,Anomali
Mining Data Impressions from Deep Models as Substitute for the Unavailable Training Data,"Pretrained deep models hold their learnt knowledge in the form of model parameters. These parameters act as ""memory"" for the trained models and help them generalize well on unseen data. However, in absence of training data, the utility of a trained model is merely limited to either inference or better initialization towards a target task. In this paper, we go further and extract synthetic data by leveraging the learnt model parameters. We dub them ""Data Impressions"", which act as proxy to the training data and can be used to realize a variety of tasks. These are useful in scenarios where only the pretrained models are available and the training data is not shared (e.g., due to privacy or sensitivity concerns). We show the applicability of data impressions in solving several computer vision tasks such as unsupervised domain adaptation, continual learning as well as knowledge distillation. We also study the adversarial robustness of lightweight models trained via knowledge distillation using these data impressions. Further, we demonstrate the efficacy of data impressions in generating data-free Universal Adversarial Perturbations (UAPs) with better fooling rates. Extensive experiments performed on benchmark datasets demonstrate competitive performance achieved using data impressions in absence of original training data.","['Gaurav Kumar Nayak', 'Konda Reddy Mopuri', 'Saksham Jain', 'Anirban Chakraborty']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.06069,Anomali
SoftNER: Mining Knowledge Graphs From Cloud Incidents,"The move from boxed products to services and the widespread adoption of cloud computing has had a huge impact on the software development life cycle and DevOps processes. Particularly, incident management has become critical for developing and operating large-scale services. Prior work on incident management has heavily focused on the challenges with incident triaging and de-duplication. In this work, we address the fundamental problem of structured knowledge extraction from service incidents. We have built SoftNER, a framework forminingKnowledge Graphs from incident reports. First, we build a novel multi-task learning based BiLSTM-CRF model which leverages not just the semantic context but also the data-types for extracting factual information in the form of named entities. Next, we present an approach tominerelations between the named entities for automatically constructing knowledge graphs. We have deployed SoftNER at Microsoft, a major cloud service provider and have evaluated it on more than 2 months of cloud incidents. We show that the unsupervised machine learning pipeline has a high precision of 0.96. Our multi-task learning based deep learning model also outperforms the state-of-the-art NER models. Lastly, using the knowledge extracted by SoftNER, we are able to build accurate models for applications such as incident triaging and recommending entities based on their relevance to incident titles.","['Manish Shetty', 'Chetan Bansal', 'Sumit Kumar', 'Nikitha Rao', 'Nachiappan Nagappan']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.05961,Anomali
Transformer-based Language Model Fine-tuning Methods for COVID-19 Fake News Detection,"With the pandemic of COVID-19, relevant fake news is spreading all over the sky throughout the social media. Believing in them without discrimination can cause great trouble to people's life. However, universal language models may perform weakly in these fake news detection for lack of large-scale annotated data and sufficient semantic understanding of domain-specific knowledge. While the model trained on corresponding corpora is also mediocre for insufficient learning. In this paper, we propose a novel transformer-based language model fine-tuning approach for these fake news detection. First, the token vocabulary of individual model is expanded for the actual semantics of professional phrases. Second, we adapt the heated-up softmax loss to distinguish the hard-miningsamples, which are common for fake news because of the disambiguation of shorttext. Then, we involve adversarial training to improve the model's robustness. Last, the predicted features extracted by universal language model RoBERTa and domain-specific model CT-BERT are fused by one multiple layer perception to integrate fine-grained and high-level specific representations. Quantitative experimental results evaluated on existing COVID-19 fake news dataset show its superior performances compared to the state-of-the-art methods among various evaluation metrics. Furthermore, the best weighted average F1 score achieves 99.02%.","['Ben Chen', 'Bin Chen', 'Dehong Gao', 'Qijin Chen', 'Chengfu Huo', 'Xiaonan Meng', 'Weijun Ren', 'Yang Zhou']",,arXiv,2023,https://doi.org/10.48550/arXiv.2101.05509,Anomali
BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph Transformer,"A biomedical relation statement is commonly expressed in multiple sentences and consists of many concepts, including gene, disease, chemical, and mutation. To automatically extract information from biomedical literature, existing biomedicaltext-miningapproaches typically formulate the problem as a cross-sentence n-ary relation-extraction task that detects relations among n entities across multiple sentences, and use either a graph neural network (GNN) with long short-term memory (LSTM) or an attention mechanism. Recently, Transformer has been shown to outperform LSTM on many natural language processing (NLP) tasks. In this work, we propose a novel architecture that combines Bidirectional Encoder Representations from Transformers with Graph Transformer (BERT-GT), through integrating a neighbor-attention mechanism into the BERT architecture. Unlike the original Transformer architecture, which utilizes the whole sentence(s) to calculate the attention of the current token, the neighbor-attention mechanism in our method calculates its attention utilizing only its neighbor tokens. Thus, each token can pay attention to its neighbor information with little noise. We show that this is critically important when thetextis very long, as in cross-sentence or abstract-level relation-extraction tasks. Our benchmarking results show improvements of 5.44% and 3.89% in accuracy and F1-measure over the state-of-the-art on n-ary and chemical-protein relation datasets, suggesting BERT-GT is a robust approach that is applicable to other biomedical relation extraction tasks or datasets.","['Po-Ting Lai', 'Zhiyong Lu']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.04158,Anomali
Evaluating Deep Learning Approaches for Covid19 Fake News Detection,"Social media platforms like Facebook, Twitter, and Instagram have enabled connection and communication on a large scale. It has revolutionized the rate at which information is shared and enhanced its reach. However, another side of the coin dictates an alarming story. These platforms have led to an increase in the creation and spread of fake news. The fake news has not only influenced people in the wrong direction but also claimed human lives. During these critical times of the Covid19 pandemic, it is easy to mislead people and make them believe in fatal information. Therefore it is important to curb fake news at source and prevent it from spreading to a larger audience. We look at automated techniques for fake news detection from a dataminingperspective. We evaluate different supervisedtextclassification algorithms on Contraint@AAAI 2021 Covid-19 Fake news detection dataset. The classification algorithms are based on Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM), and Bidirectional Encoder Representations from Transformers (BERT). We also evaluate the importance of unsupervised learning in the form of language model pre-training and distributed word representations using unlabelled covid tweets corpus. We report the best accuracy of 98.41\% on the Covid-19 Fake news detection dataset.","['Apurva Wani', 'Isha Joshi', 'Snehal Khandve', 'Vedangi Wagh', 'Raviraj Joshi']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.04012,Anomali
Knowledge AI: New Medical AI Solution for Medical image Diagnosis,"The implementation of medical AI has always been a problem. The effect of traditional perceptual AI algorithm in medical image processing needs to be improved. Here we propose a method of knowledge AI, which is a combination of perceptual AI and clinical knowledge and experience. Based on this method, the geometric informationminingof medical images can represent the experience and information and evaluate the quality of medical images.","['Yingni Wang', 'Shuge Lei', 'Jian Dai', 'Kehong Yuan']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.03063,Anomali
Studying Leaders & Their Concerns Using Online Social Media During The Times Of Crisis -- A COVID Case Study,"Online social media (OSM) has emerged as a prominent platform for debate on a wide range of issues. Even celebrities and public figures often share their opinions on a variety of topics through OSM platforms. One such subject that has gained a lot of coverage on Twitter is the Novel Coronavirus, officially known as COVID-19, which has become a pandemic and has sparked a crisis in human history. In this study, we examine 29 million tweets over three months to study highly influential users, whom we refer to as leaders. We recognize these leaders through social network techniques and analyze their tweets usingtextanalysis. Using a community detection algorithm, we categorize these leaders into four clusters: research, news, health, and politics, with each cluster containing Twitter handles (accounts) of individual users or organizations. E.g., the health cluster includes the World Health Organization (@WHO), the Director-General of WHO (@DrTedros), and so on. The emotion analysis reveals that (i) all clusters show an equal amount of fear in their tweets, (ii) research and news clusters display more sadness than others, and (iii) health and politics clusters are attempting to win public trust. According to thetextanalysis, the (i) research cluster is more concerned with recognizing symptoms and the development of vaccination; (ii) news and politics clusters are mostly concerned with travel. We then show that we can use our findings to classify tweets into clusters with a score of 96% AUC ROC.","['Rahul Goel', 'Rajesh Sharma']",Social Network Analysis and Mining 11.1 (2021): 1-12,arXiv,2021,https://doi.org/10.48550/arXiv.2101.03002,Anomali
A Framework for Deep Constrained Clustering,"The area of constrained clustering has been extensively explored by researchers and used by practitioners. Constrained clustering formulations exist for popular algorithms such as k-means, mixture models, and spectral clustering but have several limitations. A fundamental strength of deep learning is its flexibility, and here we explore a deep learning framework for constrained clustering and in particular explore how it can extend the field of constrained clustering. We show that our framework can not only handle standard together/apart constraints (without the well documented negative effects reported earlier) generated from labeled side information but more complex constraints generated from new types of side information such as continuous values and high-level domain knowledge. Furthermore, we propose an efficient training paradigm that is generally applicable to these four types of constraints. We validate the effectiveness of our approach by empirical results on both image andtextdatasets. We also study the robustness of our framework when learning with noisy constraints and show how different components of our framework contribute to the final performance. Our source code is available at$\href{https://github.com/blueocean92/deep_constrained_clustering}{\text{URL}}$.","['Hongjing Zhang', 'Tianyang Zhan', 'Sugato Basu', 'Ian Davidson']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.02792,Anomali
Mining the Relationship Between COVID-19 Sentiment and Market Performance,"At the beginning of the COVID-19 outbreak in March, we observed one of the largest stock market crashes in history. Within the months following this, a volatile bullish climb back to pre-pandemic performances and higher. In this paper, we study the stock market behavior during the initial few months of the COVID-19 pandemic in relation to COVID-19 sentiment. Usingtextsentiment analysis of Twitter data, we look at tweets that contain key words in relation to the COVID-19 pandemic and the sentiment of the tweet to understand whether sentiment can be used as an indicator for stock market performance. There has been previous research done on applying natural language processing andtextsentiment analysis to understand the stock market performance, given how prevalent the impact of COVID-19 is to the economy, we want to further the application of these techniques to understand the relationship that COVID-19 has with stock market performance. Our findings show that there is a strong relationship to COVID-19 sentiment derived from tweets that could be used to predict stock market performance in the future.","['Ziyuan Xia', 'Jeffery Chen', 'Anchen Sun']",,arXiv,2023,https://doi.org/10.48550/arXiv.2101.02587,Anomali
SuperMat: Construction of a linked annotated dataset from superconductors-related publications,"A growing number of papers are published in the area of superconducting materials science. However, noveltextand datamining(TDM) processes are still needed to efficiently access and exploit this accumulated knowledge, paving the way towards data-driven materials design. Herein, we present SuperMat (Superconductor Materials), an annotated corpus of linked data derived from scientific publications on superconductors, which comprises 142 articles, 16052 entities, and 1398 links that are characterised into six categories: the names, classes, and properties of materials; links to their respective superconducting critical temperature (Tc); and parametric conditions such as applied pressure or measurement methods. The construction of SuperMat resulted from a fruitful collaboration between computer scientists and material scientists, and its high quality is ensured through validation by domain experts. The quality of the annotation guidelines was ensured by satisfactory Inter Annotator Agreement (IAA) between the annotators and the domain experts. SuperMat includes the dataset, annotation guidelines, and annotation support tools that use automatic suggestions to help minimise human errors.","['Luca Foppiano', 'Sae Dieb', 'Akira Suzuki', 'Pedro Baptista de Castro', 'Suguru Iwasaki', 'Azusa Uzuki', 'Miren Garbine Esparza Echevarria', 'Yan Meng', 'Kensei Terashima', 'Laurent Romary', 'Yoshihiko Takano', 'Masashi Ishii']","STAM:M, 2021, VOL. 1, NO. 1, 34-44",arXiv,2021,https://doi.org/10.48550/arXiv.2101.02455,Anomali
A multi-modal approach towards mining social media data during natural disasters -- a case study of Hurricane Irma,"Streaming social media provides a real-time glimpse of extreme weather impacts. However, the volume of streaming data makesmininginformation a challenge for emergency managers, policy makers, and disciplinary scientists. Here we explore the effectiveness of data learned approaches tomineand filter information from streaming social media data from Hurricane Irma's landfall in Florida, USA. We use 54,383 Twitter messages (out of 784K geolocated messages) from 16,598 users from Sept. 10 - 12, 2017 to develop 4 independent models to filter data for relevance: 1) a geospatial model based on forcing conditions at the place and time of each tweet, 2) an image classification model for tweets that include images, 3) a user model to predict the reliability of the tweeter, and 4) atextmodel to determine if thetextis related to Hurricane Irma. All four models are independently tested, and can be combined to quickly filter and visualize tweets based on user-defined thresholds for each submodel. We envision that this type of filtering and visualization routine can be useful as a base model for data capture from noisy sources such as Twitter. The data can then be subsequently used by policy makers, environmental managers, emergency managers, and domain scientists interested in finding tweets with specific attributes to use during different stages of the disaster (e.g., preparedness, response, and recovery), or for detailed research.","['Somya D. Mohanty', 'Brown Biggers', 'Saed Sayedahmed', 'Nastaran Pourebrahim', 'Evan B. Goldstein', 'Rick Bunch', 'Guangqing Chi', 'Fereidoon Sadri', 'Tom P. McCoy', 'Arthur Cosby']",,arXiv,2021,https://doi.org/10.48550/arXiv.2101.00480,Anomali
SelectScale: Mining More Patterns from Images via Selective and Soft Dropout,"Convolutional neural networks (CNNs) have achieved remarkable success in image recognition. Although the internal patterns of the input images are effectively learned by the CNNs, these patterns only constitute a small proportion of useful patterns contained in the input images. This can be attributed to the fact that the CNNs will stop learning if the learned patterns are enough to make a correct classification. Network regularization methods like dropout and SpatialDropout can ease this problem. During training, they randomly drop the features. These dropout methods, in essence, change the patterns learned by the networks, and in turn, forces the networks to learn other patterns to make the correct classification. However, the above methods have an important drawback. Randomly dropping features is generally inefficient and can introduce unnecessary noise. To tackle this problem, we propose SelectScale. Instead of randomly dropping units, SelectScale selects the important features in networks and adjusts them during training. Using SelectScale, we improve the performance of CNNs on CIFAR and ImageNet.","['Zhengsu Chen', 'Jianwei Niu', 'Xuefeng Liu', 'Shaojie Tang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.15766,Anomali
A fast algorithm of the shear-compression damage model for the simulation of block caving,"For undergroundmine, the current usual technique for ore extraction is block caving, which generates and induces seismic activity in themine. To understand block caving method is one of the most challenging problems in undergroundmining. This method relies on gravity to break and transport large amounts of ore and waste. The state of art in damage models is not able to represent the real effect of theminingin the rock mass since for example the damage appears in the bottom of the domain under consideration and with this is not possible recover the subsidence sees in themine. In this paper we present the analysis and implementation of the shear-compression damage model applied to undergroundminingproposed in (Bonnetier et al. 2020). We propose a fast algorithm based in the usual alternated algorithm used in gradient damage models (Marigo et al. 2016) and show that this new algorithm is faster than the usual algorithm. We show some numerical tests in 3D and present interested simulations for different damage laws producing realistic damage behavior.","['Sergio Gaete', 'Alejandro Jofre', 'Rodrigo Lecaros', 'Gino Montecinos', 'Jaime H. Ortega', 'Javier Ramírez-Ganga', 'Jorge San Martín']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.14776,Anomali
Vectorial dark dissipative solitons in Kerr resonators,We report the existence of vectorial dark dissipative solitons in optical cavities subject to a coherently injected beam. We assume that the resonator is operating in a normal dispersion regime far from any modulational instability. We show that the vectorial front locking mechanism allows for the stabilisation of dark dissipative structures. These structures differ by their temporal duration and their state of polarization. We characterize them by constructing their heteroclinic snaking bifurcation diagram showing evidence of multistability within a finite range of the control parameter.,"['B. Kostet', 'S. S. Gopalakrishnan', 'E. Averlant', 'Y. Soupart', 'K. Panajotov', 'M. Tlidi']","OSA Continuum Vol. 4, Issue 5, pp. 1564-1570 (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2012.13242,Anomali
On the Granularity of Explanations in Model Agnostic NLP Interpretability,"Current methods for Black-Box NLP interpretability, like LIME or SHAP, are based on altering thetextto interpret by removing words and modeling the Black-Box response. In this paper, we outline limitations of this approach when using complex BERT-based classifiers: The word-based sampling producestextsthat are out-of-distribution for the classifier and further gives rise to a high-dimensional search space, which can't be sufficiently explored when time or computation power is limited. Both of these challenges can be addressed by using segments as elementary building blocks for NLP interpretability. As illustration, we show that the simple choice of sentences greatly improves on both of these challenges. As a consequence, the resulting explainer attains much better fidelity on a benchmark classification task.","['Yves Rychener', 'Xavier Renard', 'Djamé Seddah', 'Pascal Frossard', 'Marcin Detyniecki']",,arXiv,2022,https://doi.org/10.48550/arXiv.2012.13189,Anomali
Medical Entity Linking using Triplet Network,"Entity linking (or Normalization) is an essential task intextminingthat maps the entity mentions in the medicaltextto standard entities in a given Knowledge Base (KB). This task is of great importance in the medical domain. It can also be used for merging different medical and clinical ontologies. In this paper, we center around the problem of disease linking or normalization. This task is executed in two phases: candidate generation and candidate scoring. In this paper, we present an approach to rank the candidate Knowledge Base entries based on their similarity with disease mention. We make use of the Triplet Network for candidate ranking. While the existing methods have used carefully generated sieves and external resources for candidate generation, we introduce a robust and portable candidate generation scheme that does not make use of the hand-crafted rules. Experimental results on the standard benchmark NCBI disease dataset demonstrate that our system outperforms the prior methods by a significant margin.","['Ishani Mondal', 'Sukannya Purkayastha', 'Sudeshna Sarkar', 'Pawan Goyal', 'Jitesh Pillai', 'Amitava Bhattacharyya', 'Mahanandeeshwar Gattu']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.11164,Anomali
Explaining Black-box Models for Biomedical Text Classification,"In this paper, we propose a novel method named Biomedical Confident Itemsets Explanation (BioCIE), aiming at post-hoc explanation of black-box machine learning models for biomedicaltextclassification. Using sources of domain knowledge and a confident itemsetminingmethod, BioCIE discretizes the decision space of a black-box into smaller subspaces and extracts semantic relationships between the inputtextand class labels in different subspaces. Confident itemsets discover how biomedical concepts are related to class labels in the black-box's decision space. BioCIE uses the itemsets to approximate the black-box's behavior for individual predictions. Optimizing fidelity, interpretability, and coverage measures, BioCIE produces class-wise explanations that represent decision boundaries of the black-box. Results of evaluations on various biomedicaltextclassification tasks and black-box models demonstrated that BioCIE can outperform perturbation-based and decision set methods in terms of producing concise, accurate, and interpretable explanations. BioCIE improved the fidelity of instance-wise and class-wise explanations by 11.6% and 7.5%, respectively. It also improved the interpretability of explanations by 8%. BioCIE can be effectively used to explain how a black-box biomedicaltextclassification model semantically relates inputtextsto class labels. The source code and supplementary material are available at https://github.com/mmoradi-iut/BioCIE.","['Milad Moradi', 'Matthias Samwald']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.10928,Anomali
A hybrid deep-learning approach for complex biochemical named entity recognition,"Named entity recognition (NER) of chemicals and drugs is a critical domain of information extraction in biochemical research. NER provides support fortextminingin biochemical reactions, including entity relation extraction, attribute extraction, and metabolic response relationship extraction. However, the existence of complex naming characteristics in the biomedical field, such as polysemy and special characters, make the NER task very challenging. Here, we propose a hybrid deep learning approach to improve the recognition accuracy of NER. Specifically, our approach applies the Bidirectional Encoder Representations from Transformers (BERT) model to extract the underlying features of thetext, learns a representation of the context of thetextthrough Bi-directional Long Short-Term Memory (BILSTM), and incorporates the multi-head attention (MHATT) mechanism to extract chapter-level features. In this approach, the MHATT mechanism aims to improve the recognition accuracy of abbreviations to efficiently deal with the problem of inconsistency in full-textlabels. Moreover, conditional random field (CRF) is used to label sequence tags because this probabilistic method does not need strict independence assumptions and can accommodate arbitrary context information. The experimental evaluation on a publicly-available dataset shows that the proposed hybrid approach achieves the best recognition performance; in particular, it substantially improves performance in recognizing abbreviations, polysemes, and low-frequency entities, compared with the state-of-the-art approaches. For instance, compared with the recognition accuracies for low-frequency entities produced by the BILSTM-CRF algorithm, those produced by the hybrid approach on two entity datasets (MULTIPLE and IDENTIFIER) have been increased by 80% and 21.69%, respectively.","['Jian Liu', 'Lei Gao', 'Sujie Guo', 'Rui Ding', 'Xin Huang', 'Long Ye', 'Qinghua Meng', 'Asef Nazari', 'Dhananjay Thiruvady']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.10824,Anomali
A First Look at Human Values-Violation in App Reviews,"Ubiquitous technologies such as mobile software applications (mobile apps) have a tremendous influence on the evolution of the social, cultural, economic, and political facets of life in society. Mobile apps fulfil many practical purposes for users including entertainment, transportation, financial management, etc. Given the ubiquity of mobile apps in the lives of individuals and the consequent effect of these technologies on society, it is essential to consider the relationship between human values and the development and deployment of mobile apps. The many negative consequences of violating human values such as privacy, fairness or social justice by technology have been documented in recent times. If we can detect these violations in a timely manner, developers can look to better address them. To understand the violation of human values in a range of common mobile apps, we analysed 22,119 app reviews from Google Play Store using natural language processing techniques. We base our values violation detection approach on a widely accepted model of human values; the Schwartz theory of basic human values. The results of our analysis show that 26.5% of the reviews containedtextindicating user perceived violations of human values. We found that benevolence and self-direction were the most violated value categories, and conformity and tradition were the least violated categories. Our results also highlight the need for a proactive approach to the alignment of values amongst stakeholders and the use of app reviews as a valuable additional source forminingvalues requirements.","['Humphrey O. Obie', 'Waqar Hussain', 'Xin Xia', 'John Grundy', 'Li Li', 'Burak Turhan', 'Jon Whittle', 'Mojtaba Shahin']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.10095,Anomali
"Graph integration of structured, semistructured and unstructured data for data journalism","Digital data is a goldminefor modern journalism. However, datasets which interest journalists are extremely heterogeneous, ranging from highly structured (relational databases), semi-structured (JSON, XML, HTML), graphs (e.g., RDF), andtext. Journalists (and other classes of users lacking advanced IT expertise, such as most non-governmental-organizations, or small public administrations) need to be able to make sense of such heterogeneous corpora, even if they lack the ability to define and deploy custom extract-transform-load workflows, especially for dynamically varying sets of data sources.
  We describe a complete approach for integrating dynamic sets of heterogeneous datasets along the lines described above: the challenges we faced to make such graphs useful, allow their integration to scale, and the solutions we proposed for these problems. Our approach is implemented within the ConnectionLens system; we validate it through a set of experiments.","['Angelos-Christos Anadiotis', 'Oana Balalau', 'Catarina Conceicao', 'Helena Galhardas', 'Mhd Yamen Haddad', 'Ioana Manolescu', 'Tayeb Merabti', 'Jingmao You']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.08830,Anomali
Discovering Airline-Specific Business Intelligence from Online Passenger Reviews: An Unsupervised Text Analytics Approach,"To understand the important dimensions of service quality from the passenger's perspective and tailor service offerings for competitive advantage, airlines can capitalize on the abundantly available online customer reviews (OCR). The objective of this paper is to discover company- and competitor-specific intelligence from OCR using an unsupervisedtextanalytics approach. First, the key aspects (or topics) discussed in the OCR are extracted using three topic models - probabilistic latent semantic analysis (pLSA) and two variants of Latent Dirichlet allocation (LDA-VI and LDA-GS). Subsequently, we propose an ensemble-assisted topic model (EA-TM), which integrates the individual topic models, to classify each review sentence to the most representative aspect. Likewise, to determine the sentiment corresponding to a review sentence, an ensemble sentiment analyzer (E-SA), which combines the predictions of three opinionminingmethods (AFINN, SentiStrength, and VADER), is developed. An aspect-based opinion summary (AOS), which provides a snapshot of passenger-perceived strengths and weaknesses of an airline, is established by consolidating the sentiments associated with each aspect. Furthermore, a bi-gram analysis of the labeled OCR is employed to perform root cause analysis within each identified aspect. A case study involving 99,147 airline reviews of a US-based target carrier and four of its competitors is used to validate the proposed approach. The results indicate that a cost- and time-effective performance summary of an airline and its competitors can be obtained from OCR. Finally, besides providing theoretical and managerial implications based on our results, we also provide implications for post-pandemic preparedness in the airline industry considering the unprecedented impact of coronavirus disease 2019 (COVID-19) and predictions on similar pandemics in the future.","['Sharan Srinivas', 'Surya Ramachandiran']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.08000,Anomali
A Practical Approach towards Causality Mining in Clinical Text using Active Transfer Learning,"Objective: Causalityminingis an active research area, which requires the application of state-of-the-art natural language processing techniques. In the healthcare domain, medical experts create clinicaltextto overcome the limitation of well-defined and schema driven information systems. The objective of this research work is to create a framework, which can convert clinicaltextinto causal knowledge. Methods: A practical approach based on term expansion, phrase generation, BERT based phrase embedding and semantic matching, semantic enrichment, expert verification, and model evolution has been used to construct a comprehensive causalityminingframework. This active transfer learning based framework along with its supplementary services, is able to extract and enrich, causal relationships and their corresponding entities from clinicaltext. Results: The multi-model transfer learning technique when applied over multiple iterations, gains performance improvements in terms of its accuracy and recall while keeping the precision constant. We also present a comparative analysis of the presented techniques with their common alternatives, which demonstrate the correctness of our approach and its ability to capture most causal relationships. Conclusion: The presented framework has provided cutting-edge results in the healthcare domain. However, the framework can be tweaked to provide causality detection in other domains, as well. Significance: The presented framework is generic enough to be utilized in any domain, healthcare services can gain massive benefits due to the voluminous and various nature of its data. This causal knowledge extraction framework can be used to summarize clinicaltext, create personas, discover medical knowledge, and provide evidence to clinical decision making.","['Musarrat Hussain', 'Fahad Ahmed Satti', 'Jamil Hussain', 'Taqdir Ali', 'Syed Imran Ali', 'Hafiz Syed Muhammad Bilal', 'Gwang Hoon Park', 'Sungyoung Lee']",Journal of Biomedical Informatics 123 (2021) 103932,arXiv,2020,https://doi.org/10.48550/arXiv.2012.07563,Anomali
Learning Contextual Causality from Time-consecutive Images,"Causality knowledge is crucial for many artificial intelligence systems. Conventional textual-based causality knowledge acquisition methods typically require laborious and expensive human annotations. As a result, their scale is often limited. Moreover, as no context is provided during the annotation, the resulting causality knowledge records (e.g., ConceptNet) typically do not take the context into consideration. To explore a more scalable way of acquiring causality knowledge, in this paper, we jump out of the textual domain and investigate the possibility of learning contextual causality from the visual signal. Compared with puretext-based approaches, learning causality from the visual signal has the following advantages: (1) Causality knowledge belongs to the commonsense knowledge, which is rarely expressed in thetextbut rich in videos; (2) Most events in the video are naturally time-ordered, which provides a rich resource for us tominecausality knowledge from; (3) All the objects in the video can be used as context to study the contextual property of causal relations. In detail, we first propose a high-quality dataset Vis-Causal and then conduct experiments to demonstrate that with good language and visual representation models as well as enough training signals, it is possible to automatically discover meaningful causal knowledge from the videos. Further analysis also shows that the contextual property of causal relations indeed exists, taking which into consideration might be crucial if we want to use the causality knowledge in real applications, and the visual signal could serve as a good resource for learning such contextual causality.","['Hongming Zhang', 'Yintong Huo', 'Xinran Zhao', 'Yangqiu Song', 'Dan Roth']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.07138,Anomali
GNN-XML: Graph Neural Networks for Extreme Multi-label Text Classification,"Extreme multi-labeltextclassification (XMTC) aims to tag atextinstance with the most relevant subset of labels from an extremely large label set. XMTC has attracted much recent attention due to massive label sets yielded by modern applications, such as news annotation and product recommendation. The main challenges of XMTC are the data scalability and sparsity, thereby leading to two issues: i) the intractability to scale to the extreme label setting, ii) the presence of long-tailed label distribution, implying that a large fraction of labels have few positive training instances. To overcome these problems, we propose GNN-XML, a scalable graph neural network framework tailored for XMTC problems. Specifically, we exploit label correlations viaminingtheir co-occurrence patterns and build a label graph based on the correlation matrix. We then conduct the attributed graph clustering by performing graph convolution with a low-pass graph filter to jointly model label dependencies and label features, which induces semantic label clusters. We further propose a bilateral-branch graph isomorphism network to decouple representation learning and classifier learning for better modeling tail labels. Experimental results on multiple benchmark datasets show that GNN-XML significantly outperforms state-of-the-art methods while maintaining comparable prediction efficiency and model size.","['Daoming Zong', 'Shiliang Sun']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.05860,Anomali
Towards Coinductive Models for Natural Language Understanding. Bringing together Deep Learning and Deep Semantics,"This article contains a proposal to add coinduction to the computational apparatus of natural language understanding. This, we argue, will provide a basis for more realistic, computationally sound, and scalable models of natural language dialogue, syntax and semantics. Given that the bottom up, inductively constructed, semantic and syntactic structures are brittle, and seemingly incapable of adequately representing the meaning of longer sentences or realistic dialogues, natural language understanding is in need of a new foundation. Coinduction, which uses top down constraints, has been successfully used in the design of operating systems and programming languages. Moreover, implicitly it has been present intextmining, machine translation, and in some attempts to model intensionality and modalities, which provides evidence that it works. This article shows high level formalizations of some of such uses.
  Since coinduction and induction can coexist, they can provide a common language and a conceptual model for research in natural language understanding. In particular, such an opportunity seems to be emerging in research on compositionality. This article shows several examples of the joint appearance of induction and coinduction in natural language processing. We argue that the known individual limitations of induction and coinduction can be overcome in empirical settings by a combination of the the two methods. We see an open problem in providing a theory of their joint use.",['Wlodek W. Zadrozny'],,arXiv,2020,https://doi.org/10.48550/arXiv.2012.05715,Anomali
Social Explorative Attention based Recommendation for Content Distribution Platforms,"In modern social media platforms, an effective content recommendation should benefit both creators to bring genuine benefits to them and consumers to help them get really interesting content. To address the limitations of existing methods for social recommendation, we propose Social Explorative Attention Network (SEAN), a social recommendation framework that uses a personalized content recommendation model to encourage personal interests driven recommendation. SEAN has two versions: (1) SEAN-END2END allows user's attention vector to attend their personalized interested points in the documents. (2) SEAN-KEYWORD extracts keywords from users' historical readings to capture their long-term interests. It is much faster than the first version, more suitable for practical usage, while SEAN-END2END is more effective. Both versions allow the personalization factors to attend to users' higher-order friends on the social network to improve the accuracy and diversity of recommendation results. Constructing two datasets in two languages, English and Spanish, from a popular decentralized content distribution platform, Steemit, we compare SEAN models with state-of-the-art collaborative filtering (CF) and content based recommendation approaches. Experimental results demonstrate the effectiveness of SEAN in terms of both Gini coefficients for recommendation equality and F1 scores for recommendation accuracy.","['Wenyi Xiao', 'Huan Zhao', 'Haojie Pan', 'Yangqiu Song', 'Vincent W. Zheng', 'Qiang Yang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.04945,Anomali
"Biblical names' relationships in the Gospel of Matthew, Mark, Luke, John and Acts of Apostles","In this paper we extrapolate the information about Bible's characters and places, and their interrelationships, by usingtextminingnetwork-based approach. We study the narrative structure of the WEB version of 5 books: the Gospel of Matthew, Mark, Luke, John and Acts of the Apostles. The main focus is the protagonists' names interrelationships in an analytical way, namely using various network-based methods and descriptors. This corpus is processed for creating a network: we download the names of people and places from Wikipedia's list of biblical names, then we look for their co-occurrences in each verse and, at the end of this process, we get N co-occurred names. The strength of the link between two names is defined as the sum of the times that these occur together in all the verses, in this way we obtain 5 adjacency matrices (one per book) of N by N couples of names. After this pre-processing phase, for each of the 5 analysed books we calculate the main network centrality measures (classical degree, weighted degree, betweenness and closeness), the network vulnerability and we run the Community Detection algorithm to highlight the role of Messiah inside the overall networks and his groups (communities). We have found that the proposed approach is suitable for highlighting the structures of the names co-occurrences. The found frameworks' structures are useful for interpreting the characters' plots under a structural point of view.","['Roberto Rondinelli', 'Stefano Marmani', 'Valerio Ficcadenti']",In Proceedings of the 15th International Conference on Statistical Analysis of Textual Data. Lexicometrica 2020,arXiv,2020,https://doi.org/10.48550/arXiv.2012.04753,Anomali
Sparse Correspondence Analysis for Contingency Tables,"Since the introduction of the lasso in regression, various sparse methods have been developed in an unsupervised context like sparse principal component analysis (s-PCA), sparse canonical correlation analysis (s-CCA) and sparse singular value decomposition (s-SVD). These sparse methods combine feature selection and dimension reduction. One advantage of s-PCA is to simplify the interpretation of the (pseudo) principal components since each one is expressed as a linear combination of a small number of variables. The disadvantages lie on the one hand in the difficulty of choosing the number of non-zero coefficients in the absence of a well established criterion and on the other hand in the loss of orthogonality for the components and/or the loadings. In this paper we propose sparse variants of correspondence analysis (CA)for large contingency tables like documents-terms matrices used intextmining, together with pPMD, a deation technique derived from projected deflation in s-PCA. We use the fact that CA is a double weighted PCA (for rows and columns) or a weighted SVD, as well as a canonical correlation analysis of indicator variables. Applying s-CCA or s-SVD allows to sparsify both rows and columns weights. The user may tune the level of sparsity of rows and columns and optimize it according to some criterium, and even decide that no sparsity is needed for rows (or columns) by relaxing one sparsity constraint. The latter is equivalent to apply s-PCA to matrices of row (or column) profiles.","['Ruiping Liu', 'Ndeye Niang', 'Gilbert Saporta', 'Huiwen Wang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.04271,Anomali
Improving Clinical Document Understanding on COVID-19 Research with Spark NLP,"Following the global COVID-19 pandemic, the number of scientific papers studying the virus has grown massively, leading to increased interest in automated literate review. We present a clinicaltextminingsystem that improves on previous efforts in three ways. First, it can recognize over 100 different entity types including social determinants of health, anatomy, risk factors, and adverse events in addition to other commonly used clinical and biomedical entities. Second, thetextprocessing pipeline includes assertion status detection, to distinguish between clinical facts that are present, absent, conditional, or about someone other than the patient. Third, the deep learning models used are more accurate than previously available, leveraging an integrated pipeline of state-of-the-art pretrained named entity recognition models, and improving on the previous best performing benchmarks for assertion status detection. We illustrate extracting trends and insights, e.g. most frequent disorders and symptoms, and most common vital signs and EKG findings, from the COVID-19 Open Research Dataset (CORD-19). The system is built using the Spark NLP library which natively supports scaling to use distributed clusters, leveraging GPUs, configurable and reusable NLP pipelines, healthcare specific embeddings, and the ability to train models to support new entity types or human languages with no code changes.","['Veysel Kocaman', 'David Talby']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.04005,Anomali
Nonnegative Matrix Factorization with Zellner Penalty,"Nonnegative matrix factorization (NMF) is a relatively new unsupervised learning algorithm that decomposes a nonnegative data matrix into a parts-based, lower dimensional, linear representation of the data. NMF has applications in image processing,textmining, recommendation systems and a variety of other fields. Since its inception, the NMF algorithm has been modified and explored by numerous authors. One such modification involves the addition of auxiliary constraints to the objective function of the factorization. The purpose of these auxiliary constraints is to impose task-specific penalties or restrictions on the objective function. Though many auxiliary constraints have been studied, none have made use of data-dependent penalties. In this paper, we propose Zellner nonnegative matrix factorization (ZNMF), which uses data-dependent auxiliary constraints. We assess the facial recognition performance of the ZNMF algorithm and several other well-known constrained NMF algorithms using the Cambridge ORL database.","['Matthew Corsetti', 'Ernest Fokoué']",Open Journal of Statistics 5 (2015) 777-786,arXiv,2020,https://doi.org/10.48550/arXiv.2012.03889,Anomali
Over a Decade of Social Opinion Mining: A Systematic Review,"Social media popularity and importance is on the increase due to people using it for various types of social interaction across multiple channels. This systematic review focuses on the evolving research area of Social OpinionMining, tasked with the identification of multiple opinion dimensions, such as subjectivity, sentiment polarity, emotion, affect, sarcasm and irony, from user-generated content represented across multiple social media platforms and in various media formats, liketext, image, video and audio. Through Social OpinionMining, natural language can be understood in terms of the different opinion dimensions, as expressed by humans. This contributes towards the evolution of Artificial Intelligence which in turn helps the advancement of several real-world use cases, such as customer service and decision making. A thorough systematic review was carried out on Social OpinionMiningresearch which totals 485 published studies and spans a period of twelve years between 2007 and 2018. The in-depth analysis focuses on the social media platforms, techniques, social datasets, language, modality, tools and technologies, and other aspects derived. Social OpinionMiningcan be utilised in many application areas, ranging from marketing, advertising and sales for product/service management, and in multiple domains and industries, such as politics, technology, finance, healthcare, sports and government. The latest developments in Social OpinionMiningbeyond 2018 are also presented together with future research directions, with the aim of leaving a wider academic and societal impact in several real-world applications.","['Keith Cortis', 'Brian Davis']",,arXiv,2021,https://doi.org/10.48550/arXiv.2012.03091,Anomali
Accelerating Text Mining Using Domain-Specific Stop Word Lists,"Textpreprocessing is an essential step intextmining. Removing words that can negatively impact the quality of prediction algorithms or are not informative enough is a crucial storage-saving technique intextindexing and results in improved computational efficiency. Typically, a generic stop word list is applied to a dataset regardless of the domain. However, many common words are different from one domain to another but have no significance within a particular domain. Eliminating domain-specific common words in a corpus reduces the dimensionality of the feature space, and improves the performance oftextminingtasks. In this paper, we present a novel mathematical approach for the automatic extraction of domain-specific words called the hyperplane-based approach. This new approach depends on the notion of low dimensional representation of the word in vector space and its distance from hyperplane. The hyperplane-based approach can significantly reducetextdimensionality by eliminating irrelevant features. We compare the hyperplane-based approach with other feature selection methods, namely \c{hi}2 and mutual information. An experimental study is performed on three different datasets and five classification algorithms, and measure the dimensionality reduction and the increase in the classification performance. Results indicate that the hyperplane-based approach can reduce the dimensionality of the corpus by 90% and outperforms mutual information. The computational time to identify the domain-specific words is significantly lower than mutual information.","['Farah Alshanik', 'Amy Apon', 'Alexander Herzog', 'Ilya Safro', 'Justin Sybrandt']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.02294,Anomali
Meta-Embeddings for Natural Language Inference and Semantic Similarity tasks,"Word Representations form the core component for almost all advanced Natural Language Processing (NLP) applications such astextmining, question-answering, andtextsummarization, etc. Over the last two decades, immense research is conducted to come up with one single model to solve all major NLP tasks. The major problem currently is that there are a plethora of choices for different NLP tasks. Thus for NLP practitioners, the task of choosing the right model to be used itself becomes a challenge. Thus combining multiple pre-trained word embeddings and forming meta embeddings has become a viable approach to improve tackle NLP tasks. Meta embedding learning is a process of producing a single word embedding from a given set of pre-trained input word embeddings. In this paper, we propose to use Meta Embedding derived from few State-of-the-Art (SOTA) models to efficiently tackle mainstream NLP tasks like classification, semantic relatedness, andtextsimilarity. We have compared both ensemble and dynamic variants to identify an efficient approach. The results obtained show that even the best State-of-the-Art models can be bettered. Thus showing us that meta-embeddings can be used for several NLP tasks by harnessing the power of several individual representations.","['Shree Charran R', 'Rahul Kumar Dubey']",,arXiv,2020,https://doi.org/10.48550/arXiv.2012.00633,Anomali
Text Mining for Processing Interview Data in Computational Social Science,"We use commercially availabletextanalysis technology to process interviewtextdata from a computational social science study. We find that topical clustering and terminological enrichment provide for convenient exploration and quantification of the responses. This makes it possible to generate and test hypotheses and to compare textual and non-textual variables, and saves analyst effort. We encourage studies in social science to usetextanalysis, especially for exploratory open-ended studies. We discuss how replicability requirements are met bytextanalysis technology. We note that the most recent learning models are not designed with transparency in mind, and that research requires a model to be editable and its decisions to be explainable. The tools available today, such as the one used in the present study, are not built for processing interviewtexts. While many of the variables under consideration are quantifiable using lexical statistics, we find that some interesting and potentially valuable features are difficult or impossible to automatise reliably at present. We note that there are some potentially interesting applications for traditional natural language processing mechanisms such as named entity recognition and anaphora resolution in this application area. We conclude with a suggestion for language technologists to investigate the challenge of processing interview data comprehensively, especially the interplay between question and response, and we encourage social science researchers not to hesitate to usetextanalysis tools, especially for the exploratory phase of processing interview data.?","['Jussi Karlgren', 'Renee Li', 'Eva M Meyersson Milgrom']",,arXiv,2020,https://doi.org/10.48550/arXiv.2011.14037,Anomali
"The Geometry of Distributed Representations for Better Alignment, Attenuated Bias, and Improved Interpretability","High-dimensional representations for words,text, images, knowledge graphs and other structured data are commonly used in different paradigms of machine learning and datamining. These representations have different degrees of interpretability, with efficient distributed representations coming at the cost of the loss of feature to dimension mapping. This implies that there is obfuscation in the way concepts are captured in these embedding spaces. Its effects are seen in many representations and tasks, one particularly problematic one being in language representations where the societal biases, learned from underlying data, are captured and occluded in unknown dimensions and subspaces. As a result, invalid associations (such as different races and their association with a polar notion of good versus bad) are made and propagated by the representations, leading to unfair outcomes in different tasks where they are used. This work addresses some of these problems pertaining to the transparency and interpretability of such representations. A primary focus is the detection, quantification, and mitigation of socially biased associations in language representation.",['Sunipa Dev'],,arXiv,2020,https://doi.org/10.48550/arXiv.2011.12465,Anomali
Finding Prerequisite Relations between Concepts using Textbook,"A prerequisite is anything that you need to know or understand first before attempting to learn or understand something new. In the current work, we present a method of finding prerequisite relations between concepts using related textbooks. Previous researchers have focused on finding these relations using Wikipedia link structure through unsupervised and supervised learning approaches. In the current work, we have proposed two methods, one is statistical method and another is learning-based method. Weminethe rich and structured knowledge available in the textbooks to find the content for those concepts and the order in which they are discussed. Using this information, proposed statistical method estimates explicit as well as implicit prerequisite relations between concepts. During experiments, we have found performance of proposed statistical method is better than the popular RefD method, which uses Wikipedia link structure. And proposed learning-based method has shown a significant increase in the efficiency of supervised learning method when compared with graph andtext-based learning-based approaches.","['Shivam Pal', 'Vipul Arora', 'Pawan Goyal']",,arXiv,2020,https://doi.org/10.48550/arXiv.2011.10337,Anomali
Nonlinear interferometer for Fourier-transform mid-infrared gas spectroscopy using near-infrared detection,"Nonlinear interferometers allow for mid-infrared spectroscopy with near-infrared detection using correlated photons. Previous implementations have demonstrated a spectral resolution limited by spectrally selective detection. In our work, we demonstrate mid-infrared transmission spectroscopy in a nonlinear interferometer using single-pixel near-infrared detection and Fourier-transform analysis. A sub-wavenumber spectral resolution allows for rotational-line-resolving spectroscopy of gaseous samples in a spectral bandwidth of over 700$\,$cm$^{-1}$. We use methane transmission spectra around 3.3$\,μ$m wavelength to characterize the spectral resolution, noise limitations and transmission accuracy of our device. The combination of nonlinear interferometry and Fourier-transform analysis paves the way towards performant and efficient mid-infrared spectroscopy with near-infrared detection.","['Chiara Lindner', 'Jachin Kunz', 'Simon J. Herr', 'Sebastian Wolf', 'Jens Kiessling', 'Frank Kühnemann']","Opt. Express 29, 4035-4047 (2021)",arXiv,2021,https://doi.org/10.48550/arXiv.2011.09764,Anomali
Visual Drift Detection for Sequence Data Analysis of Business Processes,"Event sequence data is increasingly available in various application domains, such as business process management, software engineering, or medical pathways. Processes in these domains are typically represented as process diagrams or flow charts. So far, various techniques have been developed for automatically generating such diagrams from event sequence data. An open challenge is the visual analysis of drift phenomena when processes change over time. In this paper, we address this research gap. Our contribution is a system for fine-granular process drift detection and corresponding visualizations for event logs of executed business processes. We evaluated our system both on synthetic and real-world data. On synthetic logs, we achieved an average F-score of 0.96 and outperformed all the state-of-the-art methods. On real-world logs, we identified all types of process drifts in a comprehensive manner. Finally, we conducted a user study highlighting that our visualizations are easy to use and useful as perceived by processminingexperts. In this way, our work contributes to research on processmining, event sequence analysis, and visualization of temporal data.","['Anton Yeshchenko', 'Claudio Di Ciccio', 'Jan Mendling', 'Artem Polyvyanyy']",,arXiv,2021,https://doi.org/10.48550/arXiv.2011.09130,Anomali
Text Mining to Identify and Extract Novel Disease Treatments From Unstructured Datasets,"Objective: We aim to learn potential novel cures for diseases from unstructuredtextsources. More specifically, we seek to extract drug-disease pairs of potential cures to diseases by a simple reasoning over the structure of spokentext.
  Materials and Methods: We use Google Cloud to transcribe podcast episodes of an NPR radio show. We then build a pipeline for systematically pre-processing thetextto ensure quality input to the core classification model, which feeds to a series of post-processing steps for obtaining filtered results. Our classification model itself uses a language model pre-trained on PubMedtext. The modular nature of our pipeline allows for ease of future developments in this area by substituting higher quality components at each stage of the pipeline. As a validation measure, we use ROBOKOP, an engine over a medical knowledge graph with only validated pathways, as a ground truth source for checking the existence of the proposed pairs. For the proposed pairs not found in ROBOKOP, we provide further verification using Chemotext.
  Results: We found 30.4% of our proposed pairs in the ROBOKOP database. For example, our model successfully identified that Omeprazole can help treat heartburn.We discuss the significance of this result, showing some examples of the proposed pairs.
  Discussion and Conclusion: The agreement of our results with the existing knowledge source indicates a step in the right direction. Given the plug-and-play nature of our framework, it is easy to add, remove, or modify parts to improve the model as necessary. We discuss the results showing some examples, and note that this is a potentially new line of research that has further scope to be explored. Although our approach was originally oriented on radio podcast transcripts, it is input-agnostic and could be applied to any source of textual data and to any problem of interest.","['Rahul Yedida', 'Saad Mohammad Abrar', 'Cleber Melo-Filho', 'Eugene Muratov', 'Rada Chirkova', 'Alexander Tropsha']",,arXiv,2020,https://doi.org/10.48550/arXiv.2011.07959,Anomali
Challenges of Applying Deep Reinforcement Learning in Dynamic Dispatching,"Dynamic dispatching aims to smartly allocate the right resources to the right place at the right time. Dynamic dispatching is one of the core problems for operations optimization in theminingindustry. Theoretically, deep reinforcement learning (RL) should be a natural fit to solve this problem. However, the industry relies on heuristics or even human intuitions, which are often short-sighted and sub-optimal solutions. In this paper, we review the main challenges in using deep RL to address the dynamic dispatching problem in theminingindustry.","['Hamed Khorasgani', 'Haiyan Wang', 'Chetan Gupta']",,arXiv,2020,https://doi.org/10.48550/arXiv.2011.05570,Anomali
PubSqueezer: A Text-Mining Web Tool to Transform Unstructured Documents into Structured Data,"The amount of scientific papers published every day is daunting and constantly increasing. Keeping up with literature represents a challenge. If one wants to start exploring new topics it is hard to have a big picture without reading lots of articles. Furthermore, as one reads through literature, making mental connections is crucial to ask new questions which might lead to discoveries. In this work, I present a web tool which uses aTextMiningstrategy to transform large collections of unstructured biomedical articles into structured data. Generated results give a quick overview on complex topics which can possibly suggest not explicitly reported information. In particular, I show two Data Science analyses. First, I present a literature based rare diseases network build using this tool in the hope that it will help clarify some aspects of these less popular pathologies. Secondly, I show how a literature based analysis conducted with PubSqueezer results allows to describe known facts about SARS-CoV-2. In one sentence, data generated with PubSqueezer make it easy to use scientific literate in any computational analysis such as machine learning, natural language processing etc.
  Availability: http://www.pubsqueezer.com",['Alberto Calderone'],,arXiv,2020,https://doi.org/10.48550/arXiv.2011.03123,Anomali
Generating Knowledge Graphs by Employing Natural Language Processing and Machine Learning Techniques within the Scholarly Domain,"The continuous growth of scientific literature brings innovations and, at the same time, raises new challenges. One of them is related to the fact that its analysis has become difficult due to the high volume of published papers for which manual effort for annotations and management is required. Novel technological infrastructures are needed to help researchers, research policy makers, and companies to time-efficiently browse, analyse, and forecast scientific research. Knowledge graphs i.e., large networks of entities and relationships, have proved to be effective solution in this space. Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. However, the current generation of knowledge graphs lacks of an explicit representation of the knowledge presented in the research papers. As such, in this paper, we present a new architecture that takes advantage of Natural Language Processing and Machine Learning methods for extracting entities and relationships from research publications and integrates them in a large-scale knowledge graph. Within this research work, we i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing andTextMiningtools, ii) describe an approach for integrating entities and relationships generated by these tools, iii) show the advantage of such an hybrid system over alternative approaches, and vi) as a chosen use case, we generated a scientific knowledge graph including 109,105 triples, extracted from 26,827 abstracts of papers within the Semantic Web domain. As our approach is general and can be applied to any domain, we expect that it can facilitate the management, analysis, dissemination, and processing of scientific knowledge.","['Danilo Dessì', 'Francesco Osborne', 'Diego Reforgiato Recupero', 'Davide Buscaldi', 'Enrico Motta']",,arXiv,2020,https://doi.org/10.48550/arXiv.2011.01103,Anomali
The GDPR Enforcement Fines at Glance,"The General Data Protection Regulation (GDPR) came into force in 2018. After this enforcement, many fines have already been imposed by national data protection authorities in Europe. This paper examines the individual GDPR articles referenced in the enforcement decisions, as well as predicts the amount of enforcement fines with available meta-data andtextminingfeatures extracted from the enforcement decision documents. According to the results, three articles related to the general principles, lawfulness, and information security have been the most frequently referenced ones. Although the amount of fines imposed vary across the articles referenced, these three particular articles do not stand out. Furthermore, a better statistical evidence is available with other meta-data features, including information about the particular European countries in which the enforcements were made. Accurate predictions are attainable even with simple machine learning techniques for regression analysis. Basictextminingfeatures outperform the meta-data features in this regard. In addition to these results, the paper reflects the GDPR's enforcement against public administration obstacles in the European Union (EU), as well as discusses the use of automatic decision-making systems in judiciary.","['Jukka Ruohonen', 'Kalle Hjerppe']",,arXiv,2021,https://doi.org/10.48550/arXiv.2011.00946,Anomali
Multimodal Urban Sound Tagging with Spatiotemporal Context,"Noise pollution significantly affects our daily life and urban development. Urban Sound Tagging (UST) has attracted much attention recently, which aims to analyze and monitor urban noise pollution. One weakness of the previous UST studies is that the spatial and temporal context of sound signals, which contains complementary information about when and where the audio data was recorded, has not been investigated. To address this problem, in this paper, we propose a multimodal UST system that deeplyminesthe audio and spatiotemporal context together. In order to incorporate characteristics of different acoustic features, two sets of four spectrograms are first extracted as the inputs of residual neural networks. Then, the spatiotemporal context is encoded and combined with acoustic features to explore the efficiency of multimodal learning for discriminating sound signals. Moreover, a data filtering approach is adopted intextprocessing to further improve the performance of multi-modality. We evaluate the proposed method on the UST challenge (task 5) of DCASE2020. Experimental results demonstrate the effectiveness of the proposed method.","['Jisheng Bai', 'Jianfeng Chen', 'Mou Wang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2011.00175,Anomali
Constraint Translation Candidates: A Bridge between Neural Query Translation and Cross-lingual Information Retrieval,"Query translation (QT) is a key component in cross-lingual information retrieval system (CLIR). With the help of deep learning, neural machine translation (NMT) has shown promising results on various tasks. However, NMT is generally trained with large-scale out-of-domain data rather than in-domain query translation pairs. Besides, the translation model lacks a mechanism at the inference time to guarantee the generated words to match the search index. The two shortages of QT result in readabletextsfor human but inadequate candidates for the downstream retrieval task. In this paper, we propose a novel approach to alleviate these problems by limiting the open target vocabulary search space of QT to a set of important wordsminedfrom search index database. The constraint translation candidates are employed at both of training and inference time, thus guiding the translation model to learn and generate well performing target queries. The proposed methods are exploited and examined in a real-word CLIR system--Aliexpress e-Commerce search engine. Experimental results demonstrate that our approach yields better performance on both translation quality and retrieval accuracy than the strong NMT baseline.","['Tianchi Bi', 'Liang Yao', 'Baosong Yang', 'Haibo Zhang', 'Weihua Luo', 'Boxing Chen']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.13658,Anomali
RUArt: A Novel Text-Centered Solution for Text-Based Visual Question Answering,"Text-based visual question answering (VQA) requires to read and understandtextin an image to correctly answer a given question. However, most current methods simply add optical character recognition (OCR) tokens extracted from the image into the VQA model without considering contextual information of OCR tokens andminingthe relationships between OCR tokens and scene objects. In this paper, we propose a noveltext-centered method called RUArt (Reading, Understanding and Answering the RelatedText) fortext-based VQA. Taking an image and a question as input, RUArt first reads the image and obtainstextand scene objects. Then, it understands the question, OCRedtextand objects in the context of the scene, and furtherminesthe relationships among them. Finally, it answers the relatedtextfor the given question throughtextsemantic matching and reasoning. We evaluate our RUArt on twotext-based VQA benchmarks (ST-VQA and TextVQA) and conduct extensive ablation studies for exploring the reasons behind RUArt's effectiveness. Experimental results demonstrate that our method can effectively explore the contextual information of thetextandminethe stable relationships between thetextand objects.","['Zan-Xia Jin', 'Heran Wu', 'Chun Yang', 'Fang Zhou', 'Jingyan Qin', 'Lei Xiao', 'Xu-Cheng Yin']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.12917,Anomali
Extracting Body Text from Academic PDF Documents for Text Mining,"Accurate extraction of bodytextfrom PDF-formatted academic documents is essential intext-miningapplications for deeper semantic understandings. The objective is to extract complete sentences in the bodytextinto a txt file with the original sentence flow and paragraph boundaries. Existing tools for extractingtextfrom PDF documents would often mix body and nonbodytexts. We devise and implement a system called PDFBoT to detect multiple-column layouts using a line-sweeping technique, remove nonbodytextusing computedtextfeatures and syntactic tagging in backward traversal, and align the remainingtextback to sentences and paragraphs. We show that PDFBoT is highly accurate with average F1 scores of, respectively, 0.99 on extracting sentences, 0.96 on extracting paragraphs, and 0.98 on removingtexton tables, figures, and charts over a corpus of PDF documents randomly selected from arXiv.org across multiple academic disciplines.","['Changfeng Yu', 'Cheng Zhang', 'Jie Wang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.12647,Anomali
Helping users discover perspectives: Enhancing opinion mining with joint topic models,"Support or opposition concerning a debated claim such as abortion should be legal can have different underlying reasons, which we call perspectives. This paper explores how opinionminingcan be enhanced with joint topic modeling, to identify distinct perspectives within the topic, providing an informative overview from unstructuredtext. We evaluate four joint topic models (TAM, JST, VODUM, and LAM) in a user study assessing human understandability of the extracted perspectives. Based on the results, we conclude that joint topic models such as TAM can discover perspectives that align with human judgments. Moreover, our results suggest that users are not influenced by their pre-existing stance on the topic of abortion when interpreting the output of topic models.","['Tim Draws', 'Jody Liu', 'Nava Tintarev']",2020 International Conference on Data Mining Workshops (ICDMW),arXiv,2021,https://doi.org/10.48550/arXiv.2010.12505,Anomali
TweetBERT: A Pretrained Language Representation Model for Twitter Text Analysis,"Twitter is a well-known microblogging social site where users express their views and opinions in real-time. As a result, tweets tend to contain valuable information. With the advancements of deep learning in the domain of natural language processing, extracting meaningful information from tweets has become a growing interest among natural language researchers. Applying existing language representation models to extract information from Twitter does not often produce good results. Moreover, there is no existing language representation models fortextanalysis specific to the social media domain. Hence, in this article, we introduce two TweetBERT models, which are domain specific language presentation models, pre-trained on millions of tweets. We show that the TweetBERT models significantly outperform the traditional BERT models in Twittertextminingtasks by more than 7% on each Twitter dataset. We also provide an extensive analysis by evaluating seven BERT models on 31 different datasets. Our results validate our hypothesis that continuously training language models on twitter corpus help performance with Twitter.","['Mohiuddin Md Abdul Qudar', 'Vijay Mago']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.11091,Anomali
Learnable Graph-regularization for Matrix Decomposition,"Low-rank approximation models of data matrices have become important machine learning and dataminingtools in many fields including computer vision,textmining, bioinformatics and many others. They allow for embedding high-dimensional data into low-dimensional spaces, which mitigates the effects of noise and uncovers latent relations. In order to make the learned representations inherit the structures in the original data, graph-regularization terms are often added to the loss function. However, the prior graph construction often fails to reflect the true network connectivity and the intrinsic relationships. In addition, many graph-regularized methods fail to take the dual spaces into account. Probabilistic models are often used to model the distribution of the representations, but most of previous methods often assume that the hidden variables are independent and identically distributed for simplicity. To this end, we propose a learnable graph-regularization model for matrix decomposition (LGMD), which builds a bridge between graph-regularized methods and probabilistic matrix decomposition models. LGMD learns two graphical structures (i.e., two precision matrices) in real-time in an iterative manner via sparse precision matrix estimation and is more robust to noise and missing entries. Extensive numerical results and comparison with competing methods demonstrate its effectiveness.","['Penglong Zhai', 'Shihua Zhang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.08513,Anomali
Unsupervised Bitext Mining and Translation via Self-trained Contextual Embeddings,"We describe an unsupervised method to create pseudo-parallel corpora for machine translation (MT) from unalignedtext. We use multilingual BERT to create source and target sentence embeddings for nearest-neighbor search and adapt the model via self-training. We validate our technique by extracting parallel sentence pairs on the BUCC 2017 bitextminingtask and observe up to a 24.5 point increase (absolute) in F1 scores over previous unsupervised methods. We then improve an XLM-based unsupervised neural MT system pre-trained on Wikipedia by supplementing it with pseudo-paralleltextminedfrom the same corpus, boosting unsupervised translation performance by up to 3.5 BLEU on the WMT'14 French-English and WMT'16 German-English tasks and outperforming the previous state-of-the-art. Finally, we enrich the IWSLT'15 English-Vietnamese corpus with pseudo-parallel Wikipedia sentence pairs, yielding a 1.2 BLEU improvement on the low-resource MT task. We demonstrate that unsupervised bitextminingis an effective way of augmenting MT datasets and complements existing techniques like initializing with pre-trained contextual embeddings.","['Phillip Keung', 'Julian Salazar', 'Yichao Lu', 'Noah A. Smith']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.07761,Anomali
Adaptive Deep Forest for Online Learning from Drifting Data Streams,"Learning from data streams is among the most vital fields of contemporary datamining. The online analysis of information coming from those potentially unbounded data sources allows for designing reactive up-to-date models capable of adjusting themselves to continuous flows of data. While a plethora of shallow methods have been proposed for simpler low-dimensional streaming problems, almost none of them addressed the issue of learning from complex contextual data, such as images ortexts. The former is represented mainly by adaptive decision trees that have been proven to be very efficient in streaming scenarios. The latter has been predominantly addressed by offline deep learning. In this work, we attempt to bridge the gap between these two worlds and propose Adaptive Deep Forest (ADF) - a natural combination of the successful tree-based streaming classifiers with deep forest, which represents an interesting alternative idea for learning from contextual data. The conducted experiments show that the deep forest approach can be effectively transformed into an online algorithm, forming a model that outperforms all state-of-the-art shallow adaptive classifiers, especially for high-dimensional complex streams.","['Łukasz Korycki', 'Bartosz Krawczyk']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.07340,Anomali
Will This Idea Spread Beyond Academia? Understanding Knowledge Transfer of Scientific Concepts across Text Corpora,"What kind of basic research ideas are more likely to get applied in practice? There is a long line of research investigating patterns of knowledge transfer, but it generally focuses on documents as the unit of analysis and follow their transfer into practice for a specific scientific domain. Here we study translational research at the level of scientific concepts for all scientific fields. We do this throughtextminingand predictive modeling using three corpora: 38.6 million paper abstracts, 4 million patent documents, and 0.28 million clinical trials. We extract scientific concepts (i.e., phrases) from corpora as instantiations of ""research ideas"", create concept-level features as motivated by literature, and then follow the trajectories of over 450,000 new concepts (emerged from 1995-2014) to identify factors that lead only a small proportion of these ideas to be used in inventions and drug trials. Results from our analysis suggest several mechanisms that distinguish which scientific concept will be adopted in practice, and which will not. We also demonstrate that our derived features can be used to explain and predict knowledge transfer with high accuracy. Our work provides greater understanding of knowledge transfer for researchers, practitioners, and government agencies interested in encouraging translational research.","['Hancheng Cao', 'Mengjie Cheng', 'Zhepeng Cen', 'Daniel A. McFarland', 'Xiang Ren']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.06657,Anomali
Joint Adaptive Graph and Structured Sparsity Regularization for Unsupervised Feature Selection,"Feature selection is an important data preprocessing in dataminingand machine learning which can be used to reduce the feature dimension without deteriorating model's performance. Since obtaining annotated data is laborious or even infeasible in many cases, unsupervised feature selection is more practical in reality. Though lots of methods for unsupervised feature selection have been proposed, these methods select features independently, thus it is no guarantee that the group of selected features is optimal. What's more, the number of selected features must be tuned carefully to obtain a satisfactory result. To tackle these problems, we propose a joint adaptive graph and structured sparsity regularization unsupervised feature selection (JASFS) method in this paper, in which a $l_{2,0}$-norm regularization term with respect to transformation matrix is imposed in the manifold learning for feature selection, and a graph regularization term is incorporated into the learning model to learn the local geometric structure of data adaptively. An efficient and simple iterative algorithm is designed to solve the proposed optimization problem with the analysis of computational complexity. After optimized, a subset of optimal features will be selected in group, and the number of selected features will be determined automatically. Experimental results on eight benchmarks demonstrate the effectiveness and efficiency of the proposed method compared with several state-of-the-art approaches.","['Zhenzhen Sun', 'Yuanlong Yu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2010.05454,Anomali
Detecting Foodborne Illness Complaints in Multiple Languages Using English Annotations Only,"Health departments have been deployingtextclassification systems for the early detection of foodborne illness complaints in social media documents such as Yelp restaurant reviews. Current systems have been successfully applied for documents in English and, as a result, a promising direction is to increase coverage and recall by considering documents in additional languages, such as Spanish or Chinese. Training previous systems for more languages, however, would be expensive, as it would require the manual annotation of many documents for each new target language. To address this challenge, we consider cross-lingual learning and train multilingual classifiers using only the annotations for English-language reviews. Recent zero-shot approaches based on pre-trained multi-lingual BERT (mBERT) have been shown to effectively align languages for aspects such as sentiment. Interestingly, we show that those approaches are less effective for capturing the nuances of foodborne illness, our public health application of interest. To improve performance without extra annotations, we create artificial training documents in the target language through machine translation and train mBERT jointly for the source (English) and target language. Furthermore, we show that translating labeled documents to multiple languages leads to additional performance improvements for some target languages. We demonstrate the benefits of our approach through extensive experiments with Yelp restaurant reviews in seven languages. Our classifiers identify foodborne illness complaints in multilingual reviews from the Yelp Challenge dataset, which highlights the potential of our general approach for deployment in health departments.","['Ziyi Liu', 'Giannis Karamanolakis', 'Daniel Hsu', 'Luis Gravano']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.05194,Anomali
Universal Weighting Metric Learning for Cross-Modal Matching,"Cross-modal matching has been a highlighted research topic in both vision and language areas. Learning appropriateminingstrategy to sample and weight informative pairs is crucial for the cross-modal matching performance. However, most existing metric learning methods are developed for unimodal matching, which is unsuitable for cross-modal matching on multimodal data with heterogeneous features. To address this problem, we propose a simple and interpretable universal weighting framework for cross-modal matching, which provides a tool to analyze the interpretability of various loss functions. Furthermore, we introduce a new polynomial loss under the universal weighting framework, which defines a weight function for the positive and negative informative pairs respectively. Experimental results on two image-textmatching benchmarks and two video-textmatching benchmarks validate the efficacy of the proposed method.","['Jiwei Wei', 'Xing Xu', 'Yang Yang', 'Yanli Ji', 'Zheng Wang', 'Heng Tao Shen']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.03403,Anomali
Learning to Represent Image and Text with Denotation Graph,"Learning to fuse vision and language information and representing them is an important research problem with many applications. Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images. In this paper, we propose learning representations from a set of implied, visually grounded expressions between image andtext, automaticallyminedfrom those datasets. In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded. This type of generic-to-specific relations can be discovered using linguistic analysis tools. We propose methods to incorporate such relations into learning representation. We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations. The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition. Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.","['Bowen Zhang', 'Hexiang Hu', 'Vihan Jain', 'Eugene Ie', 'Fei Sha']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.02949,Anomali
Legal Sentiment Analysis and Opinion Mining (LSAOM): Assimilating Advances in Autonomous AI Legal Reasoning,"An expanding field of substantive interest for the theory of the law and the practice-of-law entails Legal Sentiment Analysis and OpinionMining(LSAOM), consisting of two often intertwined phenomena and actions underlying legal discussions and narratives: (1) Sentiment Analysis (SA) for the detection of expressed or implied sentiment about a legal matter within the context of a legal milieu, and (2) OpinionMining(OM) for the identification and illumination of explicit or implicit opinion accompaniments immersed within legal discourse. Efforts to undertake LSAOM have historically been performed by human hand and cognition, and only thinly aided in more recent times by the use of computer-based approaches. Advances in Artificial Intelligence (AI) involving especially Natural Language Processing (NLP) and Machine Learning (ML) are increasingly bolstering how automation can systematically perform either or both of Sentiment Analysis and OpinionMining, all of which is being inexorably carried over into engagement within a legal context for improving LSAOM capabilities. This research paper examines the evolving infusion of AI into Legal Sentiment Analysis and OpinionMiningand proposes an alignment with the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR), plus provides additional insights regarding AI LSAOM in its mechanizations and potential impact to the study of law and the practicing of law.",['Lance Eliot'],,arXiv,2020,https://doi.org/10.48550/arXiv.2010.02726,Anomali
Are Words Commensurate with Actions? Quantifying Commitment to a Cause from Online Public Messaging,"Public entities such as companies and politicians increasingly use online social networks to communicate directly with their constituencies. Often, this public messaging is aimed at aligning the entity with a particular cause or issue, such as the environment or public health. However, as a consumer or voter, it can be difficult to assess an entity's true commitment to a cause based on public messaging. In this paper, we present atextclassification approach to categorize a message according to its commitment level toward a cause. We then compare the volume of such messages with external ratings based on entities' actions (e.g., a politician's voting record with respect to the environment or a company's rating from environmental non-profits). We find that by distinguishing between low- and high- level commitment messages, we can more reliably identify truly committed entities. Furthermore, by measuring the discrepancy between classified messages and external ratings, we can identify entities whose public messaging does not align with their actions, thereby providing a methodology to identify potentially ""inauthentic"" messaging campaigns.","['Zhao Wang', 'Jennifer Cutler', 'Aron Culotta']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.02466,Anomali
Immigration Document Classification and Automated Response Generation,"In this paper, we consider the problem of organizing supporting documents vital to U.S. work visa petitions, as well as responding to Requests For Evidence (RFE) issued by the U.S.~Citizenship and Immigration Services (USCIS). Typically, both processes require a significant amount of repetitive manual effort. To reduce the burden of mechanical work, we apply machine learning methods to automate these processes, with humans in the loop to review and edit output for submission. In particular, we use an ensemble of image andtextclassifiers to categorize supporting documents. We also use atextclassifier to automatically identify the types of evidence being requested in an RFE, and used the identified types in conjunction with response templates and extracted fields to assemble draft responses. Empirical results suggest that our approach achieves considerable accuracy while significantly reducing processing time.","['Sourav Mukherjee', 'Tim Oates', 'Vince DiMascio', 'Huguens Jean', 'Rob Ares', 'David Widmark', 'Jaclyn Harder']","2020 International Conference on Data Mining Workshops (ICDMW), 2020, pp. 782-789",arXiv,2020,https://doi.org/10.48550/arXiv.2010.01997,Anomali
Extreme-SAX: Extreme Points Based Symbolic Representation for Time Series Classification,"Time series classification is an important problem in dataminingwith several applications in different domains. Because time series data are usually high dimensional, dimensionality reduction techniques have been proposed as an efficient approach to lower their dimensionality. One of the most popular dimensionality reduction techniques of time series data is the Symbolic Aggregate Approximation (SAX), which is inspired by algorithms fromtextminingand bioinformatics. SAX is simple and efficient because it uses precomputed distances. The disadvantage of SAX is its inability to accurately represent important points in the time series. In this paper we present Extreme-SAX (E-SAX), which uses only the extreme points of each segment to represent the time series. E-SAX has exactly the same simplicity and efficiency of the original SAX, yet it gives better results in time series classification than the original SAX, as we show in extensive experiments on a variety of time series datasets.",['Muhammad Marwan Muhammad Fuad'],"Lecture Notes in Computer Science 12393, Springer 2020,",arXiv,2020,https://doi.org/10.48550/arXiv.2010.00732,Anomali
A survey on natural language processing (nlp) and applications in insurance,"Textis the most widely used means of communication today. This data is abundant but nevertheless complex to exploit within algorithms. For years, scientists have been trying to implement different techniques that enable computers to replicate some mechanisms of human reading. During the past five years, research disrupted the capacity of the algorithms to unleash the value oftextdata. It brings today, many opportunities for the insurance industry.Understanding those methods and, above all, knowing how to apply them is a major challenge and key to unleash the value oftextdata that have been stored for many years. Processing language with computer brings many new opportunities especially in the insurance sector where reports are central in the information used by insurers. SCOR's Data Analytics team has been working on the implementation of innovative tools or products that enable the use of the latest research ontextanalysis. Understandingtextminingtechniques in insurance enhances the monitoring of the underwritten risks and many processes that finally benefit policyholders.This article proposes to explain opportunities that Natural Language Processing (NLP) are providing to insurance. It details different methods used today in practice traces back the story of them. We also illustrate the implementation of certain methods using open source libraries and python codes that we have developed to facilitate the use of these techniques.After giving a general overview on the evolution oftextminingduring the past few years,we share about how to conduct a full study withtextminingand share some examples to serve those models into insurance products or services. Finally, we explained in more details every step that composes a Natural Language Processing study to ensure the reader can have a deep understanding on the implementation.","['Antoine Ly', 'Benno Uthayasooriyar', 'Tingting Wang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2010.00462,Anomali
Efficient Time and Space Representation of Uncertain Event Data,"Processminingis a discipline which concerns the analysis of execution data of operational processes, the extraction of models from event data, the measurement of the conformance between event data and normative models, and the enhancement of all aspects of processes. Most approaches assume that event data is accurately capture behavior. However, this is not realistic in many applications: data can contain uncertainty, generated from errors in recording, imprecise measurements, and other factors. Recently, new methods have been developed to analyze event data containing uncertainty; these techniques prominently rely on representing uncertain event data by means of graph-based models explicitly capturing uncertainty. In this paper, we introduce a new approach to efficiently calculate a graph representation of the behavior contained in an uncertain process trace. We present our novel algorithm, prove its asymptotic time complexity, and show experimental results that highlight order-of-magnitude performance improvements for the behavior graph construction.","['Marco Pegoraro', 'Merih Seran Uysal', 'Wil M. P. van der Aalst']",Algorithms 13 (2020) 285,arXiv,2020,https://doi.org/10.48550/arXiv.2010.00334,Anomali
Maximum Entropy classification for record linkage,"By record linkage one joins records residing in separate files which are believed to be related to the same entity. In this paper we approach record linkage as a classification problem, and adapt the maximum entropy classification method intextminingto record linkage, both in the supervised and unsupervised settings of machine learning. The set of links will be chosen according to the associated uncertainty. On the one hand, our framework overcomes some persistent theoretical flaws of the classical approach pioneered by Fellegi and Sunter (1969); on the other hand, the proposed algorithm is scalable and fully automatic, unlike the classical approach that generally requires clerical review to resolve the undecided cases.","['Danhyang Lee', 'Li-Chun Zhang', 'Jae-Kwang Kim']",,arXiv,2021,https://doi.org/10.48550/arXiv.2009.14797,Anomali
Conformance Checking over Uncertain Event Data,"The strong impulse to digitize processes and operations in companies and enterprises have resulted in the creation and automatic recording of an increasingly large amount of process data in information systems. These are made available in the form of event logs. Processminingtechniques enable the process-centric analysis of data, including automatically discovering process models and checking if event data conform to a given model. In this paper, we analyze the previously unexplored setting of uncertain event logs. In such event logs uncertainty is recorded explicitly, i.e., the time, activity and case of an event may be unclear or imprecise. In this work, we define a taxonomy of uncertain event logs and models, and we examine the challenges that uncertainty poses on process discovery and conformance checking. Finally, we show how upper and lower bounds for conformance can be obtained by aligning an uncertain trace onto a regular process model.","['Marco Pegoraro', 'Merih Seran Uysal', 'Wil M. P. van der Aalst']",Information Systems 102 (2021) 101810,arXiv,2022,https://doi.org/10.48550/arXiv.2009.14452,Anomali
Knowledge-Aware Procedural Text Understanding with Multi-Stage Training,"Proceduraltextdescribes dynamic state changes during a step-by-step natural process (e.g., photosynthesis). In this work, we focus on the task of proceduraltextunderstanding, which aims to comprehend such documents and track entities' states and locations during a process. Although recent approaches have achieved substantial progress, their results are far behind human performance. Two challenges, the difficulty of commonsense reasoning and data insufficiency, still remain unsolved, which require the incorporation of external knowledge bases. Previous works on external knowledge injection usually rely on noisy webminingtools and heuristic rules with limited applicable scenarios. In this paper, we propose a novel KnOwledge-Aware proceduraLtextunderstAnding (KOALA) model, which effectively leverages multiple forms of external knowledge in this task. Specifically, we retrieve informative knowledge triples from ConceptNet and perform knowledge-aware reasoning while tracking the entities. Besides, we employ a multi-stage training schema which fine-tunes the BERT model over unlabeled data collected from Wikipedia before further fine-tuning it on the final model. Experimental results on two proceduraltextdatasets, ProPara and Recipes, verify the effectiveness of the proposed methods, in which our model achieves state-of-the-art performance in comparison to various baselines.","['Zhihan Zhang', 'Xiubo Geng', 'Tao Qin', 'Yunfang Wu', 'Daxin Jiang']",,arXiv,2021,https://doi.org/10.48550/arXiv.2009.13199,Anomali
AliMe KG: Domain Knowledge Graph Construction and Application in E-commerce,"Pre-sales customer service is of importance to E-commerce platforms as it contributes to optimizing customers' buying process. To better serve users, we propose AliMe KG, a domain knowledge graph in E-commerce that captures user problems, points of interests (POI), item information and relations thereof. It helps to understand user needs, answer pre-sales questions and generate explanationtexts. We applied AliMe KG to several online business scenarios such as shopping guide, question answering over properties and recommendation reason generation, and gained positive results. In the paper, we systematically introduce how we construct domain knowledge graph from freetext, and demonstrate its business value with several applications. Our experience shows thatminingstructured knowledge from freetextin vertical domain is practicable, and can be of substantial value in industrial settings.","['Feng-Lin Li', 'Hehong Chen', 'Guohai Xu', 'Tian Qiu', 'Feng Ji', 'Ji Zhang', 'Haiqing Chen']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.11684,Anomali
Effective and Efficient Variable-Length Data Series Analytics,"In the last twenty years, data series similarity search has emerged as a fundamental operation at the core of several analysis tasks and applications related to data series collections. Many solutions to differentminingproblems work by means of similarity search. In this regard, all the proposed solutions require the prior knowledge of the series length on which similarity search is performed. In several cases, the choice of the length is critical and sensibly influences the quality of the expected outcome. Unfortunately, the obvious brute-force solution, which provides an outcome for all lengths within a given range is computationally untenable. In this Ph.D. work, we present the first solutions that inherently support scalable and variable-length similarity search in data series, applied to sequence/subsequences matching, motif and discord discovery problems.The experimental results show that our approaches are up to orders of magnitude faster than the alternatives. They also demonstrate that we can remove the unrealistic constraint of performing analytics using a predefined length, leading to more intuitive and actionable results, which would have otherwise been missed.",['Michele Linardi'],,arXiv,2020,https://doi.org/10.48550/arXiv.2009.11648,Anomali
BioALBERT: A Simple and Effective Pre-trained Language Model for Biomedical Named Entity Recognition,"In recent years, with the growing amount of biomedical documents, coupled with advancement in natural language processing algorithms, the research on biomedical named entity recognition (BioNER) has increased exponentially. However, BioNER research is challenging as NER in the biomedical domain are: (i) often restricted due to limited amount of training data, (ii) an entity can refer to multiple types and concepts depending on its context and, (iii) heavy reliance on acronyms that are sub-domain specific. Existing BioNER approaches often neglect these issues and directly adopt the state-of-the-art (SOTA) models trained in general corpora which often yields unsatisfactory results. We propose biomedical ALBERT (A Lite Bidirectional Encoder Representations from Transformers for BiomedicalTextMining) bioALBERT, an effective domain-specific language model trained on large-scale biomedical corpora designed to capture biomedical context-dependent NER. We adopted a self-supervised loss used in ALBERT that focuses on modelling inter-sentence coherence to better learn context-dependent representations and incorporated parameter reduction techniques to lower memory consumption and increase the training speed in BioNER. In our experiments, BioALBERT outperformed comparative SOTA BioNER models on eight biomedical NER benchmark datasets with four different entity types. We trained four different variants of BioALBERT models which are available for the research community to be used in future research.","['Usman Naseem', 'Matloob Khushi', 'Vinay Reddy', 'Sakthivel Rajendran', 'Imran Razzak', 'Jinman Kim']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.09223,Anomali
Prior Art Search and Reranking for Generated Patent Text,"Generative models, such as GPT-2, have demonstrated impressive results recently. A fundamental question we'd like to address is: where did the generatedtextcome from? This work is our initial effort toward answering the question by using prior art search. The purpose of the prior art search is to find the most similar priortextin the training data of GPT-2. We take a reranking approach and apply it to the patent domain. Specifically, we pre-train GPT-2 models from scratch by using the patent data from the USPTO. The input for the prior art search is the patenttextgenerated by the GPT-2 model. We also pre-trained BERT models from scratch for converting patenttextto embeddings. The steps of reranking are: (1) search the most similartextin the training data of GPT-2 by taking a bag-of-word ranking approach (BM25), (2) convert the search results intextformat to BERT embeddings, and (3) provide the final result by ranking the BERT embeddings based on their similarities with the patenttextgenerated by GPT-2. The experiments in this work show that such reranking is better than ranking with embeddings alone. However, our mixed results also indicate that calculating the semantic similarities among longtextspans is still challenging. To our knowledge, this work is the first to implement a reranking system to identify retrospectively the most similar inputs to a GPT model based on its output.","['Jieh-Sheng Lee', 'Jieh Hsiang']",The 2nd Workshop on Patent Text Mining and Semantic Technologies (PatentSemTech2021) co-located with the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,arXiv,2021,https://doi.org/10.48550/arXiv.2009.09132,Anomali
PhenoTagger: A Hybrid Method for Phenotype Concept Recognition using Human Phenotype Ontology,"Automatic phenotype concept recognition from unstructuredtextremains a challenging task in biomedicaltextminingresearch. Previous works that address the task typically use dictionary-based matching methods, which can achieve high precision but suffer from lower recall. Recently, machine learning-based methods have been proposed to identify biomedical concepts, which can recognize more unseen concept synonyms by automatic feature learning. However, most methods require large corpora of manually annotated data for model training, which is difficult to obtain due to the high cost of human annotation. In this paper, we propose PhenoTagger, a hybrid method that combines both dictionary and machine learning-based methods to recognize Human Phenotype Ontology (HPO) concepts in unstructured biomedicaltext. We first use all concepts and synonyms in HPO to construct a dictionary, which is then used to automatically build a distantly supervised training dataset for machine learning. Next, a cutting-edge deep learning model is trained to classify each candidate phrase (n-gram from input sentence) into a corresponding concept label. Finally, the dictionary and machine learning-based prediction results are combined for improved performance. Our method is validated with two HPO corpora, and the results show that PhenoTagger compares favorably to previous methods. In addition, to demonstrate the generalizability of our method, we retrained PhenoTagger using the disease ontology MEDIC for disease concept recognition to investigate the effect of training on different ontologies. Experimental results on the NCBI disease corpus show that PhenoTagger without requiring manually annotated training data achieves competitive performance as compared with state-of-the-art supervised methods.","['Ling Luo', 'Shankai Yan', 'Po-Ting Lai', 'Daniel Veltri', 'Andrew Oler', 'Sandhya Xirasagar', 'Rajarshi Ghosh', 'Morgan Similuk', 'Peter N. Robinson', 'Zhiyong Lu']",,arXiv,2021,https://doi.org/10.48550/arXiv.2009.08478,Anomali
Representing Semantified Biological Assays in the Open Research Knowledge Graph,"In the biotechnology and biomedical domains, recenttextminingefforts advocate for machine-interpretable, and preferably, semantified, documentation formats of laboratory processes. This includes wet-lab protocols, (in)organic materials synthesis reactions, genetic manipulations and procedures for faster computer-mediated analysis and predictions. Herein, we present our work on the representation of semantified bioassays in the Open Research Knowledge Graph (ORKG). In particular, we describe a semantification system work-in-progress to generate, automatically and quickly, the critical semantified bioassay data mass needed to foster a consistent user audience to adopt the ORKG for recording their bioassays and facilitate the organisation of research, according to FAIR principles.","['Marco Anteghini', ""Jennifer D'Souza"", 'Vitor A. P. Martins dos Santos', 'Sören Auer']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.07642,Anomali
High-Performance Mining of COVID-19 Open Research Datasets for Text Classification and Insights in Cloud Computing Environments,"COVID-19 global pandemic is an unprecedented health crisis. Since the outbreak, many researchers around the world have produced an extensive collection of literatures. For the research community and the general public to digest, it is crucial to analyse thetextand provide insights in a timely manner, which requires a considerable amount of computational power. Clouding computing has been widely adopted in academia and industry in recent years. In particular, hybrid cloud is gaining popularity since its two-fold benefits: utilising existing resource to save cost and using additional cloud service providers to gain assess to extra computing resources on demand. In this paper, we developed a system utilising the Aneka PaaS middleware with parallel processing and multi-cloud capability to accelerate the ETL and article categorising process using machine learning technology on a hybrid cloud. The result is then persisted for further referencing, searching and visualising. Our performance evaluation shows that the system can help with reducing processing time and achieving linear scalability. Beyond COVID-19, the application might be used directly in broader scholarly article indexing and analysing.","['Jie Zhao', 'Maria A. Rodriguez', 'Rajkumar Buyya']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.07399,Anomali
Arabic Opinion Mining Using a Hybrid Recommender System Approach,"Recommender systems nowadays are playing an important role in the delivery of services and information to users. Sentiment analysis (also known as opinionmining) is the process of determining the attitude of textual opinions, whether they are positive, negative or neutral. Data sparsity is representing a big issue for recommender systems because of the insufficiency of user rating or absence of data about users or items. This research proposed a hybrid approach combining sentiment analysis and recommender systems to tackle the problem of data sparsity problems by predicting the rating of products from users reviews usingtextminingand NLP techniques. This research focuses especially on Arabic reviews, where the model is evaluated using Opinion Corpus for Arabic (OCA) dataset. Our system was efficient, and it showed a good accuracy of nearly 85 percent in predicting rating from reviews","['Fouzi Harrag', 'Abdulmalik Salman Al-Salman', 'Alaa Alquahtani']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.07397,Anomali
SA-Net: A deep spectral analysis network for image clustering,"Although supervised deep representation learning has attracted enormous attentions across areas of pattern recognition and computer vision, little progress has been made towards unsupervised deep representation learning for image clustering. In this paper, we propose a deep spectral analysis network for unsupervised representation learning and image clustering. While spectral analysis is established with solid theoretical foundations and has been widely applied to unsupervised datamining, its essential weakness lies in the fact that it is difficult to construct a proper affinity matrix and determine the involving Laplacian matrix for a given dataset. In this paper, we propose a SA-Net to overcome these weaknesses and achieve improved image clustering by extending the spectral analysis procedure into a deep learning framework with multiple layers. The SA-Net has the capability to learn deep representations and reveal deep correlations among data samples. Compared with the existing spectral analysis, the SA-Net achieves two advantages: (i) Given the fact that one spectral analysis procedure can only deal with one subset of the given dataset, our proposed SA-Net elegantly integrates multiple parallel and consecutive spectral analysis procedures together to enable interactive learning across different units towards a coordinated clustering model; (ii) Our SA-Net can identify the local similarities among different images at patch level and hence achieves a higher level of robustness against occlusions. Extensive experiments on a number of popular datasets support that our proposed SA-Net outperforms 11 benchmarks across a number of image clustering applications.","['Jinghua Wang', 'Jianmin Jiang']",Neurocomputing 2020,arXiv,2020,https://doi.org/10.48550/arXiv.2009.07026,Anomali
Refining Student Marks based on Enrolled Modules Assessment Methods using Data Mining Techniques,"Choosing the right and effective way to assess students is one of the most important tasks of higher education. Many studies have shown that students tend to receive higher scores during their studies when assessed by different study methods which include units that are fully assessed by varying the duration of study or a combination of courses and exams than by exams alone. Many Educational DataMiningstudies process data in advance through traditional data extraction, including the data preparation process. In this paper, we propose a different data preparation process by investigating more than 230000 student records for the preparation of scores. The data have been processed through diverse stages in order to extract a categorical factor through which students module marks are refined during the data preparation stage. The results of this work show that students final marks should not be isolated from the nature of the enrolled module assessment methods. They must rather be investigated thoroughly and considered during EDM data preprocessing stage. More generally, educational data should not be prepared in the same way normal data are due to the differences in data sources, applications, and error types. The effect of Module Assessment Index on the prediction process using Random Forest and Naive Bayes classification techniques were investigated. It was shown that considering MAI as attribute increases the accuracy of predicting students second year averages based on their first year averages.","['Mohammed A. Alsuwaiket', 'Anas H. Blasi', 'Khawla Altarawneh']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.06381,Anomali
Analysis and representation of Igbo text document for a text-based system,"The advancement in Information Technology (IT) has assisted in inculcating the three Nigeria major languages intext-based application such astextmining, information retrieval and natural language processing. The interest of this paper is the Igbo language, which uses compounding as a common type of word formation and as well has many vocabularies of compound words. The issues of collocation, word ordering and compounding play high role in Igbo language. The ambiguity in dealing with these compound words has made the representation of Igbo languagetextdocument very difficult because this cannot be addressed using the most common and standard approach of the Bag-Of-Words (BOW) model oftextrepresentation, which ignores the word order and relation. However, this cause for a concern and the need to develop an improved model to capture this situation. This paper presents the analysis of Igbo languagetextdocument, considering its compounding nature and describes its representation with the Word-based N-gram model to properly prepare it for anytext-based application. The result shows that Bigram and Trigram n-gramtextrepresentation models provide more semantic information as well addresses the issues of compounding, word ordering and collocations which are the major language peculiarities in Igbo. They are likely to give better performance when used in any Igbotext-based system.","['Ifeanyi-Reuben Nkechi J.', 'Ugwu Chidiebere', 'Adegbola Tunde']","International Journal of Data Mining Techniques and Applications (IJDMTA). Volume 06 Issue 01, June 2017, Page No 26-32",arXiv,2020,https://doi.org/10.48550/arXiv.2009.06376,Anomali
Characterizing Twitter Interaction during COVID-19 pandemic using Complex Networks and Text Mining,"The outbreak of covid-19 started many months ago, the reported origin was in Wuhan Market, China. Fastly, this virus was propagated to other countries because the access to international travels is affordable and many countries have a distance of some flight hours, besides borders were a constant flow of people. By the other hand, Internet users have the habits of sharing content using Social Networks and issues, problems, thoughts about Covdid-19 were not an exception. Therefore, it is possible to analyze Social Network interaction from one city, country to understand the impact generated by this global issue. South America is one region with developing countries with challenges to face related to Politics, Economy, Public Health and other. Therefore, the scope of this paper is to analyze the interaction on Twitter of South American countries and characterize the flow of data through the users using Complex Network representation andTextMining. The preliminary experiments introduces the idea of existence of patterns, similar to Complex Systems. Besides, the degree distribution confirm the idea of having a System and visualization of Adjacency Matrices show the presence of users' group publishing and interacting together during the time, there is a possibility of identification of robots sending posts constantly.",['Josimar E. Chire-Saire'],,arXiv,2020,https://doi.org/10.48550/arXiv.2009.05619,Anomali
QSAN: A Quantum-probability based Signed Attention Network for Explainable False Information Detection,"False information detection on social media is challenging as it commonly requires tedious evidence-collecting but lacks available comparative information. Cluesminedfrom user comments, as the wisdom of crowds, could be of considerable benefit to this task. However, it is non-trivial to capture the complex semantics from the contents and comments in consideration of their implicit correlations. Although deep neural networks have good expressive power, one major drawback is the lack of explainability. In this paper, we focus on how to learn from the post contents and related comments in social media to understand and detect the false information more effectively, with explainability. We thus propose a Quantum-probability based Signed Attention Network (QSAN) that integrates the quantum-driventextencoding and a novel signed attention mechanism in a unified framework. QSAN is not only able to distinguish important comments from the others, but also can exploit the conflicting social viewpoints in the comments to facilitate the detection. Moreover, QSAN is advantageous with its explainability in terms of transparency due to quantum physics meanings and the attention weights. Extensive experiments on real-world datasets show that our approach outperforms state-of-the-art baselines and can provide different kinds of user comments to explain why a piece of information is detected as false.","['Tian Tian', 'Yudong Liu', 'Xiaoyu Yang', 'Yuefei Lyu', 'Xi Zhang', 'Binxing Fang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.03823,Anomali
Leam: An Interactive System for In-situ Visual Text Analysis,"With the increase in scale and availability of digitaltextgenerated on the web, enterprises such as online retailers and aggregators often usetextanalytics tomineand analyze the data to improve their services and products alike.Textdata analysis is an iterative, non-linear process with diverse workflows spanning multiple stages, from data cleaning to visualization. Existingtextanalytics systems usually accommodate a subset of these stages and often fail to address challenges related to data heterogeneity, provenance, workflow reusability and reproducibility, and compatibility with established practices. Based on a set of design considerations we derive from these challenges, we propose Leam, a system that treats thetextanalysis process as a single continuum by combining advantages of computational notebooks, spreadsheets, and visualization tools. Leam features an interactive user interface for runningtextanalysis workflows, a new data model for managing multiple atomic and composite data types, and an expressive algebra that captures diverse sets of operations representing various stages oftextanalysis and enables coordination among different components of the system, including data, code, and visualizations. We report our current progress in Leam development while demonstrating its usefulness with usage examples. Finally, we outline a number of enhancements to Leam and identify several research directions for developing an interactive visualtextanalysis system.","['Sajjadur Rahman', 'Peter Griggs', 'Çağatay Demiralp']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.03520,Anomali
Text Mining over Curriculum Vitae of Peruvian Professionals using Official Scientific Site DINA,"During the last decade, Peruvian government started to invest and promote Science and Technology through Concytec(National Council of Science and Technology). Many programs are oriented to support research projects, expenses for paper presentation, organization of conferences/ events and more. Concytec created a National Directory of Researchers(DINA) where professionals can create and add curriculum vitae, Concytec can provide official title of Researcher following some criterion for the evaluation. The actual paper aims to conduct an exploratory analysis over the curriculum vitae of Peruvian Professionals using DataMiningApproach to understand Peruvian context.","['Josimar Edinson Chire Saire', 'Honorio Apaza Alanoca']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.03087,Anomali
Fast and Secure Distributed Nonnegative Matrix Factorization,"Nonnegative matrix factorization (NMF) has been successfully applied in several dataminingtasks. Recently, there is an increasing interest in the acceleration of NMF, due to its high cost on large matrices. On the other hand, the privacy issue of NMF over federated data is worthy of attention, since NMF is prevalently applied in image andtextanalysis which may involve leveraging privacy data (e.g, medical image and record) across several parties (e.g., hospitals). In this paper, we study the acceleration and security problems of distributed NMF. Firstly, we propose a distributed sketched alternating nonnegative least squares (DSANLS) framework for NMF, which utilizes a matrix sketching technique to reduce the size of nonnegative least squares subproblems with a convergence guarantee. For the second problem, we show that DSANLS with modification can be adapted to the security setting, but only for one or limited iterations. Consequently, we propose four efficient distributed NMF methods in both synchronous and asynchronous settings with a security guarantee. We conduct extensive experiments on several real datasets to show the superiority of our proposed methods. The implementation of our methods is available at https://github.com/qianyuqiu79/DSANLS.","['Yuqiu Qian', 'Conghui Tan', 'Danhao Ding', 'Hui Li', 'Nikos Mamoulis']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.02845,Anomali
UPB at SemEval-2020 Task 9: Identifying Sentiment in Code-Mixed Social Media Texts using Transformers and Multi-Task Learning,"Sentiment analysis is a process widely used in opinionminingcampaigns conducted today. This phenomenon presents applications in a variety of fields, especially in collecting information related to the attitude or satisfaction of users concerning a particular subject. However, the task of managing such a process becomes noticeably more difficult when it is applied in cultures that tend to combine two languages in order to express ideas and thoughts. By interleaving words from two languages, the user can express with ease, but at the cost of making thetextfar less intelligible for those who are not familiar with this technique, but also for standard opinionminingalgorithms. In this paper, we describe the systems developed by our team for SemEval-2020 Task 9 that aims to cover two well-known code-mixed languages: Hindi-English and Spanish-English.
  We intend to solve this issue by introducing a solution that takes advantage of several neural network approaches, as well as pre-trained word embeddings. Our approach (multlingual BERT) achieves promising performance on the Hindi-English task, with an average F1-score of 0.6850, registered on the competition leaderboard, ranking our team 16th out of 62 participants. For the Spanish-English task, we obtained an average F1-score of 0.7064 ranking our team 17th out of 29 participants by using another multilingual Transformer-based model, XLM-RoBERTa.","['George-Eduard Zaharia', 'George-Alexandru Vlad', 'Dumitru-Clementin Cercel', 'Traian Rebedea', 'Costin-Gabriel Chiru']",,arXiv,2020,https://doi.org/10.48550/arXiv.2009.02780,Anomali
From the digital data revolution to digital health and digital economy toward a digital society: Pervasiveness of Artificial Intelligence,"Technological progress has led to powerful computers and communication technologies that penetrate nowadays all areas of science, industry and our private lives. As a consequence, all these areas are generating digital traces of data amounting to big data resources. This opens unprecedented opportunities but also challenges toward the analysis, management, interpretation and utilization of these data. Fortunately, recent breakthroughs in deep learning algorithms complement now machine learning and statistics methods for an efficient analysis of such data. Furthermore, advances intextminingand natural language processing, e.g., word-embedding methods, enable also the processing of large amounts oftextdata from diverse sources as governmental reports, blog entries in social media or clinical health records of patients. In this paper, we present a perspective on the role of artificial intelligence in these developments and discuss also potential problems we are facing in a digital society.",['Frank Emmert-Streib'],,arXiv,2020,https://doi.org/10.48550/arXiv.2008.12672,Anomali
"Twitter Interaction to Analyze Covid-19 Impact in Ghana, Africa from March to July","The novel coronavirus, COVID-19, has impacted various aspects of the world from tourism, business, education, and many more. Like for every country, the global pandemic has imposed similar effects on Ghana. During this period, citizens of this country have used social networks as a platform to find and disseminate information about the infectious disease and also share their own opinions and sentiments. In this study, we usetextminingto draw insights from data collected from the social network, Twitter. Our exploration of the data led us to understand the most frequent topics raised in the Greater Accra region of Ghana from March to July 2020. We observe that the engagement of users of this social network was initially high in March but declined from April to July. The reason was probably that the people were becoming more adapted to the situation after an initial shock when the disease was announced in the country. We also found certain words in these tweets of users that enabled us to understand the sentiments and mental state of individuals at the time.","['Josimar Chire Saire', 'Kobby Panford-Quainoo']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.12277,Anomali
Conceptualized Representation Learning for Chinese Biomedical Text Mining,"Biomedicaltextminingis becoming increasingly important as the number of biomedical documents and web data rapidly grows. Recently, word representation models such as BERT has gained popularity among researchers. However, it is difficult to estimate their performance on datasets containing biomedicaltextsas the word distributions of general and biomedical corpora are quite different. Moreover, the medical domain has long-tail concepts and terminologies that are difficult to be learned via language models. For the Chinese biomedicaltext, it is more difficult due to its complex structure and the variety of phrase combinations. In this paper, we investigate how the recently introduced pre-trained language model BERT can be adapted for Chinese biomedical corpora and propose a novel conceptualized representation learning approach. We also release a new Chinese Biomedical Language Understanding Evaluation benchmark (\textbf{ChineseBLUE}). We examine the effectiveness of Chinese pre-trained models: BERT, BERT-wwm, RoBERTa, and our approach. Experimental results on the benchmark show that our approach could bring significant gain. We release the pre-trained model on GitHub: https://github.com/alibaba-research/ChineseBLUE.","['Ningyu Zhang', 'Qianghuai Jia', 'Kangping Yin', 'Liang Dong', 'Feng Gao', 'Nengwei Hua']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.10813,Anomali
Breaking the Communities: Characterizing community changing users using text mining and graph machine learning on Twitter,"Even though the Internet and social media have increased the amount of news and information people can consume, most users are only exposed to content that reinforces their positions and isolates them from other ideological communities. This environment has real consequences with great impact on our lives like severe political polarization, easy spread of fake news, political extremism, hate groups and the lack of enriching debates, among others. Therefore, encouraging conversations between different groups of users and breaking the closed community is of importance for healthy societies. In this paper, we characterize and study users who break their community on Twitter using natural language processing techniques and graph machine learning algorithms. In particular, we collected 9 million Twitter messages from 1.5 million users and constructed the retweet networks. We identified their communities and topics of discussion associated to them. With this data, we present a machine learning framework for social media users classification which detects ""community breakers"", i.e. users that swing from their closed community to another one. A feature importance analysis in three Twitter polarized political datasets showed that these users have low values of PageRank, suggesting that changes are driven because their messages have no response in their communities. This methodology also allowed us to identify their specific topics of interest, providing a fully characterization of this kind of users.","['Federico Albanese', 'Leandro Lombardi', 'Esteban Feuerstein', 'Pablo Balenzuela']",,arXiv,2021,https://doi.org/10.48550/arXiv.2008.10749,Anomali
Learning Attribute-Based and Relationship-Based Access Control Policies with Unknown Values,"Attribute-Based Access Control (ABAC) and Relationship-based access control (ReBAC) provide a high level of expressiveness and flexibility that promote security and information sharing, by allowing policies to be expressed in terms of attributes of and chains of relationships between entities. Algorithms for learning ABAC and ReBAC policies from legacy access control information have the potential to significantly reduce the cost of migration to ABAC or ReBAC.
  This paper presents the first algorithms forminingABAC and ReBAC policies from access control lists (ACLs) and incomplete information about entities, where the values of some attributes of some entities are unknown. We show that the core of this problem can be viewed as learning a concise three-valued logic formula from a set of labeled feature vectors containing unknowns, and we give the first algorithm (to the best of our knowledge) for that problem.","['Thang Bui', 'Scott D. Stoller']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.08444,Anomali
Using LDA and LSTM Models to Study Public Opinions and Critical Groups Towards Congestion Pricing in New York City through 2007 to 2019,"This study explores how people view and respond to the proposals of NYC congestion pricing evolve in time. To understand these responses, Twitter data is collected and analyzed. Critical groups in the recurrent process are detected by statistically analyzing the active users and the most mentioned accounts, and the trends of people's attitudes and concerns over the years are identified withtextminingand hybrid Nature Language Processing techniques, including LDA topic modeling and LSTM sentiment classification. The result shows that multiple interest groups were involved and played crucial roles during the proposal, especially Mayor and Governor, MTA, and outer-borough representatives. The public shifted the concern of focus from the plan details to a wider city's sustainability and fairness. Furthermore, the plan's approval relies on several elements, the joint agreement reached in the political process, strong motivation in the real-world, the scheme based on balancing multiple interests, and groups' awareness of tolling's benefits and necessity.","['Qian Ye', 'Xiaohong Chen', 'Onur Kalan', 'Kaan Ozbay']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.07366,Anomali
Artificial Intelligence in the Battle against Coronavirus (COVID-19): A Survey and Future Research Directions,"Artificial intelligence (AI) has been applied widely in our daily lives in a variety of ways with numerous success stories. AI has also contributed to dealing with the coronavirus disease (COVID-19) pandemic, which has been happening around the globe. This paper presents a survey of AI methods being used in various applications in the fight against the COVID-19 outbreak and outlines the crucial role of AI research in this unprecedented battle. We touch on areas where AI plays as an essential component, from medical image processing, data analytics,textminingand natural language processing, the Internet of Things, to computational biology and medicine. A summary of COVID-19 related data sources that are available for research purposes is also presented. Research directions on exploring the potential of AI and enhancing its capability and power in the pandemic battle are thoroughly discussed. We identify 13 groups of problems related to the COVID-19 pandemic and highlight promising AI methods and tools that can be used to address these problems. It is envisaged that this study will provide AI researchers and the wider community with an overview of the current status of AI applications, and motivate researchers to harness AI's potential in the fight against COVID-19.","['Thanh Thi Nguyen', 'Quoc Viet Hung Nguyen', 'Dung Tien Nguyen', 'Samuel Yang', 'Peter W. Eklund', 'Thien Huynh-The', 'Thanh Tam Nguyen', 'Quoc-Viet Pham', 'Imran Razzak', 'Edbert B. Hsu']",,arXiv,2022,https://doi.org/10.48550/arXiv.2008.07343,Anomali
Comparison of Syntactic Parsers on Biomedical Texts,Syntactic parsing is an important step in the automatedtextanalysis which aims at information extraction. Quality of the syntactic parsing determines to a large extent the recall and precision of thetextminingresults. In this paper we evaluate the performance of several popular syntactic parsers in application to the biomedicaltextmining.,['Maria Biryukov'],,arXiv,2020,https://doi.org/10.48550/arXiv.2008.07189,Anomali
Interpretable Representations in Explainable AI: From Theory to Practice,"Interpretable representations are the backbone of many explainers that target black-box predictive systems based on artificial intelligence and machine learning algorithms. They translate the low-level data representation necessary for good predictive performance into high-level human-intelligible concepts used to convey the explanatory insights. Notably, the explanation type and its cognitive complexity are directly controlled by the interpretable representation, tweaking which allows to target a particular audience and use case. However, many explainers built upon interpretable representations overlook their merit and fall back on default solutions that often carry implicit assumptions, thereby degrading the explanatory power and reliability of such techniques. To address this problem, we study properties of interpretable representations that encode presence and absence of human-comprehensible concepts. We demonstrate how they are operationalised for tabular, image andtextdata; discuss their assumptions, strengths and weaknesses; identify their core building blocks; and scrutinise their configuration and parameterisation. In particular, this in-depth analysis allows us to pinpoint their explanatory properties, desiderata and scope for (malicious) manipulation in the context of tabular data where a linear model is used to quantify the influence of interpretable concepts on a black-box prediction. Our findings lead to a range of recommendations for designing trustworthy interpretable representations; specifically, the benefits of class-aware (supervised) discretisation of tabular data, e.g., with decision trees, and sensitivity of image interpretable representations to segmentation granularity and occlusion colour.","['Kacper Sokol', 'Peter Flach']",,arXiv,2024,https://doi.org/10.48550/arXiv.2008.07007,Anomali
Efficient Knowledge Graph Validation via Cross-Graph Representation Learning,"Recent advances in information extraction have motivated the automatic construction of huge Knowledge Graphs (KGs) byminingfrom large-scaletextcorpus. However, noisy facts are unavoidably introduced into KGs that could be caused by automatic extraction. To validate the correctness of facts (i.e., triplets) inside a KG, one possible approach is to map the triplets into vector representations by capturing the semantic meanings of facts. Although many representation learning approaches have been developed for knowledge graphs, these methods are not effective for validation. They usually assume that facts are correct, and thus may overfit noisy facts and fail to detect such facts. Towards effective KG validation, we propose to leverage an external human-curated KG as auxiliary information source to help detect the errors in a target KG. The external KG is built upon human-curated knowledge repositories and tends to have high precision. On the other hand, although the target KG built by information extraction fromtextshas low precision, it can cover new or domain-specific facts that are not in any human-curated repositories. To tackle this challenging task, we propose a cross-graph representation learning framework, i.e., CrossVal, which can leverage an external KG to validate the facts in the target KG efficiently. This is achieved by embedding triplets based on their semantic meanings, drawing cross-KG negative samples and estimating a confidence score for each triplet based on its degree of correctness. We evaluate the proposed framework on datasets across different domains. Experimental results show that the proposed framework achieves the best performance compared with the state-of-the-art methods on large-scale KGs.","['Yaqing Wang', 'Fenglong Ma', 'Jing Gao']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.06995,Anomali
TopicBERT: A Transformer transfer learning based memory-graph approach for multimodal streaming social media topic detection,"Real time nature of social networks with bursty short messages and their respective large data scale spread among vast variety of topics are research interest of many researchers. These properties of social networks which are known as 5'Vs of big data has led to many unique and enlightenment algorithms and techniques applied to large social networking datasets and data streams. Many of these researches are based on detection and tracking of hot topics and trending social media events that help revealing many unanswered questions. These algorithms and in some cases software products mostly rely on the nature of the language itself. Although, other techniques such as unsupervised dataminingmethods are language independent but many requirements for a comprehensive solution are not met. Many research issues such as noisy sentences that adverse grammar and new online user invented words are challenging maintenance of a good social network topic detection and tracking methodology; The semantic relationship between words and in most cases, synonyms are also ignored by many of these researches. In this research, we use Transformers combined with an incremental community detection algorithm. Transformer in one hand, provides the semantic relation between words in different contexts. On the other hand, the proposed graphminingtechnique enhances the resulting topics with aid of simple structural rules. Named entity recognition from multimodal data, image andtext, labels the named entities with entity type and the extracted topics are tuned using them. All operations of proposed system has been applied with big social data perspective under NoSQL technologies. In order to present a working and systematic solution, we combined MongoDB with Neo4j as two major database systems of our work. The proposed system shows higher precision and recall compared to other methods in three different datasets.","['Meysam Asgari-Chenaghlu', 'Mohammad-Reza Feizi-Derakhshi', 'Leili farzinvash', 'Mohammad-Ali Balafar', 'Cina Motamed']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.06877,Anomali
"A Scalable, Adaptive and Sound Nonconvex Regularizer for Low-rank Matrix Completion","Matrix learning is at the core of many machine learning problems. A number of real-world applications such as collaborative filtering andtextminingcan be formulated as a low-rank matrix completion problem, which recovers incomplete matrix using low-rank assumptions. To ensure that the matrix solution has a low rank, a recent trend is to use nonconvex regularizers that adaptively penalize singular values. They offer good recovery performance and have nice theoretical properties, but are computationally expensive due to repeated access to individual singular values. In this paper, based on the key insight that adaptive shrinkage on singular values improve empirical performance, we propose a new nonconvex low-rank regularizer called ""nuclear norm minus Frobenius norm"" regularizer, which is scalable, adaptive and sound. We first show it provably holds the adaptive shrinkage property. Further, we discover its factored form which bypasses the computation of singular values and allows fast optimization by general optimization algorithms. Stable recovery and convergence are guaranteed. Extensive low-rank matrix completion experiments on a number of synthetic and real-world data sets show that the proposed method obtains state-of-the-art recovery performance while being the fastest in comparison to existing low-rank matrix learning methods.","['Yaqing Wang', 'Quanming Yao', 'James T. Kwok']",,arXiv,2021,https://doi.org/10.48550/arXiv.2008.06542,Anomali
Context Reinforced Neural Topic Modeling over Short Texts,"As one of the prevalent topicminingtools, neural topic modeling has attracted a lot of interests for the advantages of high efficiency in training and strong generalisation abilities. However, due to the lack of context in each shorttext, the existing neural topic models may suffer from feature sparsity on such documents. To alleviate this issue, we propose a Context Reinforced Neural Topic Model (CRNTM), whose characteristics can be summarized as follows. Firstly, by assuming that each shorttextcovers only a few salient topics, CRNTM infers the topic for each word in a narrow range. Secondly, our model exploits pre-trained word embeddings by treating topics as multivariate Gaussian distributions or Gaussian mixture distributions in the embedding space. Extensive experiments on two benchmark datasets validate the effectiveness of the proposed model on both topic discovery andtextclassification.","['Jiachun Feng', 'Zusheng Zhang', 'Cheng Ding', 'Yanghui Rao', 'Haoran Xie']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.04545,Anomali
A Survey on Large-scale Machine Learning,"Machine learning can provide deep insights into data, allowing machines to make high-quality predictions and having been widely used in real-world applications, such astextmining, visual classification, and recommender systems. However, most sophisticated machine learning approaches suffer from huge time costs when operating on large-scale data. This issue calls for the need of {Large-scale Machine Learning} (LML), which aims to learn patterns from big data with comparable performance efficiently. In this paper, we offer a systematic survey on existing LML methods to provide a blueprint for the future developments of this area. We first divide these LML methods according to the ways of improving the scalability: 1) model simplification on computational complexities, 2) optimization approximation on computational efficiency, and 3) computation parallelism on computational capabilities. Then we categorize the methods in each perspective according to their targeted scenarios and introduce representative methods in line with intrinsic strategies. Lastly, we analyze their limitations and discuss potential directions as well as open issues that are promising to address in the future.","['Meng Wang', 'Weijie Fu', 'Xiangnan He', 'Shijie Hao', 'Xindong Wu']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.03911,Anomali
Agricultural Knowledge Management Using Smart Voice Messaging Systems: Combination of Physical and Human Sensors,"The use of the Internet of Things (IoT) in agricultural knowledge management systems is one of the most promising approaches to increasing the efficiency of agriculture. However, the existing physical sensors in agriculture are limited for monitoring various changes in the characteristics of crops and may be expensive for the average farmer. We propose a combination of physical and human sensors (the five human senses). By using their own eyes, ears, noses, tongues, and fingers, farmers could check the various changes in the characteristics and conditions (colors of leaves, diseases, pests, faulty or malfunctioning equipment) of their crops and equipment, verbally describe their observations, and capture the descriptions with audio recording devices, such as smartphones. The voice recordings could be transcribed intotextby web servers. The data captured by the physical and human sensors (voice messages) are analyzed by data andtextminingto create and improve agricultural knowledge. An agricultural knowledge management system using physical and human sensors encourages to share and transfer knowledge among farmers for the purpose of improving the efficiency and productivity of agriculture. We applied one such agricultural knowledge management system (smart voice messaging system) to a greenhouse vegetable farm in Hokkaido. A qualitative analysis of accumulated voice messages and an interview with the farmer demonstrated the effectiveness of this system. The contributions of this study include a new and practical approach to an ""agricultural Internet of Everything (IoE)"" and evidence of its effectiveness as a result of our trial experiment at a real vegetable farm.","['Naoshi Uchihira', 'Masami Yoshida']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.03711,Anomali
Antibody Watch: Text Mining Antibody Specificity from the Literature,"Antibodies are widely used reagents to test for expression of proteins and other antigens. However, they might not always reliably produce results when they do not specifically bind to the target proteins that their providers designed them for, leading to unreliable research results. While many proposals have been developed to deal with the problem of antibody specificity, it is still challenging to cover the millions of antibodies that are available to researchers. In this study, we investigate the feasibility of automatically generating alerts to users of problematic antibodies by extracting statements about antibody specificity reported in the literature. The extracted alerts can be used to construct an ""Antibody Watch"" knowledge base containing supporting statements of problematic antibodies. We developed a deep neural network system and tested its performance with a corpus of more than two thousand articles that reported uses of antibodies. We divided the problem into two tasks. Given an input article, the first task is to identify snippets about antibody specificity and classify if the snippets report that any antibody exhibits non-specificity, and thus is problematic. The second task is to link each of these snippets to one or more antibodies mentioned in the snippet. The experimental evaluation shows that our system can accurately perform both classification and linking tasks with weighted F-scores over 0.925 and 0.923, respectively, and 0.914 overall when combined to complete the joint task. We leveraged Research Resource Identifiers (RRID) to precisely identify antibodies linked to the extracted specificity snippets. The result shows that it is feasible to construct a reliable knowledge base about problematic antibodies bytextmining.","['Chun-Nan Hsu', 'Chia-Hui Chang', 'Thamolwan Poopradubsil', 'Amanda Lo', 'Karen A. William', 'Ko-Wei Lin', 'Anita Bandrowski', 'Ibrahim Burak Ozyurt', 'Jeffrey S. Grethe', 'Maryann E. Martone']","PLOS Computational Biology, 2021",arXiv,2020,https://doi.org/10.48550/arXiv.2008.01937,Anomali
"Tense, aspect and mood based event extraction for situation analysis and crisis management","Nowadays event extraction systems mainly deal with a relatively small amount of information about temporal and modal qualifications of situations, primarily processing assertive sentences in the past tense. However, systems with a wider coverage of tense, aspect and mood can provide better analyses and can be used in a wider range oftextanalysis applications. This thesis develops such a system for Turkish language. This is accomplished by extending Open Source InformationMiningand Analysis (OPTIMA) research group's event extraction software, by implementing appropriate extensions in the semantic representation format, by adding a partial grammar which improves the TAM (Tense, Aspect and Mood) marker, adverb analysis and matching functions of ExPRESS, and by constructing an appropriate lexicon in the standard of CORLEONE. These extensions are based on iv the theory of anchoring relations (Temürcü, 2007, 2011) which is a crosslinguistically applicable semantic framework for analyzing tense, aspect and mood related categories. The result is a system which can, in addition to extracting basic event structures, classify sentences given in news reports according to their temporal, modal and volitional/illocutionary values. Although the focus is on news reports of natural disasters, disease outbreaks and man-made disasters in Turkish language, the approach can be adapted to other languages, domains and genres. This event extraction and classification system, with further developments, can provide a basis for automated browsing systems for preventing environmental and humanitarian risk.",['Ali Hürriyetoğlu'],,arXiv,2020,https://doi.org/10.48550/arXiv.2008.01555,Anomali
Interactive Text Graph Mining with a Prolog-based Dialog Engine,"On top of a neural network-based dependency parser and a graph-based natural language processing module we design a Prolog-based dialog engine that explores interactively a ranked fact database extracted from atextdocument.
  We reorganize dependency graphs to focus on the most relevant content elements of a sentence and integrate sentence identifiers as graph nodes.
  Additionally, after ranking the graph we take advantage of the implicit semantic information that dependency links and WordNet bring in the form of subject-verb-object, is-a and part-of relations.
  Working on the Prolog facts and their inferred consequences, the dialog engine specializes thetextgraph with respect to a query and reveals interactively the document's most relevant content elements.
  The open-source code of the integrated system is available at https://github.com/ptarau/DeepRank .
  Under consideration in Theory and Practice of Logic Programming (TPLP).","['Paul Tarau', 'Eduardo Blanco']",Theory and Practice of Logic Programming 21 (2021) 244-263,arXiv,2020,https://doi.org/10.48550/arXiv.2008.00956,Anomali
Investigating the Effect of Emoji in Opinion Classification of Uzbek Movie Review Comments,"Opinionminingon social media posts has become more and more popular. Users often express their opinion on a topic not only with words but they also use image symbols such as emoticons and emoji. In this paper, we investigate the effect of emoji-based features in opinion classification of Uzbektexts, and more specifically movie review comments from YouTube. Several classification algorithms are tested, and feature ranking is performed to evaluate the discriminative ability of the emoji-based features.","['Ilyos Rabbimov', 'Iosif Mporas', 'Vasiliki Simaki', 'Sami Kobilov']",,arXiv,2020,https://doi.org/10.48550/arXiv.2008.00482,Anomali
Muon Flux Measurement at China Jinping Underground Laboratory,"China Jinping Underground Laboratory (CJPL) is ideal for studying solar-, geo-, and supernova neutrinos. A precise measurement of the cosmic-ray background would play an essential role in proceeding with the R\&D research for these MeV-scale neutrino experiments. Using a 1-ton prototype detector for the Jinping Neutrino Experiment (JNE), we detected 264 high-energy muon events from a 645.2-day dataset at the first phase of CJPL (CJPL-I), reconstructed their directions, and measured the cosmic-ray muon flux to be$(3.53\pm0.22_{\text{stat.}}\pm0.07_{\text{sys.}})\times10^{-10}$cm$^{-2}$s$^{-1}$. The observed angular distributions indicate the leakage of cosmic-ray muon background and agree with the simulation accounting for Jinping mountain's terrain. A survey of muon fluxes at different laboratory locations situated under mountains and belowmineshaft indicated that the former is generally a factor of $(4\pm2)$ larger than the latter with the same vertical overburden. This study provides a convenient back-of-the-envelope estimation for muon flux of an underground experiment.","['Ziyi Guo', 'Lars Bathe-Peters', 'Shaomin Chen', 'Mourad Chouaki', 'Wei Dou', 'Lei Guo', 'Ghulam Hussain', 'Jinjing Li', 'Qian Liu', 'Guang Luo', 'Wentai Luo', 'Ming Qi', 'Wenhui Shao', 'Jian Tang', 'Linyan Wan', 'Zhe Wang', 'Benda Xu', 'Tong Xu', 'Weiran Xu', 'Yuzi Yang', 'Minfang Yeh', 'Lin Zhao']","Chin.Phys.C 45 (2021) 2, 025001",arXiv,2020,https://doi.org/10.48550/arXiv.2007.15925,Anomali
MessyTable: Instance Association in Multiple Camera Views,"We present an interesting and challenging dataset that features a large number of scenes with messy tables captured from multiple camera views. Each scene in this dataset is highly complex, containing multiple object instances that could be identical, stacked and occluded by other instances. The key challenge is to associate all instances given the RGB image of all views. The seemingly simple task surprisingly fails many popular methods or heuristics that we assume good performance in object association. The dataset challenges existing methods inminingsubtle appearance differences, reasoning based on contexts, and fusing appearance with geometric cues for establishing an association. We report interesting findings with some popular baselines, and discuss how this dataset could help inspire new problems and catalyse more robust formulations to tackle real-world instance association problems. Project page:$\href{https://caizhongang.github.io/projects/MessyTable/}{\text{MessyTable}}$","['Zhongang Cai', 'Junzhe Zhang', 'Daxuan Ren', 'Cunjun Yu', 'Haiyu Zhao', 'Shuai Yi', 'Chai Kiat Yeo', 'Chen Change Loy']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.14878,Anomali
CommuNety: A Deep Learning System for the Prediction of Cohesive Social Communities,"Effectiveminingof social media, which consists of a large number of users is a challenging task. Traditional approaches rely on the analysis oftextdata related to users to accomplish this task. However,textdata lacks significant information about the social users and their associated groups. In this paper, we propose CommuNety, a deep learning system for the prediction of cohesive social networks using images. The proposed deep learning model consists of hierarchical CNN architecture to learn descriptive features related to each cohesive network. The paper also proposes a novel Face Co-occurrence Frequency algorithm to quantify existence of people in images, and a novel photo ranking method to analyze the strength of relationship between different individuals in a predicted social network. We extensively evaluate the proposed technique on PIPA dataset and compare with state-of-the-art methods. Our experimental results demonstrate the superior performance of the proposed technique for the prediction of relationship between different individuals and the cohesiveness of communities.","['Syed Afaq Ali Shah', 'Weifeng Deng', 'Jianxin Li', 'Muhammad Aamir Cheema', 'Abdul Bais']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.14741,Anomali
Enriching Video Captions With Contextual Text,"Understanding video content and generating caption with context is an important and challenging task. Unlike prior methods that typically attempt to generate generic video captions without context, our architecture contextualizes captioning by infusing extracted information from relevanttextdata. We propose an end-to-end sequence-to-sequence model which generates video captions based on visual input, andminesrelevant knowledge such as names and locations from contextualtext. In contrast to previous approaches, we do not preprocess thetextfurther, and let the model directly learn to attend over it. Guided by the visual input, the model is able to copy words from the contextualtextvia a pointer-generator network, allowing to produce more specific video captions. We show competitive performance on the News Video Dataset and, through ablation studies, validate the efficacy of contextual video captioning as well as individual design choices in our model architecture.","['Philipp Rimle', 'Pelin Dogan', 'Markus Gross']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.14682,Anomali
Improving Robustness on Seasonality-Heavy Multivariate Time Series Anomaly Detection,"Robust Anomaly Detection (AD) on time series data is a key component for monitoring many complex modern systems. These systems typically generate high-dimensional time series that can be highly noisy, seasonal, and inter-correlated. This paper explores some of the challenges in such data, and proposes a new approach that makes inroads towards increased robustness on seasonal and contaminated data, while providing a better root cause identification of anomalies. In particular, we propose the use of Robust Seasonal Multivariate Generative Adversarial Network (RSM-GAN) that extends recent advancements in GAN with the adoption of convolutional-LSTM layers and attention mechanisms to produce excellent performance on various settings. We conduct extensive experiments in which not only do this model displays more robust behavior on complex seasonality patterns, but also shows increased resistance to training data contamination. We compare it with existing classical and deep-learning AD models, and show that this architecture is associated with the lowest false positive rate and improves precision by 30% and 16% in real-world and synthetic data, respectively.","['Farzaneh Khoshnevisan', 'Zhewen Fan', 'Vitor R. Carvalho']","KDD Workshop on Mining and Learning from Time Series, 2020",arXiv,2020,https://doi.org/10.48550/arXiv.2007.14254,Anomali
Emotion Correlation Mining Through Deep Learning Models on Natural Language Text,"Emotion analysis has been attracting researchers' attention. Most previous works in the artificial intelligence field focus on recognizing emotion rather thanminingthe reason why emotions are not or wrongly recognized. Correlation among emotions contributes to the failure of emotion recognition. In this paper, we try to fill the gap between emotion recognition and emotion correlationminingthrough natural languagetextfrom web news. Correlation among emotions, expressed as the confusion and evolution of emotion, is primarily caused by human emotion cognitive bias. Tomineemotion correlation from emotion recognition throughtext, three kinds of features and two deep neural network models are presented. The emotion confusion law is extracted through orthogonal basis. The emotion evolution law is evaluated from three perspectives, one-step shift, limited-step shifts, and shortest path transfer. The method is validated using three datasets-the titles, the bodies, and the comments of news articles, covering both objective and subjectivetextsin varying lengths (long and short). The experimental results show that, in subjective comments, emotions are easily mistaken as anger. Comments tend to arouse emotion circulations of love-anger and sadness-anger. In objective news, it is easy to recognizetextemotion as love and cause fear-joy circulation. That means, journalists may try to attract attention using fear and joy words but arouse the emotion love instead; After news release, netizens generate emotional comments to express their intense emotions, i.e., anger, sadness, and love. These findings could provide insights for applications regarding affective interaction such as network public sentiment, social media communication, and human-computer interaction.","['Xinzhi Wang', 'Luyao Kou', 'Vijayan Sugumaran', 'Xiangfeng Luo', 'Hui Zhang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.14071,Anomali
YNU-HPCC at SemEval-2020 Task 8: Using a Parallel-Channel Model for Memotion Analysis,"In recent years, the growing ubiquity of Internet memes on social media platforms, such as Facebook, Instagram, and Twitter, has become a topic of immense interest. However, the classification and recognition of memes is much more complicated than that of socialtextsince it involves visual cues and language understanding. To address this issue, this paper proposed a parallel-channel model to process the textual and visual information in memes and then analyze the sentiment polarity of memes. In the shared task of identifying and categorizing memes, we preprocess the dataset according to the language behaviors on social media. Then, we adapt and fine-tune the Bidirectional Encoder Representations from Transformers (BERT), and two types of convolutional neural network models (CNNs) were used to extract the features from the pictures. We applied an ensemble model that combined the BiLSTM, BIGRU, and Attention models to perform cross domain suggestionmining. The officially released results show that our system performs better than the baseline algorithm. Our team won nineteenth place in subtask A (Sentiment Classification). The code of this paper is availabled at : https://github.com/YuanLi95/Semveal2020-Task8-emotion-analysis.","['Li Yuan', 'Jin Wang', 'Xuejie Zhang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.13968,Anomali
Large Scale Subject Category Classification of Scholarly Papers with Deep Attentive Neural Networks,"Subject categories of scholarly papers generally refer to the knowledge domain(s) to which the papers belong, examples being computer science or physics. Subject category information can be used for building faceted search for digital library search engines. This can significantly assist users in narrowing down their search space of relevant documents. Unfortunately, many academic papers do not have such information as part of their metadata. Existing methods for solving this task usually focus on unsupervised learning that often relies on citation networks. However, a complete list of papers citing the current paper may not be readily available. In particular, new papers that have few or no citations cannot be classified using such methods. Here, we propose a deep attentive neural network (DANN) that classifies scholarly papers using only their abstracts. The network is trained using 9 million abstracts from Web of Science (WoS). We also use the WoS schema that covers 104 subject categories. The proposed network consists of two bi-directional recurrent neural networks followed by an attention layer. We compare our model against baselines by varying the architecture andtextrepresentation. Our best model achieves micro-F1 measure of 0.76 with F1 of individual subject categories ranging from 0.50-0.95. The results showed the importance of retraining word embedding models to maximize the vocabulary overlap and the effectiveness of the attention mechanism. The combination of word vectors with TFIDF outperforms character and sentence level embedding models. We discuss imbalanced samples and overlapping categories and suggest possible strategies for mitigation. We also determine the subject category distribution in CiteSeerX by classifying a random sample of one million academic papers.","['Bharath Kandimalla', 'Shaurya Rohatgi', 'Jian Wu', 'C Lee Giles']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.13826,Anomali
Named entity recognition in chemical patents using ensemble of contextual language models,"Chemical patent documents describe a broad range of applications holding key reaction and compound information, such as chemical structure, reaction formulas, and molecular properties. These informational entities should be first identified intextpassages to be utilized in downstream tasks.Textminingprovides means to extract relevant information from chemical patents through information extraction techniques. As part of the Information Extraction task of the Cheminformatics Elsevier Melbourne University challenge, in this work we study the effectiveness of contextualized language models to extract reaction information in chemical patents. We assess transformer architectures trained on a generic and specialised corpora to propose a new ensemble model. Our best model, based on a majority ensemble approach, achieves an exact F1-score of 92.30% and a relaxed F1-score of 96.24%. The results show that ensemble of contextualized language models can provide an effective method to extract information from chemical patents.","['Jenny Copara', 'Nona Naderi', 'Julien Knafou', 'Patrick Ruch', 'Douglas Teodoro']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.12569,Anomali
Curriculum Vitae Recommendation Based on Text Mining,"During the last years, the development in diverse areas related to computer science and internet, allowed to generate new alternatives for decision making in the selection of personnel for state and private companies. In order to optimize this selection process, the recommendation systems are the most suitable for working with explicit information related to the likes and dislikes of employers or end users, since this information allows to generate lists of recommendations based on collaboration or similarity of content. Therefore, this research takes as a basis these characteristics contained in the database of curricula and job offers, which correspond to the Peruvian ambit, which highlights the experience, knowledge and skills of each candidate, which are described in textual terms or words. This research focuses on the problem: how we can take advantage from the growth of unstructured information about job offers and curriculum vitae on different websites for CV recommendation. So, we use the techniques fromTextMiningand Natural Language Processing. Then, as a relevant technique for the present study, we emphasize the technique frequency of the Term - Inverse Frequency of the documents (TF-IDF), which allows identifying the most relevant CVs in relation to a job offer of website through the average values (TF-IDF). So, the weighted value can be used as a qualification value of the relevant curriculum vitae for the recommendation.","['Honorio Apaza Alanoca', 'Americo A. Rubin de Celis Vidal', 'Josimar Edinson Chire Saire']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.11053,Anomali
BAKSA at SemEval-2020 Task 9: Bolstering CNN with Self-Attention for Sentiment Analysis of Code Mixed Text,"Sentiment Analysis of code-mixedtexthas diversified applications in opinionminingranging from tagging user reviews to identifying social or political sentiments of a sub-population. In this paper, we present an ensemble architecture of convolutional neural net (CNN) and self-attention based LSTM for sentiment analysis of code-mixed tweets. While the CNN component helps in the classification of positive and negative tweets, the self-attention based LSTM, helps in the classification of neutral tweets, because of its ability to identify correct sentiment among multiple sentiment bearing units. We achieved F1 scores of 0.707 (ranked 5th) and 0.725 (ranked 13th) on Hindi-English (Hinglish) and Spanish-English (Spanglish) datasets, respectively. The submissions for Hinglish and Spanglish tasks were made under the usernames ayushk and harsh_6 respectively.","['Ayush Kumar', 'Harsh Agarwal', 'Keshav Bansal', 'Ashutosh Modi']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.10819,Anomali
Inferring Political Preferences from Twitter,"Sentiment analysis is the task of automatic analysis of opinions and emotions of users towards an entity or some aspect of that entity. Political Sentiment Analysis of social media helps the political strategists to scrutinize the performance of a party or candidate and improvise their weaknesses far before the actual elections. During the time of elections, the social networks get flooded with blogs, chats, debates and discussions about the prospects of political parties and politicians. The amount of data generated is much large to study, analyze and draw inferences using the latest techniques. Twitter is one of the most popular social media platforms enables us to perform domain-specific data preparation. In this work, we chose to identify the inclination of political opinions present in Tweets by modelling it as atextclassification problem using classical machine learning. The tweets related to the Delhi Elections in 2020 are extracted and employed for the task. Among the several algorithms, we observe that Support Vector Machines portrays the best performance.","['Mohd Zeeshan Ansari', 'Areesha Fatima Siddiqui', 'Mohammad Anas']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.10604,Anomali
Coronavirus Knowledge Graph: A Case Study,"The emergence of the novel COVID-19 pandemic has had a significant impact on global healthcare and the economy over the past few months. The virus's rapid widespread has led to a proliferation in biomedical research addressing the pandemic and its related topics. One of the essential Knowledge Discovery tools that could help the biomedical research community understand and eventually find a cure for COVID-19 are Knowledge Graphs. The CORD-19 dataset is a collection of publicly available full-textresearch articles that have been recently published on COVID-19 and coronavirus topics. Here, we use several Machine Learning, Deep Learning, and Knowledge Graph construction andminingtechniques to formalize and extract insights from the PubMed dataset and the CORD-19 dataset to identify COVID-19 related experts and bio-entities. Besides, we suggest possible techniques to predict related diseases, drug candidates, gene, gene mutations, and related compounds as part of a systematic effort to apply Knowledge Discovery methods to help biomedical researchers tackle the pandemic.","['Chongyan Chen', 'Islam Akef Ebeid', 'Yi Bu', 'Ying Ding']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.10287,Anomali
Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding,"Mininga set of meaningful topics organized into a hierarchy is intuitively appealing since topic correlations are ubiquitous in massivetextcorpora. To account for potential hierarchical topic structures, hierarchical topic models generalize flat topic models by incorporating latent topic hierarchies into their generative modeling process. However, due to their purely unsupervised nature, the learned topic hierarchy often deviates from users' particular needs or interests. To guide the hierarchical topic discovery process with minimal user supervision, we propose a new task, Hierarchical TopicMining, which takes a category tree described by category names only, and aims tominea set of representative terms for each category from atextcorpus to help a user comprehend his/her interested topics. We develop a novel joint tree andtextembedding method along with a principled optimization procedure that allows simultaneous modeling of the category tree structure and the corpus generative process in the spherical space for effective category-representative term discovery. Our comprehensive experiments show that our model, named JoSH,minesa high-quality set of hierarchical topics with high efficiency and benefits weakly-supervised hierarchicaltextclassification tasks.","['Yu Meng', 'Yunyi Zhang', 'Jiaxin Huang', 'Yu Zhang', 'Chao Zhang', 'Jiawei Han']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.09536,Anomali
Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype Mining and Language-Dependent Score Normalization,"In this paper we describe the top-scoring IDLab submission for thetext-independent task of the Short-duration Speaker Verification (SdSV) Challenge 2020. The main difficulty of the challenge exists in the large degree of varying phonetic overlap between the potentially cross-lingual trials, along with the limited availability of in-domain DeepMine Farsi training data. We introduce domain-balanced hard prototypeminingto fine-tune the state-of-the-art ECAPA-TDNN x-vector based speaker embedding extractor. The sampleminingtechnique efficiently exploits speaker distances between the speaker prototypes of the popular AAM-softmax loss function to construct challenging training batches that are balanced on the domain-level. To enhance the scoring of cross-lingual trials, we propose a language-dependent s-norm score normalization. The imposter cohort only contains data from the Farsi target-domain which simulates the enrollment data always being Farsi. In case a Gaussian-Backend language model detects the test speaker embedding to contain English, a cross-language compensation offset determined on the AAM-softmax speaker prototypes is subtracted from the maximum expected imposter mean score. A fusion of five systems with minor topological tweaks resulted in a final MinDCF and EER of 0.065 and 1.45% respectively on the SdSVC evaluation set.","['Jenthe Thienpondt', 'Brecht Desplanques', 'Kris Demuynck']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.07689,Anomali
Emoji Prediction: Extensions and Benchmarking,"Emojis are a succinct form of language which can express concrete meanings, emotions, and intentions. Emojis also carry signals that can be used to better understand communicative intent. They have become a ubiquitous part of our daily lives, making them an important part of understanding user-generated content. The emoji prediction task aims at predicting the proper set of emojis associated with a piece oftext. Through emoji prediction, models can learn rich representations of the communicative intent of the writtentext. While existing research on the emoji prediction task focus on a small subset of emoji types closely related to certain emotions, this setting oversimplifies the task and wastes the expressive power of emojis. In this paper, we extend the existing setting of the emoji prediction task to include a richer set of emojis and to allow multi-label classification on the task. We propose novel models for multi-class and multi-label emoji prediction based on Transformer networks. We also construct multiple emoji prediction datasets from Twitter using heuristics. The BERT models achieve state-of-the-art performances on all our datasets under all the settings, with relative improvements of 27.21% to 236.36% in accuracy, 2.01% to 88.28% in top-5 accuracy and 65.19% to 346.79% in F-1 score, compared to the prior state-of-the-art. Our results demonstrate the efficacy of deep Transformer-based models on the emoji prediction task. We also release our datasets at https://github.com/hikari-NYU/Emoji_Prediction_Datasets_MMS for future researchers.","['Weicheng Ma', 'Ruibo Liu', 'Lili Wang', 'Soroush Vosoughi']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.07389,Anomali
"The origins of nearly coplanar, non-resonant systems of close-in super-Earths","Some systems of close-in ""super-Earths"" contain five or more planets on non-resonant but compact and nearly coplanar orbits. The Kepler-11 system is an iconic representative of this class of system. It is challenging to explain their origins given that planet-disk interactions are thought to be essential to maintain such a high degree of coplanarity, yet these same interactions invariably cause planets to migrate into chains of mean motion resonances. Here weminea large dataset of dynamical simulations of super-Earth formation by migration. These simulations match the observed period ratio distribution as long as the vast majority of planet pairs in resonance become dynamically unstable. When instabilities take place resonances are broken during a late phase of giant impacts, and typical surviving systems have planet pairs with significant mutual orbital inclinations. However, a subset of our unstable simulations matches the Kepler-11 system in terms of coplanarity, compactness, planet-multiplicity and non-resonant state. This subset have dynamical instability phases typically much shorter than ordinary systems. Unstable systems may keep a high degree of coplanarity post-instability if planets collide at very low orbital inclinations ($\lesssim1^\circ$) or if collisions promote efficient damping of orbital inclinations. If planetary scattering during the instability takes place at low orbital inclinations ($\text{i}\lesssim1^\circ$), orbital inclinations are barely increased by encounters before planets collide.When planetary scattering pumps orbital inclinations to higher values ($\gtrsim 1^\circ$) planets tend to collide at higher mutual orbital inclinations, but depending on the geometry of collisions mergers' orbital inclinations may be efficiently damped. Each of these formation pathways can produce analogues to the Kepler-11 system.","['Leandro Esteves', 'André Izidoro', 'Sean N. Raymond', 'Bertram Bitsch']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.07385,Anomali
Transfer learning extensions for the probabilistic classification vector machine,"Transfer learning is focused on the reuse of supervised learning models in a new context. Prominent applications can be found in robotics, image processing or webmining. In these fields, the learning scenarios are naturally changing but often remain related to each other motivating the reuse of existing supervised models. Current transfer learning models are neither sparse nor interpretable. Sparsity is very desirable if the methods have to be used in technically limited environments and interpretability is getting more critical due to privacy regulations. In this work, we propose two transfer learning extensions integrated into the sparse and interpretable probabilistic classification vector machine. They are compared to standard benchmarks in the field and show their relevance either by sparsity or performance improvements.","['Christoph Raab', 'Frank-Michael Schleif']","Neurocomputing, Volume 397, 2020, Pages 320-330, ISSN 0925-2312",arXiv,2020,https://doi.org/10.48550/arXiv.2007.07090,Anomali
GGPONC: A Corpus of German Medical Text with Rich Metadata Based on Clinical Practice Guidelines,"The lack of publicly accessibletextcorpora is a major obstacle for progress in natural language processing. For medical applications, unfortunately, all language communities other than English are low-resourced. In this work, we present GGPONC (German Guideline Program in Oncology NLP Corpus), a freely distributable German language corpus based on clinical practice guidelines for oncology. This corpus is one of the largest ever built from German medical documents. Unlike clinical documents, clinical guidelines do not contain any patient-related information and can therefore be used without data protection restrictions. Moreover, GGPONC is the first corpus for the German language covering diverse conditions in a large medical subfield and provides a variety of metadata, such as literature references and evidence levels. By applying and evaluating existing medical information extraction pipelines for Germantext, we are able to draw comparisons for the use of medical language to other corpora, medical and non-medical ones.","['Florian Borchert', 'Christina Lohr', 'Luise Modersohn', 'Thomas Langer', 'Markus Follmann', 'Jan Philipp Sachs', 'Udo Hahn', 'Matthieu-P. Schapranow']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.06400,Anomali
An Alternating Rank-K Nonnegative Least Squares Framework (ARkNLS) for Nonnegative Matrix Factorization,"Nonnegative matrix factorization (NMF) is a prominent technique for data dimensionality reduction that has been widely used fortextmining, computer vision, pattern discovery, and bioinformatics. In this paper, a framework called ARkNLS (Alternating Rank-k Nonnegativity constrained Least Squares) is proposed for computing NMF. First, a recursive formula for the solution of the rank-k nonnegativity-constrained least squares (NLS) is established. This recursive formula can be used to derive the closed-form solution for the Rank-k NLS problem for any integer k $\ge$ 1. As a result, each subproblem for an alternating rank-k nonnegative least squares framework can be obtained based on this closed form solution. Assuming that all matrices involved in rank-k NLS in the context of NMF computation are of full rank, two of the currently best NMF algorithms HALS (hierarchical alternating least squares) and ANLS-BPP (Alternating NLS based on Block Principal Pivoting) can be considered as special cases of ARkNLS with k = 1 and k = r for rank r NMF, respectively. This paper is then focused on the framework with k = 3, which leads to a new algorithm for NMF via the closed-form solution of the rank-3 NLS problem. Furthermore, a new strategy that efficiently overcomes the potential singularity problem in rank-3 NLS within the context of NMF computation is also presented. Extensive numerical comparisons using real and synthetic data sets demonstrate that the proposed algorithm provides state-of-the-art performance in terms of computational accuracy and cpu time.","['Delin Chu', 'Wenya Shi', 'Srinivas Eswar', 'Haesun Park']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.06118,Anomali
Deep or Simple Models for Semantic Tagging? It Depends on your Data [Experiments],"Semantic tagging, which has extensive applications intextmining, predicts whether a given piece oftextconveys the meaning of a given semantic tag. The problem of semantic tagging is largely solved with supervised learning and today, deep learning models are widely perceived to be better for semantic tagging. However, there is no comprehensive study supporting the popular belief. Practitioners often have to train different types of models for each semantic tagging task to identify the best model. This process is both expensive and inefficient.
  We embark on a systematic study to investigate the following question: Are deep models the best performing model for all semantic tagging tasks? To answer this question, we compare deep models against ""simple models"" over datasets with varying characteristics. Specifically, we select three prevalent deep models (i.e. CNN, LSTM, and BERT) and two simple models (i.e. LR and SVM), and compare their performance on the semantic tagging task over 21 datasets. Results show that the size, the label ratio, and the label cleanliness of a dataset significantly impact the quality of semantic tagging. Simple models achieve similar tagging quality to deep models on large datasets, but the runtime of simple models is much shorter. Moreover, simple models can achieve better tagging quality than deep models when targeting datasets show worse label cleanliness and/or more severe imbalance. Based on these findings, our study can systematically guide practitioners in selecting the right learning model for their semantic tagging task.","['Jinfeng Li', 'Yuliang Li', 'Xiaolan Wang', 'Wang-Chiew Tan']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.05651,Anomali
DISCO PAL: Diachronic Spanish Sonnet Corpus with Psychological and Affective Labels,"Nowadays, there are many applications oftextminingover corpora from different languages. However, most of them are based ontextsin prose, lacking applications that work with poetrytexts. An example of an application oftextminingin poetry is the usage of features derived from their individual words in order to capture the lexical, sublexical and interlexical meaning, and infer the General Affective Meaning (GAM) of thetext. However, even though this proposal has been proved as useful for poetry in some languages, there is a lack of studies for both Spanish poetry and for highly-structured poetic compositions such as sonnets. This article presents a study over an annotated corpus of Spanish sonnets, in order to analyse if it is possible to build features from their individual words for predicting their GAM. The purpose of this is to model sonnets at an affective level. The article also analyses the relationship between the GAM of the sonnets and the content itself. For this, we consider the content from a psychological perspective, identifying with tags when a sonnet is related to a specific term. Then, we study how GAM changes according to each of those psychological terms.
  The corpus used contains 274 Spanish sonnets from authors of different centuries, from 15th to 19th. This corpus was annotated by different domain experts. The experts annotated the poems with affective and lexico-semantic features, as well as with domain concepts that belong to psychology. Thanks to this, the corpus of sonnets can be used in different applications, such as poetry recommender systems, personalitytextminingstudies of the authors, or the usage of poetry for therapeutic purposes.","['Alberto Barbado', 'Víctor Fresno', 'Ángeles Manjarrés Riesco', 'Salvador Ros']","Language Resources and Evaluation, 13 October 2021",arXiv,2021,https://doi.org/10.48550/arXiv.2007.04626,Anomali
A Survey of Real-Time Social-Based Traffic Detection,"Online traffic news web sites do not always announce traffic events in areas in real-time. There is a capability to employtextminingand machine learning techniques on the twitter stream to perform event detection, in order to develop a real-time traffic detection system. In this present survey paper, we will deliberate the current state-of-art techniques in detecting traffic events in real-time focusing on five papers [1, 2, 3, 4, 5]. Lastly, applyingtextminingtechniques and SVM classifiers in paper [2] gave the best results (i.e. 95.75% accuracy and 95.8% F1-score).",['Hashim Abu-gellban'],,arXiv,2020,https://doi.org/10.48550/arXiv.2007.04100,Anomali
An Evaluation of Two Commercial Deep Learning-Based Information Retrieval Systems for COVID-19 Literature,"The COVID-19 pandemic has resulted in a tremendous need for access to the latest scientific information, primarily through the use oftextminingand search tools. This has led to both corpora for biomedical articles related to COVID-19 (such as the CORD-19 corpus (Wang et al., 2020)) as well as search engines to query such data. While most research in search engines is performed in the academic field of information retrieval (IR), most academic search engines$\unicode{x2013}$though rigorously evaluated$\unicode{x2013}$are sparsely utilized, while major commercial web search engines (e.g., Google, Bing) dominate. This relates to COVID-19 because it can be expected that commercial search engines deployed for the pandemic will gain much higher traction than those produced in academic labs, and thus leads to questions about the empirical performance of these search tools. This paper seeks to empirically evaluate two such commercial search engines for COVID-19, produced by Google and Amazon, in comparison to the more academic prototypes evaluated in the context of the TREC-COVID track (Roberts et al., 2020). We performed several steps to reduce bias in the available manual judgments in order to ensure a fair comparison of the two systems with those submitted to TREC-COVID. We find that the top-performing system from TREC-COVID on bpref metric performed the best among the different systems evaluated in this study on all the metrics. This has implications for developing biomedical retrieval systems for future health crises as well as trust in popular health search engines.","['Sarvesh Soni', 'Kirk Roberts']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.03106,Anomali
Sentiment Analysis on Customer Responses,"Sentiment analysis is one of the fastest spreading research areas in computer science, making it challenging to keep track of all the activities in the area. We present a customer feedback reviews on product, where we utilize opinionmining,textminingand sentiments, which has affected the surrounded world by changing their opinion on a specific product. Data used in this study are online product reviews collected from Amazon.com. We performed a comparative sentiment analysis of retrieved reviews. This research paper provides you with sentimental analysis of various smart phone opinions on smart phones dividing them Positive, Negative and Neutral Behaviour.","['Antony Samuels', 'John Mcgonical']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.02237,Anomali
Sentiment Analysis on Social Media Content,"Nowadays, people from all around the world use social media sites to share information. Twitter for example is a platform in which users send, read posts known as tweets and interact with different communities. Users share their daily lives, post their opinions on everything such as brands and places. Companies can benefit from this massive platform by collecting data related to opinions on them. The aim of this paper is to present a model that can perform sentiment analysis of real data collected from Twitter. Data in Twitter is highly unstructured which makes it difficult to analyze. However, our proposed model is different from prior work in this field because it combined the use of supervised and unsupervised machine learning algorithms. The process of performing sentiment analysis as follows: Tweet extracted directly from Twitter API, then cleaning and discovery of data performed. After that the data were fed into several models for the purpose of training. Each tweet extracted classified based on its sentiment whether it is a positive, negative or neutral. Data were collected on two subjects McDonalds and KFC to show which restaurant has more popularity. Different machine learning algorithms were used. The result from these models were tested using various testing metrics like cross validation and f-score. Moreover, our model demonstrates strong performance onminingtextsextracted directly from Twitter.","['Antony Samuels', 'John Mcgonical']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.02144,Anomali
Data-Mining Element Charges in Inorganic Materials,"Oxidation states are well-established in chemical science teaching and research. We data-minemore than 168,000 crystallographic reports to find an optimal allocation of oxidation states to each element. In doing so we uncover discrepancies betweentext-book chemistry and reported charge states observed in materials. We go on to show how the oxidation states we recommend can significantly facilitate materials discovery and heuristic design of novel inorganic compounds.","['Yu Ding', 'Yu Kumagai', 'Fumiyasu Oba', 'Lee A. Burton']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.02085,Anomali
Language-agnostic BERT Sentence Embedding,"While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning (Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019), dual encoder translation ranking (Guo et al., 2018), and additive margin softmax (Yang et al., 2019a). We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80%. Composing the best of these methods produces a model that achieves 83.7% bi-textretrieval accuracy over 112 languages on Tatoeba, well above the 65.5% achieved by Artetxe and Schwenk (2019b), while still performing competitively on monolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel dataminedfrom CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.","['Fangxiaoyu Feng', 'Yinfei Yang', 'Daniel Cer', 'Naveen Arivazhagan', 'Wei Wang']",,arXiv,2022,https://doi.org/10.48550/arXiv.2007.01852,Anomali
How circular economy and industrial ecology concepts are intertwined? A bibliometric and text mining analysis,"Combining new insights from both bibliometric andtextmininganalyses, with prior relevant research conversations on circular economy (CE) and industrial ecology (IE), this paper aims to clarify the recent development trends and relations between these concepts, including their representations and applications. On this basis, discussions are made and recommendations provided on how CE and IE approaches, tools, and indicators can complement each other to enable and catalyze a more circular and sustainable development, by supporting sustainable policy-making and monitoring sound CE strategies in industrial practices.","['Michael Saidani', 'Bernard Yannou', 'Yann Leroy', 'François Cluzel', 'Harrison Kim']","Online Symposium on Circular Economy and Sustainability, Jul 2020, Alexandroupolis, Greece",arXiv,2020,https://doi.org/10.48550/arXiv.2007.00927,Anomali
So What's the Plan? Mining Strategic Planning Documents,"In this paper we present a corpus of Russian strategic planning documents, RuREBus. This project is grounded both from language technology and e-government perspectives. Not only new language sources and tools are being developed, but also their applications to e-goverment research. We demonstrate the pipeline for creating atextcorpus from scratch. First, the annotation schema is designed. Nexttextsare marked up using human-in-the-loop strategy, so that preliminary annotations are derived from a machine learning model and are manually corrected. The amount of annotatedtextsis large enough to showcase what insights can be gained from RuREBus.","['Ekaterina Artemova', 'Tatiana Batura', 'Anna Golenkovskaya', 'Vitaly Ivanin', 'Vladimir Ivanov', 'Veronika Sarkisyan', 'Ivan Smurov', 'Elena Tutubalina']",,arXiv,2020,https://doi.org/10.48550/arXiv.2007.00257,Anomali
A Data-driven Neural Network Architecture for Sentiment Analysis,"The fabulous results of convolution neural networks in image-related tasks, attracted attention oftextmining, sentiment analysis and othertextanalysis researchers. It is however difficult to find enough data for feeding such networks, optimize their parameters, and make the right design choices when constructing network architectures. In this paper we present the creation steps of two big datasets of song emotions. We also explore usage of convolution and max-pooling neural layers on song lyrics, product and movie reviewtextdatasets. Three variants of a simple and flexible neural network architecture are also compared. Our intention was to spot any important patterns that can serve as guidelines for parameter optimization of similar models. We also wanted to identify architecture design choices which lead to high performing sentiment analysis models. To this end, we conducted a series of experiments with neural architectures of various configurations. Our results indicate that parallel convolutions of filter lengths up to three are usually enough for capturing relevanttextfeatures. Also, max-pooling region size should be adapted to the length oftextdocuments for producing the best feature maps. Top results we got are obtained with feature maps of lengths 6 to 18. An improvement on future neural network models for sentiment analysis, could be generating sentiment polarity prediction of documents using aggregation of predictions on smaller excerpt of the entiretext.","['Erion Çano', 'Maurizio Morisio']","Data Technologies and Applications, Vol. 53, No. 1, pp. 2-19, 2019",arXiv,2020,https://doi.org/10.48550/arXiv.2006.16642,Anomali
Answering Questions on COVID-19 in Real-Time,"The recent outbreak of the novel coronavirus is wreaking havoc on the world and researchers are struggling to effectively combat it. One reason why the fight is difficult is due to the lack of information and knowledge. In this work, we outline our effort to contribute to shrinking this knowledge vacuum by creating covidAsk, a question answering (QA) system that combines biomedicaltextminingand QA techniques to provide answers to questions in real-time. Our system also leverages information retrieval (IR) approaches to provide entity-level answers that are complementary to QA models. Evaluation of covidAsk is carried out by using a manually created dataset called COVID-19 Questions which is based on information from various sources, including the CDC and the WHO. We hope our system will be able to aid researchers in their search for knowledge and information not only for COVID-19, but for future pandemics as well.","['Jinhyuk Lee', 'Sean S. Yi', 'Minbyul Jeong', 'Mujeen Sung', 'Wonjin Yoon', 'Yonghwa Choi', 'Miyoung Ko', 'Jaewoo Kang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.15830,Anomali
Seasonal Averaged One-Dependence Estimators: A Novel Algorithm to Address Seasonal Concept Drift in High-Dimensional Stream Classification,"Stream classification methods classify a continuous stream of data as new labelled samples arrive. They often also have to deal with concept drift. This paper focuses on seasonal drift in stream classification, which can be found in many real-world application data sources. Traditional approaches of stream classification consider seasonal drift by including seasonal dummy/indicator variables or building separate models for each season. But these approaches have strong limitations in high-dimensional classification problems, or with complex seasonal patterns. This paper explores how to best handle seasonal drift in the specific context of news article categorization (or classification/tagging), where seasonal drift is overwhelmingly the main type of drift present in the data, and for which the data are high-dimensional. We introduce a novel classifier named Seasonal Averaged One-Dependence Estimators (SAODE), which extends the AODE classifier to handle seasonal drift by including time as a super parent. We assess our SAODE model using two large real-worldtextminingrelated datasets each comprising approximately a million records, against nine state-of-the-art stream and concept drift classification models, with and without seasonal indicators and with separate models built for each season. Across five different evaluation techniques, we show that our model consistently outperforms other methods by a large margin where the results are statistically significant.","['Rakshitha Godahewa', 'Trevor Yann', 'Christoph Bergmeir', 'Francois Petitjean']",International Joint Conference on Neural Networks (IJCNN 2020),arXiv,2020,https://doi.org/10.48550/arXiv.2006.15311,Anomali
Attention-Based Neural Networks for Sentiment Attitude Extraction using Distant Supervision,"In the sentiment attitude extraction task, the aim is to identify <<attitudes>> -- sentiment relations between entities mentioned intext. In this paper, we provide a study on attention-based context encoders in the sentiment attitude extraction task. For this task, we adapt attentive context encoders of two types: (1) feature-based; (2) self-based. In our study, we utilize the corpus of Russian analyticaltextsRuSentRel and automatically constructed news collection RuAttitudes for enriching the training set. We consider the problem of attitude extraction as two-class (positive, negative) and three-class (positive, negative, neutral) classification tasks for whole documents. Our experiments with the RuSentRel corpus show that the three-class classification models, which employ the RuAttitudes corpus for training, result in 10% increase and extra 3% by F1, when model architectures include the attention mechanism. We also provide the analysis of attention weight distributions in dependence on the term type.","['Nicolay Rusnachenko', 'Natalia Loukachevitch']","The 10th International Conference on Web Intelligence, Mining and Semantics (WIMS 2020), June 30-July 3, 2020, Biarritz, France",arXiv,2020,this https URL,Anomali
A Neural Network for Determination of Latent Dimensionality in Nonnegative Matrix Factorization,"Non-negative Matrix Factorization (NMF) has proven to be a powerful unsupervised learning method for uncovering hidden features in complex and noisy data sets with applications in datamining,textrecognition, dimension reduction, face recognition, anomaly detection, blind source separation, and many other fields. An important input for NMF is the latent dimensionality of the data, that is, the number of hidden features, K, present in the explored data set. Unfortunately, this quantity is rarely known a priori. We utilize a supervised machine learning approach in combination with a recent method for model determination, called NMFk, to determine the number of hidden features automatically. NMFk performs a set of NMF simulations on an ensemble of matrices, obtained by bootstrapping the initial data set, and determines which K produces stable groups of latent features that reconstruct the initial data set well. We then train a Multi-Layer Perceptron (MLP) classifier network to determine the correct number of latent features utilizing the statistics and characteristics of the NMF solutions, obtained from NMFk. In order to train the MLP classifier, a training set of 58,660 matrices with predetermined latent features were factorized with NMFk. The MLP classifier in conjunction with NMFk maintains a greater than 95% success rate when applied to a held out test set. Additionally, when applied to two well-known benchmark data sets, the swimmer and MIT face data, NMFk/MLP correctly recovered the established number of hidden features. Finally, we compared the accuracy of our method to the ARD, AIC and Stability-based methods.","['Benjamin T. Nebgen', 'Raviteja Vangara', 'Miguel A. Hombrados-Herrera', 'Svetlana Kuksova', 'Boian S. Alexandrov']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.12402,Anomali
Extracting Topics from Open Educational Resources,"In recent years, Open Educational Resources (OERs) were earmarked as critical when mitigating the increasing need for education globally. Obviously, OERs have high-potential to satisfy learners in many different circumstances, as they are available in a wide range of contexts. However, the low-quality of OER metadata, in general, is one of the main reasons behind the lack of personalised services such as search and recommendation. As a result, the applicability of OERs remains limited. Nevertheless, OER metadata about covered topics (subjects) is essentially required by learners to build effective learning pathways towards their individual learning objectives. Therefore, in this paper, we report on a work in progress project proposing an OER topic extraction approach, applyingtextminingtechniques, to generate high-quality OER metadata about topic distribution. This is done by: 1) collecting 123 lectures from Coursera and Khan Academy in the area of data science related skills, 2) applying Latent Dirichlet Allocation (LDA) on the collected resources in order to extract existing topics related to these skills, and 3) defining topic distributions covered by a particular OER. To evaluate our model, we used the data-set of educational resources from Youtube, and compared our topic distribution results with their manually defined target topics with the help of 3 experts in the area of data science. As a result, our model extracted topics with 79% of F1-score.","['Mohammadreza Molavi', 'Mohammadreza Tavakoli', 'Gábor Kismihók']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.11109,Anomali
Recommendations for Emerging Air Taxi Network Operations based on Online Review Analysis of Helicopter Services,"The effects of traffic congestion are adverse, primarily including air pollution, commuter stress, and an increase in vehicle operating costs and accidents on the road. In efforts to alleviate these problems in metropolitan cities, logistics companies plan to introduce a new method of everyday commute called air taxis, an Urban Air Mobility (UAM) service. These are electric-powered vehicles that are expected to operate in the forthcoming years by international transportation companies like Airbus, Uber, and Kitty Hawk. Since these flying taxis are emerging mode of transportation, it is necessary to provide recommendations for the initial design, implementation, and operation. This study proposes managerial insights for these upcoming services by analyzing online customer reviews and conducting an internal assessment of helicopter operations. Helicopters are similar to air taxis in regards to their operations, and therefore, customer reviews pertaining to the former can enable us to obtain insights into the strengths and weaknesses of the short-distance aviation service, in general. A four-stage sequential approach is used in this research, wherein the online reviews areminedin Stage 1, analyzed using the bigram and trigram models in Stage 2, 7S internal assessment is conducted for helicopter services in Stage 3, and managerial recommendations for air taxis are proposed in Stage 4. The insights obtained in this paper could assist any air taxi companies in providing better customer service when they venture into the market.
  Keywords: Air taxi; Emerging technology; Urban Air Mobility (UAM); Helicopter services; Online customer reviews;Textanalytics;","['Suchithra Rajendran', 'Emily Pagel']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.10898,Anomali
Similarity of Precursors in Solid-state Synthesis as Text-Mined from Scientific Literature,"Collecting and analyzing the vast amount of information available in the solid-state chemistry literature may accelerate our understanding of materials synthesis. However, one major problem is the difficulty of identifying which materials from a synthesis paragraph are precursors or are target materials. In this study, we developed a two-step Chemical Named Entity Recognition (CNER) model to identify precursors and targets, based on information from the context around material entities. Using the extracted data, we conducted a meta-analysis to study the similarities and differences between precursors in the context of solid-state synthesis. To quantify precursor similarity, we built a substitution model to calculate the viability of substituting one precursor with another while retaining the target. From a hierarchical clustering of the precursors, we demonstrate that ""chemical similarity"" of precursors can be extracted fromtextdata. Quantifying the similarity of precursors helps provide a foundation for suggesting candidate reactants in a predictive synthesis model.","['Tanjin He', 'Wenhao Sun', 'Haoyan Huo', 'Olga Kononova', 'Ziqin Rong', 'Vahe Tshitoyan', 'Tiago Botari', 'Gerbrand Ceder']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.10315,Anomali
Deep Exogenous and Endogenous Influence Combination for Social Chatter Intensity Prediction,"Modeling user engagement dynamics on social media has compelling applications in user-persona detection and political discoursemining. Most existing approaches depend heavily on knowledge of the underlying user network. However, a large number of discussions happen on platforms that either lack any reliable social network or reveal only partially the inter-user ties (Reddit, Stackoverflow). Many approaches require observing a discussion for some considerable period before they can make useful predictions. In real-time streaming scenarios, observations incur costs. Lastly, most models do not capture complex interactions between exogenous events (such as news articles published externally) and in-network effects (such as follow-up discussions on Reddit) to determine engagement levels.
  To address the three limitations noted above, we propose a novel framework, ChatterNet, which, to our knowledge, is the first that can model and predict user engagement without considering the underlying user network. Given streams of timestamped news articles and discussions, the task is to observe the streams for a short period leading up to a time horizon, then predict chatter: the volume of discussions through a specified period after the horizon. ChatterNet processestextfrom news and discussions using a novel time-evolving recurrent network architecture that captures both temporal properties within news and discussions, as well as the influence of news on discussions. We report on extensive experiments using a two-month-long discussion corpus of Reddit, and a contemporaneous corpus of online news articles from the Common Crawl. ChatterNet shows considerable improvements beyond recent state-of-the-art models of engagement prediction. Detailed studies controlling observation and prediction windows, over 43 different subreddits, yield further useful insights.","['Subhabrata Dutta', 'Sarah Masud', 'Soumen Chakrabarti', 'Tanmoy Chakraborty']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.07812,Anomali
COVID-19-CT-CXR: a freely accessible and weakly labeled chest X-ray and CT image collection on COVID-19 from biomedical literature,"The latest threat to global health is the COVID-19 outbreak. Although there exist large datasets of chest X-rays (CXR) and computed tomography (CT) scans, few COVID-19 image collections are currently available due to patient privacy. At the same time, there is a rapid growth of COVID-19-relevant articles in the biomedical literature. Here, we present COVID-19-CT-CXR, a public database of COVID-19 CXR and CT images, which are automatically extracted from COVID-19-relevant articles from the PubMed Central Open Access (PMC-OA) Subset. We extracted figures, associated captions, and relevant figure descriptions in the article and separated compound figures into subfigures. We also designed a deep-learning model to distinguish them from other figure types and to classify them accordingly. The final database includes 1,327 CT and 263 CXR images (as of May 9, 2020) with their relevanttext. To demonstrate the utility of COVID-19-CT-CXR, we conducted four case studies. (1) We show that COVID-19-CT-CXR, when used as additional training data, is able to contribute to improved DL performance for the classification of COVID-19 and non-COVID-19 CT. (2) We collected CT images of influenza and trained a DL baseline to distinguish a diagnosis of COVID-19, influenza, or normal or other types of diseases on CT. (3) We trained an unsupervised one-class classifier from non-COVID-19 CXR and performed anomaly detection to detect COVID-19 CXR. (4) Fromtext-minedcaptions and figure descriptions, we compared clinical symptoms and clinical findings of COVID-19 vs. those of influenza to demonstrate the disease differences in the scientific publications. We believe that our work is complementary to existing resources and hope that it will contribute to medical image analysis of the COVID-19 pandemic. The dataset, code, and DL models are publicly available at https://github.com/ncbi-nlp/COVID-19-CT-CXR.","['Yifan Peng', 'Yu-Xing Tang', 'Sungwon Lee', 'Yingying Zhu', 'Ronald M. Summers', 'Zhiyong Lu']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.06177,Anomali
Spatial-Temporal Dynamic Graph Attention Networks for Ride-hailing Demand Prediction,"Ride-hailing demand prediction is an essential task in spatial-temporal datamining. Accurate Ride-hailing demand prediction can help to pre-allocate resources, improve vehicle utilization and user experiences. Graph Convolutional Networks (GCN) is commonly used to model the complicated irregular non-Euclidean spatial correlations. However, existing GCN-based ride-hailing demand prediction methods only assign the same importance to different neighbor regions, and maintain a fixed graph structure with static spatial relationships throughout the timeline when extracting the irregular non-Euclidean spatial correlations. In this paper, we propose the Spatial-Temporal Dynamic Graph Attention Network (STDGAT), a novel ride-hailing demand prediction method. Based on the attention mechanism of GAT, STDGAT extracts different pair-wise correlations to achieve the adaptive importance allocation for different neighbor regions. Moreover, in STDGAT, we design a novel time-specific commuting-based graph attention mode to construct a dynamic graph structure for capturing the dynamic time-specific spatial relationships throughout the timeline. Extensive experiments are conducted on a real-world ride-hailing demand dataset, and the experimental results demonstrate the significant improvement of our method on three evaluation metrics RMSE, MAPE and MAE over state-of-the-art baselines.","['Weiguo Pian', 'Yingbo Wu', 'Xiangmou Qu', 'Junpeng Cai', 'Ziyi Kou']",,arXiv,2022,https://doi.org/10.48550/arXiv.2006.05905,Anomali
Heterogeneous Graph Attention Networks for Early Detection of Rumors on Twitter,"With the rapid development of mobile Internet technology and the widespread use of mobile devices, it becomes much easier for people to express their opinions on social media. The openness and convenience of social media platforms provide a free expression for people but also cause new social problems. The widespread of false rumors on social media can bring about the panic of the public and damage personal reputation, which makes rumor automatic detection technology become particularly necessary. The majority of existing methods for rumor detection focus onminingeffective features fromtextcontents, user profiles, and patterns of propagation. Nevertheless, these methods do not take full advantage of global semantic relations of thetextcontents, which characterize the semantic commonality of rumors as a key factor for detecting rumors. In this paper, we construct a tweet-word-user heterogeneous graph based on thetextcontents and the source tweet propagations of rumors. A meta-path based heterogeneous graph attention network framework is proposed to capture the global semantic relations oftextcontents, together with the global structure information of source tweet propagations for rumor detection. Experiments on real-world Twitter data demonstrate the superiority of the proposed approach, which also has a comparable ability to detect rumors at a very early stage.","['Qi Huang', 'Junshuai Yu', 'Jia Wu', 'Bin Wang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.05866,Anomali
Towards an Argument Mining Pipeline Transforming Texts to Argument Graphs,"This paper targets the automated extraction of components of argumentative information and their relations from natural languagetext. Moreover, we address a current lack of systems to provide complete argumentative structure from arbitrary natural languagetextfor general usage. We present an argumentminingpipeline as a universally applicable approach for transforming German and English languagetextsto graph-based argument representations. We also introduce new methods for evaluating the results based on existing benchmark argument structures. Our results show that the generated argument graphs can be beneficial to detect new connections between different statements of an argumentativetext. Our pipeline implementation is publicly available on GitHub.","['Mirko Lenz', 'Premtim Sahitaj', 'Sean Kallenberg', 'Christopher Coors', 'Lorik Dumani', 'Ralf Schenkel', 'Ralph Bergmann']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.04562,Anomali
"An Algorithm for Fuzzification of WordNets, Supported by a Mathematical Proof","WordNet-like Lexical Databases (WLDs) group English words into sets of synonyms called ""synsets."" Although the standard WLDs are being used in many successfulText-Miningapplications, they have the limitation that word-senses are considered to represent the meaning associated to their corresponding synsets, to the same degree, which is not generally true. In order to overcome this limitation, several fuzzy versions of synsets have been proposed. A common trait of these studies is that, to the best of our knowledge, they do not aim to produce fuzzified versions of the existing WLD's, but build new WLDs from scratch, which has limited the attention received from theText-Miningcommunity, many of whose resources and applications are based on the existing WLDs. In this study, we present an algorithm for constructing fuzzy versions of WLDs of any language, given a corpus of documents and a word-sense disambiguation (WSD) system for that language. Then, using the Open-American-National-Corpus and UKB WSD as algorithm inputs, we construct and publish online the fuzzified version of English WordNet (FWN). We also propose a theoretical (mathematical) proof of the validity of its results.","['Sayyed-Ali Hossayni', 'Mohammad-R Akbarzadeh-T', 'Diego Reforgiato Recupero', 'Aldo Gangemi', 'Esteve Del Acebo', 'Josep Lluís de la Rosa i Esteva']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.04042,Anomali
Interpretable Time Series Classification using Linear Models and Multi-resolution Multi-domain Symbolic Representations,"The time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. Prior research has mostly focused on improving the accuracy and efficiency of classifiers, with interpretability being somewhat neglected. This aspect of classifiers has become critical for many application domains and the introduction of the EU GDPR legislation in 2018 is likely to further emphasize the importance of interpretable learning algorithms. Currently, state-of-the-art classification accuracy is achieved with very complex models based on large ensembles (COTE) or deep neural networks (FCN). These approaches are not efficient with regard to either time or space, are difficult to interpret and cannot be applied to variable-length time series, requiring pre-processing of the original series to a set fixed-length. In this paper we propose new time series classification algorithms to address these gaps. Our approach is based on symbolic representations of time series, efficient sequenceminingalgorithms and linear classification models. Our linear models are as accurate as deep learning models but are more efficient regarding running time and memory, can work with variable-length time series and can be interpreted by highlighting the discriminative symbolic features on the original time series. We show that our multi-resolution multi-domain linear classifier (mtSS-SEQL+LR) achieves a similar accuracy to the state-of-the-art COTE ensemble, and to recent deep learning methods (FCN, ResNet), but uses a fraction of the time and memory required by either COTE or deep models. To further analyse the interpretability of our classifier, we present a case study on a human motion dataset collected by the authors. We release all the results, source code and data to encourage reproducibility.","['Thach Le Nguyen', 'Severin Gsponer', 'Iulia Ilie', ""Martin O'Reilly"", 'Georgiana Ifrim']",Data Mining and Knowledge Discovery 33 (2019) 1183-1222,arXiv,2020,https://doi.org/10.48550/arXiv.2006.01667,Anomali
SANA : Sentiment Analysis on Newspapers comments in Algeria,"It is very current in today life to seek for tracking the people opinion from their interaction with occurring events. A very common way to do that is comments in articles published in newspapers web sites dealing with contemporary events. Sentiment analysis or opinionminingis an emergent field who is the purpose is finding the behind phenomenon masked in opinionatedtexts. We are interested in our work by comments in Algerian newspaper websites. For this end, two corpora were used SANA and OCA. SANA corpus is created by collection of comments from three Algerian newspapers, and annotated by two Algerian Arabic native speakers, while OCA is a freely available corpus for sentiment analysis. For the classification we adopt Supports vector machines, naive Bayes and knearest neighbors. Obtained results are very promising and show the different effects of stemming in such domain, also knearest neighbors give important improvement comparing to other classifiers unlike similar works where SVM is the most dominant. From this study we observe the importance of dedicated resources and methods the newspaper comments sentiment analysis which we look forward in future works.","['Hichem Rahab', 'Abdelhafid Zitouni', 'Mahieddine Djoudi']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.00459,Anomali
A decade of movement ecology,"Movement is fundamental to life, shaping population dynamics, biodiversity patterns, and ecosystem structure. Recent advances in tracking technology have enabled fundamental questions about movement to be tackled, leading to the development of the movement ecology framework (MEF), considered a milestone in the field [1]. The MEF introduced an integrative theory of organismal movement, linking internal state, motion capacity and navigation capacity to external factors. Here, a decade later, we investigated the current state of research in the field. Using atextminingapproach on >8000 peer-reviewed papers in movement ecology, we explored the main research topics, evaluated the impact of the MEF, and assessed changes in the use of technological devices, software and statistical methods. The number of publications has increased considerably and there have been major technological changes in the past decade (i.e.~increased use of GPS devices, accelerometers and video cameras, and a convergence towards R), yet we found that research focuses on the same questions, specifically, on the effect of environmental factors on movement and behavior. In practice, it appears that movement ecology research does not reflect the MEF. We call on researchers to transform the field from technology-driven to embrace interdisciplinary collaboration, in order to reveal key processes underlying movement (e.g.~navigation), as well as evolutionary, physiological and life-history consequences of particular strategies.","['Rocío Joo', 'Simona Picardi', 'Matthew E. Boone', 'Thomas A. Clay', 'Samantha C. Patrick', 'Vilma S. Romero-Romero', 'Mathieu Basille']",,arXiv,2020,https://doi.org/10.48550/arXiv.2006.00110,Anomali
Parallelizing Machine Learning as a Service for the End-User,"As ML applications are becoming ever more pervasive, fully-trained systems are made increasingly available to a wide public, allowing end-users to submit queries with their own data, and to efficiently retrieve results. With increasingly sophisticated such services, a new challenge is how to scale up to evergrowing user bases. In this paper, we present a distributed architecture that could be exploited to parallelize a typical ML system pipeline. We propose a case study consisting of atextminingservice and discuss how the method can be generalized to many similar applications. We demonstrate the significance of the computational gain boosted by the distributed architecture by way of an extensive experimental evaluation.","['Daniela Loreti', 'Marco Lippi', 'Paolo Torroni']",Future Generation Computer Systems 105 (2020) 275-286,arXiv,2020,https://doi.org/10.48550/arXiv.2005.14080,Anomali
A Multi-modal Approach to Fine-grained Opinion Mining on Video Reviews,"Despite the recent advances in opinionminingfor written reviews, few works have tackled the problem on other sources of reviews. In light of this issue, we propose a multi-modal approach forminingfine-grained opinions from video reviews that is able to determine the aspects of the item under review that are being discussed and the sentiment orientation towards them. Our approach works at the sentence level without the need for time annotations and uses features derived from the audio, video and language transcriptions of its contents. We evaluate our approach on two datasets and show that leveraging the video and audio modalities consistently provides increased performance overtext-only baselines, providing evidence these extra modalities are key in better understanding video reviews.","['Edison Marrese-Taylor', 'Cristian Rodriguez-Opazo', 'Jorge A. Balazs', 'Stephen Gould', 'Yutaka Matsuo']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.13362,Anomali
Self-Training for Domain Adaptive Scene Text Detection,"Though deep learning based scenetextdetection has achieved great progress, well-trained detectors suffer from severe performance degradation for different domains. In general, a tremendous amount of data is indispensable to train the detector in the target domain. However, data collection and annotation are expensive and time-consuming. To address this problem, we propose a self-training framework to automaticallyminehard examples with pseudo-labels from unannotated videos or images. To reduce the noise of hard examples, a noveltextminingmodule is implemented based on the fusion of detection and tracking results. Then, an image-to-video generation method is designed for the tasks that videos are unavailable and only images can be used. Experimental results on standard benchmarks, including ICDAR2015, MSRA-TD500, ICDAR2017 MLT, demonstrate the effectiveness of our self-training method. The simple Mask R-CNN adapted with self-training and fine-tuned on real data can achieve comparable or even superior results with the state-of-the-art methods.","['Yudi Chen', 'Wei Wang', 'Yu Zhou', 'Fei Yang', 'Dongbao Yang', 'Weiping Wang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.11487,Anomali
The Discussion Tracker Corpus of Collaborative Argumentation,"Although Natural Language Processing (NLP) research on argumentmininghas advanced considerably in recent years, most studies draw on corpora of asynchronous and writtentexts, often produced by individuals. Few published corpora of synchronous, multi-party argumentation are available. The Discussion Tracker corpus, collected in American high school English classes, is an annotated dataset of transcripts of spoken, multi-party argumentation. The corpus consists of 29 multi-party discussions of English literature transcribed from 985 minutes of audio. The transcripts were annotated for three dimensions of collaborative argumentation: argument moves (claims, evidence, and explanations), specificity (low, medium, high) and collaboration (e.g., extensions of and disagreements about others' ideas). In addition to providing descriptive statistics on the corpus, we provide performance benchmarks and associated code for predicting each dimension separately, illustrate the use of the multiple annotations in the corpus to improve performance via multi-task learning, and finally discuss other ways the corpus might be used to further NLP research.","['Christopher Olshefski', 'Luca Lugini', 'Ravneet Singh', 'Diane Litman', 'Amanda Godley']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.11344,Anomali
Intent Mining from past conversations for conversational agent,"Conversational systems are of primary interest in the AI community. Chatbots are increasingly being deployed to provide round-the-clock support and to increase customer engagement. Many of the commercial bot building frameworks follow a standard approach that requires one to build and train an intent model to recognize a user input. Intent models are trained in a supervised setting with a collection of textual utterance and intent label pairs. Gathering a substantial and wide coverage of training data for different intent is a bottleneck in the bot building process. Moreover, the cost of labeling a hundred to thousands of conversations with intent is a time consuming and laborious job. In this paper, we present an intent discovery framework that involves 4 primary steps: Extraction of textual utterances from a conversation using a pre-trained domain agnostic Dialog Act Classifier (Data Extraction), automatic clustering of similar user utterances (Clustering), manual annotation of clusters with an intent label (Labeling) and propagation of intent labels to the utterances from the previous step, which are not mapped to any cluster (Label Propagation); to generate intent training data from raw conversations. We have introduced a novel density-based clustering algorithm ITER-DBSCAN for unbalanced data clustering. Subject Matter Expert (Annotators with domain expertise) manually looks into the clustered user utterances and provides an intent label for discovery. We conducted user studies to validate the effectiveness of the trained intent model generated in terms of coverage of intents, accuracy and time saving concerning manual annotation. Although the system is developed for building an intent model for the conversational system, this framework can also be used for a shorttextclustering or as a labeling framework.","['Ajay Chatterjee', 'Shubhashis Sengupta']",https://www.aclweb.org/anthology/2020.coling-main.366,arXiv,2021,https://doi.org/10.48550/arXiv.2005.11014,Anomali
A Recommender System For Open Educational Videos Based On Skill Requirements,"In this paper, we suggest a novel method to help learners find relevant open educational videos to master skills demanded on the labour market. We have built a prototype, which 1) appliestextclassification andtextminingmethods on job vacancy announcements to match jobs and their required skills; 2) predicts the quality of videos; and 3) creates an open educational video recommender system to suggest personalized learning content to learners.
  For the first evaluation of this prototype we focused on the area of data science related jobs. Our prototype was evaluated by in-depth, semi-structured interviews. 15 subject matter experts provided feedback to assess how our recommender prototype performs in terms of its objectives, logic, and contribution to learning. More than 250 videos were recommended, and 82.8% of these recommendations were treated as useful by the interviewees. Moreover, interviews revealed that our personalized video recommender system, has the potential to improve the learning experience.","['Mohammadreza Tavakoli', 'Sherzod Hakimov', 'Ralph Ewerth', 'Gábor Kismihók']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.10595,Anomali
Learning Semantic Program Embeddings with Graph Interval Neural Network,"Learning distributed representations of source code has been a challenging task for machine learning models. Earlier works treated programs astextso that natural language methods can be readily applied. Unfortunately, such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph Neural Network (GNN) was proposed to learn embeddings of programs from their graph representations. Due to the homogeneous and expensive message-passing procedure, GNN can suffer from precision issues, especially when dealing with programs rendered into large graphs. In this paper, we present a new graph neural architecture, called Graph Interval Neural Network (GINN), to tackle the weaknesses of the existing GNN. Unlike the standard GNN, GINN generalizes from a curated graph representation obtained through an abstraction method designed to aid models to learn. In particular, GINN focuses exclusively on intervals forminingthe feature representation of a program, furthermore, GINN operates on a hierarchy of intervals for scaling the learning to large graphs. We evaluate GINN for two popular downstream applications: variable misuse prediction and method name prediction. Results show in both cases GINN outperforms the state-of-the-art models by a comfortable margin. We have also created a neural bug detector based on GINN to catch null pointer deference bugs in Java code. While learning from the same 9,000 methods extracted from 64 projects, GINN-based bug detector significantly outperforms GNN-based bug detector on 13 unseen test projects. Next, we deploy our trained GINN-based bug detector and Facebook Infer to scan the codebase of 20 highly starred projects on GitHub. Through our manual inspection, we confirm 38 bugs out of 102 warnings raised by GINN-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer.","['Yu Wang', 'Fengjuan Gao', 'Linzhang Wang', 'Ke Wang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.09997,Anomali
Non-Uniform Gaussian Blur of Hexagonal Bins in Cartesian Coordinates,"In a recent application of the Bokeh Python library for visualizing physico-chemical properties of chemical entitiestext-minedfrom the scientific literature, we found ourselves facing the task of smoothing hexagonally binned data in Cartesian coordinates. To the best of our knowledge, no documentation for how to do this exist in the public domain. This short paper shows how to accomplish this in general and for Bokeh in particular. We illustrate the method with a real-world example and discuss some potential advantages of using hexagonal bins in these and similar applications.","['Reinier Vleugels', 'Magnus Palmblad']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.09941,Anomali
Webpage Segmentation for Extracting Images and Their Surrounding Contextual Information,"Web images come in hand with valuable contextual information. Although this information has long beenminedfor various uses such as image annotation, clustering of images, inference of image semantic content, etc., insufficient attention has been given to address issues inminingthis contextual information. In this paper, we propose a webpage segmentation algorithm targeting the extraction of web images and their contextual information based on their characteristics as they appear on webpages. We conducted a user study to obtain a human-labeled dataset to validate the effectiveness of our method and experiments demonstrated that our method can achieve better results compared to an existing segmentation algorithm.","['F. Fauzi', 'H. J. Long', 'M. Belkhatir']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.09639,Anomali
"Labour Market Information Driven, Personalized, OER Recommendation System for Lifelong Learners","In this paper, we suggest a novel method to aid lifelong learners to access relevant OER based learning content to master skills demanded on the labour market. Our software prototype 1) appliesTextClassification andTextMiningmethods on vacancy announcements to decompose jobs into meaningful skills components, which lifelong learners should target; and 2) creates a hybrid OER Recommender System to suggest personalized learning content for learners to progress towards their skill targets. For the first evaluation of this prototype we focused on two job areas: Data Scientist, and Mechanical Engineer. We applied our skill extractor approach and provided OER recommendations for learners targeting these jobs. We conducted in-depth, semi-structured interviews with 12 subject matter experts to learn how our prototype performs in terms of its objectives, logic, and contribution to learning. More than 150 recommendations were generated, and 76.9% of these recommendations were treated as useful by the interviewees. Interviews revealed that a personalized OER recommender system, based on skills demanded by labour market, has the potential to improve the learning experience of lifelong learners.","['Mohammadreza Tavakoli', 'Stefan T. Mol', 'Gábor Kismihók']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.07465,Anomali
Recent advances in directional statistics,"Mainstream statistical methodology is generally applicable to data observed in Euclidean space. There are, however, numerous contexts of considerable scientific interest in which the natural supports for the data under consideration are Riemannian manifolds like the unit circle, torus, sphere and their extensions. Typically, such data can be represented using one or more directions, and directional statistics is the branch of statistics that deals with their analysis. In this paper we provide a review of the many recent developments in the field since the publication of Mardia and Jupp (1999), still the most comprehensivetexton directional statistics. Many of those developments have been stimulated by interesting applications in fields as diverse as astronomy, medicine, genetics, neurology, aeronautics, acoustics, image analysis,textmining, environmetrics, and machine learning. We begin by considering developments for the exploratory analysis of directional data before progressing to distributional models, general approaches to inference, hypothesis testing, regression, nonparametric curve estimation, methods for dimension reduction, classification and clustering, and the modelling of time series, spatial and spatio-temporal data. An overview of currently available software for analysing directional data is also provided, and potential future developments discussed.","['Arthur Pewsey', 'Eduardo García-Portugués']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.06889,Anomali
Towards Automatic building of Human-Machine Conversational System to support Maintenance Processes,"Companies are dealing with many cognitive changes with the introduction of the Industry 4.0 paradigm. In this constantly changing environment, knowledge management is a key factor. Dialog systems, being able to hold a conversation with humans, could support the knowledge management in business environment. Although, these systems are currently hand-coded and need the intervention of a human being in writing all the possible questions and answers, and then planning the interactions. This process, besides being time-consuming, is not scalable. Conversely, a dialog system, also referred to as chatbot, can be built from scratch by simply extracting rules from technical documentation. So, the goal of this research is designing a methodology for automatic building of human-machine conversational system, able to interact in an industrial environment. An initial taxonomy, containing entities expected to be found in maintenance manuals, is used to identify the relevant sentences of a manual provided by the company BOBST SA and applyingtextminingtechniques, it is automatically expanded. The final result is a taxonomy network representing the entities and their relation, that will be used in future works for managing the interactions of a maintenance chatbot.","['Elena Coli', 'Nicola Melluso', 'Gualtiero Fantoni', 'Daniele Mazzei']",R&D Management Conference 2019,arXiv,2020,https://doi.org/10.48550/arXiv.2005.06517,Anomali
Adaptive Rule Discovery for Labeling Text Data,"Creating and collecting labeled data is one of the major bottlenecks in machine learning pipelines and the emergence of automated feature generation techniques such as deep learning, which typically requires a lot of training data, has further exacerbated the problem. While weak-supervision techniques have circumvented this bottleneck, existing frameworks either require users to write a set of diverse, high-quality rules to label data (e.g., Snorkel), or require a labeled subset of the data to automaticallyminerules (e.g., Snuba). The process of manually writing rules can be tedious and time consuming. At the same time, creating a labeled subset of the data can be costly and even infeasible in imbalanced settings. This is due to the fact that a random sample in imbalanced settings often contains only a few positive instances.
  To address these shortcomings, we present Darwin, an interactive system designed to alleviate the task of writing rules for labelingtextdata in weakly-supervised settings. Given an initial labeling rule, Darwin automatically generates a set of candidate rules for the labeling task at hand, and utilizes the annotator's feedback to adapt the candidate rules. We describe how Darwin is scalable and versatile. It can operate over largetextcorpora (i.e., more than 1 million sentences) and supports a wide range of labeling functions (i.e., any function that can be specified using a context free grammar). Finally, we demonstrate with a suite of experiments over five real-world datasets that Darwin enables annotators to generate weakly-supervised labels efficiently and with a small cost. In fact, our experiments show that rules discovered by Darwin on average identify 40% more positive instances compared to Snuba even when it is provided with 1000 labeled instances.","['Sainyam Galhotra', 'Behzad Golshan', 'Wang-Chiew Tan']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.06133,Anomali
An Empirical Study of Multi-Task Learning on BERT for Biomedical Text Mining,"Multi-task learning (MTL) has achieved remarkable success in natural language processing applications. In this work, we study a multi-task learning model with multiple decoders on varieties of biomedical and clinical natural language processing tasks such astextsimilarity, relation extraction, named entity recognition, andtextinference. Our empirical results demonstrate that the MTL fine-tuned models outperform state-of-the-art transformer models (e.g., BERT and its variants) by 2.0% and 1.3% in biomedical and clinical domains, respectively. Pairwise MTL further demonstrates more details about which tasks can improve or decrease others. This is particularly helpful in the context that researchers are in the hassle of choosing a suitable model for new problems. The code and models are publicly available at https://github.com/ncbi-nlp/bluebert","['Yifan Peng', 'Qingyu Chen', 'Zhiyong Lu']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.02799,Anomali
An Algebraic Approach for High-level Text Analytics,"Textanalytical tasks like word embedding, phrasemining, and topic modeling, are placing increasing demands as well as challenges to existing database management systems.
  In this paper, we provide a novel algebraic approach based on associative arrays. Our data model and algebra can bring together relational operators andtextoperators, which enables interesting optimization opportunities for hybrid data sources that have both relational and textual data. We demonstrate its expressive power intextanalytics using several real-world tasks.","['Xiuwen Zheng', 'Amarnath Gupta']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.00993,Anomali
Biomedical Entity Representations with Synonym Marginalization,"Biomedical named entities often play important roles in many biomedicaltextminingtools. However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging. In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities. To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates. Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves. In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates. On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.","['Mujeen Sung', 'Hwisang Jeon', 'Jinhyuk Lee', 'Jaewoo Kang']",,arXiv,2020,https://doi.org/10.48550/arXiv.2005.00239,Anomali
A Named Entity Based Approach to Model Recipes,"Traditional cooking recipes follow a structure which can be modelled very well if the rules and semantics of the different sections of the recipetextare analyzed and represented accurately. We propose a structure that can accurately represent the recipe as well as a pipeline to infer the best representation of the recipe in this uniform structure. The Ingredients section in a recipe typically lists down the ingredients required and corresponding attributes such as quantity, temperature, and processing state. This can be modelled by defining these attributes and their values. The physical entities which make up a recipe can be broadly classified into utensils, ingredients and their combinations that are related by cooking techniques. The instruction section lists down a series of events in which a cooking technique or process is applied upon these utensils and ingredients. We model these relationships in the form of tuples. Thus, using a combination of these methods we model cooking recipe in the dataset RecipeDB to show the efficacy of our method. Thisminedinformation model can have several applications which include translating recipes between languages, determining similarity between recipes, generation of novel recipes and estimation of the nutritional profile of recipes. For the purpose of recognition of ingredient attributes, we train the Named Entity Relationship (NER) models and analyze the inferences with the help of K-Means clustering. Our model presented with an F1 score of 0.95 across all datasets. We use a similar NER tagging model for labelling cooking techniques (F1 score = 0.88) and utensils (F1 score = 0.90) within the instructions section. Finally, we determine the temporal sequence of relationships between ingredients, utensils and cooking techniques for modeling the instruction steps.","['Nirav Diwan', 'Devansh Batra', 'Ganesh Bagler']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.12184,Anomali
TeamTat: a collaborative text annotation tool,"Manually annotated data is key to developingtext-miningand information-extraction algorithms. However, human annotation requires considerable time, effort and expertise. Given the rapid growth of biomedical literature, it is paramount to build tools that facilitate speed and maintain expert quality. While existingtextannotation tools may provide user-friendly interfaces to domain experts, limited support is available for image display, project management, and multi-user team annotation. In response, we developed TeamTat (teamtat.org), a web-based annotation tool (local setup available), equipped to manage team annotation projects engagingly and efficiently. TeamTat is a novel tool for managing multi-user, multi-label document annotation, reflecting the entire production life cycle. Project managers can specify annotation schema for entities and relations and select annotator(s) and distribute documents anonymously to prevent bias. Document input format can be plaintext, PDF or BioC, (uploaded locally or automatically retrieved from PubMed or PMC), and output format is BioC with inline annotations. TeamTat displays figures from the fulltextfor the annotators convenience. Multiple users can work on the same document independently in their workspaces, and the team manager can track task completion. TeamTat provides corpus-quality assessment via inter-annotator agreement statistics, and a user-friendly interface convenient for annotation review and inter-annotator disagreement resolution to improve corpus quality.","['Rezarta Islamaj', 'Dongseop Kwon', 'Sun Kim', 'Zhiyong Lu']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.11894,Anomali
An approach based on Combination of Features for automatic news retrieval,"Nowadays, according to the increasingly increasing information, the importance of its presentation is also increasing. The internet has become one of the main sources of information for users and their favorite topics. It also provides access to more information. Understanding this information is very important for providing the best set of information resources for users. Content providers now need a precise and efficient way to retrieve news with the least human help. Datamininghas led to the emergence of new methods for detecting related and unrelated documents. Although the conceptual relationship between documents may be negligible, it is important to provide useful information and relevant content to users. In this paper, a new approach based on the Combination of Features (CoF) for information retrieval operations is introduced. Along with introducing this new approach, we proposed a dataset by identifying the most commonly used keywords in documents and using the most appropriate documents to help them with the abundance of vocabulary. Then, using the proposed approach, techniques oftextcategorization, evaluation criteria and ranking algorithms, the data were analyzed and examined. The evaluation results show that using the combination of features approach improves the quality and effects on efficient ranking.","['Mohammad Moradi', 'Elham Ghanbari', 'Mehrdad Maeen', 'Sasan Harifi']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.11699,Anomali
"Love, Joy, Anger, Sadness, Fear, and Surprise: SE Needs Special Kinds of AI: A Case Study on Text Mining and SE",Do you like your code? What kind of code makes developers happiest? What makes them angriest? Is it possible to monitor the mood of a large team of coders to determine when and where a codebase needs additional help?,"['Nicole Novielli', 'Fabio Calefato', 'Filippo Lanubile']","IEEE Software May/June 2020, Vol. 37, No. 3, pp. 86-91",arXiv,2020,https://doi.org/10.48550/arXiv.2004.11005,Anomali
CORD-19: The COVID-19 Open Research Dataset,"The COVID-19 Open Research Dataset (CORD-19) is a growing resource of scientific papers on COVID-19 and related historical coronavirus research. CORD-19 is designed to facilitate the development oftextminingand information retrieval systems over its rich collection of metadata and structured fulltextpapers. Since its release, CORD-19 has been downloaded over 200K times and has served as the basis of many COVID-19textminingand discovery systems. In this article, we describe the mechanics of dataset construction, highlighting challenges and key design decisions, provide an overview of how CORD-19 has been used, and describe several shared tasks built around the dataset. We hope this resource will continue to bring together the computing community, biomedical experts, and policy makers in the search for effective treatments and management policies for COVID-19.","['Lucy Lu Wang', 'Kyle Lo', 'Yoganand Chandrasekhar', 'Russell Reas', 'Jiangjiang Yang', 'Doug Burdick', 'Darrin Eide', 'Kathryn Funk', 'Yannis Katsis', 'Rodney Kinney', 'Yunyao Li', 'Ziyang Liu', 'William Merrill', 'Paul Mooney', 'Dewey Murdick', 'Devvret Rishi', 'Jerry Sheehan', 'Zhihong Shen', 'Brandon Stilson', 'Alex Wade', 'Kuansan Wang', 'Nancy Xin Ru Wang', 'Chris Wilhelm', 'Boya Xie', 'Douglas Raymond']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.10706,Anomali
Stabilization of transmittance fluctuations caused by beam wandering in continuous-variable quantum communication over free-space atmospheric channels,"Transmittance fluctuations in turbulent atmospheric channels result in quadrature excess noise which limits applicability of continuous-variable quantum communication. Such fluctuations are commonly caused by beam wandering around the receiving aperture. We study the possibility to stabilize the fluctuations by expanding the beam, and test this channel stabilization in regard of continuous-variable entanglement sharing and quantum key distribution. We perform transmittance measurements of a real free-space atmospheric channel for different beam widths and show that the beam expansion reduces the fluctuations of the channel transmittance by the cost of an increased overall loss. We also theoretically study the possibility to share an entangled state or to establish secure quantum key distribution over the turbulent atmospheric channels with varying beam widths. We show the positive effect of channel stabilization by beam expansion on continuous-variable quantum communication as well as the necessity to optimize the method in order to maximize the secret key rate or the amount of shared entanglement. Being autonomous and not requiring adaptive control of the source and detectors based on characterization of beam wandering, the method of beam expansion can be also combined with other methods aiming at stabilizing the fluctuating free-space atmospheric channels.","['Vladyslav C. Usenko', 'Christian Peuntinger', 'Bettina Heim', 'Kevin Günthner', 'Ivan Derkach', 'Dominique Elser', 'Christoph Marquardt', 'Radim Filip', 'Gerd Leuchs']",Optics Express 26(24): 31106-31115 (2018),arXiv,2020,https://doi.org/10.48550/arXiv.2004.10573,Anomali
Domain-Guided Task Decomposition with Self-Training for Detecting Personal Events in Social Media,"Miningsocial media content for tasks such as detecting personal experiences or events, suffer from lexical sparsity, insufficient training data, and inventive lexicons. To reduce the burden of creating extensive labeled data and improve classification performance, we propose to perform these tasks in two steps: 1. Decomposing the task into domain-specific sub-tasks by identifying key concepts, thus utilizing human domain understanding; and 2. Combining the results of learners for each key concept using co-training to reduce the requirements for labeled training data. We empirically show the effectiveness and generality of our approach, Co-Decomp, using three representative social mediaminingtasks, namely Personal Health Mention detection, Crisis Report detection, and Adverse Drug Reaction monitoring. The experiments show that our model is able to outperform the state-of-the-arttextclassification models--including those using the recently introduced BERT model--when small amounts of training data are available.","['Payam Karisani', 'Joyce C. Ho', 'Eugene Agichtein']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.10201,Anomali
A Sub-linear Time Framework for Geometric Optimization with Outliers in High Dimensions,"Many real-world problems can be formulated as geometric optimization problems in high dimensions, especially in the fields of machine learning and datamining. Moreover, we often need to take into account of outliers when optimizing the objective functions. However, the presence of outliers could make the problems to be much more challenging than their vanilla versions. In this paper, we study the fundamental minimum enclosing ball (MEB) with outliers problem first; partly inspired by the core-set method from Bădoiu and Clarkson, we propose a sub-linear time bi-criteria approximation algorithm based on two novel techniques, the Uniform-Adaptive Sampling method and Sandwich Lemma. To the best of our knowledge, our result is the first sub-linear time algorithm, which has the sample size ({\em i.e.,} the number of sampled points) independent of both the number of input points $n$ and dimensionality $d$, for MEB with outliers in high dimensions. Furthermore, we observe that these two techniques can be generalized to deal with a broader range of geometric optimization problems with outliers in high dimensions, including flat fitting, $k$-center clustering, and SVM with outliers, and therefore achieve the sub-linear time algorithms for these problems respectively.",['Hu Ding'],,arXiv,2020,https://doi.org/10.48550/arXiv.2004.10090,Anomali
A Study of Knowledge Sharing related to Covid-19 Pandemic in Stack Overflow,"The Covid-19 outbreak, beyond its tragic effects, has changed to an unprecedented extent almost every aspect of human activity throughout the world. At the same time, the pandemic has stimulated enormous amount of research by scientists across various disciplines, seeking to study the phenomenon itself, its epidemiological characteristics and ways to confront its consequences. Information Technology, and particularly Data Science, drive innovation in all related to Covid-19 biomedical fields. Acknowledging that software developers routinely resort to open question and answer communities like Stack Overflow to seek advice on solving technical issues, we have performed an empirical study to investigate the extent, evolution and characteristics of Covid-19 related posts. In particular, through the study of 464 Stack Overflow questions posted mainly in February and March 2020 and leveraging the power oftextmining, we attempt to shed light into the interest of developers in Covid-19 related topics and the most popular technological problems for which the users seek information. The findings reveal that indeed this global crisis sparked off an intense and increasing activity in Stack Overflow with most post topics reflecting a strong interest on the analysis of Covid-19 data, primarily using Python technologies.","['Konstantinos Georgiou', 'Nikolaos Mittas', 'Lefteris Angelis', 'Alexander Chatzigeorgiou']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.09495,Anomali
An Annotated Dataset of Stack Overflow Post Edits,"To improve software engineering, software repositories have beenminedfor code snippets and bug fixes. Typically, thisminingtakes place at the level of files or commits. To be able to dig deeper and to extract insights at a higher resolution, we hereby present an annotated dataset that contains over 7 million edits of code andtexton Stack Overflow. Our preliminary study indicates that these edits might be a treasure trove formininginformation about fine-grained patches, e.g., for the optimisation of non-functional properties.","['Sebastian Baltes', 'Markus Wagner']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.08193,Anomali
NAIST COVID: Multilingual COVID-19 Twitter and Weibo Dataset,"Since the outbreak of coronavirus disease 2019 (COVID-19) in the late 2019, it has affected over 200 countries and billions of people worldwide. This has affected the social life of people owing to enforcements, such as ""social distancing"" and ""stay at home."" This has resulted in an increasing interaction through social media. Given that social media can bring us valuable information about COVID-19 at a global scale, it is important to share the data and encourage social media studies against COVID-19 or other infectious diseases. Therefore, we have released a multilingual dataset of social media posts related to COVID-19, consisting of microblogs in English and Japanese from Twitter and those in Chinese from Weibo. The data cover microblogs from January 20, 2020, to March 24, 2020. This paper also provides a quantitative as well as qualitative analysis of these datasets by creating daily word clouds as an example oftext-mininganalysis. The dataset is now available on Github. This dataset can be analyzed in a multitude of ways and is expected to help in efficient communication of precautions related to COVID-19.","['Zhiwei Gao', 'Shuntaro Yada', 'Shoko Wakamiya', 'Eiji Aramaki']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.08145,Anomali
CO.ME.T.A. -- covid-19 media textual analysis. A dashboard for media monitoring,"The focus of this paper is to trace how mass media, particularly newspapers, have addressed the issues about the containment of contagion or the explanation of epidemiological evolution. We propose an interactive dashboard: CO.ME.T.A.. During crises it is important to shape the best communication strategies in order to respond to critical situations. In this regard, it is important to monitor the information that mass media and social platforms convey. The dashboard allows to explore theminingof contents extracted and study the lexical structure that links the main discussion topics. The dashboard merges together four methods:textmining, sentiment analysis, textual network analysis and latent topic models. Results obtained on a subset of documents show not only a health-related semantic dimension, but it also extends to social-economic dimensions.","['Emma Zavarrone', 'Maria Gabriella Grassia', 'Marina Marino', 'Rasanna Cataldo', 'Rocco Mazza', 'Nicola Canestrari']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.07742,Anomali
A parsimonious family of multivariate Poisson-lognormal distributions for clustering multivariate count data,"Multivariate count data are commonly encountered through high-throughput sequencing technologies in bioinformatics,textmining, or in sports analytics. Although the Poisson distribution seems a natural fit to these count data, its multivariate extension is computationally expensive.In most cases mutual independence among the variables is assumed, however this fails to take into account the correlation among the variables usually observed in the data. Recently, mixtures of multivariate Poisson-lognormal (MPLN) models have been used to analyze such multivariate count measurements with a dependence structure. In the MPLN model, each count is modeled using an independent Poisson distribution conditional on a latent multivariate Gaussian variable. Due to this hierarchical structure, the MPLN model can account for over-dispersion as opposed to the traditional Poisson distribution and allows for correlation between the variables. Rather than relying on a Monte Carlo-based estimation framework which is computationally inefficient, a fast variational-EM based framework is used here for parameter estimation. Further, a parsimonious family of mixtures of Poisson-lognormal distributions are proposed by decomposing the covariance matrix and imposing constraints on these decompositions. Utility of such models is shown using simulated and benchmark datasets.","['Sanjeena Subedi', 'Ryan Browne']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.06857,Anomali
Automated Spelling Correction for Clinical Text Mining in Russian,"The main goal of this paper is to develop a spell checker module for clinicaltextin Russian. The described approach combines string distance measure algorithms with technics of machine learning embedding methods. Our overall precision is 0.86, lexical precision - 0.975 and error precision is 0.74. We develop spell checker as a part of medicaltextminingtool regarding the problems of misspelling, negation, experiencer and temporality detection.","['Ksenia Balabaeva', 'Anastasia Funkner', 'Sergey Kovalchuk']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.04987,Anomali
Negation Detection for Clinical Text Mining in Russian,"Developing predictive modeling in medicine requires additional features from unstructured clinicaltexts. In Russia, there are no instruments for natural language processing to cope with problems of medical records. This paper is devoted to a module of negation detection. The corpus-free machine learning method is based on gradient boosting classifier is used to detect whether a disease is denied, not mentioned or presented in thetext. The detector classifies negations for five diseases and shows average F-score from 0.81 to 0.93. The benefits of negation detection have been demonstrated by predicting the presence of surgery for patients with the acute coronary syndrome.","['Anastasia Funkner', 'Ksenia Balabaeva', 'Sergey Kovalchuk']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.04980,Anomali
Keywords Extraction and Sentiment Analysis using Automatic Speech Recognition,"Automatic Speech Recognition (ASR) is the interdisciplinary subfield of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language intotextby computers. It incorporates knowledge and research in linguistics, computer science, and electrical engineering fields. Sentiment analysis is contextualminingoftextwhich identifies and extracts subjective information in the source material and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. According to the speech structure, three models are used in speech recognition to do the match: Acoustic Model, Phonetic Dictionary and Language Model. Any speech recognition program is evaluated using two factors: Accuracy (percentage error in converting spoken words to digital data) and Speed (the extent to which the program can keep up with a human speaker). For the purpose of converting speech totext(STT), we will be studying the following open source toolkits: CMU Sphinx and Kaldi. The toolkits use Mel-Frequency Cepstral Coefficients (MFCC) and I-vector for feature extraction. CMU Sphinx has been used with pre-trained Hidden Markov Models (HMM) and Gaussian Mixture Models (GMM), while Kaldi is used with pre-trained Neural Networks (NNET) as acoustic models. The n-gram language models contain the phonemes or pdf-ids for generating the most probable hypothesis (transcription) in the form of a lattice. The speech dataset is stored in the form of .raw or .wav file and is transcribed in .txt file. The system then tries to identify opinions within thetext, and extract the following attributes: Polarity (if the speaker expresses a positive or negative opinion) and Keywords (the thing that is being talked about).",['Rachit Shukla'],,arXiv,2020,https://doi.org/10.48550/arXiv.2004.04099,Anomali
SIA: A Scalable Interoperable Annotation Server for Biomedical Named Entities,"Recent years showed a strong increase in biomedical sciences and an inherent increase in publication volume. Extraction of specific information from these sources requires highly sophisticatedtextminingand information extraction tools. However, the integration of freely available tools into customized workflows is often cumbersome and difficult. We describe SIA (Scalable Interoperable Annotation Server), our contribution to the BeCalm-Technical interoperability and performance of annotation servers (BeCalm-TIPS) task, a scalable, extensible, and robust annotation service. The system currently covers six named entity types (i.e., Chemicals, Diseases, Genes, miRNA, Mutations, and Organisms) and is freely available under Apache 2.0 license at https://github.com/Erechtheus/sia.","['Johannes Kirschnick', 'Philippe Thomas', 'Roland Roller', 'Leonhard Hennig']","J Cheminform 10, 63 (2018)",arXiv,2020,https://doi.org/10.48550/arXiv.2004.03822,Anomali
Discovering associations in COVID-19 related research papers,"A COVID-19 pandemic has already proven itself to be a global challenge. It proves how vulnerable humanity can be. It has also mobilized researchers from different sciences and different countries in the search for a way to fight this potentially fatal disease. In line with this, our study analyses the abstracts of papers related to COVID-19 and coronavirus-related-research using association ruletextminingin order to find the most interestingness words, on the one hand, and relationships between them on the other. Then, a method, called information cartography, was applied for extracting structured knowledge from a huge amount of association rules. On the basis of these methods, the purpose of our study was to show how researchers have responded in similar epidemic/pandemic situations throughout history.","['Iztok Fister Jr.', 'Karin Fister', 'Iztok Fister']",,arXiv,2020,https://doi.org/10.48550/arXiv.2004.03397,Anomali
Is it feasible to detect FLOSS version release events from textual messages? A case study on Stack Overflow,"Topic Detection and Tracking (TDT) is a very active research question within the area oftextmining, generally applied to news feeds and Twitter datasets, where topics and events are detected. The notion of ""event"" is broad, but typically it applies to occurrences that can be detected from a single post or a message. Little attention has been drawn to what we call ""micro-events"", which, due to their nature, cannot be detected from a single piece of textual information. The study investigates the feasibility of micro-event detection on textual data using a sample of messages from the Stack Overflow Q&A platform and Free/Libre Open Source Software (FLOSS) version releases from Libraries.io dataset. We build pipelines for detection of micro-events using three different estimators whose parameters are optimized using a grid search approach. We consider two feature spaces: LDA topic modeling with sentiment analysis, and hSBM topics with sentiment analysis. The feature spaces are optimized using the recursive feature elimination with cross validation (RFECV) strategy.
  In our experiments we investigate whether there is a characteristic change in the topics distribution or sentiment features before or after micro-events take place and we thoroughly evaluate the capacity of each variant of our analysis pipeline to detect micro-events. Additionally, we perform a detailed statistical analysis of the models, including influential cases, variance inflation factors, validation of the linearity assumption, pseudo R squared measures and no-information rate. Finally, in order to study limits of micro-event detection, we design a method for generating micro-event synthetic datasets with similar properties to the real-world data, and use them to identify the micro-event detectability threshold for each of the evaluated classifiers.","['A. Sokolovsky', 'T. Gross', 'J. Bacardit']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.14257,Anomali
Topological Data Analysis in Text Classification: Extracting Features with Additive Information,"While the strength of Topological Data Analysis has been explored in many studies on high dimensional numeric data, it is still a challenging task to apply it totext. As the primary goal in topological data analysis is to define and quantify the shapes in numeric data, defining shapes in thetextis much more challenging, even though the geometries of vector spaces and conceptual spaces are clearly relevant for information retrieval and semantics. In this paper, we examine two different methods of extraction of topological features fromtext, using as the underlying representations of words the two most popular methods, namely word embeddings and TF-IDF vectors. To extract topological features from the word embedding space, we interpret the embedding of atextdocument as high dimensional time series, and we analyze the topology of the underlying graph where the vertices correspond to different embedding dimensions. For topological data analysis with the TF-IDF representations, we analyze the topology of the graph whose vertices come from the TF-IDF vectors of different blocks in the textual document. In both cases, we apply homological persistence to reveal the geometric structures under different distance resolutions. Our results show that these topological features carry some exclusive information that is not captured by conventionaltextminingmethods. In our experiments we observe adding topological features to the conventional features in ensemble models improves the classification results (up to 5\%). On the other hand, as expected, topological features by themselves may be not sufficient for effective classification. It is an open problem to see whether TDA features from word embeddings might be sufficient, as they seem to perform within a range of few points from top results obtained with a linear support vector classifier.","['Shafie Gholizadeh', 'Ketki Savle', 'Armin Seyeditabari', 'Wlodek Zadrozny']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.13138,Anomali
A Novel Method of Extracting Topological Features from Word Embeddings,"In recent years, topological data analysis has been utilized for a wide range of problems to deal with high dimensional noisy data. Whiletextrepresentations are often high dimensional and noisy, there are only a few work on the application of topological data analysis in natural language processing. In this paper, we introduce a novel algorithm to extract topological features from word embedding representation oftextthat can be used fortextclassification. Working on word embeddings, topological data analysis can interpret the embedding high-dimensional space and discover the relations among different embedding dimensions. We will use persistent homology, the most commonly tool from topological data analysis, for our experiment. Examining our topological algorithm on long textual documents, we will show our defined topological features may outperform conventionaltextminingfeatures.","['Shafie Gholizadeh', 'Armin Seyeditabari', 'Wlodek Zadrozny']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.13074,Anomali
Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining,"Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets. During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced. In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), served as a meta-learner to solve a group of similar NLP tasks for neural language models. Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge. It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions. After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability. We implement MFT upon BERT to solve several multi-domaintextminingtasks. Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.","['Chengyu Wang', 'Minghui Qiu', 'Jun Huang', 'Xiaofeng He']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.13003,Anomali
Towards Accurate Scene Text Recognition with Semantic Reasoning Networks,"Scenetextimage contains two levels of contents: visual texture and semantic information. Although the previous scenetextrecognition methods have made great progress over the past few years, the research onminingsemantic information to assisttextrecognition attracts less attention, only RNN-like structures are explored to implicitly model semantic information. However, we observe that RNN based methods have some obvious shortcomings, such as time-dependent decoding manner and one-way serial transmission of semantic context, which greatly limit the help of semantic information and the computation efficiency. To mitigate these limitations, we propose a novel end-to-end trainable framework named semantic reasoning network (SRN) for accurate scenetextrecognition, where a global semantic reasoning module (GSRM) is introduced to capture global semantic context through multi-way parallel transmission. The state-of-the-art results on 7 public benchmarks, including regulartext, irregulartextand non-Latin longtext, verify the effectiveness and robustness of the proposed method. In addition, the speed of SRN has significant advantages over the RNN based methods, demonstrating its value in practical use.","['Deli Yu', 'Xuan Li', 'Chengquan Zhang', 'Junyu Han', 'Jingtuo Liu', 'Errui Ding']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.12294,Anomali
"What is the people posting about symptoms related to Coronavirus in Bogota, Colombia?","During the last months, there is an increasing alarm about a new mutation of coronavirus, covid-19 coined by World Health Organization(WHO) with an impact in many areas: economy, health, politics and others. This situation was declared a pandemic by WHO, because of the fast expansion over many countries. At the same time, people is using Social Networks to express what they think, feel or experiment, so this people are Social Sensors and helps to analyze what is happening in their city. The objective of this paper is analyze the publications of Colombian people living in Bogota with a radius of 50 km usingTextMiningtechniques from symptomatology approach. The results support the understanding of the spread in Colombia related to symptoms of covid19.","['Josimar E. Chire Saire', 'Roberto C. Navarro']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.11159,Anomali
Understanding the perception of COVID-19 policies by mining a multilanguage Twitter dataset,"The objective of this work is to explore popular discourse about the COVID-19 pandemic and policies implemented to manage it. Using Natural Language Processing,TextMining, and Network Analysis to analyze corpus of tweets that relate to the COVID-19 pandemic, we identify common responses to the pandemic and how these responses differ across time. Moreover, insights as to how information and misinformation were transmitted via Twitter, starting at the early stages of this pandemic, are presented. Finally, this work introduces a dataset of tweets collected from all over the world, in multiple languages, dating back to January 22nd, when the total cases of reported COVID-19 were below 600 worldwide. The insights presented in this work could help inform decision makers in the face of future pandemics, and the dataset introduced can be used to acquire valuable knowledge to help mitigate the COVID-19 pandemic.","['Christian E. Lopez', 'Malolan Vasu', 'Caleb Gallemore']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.10359,Anomali
"English dictionaries, gold and silver standard corpora for biomedical natural language processing related to SARS-CoV-2 and COVID-19","Automated information extraction with natural language processing (NLP) tools is required to gain systematic insights from the large number of COVID-19 publications, reports and social media posts, which far exceed human processing capabilities. A key challenge for NLP is the extensive variation in terminology used to describe medical entities, which was especially pronounced for this newly emergent disease. Here we present an NLP toolbox comprising very large English dictionaries of synonyms for SARS-CoV-2 (including variant names) and COVID-19, which can be used with dictionary-based NLP tools. We also present a silver standard corpus generated with the dictionaries, and a gold standard corpus, consisting of PubMed abstracts manually annotated for disease, virus, symptom, protein/gene, cell type, chemical and species terms, which can be used to train and evaluate COVID-19-related NLP tools. Code for annotation, which can be used to expand the silver standard corpus or fortextminingis also included. This toolbox is freely available on GitHub (on https://github.com/Aitslab/corona) and zenodo (https://doi.org/10.5281/zenodo.6642275). The toolbox can be used for a variety oftextanalytics tasks related to the COVID-19 crisis and has already been used to create a COVID-19 knowledge graph, study the variability and evolution of COVID-19-related terminology and develop and benchmarktextminingtools.","['Salma Kazemi Rashed', 'Rafsan Ahmed', 'Johan Frid', 'Sonja Aits']",,arXiv,2022,this https URL,Anomali
Text-mining forma mentis networks reconstruct public perception of the STEM gender gap in social media,"Mindset reconstruction maps how individuals structure and perceive knowledge, a map unfolded here by investigating language and its cognitive reflection in the human mind, i.e. the mental lexicon. Textual forma mentis networks (TFMN) are glass boxes introduced for extracting, representing and understanding mindsets' structure, in Latin ""forma mentis"", from textual data. Combining network science, psycholinguistics and Big Data, TFMNs successfully identified relevant concepts, without supervision, in benchmarktexts. Once validated, TFMNs were applied to the case study of the gender gap in science, which was strongly linked to distorted mindsets by recent studies. Focusing over social media perception and online discourse, this work analysed 10,000 relevant tweets. ""Gender"" and ""gap"" elicited a mostly positive perception, with a trustful/joyous emotional profile and semantic associates that: celebrated successful female scientists, related gender gap to wage differences, and hoped for a future resolution. The perception of ""woman"" highlighted discussion about sexual harassment and stereotype threat (a form of implicit cognitive bias) relative to women in science ""sacrificing personal skills for success"". The reconstructed perception of ""man"" highlighted social users' awareness of the myth of male superiority in science. No anger was detected around ""person"", suggesting that gap-focused discourse got less tense around genderless terms. No stereotypical perception of ""scientist"" was identified online, differently from real-world surveys. The overall analysis identified the online discourse as promoting a mostly stereotype-free, positive/trustful perception of gender disparity, aware of implicit/explicit biases and projected to closing the gap. TFMNs opened new ways for investigating perceptions in different groups, offering detailed data-informed grounding for policy making.",['Massimo Stella'],,arXiv,2020,https://doi.org/10.48550/arXiv.2003.08835,Anomali
Parallel sequence tagging for concept recognition,"Background: Named Entity Recognition (NER) and Normalisation (NEN) are core components of anytext-miningsystem for biomedicaltexts. In a traditional concept-recognition pipeline, these tasks are combined in a serial way, which is inherently prone to error propagation from NER to NEN. We propose a parallel architecture, where both NER and NEN are modeled as a sequence-labeling task, operating directly on the sourcetext. We examine different harmonisation strategies for merging the predictions of the two classifiers into a single output sequence. Results: We test our approach on the recent Version 4 of the CRAFT corpus. In all 20 annotation sets of the concept-annotation task, our system outperforms the pipeline system reported as a baseline in the CRAFT shared task 2019. Conclusions: Our analysis shows that the strengths of the two classifiers can be combined in a fruitful way. However, prediction harmonisation requires individual calibration on a development set for each annotation set. This allows achieving a good trade-off between established knowledge (training set) and novel information (unseen concepts). Availability and Implementation: Source code freely available for download at https://github.com/OntoGene/craft-st. Supplementary data are available at arXiv online.","['Lenz Furrer', 'Joseph Cornelius', 'Fabio Rinaldi']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.07424,Anomali
Leveraging Foreign Language Labeled Data for Aspect-Based Opinion Mining,"Aspect-based opinionminingis the task of identifying sentiment at the aspect level in opinionatedtext, which consists of two subtasks: aspect category extraction and sentiment polarity classification. While aspect category extraction aims to detect and categorize opinion targets such as product features, sentiment polarity classification assigns a sentiment label, i.e. positive, negative, or neutral, to each identified aspect. Supervised learning methods have been shown to deliver better accuracy for this task but they require labeled data, which is costly to obtain, especially for resource-poor languages like Vietnamese. To address this problem, we present a supervised aspect-based opinionminingmethod that utilizes labeled data from a foreign language (English in this case), which is translated to Vietnamese by an automated translation tool (Google Translate). Because aspects and opinions in different languages may be expressed by different words, we propose using word embeddings, in addition to other features, to reduce the vocabulary difference between the original and translatedtexts, thus improving the effectiveness of aspect category extraction and sentiment polarity classification processes. We also introduce an annotated corpus of aspect categories and sentiment polarities extracted from restaurant reviews in Vietnamese, and conduct a series of experiments on the corpus. Experimental results demonstrate the effectiveness of the proposed approach.","['Nguyen Thi Thanh Thuy', 'Ngo Xuan Bach', 'Tu Minh Phuong']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.06858,Anomali
Joint Alignment From Pairwise Differences with a Noisy Oracle,"In this work we consider the problem of recovering $n$ discrete random variables $x_i\in \{0,\ldots,k-1\}, 1 \leq i \leq n$ (where $k$ is constant) with the smallest possible number of queries to a noisy oracle that returns for a given query pair $(x_i,x_j)$ a noisy measurement of their modulo $k$ pairwise difference, i.e., $y_{ij} = (x_i-x_j) \mod k$. This is a joint discrete alignment problem with important applications in computer vision, graphmining, and spectroscopy imaging. Our main result is a polynomial time algorithm that learns exactly with high probability the alignment (up to some unrecoverable offset) using $O(n^{1+o(1)})$ queries.","['Michael Mitzenmacher', 'Charalampos E. Tsourakakis']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.06076,Anomali
Predicting the Amount of GDPR Fines,"The General Data Protection Regulation (GDPR) was enforced in 2018. After this enforcement, many fines have already been imposed by national data protection authorities in the European Union (EU). This paper examines the individual GDPR articles referenced in the enforcement decisions, as well as predicts the amount of enforcement fines with available meta-data andtextminingfeatures extracted from the enforcement decision documents. According to the results, articles related to the general principles, lawfulness, and information security have been the most frequently referenced ones. Although the amount of fines imposed vary across the articles referenced, these three particular articles do not stand out. Furthermore, good predictions are attainable even with simple machine learning techniques for regression analysis. Basic meta-data (such as the articles referenced and the country of origin) yields slightly better performance compared to thetextminingfeatures.","['Jukka Ruohonen', 'Kalle Hjerppe']","Proceedings of the First International Workshop ""CAiSE for Legal Documents"" (COUrT 2020), Grenoble (online), CEUR-WS, pp. 3-14, http://ceur-ws.org/Vol-2690/COUrT-paper1.pdf",arXiv,2020,https://doi.org/10.48550/arXiv.2003.05151,Anomali
SAFE: Similarity-Aware Multi-Modal Fake News Detection,"Effective detection of fake news has recently attracted significant attention. Current studies have made significant contributions to predicting fake news with less focus on exploiting the relationship (similarity) between the textual and visual information in news articles. Attaching importance to such similarity helps identify fake news stories that, for example, attempt to use irrelevant images to attract readers' attention. In this work, we propose a $\mathsf{S}$imilarity-$\mathsf{A}$ware $\mathsf{F}$ak$\mathsf{E}$ news detection method ($\mathsf{SAFE}$) which investigates multi-modal (textual and visual) information of news articles. First, neural networks are adopted to separately extract textual and visual features for news representation. We further investigate the relationship between the extracted features across modalities. Such representations of news textual and visual information along with their relationship are jointly learned and used to predict fake news. The proposed method facilitates recognizing the falsity of news articles based on theirtext, images, or their ""mismatches."" We conduct extensive experiments on large-scale real-world data, which demonstrate the effectiveness of the proposed method.","['Xinyi Zhou', 'Jindi Wu', 'Reza Zafarani']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.04981,Anomali
A Dataset Independent Set of Baselines for Relation Prediction in Argument Mining,"ArgumentMiningis the research area which aims at extracting argument components and predicting argumentative relations (i.e.,support and attack) fromtext. In particular, numerous approaches have been proposed in the literature to predict the relations holding between the arguments, and application-specific annotated resources were built for this purpose. Despite the fact that these resources have been created to experiment on the same task, the definition of a single relation prediction method to be successfully applied to a significant portion of these datasets is an open research problem in ArgumentMining. This means that none of the methods proposed in the literature can be easily ported from one resource to another. In this paper, we address this problem by proposing a set of dataset independent strong neural baselines which obtain homogeneous results on all the datasets proposed in the literature for the argumentative relation prediction task. Thus, our baselines can be employed by the ArgumentMiningcommunity to compare more effectively how well a method performs on the argumentative relation prediction task.","['Oana Cocarascu', 'Elena Cabrio', 'Serena Villata', 'Francesca Toni']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.04970,Anomali
New advances in enumerative biclustering algorithms with online partitioning,"This paper further extends RIn-Close_CVC, a biclustering algorithm capable of performing an efficient, complete, correct and non-redundant enumeration of maximal biclusters with constant values on columns in numerical datasets. By avoiding a priori partitioning and itemization of the dataset, RIn-Close_CVC implements an online partitioning, which is demonstrated here to guide to more informative biclustering results. The improved algorithm is called RIn-Close_CVC3, keeps those attractive properties of RIn-Close_CVC, as formally proved here, and is characterized by: a drastic reduction in memory usage; a consistent gain in runtime; additional ability to handle datasets with missing values; and additional ability to operate with attributes characterized by distinct distributions or even mixed data types. The experimental results include synthetic and real-world datasets used to perform scalability and sensitivity analyses. As a practical case study, a parsimonious set of relevant and interpretable mixed-attribute-type rules is obtained in the context of supervised descriptive patternmining.","['Rosana Veroneze', 'Fernando J. Von Zuben']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.04726,Anomali
Data Warehouse and Decision Support on Integrated Crop Big Data,"In recent years, precision agriculture is becoming very popular. The introduction of modern information and communication technologies for collecting and processing Agricultural data revolutionise the agriculture practises. This has started a while ago (early 20th century) and it is driven by the low cost of collecting data about everything; from information on fields such as seed, soil, fertiliser, pest, to weather data, drones and satellites images. Specially, the agricultural dataminingtoday is considered as Big Data application in terms of volume, variety, velocity and veracity. Hence it leads to challenges in processing vast amounts of complex and diverse information to extract useful knowledge for the farmer, agronomist, and other businesses. It is a key foundation to establishing a crop intelligence platform, which will enable efficient resource management and high quality agronomy decision making and recommendations. In this paper, we designed and implemented a continental level agricultural data warehouse (ADW). ADW is characterised by its (1) flexible schema; (2) data integration from real agricultural multi datasets; (3) data science and business intelligent support; (4) high performance; (5) high storage; (6) security; (7) governance and monitoring; (8) consistency, availability and partition tolerant; (9) cloud compatibility. We also evaluate the performance of ADW and present some complex queries to extract and return necessary knowledge about crop management.","['V. M. Ngo', 'N. A. Le-Khac', 'M. T. Kechadi']",International Journal of Business Process Integration and Management 2020 Vol.10 No.1,arXiv,2021,https://doi.org/10.48550/arXiv.2003.04470,Anomali
Adaptive Offline Quintuplet Loss for Image-Text Matching,"Existing image-textmatching approaches typically leverage triplet loss with online hard negatives to train the model. For each image ortextanchor in a training mini-batch, the model is trained to distinguish between a positive and the most confusing negative of the anchorminedfrom the mini-batch (i.e. online hard negative). This strategy improves the model's capacity to discover fine-grained correspondences and non-correspondences between image andtextinputs. However, the above approach has the following drawbacks: (1) the negative selection strategy still provides limited chances for the model to learn from very hard-to-distinguish cases. (2) The trained model has weak generalization capability from the training set to the testing set. (3) The penalty lacks hierarchy and adaptiveness for hard negatives with different ""hardness"" degrees. In this paper, we propose solutions by sampling negatives offline from the whole training set. It provides ""harder"" offline negatives than online hard negatives for the model to distinguish. Based on the offline hard negatives, a quintuplet loss is proposed to improve the model's generalization capability to distinguish positives and negatives. In addition, a novel loss function that combines the knowledge of positives, offline hard negatives and online hard negatives is created. It leverages offline hard negatives as the intermediary to adaptively penalize them based on their distance relations to the anchor. We evaluate the proposed training approach on three state-of-the-art image-textmodels on the MS-COCO and Flickr30K datasets. Significant performance improvements are observed for all the models, proving the effectiveness and generality of our approach. Code is available at https://github.com/sunnychencool/AOQ","['Tianlang Chen', 'Jiajun Deng', 'Jiebo Luo']",,arXiv,2020,https://doi.org/10.48550/arXiv.2003.03669,Anomali
Broadband low loss and ultra-low crosstalk waveguide crossings based on multimode interferometer for 840 nm operation,"Broadband low loss and ultra-low crosstalk waveguide crossings are a crucial component for photonic integrated circuits to allow a higher integration density of functional components and an increased flexibility in the layout. We report the design of optimized silicon nitride waveguide crossings based on multimode interferometer structures for intersecting light paths of TE/TE-like, TM/TM-like and TE/TM-like polarized light in the near infrared wavelength region of 790 nm to 890 nm. The crossing design for diverse polarization modes facilitates dual polarization operation on a single chip. For all configurations the loss of a single crossing was measured to be 0.05 dB at 840 nm. Within the 100 nm bandwidth losses stayed below 0.16 dB. The crosstalk was estimated to be on the order of -60 dB by means of 3D finite difference time domain simulations.","['Stefan Nevlacsil', 'Paul Muellner', 'Martin Sagmeister', 'Jochen Kraft', 'Rainer Hainberger']","OSA Continuum 3, 334-344 (2020)",arXiv,2020,https://doi.org/10.48550/arXiv.2002.11399,Anomali
Abstractive Snippet Generation,"An abstractive snippet is an originally created piece oftextto summarize a web page on a search engine results page. Compared to the conventional extractive snippets, which are generated by extracting phrases and sentences verbatim from a web page, abstractive snippets circumvent copyright issues; even more interesting is the fact that they open the door for personalization. Abstractive snippets have been evaluated as equally powerful in terms of user acceptance and expressiveness---but the key question remains: Can abstractive snippets be automatically generated with sufficient quality?
  This paper introduces a new approach to abstractive snippet generation: We identify the first two large-scale sources for distant supervision, namely anchor contexts and web directories. Byminingthe entire ClueWeb09 and ClueWeb12 for anchor contexts and by utilizing the DMOZ Open Directory Project, we compile the Webis Abstractive Snippet Corpus 2020, comprising more than 3.5 million triples of the form $\langle$query, snippet, document$\rangle$ as training examples, where the snippet is either an anchor context or a web directory description in lieu of a genuine query-biased abstractive snippet of the web document. We propose a bidirectional abstractive snippet generation model and assess the quality of both our corpus and the generated abstractive snippets with standard measures, crowdsourcing, and in comparison to the state of the art. The evaluation shows that our novel data sources along with the proposed model allow for producing usable query-biased abstractive snippets while minimizingtextreuse.","['Wei-Fan Chen', 'Shahbaz Syed', 'Benno Stein', 'Matthias Hagen', 'Martin Potthast']",,arXiv,2020,https://doi.org/10.48550/arXiv.2002.10782,Anomali
ArcText: A Unified Text Approach to Describing Convolutional Neural Network Architectures,"The superiority of Convolutional Neural Networks (CNNs) largely relies on their architectures that are often manually crafted with extensive human expertise. Unfortunately, such kind of domain knowledge is not necessarily owned by each of the users interested. Dataminingon existing CNN can discover useful patterns and fundamental sub-comments from their architectures, providing researchers with strong prior knowledge to design proper CNN architectures when they have no expertise in CNNs. There have been various state-of-the-art dataminingalgorithms at hand, while there is only rare work that has been done for themining. One of the main reasons is the gap between CNN architectures and dataminingalgorithms. Specifically, the current CNN architecture descriptions cannot be exactly vectorized to the input of dataminingalgorithms. In this paper, we propose a unified approach, named ArcText, to describing CNN architectures based ontext. Particularly, four different units and an ordering method have been elaborately designed in ArcText, to uniquely describe the same architecture with sufficient information. Also, the resulted description can be exactly converted back to the corresponding CNN architecture. ArcText bridges the gap between CNN architectures and dataminingresearchers, and has the potentiality to be utilized to wider scenarios.","['Yanan Sun', 'Ziyao Ren', 'Gary G. Yen', 'Bing Xue', 'Mengjie Zhang', 'Jiancheng Lv']",,arXiv,2020,https://doi.org/10.48550/arXiv.2002.10233,Anomali
The interconnectedness of the economic content in the speeches of the US Presidents,"The speeches stated by influential politicians can have a decisive impact on the future of a country. In particular, the economic content of such speeches affects the economy of countries and their financial markets. For this reason, we examine a novel dataset containing the economic content of 951 speeches stated by 45 US Presidents from George Washington (April 1789) to Donald Trump (February 2017). In doing so, we use an economic glossary carried out by means oftextminingtechniques. The goal of our study is to examine the structure of significant interconnections within a network obtained from the economic content of presidential speeches. In such a network, nodes are represented by talks and links by values of cosine similarity, the latter computed using the occurrences of the economic terms in the speeches. The resulting network displays a peculiar structure made up of a core (i.e. a set of highly central and densely connected nodes) and a periphery (i.e. a set of non-central and sparsely connected nodes). The presence of different economic dictionaries employed by the Presidents characterize the core-periphery structure. The Presidents' talks belonging to the network's core share the usage of generic (non-technical) economic locutions like ""interest"" or ""trade"". While the use of more technical and less frequent terms characterizes the periphery (e.g. ""yield"" ). Furthermore, the speeches close in time share a common economic dictionary. These results together with the economics glossary usages during the US periods of boom and crisis provide unique insights on the economic content relationships among Presidents' speeches.","['Matteo Cinelli', 'Valerio Ficcadenti', 'Jessica Riccioni']",Ann Oper Res (2019),arXiv,2020,https://doi.org/10.48550/arXiv.2002.07880,Anomali
"Historical Document Processing: Historical Document Processing: A Survey of Techniques, Tools, and Trends","Historical Document Processing is the process of digitizing written material from the past for future use by historians and other scholars. It incorporates algorithms and software tools from various subfields of computer science, including computer vision, document analysis and recognition, natural language processing, and machine learning, to convert images of ancient manuscripts, letters, diaries, and early printedtextsautomatically into a digital format usable in dataminingand information retrieval systems. Within the past twenty years, as libraries, museums, and other cultural heritage institutions have scanned an increasing volume of their historical document archives, the need to transcribe the fulltextfrom these collections has become acute. Since Historical Document Processing encompasses multiple sub-domains of computer science, knowledge relevant to its purpose is scattered across numerous journals and conference proceedings. This paper surveys the major phases of, standard algorithms, tools, and datasets in the field of Historical Document Processing, discusses the results of a literature review, and finally suggests directions for further research.","['James P. Philips', 'Nasseh Tabrizi']",,arXiv,2020,https://doi.org/10.48550/arXiv.2002.06300,Anomali
An experiment exploring the theoretical and methodological challenges in developing a semi-automated approach to analysis of small-N qualitative data,"This paper experiments with designing a semi-automated qualitative data analysis (QDA) algorithm to analyse 20 transcripts by using freeware.Text-mining(TM) and QDA were guided by frequency and association measures, because these statistics remain robust when the sample size is small. The refined TM algorithm split thetextinto various sizes based on a manually revised dictionary. This lemmatisation approach may reflect the context of thetextbetter than uniformly tokenising thetextinto one single size. TM results were used for initial coding. Code repacking was guided by association measures and external data to implement a general inductive QDA approach. The information retrieved by TM and QDA was depicted in subgraphs for comparisons. The analyses were completed in 6-7 days. Both algorithms retrieved contextually consistent and relevant information. However, the QDA algorithm retrieved more specific information than TM alone. The QDA algorithm does not strictly comply with the convention of TM or of QDA, but becomes a more efficient, systematic and transparenttextanalysis approach than a conventional QDA approach. Scaling up QDA to reliably discover knowledge fromtextwas exactly the research purpose. This paper also sheds light on understanding the relations between information technologies, theory and methodologies.",['Sandro Tsang'],,arXiv,2020,https://doi.org/10.48550/arXiv.2002.04513,Anomali
Optimization of Retrieval Algorithms on Large Scale Knowledge Graphs,"Knowledge graphs have been shown to play an important role in recent knowledgeminingand discovery, for example in the field of life sciences or bioinformatics. Although a lot of research has been done on the field of query optimization, query transformation and of course in storing and retrieving large scale knowledge graphs the field of algorithmic optimization is still a major challenge and a vital factor in using graph databases. Few researchers have addressed the problem of optimizing algorithms on large scale labeled property graphs. Here, we present two optimization approaches and compare them with a naive approach of directly querying the graph database. The aim of our work is to determine limiting factors of graph databases like Neo4j and we describe a novel solution to tackle these challenges. For this, we suggest a classification schema to differ between the complexity of a problem on a graph database. We evaluate our optimization approaches on a test system containing a knowledge graph derived biomedical publication data enriched withtextminingdata. This dense graph has more than 71M nodes and 850M relationships. The results are very encouraging and - depending on the problem - we were able to show a speedup of a factor between 44 and 3839.","['Jens Dörpinghaus', 'Andreas Stefan']",Proceedings of the 2020 Federated Conference on Computer Science and Information Systems,arXiv,2020,https://doi.org/10.48550/arXiv.2002.03686,Anomali
Conjoined Dirichlet Process,"Biclustering is a class of techniques that simultaneously clusters the rows and columns of a matrix to sort heterogeneous data into homogeneous blocks. Although many algorithms have been proposed to find biclusters, existing methods suffer from the pre-specification of the number of biclusters or place constraints on the model structure. To address these issues, we develop a novel, non-parametric probabilistic biclustering method based on Dirichlet processes to identify biclusters with strong co-occurrence in both rows and columns. The proposed method utilizes dual Dirichlet process mixture models to learn row and column clusters, with the number of resulting clusters determined by the data rather than pre-specified. Probabilistic biclusters are identified by modeling the mutual dependence between the row and column clusters. We apply our method to two different applications,textminingand gene expression analysis, and demonstrate that our method improves bicluster extraction in many settings compared to existing approaches.","['Michelle N. Ngo', 'Dustin S. Pluta', 'Alexander N. Ngo', 'Babak Shahbaba']",,arXiv,2020,https://doi.org/10.48550/arXiv.2002.03223,Anomali
Mining Commonsense Facts from the Physical World,"Textual descriptions of the physical world implicitly mention commonsense facts, while the commonsense knowledge bases explicitly represent such facts as triples. Compared to dramatically increasedtextdata, the coverage of existing knowledge bases is far away from completion. Most of the prior studies on populating knowledge bases mainly focus on Freebase. To automatically complete commonsense knowledge bases to improve their coverage is under-explored. In this paper, we propose a new task ofminingcommonsense facts from the rawtextthat describes the physical world. We build an effective new model that fuses information from both sequencetextand existing knowledge base resource. Then we create two large annotated datasets each with approximate 200k instances for commonsense knowledge base completion. Empirical results demonstrate that our model significantly outperforms baselines.","['Yanyan Zou', 'Wei Lu', 'Xu Sun']",,arXiv,2020,https://doi.org/10.48550/arXiv.2002.03149,Anomali
Snippext: Semi-supervised Opinion Mining with Augmented Data,"Online services are interested in solutions to opinionmining, which is the problem of extracting aspects, opinions, and sentiments fromtext. One method tomineopinions is to leverage the recent success of pre-trained language models which can be fine-tuned to obtain high-quality extractions from reviews. However, fine-tuning language models still requires a non-trivial amount of training data. In this paper, we study the problem of how to significantly reduce the amount of labeled training data required in fine-tuning language models for opinionmining. We describe Snippext, an opinionminingsystem developed over a language model that is fine-tuned through semi-supervised learning with augmented data. A novelty of Snippext is its clever use of a two-prong approach to achieve state-of-the-art (SOTA) performance with little labeled training data through: (1) data augmentation to automatically generate more labeled training data from existing ones, and (2) a semi-supervised learning technique to leverage the massive amount of unlabeled data in addition to the (limited amount of) labeled data. We show with extensive experiments that Snippext performs comparably and can even exceed previous SOTA results on several opinionminingtasks with only half the training data required. Furthermore, it achieves new SOTA results when all training data are leveraged. By comparison to a baseline pipeline, we found that Snippext extracts significantly more fine-grained opinions which enable new opportunities of downstream applications.","['Zhengjie Miao', 'Yuliang Li', 'Xiaolan Wang', 'Wang-Chiew Tan']",,arXiv,2020,https://doi.org/10.48550/arXiv.2002.03049,Anomali
Context Aware Image Annotation in Active Learning,"Image annotation for active learning is labor-intensive. Various automatic and semi-automatic labeling methods are proposed to save the labeling cost, but a reduction in the number of labeled instances does not guarantee a reduction in cost because the queries that are most valuable to the learner may be the most difficult or ambiguous cases, and therefore the most expensive for an oracle to label accurately. In this paper, we try to solve this problem by using image metadata to offer the oracle more clues about the image during annotation process. We propose a Context Aware Image Annotation Framework (CAIAF) that uses image metadata as similarity metric to cluster images into groups for annotation. We also present useful metadata information as context for each image on the annotation interface. Experiments show that it reduces that annotation cost with CAIAF compared to the conventional framework, while maintaining a high classification performance.","['Yingcheng Sun', 'Kenneth Loparo']",2019 19th Industrial Conference on Data Mining,arXiv,2020,https://doi.org/10.48550/arXiv.2002.02775,Anomali
Phonon spectrum of underdoped $\text{HgBa}_2\text{CuO}_{4+δ}$ investigated by neutron scattering,"The cuprates exhibit a prominent charge-density-wave (CDW) instability with wavevector along [100], i.e., the Cu-O bond direction. Whereas CDW order is most prominent at moderate doping and low temperature, there exists increasing evidence for dynamic charge correlations throughout a large portion of the temperature-doping phase diagram. In particular, signatures of incipient charge order have been observed as phonon softening and/or broadening near the CDW wavevector approximately half-way through the Brillouin zone. Most of this work has focused on moderately-doped cuprates, for which the CDW order is robust, or on optimally-doped samples, for which the superconducting transition temperature ($T_c$) attains its maximum. Here we present a time-of-flight neutron scattering study of phonons in simple-tetragonal$\text{HgBa}_2\text{CuO}_{4+δ}$($T_c = 55$ K) at a low doping level where prior work showed the CDW order to be weak. We employ and showcase a new software-based technique thatminesthe large number of measured Brillouin zones for useful data in order to improve accuracy and counting statistics. Density-functional theory has not provided an accurate description of phonons in$\text{HgBa}_2\text{CuO}_{4+δ}$, yet we find the right set of parameters to qualitatively reproduce the data. The notable exception is a dispersion minimum in the longitudinal Cu-O bond-stretching branch along [100]. This discrepancy suggests that, while CDW order is weak, there exist significant dynamic charge correlations in the optic phonon range at low doping, near the edge of the superconducting dome.","['I. Ahmadova', 'T. C. Sterling', 'A. C. Sokolik', 'D. L. Abernathy', 'M. Greven', 'D. Reznik']","Phys. Rev. B 101, 184508 (2020)",arXiv,2020,https://doi.org/10.48550/arXiv.2002.02593,Anomali
"Seeing through the smoke : a world-wide comparative study of e-cigarette flavors, brands and markets using data from Reddit and Twitter","The growing popularity of E-cigarettes, an alternative to cigarettes, has motivated us to study trends of the brands, flavors and online market activity using posts from Reddit and Twitter. The main motivation for this world-wide study is to emphasize the difference that laws and regulations have on the usage and availability of different flavors and brands of vapes in different countries. Data has been obtained from subreddits belonging to e-cigarette communities from Australia, Canada, Europe, and the UK. Extensive cleaning of data, and rigoroustextminingoperations provide varying results for different countries. Varying results have been obtained from Reddit and Twitter since they provide different atmospheres to the users.","['Rohit Venkata Sai Dulam', 'Meghana Murthy', 'Jiebo Luo']",,arXiv,2020,https://doi.org/10.48550/arXiv.2002.01575,Anomali
Plague Dot Text: Text mining and annotation of outbreak reports of the Third Plague Pandemic (1894-1952),"The design of models that govern diseases in population is commonly built on information and data gathered from past outbreaks. However, epidemic outbreaks are never captured in statistical data alone but are communicated by narratives, supported by empirical observations. Outbreak reports discuss correlations between populations, locations and the disease to infer insights into causes, vectors and potential interventions. The problem with these narratives is usually the lack of consistent structure or strong conventions, which prohibit their formal analysis in larger corpora. Our interdisciplinary research investigates more than 100 reports from the third plague pandemic (1894-1952) evaluating ways of building a corpus to extract and structure this narrative information throughtextminingand manual annotation. In this paper we discuss the progress of our ongoing exploratory project, how we enhance optical character recognition (OCR) methods to improvetextcapture, our approach to structure the narratives and identify relevant entities in the reports. The structured corpus is made available via Solr enabling search and analysis across the whole collection for future research dedicated, for example, to the identification of concepts. We show preliminary visualisations of the characteristics of causation and differences with respect to gender as a result of syntactic-category-dependent corpus statistics. Our goal is to develop structured accounts of some of the most significant concepts that were used to understand the epidemiology of the third plague pandemic around the globe. The corpus enables researchers to analyse the reports collectively allowing for deep insights into the global epidemiological consideration of plague in the early twentieth century.","['Arlene Casey', 'Mike Bennett', 'Richard Tobin', 'Claire Grover', 'Iona Walker', 'Lukas Engelmann', 'Beatrice Alex']","Journal of Data Mining & Digital Humanities, HistoInformatics, HistoInformatics (January 20, 2021) jdmdh:6071",arXiv,2021,https://doi.org/10.48550/arXiv.2002.01415,Anomali
Unwanted Advances in Higher Education: Uncovering Sexual Harassment Experiences in Academia with Text Mining,"Sexual harassment in academia is often a hidden problem because victims are usually reluctant to report their experiences. Recently, a web survey was developed to provide an opportunity to share thousands of sexual harassment experiences in academia. Using an efficient approach, this study collected and investigated more than 2,000 sexual harassment experiences to better understand these unwanted advances in higher education. This paper utilizedtextminingto disclose hidden topics and explore their weight across three variables: harasser gender, institution type, and victim's field of study. We mapped the topics on five themes drawn from the sexual harassment literature and found that more than 50% of the topics were assigned to the unwanted sexual attention theme. Fourteen percent of the topics were in the gender harassment theme, in which insulting, sexist, or degrading comments or behavior was directed towards women. Five percent of the topics involved sexual coercion (a benefit is offered in exchange for sexual favors), 5% involved sex discrimination, and 7% of the topics discussed retaliation against the victim for reporting the harassment, or for simply not complying with the harasser. Findings highlight the power differential between faculty and students, and the toll on students when professors abuse their power. While some topics did differ based on type of institution, there were no differences between the topics based on gender of harasser or field of study. This research can be beneficial to researchers in further investigation of this paper's dataset, and to policymakers in improving existing policies to create a safe and supportive environment in academia.","['Amir Karami', 'Cynthia Nicole White', 'Kayla Ford', 'Suzanne Swan', 'Melek Yildiz Spinel']",,arXiv,2019,https://doi.org/10.48550/arXiv.2001.11552,Anomali
Brand Intelligence Analytics,"Leveraging the power of big data represents an opportunity for brand managers to reveal patterns and trends in consumer perceptions, while monitoring positive or negative associations of the brand with desired topics. This chapter describes the functionalities of the SBS Brand Intelligence App (SBS BI), which has been designed to assess brand importance and provides brand analytics through the analysis of (big) textual data. To better describe the SBS BI's functionalities, we present a case study focused on the 2020 US Democratic Presidential Primaries. We downloaded 50,000 online articles from the Event Registry database, which contains both mainstream and blog news collected from around the world. These online news articles were transformed into networks of co-occurring words and analyzed by combining methods and tools from social network analysis andtextmining.","['A. Fronzetti Colladon', 'F. Grippa']","In A. Przegalinska, F. Grippa, & P. A. Gloor (Eds.), Digital Transformation of Collaboration (pp. 125-141). Springer Nature Switzerland (2020)",arXiv,2020,https://doi.org/10.48550/arXiv.2001.11479,Anomali
Data Mining in Clinical Trial Text: Transformers for Classification and Question Answering Tasks,"This research on data extraction methods applies recent advances in natural language processing to evidence synthesis based on medicaltexts.Textsof interest include abstracts of clinical trials in English and in multilingual contexts. The main focus is on information characterized via the Population, Intervention, Comparator, and Outcome (PICO) framework, but data extraction is not limited to these fields. Recent neural network architectures based on transformers show capacities for transfer learning and increased performance on downstream natural language processing tasks such as universal reading comprehension, brought forward by this architecture's use of contextualized word embeddings and self-attention mechanisms. This paper contributes to solving problems related to ambiguity in PICO sentence prediction tasks, as well as highlighting how annotations for training named entity recognition systems are used to train a high-performing, but nevertheless flexible architecture for question answering in systematic review automation. Additionally, it demonstrates how the problem of insufficient amounts of training annotations for PICO entity extraction is tackled by augmentation. All models in this paper were created with the aim to support systematic review (semi)automation. They achieve high F1 scores, and demonstrate the feasibility of applying transformer-based classification methods to support dataminingin the biomedical literature.","['Lena Schmidt', 'Julie Weeds', 'Julian P. T. Higgins']",HEALTHINF 2020,arXiv,2020,https://doi.org/10.48550/arXiv.2001.11268,Anomali
Investigating Classification Techniques with Feature Selection For Intention Mining From Twitter Feed,"In the last decade, social networks became most popular medium for communication and interaction. As an example, micro-blogging service Twitter has more than 200 million registered users who exchange more than 65 million posts per day. Users express their thoughts, ideas, and even their intentions through these tweets. Most of the tweets are written informally and often in slang language, that contains misspelt and abbreviated words. This paper investigates the problem of selecting features that affect extracting user's intention from Twitter feeds based ontextminingtechniques. It starts by presenting the method we used to construct our own dataset from extracted Twitter feeds. Following that, we present two techniques of feature selection followed by classification. In the first technique, we use Information Gain as a one-phase feature selection, followed by supervised classification algorithms. In the second technique, we use a hybrid approach based on forward feature selection algorithm in which two feature selection techniques employed followed by classification algorithms. We examine these two techniques with four classification algorithms. We evaluate them using our own dataset, and we critically review the results.","['Qadri Mishael', 'Aladdin Ayesh']",,arXiv,2020,https://doi.org/10.48550/arXiv.2001.10380,Anomali
Mining Changes in User Expectation Over Time From Online Reviews,"Customers post online reviews at any time. With the timestamp of online reviews, they can be regarded as a flow of information. With this characteristic, designers can capture the changes in customer feedback to help set up product improvement strategies. Here we propose an approach for capturing changes of user expectation on product affordances based on the online reviews for two generations of products. First, the approach uses a rule-based natural language processing method to automatically identify and structure product affordances from reviewtext. Then, inspired by the Kano model which classifies preferences of product attributes in five categories, conjoint analysis is used to quantitatively categorize the structured affordances. Finally, changes of user expectation can be found by applying the conjoint analysis on the online reviews posted for two successive generations of products. A case study based on the online reviews of Kindle e-readers downloaded from amazon.com shows that designers can use our proposed approach to evaluate their product improvement strategies for previous products and develop new product improvement strategies for future products.","['Tianjun Hou', 'Bernard Yannou', 'Yann Leroy', 'Emilie Poirson']","Journal of Mechanical Design, American Society of Mechanical Engineers, 2019, 141 (9)",arXiv,2020,https://doi.org/10.48550/arXiv.2001.09898,Anomali
The side effect profile of Clozapine in real world data of three large mental hospitals,"Objective:Miningthe data contained within Electronic Health Records (EHRs) can potentially generate a greater understanding of medication effects in the real world, complementing what we know from Randomised control trials (RCTs). We Propose atextminingapproach to detect adverse events and medication episodes from the clinicaltextto enhance our understanding of adverse effects related to Clozapine, the most effective antipsychotic drug for the management of treatment-resistant schizophrenia, but underutilised due to concerns over its side effects. Material and Methods: We used data from de-identified EHRs of three mental health trusts in the UK (>50 million documents, over 500,000 patients, 2835 of which were prescribed Clozapine). We explored the prevalence of 33 adverse effects by age, gender, ethnicity, smoking status and admission type three months before and after the patients started Clozapine treatment. We compared the prevalence of adverse effects with those reported in the Side Effects Resource (SIDER) where possible. Results: Sedation, fatigue, agitation, dizziness, hypersalivation, weight gain, tachycardia, headache, constipation and confusion were amongst the highest recorded Clozapine adverse effect in the three months following the start of treatment. Higher percentages of all adverse effects were found in the first month of Clozapine therapy. Using a significance level of (p< 0.05) out chi-square tests show a significant association between most of the ADRs in smoking status and hospital admissions and some in gender and age groups. Further, the data was combined from three trusts, and chi-square tests were applied to estimate the average effect of ADRs in each monthly interval. Conclusion: A better understanding of how the drug works in the real world can complement clinical trials and precision medicine.","['Ehtesham Iqbal', 'Risha Govind', 'Alvin Romero', 'Olubanke Dzahini', 'Matthew Broadbent', 'Robert Stewart', 'Tanya Smith', 'Chi-Hun Kim', 'Nomi Werbeloff', 'Richard Dobson', 'Zina Ibrahim']",,arXiv,2020,https://doi.org/10.48550/arXiv.2001.09698,Anomali
Stochastic Optimization of Plain Convolutional Neural Networks with Simple methods,"Convolutional neural networks have been achieving the best possible accuracies in many visual pattern classification problems. However, due to the model capacity required to capture such representations, they are often oversensitive to overfitting and therefore require proper regularization to generalize well. In this paper, we present a combination of regularization techniques which work together to get better performance, we built plain CNNs, and then we used data augmentation, dropout and customized early stopping function, we tested and evaluated these techniques by applying models on five famous datasets, MNIST, CIFAR10, CIFAR100, SVHN, STL10, and we achieved three state-of-the-art-of (MNIST, SVHN, STL10) and very high-Accuracy on the other two datasets.",['Yahia Assiri'],"15th International Conference on Machine Learning and Data Mining, MLDM 2019, vol.II, New York, NY, USA, July 20-25, 2019, ibai-publishing, ISSN 1864-9734 ISBN 978-3-942952-63-7, pages(833-844)",arXiv,2020,https://doi.org/10.48550/arXiv.2001.08856,Anomali
Towards context in large scale biomedical knowledge graphs,"Contextual information is widely considered for NLP and knowledge discovery in life sciences since it highly influences the exact meaning of natural language. The scientific challenge is not only to extract such context data, but also to store this data for further query and discovery approaches. Here, we propose a multiple step knowledge graph approach using labeled property graphs based on polyglot persistence systems to utilize context data for contextmining, graph queries, knowledge discovery and extraction. We introduce the graph-theoretic foundation for a general context concept within semantic networks and show a proof-of-concept based on biomedical literature andtextmining. Our test system contains a knowledge graph derived from the entirety of PubMed and SCAIView data and is enriched withtextminingdata and domain specific language data using BEL. Here, context is a more general concept than annotations. This dense graph has more than 71M nodes and 850M relationships. We discuss the impact of this novel approach with 27 real world use cases represented by graph queries.","['Jens Dörpinghaus', 'Andreas Stefan', 'Bruce Schultz', 'Marc Jacobs']",,arXiv,2020,https://doi.org/10.48550/arXiv.2001.08392,Anomali
Population-based metaheuristics for Association Rule Text Mining,"Nowadays, the majority of data on the Internet is held in an unstructured format, like websites and e-mails. The importance of analyzing these data has been growing day by day. Similar to dataminingon structured data,textminingmethods for handling unstructured data have also received increasing attention from the research community. The paper deals with the problem of Association RuleTextMining. To solve the problem, the PSO-ARTM method was proposed, that consists of three steps:Textpreprocessing, Association RuleTextMiningusing population-based metaheuristics, andtextpostprocessing. The method was applied to a transaction database obtained from professional triathlon athletes' blogs and news posted on their websites. The obtained results reveal that the proposed method is suitable for Association RuleTextMiningand, therefore, offers a promising way for further development.","['Iztok Fister Jr.', 'Suash Deb', 'Iztok Fister']",,arXiv,2020,https://doi.org/10.48550/arXiv.2001.06517,Anomali
"Comparing Rule-based, Feature-based and Deep Neural Methods for De-identification of Dutch Medical Records","Unstructured information in electronic health records provide an invaluable resource for medical research. To protect the confidentiality of patients and to conform to privacy regulations, de-identification methods automatically remove personally identifying information from these medical records. However, due to the unavailability of labeled data, most existing research is constrained to English medicaltextand little is known about the generalizability of de-identification methods across languages and domains. In this study, we construct a varied dataset consisting of the medical records of 1260 patients by sampling data from 9 institutes and three domains of Dutch healthcare. We test the generalizability of three de-identification methods across languages and domains. Our experiments show that an existing rule-based method specifically developed for the Dutch language fails to generalize to this new data. Furthermore, a state-of-the-art neural architecture performs strongly across languages and domains, even with limited training data. Compared to feature-based and rule-based methods the neural method requires significantly less configuration effort and domain-knowledge. We make all code and pre-trained de-identification models available to the research community, allowing practitioners to apply them to their datasets and to enable future benchmarks.","['Jan Trienes', 'Dolf Trieschnigg', 'Christin Seifert', 'Djoerd Hiemstra']",,arXiv,2020,https://doi.org/10.48550/arXiv.2001.05714,Anomali
A BERT based Sentiment Analysis and Key Entity Detection Approach for Online Financial Texts,"The emergence and rapid progress of the Internet have brought ever-increasing impact on financial domain. How to rapidly and accuratelyminethe key information from the massive negative financialtextshas become one of the key issues for investors and decision makers. Aiming at the issue, we propose a sentiment analysis and key entity detection approach based on BERT, which is applied in online financialtextminingand public opinion analysis in social media. By using pre-train model, we first study sentiment analysis, and then we consider key entity detection as a sentence matching or Machine Reading Comprehension (MRC) task in different granularity. Among them, we mainly focus on negative sentimental information. We detect the specific entity by using our approach, which is different from traditional Named Entity Recognition (NER). In addition, we also use ensemble learning to improve the performance of proposed approach. Experimental results show that the performance of our approach is generally higher than SVM, LR, NBM, and BERT for two financial sentiment analysis and key entity detection datasets.","['Lingyun Zhao', 'Lin Li', 'Xinhao Zheng']",,arXiv,2020,https://doi.org/10.48550/arXiv.2001.05326,Anomali
Text Complexity Classification Based on Linguistic Information: Application to Intelligent Tutoring of ESL,"The goal of this work is to build a classifier that can identifytextcomplexity within the context of teaching reading to English as a Second Language (ESL) learners. To present language learners withtextsthat are suitable to their level of English, a set of features that can describe the phonological, morphological, lexical, syntactic, discursive, and psychological complexity of a giventextwere identified. Using a corpus of 6171texts, which had already been classified into three different levels of difficulty by ESL experts, different experiments were conducted with five machine learning algorithms. The results showed that the adopted linguistic features provide a good overall classification performance (F-Score = 0.97). A scalability evaluation was conducted to test if such a classifier could be used within real applications, where it can be, for example, plugged into a search engine or a web-scraping module. In this evaluation, thetextsin the test set are not only different from those from the training set but also of different types (ESLtextsvs. children readingtexts). Although the overall performance of the classifier decreased significantly (F-Score = 0.65), the confusion matrix shows that most of the classification errors are between the classes two and three (the middle-level classes) and that the system has a robust performance in categorizingtextsof class one and four. This behavior can be explained by the difference in classification criteria between the two corpora. Hence, the observed results confirm the usability of such a classifier within a real-world application.",['M. Zakaria Kurdi'],"Journal of Data Mining & Digital Humanities, 2020 (September 19, 2020) jdmdh:6012",arXiv,2020,https://doi.org/10.48550/arXiv.2001.01863,Anomali
Semi-supervised Classification using Attention-based Regularization on Coarse-resolution Data,"Many real-world phenomena are observed at multiple resolutions. Predictive models designed to predict these phenomena typically consider different resolutions separately. This approach might be limiting in applications where predictions are desired at fine resolutions but available training data is scarce. In this paper, we propose classification algorithms that leverage supervision from coarser resolutions to help train models on finer resolutions. The different resolutions are modeled as different views of the data in a multi-view framework that exploits the complementarity of features across different views to improve models on both views. Unlike traditional multi-view learning problems, the key challenge in our case is that there is no one-to-one correspondence between instances across different views in our case, which requires explicit modeling of the correspondence of instances across resolutions. We propose to use the features of instances at different resolutions to learn the correspondence between instances across resolutions using an attention mechanism.Experiments on the real-world application of mapping urban areas using satellite observations and sentiment classification ontextdata show the effectiveness of the proposed methods.","['Guruprasad Nayak', 'Rahul Ghosh', 'Xiaowei Jia', 'Varun Mithal', 'Vipin Kumar']",,arXiv,2020,https://doi.org/10.48550/arXiv.2001.00994,Anomali
Knowledge-guided Text Structuring in Clinical Trials,"Clinical trial records are variable resources or the analysis of patients and diseases. Information extraction from freetextsuch as eligibility criteria and summary of results and conclusions in clinical trials would better support computer-based eligibility query formulation and electronic patient screening. Previous research has focused on extracting information from eligibility criteria, with usually a single pair of medical entity and attribute, but seldom considering other kinds of freetextwith multiple entities, attributes and relations that are more complex for parsing. In this paper, we propose a knowledge-guidedtextstructuring framework with an automatically generated knowledge base as training corpus and word dependency relations as context information to transfer freetextinto formal, computer-interpretable representations. Experimental results show that our method can achieve overall high precision and recall, demonstrating the effectiveness and efficiency of the proposed method.","['Yingcheng Sun', 'Kenneth Loparo']",2019 19th Industrial Conference on Data Mining,arXiv,2019,https://doi.org/10.48550/arXiv.1912.12380,Anomali
Self-adaption grey DBSCAN clustering,"Clustering analysis, a classical issue in datamining, is widely used in various research areas. This article aims at proposing a self-adaption grey DBSCAN clustering (SAG-DBSCAN) algorithm. First, the grey relational matrix is used to obtain the grey local density indicator, and then this indicator is applied to make self-adapting noise identification for obtaining a dense subset of clustering dataset, finally, the DBSCAN which automatically selects parameters is utilized to cluster the dense subset. Several frequently-used datasets were used to demonstrate the performance and effectiveness of the proposed clustering algorithm and to compare the results with those of other state-of-the-art algorithms. The comprehensive comparisons indicate that our method has advantages over other compared methods.",['Shizhan Lu'],,arXiv,2019,https://doi.org/10.48550/arXiv.1912.11477,Anomali
What do Asian Religions Have in Common? An Unsupervised Text Analytics Exploration,"The main source of various religious teachings is their sacredtextswhich vary from religion to religion based on different factors like the geographical location or time of the birth of a particular religion. Despite these differences, there could be similarities between the sacredtextsbased on what lessons it teaches to its followers. This paper attempts to find the similarity usingtextminingtechniques. The corpus consisting of Asian (Tao Te Ching, Buddhism, Yogasutra, Upanishad) and non-Asian (four Bibletexts) is used to explore findings of similarity measures like Euclidean, Manhattan, Jaccard and Cosine on raw Document Term Frequency [DTM], normalized DTM which reveals similarity based on word usage. The performance of Supervised learning algorithms like K-Nearest Neighbor [KNN], Support Vector Machine [SVM] and Random Forest is measured based on its accuracy to predict correct scaredtextfor any given chapter in the corpus. The K-means clustering visualizations on Euclidean distances of raw DTM reveals that there exists a pattern of similarity among these sacredtextswith Upanishads and Tao Te Ching is the most similartextin the corpus.","['Preeti Sah', 'Ernest Fokoué']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.10847,Anomali
BioConceptVec: creating and evaluating literature-based biomedical concept embeddings on a large scale,"Capturing the semantics of related biological concepts, such as genes and mutations, is of significant importance to many research tasks in computational biology such as protein-protein interaction detection, gene-drug association prediction, and biomedical literature-based discovery. Here, we propose to leverage state-of-the-arttextminingtools and machine learning models to learn the semantics via vector representations (aka. embeddings) of over 400,000 biological concepts mentioned in the entire PubMed abstracts. Our learned embeddings, namely BioConceptVec, can capture related concepts based on their surrounding contextual information in the literature, which is beyond exact term match or co-occurrence-based methods. BioConceptVec has been thoroughly evaluated in multiple bioinformatics tasks consisting of over 25 million instances from nine different biological datasets. The evaluation results demonstrate that BioConceptVec has better performance than existing methods in all tasks. Finally, BioConceptVec is made freely available to the research community and general public via https://github.com/ncbi-nlp/BioConceptVec.","['Qingyu Chen', 'Kyubum Lee', 'Shankai Yan', 'Sun Kim', 'Chih-Hsuan Wei', 'Zhiyong Lu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.10846,Anomali
Mining the Automotive Industry: A Network Analysis of Corporate Positioning and Technological Trends,"The digital transformation is driving revolutionary innovations and new market entrants threaten established sectors of the economy such as the automotive industry. Following the need for monitoring shifting industries, we present a network-centred analysis of car manufacturer web pages. Solely exploiting publicly-available information, we construct large networks from web pages and hyperlinks. The network properties disclose the internal corporate positioning of the three largest automotive manufacturers, Toyota, Volkswagen and Hyundai with respect to innovative trends and their international outlook. We tag web pages concerned with topics like e-mobility and environment or autonomous driving, and investigate their relevance in the network. Sentiment analysis on individual web pages uncovers a relationship between page linking and use of positive language, particularly with respect to innovative trends. Web pages of the same country domain form clusters of different size in the network that reveal strong correlations with sales market orientation. Our approach maintains the web content's hierarchical structure imposed by the web page networks. It, thus, presents a method to reveal hierarchical structures of unstructuredtextcontent obtained from web scraping. It is highly transparent, reproducible and data driven, and could be used to gain complementary insights into innovative strategies of firms and competitive landscapes, which would not be detectable by the analysis of web content alone.","['Niklas Stoehr', 'Fabian Braesemann', 'Michael Frommelt', 'Shi Zhou']",,arXiv,2020,https://doi.org/10.48550/arXiv.1912.10097,Anomali
HAMBox: Delving into Online High-quality Anchors Mining for Detecting Outer Faces,"Current face detectors utilize anchors to frame a multi-task learning problem which combines classification and bounding box regression. Effective anchor design and anchor matching strategy enable face detectors to localize faces under large pose and scale variations. However, we observe that more than 80% correctly predicted bounding boxes are regressed from the unmatched anchors (the IoUs between anchors and target faces are lower than a threshold) in the inference phase. It indicates that these unmatched anchors perform excellent regression ability, but the existing methods neglect to learn from them. In this paper, we propose an Online High-quality AnchorMiningStrategy (HAMBox), which explicitly helps outer faces compensate with high-quality anchors. Our proposed HAMBox method could be a general strategy for anchor-based single-stage face detection. Experiments on various datasets, including WIDER FACE, FDDB, AFW and PASCAL Face, demonstrate the superiority of the proposed method. Furthermore, our team win the championship on the Face Detection test track of WIDER Face and Pedestrian Challenge 2019. We will release the codes with PaddlePaddle.","['Yang Liu', 'Xu Tang', 'Xiang Wu', 'Junyu Han', 'Jingtuo Liu', 'Errui Ding']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.09231,Anomali
Data Exploration and Validation on dense knowledge graphs for biomedical research,"Here we present a holistic approach for data exploration on dense knowledge graphs as a novel approach with a proof-of-concept in biomedical research. Knowledge graphs are increasingly becoming a vital factor in knowledgeminingand discovery as they connect data using technologies from the semantic web. In this paper we extend a basic knowledge graph extracted from biomedical literature by context data like named entities and relations obtained bytextminingand other linked data sources like ontologies and databases. We will present an overview about this novel network. The aim of this work was to extend this current knowledge with approaches from graph theory. This method will build the foundation for quality control, validation of hypothesis, detection of missing data and time series analysis of biomedical knowledge in general. In this context we tried to apply multiple-valued decision diagrams to these questions. In addition this knowledge representation of linked data can be used as FAIR approach to answer semantic questions. This paper sheds new lights on dense and very large knowledge graphs and the importance of a graph-theoretic understanding of these networks.","['Jens Dörpinghaus', 'Alexander Apke', 'Vanessa Lage-Rupprecht', 'Andreas Stefan']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.06194,Anomali
Integrative Generalized Convex Clustering Optimization and Feature Selection for Mixed Multi-View Data,"In mixed multi-view data, multiple sets of diverse features are measured on the same set of samples. By integrating all available data sources, we seek to discover common group structure among the samples that may be hidden in individualistic cluster analyses of a single data-view. While several techniques for such integrative clustering have been explored, we propose and develop a convex formalization that will inherit the strong statistical, mathematical and empirical properties of increasingly popular convex clustering methods. Specifically, our Integrative Generalized Convex Clustering Optimization (iGecco) method employs different convex distances, losses, or divergences for each of the different data views with a joint convex fusion penalty that leads to common groups. Additionally, integrating mixed multi-view data is often challenging when each data source is high-dimensional. To perform feature selection in such scenarios, we develop an adaptive shifted group-lasso penalty that selects features by shrinking them towards their loss-specific centers. Our so-called iGecco+ approach selects features from each data-view that are best for determining the groups, often leading to improved integrative clustering. To fit our model, we develop a new type of generalized multi-block ADMM algorithm using sub-problem approximations that more efficiently fits our model for big data sets. Through a series of numerical experiments and real data examples ontextminingand genomics, we show that iGecco+ achieves superior empirical performance for high-dimensional mixed multi-view data.","['Minjie Wang', 'Genevera I. Allen']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.05449,Anomali
A Collaborative Ecosystem for Digital Coptic Studies,"Scholarship on underresourced languages bring with them a variety of challenges which make access to the full spectrum of source materials and their evaluation difficult. For Coptic in particular, large scale analyses and any kind of quantitative work become difficult due to the fragmentation of manuscripts, the highly fusional nature of an incorporational morphology, and the complications of dealing with influences from Hellenistic era Greek, among other concerns. Many of these challenges, however, can be addressed using Digital Humanities tools and standards. In this paper, we outline some of the latest developments in Coptic Scriptorium, a DH project dedicated to bringing Coptic resources online in uniform, machine readable, and openly available formats. Collaborative web-based tools create online 'virtual departments' in which scholars dispersed sparsely across the globe can collaborate, and natural language processing tools counterbalance the scarcity of trained editors by enabling machine processing of Coptictextto produce searchable, annotated corpora.","['Caroline T. Schroeder', 'Amir Zeldes']","Journal of Data Mining & Digital Humanities, Special Issue on Collecting, Preserving, and Disseminating Endangered Cultural Heritage for New Understandings through Multilingual Approaches (September 23, 2020) jdmdh:5969",arXiv,2020,https://doi.org/10.48550/arXiv.1912.05082,Anomali
Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with Mainstream Religious Group,"Online propaganda is central to the recruitment strategies of extremist groups and in recent years these efforts have increasingly extended to women. To investigate ISIS' approach to targeting women in their online propaganda and uncover implications for counterterrorism, we rely ontextminingand natural language processing (NLP). Specifically, we extract articles published in Dabiq and Rumiyah (ISIS's online English language publications) to identify prominent topics. To identify similarities or differences between thesetextsand those produced by non-violent religious groups, we extend the analysis to articles from a Catholic forum dedicated to women. We also perform an emotional analysis of both of these resources to better understand the emotional components of propaganda. We rely on Depechemood (a lexical-base emotion analysis method) to detect emotions most likely to be evoked in readers of these materials. The findings indicate that the emotional appeal of ISIS and Catholic materials are similar","['Mojtaba Heidarysafa', 'Kamran Kowsari', 'Tolu Odukoya', 'Philip Potter', 'Laura E. Barnes', 'Donald E. Brown']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.03804,Anomali
Visual-Textual Association with Hardest and Semi-Hard Negative Pairs Mining for Person Search,"Searching persons in large-scale image databases with the query of natural language description is a more practical important applications in video surveillance. Intuitively, for person search, the core issue should be visual-textual association, which is still an extremely challenging task, due to the contradiction between the high abstraction of textual description and the intuitive expression of visual images. However, for this task, while positive image-textpairs are always well provided, most existing methods doesn't tackle this problem effectively byminingmore reasonable negative pairs. In this paper, we proposed a novel visual-textual association approach with visual and textual attention, and cross-modality hardest and semi-hard negative pairmining. In order to evaluate the effectiveness and feasibility of the proposed approach, we conduct extensive experiments on typical person search datasdet: CUHK-PEDES, in which our approach achieves the top1 score of 55.32% as a new state-of-the-art. Besides, we also evaluate the semi-hard pairminingapproach in COCO caption dataset, and validate the effectiveness and complementarity of the methods.","['Jing Ge', 'Guangyu Gao', 'Zhen Liu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.03083,Anomali
Autonomous Robot Swarms for Off-World Construction and Resource Mining,"Kick-starting the space economy requires identification of critical resources that can lower the cost of space transport, sustain logistic bases and communication relay networks between major nodes in the network. One important challenge with this space-economy is ensuring the low-cost transport of raw materials from one gravity-well to another. The escape delta-v of 11.2 km/s from Earth makes this proposition very expensive. Transporting materials from the Moon takes 2.4 km/s and from Mars 5.0 km/s. Based on these factors, the Moon and Mars have the potential to export material into this space economy. Water has been identified as a critical resource both to sustain human-life but also for use in propulsion, attitude-control, power, thermal storage and radiation protection systems.There is also important need for construction materials such as aluminum, iron/steel, and titanium. Based upon these important findings, we have developed an energy model to determine the feasibility of developing aminingbase on the Moon and Mars. Theseminingbasemineand principally exports water, aluminum, titanium and steel. Our designs for aminingbase utilize renewable energy sources namely photovoltaics and solar-thermal concentrators to provide power to construct the base, keep it operational and export water and other resources using a Mass Driver. Using the energy model developed, we will determine the energy per Earth-day to export 100 tons each of water, titanium, aluminum and low-grade steel into escape velocity of the Moon and Mars. We perform a detailed comparison of the energy required for construction of similar bases on the Moon and Mars, in addition to the operating energy required for regolith excavation, processing, refining and finally transport off-the-body.",['Jekan Thangavelautham'],,arXiv,2019,https://doi.org/10.48550/arXiv.1912.02652,Anomali
SemEval-2016 Task 4: Sentiment Analysis in Twitter,"This paper discusses the fourth year of the ``Sentiment Analysis in Twitter Task''. SemEval-2016 Task 4 comprises five subtasks, three of which represent a significant departure from previous editions. The first two subtasks are reruns from prior years and ask to predict the overall sentiment, and the sentiment towards a topic in a tweet. The three new subtasks focus on two variants of the basic ``sentiment classification in Twitter'' task. The first variant adopts a five-point scale, which confers an ordinal character to the classification task. The second variant focuses on the correct estimation of the prevalence of each class of interest, a task which has been called quantification in the supervised learning literature. The task continues to be very popular, attracting a total of 43 teams.","['Preslav Nakov', 'Alan Ritter', 'Sara Rosenthal', 'Fabrizio Sebastiani', 'Veselin Stoyanov']","Final version published in the Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval 2016), San Diego, US, 2016, pp. 1-18",arXiv,2019,https://doi.org/10.48550/arXiv.1912.01973,Anomali
Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian,"News website comment sections are spaces where potentially conflicting opinions and beliefs are voiced. Addressing questions of how to study such cultural and societal conflicts through technological means, the present article critically examines possibilities and limitations of machine-guided exploration and potential facilitation of on-line opinion dynamics. These investigations are guided by a discussion of an experimental observatory forminingand analyzing opinions from climate change-related user comments on news articles from the TheGuardian.com. This observatory combines causal mapping methods with computationaltextanalysis in order tominebeliefs and visualize opinion landscapes based on expressions of causation. By (1) introducing digital methods and open infrastructures for data exploration and analysis and (2) engaging in debates about the implications of such methods and infrastructures, notably in terms of the leap from opinion observation to debate facilitation, the article aims to make a practical and theoretical contribution to the study of opinion dynamics and conflict in new media environments.","['Tom Willaert', 'Sven Banisch', 'Paul Van Eecke', 'Katrien Beuls']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.01252,Anomali
Low Rank Factorization for Compact Multi-Head Self-Attention,"Effective representation learning fromtexthas been an active area of research in the fields of NLP andtextmining. Attention mechanisms have been at the forefront in order to learn contextual sentence representations. Current state-of-the-art approaches for many NLP tasks use large pre-trained language models such as BERT, XLNet and so on for learning representations. These models are based on the Transformer architecture that involves recurrent blocks of computation consisting of multi-head self-attention and feedforward networks. One of the major bottlenecks largely contributing to the computational complexity of the Transformer models is the self-attention layer, that is both computationally expensive and parameter intensive. In this work, we introduce a novel multi-head self-attention mechanism operating on GRUs that is shown to be computationally cheaper and more parameter efficient than self-attention mechanism proposed in Transformers fortextclassification tasks. The efficiency of our approach mainly stems from two optimizations; 1) we use low-rank matrix factorization of the affinity matrix to efficiently get multiple attention distributions instead of having separate parameters for each head 2) attention scores are obtained by querying a global context vector instead of densely querying all the words in the sentence. We evaluate the performance of the proposed model on tasks such as sentiment analysis from movie reviews, predicting business ratings from reviews and classifying news articles into topics. We find that the proposed approach matches or outperforms a series of strong baselines and is more parameter efficient than comparable multi-head approaches. We also perform qualitative analyses to verify that the proposed approach is interpretable and captures context-dependent word importance.","['Sneha Mehta', 'Huzefa Rangwala', 'Naren Ramakrishnan']",,arXiv,2020,https://doi.org/10.48550/arXiv.1912.00835,Anomali
Mis-classified Vector Guided Softmax Loss for Face Recognition,"Face recognition has witnessed significant progress due to the advances of deep convolutional neural networks (CNNs), the central task of which is how to improve the feature discrimination. To this end, several margin-based (\textit{e.g.}, angular, additive and additive angular margins) softmax loss functions have been proposed to increase the feature margin between different classes. However, despite great achievements have been made, they mainly suffer from three issues: 1) Obviously, they ignore the importance of informative featuresminingfor discriminative learning; 2) They encourage the feature margin only from the ground truth class, without realizing the discriminability from other non-ground truth classes; 3) The feature margin between different classes is set to be same and fixed, which may not adapt the situations very well. To cope with these issues, this paper develops a novel loss function, which adaptively emphasizes the mis-classified feature vectors to guide the discriminative feature learning. Thus we can address all the above issues and achieve more discriminative face features. To the best of our knowledge, this is the first attempt to inherit the advantages of feature margin and featuremininginto a unified loss function. Experimental results on several benchmarks have demonstrated the effectiveness of our method over state-of-the-art alternatives.","['Xiaobo Wang', 'Shifeng Zhang', 'Shuo Wang', 'Tianyu Fu', 'Hailin Shi', 'Tao Mei']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.00833,Anomali
Learning a faceted customer segmentation for discovering new business opportunities at Intel,"For sales and marketing organizations within large enterprises, identifying and understanding new markets, customers and partners is a key challenge. Intel's Sales and Marketing Group (SMG) faces similar challenges while growing in new markets and domains and evolving its existing business. In today's complex technological and commercial landscape, there is need for intelligent automation supporting a fine-grained understanding of businesses in order to help SMG sift through millions of companies across many geographies and languages and identify relevant directions. We present a system developed in our company thatminesmillions of public business web pages, and extracts a faceted customer representation. We focus on two key customer aspects that are essential for finding relevant opportunities: industry segments (ranging from broad verticals such as healthcare, to more specific fields such as 'video analytics') and functional roles (e.g., 'manufacturer' or 'retail'). To address the challenge of labeled data collection, we enrich our data with external information gleaned from Wikipedia, and develop a semi-supervised multi-label, multi-lingual deep learning model that parses customer websitetextsand classifies them into their respective facets. Our system scans and indexes companies as part of a large-scale knowledge graph that currently holds tens of millions of connected entities with thousands being fetched, enriched and connected to the graph by the hour in real time, and also supports knowledge and insight discovery. In experiments conducted in our company, we are able to significantly boost the performance of sales personnel in the task of discovering new customers and commercial partnership opportunities.","['Itay Lieder', 'Meirav Segal', 'Eran Avidan', 'Asaf Cohen', 'Tom Hope']",,arXiv,2019,https://doi.org/10.48550/arXiv.1912.00778,Anomali
Sideways Transliteration: How to Transliterate Multicultural Person Names?,"In a global setting,textscontain transliterated names from many cultural origins. Correct transliteration depends not only on target and source languages but also, on the source language of the name. We introduce a novel methodology for transliteration of names originating in different languages using only monolingual resources. Our method is based on a step of noisy transliteration and then ranking of the results based on origin specific letter models. The transliteration table used for noisy generation is learned in an unsupervised manner for each possible origin language. We present a solution for gathering monolingual training data used by our method byminingof social media sites such as Facebook and Wikipedia. We present results in the context of transliterating from English to Hebrew and provide an online web service for transliteration from English to Hebrew","['Raphael Cohen', 'Michael Elhadad']",,arXiv,2019,https://doi.org/10.48550/arXiv.1911.12022,Anomali
Self-interaction of ultrashort pulses in an epsilon-near-zero nonlinear material at the telecom wavelength,"Dynamics of femtosecond pulses with the telecom carrier wavelength is investigated numerically in a subwavelength layer of an indium tin oxide (ITO) epsilon-near-zero (ENZ) material with high dispersion and high nonlinearity. Due to the subwavelength thickness of the ITO ENZ material, and the fact that the pulse's propagation time is shorter than its temporal width, multiple reflections give rise to self-interaction in both spectral and temporal domains, especially at wavelengths longer than the ENZ point, at which the reflections are significantly stronger. A larger absolute value of the pulse's chirp strongly affects the self-interaction by redistributing energy between wavelengths, while the sign of the chirp affects the interaction in the temporal domain. It is also found that, when two identical pulses are launched simultaneously from both ends, a subwavelength counterpart of a standing-wave state can be established. It shows robust energy localization in the middle of the sample, in terms of both the spectral and temporal intensity distributions.","['Jiaye Wu', 'Boris A. Malomed', 'H. Y. Fu', 'Qian Li']","Optics Express 27(26): 37298-37307, 2019",arXiv,2019,https://doi.org/10.48550/arXiv.1911.11917,Anomali
Word-Class Embeddings for Multiclass Text Classification,"Pre-trained word embeddings encode general word semantics and lexical regularities of natural language, and have proven useful across many NLP tasks, including word sense disambiguation, machine translation, and sentiment analysis, to name a few. In supervised tasks such as multiclasstextclassification (the focus of this article) it seems appealing to enhance word representations with ad-hoc embeddings that encode task-specific information. We propose (supervised) word-class embeddings (WCEs), and show that, when concatenated to (unsupervised) pre-trained word embeddings, they substantially facilitate the training of deep-learning models in multiclass classification by topic. We show empirical evidence that WCEs yield a consistent improvement in multiclass classification accuracy, using four popular neural architectures and six widely used and publicly available datasets for multiclasstextclassification. Our code that implements WCEs is publicly available at https://github.com/AlexMoreo/word-class-embeddings","['Alejandro Moreo', 'Andrea Esuli', 'Fabrizio Sebastiani']","Final version published in Data Mining and Knowledge Discovery 35(3), 911-963, 2021",arXiv,2019,https://doi.org/10.48550/arXiv.1911.11506,Anomali
My Approach = Your Apparatus? Entropy-Based Topic Modeling on Multiple Domain-Specific Text Collections,"Comparativetextminingextends from genre analysis and political bias detection to the revelation of cultural and geographic differences, through to the search for prior art across patents and scientific papers. These applications use cross-collection topic modeling for the exploration, clustering, and comparison of large sets of documents, such as digital libraries. However, topic modeling on documents from different collections is challenging because of domain-specific vocabulary. We present a cross-collection topic model combined with automatic domain term extraction and phrase segmentation. This model distinguishes collection-specific and collection-independent words based on information entropy and reveals commonalities and differences of multipletextcollections. We evaluate our model on patents, scientific papers, newspaper articles, forum posts, and Wikipedia articles. In comparison to state-of-the-art cross-collection topic modeling, our model achieves up to 13% higher topic coherence, up to 4% lower perplexity, and up to 31% higher document classification accuracy. More importantly, our approach is the first topic model that ensures disjunct general and specific word distributions, resulting in clear-cut topic representations.","['Julian Risch', 'Ralf Krestel']",Proceedings of the 18th ACM/IEEE Joint Conference on Digital Libraries (JCDL). bll. 283-292 (2018),arXiv,2019,https://doi.org/10.48550/arXiv.1911.11240,Anomali
Efficient Global String Kernel with Random Features: Beyond Counting Substructures,"Analysis of large-scale sequential data has been one of the most crucial tasks in areas such as bioinformatics,text, and audiomining. Existing string kernels, however, either (i) rely on local features of short substructures in the string, which hardly capture long discriminative patterns, (ii) sum over too many substructures, such as all possible subsequences, which leads to diagonal dominance of the kernel matrix, or (iii) rely on non-positive-definite similarity measures derived from the edit distance. Furthermore, while there have been works addressing the computational challenge with respect to the length of string, most of them still experience quadratic complexity in terms of the number of training samples when used in a kernel-based classifier. In this paper, we present a new class of global string kernels that aims to (i) discover global properties hidden in the strings through global alignments, (ii) maintain positive-definiteness of the kernel, without introducing a diagonal dominant kernel matrix, and (iii) have a training cost linear with respect to not only the length of the string but also the number of training string samples. To this end, the proposed kernels are explicitly defined through a series of different random feature maps, each corresponding to a distribution of random strings. We show that kernels defined this way are always positive-definite, and exhibit computational benefits as they always produce \emph{Random String Embeddings (RSE)} that can be directly used in any linear classification models. Our extensive experiments on nine benchmark datasets corroborate that RSE achieves better or comparable accuracy in comparison to state-of-the-art baselines, especially with the strings of longer lengths. In addition, we empirically show that RSE scales linearly with the increase of the number and the length of string.","['Lingfei Wu', 'Ian En-Hsu Yen', 'Siyu Huo', 'Liang Zhao', 'Kun Xu', 'Liang Ma', 'Shouling Ji', 'Charu Aggarwal']",,arXiv,2019,https://doi.org/10.48550/arXiv.1911.11121,Anomali
Analysing Time-Stamped Co-Editing Networks in Software Development Teams using git2net,"Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repositoryminingliterature is the inference of developer networks capturing e.g. collaboration, coordination, or communication from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts. Because this neglects detailed information on code changes and code ownership we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It usestextminingtechniques to analyse the detailed history of textual modifications within files. We apply our tool in two case studies using GitHub repositories of multiple Open Source as well as a commercial software project. Specifically, we use data on more than 1.2 million commits and more than 25'000 developers to test a hypothesis on the relation between developer productivity and co-editing patterns in software teams. We argue that git2net opens up a massive new source of high-resolution data on human collaboration patterns that can be used to advance theory in empirical software engineering, computational social science, and organisational studies.","['Christoph Gote', 'Ingo Scholtes', 'Frank Schweitzer']",,arXiv,2019,https://doi.org/10.48550/arXiv.1911.09484,Anomali
Entity Extraction with Knowledge from Web Scale Corpora,"Entity extraction is an important task intextminingand natural language processing. A popular method for entity extraction is by comparing substrings from freetextagainst a dictionary of entities. In this paper, we present several techniques as a post-processing step for improving the effectiveness of the existing entity extraction technique. These techniques utilise models trained with the web-scale corpora which makes our techniques robust and versatile. Experiments show that our techniques bring a notable improvement on efficiency and effectiveness.","['Zeyi Wen', 'Zeyu Huang', 'Rui Zhang']",,arXiv,2019,https://doi.org/10.48550/arXiv.1911.09373,Anomali
Python vs. R: A Text Mining Approach for analyzing the Research Trends in Scopus Database,"In the contemporary world, with the incubation of advanced technologies and tremendous outbursts of research works, analyzing big data to incorporate research strategies becomes more helpful using the tools and techniques presented in the current research scenario. This paper indeed tries to tackle the most prominent challenges relating to big data analysis by utilizing atextminingapproach to analyze research data published in the field of production management as a case to begin with. The study has been conducted by considering research data of International Journal of Production Research (IJPR) indexed in Scopus between 1961-2017 by dividing the analysis incurred into 3 fragments being 1961-1990, 1991-2010 and finally 2011-2017 as a case to highlight the focus of journal. This has indeed provided multi-faceted benefits such as increasing the effectiveness of the procured data with well-established comparisons between R and Python Programming along with providing detailed research trends on the research work incubated. The results of the study highlighted some most prominent topics in the existing IJPR literature such as system's optimization, supplier selection, process design, etc. providing well-established details relating to ongoing research works. The study also compared both languages suiting to a particular field of study for better comprehension and vastness of the research topics. The current research work is one of the part of a copyright work with registration number SW-10310/2018 titled Program for Analyzing Key Trends in Research Data-set. It has been designed in Python for carrying out detailed content analysis based on the available research database in bib format as in the current context it has been applied for IJPR journal and can be replicated on articles of any domain found using keyword search.","['Neeraj Bhanot', 'Harwinder Singh', 'Divyansu Sharma', 'Harshit Jain', 'Shreyansh Jain']",,arXiv,2019,https://doi.org/10.48550/arXiv.1911.08271,Anomali
Sentiment Analysis for Arabic in Social Media Network: A Systematic Mapping Study,"With the expansion in tenders on the Internet and social media, Arabic Sentiment Analysis (ASA) has assumed a significant position in the field oftextminingstudy and has since remained used to explore the sentiments of users about services, various products or topics conversed over the Internet. This mapping paper designs to comprehensively investigate the papers demographics, fertility, and directions of the ASA research domain. Furthermore, plans to analyze current ASA techniques and find movements in the research. This paper describes a systematic mapping study (SMS) of 51 primary selected studies (PSS) is handled with the approval of an evidence-based systematic method to ensure handling of all related papers. The analyzed results showed the increase of both the ASA research area and numbers of publications per year since 2015. Three main research facets were found, i.e. validation, solution, and evaluation research, with solution research becoming more treatment than another research type. Therefore numerous contribution facets were singled out. In totality, the general demographics of the ASA research field were highlighted and discussed","['Mohamed Elhag M. Abo', 'Ram Gopal Raj', 'Atika Qazi', 'Abubakar Zakari']",,arXiv,2019,https://doi.org/10.48550/arXiv.1911.05483,Anomali
CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB,"We show that margin-based bitextminingin a multilingual sentence space can be applied to monolingual corpora of billions of sentences. We are using ten snapshots of a curated common crawl corpus (Wenzek et al., 2019) totalling 32.7 billion unique sentences. Using one unified approach for 38 languages, we were able tomine4.5 billions parallel sentences, out of which 661 million are aligned with English. 20 language pairs have more then 30 million parallel sentences, 112 more then 10 million, and most more than one million, including direct alignments between many European or Asian languages.
  To evaluate the quality of theminedbitexts, we train NMT systems for most of the language pairs and evaluate them on TED, WMT and WAT test sets. Using ourminedbitexts only and no human translated parallel data, we achieve a new state-of-the-art for a single system on the WMT'19 test set for translation between English and German, Russian and Chinese, as well as German/French. In particular, our English/German system outperforms the best single one by close to 4 BLEU points and is almost on pair with best WMT'19 evaluation system which uses system combination and back-translation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2019 workshop on Asian Translation (WAT).","['Holger Schwenk', 'Guillaume Wenzek', 'Sergey Edunov', 'Edouard Grave', 'Armand Joulin']",,arXiv,2020,https://doi.org/10.48550/arXiv.1911.04944,Anomali
Text Mining using Nonnegative Matrix Factorization and Latent Semantic Analysis,"Textclustering is arguably one of the most important topics in modern datamining. Nevertheless,textdata require tokenization which usually yields a very large and highly sparse term-document matrix, which is usually difficult to process using conventional machine learning algorithms. Methods such as Latent Semantic Analysis have helped mitigate this issue, but are nevertheless not completely stable in practice. As a result, we propose a new feature agglomeration method based on Nonnegative Matrix Factorization, which is employed to separate the terms into groups, and then each group's term vectors are agglomerated into a new feature vector. Together, these feature vectors create a new feature space much more suitable for clustering. In addition, we propose a new deterministic initialization for spherical K-Means, which proves very useful for this specific type of data. In order to evaluate the proposed method, we compare it to some of the latest research done in this field, as well as some of the most practiced methods. In our experiments, we conclude that the proposed method either significantly improves clustering performance, or maintains the performance of other methods, while improving stability in results.","['Ali Hassani', 'Amir Iranmanesh', 'Najme Mansouri']",,arXiv,2020,https://doi.org/10.48550/arXiv.1911.04705,Anomali
GRASS: Graph Spectral Sparsification Leveraging Scalable Spectral Perturbation Analysis,"Spectral graph sparsification aims to find ultra-sparse subgraphs whose Laplacian matrix can well approximate the original Laplacian eigenvalues and eigenvectors. In recent years, spectral sparsification techniques have been extensively studied for accelerating various numerical and graph-related applications. Prior nearly-linear-time spectral sparsification methods first extract low-stretch spanning tree from the original graph to form the backbone of the sparsifier, and then recover small portions of spectrally-critical off-tree edges to the spanning tree to significantly improve the approximation quality. However, it is not clear how many off-tree edges should be recovered for achieving a desired spectral similarity level within the sparsifier. Motivated by recent graph signal processing techniques, this paper proposes a similarity-aware spectral graph sparsification framework that leverages efficient spectral off-tree edge embedding and filtering schemes to construct spectral sparsifiers with guaranteed spectral similarity (relative condition number) level. An iterative graph densification scheme is also introduced to facilitate efficient and effective filtering of off-tree edges for highly ill-conditioned problems. The proposed method has been validated using various kinds of graphs obtained from public domain sparse matrix collections relevant to VLSI CAD, finite element analysis, as well as social and data networks frequently studied in many machine learning and dataminingapplications. For instance, a sparse SDD matrix with 40 million unknowns and 180 million nonzeros can be solved (1E-3 accuracy level) within two minutes using a single CPU core and about 6GB memory.",['Zhuo Feng'],,arXiv,2020,https://doi.org/10.48550/arXiv.1911.04382,Anomali
S2ORC: The Semantic Scholar Open Research Corpus,"We introduce S2ORC, a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured fulltextfor 8.1M open access papers. Fulltextis annotated with automatically-detected inline mentions of citations, figures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a unified source, and create the largest publicly-available collection of machine-readable academictextto date. We hope this resource will facilitate research and development of tools and tasks fortextminingover academictext.","['Kyle Lo', 'Lucy Lu Wang', 'Mark Neumann', 'Rodney Kinney', 'Dan S. Weld']",,arXiv,2020,https://doi.org/10.48550/arXiv.1911.02782,Anomali
How to Better Distinguish Security Bug Reports (using Dual Hyperparameter Optimization,"Background: In order that the general public is not vulnerable to hackers, security bug reports need to be handled by small groups of engineers before being widely discussed. But learning how to distinguish the security bug reports from other bug reports is challenging since they may occur rarely. Dataminingmethods that can find such scarce targets require extensive optimization effort.
  Goal: The goal of this research is to aid practitioners as they struggle to optimize methods that try to distinguish between rare security bug reports and other bug reports.
  Method: Our proposed method, called Swift, is a dual optimizer that optimizes both learner and pre-processor options. Since this is a large space of options, Swift uses a technique called epsilon-dominance that learns how to avoid operations that do not significantly improve performance.
  Result: When compared to recent state-of-the-art results (from FARSEC which is published in TSE'18), we find that the Swift's dual optimization of both pre-processor and learner is more useful than optimizing each of them individually. For example, in a study of security bug reports from the Chromium dataset, the median recalls of FARSEC and Swift were 15.7% and 77.4%, respectively. For another example, in experiments with data from the Ambari project, the median recalls improved from 21.5% to 85.7% (FARSEC to SWIFT).
  Conclusion: Overall, our approach can quickly optimize models that achieve better recalls than the prior state-of-the-art. These increases in recall are associated with moderate increases in false positive rates (from 8% to 24%, median). For future work, these results suggest that dual optimization is both practical and useful.","['Rui Shu', 'Tianpei Xia', 'Jianfeng Chen', 'Laurie Williams', 'Tim Menzies']",,arXiv,2021,https://doi.org/10.48550/arXiv.1911.02476,Anomali
A BaTiO3-Based Electro-Optic Pockels Modulator Monolithically Integrated on an Advanced Silicon Photonics Platform,"To develop a new generation of high-speed photonic modulators on silicon-technology-based photonics, new materials with large Pockels coefficients have been transferred to silicon substrates. Previous approaches focus on realizing stand-alone devices on dedicated silicon substrates, incompatible with the fabrication process in silicon foundries. In this work, we demonstrate monolithic integration of electro-optic modulators based on the Pockels effect in barium titanate (BTO) thin films into the back-end-of-line of a photonic integrated circuit (PIC) platform. Molecular wafer bonding allows fully PIC-compatible integration of BTO-based devices and is, as shown, scalable to 200 mm wafers. The PIC-integrated BTO Mach-Zehnder modulators outperform conventional Si photonic modulators in modulation efficiency, losses, and static tuning power. The devices show excellent VπL (0.2 Vcm) and VπLα (1.3 VdB), work at high speed (25 Gbps), and can be tuned at low static power consumption (100 nW). Our concept demonstrates the possibility of monolithic integration of Pockels-based electro-optic modulators in advanced silicon photonic platforms.
  {\c} 2019 Optical Society of America. Users may use, reuse, and build upon the article, or use the article fortextor datamining, so long as such uses are for non-commercial purposes and appropriate attribution is maintained. All other rights are reserved.
  https://www.osapublishing.org/jlt/abstract.cfm?URI=jlt-37-5-1456
  Publication date: March 1, 2019
  This work was supported in part by the European Union (EU) under Horizon 2020 grant agreements no. H2020-ICT-2015-25-688579 (PHRESCO) and H2020-ICT-2017-1-780997 (plaCMOS).","['Felix Eltes', 'Christian Mai', 'Daniele Caimi', 'Marcel Kroh', 'Youri Popoff', 'Georg Winzer', 'Despoina Petousi', 'Stefan Lischke', 'J. Elliott Ortmann', 'Lukas Czornomaz', 'Lars Zimmermann', 'Jean Fompeyrine', 'Stefan Abel']","Journal of Lightwave Technology 37, 1456-1462 (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1911.02317,Anomali
GRACE: Generating Concise and Informative Contrastive Sample to Explain Neural Network Model's Prediction,"Despite the recent development in the topic of explainable AI/ML for image andtextdata, the majority of current solutions are not suitable to explain the prediction of neural network models when the datasets are tabular and their features are in high-dimensional vectorized formats. To mitigate this limitation, therefore, we borrow two notable ideas (i.e., ""explanation by intervention"" from causality and ""explanation are contrastive"" from philosophy) and propose a novel solution, named as GRACE, that better explains neural network models' predictions for tabular datasets. In particular, given a model's prediction as label X, GRACE intervenes and generates a minimally-modified contrastive sample to be classified as Y, with an intuitive textual explanation, answering the question of ""Why X rather than Y?"" We carry out comprehensive experiments using eleven public datasets of different scales and domains (e.g., # of features ranges from 5 to 216) and compare GRACE with competing baselines on different measures: fidelity, conciseness, info-gain, and influence. The user-studies show that our generated explanation is not only more intuitive and easy-to-understand but also facilitates end-users to make as much as 60% more accurate post-explanation decisions than that of Lime.","['Thai Le', 'Suhang Wang', 'Dongwon Lee']",,arXiv,2020,https://doi.org/10.48550/arXiv.1911.02042,Anomali
A Model for Spatial Outlier Detection Based on Weighted Neighborhood Relationship,"Spatial outliers are used to discover inconsistent objects producing implicit, hidden, and interesting knowledge, which has an effective role in decision-making process. In this paper, we propose a model to redefine the spatial neighborhood relationship by considering weights of the most effective parameters of neighboring objects in a given spatial data set. The spatial parameters, which are taken into our consideration, are distance, cost, and number of direct connections between neighboring objects. This model is adaptable to be applied on polygonal objects. The proposed model is applied to a GIS system supporting literacy project in Fayoum governorate.","['Ayman Taha', 'Hoda M. Onsi', 'Mohammed Nour El din', 'Osman M. Hegazy']","Egyptian Informatics Journal 2, 2005",arXiv,2019,https://doi.org/10.48550/arXiv.1911.01867,Anomali
Sentiment analysis model for Twitter data in Polish language,"Textmininganalysis of tweets gathered during Polish presidential election on May 10th, 2015. The project included implementation of engine to retrieve information from Twitter, building document corpora, corpora cleaning, and creating Term-Document Matrix. Each tweet from thetextcorpora was assigned a category based on its sentiment score. The score was calculated using the number of positive and/or negative emoticons and Polish words in each document. The result data set was used to train and test four machine learning classifiers, to select these providing most accurate automatic tweet classification results. The Naive Bayes and Maximum Entropy algorithms achieved the best accuracy of respectively 71.76% and 77.32%. All implementation tasks were completed using R programming language.",['Karol Chlasta'],,arXiv,2019,https://doi.org/10.48550/arXiv.1911.00985,Anomali
Dreaddit: A Reddit Dataset for Stress Analysis in Social Media,"Stress is a nigh-universal human experience, particularly in the online world. While stress can be a motivator, too much stress is associated with many negative health outcomes, making its identification useful across a range of domains. However, existing computational research typically only studies stress in domains such as speech, or in short genres such as Twitter. We present Dreaddit, a newtextcorpus of lengthy multi-domain social media data for the identification of stress. Our dataset consists of 190K posts from five different categories of Reddit communities; we additionally label 3.5K total segments taken from 3K posts using Amazon Mechanical Turk. We present preliminary supervised learning methods for identifying stress, both neural and traditional, and analyze the complexity and diversity of the data and characteristics of each category.","['Elsbeth Turcan', 'Kathleen McKeown']",,arXiv,2019,https://doi.org/10.48550/arXiv.1911.00133,Anomali
Great New Design: How Do We Talk about Media Architecture in Social Media,"In social media, we communicate through pictures, videos, short codes, links, partial phrases. It is a rich, and digitally documented communication channel that relies on a multitude of media and forms. These channels are sorted by algorithms as organizers of discourse, mostly with the goal of channeling attention. In this research, we used Twitter to study the way Media Architecture is discussed within the community of architects, designers, researchers and policy makers. We look at the way they spontaneously share opinions on their engagement with digital infrastructures, networked places and hybrid public spaces. What can we do with all those opinions? We propose here the use oftext-miningand machine learning techniques to identify important concepts and patterns in this prolific communication stream. We discuss how such techniques could inform the practice and emergence of future trends.",['Selena Savic'],,arXiv,2019,https://doi.org/10.48550/arXiv.1910.14395,Anomali
Explainable Prediction of Adverse Outcomes Using Clinical Notes,"Clinical notes contain a large amount of clinically valuable information that is ignored in many clinical decision support systems due to the difficulty that comes withminingthat information. Recent work has found success leveraging deep learning models for the prediction of clinical outcomes using clinical notes. However, these models fail to provide clinically relevant and interpretable information that clinicians can utilize for informed clinical care. In this work, we augment a popular convolutional model with an attention mechanism and apply it to unstructured clinical notes for the prediction of ICU readmission and mortality. We find that the addition of the attention mechanism leads to competitive performance while allowing for the straightforward interpretation of predictions. We develop clear visualizations to present important spans oftextfor both individual predictions and high-risk cohorts. We then conduct a qualitative analysis and demonstrate that our model is consistently attending to clinically meaningful portions of the narrative for all of the outcomes that we explore.","['Justin R. Lovelace', 'Nathan C. Hurley', 'Adrian D. Haimovich', 'Bobak J. Mortazavi']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.14095,Anomali
A Heuristically Modified FP-Tree for Ontology Learning with Applications in Education,"We propose a heuristically modified FP-Tree for ontology learning fromtext. Unlike previous research, for concept extraction, we use a regular expression parser approach widely adopted in compiler construction, i.e., deterministic finite automata (DFA). Thus, the concepts are extracted from unstructured documents. For ontology learning, we use a frequent patternminingapproach and employ a ruleminingheuristic function to enhance its quality. This process does not rely on predefined lexico-syntactic patterns, thus, it is applicable for different subjects. We employ the ontology in a question-answering system for students' content-related questions. For validation, we used textbook questions/answers and questions from online course forums. Subject experts rated the quality of the system's answers on a subset of questions and their ratings were used to identify the most appropriate automatic semantictextsimilarity metric to use as a validation metric for all answers. The Latent Semantic Analysis was identified as the closest to the experts' ratings. We compared the use of our ontology with the use of Text2Onto for the question-answering system and found that with our ontology 80% of the questions were answered, while with Text2Onto only 28.4% were answered, thanks to the finer grained hierarchy our approach is able to produce.","['Safwan Shatnawi', 'Mohamed Medhat Gaber', 'Mihaela Cocea']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.13561,Anomali
Online News Media Website Ranking Using User Generated Content,"News media websites are important online resources that have drawn great attention oftextminingresearchers. The main aim of this study is to propose a framework for ranking online news websites from different viewpoints. The ranking of news websites is useful information, which can benefit many news-related tasks such as news retrieval and news recommendation. In the proposed framework, the ranking of news websites is obtained by calculating three measures introduced in the paper and based on user-generated content. Each proposed measure is concerned with the performance of news websites from a particular viewpoint including the completeness of news reports, the diversity of events being covered by the website and its speed. The use of user-generated content in this framework, as a partly-unbiased, real-time and low cost content on the web distinguishes the proposed news website ranking framework from the literature. The results obtained for three prominent news websites, BBC, CNN, NYTimes, show that BBC has the best performance in terms of news completeness and speed, and NYTimes has the best diversity in comparison with the other two websites.","['Samaneh Karimi', 'Azadeh Shakery', 'Rakesh Verma']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.12441,Anomali
SoulMate: Short-text author linking through Multi-aspect temporal-textual embedding,"Linking authors of short-textcontents has important usages in many applications, including Named Entity Recognition (NER) and human community detection. However, certain challenges lie ahead. Firstly, the input short-textcontents are noisy, ambiguous, and do not follow the grammatical rules. Secondly, traditionaltextminingmethods fail to effectively extract concepts through words and phrases. Thirdly, the textual contents are temporally skewed, which can affect the semantic understanding by multiple time facets. Finally, using the complementary knowledge-bases makes the results biased to the content of the external database and deviates the understanding and interpretation away from the real nature of the given shorttextcorpus. To overcome these challenges, we devise a neural network-based temporal-textual framework that generates the tightly connected author subgraphs from microblog short-textcontents. Our approach, on the one hand, computes the relevance score (edge weight) between the authors through considering a portmanteau of contents and concepts, and on the other hand, employs a stack-wise graph cutting algorithm to extract the communities of the related authors. Experimental results show that compared to other knowledge-centered competitors, our multi-aspect vector space model can achieve a higher performance in linking short-textauthors. Additionally, given the author linking task, the more comprehensive the dataset is, the higher the significance of the extracted concepts will be.","['Saeed Najafipour', 'Saeid Hosseini', 'Wen Hua', 'Mohammad Reza Kangavari', 'Xiaofang Zhou']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.12180,Anomali
A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image Synthesis,"Text-to-image synthesis refers to computational methods which translate human written textual descriptions, in the form of keywords or sentences, into images with similar semantic meaning to thetext. In earlier research, image synthesis relied mainly on word to image correlation analysis combined with supervised methods to find best alignment of the visual content matching to thetext. Recent progress in deep learning (DL) has brought a new set of unsupervised deep learning methods, particularly deep generative models which are able to generate realistic visual images using suitably trained neural network models. In this paper, we review the most recent development in thetext-to-image synthesis research domain. Our survey first introduces image synthesis and its challenges, and then reviews key concepts such as generative adversarial networks (GANs) and deep convolutional encoder-decoder neural networks (DCNN). After that, we propose a taxonomy to summarize GAN basedtext-to-image synthesis into four major categories: Semantic Enhancement GANs, Resolution Enhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs. We elaborate the main objective of each group, and further review typical GAN architectures in each group. The taxonomy and the review outline the techniques and the evolution of different approaches, and eventually provide a clear roadmap to summarize the list of contemporaneous solutions that utilize GANs and DCNNs to generate enthralling results in categories such as human faces, birds, flowers, room interiors, object reconstruction from edge maps (games) etc. The survey will conclude with a comparison of the proposed solutions, challenges that remain unresolved, and future developments in thetext-to-image synthesis domain.","['Jorge Agnese', 'Jonathan Herrera', 'Haicheng Tao', 'Xingquan Zhu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.09399,Anomali
Rosetta: Large scale system for text detection and recognition in images,"In this paper we present a deployed, scalable optical character recognition (OCR) system, which we call Rosetta, designed to process images uploaded daily at Facebook scale. Sharing of image content has become one of the primary ways to communicate information among internet users within social networks such as Facebook and Instagram, and the understanding of such media, including its textual information, is of paramount importance to facilitate search and recommendation applications. We present modeling techniques for efficient detection and recognition oftextin images and describe Rosetta's system architecture. We perform extensive evaluation of presented technologies, explain useful practical approaches to build an OCR system at scale, and provide insightful intuitions as to why and how certain components work based on the lessons learnt during the development and deployment of the system.","['Fedor Borisyuk', 'Albert Gordo', 'Viswanath Sivakumar']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.05085,Anomali
Multi-modal Deep Analysis for Multimedia,"With the rapid development of Internet and multimedia services in the past decade, a huge amount of user-generated and service provider-generated multimedia data become available. These data are heterogeneous and multi-modal in nature, imposing great challenges for processing and analyzing them. Multi-modal data consist of a mixture of various types of data from different modalities such astexts, images, videos, audios etc. In this article, we present a deep and comprehensive overview for multi-modal analysis in multimedia. We introduce two scientific research problems, data-driven correlational representation and knowledge-guided fusion for multimedia analysis. To address the two scientific problems, we investigate them from the following aspects: 1) multi-modal correlational representation: multi-modal fusion of data across different modalities, and 2) multi-modal data and knowledge fusion: multi-modal fusion of data with domain knowledge. More specifically, on data-driven correlational representation, we highlight three important categories of methods, such as multi-modal deep representation, multi-modal transfer learning, and multi-modal hashing. On knowledge-guided fusion, we discuss the approaches for fusing knowledge with data and four exemplar applications that require various kinds of domain knowledge, including multi-modal visual question answering, multi-modal video summarization, multi-modal visual patternminingand multi-modal recommendation. Finally, we bring forward our insights and future research directions.","['Wenwu Zhu', 'Xin Wang', 'Hongzhi Li']",,arXiv,2020,https://doi.org/10.48550/arXiv.1910.04964,Anomali
Dealing with Stochasticity in Biological ODE Models,"Mathematical modeling with Ordinary Differential Equations (ODEs) has proven to be extremely successful in a variety of fields, including biology. However, these models are completely deterministic given a certain set of initial conditions. We convert mathematical ODE models of three benchmark biological systems to Dynamic Bayesian Networks (DBNs). The DBN model can handle model uncertainty and data uncertainty in a principled manner. They can be used for temporal dataminingfor noisy and missing variables. We apply Particle Filtering algorithm to infer the model variables by re-estimating the models parameters of various biological ODE models. The model parameters are automatically re-estimated using temporal evidence in the form of data streams. The results show that DBNs are capable of inferring the model variables of the ODE model with high accuracy in situations where data is missing, incomplete, sparse and irregular and true values of model parameters are not known.","['Hamda Ajmal', 'Michael Madden', 'Catherine Enright']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.04909,Anomali
Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation,"Randomized controlled trials (RCTs) represent the paramount evidence of clinical medicine. Using machines to interpret the massive amount of RCTs has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for conclusion generation. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and correctness in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore.","['Alexander Te-Wei Shieh', 'Yung-Sung Chuang', 'Shang-Yu Su', 'Yun-Nung Chen']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.01462,Anomali
"CubeNet: Multi-Facet Hierarchical Heterogeneous Network Construction, Analysis, and Mining","Due to the ever-increasing size of data, construction, analysis andminingof universal massive networks are becoming forbidden and meaningless. In this work, we outline a novel framework called CubeNet, which systematically constructs and organizes real-world networks into different but correlated semantic cells, to support various downstream network analysis andminingtasks with better flexibility, deeper insights and higher efficiency. Particular, we promote our recent research ontextand networkminingwith novel concepts and techniques to (1) construct four real-world large-scale multi-facet hierarchical heterogeneous networks; (2) enable insightful OLAP-style network analysis; (3) facilitate localized and contextual networkmining. Although some functions have been covered individually in our previous work, a systematic and efficient realization of an organic system has not been studied, while some functions are still our on-going research tasks. By integrating them, CubeNet may not only showcase the utility of our recent research, but also inspire and stimulate future research on effective, insightful and scalable knowledge discovery under this novel framework.","['Carl Yang', 'Dai Teng', 'Siyang Liu', 'Sayantani Basu', 'Jieyu Zhang', 'Jiaming Shen', 'Chao Zhang', 'Jingbo Shang', 'Lance Kaplan', 'Timothy Harratty', 'Jiawei Han']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.01451,Anomali
Weakly Supervised Attention Networks for Fine-Grained Opinion Mining and Public Health,"In many review classification applications, a fine-grained analysis of the reviews is desirable, because different segments (e.g., sentences) of a review may focus on different aspects of the entity in question. However, training supervised models for segment-level classification requires segment labels, which may be more difficult or expensive to obtain than review labels. In this paper, we employ Multiple Instance Learning (MIL) and use only weak supervision in the form of a single label per review. First, we show that when inappropriate MIL aggregation functions are used, then MIL-based networks are outperformed by simpler baselines. Second, we propose a new aggregation function based on the sigmoid attention mechanism and show that our proposed model outperforms the state-of-the-art models for segment-level sentiment classification (by up to 9.8% in F1). Finally, we highlight the importance of fine-grained predictions in an important public-health application: finding actionable reports of foodborne illness. We show that our model achieves 48.6% higher recall compared to previous models, thus increasing the chance of identifying previously unknown foodborne outbreaks.","['Giannis Karamanolakis', 'Daniel Hsu', 'Luis Gravano']",,arXiv,2019,https://doi.org/10.48550/arXiv.1910.00054,Anomali
Optical scheme for cryptographic commitments with physical unclonable keys,"We investigate the possibility of using multiple-scattering optical media, as resources of randomness in cryptographic tasks pertaining to commitments and auctions. The proposed commitment protocol exploits standard wavefront-shaping and heterodyne-detection techniques, and can be implemented with current technology. Its security is discussed in the framework of a tamper-resistant trusted setup.",['Georgios M. Nikolopoulos'],"Optics Express Vol. 27, 29367-29379 (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1909.13094,Anomali
W-RNN: News text classification based on a Weighted RNN,"Most of the information is stored astext, sotextminingis regarded as having high commercial potential. Aiming at the semantic constraint problem of classification methods based on sparse representation, we propose a weighted recurrent neural network (W-RNN), which can fully extracttextserialization semantic information. For the problem that the feature high dimensionality and unclear semantic relationship intextdata representation, we first utilize the word vector to represent the vocabulary in thetextand use Recurrent Neural Network (RNN) to extract features of the serializedtextdata. The word vector is then automatically weighted and summed using the intermediate output of the word vector to form thetextrepresentation vector. Finally, the neural network is used for classification. W-RNN is verified on the news dataset and proves that W-RNN is superior to other four baseline methods in Precision, Recall, F1 and loss values, which is suitable fortextclassification.","['Dan Wang', 'Jibing Gong', 'Yaxi Song']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.13077,Anomali
Stock Market Forecasting Based on Text Mining Technology: A Support Vector Machine Method,"News items have a significant impact on stock markets but the ways are obscure. Many previous works have aimed at finding accurate stock market forecasting models. In this paper, we usetextminingand sentiment analysis on Chinese online financial news, to predict Chinese stock tendency and stock prices based on support vector machine (SVM). Firstly, we collect 2,302,692 news items, which date from 1/1/2008 to 1/1/2015. Secondly, based on this dataset, a specific domain stop-word dictionary and a precise sentiment dictionary are formed. Thirdly, we propose a forecasting model using SVM. On the algorithm of SVM implementation, we also propose two-parameter optimization algorithms to search for the best initial parameter setting. The result shows that parameter G has the main effect, while parameter C's effect is not obvious. Furthermore, support vector regression (SVR) models for different Chinese stocks are similar whereas in support vector classification (SVC) models best parameters are quite differential. Series of contrast experiments show that: a) News has significant influence on stock market; b) Expansion input vector for additional situations when that day has no news data is better than normal input in SVR, yet is worse in SVC; c) SVR shows a fantastic degree of fitting in predicting stock fluctuation while such result has some time lag; d) News effect time lag for stock market is less than two days; e) In SVC, historic stock data has a most efficient time lag which is about 10 days, whereas in SVR this effect is not obvious. Besides, based on the special structure of the input vector, we also design a method to calculate the financial source impact factor. Result suggests that the news quality and audience number both have a significant effect on the source impact factor. Besides, for Chinese investors, traditional media has more influence than digital media.","['Yancong Xie', 'Hongxun Jiang']",J. Comp. 12 (2017) 500-510,arXiv,2019,https://doi.org/10.48550/arXiv.1909.12789,Anomali
Coin_flipper at eHealth-KD Challenge 2019: Voting LSTMs for Key Phrases and Semantic Relation Identification Applied to Spanish eHealth Texts,"This paper describes our approach presented for the eHealth-KD 2019 challenge. Our participation was aimed at testing how far we could go using generic tools forText-Processing but, at the same time, using common optimization techniques in the field of DataMining. The architecture proposed for both tasks of the challenge is a standard stacked 2-layer bi-LSTM. The main particularities of our approach are: (a) The use of a surrogate function of F1 as loss function to close the gap between the minimization function and the evaluation metric, and (b) The generation of an ensemble of models for generating predictions by majority vote. Our system ranked second with an F1 score of 62.18% in the main task by a narrow margin with the winner that scored 63.94%.","['Neus Català', 'Mario Martin']",Proceedings of the Iberian Languages Evaluation Forum (IberLEF 2019) co-located with 35th Conference of the Spanish Society for Natural Language Processing (SEPLN 2019) http://ceur-ws.org/Vol-2421/,arXiv,2019,https://doi.org/10.48550/arXiv.1909.12339,Anomali
A Decision Tree Learning Approach for Mining Relationship-Based Access Control Policies,"Relationship-based access control (ReBAC) provides a high level of expressiveness and flexibility that promotes security and information sharing, by allowing policies to be expressed in terms of chains of relationships between entities. ReBAC policyminingalgorithms have the potential to significantly reduce the cost of migration from legacy access control systems to ReBAC, by partially automating the development of a ReBAC policy.
  This paper presents new algorithms, called DTRM (Decision Tree ReBAC Miner) and DTRM$^-$, based on decision trees, forminingReBAC policies from access control lists (ACLs) and information about entities. Compared to state-of-the-art ReBACminingalgorithms, our algorithms are significantly faster, achieve comparable policy quality, and canminepolicies in a richer language.","['Thang Bui', 'Scott D. Stoller']",,arXiv,2020,https://doi.org/10.48550/arXiv.1909.12095,Anomali
Deep Learning and Random Forest-Based Augmentation of sRNA Expression Profiles,"The lack of well-structured annotations in a growing amount of RNA expression data complicates data interoperability and reusability. Commonly - usedtextminingmethods extract annotations from existing unstructured data descriptions and often provide inaccurate output that requires manual curation. Automatic data-based augmentation (generation of annotations on the base of expression data) can considerably improve the annotation quality and has not been well-studied. We formulate an automatic augmentation of small RNA-seq expression data as a classification problem and investigate deep learning (DL) and random forest (RF) approaches to solve it. We generate tissue and sex annotations from small RNA-seq expression data for tissues and cell lines of homo sapiens. We validate our approach on 4243 annotated small RNA-seq samples from the Small RNA Expression Atlas (SEA) database. The average prediction accuracy for tissue groups is 98% (DL), for tissues - 96.5% (DL), and for sex - 77% (DL). The ""one dataset out"" average accuracy for tissue group prediction is 83% (DL) and 59% (RF). On average, DL provides better results as compared to RF, and considerably improves classification performance for 'unseen' datasets.","['Jelena Fiosina', 'Maksims Fiosins', 'Stefan Bonn']","Lecture Notes in Computer Science, 11490 (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1909.11943,Anomali
Mining user interaction patterns in the darkweb to predict enterprise cyber incidents,"With rise in security breaches over the past few years, there has been an increasing need tomineinsights from social media platforms to raise alerts of possible attacks in an attempt to defend conflict during competition. In this study, we attempt to build a framework that utilizes unconventional signals from the darkweb forums by leveraging the reply network structure of user interactions with the goal of predicting enterprise related external cyber attacks. We use both unsupervised and supervised learning models that address the challenges that come with the lack of enterprise attack metadata for ground truth validation as well as insufficient data for training the models. We validate our models on a binary classification problem that attempts to predict cyber attacks on a daily basis for an organization. Using several controlled studies on features leveraging the network structure, we measure the extent to which the indicators from the darkweb forums can be successfully used to predict attacks. We use information from 53 forums in the darkweb over a span of 17 months for the task. Our framework to predict real world organization cyber attacks of 3 different security events, suggest that focusing on the reply path structure between groups of users based on random walk transitions and community structures has an advantage in terms of better performance solely relying on forum or user posting statistics prior to attacks.","['Soumajyoti Sarkar', 'Mohammad Almukaynizi', 'Jana Shakarian', 'Paulo Shakarian']",,arXiv,2020,https://doi.org/10.48550/arXiv.1909.11592,Anomali
Multi-task Batch Reinforcement Learning with Metric Learning,"We tackle the Multi-task Batch Reinforcement Learning problem. Given multiple datasets collected from different tasks, we train a multi-task policy to perform well in unseen tasks sampled from the same distribution. The task identities of the unseen tasks are not provided. To perform well, the policy must infer the task identity from collected transitions by modelling its dependency on states, actions and rewards. Because the different datasets may have state-action distributions with large divergence, the task inference module can learn to ignore the rewards and spuriously correlate $\textit{only}$ state-action pairs to the task identity, leading to poor test time performance. To robustify task inference, we propose a novel application of the triplet loss. Tominehard negative examples, we relabel the transitions from the training tasks by approximating their reward functions. When we allow further training on the unseen tasks, using the trained policy as an initialization leads to significantly faster convergence compared to randomly initialized policies (up to $80\%$ improvement and across 5 different Mujoco task distributions). We name our method $\textbf{MBML}$ ($\textbf{M}\text{ulti-task}$$\textbf{B}\text{atch}$RL with$\textbf{M}\text{etric}$$\textbf{L}\text{earning}$).","['Jiachen Li', 'Quan Vuong', 'Shuang Liu', 'Minghua Liu', 'Kamil Ciosek', 'Keith Ross', 'Henrik Iskov Christensen', 'Hao Su']",,arXiv,2020,https://doi.org/10.48550/arXiv.1909.11373,Anomali
Deep Text Mining of Instagram Data Without Strong Supervision,"With the advent of social media, our online feeds increasingly consist of short, informal, and unstructuredtext. This textual data can be analyzed for the purpose of improving user recommendations and detecting trends. Instagram is one of the largest social media platforms, containing bothtextand images. However, most of the prior research ontextprocessing in social media is focused on analyzing Twitter data, and little attention has been paid totextminingof Instagram data. Moreover, manytextminingmethods rely on annotated training data, which in practice is both difficult and expensive to obtain. In this paper, we present methods for unsupervisedminingof fashion attributes from Instagramtext, which can enable a new kind of user recommendation in the fashion domain. In this context, we analyze a corpora of Instagram posts from the fashion domain, introduce a system for extracting fashion attributes from Instagram, and train a deep clothing classifier with weak supervision to classify Instagram posts based on the associatedtext.
  With our experiments, we confirm that word embeddings are a useful asset for information extraction. Experimental results show that information extraction using word embeddings outperforms a baseline that uses Levenshtein distance. The results also show the benefit of combining weak supervision signals using generative models instead of majority voting. Using weak supervision and generative modeling, an F1 score of 0.61 is achieved on the task of classifying the image contents of Instagram posts based solely on the associatedtext, which is on level with human performance. Finally, our empirical study provides one of the few available studies on Instagramtextand shows that thetextis noisy, that thetextdistribution exhibits the long-tail phenomenon, and that comment sections on Instagram are multi-lingual.","['Kim Hammar', 'Shatha Jaradat', 'Nima Dokoohaki', 'Mihhail Matskin']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.10812,Anomali
Learning Dense Representations for Entity Retrieval,"We show that it is feasible to perform entity linking by training a dual encoder (two-tower) model that encodes mentions and entities in the same dense vector space, where candidate entities are retrieved by approximate nearest neighbor search. Unlike prior work, this setup does not rely on an alias table followed by a re-ranker, and is thus the first fully learned entity retrieval model. We show that our dual encoder, trained using only anchor-textlinks in Wikipedia, outperforms discrete alias table and BM25 baselines, and is competitive with the best comparable results on the standard TACKBP-2010 dataset. In addition, it can retrieve candidates extremely fast, and generalizes well to a new dataset derived from Wikinews. On the modeling side, we demonstrate the dramatic value of an unsupervised negativeminingalgorithm for this task.","['Daniel Gillick', 'Sayali Kulkarni', 'Larry Lansing', 'Alessandro Presta', 'Jason Baldridge', 'Eugene Ie', 'Diego Garcia-Olano']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.10506,Anomali
Biomedical Mention Disambiguation using a Deep Learning Approach,"Automatically locating named entities in natural languagetext- named entity recognition - is an important task in the biomedical domain. Many named entity mentions are ambiguous between several bioconcept types, however, causingtextspans to be annotated as more than one type when simultaneously recognizing multiple entity types. The straightforward solution is a rule-based approach applying a priority order based on the precision of each entity tagger (from highest to lowest). While this method is straightforward and useful, imprecise disambiguation remains a significant source of error. We address this issue by generating a partially labeled corpus of ambiguous concept mentions. We first collect named entity mentions from multiple human-curated databases (e.g. CTDbase, gene2pubmed), then correlate them with thetextminedspan from PubTator to provide the context where the mention appears. Our corpus contains more than 3 million concept mentions that ambiguous between one or more concept types in PubTator (about 3% of all mentions). We approached this task as a classification problem and developed a deep learning-based method which uses the semantics of the span being classified and the surrounding words to identify the most likely bioconcept type. More specifically, we develop a convolutional neural network (CNN) and along short-term memory (LSTM) network to respectively handle the semantic syntax features, then concatenate these within a fully connected layer for final classification. The priority ordering rule-based approach demonstrated F1-scores of 71.29% (micro-averaged) and 41.19% (macro-averaged), while the new disambiguation method demonstrated F1-scores of 91.94% (micro-averaged) and 85.42% (macro-averaged), a very substantial increase.","['Chih-Hsuan Wei', 'Kyubum Lee', 'Robert Leaman', 'Zhiyong Lu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.10416,Anomali
A Consolidated System for Robust Multi-Document Entity Risk Extraction and Taxonomy Augmentation,"We introduce a hybrid human-automated system that provides scalable entity-risk relation extractions across large data sets. Given an expert-defined keyword taxonomy, entities, and data sources, the system returnstextextractions based on bidirectional token distances between entities and keywords and expands taxonomy coverage with word vector encodings. Our system represents a more simplified architecture compared to alerting focused systems - motivated by high coverage use cases in the riskminingspace such as due diligence activities and intelligence gathering. We provide an overview of the system and expert evaluations for a range of token distances. We demonstrate that single and multi-sentence distance groups significantly outperform baseline extractions with shorter, single sentences being preferred by analysts. As the taxonomy expands, the amount of relevant information increases and multi-sentence extractions become more preferred, but this is tempered against entity-risk relations become more indirect. We discuss the implications of these observations on users, management of ambiguity and taxonomy expansion, and future system modifications.","['Berk Ekmekci', 'Eleanor Hagerman', 'Blake Howald']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.10368,Anomali
Material Gain Concentration Quenching in Organic Dye-Doped Polymer Thin Films,"The optimization of material gain in optically pumped dye-doped polymer thin films is an important task in the development of organic solid-state lasers. In this work, we present a theoretical model that accommodates the influence of concentration quenching on material gain and employ it to study the novel dye molecule 2-(4-(bis(4-(tert-butyl)phenyl)amino)benzylidene)malononitrile (PMN) and the well-established dye molecule 4-(dicyanomethylene)-2-methyl-6-(4-dimethylaminostyryl)-4H-pyran (DCM) embedded in poly(methyl methacrylate) (PMMA). Polycarbonate was tested as an alternative host material for PMN. The material gain in these dye-doped polymer thin films was determined by the variable stripe length method. The inclusion of concentration quenching in the material gain expression is able to significantly reduce the overestimation of the gain efficiency inherent to a linear model.","['Florian Vogelbacher', 'Xue Zhou', 'Jinhua Huang', 'Mingzhu Li', 'Ke-Jian Jiang', 'Yanlin Song', 'Karl Unterrainer', 'Rainer Hainberger']","Opt. Mater. Express 9, 1208-1222 (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1909.09590,Anomali
Induction of Non-monotonic Logic Programs To Explain Statistical Learning Models,"We present a fast and scalable algorithm to induce non-monotonic logic programs from statistical learning models. We reduce the problem of search for best clauses to instances of the High-Utility ItemsetMining(HUIM) problem. In the HUIM problem, feature values and their importance are treated as transactions and utilities respectively. We make use of TreeExplainer, a fast and scalable implementation of the Explainable AI tool SHAP, to extract locally important features and their weights from ensemble tree models. Our experiments with UCI standard benchmarks suggest a significant improvement in terms of classification evaluation metrics and running time of the training algorithm compared to ALEPH, a state-of-the-art Inductive Logic Programming (ILP) system.",['Farhad Shakerin'],"EPTCS 306, 2019, pp. 379-388",arXiv,2019,https://doi.org/10.48550/arXiv.1909.09017,Anomali
Pockels-effect-based adiabatic frequency conversion in ultrahigh-$Q$ microresonators,"Adiabatic frequency conversion has some key advantages over nonlinear frequency conversion. No threshold and no phase-matching conditions need to be fulfilled. Moreover, it exhibits a conversion efficiency of $100\,\%$ down to the single-photon level. Adiabatic frequency conversion schemes in microresonators demonstrated so far suffer either from low quality factors of the employed resonators resulting in short photon lifetimes or small frequency shifts. Here, we present an adiabatic frequency conversion (AFC) scheme by employing the Pockels effect. We use a non-centrosymmetric ultrahigh-$Q$ microresonator made out of lithium niobate. Frequency shifts of more than $5\,$GHz are achieved by applying just $20\,$V to $70$-micrometer-thick crystal. Furthermore, we demonstrate that already with the same setup positive and a negative frequency chirps can be generated. With this method, by controlling the voltage applied to the crystal, almost arbitrary frequency shifts can be realized. The general advances in on-chip fabrication of lithium-niobate-based devices make it feasible to transfer the current apparatus onto a chip suitable for mass production.","['Yannick Minet', 'Luís Reis', 'Jan Szabados', 'Christoph S. Werner', 'Hans Zappe', 'Karsten Buse', 'Ingo Breunig']",Optics Express 28 (2020) 2939-2947,arXiv,2020,https://doi.org/10.48550/arXiv.1909.07958,Anomali
Learning Dynamic Author Representations with Temporal Language Models,"Language models are at the heart of numerous works, notably in thetextminingand information retrieval communities. These statistical models aim at extracting word distributions, from simple unigram models to recurrent approaches with latent variables that capture subtle dependencies intexts. However, those models are learned from word sequences only, and authors' identities, as well as publication dates, are seldom considered. We propose a neural model, based on recurrent language modeling, which aims at capturing language diffusion tendencies in author communities through time. By conditioning language models with author and temporal vector states, we are able to leverage the latent dependencies between thetextcontexts. This allows us to beat several temporal and non-temporal language baselines on two real-world corpora, and to learn meaningful author representations that vary through time.","['Edouard Delasalles', 'Sylvain Lamprier', 'Ludovic Denoyer']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.04985,Anomali
Global Locality in Biomedical Relation and Event Extraction,"Due to the exponential growth of biomedical literature, event and relation extraction are important tasks in biomedicaltextmining. Most work only focus on relation extraction, and detect a single entity pair mention on a short span oftext, which is not ideal due to long sentences that appear in biomedical contexts. We propose an approach to both relation and event extraction, for simultaneously predicting relationships between all mention pairs in atext. We also perform an empirical study to discuss different network setups for this purpose. The best performing model includes a set of multi-head attentions and convolutions, an adaptation of the transformer architecture, which offers self-attention the ability to strengthen dependencies among related elements, and models the interaction between features extracted by multiple attention heads. Experiment results demonstrate that our approach outperforms the state of the art on a set of benchmark biomedical corpora including BioNLP 2009, 2011, 2013 and BioCreative 2017 shared tasks.","['Elaheh ShafieiBavani', 'Antonio Jimeno Yepes', 'Xu Zhong', 'David Martinez Iraola']",,arXiv,2020,https://doi.org/10.48550/arXiv.1909.04822,Anomali
Jointly embedding the local and global relations of heterogeneous graph for rumor detection,"The development of social media has revolutionized the way people communicate, share information and make decisions, but it also provides an ideal platform for publishing and spreading rumors. Existing rumor detection methods focus on finding clues fromtextcontent, user profiles, and propagation patterns. However, the local semantic relation and global structural information in the message propagation graph have not been well utilized by previous works.
  In this paper, we present a novel global-local attention network (GLAN) for rumor detection, which jointly encodes the local semantic and global structural information. We first generate a better integrated representation for each source tweet by fusing the semantic information of related retweets with the attention mechanism. Then, we model the global relationships among all source tweets, retweets, and users as a heterogeneous graph to capture the rich structural information for rumor detection. We conduct experiments on three real-world datasets, and the results demonstrate that GLAN significantly outperforms the state-of-the-art models in both rumor detection and early detection scenarios.","['Chunyuan Yuan', 'Qianwen Ma', 'Wei Zhou', 'Jizhong Han', 'Songlin Hu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.04465,Anomali
Identifying Different Definitions of Future in the Assessment of Future Economic Conditions: Application of PU Learning and Text Mining,"The Economy Watcher Survey, which is a market survey published by the Japanese government, contains \emph{assessments of current and future economic conditions} by people from various fields. Although this survey provides insights regarding economic policy for policymakers, a clear definition of the word ""future"" in future economic conditions is not provided. Hence, the assessments respondents provide in the survey are simply based on their interpretations of the meaning of ""future."" This motivated us to reveal the different interpretations of the future in their judgments of future economic conditions by applying weakly supervised learning andtextmining. In our research, we separate the assessments of future economic conditions into economic conditions of the near and distant future using learning from positive and unlabeled data (PU learning). Because the dataset includes data from several periods, we devised new architecture to enable neural networks to conduct PU learning based on the idea of multi-task learning to efficiently learn a classifier. Our empirical analysis confirmed that the proposed method could separate the future economic conditions, and we interpreted the classification results to obtain intuitions for policymaking.",['Masahiro Kato'],,arXiv,2020,https://doi.org/10.48550/arXiv.1909.03348,Anomali
Deep learning with sentence embeddings pre-trained on biomedical corpora improves the performance of finding similar sentences in electronic medical records,"Capturing sentence semantics plays a vital role in a range oftextminingapplications. Despite continuous efforts on the development of related datasets and models in the general domain, both datasets and models are limited in biomedical and clinical domains. The BioCreative/OHNLP organizers have made the first attempt to annotate 1,068 sentence pairs from clinical notes and have called for a community effort to tackle the Semantic Textual Similarity (BioCreative/OHNLP STS) challenge. We developed models using traditional machine learning and deep learning approaches. For the post challenge, we focus on two models: the Random Forest and the Encoder Network. We applied sentence embeddings pre-trained on PubMed abstracts and MIMIC-III clinical notes and updated the Random Forest and the Encoder Network accordingly. The official results demonstrated our best submission was the ensemble of eight models. It achieved a Person correlation coefficient of 0.8328, the highest performance among 13 submissions from 4 teams. For the post challenge, the performance of both Random Forest and the Encoder Network was improved; in particular, the correlation of the Encoder Network was improved by ~13%. During the challenge task, no end-to-end deep learning models had better performance than machine learning models that take manually-crafted features. In contrast, with the sentence embeddings pre-trained on biomedical corpora, the Encoder Network now achieves a correlation of ~0.84, which is higher than the original best model. The ensembled model taking the improved versions of the Random Forest and Encoder Network as inputs further increased performance to 0.8528. Deep learning models with sentence embeddings pre-trained on biomedical corpora achieve the highest performance on the test set.","['Qingyu Chen', 'Jingcheng Du', 'Sun Kim', 'W. John Wilbur', 'Zhiyong Lu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.03044,Anomali
Avaya Conversational Intelligence: A Real-Time System for Spoken Language Understanding in Human-Human Call Center Conversations,"Avaya Conversational Intelligence(ACI) is an end-to-end, cloud-based solution for real-time Spoken Language Understanding for call centers. It combines large vocabulary, real-time speech recognition, transcript refinement, and entity and intent recognition in order to convert live audio into a rich, actionable stream of structured events. These events can be further leveraged with a business rules engine, thus serving as a foundation for real-time supervision and assistance applications. After the ingestion, calls are enriched with unsupervised keyword extraction, abstractive summarization, and business-defined attributes, enabling offline use cases, such as business intelligence, topicmining, full-textsearch, quality assurance, and agent training. ACI comes with a pretrained, configurable library of hundreds of intents and a robust intent training environment that allows for efficient, cost-effective creation and customization of customer-specific intents.","['Jan Mizgajski', 'Adrian Szymczak', 'Robert Głowski', 'Piotr Szymański', 'Piotr Żelasko', 'Łukasz Augustyniak', 'Mikołaj Morzy', 'Yishay Carmiel', 'Jeff Hodson', 'Łukasz Wójciak', 'Daniel Smoczyk', 'Adam Wróbel', 'Bartosz Borowik', 'Adam Artajew', 'Marcin Baran', 'Cezary Kwiatkowski', 'Marzena Żyła-Hoppe']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.02851,Anomali
CT Data Curation for Liver Patients: Phase Recognition in Dynamic Contrast-Enhanced CT,"As the demand for more descriptive machine learning models grows within medical imaging, bottlenecks due to data paucity will exacerbate. Thus, collecting enough large-scale data will require automated tools to harvest data/label pairs from messy and real-world datasets, such as hospital PACS. This is the focus of our work, where we present a principled data curation tool to extract multi-phase CT liver studies and identify each scan's phase from a real-world and heterogenous hospital PACS dataset. Emulating a typical deployment scenario, we first obtain a set of noisy labels from our institutional partners that aretextminedusing simple rules from DICOM tags. We train a deep learning system, using a customized and streamlined 3D SE architecture, to identify non-contrast, arterial, venous, and delay phase dynamic CT liver scans, filtering out anything else, including other types of liver contrast studies. To exploit as much training data as possible, we also introduce an aggregated cross entropy loss that can learn from scans only identified as ""contrast"". Extensive experiments on a dataset of 43K scans of 7680 patient imaging studies demonstrate that our 3DSE architecture, armed with our aggregated loss, can achieve a mean F1 of 0.977 and can correctly harvest up to 92.7% of studies, which significantly outperforms thetext-minedand standard-loss approach, and also outperforms other, and more complex, model architectures.","['Bo Zhou', 'Adam P. Harrison', 'Jiawen Yao', 'Chi-Tung Cheng', 'Jing Xiao', 'Chien-Hung Liao', 'Le Lu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1909.02511,Anomali
NERO: A Neural Rule Grounding Framework for Label-Efficient Relation Extraction,"Deep neural models for relation extraction tend to be less reliable when perfectly labeled data is limited, despite their success in label-sufficient scenarios. Instead of seeking more instance-level labels from human annotators, here we propose to annotate frequent surface patterns to form labeling rules. These rules can be automaticallyminedfrom largetextcorpora and generalized via a soft rule matching mechanism. Prior works use labeling rules in an exact matching fashion, which inherently limits the coverage of sentence matching and results in the low-recall issue. In this paper, we present a neural approach to ground rules for RE, named NERO, which jointly learns a relation extraction module and a soft matching module. One can employ any neural relation extraction models as the instantiation for the RE module. The soft matching module learns to match rules with semantically similar sentences such that raw corpora can be automatically labeled and leveraged by the RE module (in a much better coverage) as augmented supervision, in addition to the exactly matched sentences. Extensive experiments and analysis on two public and widely-used datasets demonstrate the effectiveness of the proposed NERO framework, comparing with both rule-based and semi-supervised methods. Through user studies, we find that the time efficiency for a human to annotate rules and sentences are similar (0.30 vs. 0.35 min per label). In particular, NERO's performance using 270 rules is comparable to the models trained using 3,000 labeled sentences, yielding a 9.5x speedup. Moreover, NERO can predict for unseen relations at test time and provide interpretable predictions. We release our code to the community for future research.","['Wenxuan Zhou', 'Hongtao Lin', 'Bill Yuchen Lin', 'Ziqi Wang', 'Junyi Du', 'Leonardo Neves', 'Xiang Ren']",,arXiv,2020,https://doi.org/10.48550/arXiv.1909.02177,Anomali
Mining for Dark Matter Substructure: Inferring subhalo population properties from strong lenses with machine learning,"The subtle and unique imprint of dark matter substructure on extended arcs in strong lensing systems contains a wealth of information about the properties and distribution of dark matter on small scales and, consequently, about the underlying particle physics. However, teasing out this effect poses a significant challenge since the likelihood function for realistic simulations of population-level parameters is intractable. We apply recently-developed simulation-based inference techniques to the problem of substructure inference in galaxy-galaxy strong lenses. By leveraging additional information extracted from the simulator, neural networks are efficiently trained to estimate likelihood ratios associated with population-level parameters characterizing substructure. Through proof-of-principle application to simulated data, we show that these methods can provide an efficient and principled way to simultaneously analyze an ensemble of strong lenses, and can be used tominethe large sample of lensing images deliverable by near-future surveys for signatures of dark matter substructure.","['Johann Brehmer', 'Siddharth Mishra-Sharma', 'Joeri Hermans', 'Gilles Louppe', 'Kyle Cranmer']","The Astrophysical Journal, Volume 886, Issue 1, article id. 49, 16 pp. (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1909.02005,Anomali
Cross-domain Aspect Category Transfer and Detection via Traceable Heterogeneous Graph Representation Learning,"Aspect category detection is an essential task for sentiment analysis and opinionmining. However, the cost of categorical data labeling, e.g., label the review aspect information for a large number of product domains, can be inevitable but unaffordable. In this study, we propose a novel problem, cross-domain aspect category transfer and detection, which faces three challenges: various feature spaces, different data distributions, and diverse output spaces. To address these problems, we propose an innovative solution, Traceable Heterogeneous Graph Representation Learning (THGRL). Unlike priortext-based aspect detection works, THGRL explores latent domain aspect category connections via massive user behavior information on a heterogeneous graph. Moreover, an innovative latent variable ""Walker Tracer"" is introduced to characterize the global semantic/aspect dependencies and capture the informative vertexes on the random walk paths. By using THGRL, we project different domains' feature spaces into a common one, while allowing data distributions and output spaces stay differently. Experiment results show that the proposed method outperforms a series of state-of-the-art baseline models.","['Zhuoren Jiang', 'Jian Wang', 'Lujun Zhao', 'Changlong Sun', 'Yao Lu', 'Xiaozhong Liu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1908.11610,Anomali
Experimental generation of a flat-top beam profile in a stable ring cavity,"We create a spatially homogeneous field inside of a ring cavity by combining two transverse modes generated by a single laser through modulation. The interference term between the two modes averages out because of the frequency difference between them, eliminating the need for interferometric control of their relative phase. The use of a ring cavity allows for a large waist for the flat-top profile, big enough to cover the atoms in an atomic trap. The cavity is mechanically and thermally isolated, and the laser light is locked to the cavity using the Pound-Drever-Hall technique. The flat-top profile technique reported here fulfill the vanishing curvature criterion at the center of the profile.","['A. López-Vázquez', 'Y. M. Torres', 'M. S. Billión', 'W. M. Pimenta', 'J. A. Franco-Villafañe', 'E. Gomez']","Opt. Lett. 44, 4428-4431 (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1908.11477,Anomali
Ordered Sets for Data Analysis,"This book dwells on mathematical and algorithmic issues of data analysis based on generality order of descriptions and respective precision. To speak of these topics correctly, we have to go some way getting acquainted with the important notions of relation and order theory. On the one hand, data often have a complex structure with natural order on it. On the other hand, many symbolic methods of data analysis and machine learning allow to compare the obtained classifiers w.r.t. their generality, which is also an order relation. Efficient algorithms are very important in data analysis, especially when one deals with big data, so scalability is a real issue. That is why we analyze the computational complexity of algorithms and problems of data analysis. We start from the basic definitions and facts of algorithmic complexity theory and analyze the complexity of various tools of data analysis we consider. The tools and methods of data analysis, like computing taxonomies, groups of similar objects (concepts and n-clusters), dependencies in data, classification, etc., are illustrated with applications in particular subject domains, from chemoinformatics totextminingand natural language processing.",['Sergei O. Kuznetsov'],,arXiv,2019,https://doi.org/10.48550/arXiv.1908.11341,Anomali
Unsupervised Construction of Knowledge Graphs From Text and Code,"The scientific literature is a rich source of information for dataminingwith conceptual knowledge graphs; the open science movement has enriched this literature with complementary source code that implements scientific models. To exploit this new resource, we construct a knowledge graph using unsupervised learning methods to identify conceptual entities. We associate source code entities to these natural language concepts using word embedding and clustering techniques. Practical naming conventions for methods and functions tend to reflect the concept(s) they implement. We take advantage of this specificity by presenting a novel process for joint clusteringtextconcepts that combines word-embeddings, nonlinear dimensionality reduction, and clustering techniques to assist in understanding, organizing, and comparing software in the open science ecosystem. With our pipeline, we aim to assist scientists in building on existing models in their discipline when making novel models for new phenomena. By combining source code and conceptual information, our knowledge graph enhances corpus-wide understanding of scientific literature.","['Kun Cao', 'James Fairbanks']",,arXiv,2019,https://doi.org/10.48550/arXiv.1908.09354,Anomali
Neural Cognitive Diagnosis for Intelligent Education Systems,"Cognitive diagnosis is a fundamental issue in intelligent education, which aims to discover the proficiency level of students on specific knowledge concepts. Existing approaches usuallyminelinear interactions of student exercising process by manual-designed function (e.g., logistic function), which is not sufficient for capturing complex relations between students and exercises. In this paper, we propose a general Neural Cognitive Diagnosis (NeuralCD) framework, which incorporates neural networks to learn the complex exercising interactions, for getting both accurate and interpretable diagnosis results. Specifically, we project students and exercises to factor vectors and leverage multi neural layers for modeling their interactions, where the monotonicity assumption is applied to ensure the interpretability of both factors. Furthermore, we propose two implementations of NeuralCD by specializing the required concepts of each exercise, i.e., the NeuralCDM with traditional Q-matrix and the improved NeuralCDM+ exploring the richtextcontent. Extensive experimental results on real-world datasets show the effectiveness of NeuralCD framework with both accuracy and interpretability.","['Fei Wang', 'Qi Liu', 'Enhong Chen', 'Zhenya Huang', 'Yuying Chen', 'Yu Yin', 'Zai Huang', 'Shijin Wang']",,arXiv,2020,https://doi.org/10.48550/arXiv.1908.08733,Anomali
"Training Optimus Prime, M.D.: Generating Medical Certification Items by Fine-Tuning OpenAI's gpt2 Transformer Model","This article describes new results of an application using transformer-based language models to automated item generation (AIG), an area of ongoing interest in the domain of certification testing as well as in educational measurement and psychological testing. OpenAI's gpt2 pre-trained 345M parameter language model was retrained using the public domaintextminingset of PubMed articles and subsequently used to generate item stems (case vignettes) as well as distractor proposals for multiple-choice items. This case study shows promise and produces drafttextthat can be used by human item writers as input for authoring. Future experiments with more recent transformer models (such as Grover, TransformerXL) using existing item pools are expected to improve results further and to facilitate the development of assessment materials.",['Matthias von Davier'],,arXiv,2019,https://doi.org/10.48550/arXiv.1908.08594,Anomali
Parsimonious Morpheme Segmentation with an Application to Enriching Word Embeddings,"Traditionally, manytext-miningtasks treat individual word-tokens as the finest meaningful semantic granularity. However, in many languages and specialized corpora, words are composed by concatenating semantically meaningful subword structures. Word-level analysis cannot leverage the semantic information present in such subword structures. With regard to word embedding techniques, this leads to not only poor embeddings for infrequent words in long-tailedtextcorpora but also weak capabilities for handling out-of-vocabulary words. In this paper we propose MorphMine for unsupervised morpheme segmentation. MorphMine applies a parsimony criterion to hierarchically segment words into the fewest number of morphemes at each level of the hierarchy. This leads to longer shared morphemes at each level of segmentation. Experiments show that MorphMine segments words in a variety of languages into human-verified morphemes. Additionally, we experimentally demonstrate that utilizing MorphMine morphemes to enrich word embeddings consistently improves embedding quality on a variety of of embedding evaluations and a downstream language modeling task.","['Ahmed El-Kishky', 'Frank Xu', 'Aston Zhang', 'Jiawei Han']",,arXiv,2019,https://doi.org/10.48550/arXiv.1908.07832,Anomali
A Multi-level Neural Network for Implicit Causality Detection in Web Texts,"Miningcausality fromtextis a complex and crucial natural language understanding task corresponding to the human cognition. Existing studies at its solution can be grouped into two primary categories: feature engineering based and neural model based methods. In this paper, we find that the former has incomplete coverage and inherent errors but provide prior knowledge; while the latter leverages context information but causal inference of which is insufficiency. To handle the limitations, we propose a novel causality detection model named MCDN to explicitly model causal reasoning process, and furthermore, to exploit the advantages of both methods. Specifically, we adopt multi-head self-attention to acquire semantic feature at word level and develop the SCRN to infer causality at segment level. To the best of our knowledge, with regards to the causality tasks, this is the first time that the Relation Network is applied. The experimental results show that: 1) the proposed approach performs prominent performance on causality detection; 2) further analysis manifests the effectiveness and robustness of MCDN.","['Shining Liang', 'Wanli Zuo', 'Zhenkun Shi', 'Sen Wang', 'Junhu Wang', 'Xianglin Zuo']",,arXiv,2021,https://doi.org/10.48550/arXiv.1908.07822,Anomali
Learning Joint Embedding for Cross-Modal Retrieval,"A cross-modal retrieval process is to use a query in one modality to obtain relevant data in another modality. The challenging issue of cross-modal retrieval lies in bridging the heterogeneous gap for similarity computation, which has been broadly discussed in image-text, audio-text, and video-textcross-modal multimedia dataminingand retrieval. However, the gap in temporal structures of different data modalities is not well addressed due to the lack of alignment relationship between temporal cross-modal structures. Our research focuses on learning the correlation between different modalities for the task of cross-modal retrieval. We have proposed an architecture: Supervised-Deep Canonical Correlation Analysis (S-DCCA), for cross-modal retrieval. In this forum paper, we will talk about how to exploit triplet neural networks (TNN) to enhance the correlation learning for cross-modal retrieval. The experimental result shows the proposed TNN-based supervised correlation learning architecture can get the best result when the data representation extracted by supervised learning.",['Donghuo Zeng'],,arXiv,2019,https://doi.org/10.48550/arXiv.1908.07673,Anomali
Discriminative Topic Mining via Category-Name Guided Text Embedding,"Mininga set of meaningful and distinctive topics automatically from massivetextcorpora has broad applications. Existing topic models, however, typically work in a purely unsupervised way, which often generate topics that do not fit users' particular needs and yield suboptimal performance on downstream tasks. We propose a new task, discriminative topicmining, which leverages a set of user-provided category names tominediscriminative topics fromtextcorpora. This new task not only helps a user understand clearly and distinctively the topics he/she is most interested in, but also benefits directly keyword-driven classification tasks. We develop CatE, a novel category-name guidedtextembedding method for discriminative topicmining, which effectively leverages minimal user guidance to learn a discriminative embedding space and discover category representative terms in an iterative manner. We conduct a comprehensive set of experiments to show that CatEmineshigh-quality set of topics guided by category names only, and benefits a variety of downstream applications including weakly-supervised classification and lexical entailment direction identification.","['Yu Meng', 'Jiaxin Huang', 'Guangyuan Wang', 'Zihan Wang', 'Chao Zhang', 'Yu Zhang', 'Jiawei Han']",,arXiv,2020,https://doi.org/10.48550/arXiv.1908.07162,Anomali
Generating an Overview Report over Many Documents,"How to efficiently generate an accurate, well-structured overview report (ORPT) over thousands of related documents is challenging. A well-structured ORPT consists of sections of multiple levels (e.g., sections and subsections). None of the existing multi-document summarization (MDS) algorithms is directed toward this task. To overcome this obstacle, we present NDORGS (Numerous Documents' Overview Report Generation Scheme) that integratestextfiltering, keyword scoring, single-document summarization (SDS), topic modeling, MDS, and title generation to generate a coherent, well-structured ORPT. We then devise a multi-criteria evaluation method using techniques oftextminingand multi-attribute decision making on a combination of human judgments, running time, information coverage, and topic diversity. We evaluate ORPTs generated by NDORGS on two large corpora of documents, where one is classified and the other unclassified. We show that, using Saaty's pairwise comparison 9-point scale and under TOPSIS, the ORPTs generated on SDS's with the length of 20% of the original documents are the best overall on both datasets.","['Jingwen Wang', 'Hao Zhang', 'Cheng Zhang', 'Wenjing Yang', 'Liqun Shao', 'Jie Wang']",,arXiv,2019,https://doi.org/10.48550/arXiv.1908.06216,Anomali
Self-supervised Data Bootstrapping for Deep Optical Character Recognition of Identity Documents,"The essential task of verifying person identities at airports and national borders is very time consuming. To accelerate it, optical character recognition for identity documents (IDs) using dictionaries is not appropriate due to high variability of thetextcontent in IDs, e.g., individual street names or surnames. Additionally, no properties of the used fonts in IDs are known. Therefore, we propose an iterative self-supervised bootstrapping approach using a smart strategy tominereal character data from IDs. In combination with synthetically generated character data, the real data is used to train efficient convolutional neural networks for character classification serving a practical runtime as well as a high accuracy. On a dataset with 74 character classes, we achieve an average class-wise accuracy of 99.4 %. In contrast, if we would apply a classifier trained only using synthetic data, the accuracy is reduced to 58.1 %. Finally, we show that our whole proposed pipeline outperforms an established open-source framework","['Oliver Mothes', 'Joachim Denzler']",,arXiv,2019,https://doi.org/10.48550/arXiv.1908.04027,Anomali
Deep Structured Cross-Modal Anomaly Detection,"Anomaly detection is a fundamental problem in dataminingfield with many real-world applications. A vast majority of existing anomaly detection methods predominately focused on data collected from a single source. In real-world applications, instances often have multiple types of features, such as images (ID photos, finger prints) andtexts(bank transaction histories, user online social media posts), resulting in the so-called multi-modal data. In this paper, we focus on identifying anomalies whose patterns are disparate across different modalities, i.e., cross-modal anomalies. Some of the data instances within a multi-modal context are often not anomalous when they are viewed separately in each individual modality, but contains inconsistent patterns when multiple sources are jointly considered. The existence of multi-modal data in many real-world scenarios brings both opportunities and challenges to the canonical task of anomaly detection. On the one hand, in multi-modal data, information of different modalities may complement each other in improving the detection performance. On the other hand, complicated distributions across different modalities call for a principled framework to characterize their inherent and complex correlations, which is often difficult to capture with conventional linear models. To this end, we propose a novel deep structured anomaly detection framework to identify the cross-modal anomalies embedded in the data. Experiments on real-world datasets demonstrate the effectiveness of the proposed framework comparing with the state-of-the-art.","['Yuening Li', 'Ninghao Liu', 'Jundong Li', 'Mengnan Du', 'Xia Hu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1908.03848,Anomali
BERT-based Ranking for Biomedical Entity Normalization,"Developing high-performance entity normalization algorithms that can alleviate the term variation problem is of great interest to the biomedical community. Although deep learning-based methods have been successfully applied to biomedical entity normalization, they often depend on traditional context-independent word embeddings. Bidirectional Encoder Representations from Transformers (BERT), BERT for BiomedicalTextMining(BioBERT) and BERT for ClinicalTextMining(ClinicalBERT) were recently introduced to pre-train contextualized word representation models using bidirectional Transformers, advancing the state-of-the-art for many natural language processing tasks. In this study, we proposed an entity normalization architecture by fine-tuning the pre-trained BERT / BioBERT / ClinicalBERT models and conducted extensive experiments to evaluate the effectiveness of the pre-trained models for biomedical entity normalization using three different types of datasets. Our experimental results show that the best fine-tuned models consistently outperformed previous methods and advanced the state-of-the-art for biomedical entity normalization, with up to 1.17% increase in accuracy.","['Zongcheng Ji', 'Qiang Wei', 'Hua Xu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1908.03548,Anomali
Text mining policy: Classifying forest and landscape restoration policy agenda with neural information retrieval,"Dozens of countries have committed to restoring the ecological functionality of 350 million hectares of land by 2030. In order to achieve such wide-scale implementation of restoration, the values and priorities of multi-sectoral stakeholders must be aligned and integrated with national level commitments and other development agenda. Although misalignment across scales of policy and between stakeholders are well known barriers to implementing restoration, fast-paced policy making in multi-stakeholder environments complicates the monitoring and analysis of governance and policy. In this work, we assess the potential of machine learning to identify restoration policy agenda across diverse policy documents. An unsupervised neural information retrieval architecture is introduced that leverages transfer learning and word embeddings to create high-dimensional representations of paragraphs. Policy agenda labels are recast as information retrieval queries in order to classify policies with a cosine similarity threshold between paragraphs and query embeddings. This approach achieves a 0.83 F1-score measured across 14 policy agenda in 31 policy documents in Malawi, Kenya, and Rwanda, indicating that automatedtextminingcan provide reliable, generalizable, and efficient analyses of restoration policy.",['John Brandt'],,arXiv,2019,https://doi.org/10.48550/arXiv.1908.02425,Anomali
Performance Evaluation of Supervised Machine Learning Techniques for Efficient Detection of Emotions from Online Content,"Emotion detection from thetextis an important and challenging problem intextanalytics. The opinion-miningexperts are focusing on the development of emotion detection applications as they have received considerable attention of online community including users and business organization for collecting and interpreting public emotions. However, most of the existing works on emotion detection used less efficient machine learning classifiers with limited datasets, resulting in performance degradation. To overcome this issue, this work aims at the evaluation of the performance of different machine learning classifiers on a benchmark emotion dataset. The experimental results show the performance of different machine learning classifiers in terms of different evaluation metrics like precision, recall ad f-measure. Finally, a classifier with the best performance is recommended for the emotion classification.","['Muhammad Zubair Asghar', 'Fazli Subhan', 'Muhammad Imran', 'Fazal Masud Kundi', 'Shahboddin Shamshirband', 'Amir Mosavi', 'Peter Csiba', 'Annamaria R. Varkonyi-Koczy']",,arXiv,2019,https://doi.org/10.48550/arXiv.1908.01587,Anomali
VIANA: Visual Interactive Annotation of Argumentation,"ArgumentationMiningaddresses the challenging tasks of identifying boundaries of argumentativetextfragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting whichtextfragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring alltext- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration betweentextand graph views.","['Fabian Sperrle', 'Rita Sevastjanova', 'Rebecca Kehlbeck', 'Mennatallah El-Assady']",2019 IEEE Conference on Visual Analytics Science and Technology (VAST),arXiv,2019,https://doi.org/10.48550/arXiv.1907.12413,Anomali
Exhaustive Exact String Matching: The Analysis of the Full Human Genome,"Exact string matching has been a fundamental problem in computer science for decades because of many practical applications. Some are related to common procedures, such as searching in files andtexteditors, or, more recently, to more advanced problems such as pattern detection in Artificial Intelligence and Bioinformatics. Tens of algorithms and methodologies have been developed for pattern matching and several programming languages, packages, applications and online systems exist that can perform exact string matching in biological sequences. These techniques, however, are limited to searching for specific and predefined strings in a sequence. In this paper a novel methodology (called Ex2SM) is presented, which is a pipeline of execution of advanced data structures and algorithms, explicitly designed fortextmining, that can detect every possible repeated string in multivariate biological sequences. In contrast to known algorithms in literature, the methodology presented here is string agnostic, i.e., it does not require an input string to search for it, rather it can detect every string that exists at least twice, regardless of its attributes such as length, frequency, alphabet, overlapping etc. The complexity of the problem solved and the potential of the proposed methodology is demonstrated with the experimental analysis performed on the entire human genome. More specifically, all repeated strings with a length of up to 50 characters have been detected, an achievement which is practically impossible using other algorithms due to the exponential number of possible permutations of such long strings.",['Konstantinos F. Xylogiannopoulos'],,arXiv,2019,https://doi.org/10.48550/arXiv.1907.11232,Anomali
On Mining IoT Data for Evaluating the Operation of Public Educational Buildings,"Public educational systems operate thousands of buildings with vastly different characteristics in terms of size, age, location, construction, thermal behavior and user communities. Their strategic planning and sustainable operation is an extremely complex and requires quantitative evidence on the performance of buildings such as the interaction of indoor-outdoor environment. Internet of Things (IoT) deployments can provide the necessary data to evaluate, redesign and eventually improve the organizational and managerial measures. In this work a dataminingapproach is presented to analyze the sensor data collected over a period of 2 years from an IoT infrastructure deployed over 18 school buildings spread in Greece, Italy and Sweden. The real-world evaluation indicates that dataminingon sensor data can provide critical insights to building managers and custodial staff about ways to lower a building's energy footprint through effectively managing building operations.","['Na Zhu', 'Aris Anagnostopoulos', 'Ioannis Chatzigiannakis']",,arXiv,2019,https://doi.org/10.48550/arXiv.1907.10818,Anomali
CUR Low Rank Approximation at Deterministic Sublinear Cost,"A matrix algorithm runs at {\em sublinear cost} if it uses much fewer memory cells and arithmetic operations than the input matrix has entries. Such algorithms are indispensable for Big DataMiningand Analysis. Quite typically in that area the input matrices are so immense that realistically one can only access a small fraction of all their entries but can access and process at sublinear cost their Low Rank Approximation {\em (LRA)}. Can, however, we compute LRA at sublinear cost? Adversary argument shows that the output of any algorithm running at sublinear cost is extremely far from LRA of the worst case input matrices and even of the matrices of small families of our Appendix, but we prove that some deterministic sublinear cost algorithms output reasonably close LRA in a memory efficient form of CUR LRA if an input matrix admits LRA and is Symmetric Positive Semidefinite or is very close to a low rank matrix. The latter result is technically simple but provides some (very limited but long overdue) support for the well-known empirical efficiency of sublinear cost LRA by means of Cross-Approximation. We demonstrate the power of application of such LRA by turning the Fast Multipole celebrated Method into Superfast Multipole Method. The design and analysis of our algorithms rely on extensive prior study of the link of LRA of a matrix to maximization of its volume.","['Qi Luan', 'Victor Y. Pan', 'John Svadlenka']",,arXiv,2021,https://doi.org/10.48550/arXiv.1907.10481,Anomali
ER-AE: Differentially Private Text Generation for Authorship Anonymization,"Most of privacy protection studies for textual data focus on removing explicit sensitive identifiers. However, personal writing style, as a strong indicator of the authorship, is often neglected. Recent studies, such as SynTF, have shown promising results on privacy-preservingtextmining. However, their anonymization algorithm can only output numeric term vectors which are difficult for the recipients to interpret. We propose a noveltextgeneration model with a two-set exponential mechanism for authorship anonymization. By augmenting the semantic information through a REINFORCE training reward function, the model can generate differentially privatetextthat has a close semantic and similar grammatical structure to the originaltextwhile removing personal traits of the writing style. It does not assume any conditioned labels or paralleledtextdata for training. We evaluate the performance of the proposed model on the real-life peer reviews dataset and the Yelp review dataset. The result suggests that our model outperforms the state-of-the-art on semantic preservation, authorship obfuscation, and stylometric transformation.","['Haohan Bo', 'Steven H. H. Ding', 'Benjamin C. M. Fung', 'Farkhund Iqbal']",,arXiv,2021,https://doi.org/10.48550/arXiv.1907.08736,Anomali
Distillation of Squeezing using a pulsed engineered PDC source,"Hybrid quantum information processing combines the advantages of discrete and continues variable protocols by realizing protocols consisting of photon counting and homodyne measurements. However, the mode structure of pulsed sources and the properties of the detection schemes often require the use optical filters in order to combine both detection methods in a common experiment. This limits the efficiency and the overall achievable squeezing of the experiment. In our work, we use photon subtraction to implement the distillation of pulsed squeezed states originating from a genuinely spatially and temporally single-mode parametric down-conversion source in non-linear waveguides. Due to the distillation, we witness an improvement of $0.17~\mathrm{dB}$ from an initial squeezing value of $-1.648 \pm 0.002~\mathrm{dB}$, while achieving a purity of $0.58$, and confirm the non-Gaussianity of the distilled state via the higher-order cumulants. With this, we demonstrate the source's suitability for scalable hybrid quantum network applications with pulsed quantum light.","['Thomas Dirmeier', 'Johannes Tiedau', 'Imran Khan', 'Vahid Ansari', 'Christian R. Müller', 'Christine Silberhorn', 'Christoph Marquardt', 'Gerd Leuchs']","Optics Express Vol. 28, Issue 21, pp. 30784-30796 (2020)",arXiv,2020,https://doi.org/10.48550/arXiv.1907.08004,Anomali
Exploring the context of course rankings on online academic forums,"University students routinely use the tools provided by online course ranking forums to share and discuss their satisfaction with the quality of instruction and content in a wide variety of courses. Student perception of the efficacy of pedagogies employed in a course is a reflection of a multitude of decisions by professors, instructional designers and university administrators. This complexity has motivated a large body of research on the utility, reliability, and behavioral correlates of course rankings. There is, however, little investigation of the (potential) implicit student bias on these forums towards desirable course outcomes at the institution level. To that end, we examine the connection between course outcomes (student-reported GPA) and the overall ranking of the primary course instructor, as well as rating disparity by nature of course outcomes, based on data from two popular academic rating forums. Our experiments with ranking data about over ten thousand courses taught at Virginia Tech and its 25 SCHEV-approved peer institutions indicate that there is a discernible albeit complex bias towards course outcomes in the professor ratings registered by students.","['Taha Hassan', 'Bob Edmison', 'Larry Cox II', 'Matthew Louvet', 'Daron Williams']",Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Network Analysis and Mining (ASONAM '19),arXiv,2019,https://doi.org/10.48550/arXiv.1907.05846,Anomali
Improving Neural Relation Extraction with Implicit Mutual Relations,"Relation extraction (RE) aims at extracting the relation between two entities from thetextcorpora. It is a crucial task for Knowledge Graph (KG) construction. Most existing methods predict the relation between an entity pair by learning the relation from the training sentences, which contain the targeted entity pair. In contrast to existing distant supervision approaches that suffer from insufficient training corpora to extract relations, our proposal ofminingimplicit mutual relation from the massive unlabeled corpora transfers the semantic information of entity pairs into the RE model, which is more expressive and semantically plausible. After constructing an entity proximity graph based on the implicit mutual relations, we preserve the semantic relations of entity pairs via embedding each vertex of the graph into a low-dimensional space. As a result, we can easily and flexibly integrate the implicit mutual relations and other entity information, such as entity types, into the existing RE methods.
  Our experimental results on a New York Times and another Google Distant Supervision datasets suggest that our proposed neural RE framework provides a promising improvement for the RE task, and significantly outperforms the state-of-the-art methods. Moreover, the component forminingimplicit mutual relations is so flexible that can help to improve the performance of both CNN-based and RNN-based RE models significant.","['Jun Kuang', 'Yixin Cao', 'Jianbing Zheng', 'Xiangnan He', 'Ming Gao', 'Aoying Zhou']",,arXiv,2019,https://doi.org/10.48550/arXiv.1907.05333,Anomali
Exploiting user-frequency information for mining regionalisms from Social Media texts,"The task of detecting regionalisms (expressions or words used in certain regions) has traditionally relied on the use of questionnaires and surveys, and has also heavily depended on the expertise and intuition of the surveyor. The irruption of Social Media and its microblogging services has produced an unprecedented wealth of content, mainly informaltextgenerated by users, opening new opportunities for linguists to extend their studies of language variation. Previous work on automatic detection of regionalisms depended mostly on word frequencies. In this work, we present a novel metric based on Information Theory that incorporates user frequency. We tested this metric on a corpus of Argentinian Spanish tweets in two ways: via manual annotation of the relevance of the retrieved terms, and also as a feature selection method for geolocation of users. In either case, our metric outperformed other techniques based solely in word frequency, suggesting that measuring the amount of users that produce a word is informative. This tool has helped lexicographers discover several unregistered words of Argentinian Spanish, as well as different meanings assigned to registered words.","['Juan Manuel Pérez', 'Damián E. Aleman', 'Santiago N. Kalinowski', 'Agustín Gravano']",,arXiv,2019,https://doi.org/10.48550/arXiv.1907.04492,Anomali
Computer-Aided Data Mining: Automating a Novel Knowledge Discovery and Data Mining Process Model for Metabolomics,"This work presents MeKDDaM-SAGA, computer-aided automation software for implementing a novel knowledge discovery and dataminingprocess model that was designed for performing justifiable, traceable and reproducible metabolomics data analysis. The process model focuses on achieving metabolomics analytical objectives and on considering the nature of its involved data. MeKDDaM-SAGA was successfully used for guiding the process model execution in a number of metabolomics applications. It satisfies the requirements of the proposed process model design and execution. The software realises the process model layout, structure and flow and it enables its execution externally using various dataminingand machine learning tools or internally using a number of embedded facilities that were built for performing a number of automated activities such as data preprocessing, data exploration, data acclimatization, modelling, evaluation and visualization. MeKDDaM-SAGA was developed using object-oriented software engineering methodology and was constructed in Java. It consists of 241 design classes that were designed to implement 27 use-cases. The software uses an XML database to guarantee portability and uses a GUI interface to ensure its user-friendliness. It implements an internal embedded version control system that is used to realise and manage the process flow, feedback and iterations and to enable undoing and redoing the execution of the process phases, activities, and the internal tasks within its phases.","['Ahmed BaniMustafa', 'Nigel Hardy']",,arXiv,2019,https://doi.org/10.48550/arXiv.1907.04318,Anomali
TEAGS: Time-aware Text Embedding Approach to Generate Subgraphs,"Contagions (e.g. virus, gossip) spread over the nodes in propagation graphs. We can use the temporal and textual data of the nodes to compute the edge weights and then generate subgraphs with highly relevant nodes. This is beneficial to many applications. Yet, challenges abound. First, the propagation pattern between each pair of nodes may change by time. Second, not always the same contagion propagates. Hence, the state-of-the-arttextminingapproaches including topic-modeling cannot effectively compute the edge weights. Third, since the propagation is affected by time, the word-word co-occurrence patterns may differ in various temporal dimensions, that can decrease the effectiveness of word embedding approaches. We argue that multi-aspect temporal dimensions (hour, day, etc) should be considered to better calculate the correlation weights between the nodes. In this work, we devise a novel framework that on the one hand, integrates a neural network based time-aware word embedding component to construct the word vectors through multiple temporal facets, and on the other hand, uses a temporal generative model to compute the weights. Subsequently, we propose a Max-Heap Graph cutting algorithm to generate subgraphs. We validate our model through comprehensive experiments on real-world datasets. The results show that our model can retrieve the subgraphs more effective than other rivals and the temporal dynamics should be noticed both in word embedding and propagation processes.","['Saeid Hosseini', 'Saeed Najafipour', 'Ngai-Man Cheung', 'Hongzhi Yin', 'Mohammad Reza Kangavari', 'Xiaofang Zhou']",,arXiv,2019,https://doi.org/10.48550/arXiv.1907.03191,Anomali
Transfer Learning for Risk Classification of Social Media Posts: Model Evaluation Study,"Mental illness affects a significant portion of the worldwide population. Online mental health forums can provide a supportive environment for those afflicted and also generate a large amount of data which can beminedto predict mental health states using machine learning methods. We benchmark multiple methods oftextfeature representation for social media posts and compare their downstream use with automated machine learning (AutoML) tools to triage content for moderator attention. We used 1588 labeled posts from the CLPsych 2017 shared task collected from the Reachout.com forum (Milne et al., 2019). Posts were represented using lexicon based tools including VADER, Empath, LIWC and also used pre-trained artificial neural network models including DeepMoji, Universal Sentence Encoder, and GPT-1. We used TPOT and auto-sklearn as AutoML tools to generate classifiers to triage the posts. The top-performing system used features derived from the GPT-1 model, which was finetuned on over 150,000 unlabeled posts from Reachout.com. Our top system had a macro averaged F1 score of 0.572, providing a new state-of-the-art result on the CLPsych 2017 task. This was achieved without additional information from meta-data or preceding posts. Error analyses revealed that this top system often misses expressions of hopelessness. We additionally present visualizations that aid understanding of the learned classifiers. We show that transfer learning is an effective strategy for predicting risk with relatively little labeled data. We note that finetuning of pretrained language models provides further gains when large amounts of unlabeledtextis available.","['Derek Howard', 'Marta Maslej', 'Justin Lee', 'Jacob Ritchie', 'Geoffrey Woollard', 'Leon French']",,arXiv,2019,https://doi.org/10.48550/arXiv.1907.02581,Anomali
The evolution of argumentation mining: From models to social media and emerging tools,"Argumentationminingis a rising subject in the computational linguistics domain focusing on extracting structured arguments from naturaltext, often from unstructured or noisytext. The initial approaches on modeling arguments was aiming to identify a flawless argument on specific fields (Law, Scientific Papers) serving specific needs (completeness, effectiveness). With the emerge of Web 2.0 and the explosion in the use of social media both the diffusion of the data and the argument structure have changed. In this survey article, we bridge the gap between theoretical approaches of argumentationminingand pragmatic schemes that satisfy the needs of social media generated data, recognizing the need for adapting more flexible and expandable schemes, capable to adjust to the argumentation conditions that exist in social media. We review, compare, and classify existing approaches, techniques and tools, identifying the positive outcome of combining tasks and features, and eventually propose a conceptual architecture framework. The proposed theoretical framework is an argumentationminingscheme able to identify the distinct sub-tasks and capture the needs of social mediatext, revealing the need for adopting more flexible and extensible frameworks.","['Anastasios Lytos', 'Thomas Lagkas', 'Panagiotis Sarigiannidis', 'Kalina Bontcheva']","Information Processing & Management, Volume 56, Issue 6, 2019",arXiv,2019,https://doi.org/10.48550/arXiv.1907.02258,Anomali
On computing distances and latencies in Link Streams,"Link Streams were proposed a few years ago as a model of temporal networks. We seek to understand the topological and temporal nature of those objects through efficiently computing the distances, latencies and lengths of shortest fastest paths. We develop different algorithms to compute those values efficiently. Proofs of correctness for those methods are presented as well as bounds on their temporal complexities as functions of link stream parameters. One purpose of this study is to help develop algorithms to compute centrality functions on link streams such as the betweenness centrality and the closeness centrality.",['Frédéric Simard'],,arXiv,2019,https://doi.org/10.48550/arXiv.1907.02146,Anomali
Deep neural network-based classification model for Sentiment Analysis,"The growing prosperity of social networks has brought great challenges to the sentimental tendencyminingof users. As more and more researchers pay attention to the sentimental tendency of online users, rich research results have been obtained based on the sentiment classification of explicittexts. However, research on the implicit sentiment of users is still in its infancy. Aiming at the difficulty of implicit sentiment classification, a research on implicit sentiment classification model based on deep neural network is carried out. Classification models based on DNN, LSTM, Bi-LSTM and CNN were established to judge the tendency of the user's implicit sentimenttext. Based on the Bi-LSTM model, the classification model of word-level attention mechanism is studied. The experimental results on the public dataset show that the established LSTM series classification model and CNN classification model can achieve good sentiment classification effect, and the classification effect is significantly better than the DNN model. The Bi-LSTM based attention mechanism classification model obtained the optimal R value in the positive category identification.","['Donghang Pan', 'Jingling Yuan', 'Lin Li', 'Deming Sheng']",,arXiv,2019,https://doi.org/10.48550/arXiv.1907.02046,Anomali
Analyses of Multi-collection Corpora via Compound Topic Modeling,"As electronically stored data grow in daily life, obtaining novel and relevant information becomes challenging intextmining. Thus people have sought statistical methods based on term frequency, matrix algebra, or topic modeling fortextmining. Popular topic models have centered on one singletextcollection, which is deficient for comparativetextanalyses. We consider a setting where one can partition the corpus into subcollections. Each subcollection shares a common set of topics, but there exists relative variation in topic proportions among collections. Including any prior knowledge about the corpus (e.g. organization structure), we propose the compound latent Dirichlet allocation (cLDA) model, improving on previous work, encouraging generalizability, and depending less on user-input parameters. To identify the parameters of interest in cLDA, we study Markov chain Monte Carlo (MCMC) and variational inference approaches extensively, and suggest an efficient MCMC method. We evaluate cLDA qualitatively and quantitatively using both synthetic and real-world corpora. The usability study on some real-world corpora illustrates the superiority of cLDA to explore the underlying topics automatically but also model their connections and variations across multiple collections.","['Clint P. George', 'Wei Xia', 'George Michailidis']",,arXiv,2019,https://doi.org/10.48550/arXiv.1907.01636,Anomali
Hidden in Plain Sight For Too Long: Using Text Mining Techniques to Shine a Light on Workplace Sexism and Sexual Harassment,"Objective: The goal of this study is to understand how people experience sexism and sexual harassment in the workplace by discovering themes in 2,362 experiences posted on the Everyday Sexism Project's website everydaysexism.com. Method: This study used both quantitative and qualitative methods. The quantitative method was a computational framework to collect and analyze a large number of workplace sexual harassment experiences. The qualitative method was the analysis of the topics generated by atextminingmethod. Results: Twenty-three topics were coded and then grouped into three overarching themes from the sex discrimination and sexual harassment literature. The Sex Discrimination theme included experiences in which women were treated unfavorably due to their sex, such as being passed over for promotion, denied opportunities, paid less than men, and ignored or talked over in meetings. The Sex Discrimination and Gender harassment theme included stories about sex discrimination and gender harassment, such as sexist hostility behaviors ranging from insults and jokes invoking misogynistic stereotypes to bullying behavior. The last theme, Unwanted Sexual Attention, contained stories describing sexual comments and behaviors used to degrade women. Unwanted touching was the highest weighted topic, indicating how common it was for website users to endure being touched, hugged or kissed, groped, and grabbed. Conclusions: This study illustrates how researchers can use automatic processes to go beyond the limits of traditional research methods and investigate naturally occurring large scale datasets on the internet to achieve a better understanding of everyday workplace sexism experiences.","['Amir Karami', 'Suzanne C. Swan', 'Cynthia Nicole White', 'Kayla Ford']",,arXiv,2019,https://doi.org/10.48550/arXiv.1907.00510,Anomali
Program Synthesis and Semantic Parsing with Learned Code Idioms,"Program synthesis of general-purpose source code from natural language specifications is challenging due to the need to reason about high-level patterns in the target program and low-level implementation details at the same time. In this work, we present PATOIS, a system that allows a neural program synthesizer to explicitly interleave high-level and low-level reasoning at every generation step. It accomplishes this by automaticallyminingcommon code idioms from a given corpus, incorporating them into the underlying language for neural synthesis, and training a tree-based neural synthesizer to use these idioms during code generation. We evaluate PATOIS on two complex semantic parsing datasets and show that using learned code idioms improves the synthesizer's accuracy.","['Richard Shin', 'Miltiadis Allamanis', 'Marc Brockschmidt', 'Oleksandr Polozov']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.10816,Anomali
Cross-lingual Data Transformation and Combination for Text Classification,"Textclassification is a fundamental task fortextdatamining. In order to train a generalizable model, a large volume oftextmust be collected. To address data insufficiency, cross-lingual data may occasionally be necessary. Cross-lingual data sources may however suffer from data incompatibility, astextwritten in different languages can hold distinct word sequences and semantic patterns. Machine translation and word embedding alignment provide an effective way to transform and combine data for cross-lingual data training. To the best of our knowledge, there has been little work done on evaluating how the methodology used to conduct semantic space transformation and data combination affects the performance of classification models trained from cross-lingual resources. In this paper, we systematically evaluated the performance of two commonly used CNN (Convolutional Neural Network) and RNN (Recurrent Neural Network)textclassifiers with differing data transformation and combination strategies. Monolingual models were trained from English and French alongside their translated and aligned embeddings. Our results suggested that semantic space transformation may conditionally promote the performance of monolingual models. Bilingual models were trained from a combination of both English and French. Our results indicate that a cross-lingual classification model can significantly benefit from cross-lingual data by learning from translated or aligned embedding spaces.","['Jun Jiang', 'Shumao Pang', 'Xia Zhao', 'Liwei Wang', 'Andrew Wen', 'Hongfang Liu', 'Qianjin Feng']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.09543,Anomali
Explainable Fact Checking with Probabilistic Answer Set Programming,"One challenge in fact checking is the ability to improve the transparency of the decision. We present a fact checking method that uses reference information in knowledge graphs (KGs) to assess claims and explain its decisions. KGs contain a formal representation of knowledge with semantic descriptions of entities and their relationships. We exploit such rich semantics to produce interpretable explanations for the fact checking output. As information in a KG is inevitably incomplete, we rely on logical rule discovery and on Webtextminingto gather the evidence to assess a given claim. Uncertain rules and facts are turned into logical programs and the checking task is modeled as an inference problem in a probabilistic extension of answer set programs. Experiments show that the probabilistic inference enables the efficient labeling of claims with interpretable explanations, and the quality of the results is higher than state of the art baselines.","['Naser Ahmadi', 'Joohyung Lee', 'Paolo Papotti', 'Mohammed Saeed']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.09198,Anomali
Meta-learning of textual representations,"Recent progress in AutoML has lead to state-of-the-art methods (e.g., AutoSKLearn) that can be readily used by non-experts to approach any supervised learning problem. Whereas these methods are quite effective, they are still limited in the sense that they work for tabular (matrix formatted) data only. This paper describes one step forward in trying to automate the design of supervised learning methods in the context oftextmining. We introduce a meta learning methodology for automatically obtaining a representation fortextminingtasks starting from rawtext. We report experiments considering 60 different textual representations and more than 80textminingdatasets associated to a wide variety of tasks. Experimental results show the proposed methodology is a promising solution to obtain highly effective off the shelltextclassification pipelines.","['Jorge Madrid', 'Hugo Jair Escalante', 'Eduardo Morales']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.08934,Anomali
Low-resource Deep Entity Resolution with Transfer and Active Learning,"Entity resolution (ER) is the task of identifying different representations of the same real-world entities across databases. It is a key step for knowledge base creation andtextmining. Recent adaptation of deep learning methods for ER mitigates the need for dataset-specific feature engineering by constructing distributed representations of entity records. While these methods achieve state-of-the-art performance over benchmark data, they require large amounts of labeled data, which are typically unavailable in realistic ER applications. In this paper, we develop a deep learning-based method that targets low-resource settings for ER through a novel combination of transfer learning and active learning. We design an architecture that allows us to learn a transferable model from a high-resource setting to a low-resource one. To further adapt to the target dataset, we incorporate active learning that carefully selects a few informative examples to fine-tune the transferred model. Empirical evaluation demonstrates that our method achieves comparable, if not better, performance compared to state-of-the-art learning-based methods while using an order of magnitude fewer labels.","['Jungo Kasai', 'Kun Qian', 'Sairam Gurajada', 'Yunyao Li', 'Lucian Popa']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.08042,Anomali
Yoga-Veganism: Correlation Mining of Twitter Health Data,"Nowadays social media is a huge platform of data. People usually share their interest, thoughts via discussions, tweets, status. It is not possible to go through all the data manually. We need tominethe data to explore hidden patterns or unknown correlations, find out the dominant topic in data and understand people's interest through the discussions. In this work, we explore Twitter data related to health. We extract the popular topics under different categories (e.g. diet, exercise) discussed in Twitter via topic modeling, observe model behavior on new tweets, discover interesting correlation (i.e. Yoga-Veganism). We evaluate accuracy by comparing with ground truth using manual annotation both for train and test data.",['Tunazzina Islam'],,arXiv,2019,https://doi.org/10.48550/arXiv.1906.07668,Anomali
Transfer Learning for Causal Sentence Detection,"We consider the task of detecting sentences that express causality, as a step towardsminingcausal relations fromtexts. To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO and BERT, using a bidirectional GRU with self-attention (BIGRUATT) as a baseline. We experiment with both generic public relation extraction datasets and a new biomedical causal sentence detection dataset, a subset of which we make publicly available. We find that transfer learning helps only in very small datasets. With larger datasets, BIGRUATT reaches a performance plateau, then larger datasets and transfer learning do not help.","['Manolis Kyriakakis', 'Ion Androutsopoulos', 'Joan Ginés i Ametllé', 'Artur Saudabayev']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.07544,Anomali
A Simple Text Mining Approach for Ranking Pairwise Associations in Biomedical Applications,"We present a simpletextminingmethod that is easy to implement, requires minimal data collection and preparation, and is easy to use for proposing ranked associations between a list of target terms and a key phrase. We call this method KinderMiner, and apply it to two biomedical applications. The first application is to identify relevant transcription factors for cell reprogramming, and the second is to identify potential drugs for investigation in drug repositioning. We compare the results from our algorithm to existing data and state-of-the-art algorithms, demonstrating compelling results for both application areas. While we apply the algorithm here for biomedical applications, we argue that the method is generalizable to any available corpus of sufficient size.","['Finn Kuusisto', 'John Steill', 'Zhaobin Kuang', 'James Thomson', 'David Page', 'Ron Stewart']",AMIA Joint Summits on Translational Science Proceedings (2017) 166-174,arXiv,2019,https://doi.org/10.48550/arXiv.1906.05255,Anomali
Deep Learning for Spatio-Temporal Data Mining: A Survey,"With the fast development of various positioning techniques such as Global Position System (GPS), mobile devices and remote sensing, spatio-temporal data has become increasingly available nowadays.Miningvaluable knowledge from spatio-temporal data is critically important to many real world applications including human mobility understanding, smart transportation, urban planning, public safety, health care and environmental management. As the number, volume and resolution of spatio-temporal datasets increase rapidly, traditional dataminingmethods, especially statistics based methods for dealing with such data are becoming overwhelmed. Recently, with the advances of deep learning techniques, deep leaning models such as convolutional neural network (CNN) and recurrent neural network (RNN) have enjoyed considerable success in various machine learning tasks due to their powerful hierarchical feature learning ability in both spatial and temporal domains, and have been widely applied in various spatio-temporal datamining(STDM) tasks such as predictive learning, representation learning, anomaly detection and classification. In this paper, we provide a comprehensive survey on recent progress in applying deep learning techniques for STDM. We first categorize the types of spatio-temporal data and briefly introduce the popular deep learning models that are used in STDM. Then a framework is introduced to show a general pipeline of the utilization of deep learning models for STDM. Next we classify existing literatures based on the types of ST data, the dataminingtasks, and the deep learning models, followed by the applications of deep learning for STDM in different domains including transportation, climate science, human mobility, location based social network, crime analysis, and neuroscience. Finally, we conclude the limitations of current research and point out future research directions.","['Senzhang Wang', 'Jiannong Cao', 'Philip S. Yu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.04928,Anomali
Innovating HR Using an Expert System for Recruiting IT Specialists -- ESRIT,"One of the most rapidly evolving and dynamic business sector is the IT domain, where there is a problem finding experienced, skilled and qualified employees. Specialists are essential for developing and implementing new ideas into products. Human resources (HR) department plays a major role in the recruitment of qualified employees by assessing their skills, using different HR metrics, and selecting the best candidates for a specific job. Most recruiters are not qualified to evaluate IT specialists. In order to decrease the gap between the HR department and IT specialists, we designed, implemented and tested an Expert System for Recruiting IT specialist - ESRIT. The expert system usestextmining, natural language processing, and classification algorithms to extract relevant information from resumes by using a knowledge base that stores the relevant key skills and phrases. The recruiter is looking for the same abilities and certificates, trying to place the best applicant into a specific position. The article presents a developing picture of the top major IT skills that will be required in 2014 and it argues for the choice of the IT abilities domain.","['Ciprian-Octavian Truică', 'Adriana Barnoschi']","Journal of Software & Systems Development, 2015",arXiv,2019,https://doi.org/10.48550/arXiv.1906.04915,Anomali
Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for Large-Scale Multi-Label Text Classification,"CNNs, RNNs, GCNs, and CapsNets have shown significant insights in representation learning and are widely used in varioustextminingtasks such as large-scale multi-labeltextclassification. However, most existing deep models for multi-labeltextclassification consider either the non-consecutive and long-distance semantics or the sequential semantics, but how to consider them both coherently is less studied. In addition, most existing methods treat output labels as independent methods, but ignore the hierarchical relations among them, leading to useful semantic information loss. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent CNNs framework for large-scale multi-labeltextclassification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding words-matrix representation which preserves both the non-consecutive, long-distance and local sequential semantics. Then the words-matrix is input to the proposed attentional graph capsule recurrent CNNs for more effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-labeltextclassification by comparing with state-of-the-art approaches.","['Hao Peng', 'Jianxin Li', 'Qiran Gong', 'Senzhang Wang', 'Lifang He', 'Bo Li', 'Lihong Wang', 'Philip S. Yu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.04898,Anomali
Fine-grained Event Categorization with Heterogeneous Graph Convolutional Networks,"Events are happening in real-world and real-time, which can be planned and organized occasions involving multiple people and objects. Social media platforms publish a lot oftextmessages containing public events with comprehensive topics. However,miningsocial events is challenging due to the heterogeneous event elements intextsand explicit and implicit social network structures. In this paper, we design an event meta-schema to characterize the semantic relatedness of social events and build an event-based heterogeneous information network (HIN) integrating information from external knowledge base, and propose a novel Pair-wise Popularity Graph Convolutional Network (PP-GCN) based fine-grained social event categorization model. We propose a Knowledgeable meta-paths Instances based social Event Similarity (KIES) between events and build a weighted adjacent matrix as input to the PP-GCN model. Comprehensive experiments on real data collections are conducted to compare various social event detection and clustering tasks. Experimental results demonstrate that our proposed framework outperforms other alternative social event categorization techniques.","['Hao Peng', 'Jianxin Li', 'Qiran Gong', 'Yangqiu Song', 'Yuanxing Ning', 'Kunfeng Lai', 'Philip S. Yu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.04580,Anomali
CUR Low Rank Approximation of a Matrix at Sublinear Cost,"Low rank approximation of a matrix (hereafter LRA) is a highly important area of Numerical Linear and Multilinear Algebra and DataMiningand Analysis. One can operate with LRA at sublinear cost, that is, by using much fewer memory cells and flops than an input matrix has entries, but no sublinear cost algorithm can compute accurate LRA of the worst case input matrices or even of the matrices of small families in our Appendix. Nevertheless we prove that Cross-Approximation celebrated algorithms and even more primitive sublinear cost algorithms output quite accurate LRA for a large subclass of the class of all matrices that admit LRA and in a sense for most of such matrices. Moreover, we accentuate the power of sublinear cost LRA by means of multiplicative pre-processing of an input matrix, and this also reveals a link between C-A algorithms and Randomized and Sketching LRA algorithms. Our tests are in good accordance with our formal study.","['Victor Y. Pan', 'Qi Luan', 'John Svadlenka', 'Liang Zhao']",,arXiv,2023,https://doi.org/10.48550/arXiv.1906.04112,Anomali
Real or Fake? Learning to Discriminate Machine from Human Generated Text,"Energy-based models (EBMs), a.k.a. un-normalized models, have had recent successes in continuous spaces. However, they have not been successfully applied to modeltextsequences. While decreasing the energy at training samples is straightforward,mining(negative) samples where the energy should be increased is difficult. In part, this is because standard gradient-based methods are not readily applicable when the input is high-dimensional and discrete. Here, we side-step this issue by generating negatives using pre-trained auto-regressive language models. The EBM then works in the residual of the language model; and is trained to discriminate realtextfromtextgenerated by the auto-regressive models. We investigate the generalization ability of residual EBMs, a pre-requisite for using them in other applications. We extensively analyze generalization for the task of classifying whether an input is machine or human generated, a natural task given the training loss and how weminenegatives. Overall, we observe that EBMs can generalize remarkably well to changes in the architecture of the generators producing negatives. However, EBMs exhibit more sensitivity to the training set used by such generators.","['Anton Bakhtin', 'Sam Gross', 'Myle Ott', 'Yuntian Deng', ""Marc'Aurelio Ranzato"", 'Arthur Szlam']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.03351,Anomali
Dissecting Content and Context in Argumentative Relation Analysis,"When assessing relations between argumentative units (e.g., support or attack), computational systems often exploit disclosing indicators or markers that are not part of elementary argumentative units (EAUs) themselves, but are gained from their context (position in paragraph, preceding tokens, etc.). We show that this dependency is much stronger than previously assumed. In fact, we show that by completely masking the EAUtextspans and only feeding information from their context, a competitive system may function even better. We argue that an argument analysis system that relies more on discourse context than the argument's content is unsafe, since it can easily be tricked. To alleviate this issue, we separate argumentative units from their context such that the system is forced to model and rely on an EAU's content. We show that the resulting classification system is more robust, and argue that such models are better suited for predicting argumentative relations across documents.","['Juri Opitz', 'Anette Frank']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.03338,Anomali
Exploring Diseases and Syndromes in Neurology Case Reports from 1955 to 2017 with Text Mining,"Background: A large number of neurology case reports have been published, but it is a challenging task for human medical experts to explore all of these publications.Textminingoffers a computational approach to investigate neurology literature and capture meaningful patterns. The overarching goal of this study is to provide a new perspective on case reports of neurological disease and syndrome analysis over the last six decades usingtextmining.
  Methods: We extracted diseases and syndromes (DsSs) from more than 65,000 neurology case reports from 66 journals in PubMed over the last six decades from 1955 to 2017.Textminingwas applied to reports on the detected DsSs to investigate high-frequency DsSs, categorize them, and explore the linear trends over the 63-year time frame.
  Results: Thetextminingmethods explored high-frequency neurologic DsSs and their trends and the relationships between them from 1955 to 2017. We detected more than 18,000 unique DsSs and found 10 categories of neurologic DsSs. While the trend analysis showed the increasing trends in the case reports for top-10 high-frequency DsSs, the categories had mixed trends.
  Conclusion: Our study provided new insights into the application oftextminingmethods to investigate DsSs in a large number of medical case reports that occur over several decades. The proposed approach can be used to provide a macro level analysis of medical literature by discovering interesting patterns and tracking them over several years to help physicians explore these case reports more efficiently.","['Amir Karami', 'Mehdi Ghasemi', 'Souvik Sen', 'Marcos Moraes', 'Vishal Shah']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.03183,Anomali
Two-Stream Region Convolutional 3D Network for Temporal Activity Detection,"We address the problem of temporal activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network (R-C3D), which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. We further improve the detection performance by efficiently integrating an optical flow based motion stream with the original RGB stream. The two-stream network is jointly optimized by fusing the flow and RGB feature maps at different levels. Additionally, the training stage incorporates an online hard exampleminingstrategy to address the extreme foreground-background imbalance typically observed in any detection pipeline. Instead of heuristically sampling the candidate segments for the final activity classification stage, we rank them according to their performance and only select the worst performers to update the model. This improves the model without heavy hyper-parameter tuning. Extensive experiments on three benchmark datasets are carried out to show superior performance over existing temporal activity detection methods. Our model achieves state-of-the-art results on the THUMOS'14 and Charades datasets. We further demonstrate that our model is a general temporal activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on the ActivityNet dataset.","['Huijuan Xu', 'Abir Das', 'Kate Saenko']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.02182,Anomali
Coherent electrical control of a single high-spin nucleus in silicon,"Nuclear spins are highly coherent quantum objects. In large ensembles, their control and detection via magnetic resonance is widely exploited, e.g. in chemistry, medicine, materials science andmining. Nuclear spins also featured in early ideas and demonstrations of quantum information processing. Scaling up these ideas requires controlling individual nuclei, which can be detected when coupled to an electron. However, the need to address the nuclei via oscillating magnetic fields complicates their integration in multi-spin nanoscale devices, because the field cannot be localized or screened. Control via electric fields would resolve this problem, but previous methods relied upon transducing electric signals into magnetic fields via the electron-nuclear hyperfine interaction, which severely affects the nuclear coherence. Here we demonstrate the coherent quantum control of a single antimony (spin-7/2) nucleus, using localized electric fields produced within a silicon nanoelectronic device. The method exploits an idea first proposed in 1961 but never realized experimentally with a single nucleus. Our results are quantitatively supported by a microscopic theoretical model that reveals how the purely electrical modulation of the nuclear electric quadrupole interaction, in the presence of lattice strain, results in coherent nuclear spin transitions. The spin dephasing time, 0.1 seconds, surpasses by orders of magnitude those obtained via methods that require a coupled electron spin for electrical drive. These results show that high-spin quadrupolar nuclei could be deployed as chaotic models, strain sensors and hybrid spin-mechanical quantum systems using all-electrical controls. Integrating electrically controllable nuclei with quantum dots could pave the way to scalable nuclear- and electron-spin-based quantum computers in silicon that operate without the need for oscillating magnetic fields.","['Serwan Asaad', 'Vincent Mourik', 'Benjamin Joecker', 'Mark A. I. Johnson', 'Andrew D. Baczewski', 'Hannes R. Firgau', 'Mateusz T. Mądzik', 'Vivien Schmitt', 'Jarryd J. Pla', 'Fay E. Hudson', 'Kohei M. Itoh', 'Jeffrey C. McCallum', 'Andrew S. Dzurak', 'Arne Laucht', 'Andrea Morello']","Nature 579, 205 (2020)",arXiv,2019,https://doi.org/10.48550/arXiv.1906.01086,Anomali
TACAM: Topic And Context Aware Argument Mining,"In this work we address the problem of argument search. The purpose of argument search is the distillation of pro and contra arguments for requested topics from largetextcorpora. In previous works, the usual approach is to use a standard search engine to extracttextparts which are relevant to the given topic and subsequently use an argument recognition algorithm to select arguments from them. The main challenge in the argument recognition task, which is also known as argumentmining, is that often sentences containing arguments are structurally similar to purely informative sentences without any stance about the topic. In fact, they only differ semantically. Most approaches use topic or search term information only for the first search step and therefore assume that arguments can be classified independently of a topic. We argue that topic information is crucial for argumentmining, since the topic defines the semantic context of an argument. Precisely, we propose different models for the classification of arguments, which take information about a topic of an argument into account. Moreover, to enrich the context of a topic and to let models understand the context of the potential argument better, we integrate information from different external sources such as Knowledge Graphs or pre-trained NLP models. Our evaluation shows that considering topic information, especially in connection with external information, provides a significant performance boost for the argumentminingtask.","['Michael Fromm', 'Evgeniy Faerman', 'Thomas Seidl']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.00923,Anomali
TechNet: Technology Semantic Network Based on Patent Data,"The growing developments in general semantic networks, knowledge graphs and ontology databases have motivated us to build a large-scale comprehensive semantic network of technology-related data for engineering knowledge discovery, technology search and retrieval, and artificial intelligence for engineering design and innovation. Specially, we constructed a technology semantic network (TechNet) that covers the elemental concepts in all domains of technology and their semantic associations byminingthe complete U.S. patent database from 1976. To derive the TechNet, natural language processing techniques were utilized to extract terms from massive patenttextsand recent word embedding algorithms were employed to vectorize such terms and establish their semantic relationships. We report and evaluate the TechNet for retrieving terms and their pairwise relevance that is meaningful from a technology and engineering design perspective. The TechNet may serve as an infrastructure to support a wide range of applications, e.g., technicaltextsummaries, search query predictions, relational knowledge discovery, and design ideation support, in the context of engineering and technology, and complement or enrich existing semantic databases. To enable such applications, the TechNet is made public via an online interface and APIs for public users to retrieve technology-related terms and their relevancies.","['Serhad Sarica', 'Jianxi Luo', 'Kristin L. Wood']",,arXiv,2019,https://doi.org/10.48550/arXiv.1906.00411,Anomali
Assessing The Factual Accuracy of Generated Text,"We propose a model-based metric to estimate the factual accuracy of generatedtextthat is complementary to typical scoring schemes like ROUGE (Recall-Oriented Understudy for Gisting Evaluation) and BLEU (Bilingual Evaluation Understudy). We introduce and release a new large-scale dataset based on Wikipedia and Wikidata to train relation classifiers and end-to-end fact extraction models. The end-to-end models are shown to be able to extract complete sets of facts from datasets with full pages oftext. We then analyse multiple models that estimate factual accuracy on a Wikipediatextsummarization task, and show their efficacy compared to ROUGE and other model-free variants by conducting a human evaluation study.","['Ben Goodrich', 'Vinay Rao', 'Mohammad Saleh', 'Peter J Liu']","The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '19), August 4--8, 2019, Anchorage, AK, USA",arXiv,2021,https://doi.org/10.48550/arXiv.1905.13322,Anomali
Generalized Separable Nonnegative Matrix Factorization,"Nonnegative matrix factorization (NMF) is a linear dimensionality technique for nonnegative data with applications such as image analysis,textmining, audio source separation and hyperspectral unmixing. Given a data matrix $M$ and a factorization rank $r$, NMF looks for a nonnegative matrix $W$ with $r$ columns and a nonnegative matrix $H$ with $r$ rows such that $M \approx WH$. NMF is NP-hard to solve in general. However, it can be computed efficiently under the separability assumption which requires that the basis vectors appear as data points, that is, that there exists an index set $\mathcal{K}$ such that $W = M(:,\mathcal{K})$. In this paper, we generalize the separability assumption: We only require that for each rank-one factor $W(:,k)H(k,:)$ for $k=1,2,\dots,r$, either $W(:,k) = M(:,j)$ for some $j$ or $H(k,:) = M(i,:)$ for some $i$. We refer to the corresponding problem as generalized separable NMF (GS-NMF). We discuss some properties of GS-NMF and propose a convex optimization model which we solve using a fast gradient method. We also propose a heuristic algorithm inspired by the successive projection algorithm. To verify the effectiveness of our methods, we compare them with several state-of-the-art separable NMF algorithms on synthetic, document and image data sets.","['Junjun Pan', 'Nicolas Gillis']","IEEE Trans. on Pattern Analysis and Machine Intelligence 43 (5), pp. 1546-1561, 2021",arXiv,2019,https://doi.org/10.48550/arXiv.1905.12995,Anomali
TopExNet: Entity-Centric Network Topic Exploration in News Streams,"The recent introduction of entity-centric implicit network representations of unstructuredtextoffers novel ways for exploring entity relations in document collections and streams efficiently and interactively. Here, we present TopExNet as a tool for exploring entity-centric network topics in streams of news articles. The application is available as a web service at https://topexnet.ifi.uni-heidelberg.de/ .","['Andreas Spitz', 'Satya Almasian', 'Michael Gertz']",,arXiv,2019,https://doi.org/10.48550/arXiv.1905.12335,Anomali
Induction of Non-Monotonic Rules From Statistical Learning Models Using High-Utility Itemset Mining,"We present a fast and scalable algorithm to induce non-monotonic logic programs from statistical learning models. We reduce the problem of search for best clauses to instances of the High-Utility ItemsetMining(HUIM) problem. In the HUIM problem, feature values and their importance are treated as transactions and utilities respectively. We make use of TreeExplainer, a fast and scalable implementation of the Explainable AI tool SHAP, to extract locally important features and their weights from ensemble tree models. Our experiments with UCI standard benchmarks suggest a significant improvement in terms of classification evaluation metrics and running time of the training algorithm compared to ALEPH, a state-of-the-art Inductive Logic Programming (ILP) system.","['Farhad Shakerin', 'Gopal Gupta']",,arXiv,2019,https://doi.org/10.48550/arXiv.1905.11226,Anomali
Transcribing Content from Structural Images with Spotlight Mechanism,"Transcribing content from structural images, e.g., writing notes from music scores, is a challenging task as not only the content objects should be recognized, but the internal structure should also be preserved. Existing image recognition methods mainly work on images with simple content (e.g.,textlines with characters), but are not capable to identify ones with more complex content (e.g., structured symbols), which often follow a fine-grained grammar. To this end, in this paper, we propose a hierarchical Spotlight Transcribing Network (STN) framework followed by a two-stage ""where-to-what"" solution. Specifically, we first decide ""where-to-look"" through a novel spotlight mechanism to focus on different areas of the original image following its structure. Then, we decide ""what-to-write"" by developing a GRU based network with the spotlight areas for transcribing the content accordingly. Moreover, we propose two implementations on the basis of STN, i.e., STNM and STNR, where the spotlight movement follows the Markov property and Recurrent modeling, respectively. We also design a reinforcement method to refine the framework by self-improving the spotlight mechanism. We conduct extensive experiments on many structural image datasets, where the results clearly demonstrate the effectiveness of STN framework.","['Yu Yin', 'Zhenya Huang', 'Enhong Chen', 'Qi Liu', 'Fuzheng Zhang', 'Xing Xie', 'Guoping Hu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1905.10954,Anomali
From web crawled text to project descriptions: automatic summarizing of social innovation projects,"In the past decade, social innovation projects have gained the attention of policy makers, as they address important social issues in an innovative manner. A database of social innovation is an important source of information that can expand collaboration between social innovators, drive policy and serve as an important resource for research. Such a database needs to have projects described and summarized. In this paper, we propose and compare several methods (e.g. SVM-based, recurrent neural network based, ensambled) for describing projects based on thetextthat is available on project websites. We also address and propose a new metric for automated evaluation of summaries based on topic modelling.","['Nikola Milosevic', 'Dimitar Marinov', 'Abdullah Gok', 'Goran Nenadic']",Preceeding of 24th International Conference on Applications of Natural Language to Information Systems (NLDB2019),arXiv,2019,https://doi.org/10.48550/arXiv.1905.09086,Anomali
A User-Centered Concept Mining System for Query and Document Understanding at Tencent,"Concepts embody the knowledge of the world and facilitate the cognitive processes of human beings.Miningconcepts from web documents and constructing the corresponding taxonomy are core research problems intextunderstanding and support many downstream tasks such as query analysis, knowledge base construction, recommendation, and search. However, we argue that most prior studies extract formal and overly general concepts from Wikipedia or static web pages, which are not representing the user perspective. In this paper, we describe our experience of implementing and deploying ConcepT in Tencent QQ Browser. It discovers user-centered concepts at the right granularity conforming to user interests, bymininga large amount of user queries and interactive search click logs. The extracted concepts have the proper granularity, are consistent with user language styles and are dynamically updated. We further present our techniques to tag documents with user-centered concepts and to construct a topic-concept-instance taxonomy, which has helped to improve search as well as news feeds recommendation in Tencent QQ Browser. We performed extensive offline evaluation to demonstrate that our approach could extract concepts of higher quality compared to several other existing methods. Our system has been deployed in Tencent QQ Browser. Results from online A/B testing involving a large number of real users suggest that the Impression Efficiency of feeds users increased by 6.01% after incorporating the user-centered concepts into the recommendation framework of Tencent QQ Browser.","['Bang Liu', 'Weidong Guo', 'Di Niu', 'Chaoyue Wang', 'Shunnan Xu', 'Jinghong Lin', 'Kunfeng Lai', 'Yu Xu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1905.08487,Anomali
Squeezed vacuum states from a whispering gallery mode resonator,"Squeezed vacuum states enable optical measurements below the quantum limit and hence are a valuable resource for applications in quantum metrology and also quantum communication. However, most available sources require high pump powers in the milliwatt range and large setups, which hinders real world applications. Furthermore, degenerate operation of such systems presents a challenge. Here, we use a compact crystalline whispering gallery mode resonator made of lithium niobate as a degenerate parametric oscillator. We demonstrate about 1.4 dB noise reduction below the shot noise level for only 300$μ\text{W}$of pump power in degenerate single mode operation. Furthermore, we report a record pump threshold as low as 1.35$μ\text{W}$. Our results show that the whispering gallery based approach presents a promising platform for a compact and efficient source for nonclassical light.","['Alexander Otterpohl', 'Florian Sedlmeir', 'Ulrich Vogl', 'Thomas Dirmeier', 'Golnoush Shafiee', 'Gerhard Schunk', 'Dmitry V. Strekalov', 'Harald G. L. Schwefel', 'Tobias Gehring', 'Ulrik L. Andersen', 'Gerd Leuchs', 'Christoph Marquardt']","Optica 6, 1375-1380 (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1905.07955,Anomali
A Compact Millimeter-Wavelength Fourier-Transform Spectrometer,"We have constructed a Fourier-transform spectrometer (FTS) operating between 50 and 330 GHz with minimum volume (355 x260 x64 mm) and weight (13 lbs) while maximizing optical throughput (100 $\mathrm{mm}^2$ sr) and optimizing the spectral resolution (4 GHz). This FTS is designed as a polarizing Martin-Puplett interferometer with unobstructed input and output in which both input polarizations undergo interference. The instrument construction is simple with mirrors milled on the box walls and one motorized stage as the single moving element. We characterize the performance of the FTS, compare the measurements to an optical simulation, and discuss features that relate to details of the FTS design. The simulation is also used to determine the tolerance of optical alignments for the required specifications. We detail the FTS mechanical design and provide the control software as well as the analysis code online.","['Zhaodi Pan', 'Mira Liu', 'Ritoban Basu Thakur', 'Bradford A. Benson', 'Dale J. Fixsen', 'Hazal Goksu', 'Eleanor Rath', 'Stephan S. Meyer']",,arXiv,2019,https://doi.org/10.48550/arXiv.1905.07399,Anomali
Conversion Prediction Using Multi-task Conditional Attention Networks to Support the Creation of Effective Ad Creative,"Accurately predicting conversions in advertisements is generally a challenging task, because such conversions do not occur frequently. In this paper, we propose a new framework to support creating high-performing ad creatives, including the accurate prediction of ad creativetextconversions before delivering to the consumer. The proposed framework includes three key ideas: multi-task learning, conditional attention, and attention highlighting. Multi-task learning is an idea for improving the prediction accuracy of conversion, which predicts clicks and conversions simultaneously, to solve the difficulty of data imbalance. Furthermore, conditional attention focuses attention of each ad creative with the consideration of its genre and target gender, thus improving conversion prediction accuracy. Attention highlighting visualizes important words and/or phrases based on conditional attention. We evaluated the proposed framework with actual delivery history data (14,000 creatives displayed more than a certain number of times from Gunosy Inc.), and confirmed that these ideas improve the prediction performance of conversions, and visualize noteworthy words according to the creatives' attributes.","['Shunsuke Kitada', 'Hitoshi Iyatomi', 'Yoshifumi Seki']","Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '19), August 4--8, 2019, Anchorage, AK, USA",arXiv,2019,https://doi.org/10.48550/arXiv.1905.07289,Anomali
Detection of Review Abuse via Semi-Supervised Binary Multi-Target Tensor Decomposition,"Product reviews and ratings on e-commerce websites provide customers with detailed insights about various aspects of the product such as quality, usefulness, etc. Since they influence customers' buying decisions, product reviews have become a fertile ground for abuse by sellers (colluding with reviewers) to promote their own products or to tarnish the reputation of competitor's products. In this paper, our focus is on detecting such abusive entities (both sellers and reviewers) by applying tensor decomposition on the product reviews data. While tensor decomposition is mostly unsupervised, we formulate our problem as a semi-supervised binary multi-target tensor decomposition, to take advantage of currently known abusive entities. We empirically show that our multi-target semi-supervised model achieves higher precision and recall in detecting abusive entities as compared to unsupervised techniques. Finally, we show that our proposed stochastic partial natural gradient inference for our model empirically achieves faster convergence than stochastic gradient and Online-EM with sufficient statistics.","['Anil R. Yelundur', 'Vineet Chaoji', 'Bamdev Mishra']",,arXiv,2019,https://doi.org/10.48550/arXiv.1905.06246,Anomali
A joint text mining-rank size investigation of the rhetoric structures of the US Presidents' speeches,"This work presents atextminingcontext and its use for a deep analysis of the messages delivered by the politicians. Specifically, we deal with an expert systems-based exploration of the rhetoric dynamics of a large collection of US Presidents' speeches, ranging from Washington to Trump. In particular, speeches are viewed as complex expert systems whose structures can be effectively analyzed through rank-size laws. The methodological contribution of the paper is twofold. First, we develop atextmining-based procedure for the construction of the dataset by using a web scraping routine on the Miller Center website -- the repository collecting the speeches. Second, we explore the implicit structure of the discourse data by implementing a rank-size procedure over the individual speeches, being the words of each speech ranked in terms of their frequencies. The scientific significance of the proposed combination oftext-miningand rank-size approaches can be found in its flexibility and generality, which let it be reproducible to a wide set of expert systems andtextminingcontexts. The usefulness of the proposed method and the speech subsequent analysis is demonstrated by the findings themselves. Indeed, in terms of impact, it is worth noting that interesting conclusions of social, political and linguistic nature on how 45 United States Presidents, from April 30, 1789 till February 28, 2017 delivered political messages can be carried out. Indeed, the proposed analysis shows some remarkable regularities, not only inside a given speech, but also among different speeches. Moreover, under a purely methodological perspective, the presented contribution suggests possible ways of generating a linguistic decision-making algorithm.","['Valerio Ficcadenti', 'Roy Cerqueti', 'Marcel Ausloos']",Expert Systems With Applications 123 (2019) 127-142,arXiv,2019,https://doi.org/10.48550/arXiv.1905.04705,Anomali
Metadata Management for Textual Documents in Data Lakes,"Data lakes have emerged as an alternative to data warehouses for the storage, exploration and analysis of big data. In a data lake, data are stored in a raw state and bear no explicit schema. Thence, an efficient metadata system is essential to avoid the data lake turning to a so-called data swamp. Existing works about managing data lake metadata mostly focus on structured and semi-structured data, with little research on unstructured data. Thus, we propose in this paper a methodological approach to build and manage a metadata system that is specific to textual documents in data lakes. First, we make an inventory of usual and meaningful metadata to extract. Then, we apply some specific techniques  from thetextminingand information retrieval domains to extract, store and reuse these metadata within the COREL research project, in order to validate our proposals.","['Pegdwendé Sawadogo', 'Tokio Kibata', 'Jérôme Darmont']","21st International Conference on Enterprise Information Systems (ICEIS 2019), May 2019, Heraklion, Greece. pp.72-83",arXiv,2019,https://doi.org/10.48550/arXiv.1905.04037,Anomali
Investigation of Multifractal Properties of Additive Data Stream,"The work presents results of a numerical study of fractal characteristics of multifractal stream at addition of stream, which does not have multifractal properties. They showed that the generalized Hurst exponent of total stream tends to one of original multifractal stream with increase in signal/noise ratio.","['Igor Ivanisenko', 'Lyudmyla Kirichenko', 'Tamara Radivilova']","2016 IEEE First International Conference on Data Stream Mining & Processing (DSMP), Lviv, 2016, pp. 305-308",arXiv,2019,https://doi.org/10.48550/arXiv.1905.03585,Anomali
Where does active travel fit within local community narratives of mobility space and place?,"Encouraging sustainable mobility patterns is at the forefront of policymaking at all scales of governance as the collective consciousness surrounding climate change continues to expand. Not every community, however, possesses the necessary economic or socio-cultural capital to encourage modal shifts away from private motorized vehicles towards active modes. The current literature on `soft' policy emphasizes the importance of tailoring behavior change campaigns to individual or geographic context. Yet, there is a lack of insight and appropriate tools to promote active mobility and overcome transport disadvantage from the local community perspective. The current study investigates the promotion of walking and cycling adoption using a series of focus groups with local residents in two geographic communities, namely Chicago's (1) Humboldt Park neighborhood and (2) suburb of Evanston. The research approach combines traditional qualitative discourse analysis with quantitativetext-miningtools, namely topic modeling and sentiment analysis. The analysis uncovers the local mobility culture, embedded norms and values associated with acceptance of active travel modes in different communities. We observe that underserved populations within diverse communities view active mobility simultaneously as a necessity and as a symbol of privilege that is sometimes at odds with the local culture. The mixed methods approach to analyzing community member discourses is translated into policy findings that are either tailored to local context or broadly applicable to curbing automobile dominance. Overall, residents of both Humboldt Park and Evanston envision a society in which multimodalism replaces car-centrism, but differences in the local physical and social environments would and should influence the manner in which overarching policy objectives are met.","['Alec Biehl', 'Ying Chen', 'Karla Sanabria-Veaz', 'David Uttal', 'Amanda Stathopoulos']","Transportation Research Part A: Policy and Practice, Volume 123, 2019, Pages 269-287",arXiv,2019,https://doi.org/10.48550/arXiv.1905.02674,Anomali
Nested Variational Autoencoder for Topic Modeling on Microtexts with Word Vectors,"Most of the information on the Internet is represented in the form of microtexts, which are shorttextsnippets such as news headlines or tweets. These sources of information are abundant, andminingthese data could uncover meaningful insights. Topic modeling is one of the popular methods to extract knowledge from a collection of documents; however, conventional topic models such as latent Dirichlet allocation (LDA) are unable to perform well on short documents, mostly due to the scarcity of word co-occurrence statistics embedded in the data. The objective of our research is to create a topic model that can achieve great performances on microtexts while requiring a small runtime for scalability to large datasets. To solve the lack of information of microtexts, we allow our method to take advantage of word embeddings for additional knowledge of relationships between words. For speed and scalability, we apply autoencoding variational Bayes, an algorithm that can perform efficient black-box inference in probabilistic models. The result of our work is a novel topic model called the nested variational autoencoder, which is a distribution that takes into account word vectors and is parameterized by a neural network architecture. For optimization, the model is trained to approximate the posterior distribution of the original LDA model. Experiments show the improvements of our model on microtexts as well as its runtime advantage.","['Trung Trinh', 'Tho Quan', 'Trung Mai']",,arXiv,2019,https://doi.org/10.48550/arXiv.1905.00195,Anomali
Analysis of Chinese Tourists in Japan by Text Mining of a Hotel Portal Site,"With an increasingly large number of Chinese tourists in Japan, the hotel industry is in need of an affordable market research tool that does not rely on expensive and time-consuming surveys or interviews. Because this problem is real and relevant to the hotel industry in Japan, and otherwise completely unexplored in other studies, we have extracted a list of potential keywords from Chinese reviews of Japanese hotels in the hotel portal site Ctrip1 using a mathematical model to then use them in a sentiment analysis with a machine learning classifier. While most studies that use information collected from the internet use pre-existing data analysis tools, in our study, we designed the mathematical model to have the highest possible performing results in classification, while also exploring on the potential business implications these may have.","['Elisa Claire Alemán Carreón', 'Hirofumi Nonaka', 'Toru Hiraoka']","In proceedings of the 18th International Symposium on Advanced Intelligent Systems (ISIS2017), pp. 191 - 198. Daegu, South Korea (2017, October 12)",arXiv,2019,https://doi.org/10.48550/arXiv.1904.13214,Anomali
Computational Approaches to Access Probabilistic Population Codes for Higher Cognition an Decision-Making,"In recent years, research unveiled more and more evidence for the so-called Bayesian Brain Paradigm, i.e. the human brain is interpreted as a probabilistic inference machine and Bayesian modelling approaches are hence used successfully. One of the many theories is that of Probabilistic Population Codes (PPC). Although this model has so far only been considered as meaningful and useful for sensory perception as well as motor control, it has always been suggested that this mechanism also underlies higher cognition and decision-making. However, the adequacy of PPC for this regard cannot be confirmed by means of neurological standard measurement procedures.
  In this article we combine the parallel research branches of recommender systems and predictive dataminingwith theoretical neuroscience. The nexus of both fields is given by behavioural variability and resulting internal distributions. We adopt latest experimental settings and measurement approaches from predictive dataminingto obtain these internal distributions, to inform the theoretical PPC approach and to deduce medical correlates which can indeed be measured in vivo. This is a strong hint for the applicability of the PPC approach and the Bayesian Brain Paradigm for higher cognition and human decision-making.","['Kevin Jasberg', 'Sergej Sizov']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.12651,Anomali
Exploring the Daschle Collection using Text Mining,A U.S. Senator from South Dakota donated documents that were accumulated during his service as a house representative and senator to be housed at the Bridges library at South Dakota State University. This project investigated the utility of quantitative statistical methods to explore some portions of this vast document collection. The available scanned documents and emails from constituents are analyzed using natural language processing methods including the Latent Dirichlet Allocation (LDA) model. This model identified major topics being discussed in a given collection of documents. Important events and popular issues from the Senator Daschles career are reflected in the changing topics from the model. These quantitative statistical methods provide a summary of the massive amount oftextwithout requiring significant human effort or time and can be applied to similar collections.,"['Damon Bayer', 'Semhar Michael']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.12623,Anomali
Twitter Sentiment Analysis using Distributed Word and Sentence Representation,"An important part of the information gathering and data analysis is to find out what people think about, either a product or an entity. Twitter is an opinion rich social networking site. The posts or tweets from this data can be used forminingpeople's opinions. The recent surge of activity in this area can be attributed to the computational treatment of data, which made opinion extraction and sentiment analysis easier. This paper classifies tweets into positive and negative sentiments, but instead of using traditional methods or preprocessingtextdata here we use the distributed representations of words and sentences to classify the tweets. We use Long Short Term Memory (LSTM) Networks, Convolutional Neural Networks (CNNs) and Artificial Neural Networks. The first two are used on Distributed Representation of words while the latter is used on the distributed representation of sentences. This paper achieves accuracies as high as 81%. It also suggests the best and optimal ways for creating distributed representations of words for sentiment analysis, out of the available methods.","['Dwarampudi Mahidhar Reddy', 'N V Subba Reddy', 'N V Subba Reddy']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.12580,Anomali
ASIME 2018 White Paper. In-Space Utilisation of Asteroids: Asteroid Composition -- Answers to Questions from the Asteroid Miners,"In keeping with the Luxembourg government's initiative to support the future use of space resources, ASIME 2018 was held in Belval, Luxembourg on April 16-17, 2018.
  The goal of ASIME 2018: Asteroid Intersections withMineEngineering, was to focus on asteroid composition for advancing the asteroid in-space resource utilisation domain. What do we know about asteroid composition from remote-sensing observations? What are the potential caveats in the interpretation of Earth-based spectral observations? What are the next steps to improve our knowledge on asteroid composition by means of ground-based and space-based observations and asteroid rendez-vous and sample return missions? How can asteroidminingcompanies use this knowledge?
  ASIME 2018 was a two-day workshop of almost 70 scientists and engineers in the context of the engineering needs of space missions with in-space asteroid utilisation. The 21 Questions from the asteroidminingcompanies were sorted into the four asteroid science themes: 1) Potential Targets, 2) Asteroid-Meteorite Links, 3) In-Situ Measurements and 4) Laboratory Measurements. The Answers to those Questions were provided by the scientists with their conference presentations and collected by A. Graps or edited directly into an open-access collaborative Google document or inserted by A. Graps using additional reference materials. During the ASIME 2018, first day and second day Wrap-Ups, the answers to the questions were discussed further. New readers to the asteroidminingtopic may find the Conversation boxes and the Mission Design discussions especially interesting.","['Amara L. Graps', 'Angel Abbud-Madrid', 'Paul Abell', 'Antonella Barucci', 'Pierre Beck', 'Lydie Bonal', 'Grant Bonin', 'Øystein Risan Borgersen', 'Daniel Britt', 'Humberto Campins', 'Kevin Cannon', 'Ian Carnelli', 'Benoît Carry', 'Ian Crawford', 'Julia de Leon', 'Line Drube', 'Kerri Donaldson-Hanna', 'Martin Elvis', 'Alan Fitzsimmons', 'JL Galache', 'Simon F. Green', 'Jan Thimo Grundmann', 'Alan Herique', 'Daniel Hestroffer', 'Henry Hsieh']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.11831,Anomali
Producing Corpora of Medieval and Premodern Occitan,"At a time when the quantity of - more or less freely - available data is increasing significantly, thanks to digital corpora, editions or libraries, the development of dataminingtools or deep learning methods allows researchers to build a corpus of study tailored for their research, to enrich their data and to exploit them.Open optical character recognition (OCR) tools can be adapted to old prints, incunabula or even manuscripts, with usable results, allowing the rapid creation of textual corpora. The alternation of training and correction phases makes it possible to improve the quality of the results by rapidly accumulating rawtextdata. These can then be structured, for example in XML/TEI, and enriched.The enrichment of thetextswith graphic or linguistic annotations can also be automated. These processes, known to linguists and functional for modern languages, present difficulties for languages such as Medieval Occitan, due in part to the absence of big enough lemmatized corpora. Suggestions for the creation of tools adapted to the considerable spelling variation of ancient languages will be presented, as well as experiments for the lemmatization of Medieval and Premodern Occitan.These techniques open the way for many exploitations. The much desired increase in the amount of available qualitytextsand data  makes it possible to improve digital philology methods, if everyone takes the trouble to make their data freely available online and reusable.By exposing different technical solutions and some micro-analyses as examples, this paper aims to show part of what digital philology can offer to researchers in the Occitan domain, while recalling the ethical issues on which such practices are based.","['Jean-Baptiste Camps', 'Gilles Guilhem Couffignal']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.11815,Anomali
Suggestion Mining from Online Reviews using ULMFiT,"In this paper we present our approach and the system description for Sub Task A of SemEval 2019 Task 9: SuggestionMiningfrom Online Reviews and Forums. Given a sentence, the task asks to predict whether the sentence consists of a suggestion or not. Our model is based on Universal Language Model Fine-tuning forTextClassification. We apply various pre-processing techniques before training the language and the classification model. We further provide detailed analysis of the results obtained using the trained model. Our team ranked 10th out of 34 participants, achieving an F1 score of 0.7011. We publicly share our implementation at https://github.com/isarth/SemEval9_MIDAS","['Sarthak Anand', 'Debanjan Mahata', 'Kartik Aggarwal', 'Laiba Mehnaz', 'Simra Shahid', 'Haimin Zhang', 'Yaman Kumar', 'Rajiv Ratn Shah', 'Karan Uppal']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.09076,Anomali
#Cyberbullying in the Digital Age: Exploring People's Opinions with Text Mining,"This study usedtextminingto investigate people's insights about cyberbullying. English-language tweets were collected and analyzed by R software. Our analysis demonstrated three major themes: (a) the major actions that needed to be taken into consideration (e.g. guiding parents and teachers to cyberbullying prevention, funding schools to fight cyberbullying), (b) certain events that were important to people (e.g. the Michigan cyberbullying law), and (c) people's major concerns in this regard (e.g. mental health issues among students). Parents and teachers have an important role in educating, informing, warning, preventing, and protecting against cyberbullying behaviors. The frequency of negative sentiments was almost 2.45 times more than positive sentiments.","['Iman Tahamtan', 'Li-Min Huang']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.09032,Anomali
Creative Procedural-Knowledge Extraction From Web Design Tutorials,"Complex design tasks often require performing diverse actions in a specific order. To (semi-)autonomously accomplish these tasks, applications need to understand and learn a wide range of design procedures, i.e., Creative Procedural-Knowledge (CPK). Prior knowledge base construction andmininghave not typically addressed the creative fields, such as design and arts. In this paper, we formalize an ontology of CPK using five components: goal, workflow, action, command and usage; and extract components' values from online design tutorials. We scraped 19.6K tutorial-related webpages and built a web application for professional designers to identify and summarize CPK components. The annotated dataset consists of 819 unique commands, 47,491 actions, and 2,022 workflows and goals. Based on this dataset, we propose a general CPK extraction pipeline and demonstrate that existingtextclassification and sequence-to-sequence models are limited in identifying, predicting and summarizing complex operations described in heterogeneous styles. Through quantitative and qualitative error analysis, we discuss CPK extraction challenges that need to be addressed by future research.","['Longqi Yang', 'Chen Fang', 'Hailin Jin', 'Walter Chang', 'Deborah Estrin']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.08587,Anomali
Hard Sample Mining for the Improved Retraining of Automatic Speech Recognition,"It is an effective way that improves the performance of the existing Automatic Speech Recognition (ASR) systems by retraining with more and more new training data in the target domain. Recently, Deep Neural Network (DNN) has become a successful model in the ASR field. In the training process of the DNN based methods, a back propagation of error between the transcription and the corresponding annotatedtextis used to update and optimize the parameters. Thus, the parameters are more influenced by the training samples with a big propagation error than the samples with a small one. In this paper, we define the samples with significant error as the hard samples and try to improve the performance of the ASR system by adding many of them. Unfortunately, the hard samples are sparse in the training data of the target domain, and manually label them is expensive. Therefore, we propose a hard samplesminingmethod based on an enhanced deep multiple instance learning, which can find the hard samples from unlabeled training data by using a small subset of the dataset with manual labeling in the target domain. We applied our method to an End2End ASR task and obtained the best performance.","['Jiabin Xue', 'Jiqing Han', 'Tieran Zheng', 'Jiaxing Guo', 'Boyong Wu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.08031,Anomali
Discovering Episodes with Compact Minimal Windows,"Discovering the most interesting patterns is the key problem in the field of patternmining. While ranking or selecting patterns is well-studied for itemsets it is surprisingly under-researched for other, more complex, pattern types.
  In this paper we propose a new quality measure for episodes. An episode is essentially a set of events with possible restrictions on the order of events. We say that an episode is significant if its occurrence is abnormally compact, that is, only few gap events occur between the actual episode events, when compared to the expected length according to the independence model. We can apply this measure as a post-pruning step by first discovering frequent episodes and then rank them according to this measure.
  In order to compute the score we will need to compute the mean and the variance according to the independence model. As a main technical contribution we introduce a technique that allows us to compute these values. Such a task is surprisingly complex and in order to solve it we develop intricate finite state machines that allow us to compute the needed statistics. We also show that asymptotically our score can be interpreted as a P-value. In our experiments we demonstrate that despite its intricacy our ranking is fast: we can rank tens of thousands episodes in seconds. Our experiments withtextdata demonstrate that our measure ranks interpretable episodes high.",['Nikolaj Tatti'],,arXiv,2019,https://doi.org/10.48550/arXiv.1904.07974,Anomali
Text segmentation on multilabel documents: A distant-supervised approach,"Segmentingtextinto semantically coherent segments is an important task with applications in information retrieval andtextsummarization. Developing accurate topical segmentation requires the availability of training data with ground truth information at the segment level. However, generating such labeled datasets, especially for applications in which the meaning of the labels is user-defined, is expensive and time-consuming. In this paper, we develop an approach that instead of using segment-level ground truth information, it instead uses the set of labels that are associated with a document and are easier to obtain as the training data essentially corresponds to a multilabel dataset. Our method, which can be thought of as an instance of distant supervision, improves upon the previous approaches by exploiting the fact that consecutive sentences in a document tend to talk about the same topic, and hence, probably belong to the same class. Experiments on thetextsegmentation task on a variety of datasets show that the segmentation produced by our method beats the competing approaches on four out of five datasets and performs at par on the fifth dataset. On the multilabeltextclassification task, our method performs at par with the competing approaches, while requiring significantly less time to estimate than the competing approaches.","['Saurav Manchanda', 'George Karypis']","2018 IEEE International Conference on Data Mining (ICDM), 1170-1175",arXiv,2019,https://doi.org/10.48550/arXiv.1904.06730,Anomali
Holistic and Comprehensive Annotation of Clinically Significant Findings on Diverse CT Images: Learning from Radiology Reports and Label Ontology,"In radiologists' routine work, one major task is to read a medical image, e.g., a CT scan, find significant lesions, and describe them in the radiology report. In this paper, we study the lesion description or annotation problem. Given a lesion image, our aim is to predict a comprehensive set of relevant labels, such as the lesion's body part, type, and attributes, which may assist downstream fine-grained diagnosis. To address this task, we first design a deep learning module to extract relevant semantic labels from the radiology reports associated with the lesion images. With the images andtext-minedlabels, we propose a lesion annotation network (LesaNet) based on a multilabel convolutional neural network (CNN) to learn all labels holistically. Hierarchical relations and mutually exclusive relations between the labels are leveraged to improve the label prediction accuracy. The relations are utilized in a label expansion strategy and a relational hard exampleminingalgorithm. We also attach a simple score propagation layer on LesaNet to enhance recall and explore implicit relation between labels. Multilabel metric learning is combined with classification to enable interpretable prediction. We evaluated LesaNet on the public DeepLesion dataset, which contains over 32K diverse lesion images. Experiments show that LesaNet can precisely annotate the lesions using an ontology of 171 fine-grained labels with an average AUC of 0.9344.","['Ke Yan', 'Yifan Peng', 'Veit Sandfort', 'Mohammadhadi Bagheri', 'Zhiyong Lu', 'Ronald M. Summers']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.04661,Anomali
Transient vibration imaging with time-resolved synthetic holographic confocal microscopy,"We introduce a new modality for dynamic phase imaging in confocal microscopy based on synthetic optical holography. By temporal demultiplexing of the detector signal into a series of holograms, we record time-resolved phase images directly in the time domain at a bandwidth as determined by the photo detector and digitizer. We demonstrate our method by optical imaging of transient vibrations in an atomic force microscope cantilever with 100 ns time resolution, and observe the dynamic deformation of the cantilever surface after excitation with broadband mechanical pulses. Temporal Fourier transform of a single data set acquired in 4.2 minutes yields frequency and mode profile of all excited out-of-plane vibration modes with sub-picometer vertical sensitivity and sub-micrometer lateral resolution. Our method has the potential for transient and spectroscopic vibration imaging of micromechanical systems at nano- and picosecond scale time resolution.","['Martin Schnell', 'P. Scott Carney', 'Rainer Hillenbrand']","Opt. Express 26, 26688-26699 (2018)",arXiv,2019,this https URL,Anomali
Outlier Detection for Improved Data Quality and Diversity in Dialog Systems,"In a corpus of data, outliers are either errors: mistakes in the data that are counterproductive, or are unique: informative samples that improve model robustness. Identifying outliers can lead to better datasets by (1) removing noise in datasets and (2) guiding collection of additional data to fill gaps. However, the problem of detecting both outlier types has received relatively little attention in NLP, particularly for dialog systems. We introduce a simple and effective technique for detecting both erroneous and unique samples in a corpus of shorttextsusing neural sentence embeddings combined with distance-based outlier detection. We also present a novel data collection pipeline built atop our detection technique to automatically and iterativelymineunique data samples while discarding erroneous samples. Experiments show that our outlier detection technique is effective at finding errors while our data collection pipeline yields highly diverse corpora that in turn produce more robust intent classification and slot-filling models.","['Stefan Larson', 'Anish Mahendran', 'Andrew Lee', 'Jonathan K. Kummerfeld', 'Parker Hill', 'Michael A. Laurenzano', 'Johann Hauswald', 'Lingjia Tang', 'Jason Mars']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.03122,Anomali
Summarizing Event Sequences with Serial Episodes: A Statistical Model and an Application,"In this paper we address the problem of discovering a small set of frequent serial episodes from sequential data so as to adequately characterize or summarize the data. We discuss an algorithm based on the Minimum Description Length (MDL) principle and the algorithm is a slight modification of an earlier method, called CSC-2. We present a novel generative model for sequence data containing prominent pairs of serial episodes and, using this, provide some statistical justification for the algorithm. We believe this is the first instance of such a statistical justification for an MDL based algorithm for summarizing event sequence data. We then present a novel application of this dataminingalgorithm intextclassification. By consideringtextdocuments as temporal sequences of words, the dataminingalgorithm can find a set of characteristic episodes for all the training data as a whole. The words that are part of these characteristic episodes could then be considered the only relevant words for the dictionary thus resulting in a considerably reduced feature vector dimension. We show, through simulation experiments using benchmark data sets, that the discovered frequent episodes can be used to achieve more than four-fold reduction in dictionary size without losing any classification accuracy.","['Soumyajit Mitra', 'P S Sastry']",,arXiv,2019,https://doi.org/10.48550/arXiv.1904.00516,Anomali
Broadband SiN asymmetric directional coupler for 840 nm operation,"Silicon nitride based photonic integrated circuits offer a wavelength operation window in the near infrared down to visible light, which makes them attractive for life science applications. However, they exhibit significantly different behavior in comparison with better-established silicon on insulator counterparts due to the lower index contrast. Among the most important building blocks in photonic integrated circuits are broadband couplers with a defined coupling ratio. We present silicon nitride broadband asymmetric directional coupler designs with 50/50 and 90/10 splitting ratios with a central wavelength of 840 nm for both TE- and TM-like polarization. We show that silicon nitride broadband asymmetric directional couplers can be designed accurately in a time efficient way by using a general implementation of the coupled mode theory. The accuracy of the coupled mode theory approach is validated with finite difference time domain simulations and confirmed with measurements of four coupler configurations.","['Stefan Nevlacsil', 'Moritz Eggeling', 'Paul Muellner', 'Guenther Koppitsch', 'Martin Sagmeister', 'Jochen Kraft', 'Rainer Hainberger']","OSA Continuum 1, 1324-1331 (2018)",arXiv,2019,https://doi.org/10.48550/arXiv.1903.12585,Anomali
Stable prediction with radiomics data,"Motivation: Radiomics refers to the high-throughputminingof quantitative features from radiographic images. It is a promising field in that it may provide a non-invasive solution for screening and classification. Standard machine learning classification and feature selection techniques, however, tend to display inferior performance in terms of (the stability of) predictive performance. This is due to the heavy multicollinearity present in radiomic data. We set out to provide an easy-to-use approach that deals with this problem.
  Results: We developed a four-step approach that projects the original high-dimensional feature space onto a lower-dimensional latent-feature space, while retaining most of the covariation in the data. It consists of (i) penalized maximum likelihood estimation of a redundancy filtered correlation matrix. The resulting matrix (ii) is the input for a maximum likelihood factor analysis procedure. This two-stage maximum-likelihood approach can be used to (iii) produce a compact set of stable features that (iv) can be directly used in any (regression-based) classifier or predictor. It outperforms other classification (and feature selection) techniques in both external and internal validation settings regarding survival in squamous cell cancers.","['Carel F. W. Peeters', 'Caroline Übelhör', 'Steven W. Mes', 'Roland Martens', 'Thomas Koopman', 'Pim de Graaf', 'Floris H. P. van Velden', 'Ronald Boellaard', 'Jonas A. Castelijns', 'Dennis E. te Beest', 'Martijn W. Heymans', 'Mark A. van de Wiel']",,arXiv,2019,https://doi.org/10.48550/arXiv.1903.11696,Anomali
On Attribution of Recurrent Neural Network Predictions via Additive Decomposition,"RNN models have achieved the state-of-the-art performance in a wide range oftextminingtasks. However, these models are often regarded as black-boxes and are criticized due to the lack of interpretability. In this paper, we enhance the interpretability of RNNs by providing interpretable rationales for RNN predictions. Nevertheless, interpreting RNNs is a challenging problem. Firstly, unlike existing methods that rely on local approximation, we aim to provide rationales that are more faithful to the decision making process of RNN models. Secondly, a flexible interpretation method should be able to assign contribution scores totextsegments of varying lengths, instead of only to individual words. To tackle these challenges, we propose a novel attribution method, called REAT, to provide interpretations to RNN predictions. REAT decomposes the final prediction of a RNN into additive contribution of each word in the inputtext. This additive decomposition enables REAT to further obtain phrase-level attribution scores. In addition, REAT is generally applicable to various RNN architectures, including GRU, LSTM and their bidirectional versions. Experimental results demonstrate the faithfulness and interpretability of the proposed attribution method. Comprehensive analysis shows that our attribution method could unveil the useful linguistic knowledge captured by RNNs. Some analysis further demonstrates our method could be utilized as a debugging tool to examine the vulnerability and failure reasons of RNNs, which may lead to several promising future directions to promote generalization ability of RNNs.","['Mengnan Du', 'Ninghao Liu', 'Fan Yang', 'Shuiwang Ji', 'Xia Hu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1903.11245,Anomali
git2net - Mining Time-Stamped Co-Editing Networks from Large git Repositories,"Data from software repositories have become an important foundation for the empirical study of software engineering processes. A recurring theme in the repositoryminingliterature is the inference of developer networks capturing e.g. collaboration, coordination, or communication from the commit history of projects. Most of the studied networks are based on the co-authorship of software artefacts defined at the level of files, modules, or packages. While this approach has led to insights into the social aspects of software development, it neglects detailed information on code changes and code ownership, e.g. which exact lines of code have been authored by which developers, that is contained in the commit log of software projects. Addressing this issue, we introduce git2net, a scalable python software that facilitates the extraction of fine-grained co-editing networks in large git repositories. It usestextminingtechniques to analyse the detailed history of textual modifications within files. This information allows us to construct directed, weighted, and time-stamped networks, where a link signifies that one developer has edited a block of source code originally written by another developer. Our tool is applied in case studies of an Open Source and a commercial software project. We argue that it opens up a massive new source of high-resolution data on human collaboration patterns.","['Christoph Gote', 'Ingo Scholtes', 'Frank Schweitzer']","MSR '19 Proceedings of the 16th International Conference on Mining Software Repositories, 2019, Pages 433-444",arXiv,2019,https://doi.org/10.48550/arXiv.1903.10180,Anomali
EMTk -- The Emotion Mining Toolkit,"The EmotionMiningToolkit (EMTk) is a suite of modules and datasets offering a comprehensive solution forminingsentiment and emotions from technicaltextcontributed by developers on communication channels. The toolkit is written in Java, Python, and R, and is released under the MIT open source license. In this paper, we describe its architecture and the benchmark against the previous, standalone versions of our sentiment analysis tools. Results show large improvements in terms of speed.","['Fabio Calefato', 'Filippo Lanubile', 'Nicole Novielli', 'Luigi Quaranta']",,arXiv,2021,https://doi.org/10.48550/arXiv.1903.09525,Anomali
Brillouin-based phase shifter in a silicon waveguide,"Integrated silicon microwave photonics offers great potential in microwave phase shifter elements, and promises compact and scalable multi-element chips that are free from electromagnetic interference. Stimulated Brillouin scattering, which was recently demonstrated in silicon, is a particularly powerful approach to induce a phase shift due to its inherent flexibility, offering an optically controllable and selective phase shift. However, to date, only moderate amounts of Brillouin gain has been achieved and theoretically this would restrict the phase shift to a few tens of degrees, significantly less than the required 360 degrees. Here, we overcome this limitation with a phase enhancement method using RF interference, showing a 360 degrees broadband phase shifter based on Brillouin scattering in a suspended silicon waveguide. We achieve a full 360 degrees phase-shift over a bandwidth of 15 GHz using a phase enhancement factor of 25, thereby enabling practical broadband Brillouin phase shifter for beam forming and other applications.","['Luke Mckay', 'Moritz Merklein', 'Alvaro Casas Bedoya', 'Amol Choudhary', 'Micah Jenkins', 'Charles Middleton', 'Alex Cramer', 'Joseph Devenport', 'Anthony Klee', 'Richard DeSalvo', 'Benjamin J. Eggleton']","Optica Vol. 6, Issue 7, pp. 907-913 (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1903.08363,Anomali
Redditors in Recovery: Text Mining Reddit to Investigate Transitions into Drug Addiction,"Increasing rates of opioid drug abuse and heightened prevalence of online support communities underscore the necessity of employing dataminingtechniques to better understand drug addiction using these rapidly developing online resources. In this work, we obtain data from Reddit, an online collection of forums, to gather insight into drug use/misuse usingtextdata from users themselves. Specifically, using user posts, we trained 1) a binary classifier which predicts transitions from casual drug discussion forums to drug recovery forums and 2) a Cox regression model that outputs likelihoods of such transitions. In doing so, we found that utterances of select drugs and certain linguistic features contained in one's posts can help predict these transitions. Using unfiltered drug-related posts, our research delineates drugs that are associated with higher rates of transitions from recreational drug discussion to support/recovery discussion, offers insight into modern drug culture, and provides tools with potential applications in combating the opioid crisis.","['John Lu', 'Sumati Sridhar', 'Ritika Pandey', 'Mohammad Al Hasan', 'George Mohler']",,arXiv,2019,https://doi.org/10.48550/arXiv.1903.04081,Anomali
Efficiently Reusing Natural Language Processing Models for Phenotype-Mention Identification in Free-text Electronic Medical Records: Methodology Study,"Background: Many efforts have been put into the use of automated approaches, such as natural language processing (NLP), tomineor extract data from free-textmedical records to construct comprehensive patient profiles for delivering better health-care. Reusing NLP models in new settings, however, remains cumbersome - requiring validation and/or retraining on new data iteratively to achieve convergent results.
  Objective: The aim of this work is to minimize the effort involved in reusing NLP models on free-textmedical records.
  Methods: We formally define and analyse the model adaptation problem in phenotype-mention identification tasks. We identify ""duplicate waste"" and ""imbalance waste"", which collectively impede efficient model reuse. We propose a phenotype embedding based approach to minimize these sources of waste without the need for labelled data from new settings.
  Results: We conduct experiments on data from a large mental health registry to reuse NLP models in four phenotype-mention identification tasks. The proposed approach can choose the best model for a new task, identifying up to 76% (duplicate waste), i.e. phenotype mentions without the need for validation and model retraining, and with very good performance (93-97% accuracy). It can also provide guidance for validating and retraining the selected model for novel language patterns in new tasks, saving around 80% (imbalance waste), i.e. the effort required in ""blind"" model-adaptation approaches.
  Conclusions: Adapting pre-trained NLP models for new tasks can be more efficient and effective if the language pattern landscapes of old settings and new settings can be made explicit and comparable. Our experiments show that the phenotype-mention embedding approach is an effective way to model language patterns for phenotype-mention identification tasks and that its use can guide efficient NLP model reuse.","['Honghan Wu', 'Karen Hodgson', 'Sue Dyson', 'Katherine I. Morley', 'Zina M. Ibrahim', 'Ehtesham Iqbal', 'Robert Stewart', 'Richard JB Dobson', 'Cathie Sudlow']",,arXiv,2019,https://doi.org/10.48550/arXiv.1903.03995,Anomali
Twitter Speaks: A Case of National Disaster Situational Awareness,"In recent years, we have been faced with a series of natural disasters causing a tremendous amount of financial, environmental, and human losses. The unpredictable nature of natural disasters' behavior makes it hard to have a comprehensive situational awareness (SA) to support disaster management. Using opinion surveys is a traditional approach to analyze public concerns during natural disasters; however, this approach is limited, expensive, and time-consuming. Luckily the advent of social media has provided scholars with an alternative means of analyzing public concerns. Social media enable users (people) to freely communicate their opinions and disperse information regarding current events including natural disasters. This research emphasizes the value of social media analysis and proposes an analytical framework: Twitter Situational Awareness (TwiSA). This framework usestextminingmethods including sentiment analysis and topic modeling to create a better SA for disaster preparedness, response, and recovery. TwiSA has also effectively deployed on a large number of tweets and tracks the negative concerns of people during the 2015 South Carolina flood.","['Amir Karami', 'Vishal Shah', 'Reza Vaezi', 'Amit Bansal']",,arXiv,2019,https://doi.org/10.48550/arXiv.1903.02706,Anomali
Adaptive computation of the Symmetric Nonnegative Matrix Factorization (NMF),"Nonnegative Matrix Factorization (NMF), first proposed in 1994 for data analysis, has received successively much attention in a great variety of contexts such as datamining,textclustering, computer vision, bioinformatics, etc. In this paper the case of a symmetric matrix is considered and the symmetric nonnegative matrix factorization (SymNMF) is obtained by using a penalized nonsymmetric minimization problem. Instead of letting the penalizing parameter increase according to an a priori fixed rule, as suggested in literature, we propose a heuristic approach based on an adaptive technique. Extensive experimentation shows that the proposed algorithm is effective.","['Paola Favati', 'Grazia Lotti', 'Ornella Menchi', 'Francesco Romani']",,arXiv,2019,https://doi.org/10.48550/arXiv.1903.01321,Anomali
Large-Scale Object Mining for Object Discovery from Unlabeled Video,"This paper addresses the problem of object discovery from unlabeled driving videos captured in a realistic automotive setting. Identifying recurring object categories in such raw video streams is a very challenging problem. Not only do object candidates first have to be localized in the input images, but many interesting object categories occur relatively infrequently. Object discovery will therefore have to deal with the difficulties of operating in the long tail of the object distribution. We demonstrate the feasibility of performing fully automatic object discovery in such a setting byminingobject tracks using a generic object tracker. In order to facilitate further research in object discovery, we release a collection of more than 360,000 automaticallyminedobject tracks from 10+ hours of video data (560,000 frames). We use this dataset to evaluate the suitability of different feature representations and clustering strategies for object discovery.","['Aljosa Osep', 'Paul Voigtlaender', 'Jonathon Luiten', 'Stefan Breuers', 'Bastian Leibe']",,arXiv,2019,https://doi.org/10.48550/arXiv.1903.00362,Anomali
Leveraging Deep Graph-Based Text Representation for Sentiment Polarity Applications,"Over the last few years, machine learning over graph structures has manifested a significant enhancement intextminingapplications such as event detection, opinionmining, and news recommendation. One of the primary challenges in this regard is structuring a graph that encodes and encompasses the features of textual data for the effective machine learning algorithm. Besides, exploration and exploiting of semantic relations is regarded as a principal step intextminingapplications. However, most of the traditionaltextminingmethods perform somewhat poor in terms of employing such relations. In this paper, we propose a sentence-level graph-basedtextrepresentation which includes stop words to consider semantic and term relations. Then, we employ a representation learning approach on the combined graphs of sentences to extract the latent and continuous features of the documents. Eventually, the learned features of the documents are fed into a deep neural network for the sentiment classification task. The experimental results demonstrate that the proposed method substantially outperforms the related sentiment analysis approaches based on several benchmark datasets. Furthermore, our method can be generalized on different datasets without any dependency on pre-trained word embeddings.","['Kayvan Bijari', 'Hadi Zare', 'Emad Kebriaei', 'Hadi Veisi']","Expert Systems with Applications Volume 144, 15 April 2020, 113090",arXiv,2019,https://doi.org/10.48550/arXiv.1902.10247,Anomali
A framework for information extraction from tables in biomedical literature,"The scientific literature is growing exponentially, and professionals are no more able to cope with the current amount of publications.Textminingprovided in the past methods to retrieve and extract information fromtext; however, most of these approaches ignored tables and figures. The research done inminingtable data still does not have an integrated approach forminingthat would consider all complexities and challenges of a table. Our research is examining the methods for extracting numerical (number of patients, age, gender distribution) and textual (adverse reactions) information from tables in the clinical literature. We present a requirement analysis template and an integral methodology for information extraction from tables in clinical domain that contains 7 steps: (1) table detection, (2) functional processing, (3) structural processing, (4) semantic tagging, (5) pragmatic processing, (6) cell selection and (7) syntactic processing and extraction. Our approach performed with the F-measure ranged between 82 and 92%, depending on the variable, task and its complexity.","['Nikola Milosevic', 'Cassie Gregson', 'Robert Hernandez', 'Goran Nenadic']","2019, International Journal on Document Analysis and Recognition (IJDAR)",arXiv,2019,https://doi.org/10.48550/arXiv.1902.10031,Anomali
Edge Replacement Grammars: A Formal Language Approach for Generating Graphs,"Graphs are increasingly becoming ubiquitous as models for structured data. A generative model that closely mimics the structural properties of a given set of graphs has utility in a variety of domains. Much of the existing work require that a large number of parameters, in fact exponential in size of the graphs, be estimated from the data. We take a slightly different approach to this problem, leveraging the extensive prior work in the formal graph grammar literature. In this paper, we propose a graph generation model based on Probabilistic Edge Replacement Grammars (PERGs). We propose a variant of PERG called Restricted PERG (RPERG), which is analogous to PCFGs in string grammar literature. With this restriction, we are able to derive a learning algorithm for estimating the parameters of the grammar from graph data. We empirically demonstrate on real life datasets that RPERGs outperform existing methods for graph generation. We improve on the performance of the state-of-the-art Hyperedge Replacement Grammar based graph generative model. Despite being a context free grammar, the proposed model is able to capture many of the structural properties of real networks, such as degree distributions, power law and spectral characteristics.","['Revanth Reddy', 'Sarath Chandar', 'Balaraman Ravindran']",,arXiv,2019,https://doi.org/10.48550/arXiv.1902.07159,Anomali
Comprehensive review of models and methods for inferences in bio-chemical reaction networks,"Key processes in biological and chemical systems are described by networks of chemical reactions. From molecular biology to biotechnology applications, computational models of reaction networks are used extensively to elucidate their non-linear dynamics. Model dynamics are crucially dependent on parameter values which are often estimated from observations. Over past decade, the interest in parameter and state estimation in models of (bio-)chemical reaction networks (BRNs) grew considerably. Statistical inference problems are also encountered in many other tasks including model calibration, discrimination, identifiability and checking as well as optimum experiment design, sensitivity analysis, bifurcation analysis and other. The aim of this review paper is to explore developments of past decade to understand what BRN models are commonly used in literature, and for what inference tasks and inference methods. Initial collection of about 700 publications excluding books in computational biology and chemistry were screened to select over 260 research papers and 20 graduate theses concerning estimation problems in BRNs. The paper selection was performed astextminingusing scripts to automate search for relevant keywords and terms. The outcome are tables revealing the level of interest in different inference tasks and methods for given models in literature as well as recent trends. In addition, a brief survey of general estimation strategies is provided to facilitate understanding of estimation methods which are used for BRNs. Our findings indicate that many combinations of models, tasks and methods are still relatively sparse representing new research opportunities to explore those that have not been considered - perhaps for a good reason. The paper concludes by discussing future research directions including research problems which cannot be directly deduced from presented tables.","['Pavel Loskot', 'Komlan Atitey', 'Lyudmila Mihaylova']",,arXiv,2019,https://doi.org/10.48550/arXiv.1902.05828,Anomali
Replication Can Improve Prior Results: A GitHub Study of Pull Request Acceptance,"Crowdsourcing and dataminingcan be used to effectively reduce the effort associated with the partial replication and enhancement of qualitative studies.
  For example, in a primary study, other researchers explored factors influencing the fate of GitHub pull requests using an extensive qualitative analysis of 20 pull requests. Guided by their findings, we mapped some of their qualitative insights onto quantitative questions. To determine how well their findings generalize, we collected much more data (170 additional pull requests from 142 GitHub projects). Using crowdsourcing, that data was augmented with subjective qualitative human opinions about how pull requests extended the original issue. The crowd's answers were then combined with quantitative features and, using datamining, used to build a predictor for whether code would be merged. That predictor was far more accurate that one built from the primary study's qualitative factors (F1=90 vs 68\%), illustrating the value of a mixed-methods approach and replication to improve prior results.
  To test the generality of this approach, the next step in future work is to conduct other studies that extend qualitative studies with crowdsourcing and datamining.","['Di Chen', 'Kathyrn Stolee', 'Tim Menzies']",,arXiv,2019,https://doi.org/10.48550/arXiv.1902.04060,Anomali
A new simple and effective measure for bag-of-word inter-document similarity measurement,"To measure the similarity of two documents in the bag-of-words (BoW) vector representation, different term weighting schemes are used to improve the performance of cosine similarity---the most widely used inter-document similarity measure intextmining. In this paper, we identify the shortcomings of the underlying assumptions of term weighting in the inter-document similarity measurement task; and provide a more fit-to-the-purpose alternative. Based on this new assumption, we introduce a new simple but effective similarity measure which does not require explicit term weighting. The proposed measure employs a more nuanced probabilistic approach than those used in term weighting to measure the similarity of two documents w.r.t each term occurring in the two documents. Our empirical comparison with the existing similarity measures using different term weighting schemes shows that the new measure produces (i) better results in the binary BoW representation; and (ii) competitive and more consistent results in the term-frequency-based BoW representation.","['Sunil Aryal', 'Kai Ming Ting', 'Takashi Washio', 'Gholamreza Haffari']",,arXiv,2019,https://doi.org/10.48550/arXiv.1902.03402,Anomali
Multi-task Learning for Target-dependent Sentiment Classification,"Detecting and aggregating sentiments toward people, organizations, and events expressed in unstructured social media have become criticaltextminingoperations. Early systems detected sentiments over whole passages, whereas more recently, target-specific sentiments have been of greater interest. In this paper, we present MTTDSC, a multi-task target-dependent sentiment classification system that is informed by feature representation learnt for the related auxiliary task of passage-level sentiment classification. The auxiliary task uses a gated recurrent unit (GRU) and pools GRU states, followed by an auxiliary fully-connected layer that outputs passage-level predictions. In the main task, these GRUs contribute auxiliary per-token representations over and above word embeddings. The main task has its own, separate GRUs. The auxiliary and main GRUs send their states to a different fully connected layer, trained for the main task. Extensive experiments using two auxiliary datasets and three benchmark datasets (of which one is new, introduced by us) for the main task demonstrate that MTTDSC outperforms state-of-the-art baselines. Using word-level sensitivity analysis, we present anecdotal evidence that prior systems can make incorrect target-specific predictions because they miss sentiments expressed by words independent of target.","['Divam Gupta', 'Kushagra Singh', 'Soumen Chakrabarti', 'Tanmoy Chakraborty']",,arXiv,2019,https://doi.org/10.48550/arXiv.1902.02930,Anomali
A Multi-Resolution Word Embedding for Document Retrieval from Large Unstructured Knowledge Bases,"Deep language models learning a hierarchical representation proved to be a powerful tool for natural language processing,textminingand information retrieval. However, representations that perform well for retrieval must capture semantic meaning at different levels of abstraction or context-scopes. In this paper, we propose a new method to generate multi-resolution word embeddings that represent documents at multiple resolutions in terms of context-scopes. In order to investigate its performance,we use the Stanford Question Answering Dataset (SQuAD) and the Question Answering by Search And Reading (QUASAR) in an open-domain question-answering setting, where the first task is to find documents useful for answering a given question. To this end, we first compare the quality of varioustext-embedding methods for retrieval performance and give an extensive empirical comparison with the performance of various non-augmented base embeddings with and without multi-resolution representation. We argue that multi-resolution word embeddings are consistently superior to the original counterparts and deep residual neural models specifically trained for retrieval purposes can yield further significant gains when they are used for augmenting those embeddings.","['Tolgahan Cakaloglu', 'Xiaowei Xu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1902.00663,Anomali
Revised JNLPBA Corpus: A Revised Version of Biomedical NER Corpus for Relation Extraction Task,"The advancement of biomedical named entity recognition (BNER) and biomedical relation extraction (BRE) researches promotes the development oftextminingin biological domains. As a cornerstone of BRE, robust BNER system is required to identify the mentioned NEs in plaintextsfor further relation extraction stage. However, the current BNER corpora, which play important roles in these tasks, paid less attention to achieve the criteria for BRE task. In this study, we present Revised JNLPBA corpus, the revision of JNLPBA corpus, to broaden the applicability of a NER corpus from BNER to BRE task. We preserve the original entity types including protein, DNA, RNA, cell line and cell type while all the abstracts in JNLPBA corpus are manually curated by domain experts again basis on the new annotation guideline focusing on the specific NEs instead of general terms. Simultaneously, several imperfection issues in JNLPBA are pointed out and made up in the new corpus. To compare the adaptability of different NER systems in Revised JNLPBA and JNLPBA corpora, the F1-measure was measured in three open sources NER systems including BANNER, Gimli and NERSuite. In the same circumstance, all the systems perform average 10% better in Revised JNLPBA than in JNLPBA. Moreover, the cross-validation test is carried out which we train the NER systems on JNLPBA/Revised JNLPBA corpora and access the performance in both protein-protein interaction extraction (PPIE) and biomedical event extraction (BEE) corpora to confirm that the newly refined Revised JNLPBA is a competent NER corpus in biomedical relation application. The revised JNLPBA corpus is freely available at iasl-btm.iis.sinica.edu.tw/BNER/Content/Revised_JNLPBA.zip.","['Ming-Siang Huang', 'Po-Ting Lai', 'Richard Tzong-Han Tsai', 'Wen-Lian Hsu']","Briefings in Bioinformatics, 2020, bbaa054",arXiv,2019,https://doi.org/10.48550/arXiv.1901.10219,Anomali
CRAQL: A Composable Language for Querying Source Code,"This paper describes the design and implementation of CRAQL (Composable Repository Analysis and Query Language), a new query language for source code. The growth of source codeminingand its applications suggest the need for a query language that can fully utilize and correlate across the unique structure and metadata of parsed source code.
  CRAQL is built on an underlying abstraction analogous to the underpinnings of SQL, but aimed at parsed source code. Thus, while SQL queries' inputs and outputs are sets of tuples, CRAQL queries' inputs and outputs are sets of abstract syntax trees (ASTs). This abstraction makes CRAQL queries composable (the output of one query can become the input to another) and improves the power of the language by allowing for querying of the tree structure and metadata, as well as rawtext. Furthermore, the abstraction enables tree-specific language optimizations and allows CRAQL to be easily applied to any language that is parsable into ASTs. These attributes, along with a familiar syntax similar to SQL, allow complex queries to be expressed in a compact, straightforward manner. Questions such as ""find the longest series of statements without any loops,"" ""find methods that are never called,"" ""find getters (0-parameter methods with a single statement that returns a member variable),"" or ""find the percentage of variables declared at the top of a block"" all translate to simple, understandable queries in CRAQL.
  In this paper we describe the language, its features and capabilities. We compare CRAQL to other languages for querying source code and find that it has potential advantages in clarity and compactness. We discuss the features and optimizations added to support searching parse tree collections effectively and efficiently. Finally, we summarize the application of the language to millions of Java source files, the details of which are in a companion paper.","['Blake Johnson', 'Rahul Simha']",,arXiv,2019,https://doi.org/10.48550/arXiv.1901.09409,Anomali
BioBERT: a pre-trained biomedical language representation model for biomedical text mining,"Biomedicaltextminingis becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedicaltextminingmodels. However, directly applying the advancements in NLP to biomedicaltextminingoften yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for BiomedicalTextMining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedicaltextminingtasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedicaltextminingtasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedicaltexts. We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.","['Jinhyuk Lee', 'Wonjin Yoon', 'Sungdong Kim', 'Donghyeon Kim', 'Sunkyu Kim', 'Chan Ho So', 'Jaewoo Kang']",,arXiv,2019,https://doi.org/10.48550/arXiv.1901.08746,Anomali
Emotion Detection and Analysis on Social Media,"In this paper, we address the problem of detection, classification and quantification of emotions oftextin any form. We consider Englishtextcollected from social media like Twitter, which can provide information having utility in a variety of ways, especially opinionmining. Social media like Twitter and Facebook is full of emotions, feelings and opinions of people all over the world. However, analyzing and classifyingtexton the basis of emotions is a big challenge and can be considered as an advanced form of Sentiment Analysis. This paper proposes a method to classifytextinto six different Emotion-Categories: Happiness, Sadness, Fear, Anger, Surprise and Disgust. In our model, we use two different approaches and combine them to effectively extract these emotions fromtext. The first approach is based on Natural Language Processing, and uses several textual features like emoticons, degree words and negations, Parts Of Speech and other grammatical analysis. The second approach is based on Machine Learning classification algorithms. We have also successfully devised a method to automate the creation of the training-set itself, so as to eliminate the need of manual annotation of large datasets. Moreover, we have managed to create a large bag of emotional words, along with their emotion-intensities. On testing, it is shown that our model provides significant accuracy in classifying tweets taken from Twitter.","['Bharat Gaind', 'Varun Syal', 'Sneha Padgalwar']",Global Journal of Engineering Science and Researches (ICRTCET-18) (2019) 78-89,arXiv,2019,https://doi.org/10.48550/arXiv.1901.08458,Anomali
ACMiner: Extraction and Analysis of Authorization Checks in Android's Middleware,"Billions of users rely on the security of the Android platform to protect phones, tablets, and many different types of consumer electronics. While Android's permission model is well studied, the enforcement of the protection policy has received relatively little attention. Much of this enforcement is spread across system services, taking the form of hard-coded checks within their implementations. In this paper, we propose Authorization Check Miner (ACMiner), a framework for evaluating the correctness of Android's access control enforcement through consistency analysis of authorization checks. ACMiner combines program andtextanalysis techniques to generate a rich set of authorization checks,minesthe corresponding protection policy for each service entry point, and uses association ruleminingat a service granularity to identify inconsistencies that may correspond to vulnerabilities. We used ACMiner to study the AOSP version of Android 7.1.1 to identify 28 vulnerabilities relating to missing authorization checks. In doing so, we demonstrate ACMiner's ability to help domain experts process thousands of authorization checks scattered across millions of lines of code.","['Sigmund Albert Gorski III', 'Benjamin Andow', 'Adwait Nadkarni', 'Sunil Manandhar', 'William Enck', 'Eric Bodden', 'Alexandre Bartel']",,arXiv,2019,https://doi.org/10.48550/arXiv.1901.03603,Anomali
Inference of Demographic Attributes based on Mobile Phone Usage Patterns and Social Network Topology,"Mobile phone usage provides a wealth of information, which can be used to better understand the demographic structure of a population. In this paper, we focus on the population of Mexican mobile phone users. We first present an observational study of mobile phone usage according to gender and age groups. We are able to detect significant differences in phone usage among different subgroups of the population. We then study the performance of different machine learning (ML) methods to predict demographic features (namely, age and gender) of unlabeled users by leveraging individual calling patterns, as well as the structure of the communication graph. We show how a specific implementation of a diffusion model, harnessing the graph structure, has significantly better performance over other node-based standard ML methods. We provide details of the methodology together with an analysis of the robustness of our results to changes in the model parameters. Furthermore, by carefully examining the topological relations of the training nodes (seed nodes) to the rest of the nodes in the network, we find topological metrics which have a direct influence on the performance of the algorithm.","['Carlos Sarraute', 'Jorge Brea', 'Javier Burroni', 'Pablo Blanc']",Social Network Analysis and Mining 5.1 (2015): 39,arXiv,2019,https://doi.org/10.48550/arXiv.1901.02932,Anomali
Text Mining Customer Reviews For Aspect-based Restaurant Rating,"This study appliestextminingto analyze customer reviews and automatically assign a collective restaurant star rating based on five predetermined aspects: ambiance, cost, food, hygiene, and service. The application provides a web and mobile crowd sourcing platform where users share dining experiences and get insights about the strengths and weaknesses of a restaurant through user contributed feedback.Textreviews are tokenized into sentences. Noun-adjective pairs are extracted from each sentence using Stanford Core NLP library and are associated to aspects based on the bag of associated words fed into the system. The sentiment weight of the adjectives is determined through AFINN library. An overall restaurant star rating is computed based on the individual aspect rating. Further, a word cloud is generated to provide visual display of the most frequently occurring terms in the reviews. The more feedbacks are added the more reflective the sentiment score to the restaurants' performance.","['Jovelyn C. Cuizon', 'Jesserine Lopez', 'Danica Rose Jones']","International Journal of Computer Science & Information Technology (IJCSIT) Vol 10, No 6, December 2018",arXiv,2019,https://doi.org/10.48550/arXiv.1901.01642,Anomali
QRP Variation of Cross--Approximation Iterations for Low Rank Approximation,"We call matrix algorithms superfast if they use much fewer flops and memory cells than the input matrix has entries. Using such algorithms is indispensable for Big DataMiningand Analysis, where the input matrices are so immense that one can only access a small fraction of all their entries. A natural remedy is Low Rank Approximation (LRA) of these matrices, which is routinely computed by means of Cross-Approximation iterations for more than a decade of worldwide application in computational practice. We point out and extensively test an important application of superfast LRA to significant acceleration of the celebrated Fast Multipole Method, which turns it into Superfast Multipole Method.","['Victor Y. Pan', 'John Svadlenka']",,arXiv,2019,https://doi.org/10.48550/arXiv.1901.00377,Anomali
Sentiment Classification of Customer Reviews about Automobiles in Roman Urdu,"Textminingis a broad field having sentimentminingas its important constituent in which we try to deduce the behavior of people towards a specific item, merchandise, politics, sports, social media comments, review sites etc. Out of many issues in sentimentmining, analysis and classification, one major issue is that the reviews and comments can be in different languages like English, Arabic, Urdu etc. Handling each language according to its rules is a difficult task. A lot of research work has been done in English Language for sentiment analysis and classification but limited sentiment analysis work is being carried out on other regional languages like Arabic, Urdu and Hindi. In this paper, Waikato Environment for Knowledge Analysis (WEKA) is used as a platform to execute different classification models fortextclassification of Roman Urdutext. Reviews dataset has been scrapped from different automobiles sites. These extracted Roman Urdu reviews, containing 1000 positive and 1000 negative reviews, are then saved in WEKA attribute-relation file format (arff) as labeled examples. Training is done on 80% of this data and rest of it is used for testing purpose which is done using different models and results are analyzed in each case. The results show that Multinomial Naive Bayes outperformed Bagging, Deep Neural Network, Decision Tree, Random Forest, AdaBoost, k-NN and SVM Classifiers in terms of more accuracy, precision, recall and F-measure.","['Moin Khan', 'Kamran Malik']","Advances in Intelligent Systems and Computing, vol 887 (2018) 630-640",arXiv,2018,this https URL,Anomali
Applying Text Mining to Protest Stories as Voice against Media Censorship,"Data driven activism attempts to collect, analyze and visualize data to foster social change. However, during media censorship it is often impossible to collect such data. Here we demonstrate that data from personal stories can also help us to gain insights about protests and activism which can work as a voice for the activists. We analyze protest story data by extracting location network from the stories and perform emotionminingto get insight about the protest.","['Tahsin Mayeesha', 'Zareen Tasneem', 'Jasmine Jones', 'Nova Ahmed']",,arXiv,2018,https://doi.org/10.48550/arXiv.1812.11430,Anomali
Big Data Information Reconstruction on an Infinite Tree for a $4\times 4$-state Asymmetric Model with Community Effects,"The information reconstruction problem on an infinite tree, is to collect and analyze massive data samples at the $n$th level of the tree to identify whether there is non-vanishing information of the root, as $n$ goes to infinity. This problem has wide applications in various fields such as biology, information theory and statistical physics, and its close connections to cluster learning, dataminingand deep learning have been well established in recent years. Although it has been studied in numerous contexts, the existing literatures with rigorous reconstruction thresholds established are very limited. In this paper, motivated by a classical deoxyribonucleic acid (DNA) evolution model, the F$81$ model, and taking into consideration of the Chargaff's parity rule by allowing the existence of a guanine-cytosine content bias, we study the noise channel in terms of a $4\times 4$-state asymmetric probability transition matrix with community effects, for four nucleobases of DNA. The corresponding information reconstruction problem in molecular phylogenetics is explored, by means of refined analyses of moment recursion, in-depth concentration estimates, and thorough investigations on an asymptotic $4$-dimensional nonlinear second order dynamical system. We rigorously show that the reconstruction bound is not tight when the sum of the base frequencies of adenine and thymine falls in the interval $\left(0,1/2-\sqrt{3}/6\right)\bigcup \left(1/2+\sqrt{3}/6,1\right)$, which is the first rigorous result on asymmetric noisy channels with community effects.","['Wenjian Liu', 'Ning Ning']",,arXiv,2018,https://doi.org/10.48550/arXiv.1812.10475,Anomali
Supervised Sentiment Classification with CNNs for Diverse SE Datasets,"Sentiment analysis, a popular technique for opinionmining, has been used by the software engineering research community for tasks such as assessing app reviews, developer emotions in issue trackers and developer opinions on APIs. Past research indicates that state-of-the-art sentiment analysis techniques have poor performance on SE data. This is because sentiment analysis tools are often designed to work on non-technical documents such as movie reviews. In this study, we attempt to solve the issues with existing sentiment analysis techniques for SEtextsby proposing a hierarchical model based on convolutional neural networks (CNN) and long short-term memory (LSTM) trained on top of pre-trained word vectors. We assessed our model's performance and reliability by comparing it with a number of frequently used sentiment analysis tools on five gold standard datasets. Our results show that our model pushes the state of the art further on all datasets in terms of accuracy. We also show that it is possible to get better accuracy after labelling a small sample of the dataset and re-training our model rather than using an unsupervised classifier.","['Achyudh Ram', 'Meiyappan Nagappan']",,arXiv,2018,https://doi.org/10.48550/arXiv.1812.09653,Anomali
Mining Interpretable AOG Representations from Convolutional Networks via Active Question Answering,"In this paper, we present a method tomineobject-part patterns from conv-layers of a pre-trained convolutional neural network (CNN). Theminedobject-part patterns are organized by an And-Or graph (AOG). This interpretable AOG representation consists of a four-layer semantic hierarchy, i.e., semantic parts, part templates, latent patterns, and neural units. The AOG associates each object part with certain neural units in feature maps of conv-layers. The AOG is constructed in a weakly-supervised manner, i.e., very few annotations (e.g., 3-20) of object parts are used to guide the learning of AOGs. We develop a question-answering (QA) method that uses active human-computer communications tominepatterns from a pre-trained CNN, in order to incrementally explain more features in conv-layers. During the learning process, our QA method uses the current AOG for part localization. The QA method actively identifies objects, whose feature maps cannot be explained by the AOG. Then, our method asks people to annotate parts on the unexplained objects, and uses answers to discover CNN patterns corresponding to the newly labeled parts. In this way, our method gradually grows new branches and refines existing branches on the AOG to semanticize CNN representations. In experiments, our method exhibited a high learning efficiency. Our method used about 1/6-1/3 of the part annotations for training, but achieved similar or better part-localization performance than fast-RCNN methods.","['Quanshi Zhang', 'Ruiming Cao', 'Ying Nian Wu', 'Song-Chun Zhu']",,arXiv,2018,https://doi.org/10.48550/arXiv.1812.07996,Anomali
Exploiting Anti-monotonicity of Multi-label Evaluation Measures for Inducing Multi-label Rules,"Exploiting dependencies between labels is considered to be crucial for multi-label classification. Rules are able to expose label dependencies such as implications, subsumptions or exclusions in a human-comprehensible and interpretable manner. However, the induction of rules with multiple labels in the head is particularly challenging, as the number of label combinations which must be taken into account for each rule grows exponentially with the number of available labels. To overcome this limitation, algorithms for exhaustive ruleminingtypically use properties such as anti-monotonicity or decomposability in order to prune the search space. In the present paper, we examine whether commonly used multi-label evaluation metrics satisfy these properties and therefore are suited to prune the search space for multi-label heads.","['Michael Rapp', 'Eneldo Loza Mencía', 'Johannes Fürnkranz']",Proc. PAKDD (1) 2018: 29-42,arXiv,2018,https://doi.org/10.48550/arXiv.1812.06833,Anomali
Text data mining and data quality management for research information systems in the context of open data and open science,"In the implementation and use of research information systems (RIS) in scientific institutions,textdataminingand semantic technologies are a key technology for the meaningful use of large amounts of data. It is not the collection of data that is difficult, but the further processing and integration of the data in RIS. Data is usually not uniformly formatted and structured, such astextsand tables that cannot be linked. These include various source systems with their different data formats such as project and publication databases, CERIF and RCD data model, etc. Internal and external data sources continue to develop. On the one hand, they must be constantly synchronized and the results of the data links checked. On the other hand, thetextsmust be processed in natural language and certain information extracted. Usingtextdatamining, the quality of the metadata is analyzed and this identifies the entities and general keywords. So that the user is supported in the search for interesting research information. The information age makes it easier to store huge amounts of data and increase the number of documents on the internet, in institutions' intranets, in newswires and blogs is overwhelming. Search engines should help to specifically open up these sources of information and make them usable for administrative and research purposes. Against this backdrop, the aim of this paper is to provide an overview oftextdataminingtechniques and the management of successful data quality for RIS in the context of open data and open science in scientific institutions and libraries, as well as to provide ideas for their application. In particular, solutions for the RIS will be presented.","['Otmane Azeroual', 'Gunter Saake', 'Mohammad Abuosba', 'Joachim Schöpfel']","ICOA 2018 3e colloque international sur le libre acc{è}s, Nov 2018, Rabat, Morocco. 2018, Actes du 3e colloque international sur le libre acc{è}s. Le libre acc{è}s {à} la science : fondements, enjeux et dynamiques. https://icoa2018.sciencesconf.org/",arXiv,2018,https://doi.org/10.48550/arXiv.1812.04298,Anomali
Asynchronous Training of Word Embeddings for Large Text Corpora,"Word embeddings are a powerful approach for analyzing language and have been widely popular in numerous tasks in information retrieval andtextmining. Training embeddings over huge corpora is computationally expensive because the input is typically sequentially processed and parameters are synchronously updated. Distributed architectures for asynchronous training that have been proposed either focus on scaling vocabulary sizes and dimensionality or suffer from expensive synchronization latencies.
  In this paper, we propose a scalable approach to train word embeddings by partitioning the input space instead in order to scale to massivetextcorpora while not sacrificing the performance of the embeddings. Our training procedure does not involve any parameter synchronization except a final sub-model merge phase that typically executes in a few minutes. Our distributed training scales seamlessly to large corpus sizes and we get comparable and sometimes even up to 45% performance improvement in a variety of NLP benchmarks using models trained by our distributed procedure which requires $1/10$ of the time taken by the baseline approach. Finally we also show that we are robust to missing words in sub-models and are able to effectively reconstruct word representations.","['Avishek Anand', 'Megha Khosla', 'Jaspreet Singh', 'Jan-Hendrik Zab', 'Zijian Zhang']",,arXiv,2018,https://doi.org/10.48550/arXiv.1812.03825,Anomali
Generation of Synthetic Electronic Medical Record Text,"Machine learning (ML) and Natural Language Processing (NLP) have achieved remarkable success in many fields and have brought new opportunities and high expectation in the analyses of medical data. The most common type of medical data is the massive free-textelectronic medical records (EMR). It is widely regarded thatminingsuch massive data can bring up important information for improving medical practices as well as for possible new discoveries on complex diseases. However, the free EMRtextsare lacking consistent standards, rich of private information, and limited in availability. Also, as they are accumulated from everyday practices, it is often hard to have a balanced number of samples for the types of diseases under study. These problems hinder the development of ML and NLP methods for EMR data analysis. To tackle these problems, we developed a model to generate synthetictextof EMRs called MedicalTextGenerative Adversarial Network or mtGAN. It is based on the GAN framework and is trained by the REINFORCE algorithm. It takes disease features as inputs and generates synthetictextsas EMRs for the corresponding diseases. We evaluate the model from micro-level, macro-level and application-level on a Chinese EMRtextdataset. The results show that the method has a good capacity to fit real data and can generate realistic and diverse EMR samples. This provides a novel way to avoid potential leakage of patient privacy while still supply sufficient well-controlled cohort data for developing downstream ML and NLP methods. It can also be used as a data augmentation method to assist studies based on real EMR data.","['Jiaqi Guan', 'Runzhe Li', 'Sheng Yu', 'Xuegong Zhang']",,arXiv,2018,https://doi.org/10.48550/arXiv.1812.02793,Anomali
Distributed mining of time--faded heavy hitters,"We present \textsc{P2PTFHH} (Peer--to--Peer Time--Faded Heavy Hitters) which, to the best of our knowledge, is the first distributed algorithm forminingtime--faded heavy hitters on unstructured P2P networks. \textsc{P2PTFHH} is based on the \textsc{FDCMSS} (Forward Decay Count--Min Space-Saving) sequential algorithm, and efficiently exploits an averaging gossip protocol, by merging in each interaction the involved peers' underlying data structures. We formally prove the convergence and correctness properties of our distributed algorithm and show that it is fast and simple to implement. Extensive experimental results confirm that \textsc{P2PTFHH} retains the extreme accuracy and error bound provided by \textsc{FDCMSS} whilst showing excellent scalability. Our contributions are three-fold: (i) we prove that the averaging gossip protocol can be used jointly with our augmented sketch data structure forminingtime--faded heavy hitters; (ii) we prove the error bounds on frequency estimation; (iii) we experimentally prove that \textsc{P2PTFHH} is extremely accurate and fast, allowing near real time processing of large datasets.","['Marco Pulimeno', 'Italo Epicoco', 'Massimo Cafaro']",,arXiv,2018,https://doi.org/10.48550/arXiv.1812.01450,Anomali
A PMU-based Multivariate Model for Classifying Power System Events,"Real-time transient event identification is essential for power system situational awareness and protection. The increased penetration of Phasor Measurement Units (PMUs) enhance power system visualization and real time monitoring and control. However, a malicious false data injection attack on PMUs can provide wrong data that might prompt the operator to take incorrect actions which can eventually jeopardize system reliability. In this paper, a multivariate method based ontextminingis applied to detect false data and identify transient events by analyzing the attributes of each individual PMU time series and their relationship. It is shown that the proposed approach is efficient in detecting false data and identifying each transient event regardless of the system topology and loading condition as well as the coverage rate and placement of PMUs. The proposed method is tested on IEEE 30-bus system and the classification results are provided.","['Rui Ma', 'Sagnik Basumallik', 'Sara Eftekharnejad']",,arXiv,2018,https://doi.org/10.48550/arXiv.1812.00246,Anomali
Naive Dictionary On Musical Corpora: From Knowledge Representation To Pattern Recognition,"In this paper, we propose and develop the novel idea of treating musical sheets as literary documents in the traditionaltextanalytics parlance, to fully benefit from the vast amount of research already existing in statisticaltextminingand topic modelling. We specifically introduce the idea of representing any given piece of music as a collection of ""musical words"" that we codenamed ""muselets"", which are essentially musical words of various lengths. Given the novelty and therefore the extremely difficulty of properly forming a complete version of a dictionary of muselets, the present paper focuses on a simpler albeit naive version of the ultimate dictionary, which we refer to as a Naive Dictionary because of the fact that all the words are of the same length. We specifically herein construct a naive dictionary featuring a corpus made up of African American, Chinese, Japanese and Arabic music, on which we perform both topic modelling and pattern recognition. Although some of the results based on the Naive Dictionary are reasonably good, we anticipate phenomenal predictive performances once we get around to actually building a full scale complete version of our intended dictionary of muselets.","['Qiuyi Wu', 'Ernest Fokoue']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.12802,Anomali
A Concept-Centered Hypertext Approach to Case-Based Retrieval,"The goal of case-based retrieval is to assist physicians in the clinical decision making process, by finding relevant medical literature in large archives. We propose a research that aims at improving the effectiveness of case-based retrieval systems through the use of automatically created document-level semantic networks. The proposed research tackles different aspects of information systems and leverages the recent advancements in information extraction and relational learning to revisit and advance the core ideas of concept-centered hypertext models. We propose a two-step methodology that in the first step addresses the automatic creation of document-level semantic networks, then in the second step it designs methods that exploit such document representations to retrieve relevant cases from medical literature. For the automatic creation of documents' semantic networks, we design a combination of information extraction techniques and relational learning models.Miningconcepts and relations fromtext, information extraction techniques represent the core of the document-level semantic networks' building process. On the other hand, relational learning models have the task of enriching the graph with additional connections that have not been detected by information extraction algorithms and strengthening the confidence score of extracted relations. For the retrieval of relevant medical literature, we investigate methods that are capable of comparing the documents' semantic networks in terms of structure and semantics. The automatic extraction of semantic relations from documents, and their centrality in the creation of the documents' semantic networks, represent our attempt to go one step further than previous graph-based approaches.",['Stefano Marchesin'],,arXiv,2018,https://doi.org/10.48550/arXiv.1811.11133,Anomali
Sentiment Analysis of Financial News Articles using Performance Indicators,"Miningfinancialtextdocuments and understanding the sentiments of individual investors, institutions and markets is an important and challenging problem in the literature. Current approaches tominesentiments from financialtextslargely rely on domain specific dictionaries. However, dictionary based methods often fail to accurately predict the polarity of financialtexts. This paper aims to improve the state-of-the-art and introduces a novel sentiment analysis approach that employs the concept of financial and non-financial performance indicators. It presents an association ruleminingbased hierarchical sentiment classifier model to predict the polarity of financialtextsas positive, neutral or negative. The performance of the proposed model is evaluated on a benchmark financial dataset. The model is also compared against other state-of-the-art dictionary and machine learning based approaches and the results are found to be quite promising. The novel use of performance indicators for financial sentiment analysis offers interesting and useful insights.",['Srikumar Krishnamoorthy'],,arXiv,2018,https://doi.org/10.48550/arXiv.1811.11008,Anomali
Scalable graph-based individual named entity identification,"Named entity discovery (NED) is an important information retrieval problem that can be decomposed into two sub-problems. The first sub-problem, named entity recognition (NER), aims to tag pre-defined sets of words in a vocabulary (called ""named entities"": names, places, locations, ...) when they appear in natural language. The second subproblem, named entity linking/identification (NEL), considers these entity mentions as queries to be identified in a pre-existing database. In this paper, we consider the NEL problem, and assume a set of queries (or mentions) that have to be identified within a knowledge base. This knowledge base is represented by atextdatabase paired with a semantic graph. We present state-of-the-art methods in NEL, and propose a 2-step method for individual identification of named entities. Our approach is well-motivated by the limitations brought by recent deep learning approaches that lack interpratability, and require lots of parameter tuning along with large volume of annotated data.
  First of all, we propose a filtering algorithm designed with information retrieval andtextminingtechniques, aiming to maximize precision at K (typically for 5 <= K <=20). Then, we introduce two graph-based methods for named entity identification to maximize precision at 1 by re-ranking the remaining top entity candidates. The first identification method is using parametrized graphmining, and the second similarity with graph kernels. Our approach capitalizes on a fine-grained classification of entities from annotated web data. We present our algorithms in details, and show experimentally on standard datasets (NIST TAC-KBP, CONLL/AIDA) their performance in terms of precision are better than any graph-based method reported, and competitive with state-of-the-art systems. Finally, we conclude on the advantages of our graph-based approach compared to recent deep learning methods.","['Sammy Khalife', 'Michalis Vazirgiannis']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.10547,Anomali
Creating a contemporary corpus of similes in Serbian by using natural language processing,"Simile is a figure of speech that compares two things through the use of connection words, but where comparison is not intended to be taken literally. They are often used in everyday communication, but they are also a part of linguistic cultural heritage. In this paper we present a methodology for semi-automated collection of similes from the World Wide Web usingtextminingand machine learning techniques. We expanded an existing corpus by collecting 442 similes from the internet and adding them to the existing corpus collected by Vuk Stefanovic Karadzic that contained 333 similes. We, also, introduce crowdsourcing to the collection of figures of speech, which helped us to build corpus containing 787 unique similes.","['Nikola Milosevic', 'Goran Nenadic']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.10422,Anomali
A Rule-based Kurdish Text Transliteration System,"In this article, we present a rule-based approach for transliterating two mostly used orthographies in Sorani Kurdish. Our work consists of detecting a character in a word by removing the possible ambiguities and mapping it into the target orthography. We describe different challenges in Kurdishtextminingand propose novel ideas concerning the transliteration task for Sorani Kurdish. Our transliteration system, named Wergor, achieves 82.79% overall precision and more than 99% in detecting the double-usage characters. We also present a manually transliterated corpus for Kurdish.",['Sina Ahmadi'],,arXiv,2018,https://doi.org/10.48550/arXiv.1811.10278,Anomali
Estimation of Inter-Sentiment Correlations Employing Deep Neural Network Models,"This paper focuses on sentimentminingand sentiment correlation analysis of web events. Although neural network models have contributed a lot tominingtextinformation, little attention is paid to analysis of the inter-sentiment correlations. This paper fills the gap between sentiment calculation and inter-sentiment correlations. In this paper, the social emotion is divided into six categories: love, joy, anger, sadness, fear, and surprise. Two deep neural network models are presented for sentiment calculation. Three datasets - the titles, the bodies, the comments of news articles - are collected, covering both objective and subjectivetextsin varying lengths (long and short). From each dataset, three kinds of features are extracted: explicit expression, implicit expression, and alphabet characters. The performance of the two models are analyzed, with respect to each of the three kinds of the features. There is controversial phenomenon on the interpretation of anger (fn) and love (gd). In subjectivetext, other emotions are easily to be considered as anger. By contrast, in objective news bodies and titles, it is easy to regardtextas caused love (gd). It means, journalist may want to arouse emotion love by writing news, but cause anger after the news is published. This result reflects the sentiment complexity and unpredictability.","['Xinzhi Wang', 'Shengcheng Yuan', 'Hui Zhang', 'Yi Liu']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.09755,Anomali
"Autonomous Extraction of a Hierarchical Structure of Tasks in Reinforcement Learning, A Sequential Associate Rule Mining Approach","Reinforcement learning (RL) techniques, while often powerful, can suffer from slow learning speeds, particularly in high dimensional spaces. Decomposition of tasks into a hierarchical structure holds the potential to significantly speed up learning, generalization, and transfer learning. However, the current task decomposition techniques often rely on high-level knowledge provided by an expert (e.g. using dynamic Bayesian networks) to extract a hierarchical task structure; which is not necessarily available in autonomous systems. In this paper, we propose a novel method based on Sequential Association RuleMiningthat can extract Hierarchical Structure of Tasks in Reinforcement Learning (SARM-HSTRL) in an autonomous manner for both Markov decision processes (MDPs) and factored MDPs. The proposed method leverages association ruleminingto discover the causal and temporal relationships among states in different trajectories, and extracts a task hierarchy that captures these relationships among sub-goals as termination conditions of different sub-tasks. We prove that the extracted hierarchical policy offers a hierarchically optimal policy in MDPs and factored MDPs. It should be noted that SARM-HSTRL extracts this hierarchical optimal policy without having dynamic Bayesian networks in scenarios with a single task trajectory and also with multiple tasks' trajectories. Furthermore, it has been theoretically and empirically shown that the extracted hierarchical task structure is consistent with trajectories and provides the most efficient, reliable, and compact structure under appropriate assumptions. The numerical results compare the performance of the proposed SARM-HSTRL method with conventional HRL algorithms in terms of the accuracy in detecting the sub-goals, the validity of the extracted hierarchies, and the speed of learning in several testbeds.","['Behzad Ghazanfari', 'Fatemeh Afghah', 'Matthew E. Taylor']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.08275,Anomali
A Big data analytical framework for portfolio optimization,"With the advent of Web 2.0, various types of data are being produced every day. This has led to the revolution of big data. Huge amount of structured and unstructured data are produced in financial markets. Processing these data could help an investor to make an informed investment decision. In this paper, a framework has been developed to incorporate both structured and unstructured data for portfolio optimization. Portfolio optimization consists of three processes: Asset selection, Asset weighting and Asset management. This framework proposes to achieve the first two processes using a 5-stage methodology. The stages include shortlisting stocks using Data Envelopment Analysis (DEA), incorporation of the qualitative factors usingtextmining, stock clustering, stock ranking and optimizing the portfolio using heuristics. This framework would help the investors to select appropriate assets to make portfolio, invest in them to minimize the risk and maximize the return and monitor their performance.","['Dhanya Jothimani', 'Ravi Shankar', 'Surendra S. Yadav']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.07188,Anomali
Text Assisted Insight Ranking Using Context-Aware Memory Network,"Extracting valuable facts or informative summaries from multi-dimensional tables, i.e. insightmining, is an important task in data analysis and business intelligence. However, ranking the importance of insights remains a challenging and unexplored task. The main challenge is that explicitly scoring an insight or giving it a rank requires a thorough understanding of the tables and costs a lot of manual efforts, which leads to the lack of available training data for the insight ranking problem. In this paper, we propose an insight ranking model that consists of two parts: A neural ranking model explores the data characteristics, such as the header semantics and the data statistical features, and a memory network model introduces table structure and context information into the ranking process. We also build a dataset withtextassistance. Experimental results show that our approach largely improves the ranking precision as reported in multi evaluation metrics.","['Qi Zeng', 'Liangchen Luo', 'Wenhao Huang', 'Yang Tang']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.05563,Anomali
Optimizing Taxi Carpool Policies via Reinforcement Learning and Spatio-Temporal Mining,"In this paper, we develop a reinforcement learning (RL) based system to learn an effective policy for carpooling that maximizes transportation efficiency so that fewer cars are required to fulfill the given amount of trip demand. For this purpose, first, we develop a deep neural network model, called ST-NN (Spatio-Temporal Neural Network), to predict taxi trip time from the raw GPS trip data. Secondly, we develop a carpooling simulation environment for RL training, with the output of ST-NN and using the NYC taxi trip dataset. In order to maximize transportation efficiency and minimize traffic congestion, we choose the effective distance covered by the driver on a carpool trip as the reward. Therefore, the more effective distance a driver achieves over a trip (i.e. to satisfy more trip demand) the higher the efficiency and the less will be the traffic congestion. We compared the performance of RL learned policy to a fixed policy (which always accepts carpool) as a baseline and obtained promising results that are interpretable and demonstrate the advantage of our RL approach. We also compare the performance of ST-NN to that of state-of-the-art travel time estimation methods and observe that ST-NN significantly improves the prediction performance and is more robust to outliers.","['Ishan Jindal', 'Zhiwei Qin', 'Xuewen Chen', 'Matthew Nokleby', 'Jieping Ye']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.04345,Anomali
Spatter: A Tool for Evaluating Gather / Scatter Performance,"This paper describes a new benchmark tool, Spatter, for assessing memory system architectures in the context of a specific category of indexed accesses known as gather and scatter. These types of operations are increasingly used to express sparse and irregular data access patterns, and they have widespread utility in many modern HPC applications including scientific simulations, dataminingand analysis computations, and graph processing. However, many traditional benchmarking tools like STREAM, STRIDE, and GUPS focus on characterizing only uniform stride or fully random accesses despite evidence that modern applications use varied sets of more complex access patterns.
  Spatter is an open-source benchmark that provides a tunable and configurable framework to benchmark a variety of indexed access patterns, including variations of gather/scatter that are seen in HPC mini-apps evaluated in this work. The design of Spatter includes tunable backends for OpenMP and CUDA, and experiments show how it can be used to evaluate 1) uniform access patterns for CPU and GPU, 2) prefetching regimes for gather/scatter, 3) compiler implementations of vectorization for gather/scatter, and 4) trace-driven ""proxy patterns"" that reflect the patterns found in multiple applications. The results from Spatter experiments show that GPUs typically outperform CPUs for these operations, and that Spatter can better represent the performance of some cache-dependent mini-apps than traditional STREAM bandwidth measurements.","['Patrick Lavin', 'Jeffrey Young', 'Jason Riedy', 'Richard Vuduc', 'Aaron Vose', 'Dan Ernst']",,arXiv,2020,https://doi.org/10.48550/arXiv.1811.03743,Anomali
Towards a Near Universal Time Series Data Mining Tool: Introducing the Matrix Profile,"The last decade has seen a flurry of research on all-pairs-similarity-search (or, self-join) fortext, DNA, and a handful of other datatypes, and these systems have been applied to many diverse dataminingproblems. Surprisingly, however, little progress has been made on addressing this problem for time series subsequences. In this thesis, we have introduced a near universal time series dataminingtool called matrix profile which solves the all-pairs-similarity-search problem and caches the output in an easy-to-access fashion. The proposed algorithm is not only parameter-free, exact and scalable, but also applicable for both single and multidimensional time series. By building time series dataminingmethods on top of matrix profile, many time series dataminingtasks (e.g., motif discovery, discord discovery, shapelet discovery, semantic segmentation, and clustering) can be efficiently solved. Because the same matrix profile can be shared by a diverse set of time series dataminingmethods, matrix profile is versatile and computed-once-use-many-times data structure. We demonstrate the utility of matrix profile for many time series dataminingproblems, including motif discovery, discord discovery, weakly labeled time series classification, and representation learning on domains as diverse as seismology, entomology, music processing, bioinformatics, human activity monitoring, electrical power-demand monitoring, and medicine. We hope the matrix profile is not the end but the beginning of many more time series dataminingprojects.",['Chin-Chia Michael Yeh'],,arXiv,2020,https://doi.org/10.48550/arXiv.1811.03064,Anomali
DAPPER: Scaling Dynamic Author Persona Topic Model to Billion Word Corpora,"Extracting common narratives from multi-author dynamictextcorpora requires complex models, such as the Dynamic Author Persona (DAP) topic model. However, such models are complex and can struggle to scale to large corpora, often because of challenging non-conjugate terms. To overcome such challenges, in this paper we adapt new ideas in approximate inference to the DAP model, resulting in the DAP Performed Exceedingly Rapidly (DAPPER) topic model. Specifically, we develop Conjugate-Computation Variational Inference (CVI) based variational Expectation-Maximization (EM) for learning the model, yielding fast, closed form updates for each document, replacing iterative optimization in earlier work. Our results show significant improvements in model fit and training time without needing to compromise the model's temporal structure or the application of Regularized Variation Inference (RVI). We demonstrate the scalability and effectiveness of the DAPPER model by extracting health journeys from the CaringBridge corpus --- a collection of 9 million journals written by 200,000 authors during health crises.","['Robert Giaquinto', 'Arindam Banerjee']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.01931,Anomali
Representation Learning by Reconstructing Neighborhoods,"Since its introduction, unsupervised representation learning has attracted a lot of attention from the research community, as it is demonstrated to be highly effective and easy-to-apply in tasks such as dimension reduction, clustering, visualization, information retrieval, and semi-supervised learning. In this work, we propose a novel unsupervised representation learning framework called neighbor-encoder, in which domain knowledge can be easily incorporated into the learning process without modifying the general encoder-decoder architecture of the classic autoencoder.In contrast to autoencoder, which reconstructs the input data itself, neighbor-encoder reconstructs the input data's neighbors. As the proposed representation learning problem is essentially a neighbor reconstruction problem, domain knowledge can be easily incorporated in the form of an appropriate definition of similarity between objects. Based on that observation, our framework can leverage any off-the-shelf similarity search algorithms or side information to find the neighbor of an input object. Applications of other algorithms (e.g., association rulemining) in our framework are also possible, given that the appropriate definition of neighbor can vary in different contexts. We have demonstrated the effectiveness of our framework in many diverse domains, including images,text, and time series, and for various dataminingtasks including classification, clustering, and visualization. Experimental results show that neighbor-encoder not only outperforms autoencoder in most of the scenarios we consider, but also achieves the state-of-the-art performance ontextdocument clustering.","['Chin-Chia Michael Yeh', 'Yan Zhu', 'Evangelos E. Papalexakis', 'Abdullah Mueen', 'Eamonn Keogh']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.01557,Anomali
Unsupervised Identification of Study Descriptors in Toxicology Research: An Experimental Study,"Identifying and extracting data elements such as study descriptors in publication fulltextsis a critical yet manual and labor-intensive step required in a number of tasks. In this paper we address the question of identifying data elements in an unsupervised manner. Specifically, provided a set of criteria describing specific study parameters, such as species, route of administration, and dosing regimen, we develop an unsupervised approach to identifytextsegments (sentences) relevant to the criteria. A binary classifier trained to identify publications that met the criteria performs better when trained on the candidate sentences than when trained on sentences randomly picked from thetext, supporting the intuition that our method is able to accurately identify study descriptors.","['Drahomira Herrmannova', 'Steven R. Young', 'Robert M. Patton', 'Christopher G. Stahl', 'Nicole C. Kleinstreuer', 'Mary S. Wolfe']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.01183,Anomali
Helping each Other: A Framework for Customer-to-Customer Suggestion Mining using a Semi-supervised Deep Neural Network,"Suggestionminingis increasingly becoming an important task along with sentiment analysis. In today's cyberspace world, people not only express their sentiments and dispositions towards some entities or services, but they also spend considerable time sharing their experiences and advice to fellow customers and the product/service providers with two-fold agenda: helping fellow customers who are likely to share a similar experience, and motivating the producer to bring specific changes in their offerings which would be more appreciated by the customers. In our current work, we propose a hybrid deep learning model to identify whether a reviewtextcontains any suggestion. The model employs semi-supervised learning to leverage the useful information from the large amount of unlabeled data. We evaluate the performance of our proposed model on a benchmark customer review dataset, comprising of the reviews of Hotel and Electronics domains. Our proposed approach shows the F-scores of 65.6% and 65.5% for the Hotel and Electronics review datasets, respectively. These performances are significantly better compared to the existing state-of-the-art system.","['Hitesh Golchha', 'Deepak Gupta', 'Asif Ekbal', 'Pushpak Bhattacharyya']",,arXiv,2018,https://doi.org/10.48550/arXiv.1811.00379,Anomali
Discovering Entities with Just a Little Help from You,"Linking entities like people, organizations, books, music groups and their songs intextto knowledge bases (KBs) is a fundamental task for many downstream search andminingapplications. Achieving high disambiguation accuracy crucially depends on a rich and holistic representation of the entities in the KB. For popular entities, such a representation can be easilyminedfrom Wikipedia, and many current entity disambiguation and linking methods make use of this fact. However, Wikipedia does not contain long-tail entities that only few people are interested in, and also at times lags behind until newly emerging entities are added. For such entities,mininga suitable representation in a fully automated fashion is very difficult, resulting in poor linking accuracy.
  What can automatically bemined, though, is a high-quality representation given the context of a new entity occurring in anytext. Due to the lack of knowledge about the entity, no method can retrieve these occurrences automatically with high precision, resulting in a chicken-egg problem. To address this, our approach automatically generates candidate occurrences of entities, prompting the user for feedback to decide if the occurrence refers to the actual entity in question. This feedback gradually improves the knowledge and allows our methods to provide better candidate suggestions to keep the user engaged. We propose novel human-in-the-loop retrieval methods for generating candidates based on gradient interleaving of diversification and textual relevance approaches.
  We conducted extensive experiments on the FACC dataset, showing that our approaches convincingly outperform carefully selected baselines in both intrinsic and extrinsic measures while keeping users engaged.","['Jaspreet Singh', 'Johannes Hoffart', 'Avishek Anand']",,arXiv,2018,https://doi.org/10.48550/arXiv.1810.10252,Anomali
Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding,"Developers increasingly rely ontextmatching tools to analyze the relation between natural language words and APIs. However, semantic gaps, namely textual mismatches between words and APIs, negatively affect these tools. Previous studies have transformed words or APIs into low-dimensional vectors for matching; however, inaccurate results were obtained due to the failure of modeling words and APIs simultaneously. To resolve this problem, two main challenges are to be addressed: the acquisition of massive words and APIs forminingand the alignment of words and APIs for modeling. Therefore, this study proposes Word2API to effectively estimate relatedness of words and APIs. Word2API collects millions of commonly used words and APIs from code repositories to address the acquisition challenge. Then, a shuffling strategy is used to transform related words and APIs into tuples to address the alignment challenge. Using these tuples, Word2API models words and APIs simultaneously. Word2API outperforms baselines by 10%-49.6% of relatedness estimation in terms of precision and NDCG. Word2API is also effective on solving typical software tasks, e.g., query expansion and API documents linking. A simple system with Word2API-expanded queries recommends up to 21.4% more related APIs for developers. Meanwhile, Word2API improves comparison algorithms by 7.9%-17.4% in linking questions in Question&Answer communities to API documents.","['Xiaochen Li', 'He Jiang', 'Yasutaka Kamei', 'Xin Chen']",,arXiv,2018,https://doi.org/10.48550/arXiv.1810.09723,Anomali
Biomedical Document Clustering and Visualization based on the Concepts of Diseases,"Document clustering is atextminingtechnique used to provide better document search and browsing in digital libraries or online corpora. A lot of research has been done on biomedical document clustering that is based on using existing ontology. But, associations and co-occurrences of the medical concepts are not well represented by using ontology. In this research, a vector representation of concepts of diseases and similarity measurement between concepts are proposed. They identify the closest concepts of diseases in the context of a corpus. Each document is represented by using the vector space model. A weight scheme is proposed to consider both local content and associations between concepts. A Self-Organizing Map is used as document clustering algorithm. The vector projection and visualization features of SOM enable visualization and analysis of the clusters distributions and relationships on the two dimensional space. The experimental results show that the proposed document clustering framework generates meaningful clusters and facilitate visualization of the clusters based on the concepts of diseases.","['Setu Shah', 'Xiao Luo']",,arXiv,2018,https://doi.org/10.48550/arXiv.1810.09597,Anomali
BioSentVec: creating sentence embeddings for biomedical texts,"Sentence embeddings have become an essential part of today's natural language processing (NLP) systems, especially together advanced deep learning methods. Although pre-trained sentence encoders are available in the general domain, none exists for biomedicaltextsto date. In this work, we introduce BioSentVec: the first open set of sentence embeddings trained with over 30 million documents from both scholarly articles in PubMed and clinical notes in the MIMIC-III Clinical Database. We evaluate BioSentVec embeddings in two sentence pair similarity tasks in differenttextgenres. Our benchmarking results demonstrate that the BioSentVec embeddings can better capture sentence semantics compared to the other competitive alternatives and achieve state-of-the-art performance in both tasks. We expect BioSentVec to facilitate the research and development in biomedicaltextminingand to complement the existing resources in biomedical word embeddings. BioSentVec is publicly available at https://github.com/ncbi-nlp/BioSentVec","['Qingyu Chen', 'Yifan Peng', 'Zhiyong Lu']",,arXiv,2020,https://doi.org/10.48550/arXiv.1810.09302,Anomali
Compositional Coding Capsule Network with K-Means Routing for Text Classification,"Textclassification is a challenging problem which aims to identify the category oftexts. In the process of training, word embeddings occupy a large part of parameters. Under the limitation of limited computing resources, it indirectly limits the ability of subsequent network designs. In order to reduce the number of parameters, the compositional coding mechanism has been proposed recently. Based on this, this paper further explores compositional coding and proposes a compositional weighted coding method. And we apply capsule network to model the relationship between word embeddings, a new routing algorithm, which is based on k-means clustering theory, is proposed to fullyminethe relationship between word embeddings. Combined with our compositional weighted coding method and the routing algorithm, we design a neural network fortextclassification. Experiments conducted on eight challengingtextclassification datasets show that the proposed method achieves competitive accuracy compared to the state-of-the-art approach with significantly fewer parameters.","['Hao Ren', 'Hong Lu']",,arXiv,2022,https://doi.org/10.48550/arXiv.1810.09177,Anomali
Responsible team players wanted: an analysis of soft skill requirements in job advertisements,"During the past decades the importance of soft skills for labour market outcomes has grown substantially. This carries implications for labour market inequality, since previous research shows that soft skills are not valued equally across race and gender. This work explores the role of soft skills in job advertisements by drawing on methods from computational science as well as on theoretical and empirical insights from economics, sociology and psychology. We present a semi-automatic approach based on crowdsourcing andtextminingfor extracting a list of soft skills. We find that soft skills are a crucial component of job ads, especially of low-paid jobs and jobs in female-dominated professions. Our work shows that soft skills can serve as partial predictors of the gender composition in job categories and that not all soft skills receive equal wage returns at the labour market. Especially ""female"" skills are frequently associated with wage penalties. Our results expand the growing literature on the association of soft skills on wage inequality and highlight their importance for occupational gender segregation at labour markets.","['Federica Calanca', 'Luiza Sayfullina', 'Lara Minkus', 'Claudia Wagner', 'Eric Malmi']",,arXiv,2019,https://doi.org/10.48550/arXiv.1810.07781,Anomali
Architecture of Text Mining Application in Analyzing Public Sentiments of West Java Governor Election using Naive Bayes Classification,"The selection of West Java governor is one event that seizes the attention of the public is no exception to social media users. Public opinion on a prospective regional leader can help predict electability and tendency of voters. Data that can be used by the opinionminingprocess can be obtained from Twitter. Because the data is very varied form and very unstructured, it must be managed and uninformed using data pre-processing techniques into semi-structured data. This semi-structured information is followed by a classification stage to categorize the opinion into negative or positive opinions. The research methodology uses a literature study where the research will examine previous research on a similar topic. The purpose of this study is to find the right architecture to develop it into the application of twitter opinionminingto know public sentiments toward the election of the governor of west java. The result of this research is that Twitter opinionminingis part oftextminingwhere opinions in Twitter if they want to be classified, must go through the preprocessingtextstage first. The preprocessing step required from twitter data is cleansing, case folding, POS Tagging and stemming. The resultingtextminingarchitecture is an architecture that can be used fortextminingresearch with different topics.","['Suryanto Nugroho', 'Prihandoko']",,arXiv,2018,https://doi.org/10.48550/arXiv.1810.07767,Anomali
Text-based Sentiment Analysis and Music Emotion Recognition,"Sentiment polarity of tweets, blog posts or product reviews has become highly attractive and is utilized in recommender systems, market predictions, business intelligence and more. Deep learning techniques are becoming top performers on analyzing suchtexts. There are however several problems that need to be solved for efficient use of deep neural networks ontextminingandtextpolarity analysis. First, deep neural networks need to be fed with data sets that are big in size as well as properly labeled. Second, there are various uncertainties regarding the use of word embedding vectors: should they be generated from the same data set that is used to train the model or it is better to source them from big and popular collections? Third, to simplify model creation it is convenient to have generic neural network architectures that are effective and can adapt to varioustexts, encapsulating much of design complexity. This thesis addresses the above problems to provide methodological and practical insights for utilizing neural networks on sentiment analysis oftextsand achieving state of the art results. Regarding the first problem, the effectiveness of various crowdsourcing alternatives is explored and two medium-sized and emotion-labeled song data sets are created utilizing social tags. To address the second problem, a series of experiments with largetextcollections of various contents and domains were conducted, trying word embeddings of various parameters. Regarding the third problem, a series of experiments involving convolution and max-pooling neural layers were conducted. Combining convolutions of words, bigrams, and trigrams with regional max-pooling layers in a couple of stacks produced the best results. The derived architecture achieves competitive performance on sentiment polarity analysis of movie, business and product reviews.",['Erion Çano'],,arXiv,2018,https://doi.org/10.48550/arXiv.1810.03031,Anomali
Sifaka: Text Mining Above a Search API,"Textminingand analytics software has become popular, but little attention has been paid to the software architectures of such systems. Often they are built from scratch using special-purpose software and data structures, which increases their cost and complexity. This demo paper describes Sifaka, a new open-sourcetextminingapplication constructed above a standard search engine index using existing application programmer interface (API) calls. Indexing integrates popular annotation software libraries to augment the full-textindex with noun phrase and named-entities; n-grams are also provided. Sifaka enables a person to quickly explore and analyze largetextcollections using search, frequency analysis, and co-occurrence analysis; and import existing document labels or interactively construct document sets that are positive or negative examples of new concepts, perform feature selection, and export feature vectors compatible with popular machine learning software. Sifaka demonstrates that search engines are good platforms fortextminingapplications while also making common IRtextminingcapabilities accessible to researchers in disciplines where programming skills are less common.","['Cameron VandenBerg', 'Jamie Callan']",,arXiv,2018,https://doi.org/10.48550/arXiv.1810.02907,Anomali
Clust-LDA: Joint Model for Text Mining and Author Group Inference,"Social media corpora pose unique challenges and opportunities, including typically short document lengths and rich meta-data such as author characteristics and relationships. This creates great potential for systematic analysis of the enormous body of the users and thus provides implications for industrial strategies such as targeted marketing. Here we propose a novel and statistically principled method, clust-LDA, which incorporates authorship structure into the topical modeling, thus accomplishing the task of the topical inferences across documents on the basis of authorship and, simultaneously, the identification of groupings between authors. We develop an inference procedure for clust-LDA and demonstrate its performance on simulated data, showing that clust-LDA out-performs the ""vanilla"" LDA on the topic identification task where authors exhibit distinctive topical preference. We also showcase the empirical performance of clust-LDA based on a real-world social media dataset from Reddit.","['Shaoyang Ning', 'Xi Qu', 'Victor Cai', 'Nathan Sanders']",,arXiv,2018,https://doi.org/10.48550/arXiv.1810.02717,Anomali
Deep Factor Model,"We propose to represent a return model and risk model in a unified manner with deep learning, which is a representative model that can express a nonlinear relationship. Although deep learning performs quite well, it has significant disadvantages such as a lack of transparency and limitations to the interpretability of the prediction. This is prone to practical problems in terms of accountability. Thus, we construct a multifactor model by using interpretable deep learning. We implement deep learning as a return model to predict stock returns with various factors. Then, we present the application of layer-wise relevance propagation (LRP) to decompose attributes of the predicted return as a risk model. By applying LRP to an individual stock or a portfolio basis, we can determine which factor contributes to prediction. We call this model a deep factor model. We then perform an empirical analysis on the Japanese stock market and show that our deep factor model has better predictive capability than the traditional linear model or other machine learning methods. In addition , we illustrate which factor contributes to prediction.","['Kei Nakagawa', 'Takumi Uchida', 'Tomohisa Aoshima']",,arXiv,2018,https://doi.org/10.48550/arXiv.1810.01278,Anomali
"Knowledge extraction, modeling and formalization: EEG case study","Formal Concept Analysis (FCA) is a well-established method for data analysis which finds many applications in datamining. Its extension on complex data representation formats brought a wave of new applications to the problems such as gene expressionmining, prediction of toxicity of chemical compounds or clustering of sequences in process event logs. Insipired from this work our research inherits their model and designs an experiment forminingelectroencephalographic recordings for patterns of sleep spindles. The contribution of this paper lies in the specification of desritizition procedure and the architecture of FCA experiment. We also provide some reflection on the related research papers.","['Dmitry Morozov', 'Mario Lezoche', 'Hervé Panetto']",,arXiv,2018,https://doi.org/10.48550/arXiv.1809.09955,Anomali
Recognizing Film Entities in Podcasts,"In this paper, we propose a Named Entity Recognition (NER) system to identify film titles in podcast audio. Taking inspiration from NER systems for noisytextin social media, we implement a two-stage approach that is robust to computer transcription errors and does not require significant computational expense to accommodate new film titles/releases. Evaluating on a diverse set of podcasts, we demonstrate more than a 20% increase in F1 score across three baseline approaches when combining fuzzy-matching with a linear model aware of film-specific metadata.","['Ahmet Salih Gundogdu', 'Arjun Sanghvi', 'Keith Harrigian']",,arXiv,2018,https://doi.org/10.48550/arXiv.1809.08711,Anomali
CollaboNet: collaboration of deep neural networks for biomedical named entity recognition,"Background: Finding biomedical named entities is one of the most essential tasks in biomedicaltextmining. Recently, deep learning-based approaches have been applied to biomedical named entity recognition (BioNER) and showed promising results. However, as deep learning approaches need an abundant amount of training data, a lack of data can hinder performance. BioNER datasets are scarce resources and each dataset covers only a small subset of entity types. Furthermore, many bio entities are polysemous, which is one of the major obstacles in named entity recognition. Results: To address the lack of data and the entity type misclassification problem, we propose CollaboNet which utilizes a combination of multiple NER models. In CollaboNet, models trained on a different dataset are connected to each other so that a target model obtains information from other collaborator models to reduce false positives. Every model is an expert on their target entity type and takes turns serving as a target and a collaborator model during training time. The experimental results show that CollaboNet can be used to greatly reduce the number of false positives and misclassified entities including polysemous words. CollaboNet achieved state-of-the-art performance in terms of precision, recall and F1 score. Conclusions: We demonstrated the benefits of combining multiple models for BioNER. Our model has successfully reduced the number of misclassified entities and improved the performance by leveraging multiple datasets annotated for different entity types. Given the state-of-the-art performance of our model, we believe that CollaboNet can improve the accuracy of downstream biomedicaltextminingapplications such as bio-entity relation extraction.","['Wonjin Yoon', 'Chan Ho So', 'Jinhyuk Lee', 'Jaewoo Kang']","BMC Bioinformatics 2019, 20(Suppl 10):249",arXiv,2019,https://doi.org/10.48550/arXiv.1809.07950,Anomali
Clustering students' open-ended questionnaire answers,"Open responses form a rich but underused source of information in educational dataminingand intelligent tutoring systems. One of the major obstacles is the difficulty of clustering shorttextsautomatically. In this paper, we investigate the problem of clustering free-formed questionnaire answers. We present comparative experiments on clustering ten sets of open responses from course feedback queries in English and Finnish. We also evaluate how well the main topics could be extracted from clusterings with the HITS algorithm. The main result is that, for English data, affinity propagation performed well despite frequent outliers and considerable overlapping between real clusters. However, for Finnish data, the performance was poorer and none of the methods clearly outperformed the others. Similarly, topic extraction was very successful for the English data but only satisfactory for the Finnish data. The most interesting discovery was that stemming could actually deteriorate the clustering quality significantly.","['Wilhelmiina Hämäläinen', 'Mike Joy', 'Florian Berger', 'Sami Huttunen']",,arXiv,2018,https://doi.org/10.48550/arXiv.1809.07306,Anomali
Supervised Machine Learning for Extractive Query Based Summarisation of Biomedical Data,"The automation oftextsummarisation of biomedical publications is a pressing need due to the plethora of information available on-line. This paper explores the impact of several supervised machine learning approaches for extracting multi-document summaries for given queries. In particular, we compare classification and regression approaches for query-based extractive summarisation using data provided by the BioASQ Challenge. We tackled the problem of annotating sentences for training classification systems and show that a simple annotation approach outperforms regression-based summarisation.","['Mandeep Kaur', 'Diego Mollá']",W18-5604,arXiv,2018,https://doi.org/10.48550/arXiv.1809.05268,Anomali
Automatic Catchphrase Extraction from Legal Case Documents via Scoring using Deep Neural Networks,"In this paper, we present a method of automatic catchphrase extracting from legal case documents. We utilize deep neural networks for constructing scoring model of our extraction system. We achieve comparable performance with systems using corpus-wide and citation information which we do not use in our system.","['Vu Tran', 'Minh Le Nguyen', 'Ken Satoh']",,arXiv,2018,https://doi.org/10.48550/arXiv.1809.05219,Anomali
Learning to Summarize Radiology Findings,"The Impression section of a radiology report summarizes crucial radiology findings in natural language and plays a central role in communicating these findings to physicians. However, the process of generating impressions by summarizing findings is time-consuming for radiologists and prone to errors. We propose to automate the generation of radiology impressions with neural sequence-to-sequence learning. We further propose a customized neural model for this task which learns to encode the study background information and use this information to guide the decoding process. On a large dataset of radiology reports collected from actual hospital studies, our model outperforms existing non-neural and neural baselines under the ROUGE metrics. In a blind experiment, a board-certified radiologist indicated that 67% of sampled system summaries are at least as good as the corresponding human-written summaries, suggesting significant clinical validity. To our knowledge our work represents the first attempt in this direction.","['Yuhao Zhang', 'Daisy Yi Ding', 'Tianpei Qian', 'Christopher D. Manning', 'Curtis P. Langlotz']",,arXiv,2018,https://doi.org/10.48550/arXiv.1809.04698,Anomali
Deep learning for time series classification: a review,"Time Series Classification (TSC) is an important and challenging problem in datamining. With the increase of time series data availability, hundreds of TSC algorithms have been proposed. Among these methods, only a few have considered Deep Neural Networks (DNNs) to perform this task. This is surprising as deep learning has seen very successful applications in the last years. DNNs have indeed revolutionized the field of computer vision especially with the advent of novel deeper architectures such as Residual and Convolutional Neural Networks. Apart from images, sequential data such astextand audio can also be processed with DNNs to reach state-of-the-art performance for document classification and speech recognition. In this article, we study the current state-of-the-art performance of deep learning algorithms for TSC by presenting an empirical study of the most recent DNN architectures for TSC. We give an overview of the most successful deep learning applications in various time series domains under a unified taxonomy of DNNs for TSC. We also provide an open source deep learning framework to the TSC community where we implemented each of the compared approaches and evaluated them on a univariate TSC benchmark (the UCR/UEA archive) and 12 multivariate time series datasets. By training 8,730 deep learning models on 97 time series datasets, we propose the most exhaustive study of DNNs for TSC to date.","['Hassan Ismail Fawaz', 'Germain Forestier', 'Jonathan Weber', 'Lhassane Idoumghar', 'Pierre-Alain Muller']",,arXiv,2019,https://doi.org/10.48550/arXiv.1809.04356,Anomali
"SOTorrent: Studying the Origin, Evolution, and Usage of Stack Overflow Code Snippets","Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of copyable code snippets. Like other software artifacts, code on SO evolves over time, for example when bugs are fixed or APIs are updated to the most recent version. To be able to analyze how code and the surroundingtexton SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individualtextand code blocks. It connects code snippets from SO posts to other platforms by aggregating URLs from surroundingtextblocks and comments, and by collecting references from GitHub files to SO posts. Our vision is that researchers will use SOTorrent to investigate and understand the evolution and maintenance of code on SO and its relation to other platforms such as GitHub.","['Sebastian Baltes', 'Christoph Treude', 'Stephan Diehl']",,arXiv,2019,https://doi.org/10.48550/arXiv.1809.02814,Anomali
Multi-label Classification of User Reactions in Online News,"The increase in the number of Internet users and the strong interaction brought by Web 2.0 made the OpinionMiningan important task in the area of natural language processing. Although several methods are capable of performing this task, few use multi-label classification, where there is a group of true labels for each example. This type of classification is useful for situations where the opinions are analyzed from the perspective of the reader, this happens because each person can have different interpretations and opinions on the same subject. This paper discuss the efficiency of problem transformation methods combined with different classification algorithms for the task of multi-label classification of reactions in newstexts. To do that, extensive tests were carried out on two news corpora written in Brazilian Portuguese annotated with reactions. A new corpus called BFRC-PT is presented. In the tests performed, the highest number of correct predictions was obtained with the Classifier Chains method combined with the Random Forest algorithm. When considering the class distribution, the best results were obtained with the Binary Relevance method combined with the LSTM and Random Forest algorithms.","['Zacarias Curi', 'Alceu de Souza Britto Jr', 'Emerson Cabrera Paraiso']",,arXiv,2018,https://doi.org/10.48550/arXiv.1809.02811,Anomali
Machine Learning Methods for Network Intrusion Detection,"Network security engineers work to keep services available all the time by handling intruder attacks. Intrusion Detection System (IDS) is one of the obtainable mechanisms that is used to sense and classify any abnormal actions. Therefore, the IDS must be always up to date with the latest intruder attacks signatures to preserve confidentiality, integrity, and availability of the services. The speed of the IDS is a very important issue as well learning the new attacks. This research work illustrates how the Knowledge Discovery and DataMining(or Knowledge Discovery in Databases) KDD dataset is very handy for testing and evaluating different Machine Learning Techniques. It mainly focuses on the KDD preprocess part in order to prepare a decent and fair experimental data set. The J48, MLP, and Bayes Network classifiers have been chosen for this study. It has been proven that the J48 classifier has achieved the highest accuracy rate for detecting and classifying all KDD dataset attacks, which are of type DOS, R2L, U2R, and PROBE.","['Mouhammad Alkasassbeh', 'Mohammad Almseidin']",,arXiv,2018,https://doi.org/10.48550/arXiv.1809.02610,Anomali
Deep learning for language understanding of mental health concepts derived from Cognitive Behavioural Therapy,"In recent years, we have seen deep learning and distributed representations of words and sentences make impact on a number of natural language processing tasks, such as similarity, entailment and sentiment analysis. Here we introduce a new task: understanding of mental health concepts derived from Cognitive Behavioural Therapy (CBT). We define a mental health ontology based on the CBT principles, annotate a large corpus where this phenomena is exhibited and perform understanding using deep learning and distributed representations. Our results show that the performance of deep learning models combined with word embeddings or sentence embeddings significantly outperform non-deep-learning models in this difficult task. This understanding module will be an essential component of a statistical dialogue system delivering therapy.","['Lina Rojas-Barahona', 'Bo-Hsiang Tseng', 'Yinpei Dai', 'Clare Mansfield', 'Osman Ramadan', 'Stefan Ultes', 'Michael Crawford', 'Milica Gasic']",,arXiv,2018,https://doi.org/10.48550/arXiv.1809.00640,Anomali
"Total Recall, Language Processing, and Software Engineering","A broad class of software engineering problems can be generalized as the ""total recall problem"". This short paper claims that identifying and exploring total recall language processing problems in software engineering is an important task with wide applicability.
  To make that case, we show that by applying and adapting the state of the art active learning andtextmining, solutions of the total recall problem, can help solve two important software engineering tasks: (a) supporting large literature reviews and (b) identifying software security vulnerabilities. Furthermore, we conjecture that (c) test case prioritization and (d) static warning identification can also be categorized as the total recall problem.
  The widespread applicability of ""total recall"" to software engineering suggests that there exists some underlying framework that encompasses not just natural language processing, but a wide range of important software engineering tasks.","['Zhe Yu', 'Tim Menzies']",,arXiv,2018,https://doi.org/10.48550/arXiv.1809.00039,Anomali
WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse,"We release a corpus of 43 million atomic edits across 8 languages. These edits areminedfrom Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. We use the collected data to show that the language generated during editing differs from the language that we observe in standard corpora, and that models trained on edits encode different aspects of semantics and discourse than models trained on raw, unstructuredtext. We release the full corpus as a resource to aid ongoing research in semantics, discourse, and representation learning.","['Manaal Faruqui', 'Ellie Pavlick', 'Ian Tenney', 'Dipanjan Das']",Proc. of EMNLP 2018,arXiv,2018,https://doi.org/10.48550/arXiv.1808.09422,Anomali
Big Data Privacy Context: Literature Effects On Secure Informational Assets,"This article's objective is the identification of research opportunities in the current big data privacy domain, evaluating literature effects on secure informational assets. Until now, no study has analyzed such relation. Its results can foster science, technologies and businesses. To achieve these objectives, a big data privacy Systematic Literature Review (SLR) is performed on the main scientific peer reviewed journals in Scopus database. Bibliometrics andtextmininganalysis complement the SLR. This study provides support to big data privacy researchers on: most and least researched themes, research novelty, most cited works and authors, themes evolution through time and many others. In addition, TOPSIS and VIKOR ranks were developed to evaluate literature effects versus informational assets indicators. Secure Internet Servers (SIS) was chosen as decision criteria. Results show that big data privacy literature is strongly focused on computational aspects. However, individuals, societies, organizations and governments face a technological change that has just started to be investigated, with growing concerns on law and regulation aspects. TOPSIS and VIKOR Ranks differed in several positions and the only consistent country between literature and SIS adoption is the United States. Countries in the lowest ranking positions represent future research opportunities.","['Celina Rebello', 'Elaine Tavares']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.08537,Anomali
A MapReduce based Big-data Framework for Object Extraction from Mosaic Satellite Images,"We propose a framework stitching of vector representations of large scale raster mosaic images in distributed computing model. In this way, the negative effect of the lack of resources of the central system and scalability problem can be eliminated. The product obtained by this study can be used in applications requiring spatial and temporal analysis on big satellite map images. This study also shows that big data frameworks are not only used in applications oftext-based dataminingand machine learning algorithms, but also used in applications of algorithms in image processing. The effectiveness of the product realized with this project is also going to be proven by scalability and performance tests performed on real world LandSat-8 satellite images.","['Suleyman Eken', 'Ahmet Sayar']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.08528,Anomali
Comparing CNN and LSTM character-level embeddings in BiLSTM-CRF models for chemical and disease named entity recognition,"We compare the use of LSTM-based and CNN-based character-level word embeddings in BiLSTM-CRF models to approach chemical and disease named entity recognition (NER) tasks. Empirical results over the BioCreative V CDR corpus show that the use of either type of character-level word embeddings in conjunction with the BiLSTM-CRF models leads to comparable state-of-the-art performance. However, the models using CNN-based character-level word embeddings have a computational performance advantage, increasing training time over word-based models by 25% while the LSTM-based character-level word embeddings more than double the required training time.","['Zenan Zhai', 'Dat Quoc Nguyen', 'Karin Verspoor']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.08450,Anomali
Measuring LDA Topic Stability from Clusters of Replicated Runs,"Background: Unstructured and textual data is increasing rapidly and Latent Dirichlet Allocation (LDA) topic modeling is a popular data analysis methods for it. Past work suggests that instability of LDA topics may lead to systematic errors. Aim: We propose a method that relies on replicated LDA runs, clustering, and providing a stability metric for the topics. Method: We generate k LDA topics and replicate this process n times resulting in n*k topics. Then we use K-medioids to cluster the n*k topics to k clusters. The k clusters now represent the original LDA topics and we present them like normal LDA topics showing the ten most probable words. For the clusters, we try multiple stability metrics, out of which we recommend Rank-Biased Overlap, showing the stability of the topics inside the clusters. Results: We provide an initial validation where our method is used for 270,000 Mozilla Firefox commit messages with k=20 and n=20. We show how our topic stability metrics are related to the contents of the topics. Conclusions: Advances intextminingenable us to analyze large masses oftextin software engineering but non-deterministic algorithms, such as LDA, may lead to unreplicable conclusions. Our approach makes LDA stability transparent and is also complementary rather than alternative to many prior works that focus on LDA parameter tuning.","['Mika Mäntylä', 'Maëlick Claes', 'Umar Farooq']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.08098,Anomali
Linguistic data mining with complex networks: a stylometric-oriented approach,"By representing atextby a set of words and their co-occurrences, one obtains a word-adjacency network being a reduced representation of a given language sample. In this paper, the possibility of using network representation to extract information about individual language styles of literarytextsis studied. By determining selected quantitative characteristics of the networks and applying machine learning algorithms, it is possible to distinguish betweentextsof different authors. Within the studied set oftexts, English and Polish, a properly rescaled weighted clustering coefficients and weighted degrees of only a few nodes in the word-adjacency networks are sufficient to obtain the authorship attribution accuracy over 90%. A correspondence between thetextauthorship and the word-adjacency network structure can therefore be found. The network representation allows to distinguish individual language styles by comparing the way the authors use particular words and punctuation marks. The presented approach can be viewed as a generalization of the authorship attribution methods based on simple lexical features.
  Additionally, other network parameters are studied, both local and global ones, for both the unweighted and weighted networks. Their potential to capture the writing style diversity is discussed; some differences between languages are observed.","['Tomasz Stanisz', 'Jarosław Kwapień', 'Stanisław Drożdż']","Information Sciences 482, 301-320 (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1808.05439,Anomali
Methodology for identifying study sites in scientific corpus,"The TERRE-ISTEX project aims at identifying the evolution of research working relation to study areas, disciplinary crossings and concrete research methods based on the heterogeneous digital content available in scientific corpora. The project is divided into three main actions: (1) to identify the periods and places which have been the subject of empirical studies, and which reflect the publications resulting from the corpus analyzed, (2) to identify the thematics addressed in these works and (3) to develop a web-based geographical information retrieval tool (GIR). The first two actions involve approaches combining Natural languages processing patterns withtextminingmethods. By crossing the three dimensions (spatial, thematic and temporal) in a GIR engine, it will be possible to understand what research has been carried out on which territories and at what time. In the project, the experiments are carried out on a heterogeneous corpus including electronic thesis and scientific articles from the ISTEX digital libraries and the CIRAD research center.","['Eric Kergosien', 'Marie-Noëlle Bessagnet', 'Maguelonne Teisseire', 'Joachim Schöpfel', 'Mohammad Amin Farvardin', 'Stéphane Chaudiron', 'Bernard Jacquemin', 'Annig Le Parc-Lacayrelle', 'Mathieu Roche', 'Christian Sallaberry', 'Jean-Philippe Tonneau', 'Marie-Noelle Bessagnet', 'Amin Farvardin', 'Annig Lacayrelle']","Revue des Sciences et Technologies de l'Information - Série Document Numérique, Lavoisier, 2017, 20 (2-3), pp.11-30",arXiv,2018,https://doi.org/10.48550/arXiv.1808.04124,Anomali
On feature selection and evaluation of transportation mode prediction strategies,"Transportation modes prediction is a fundamental task for decision making in smart cities and traffic management systems. Traffic policies designed based on trajectoryminingcan save money and time for authorities and the public. It may reduce the fuel consumption and commute time and moreover, may provide more pleasant moments for residents and tourists. Since the number of features that may be used to predict a user transportation mode can be substantial, finding a subset of features that maximizes a performance measure is worth investigating. In this work, we explore wrapper and information retrieval methods to find the best subset of trajectory features. After finding the best classifier and the best feature subset, our results were compared with two related papers that applied deep learning methods and the results showed that our framework achieved better performance. Furthermore, two types of cross-validation approaches were investigated, and the performance results show that the random cross-validation method provides optimistic results.","['Mohammad Etemad', 'Amilcar Soares Junior', 'Stan Matwin']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.03096,Anomali
How did the discussion go: Discourse act classification in social media conversations,"We propose a novel attention based hierarchical LSTM model to classify discourse act sequences in social media conversations, aimed atminingdata from online discussion using textual meanings beyond sentence level. The very uniqueness of the task is the complete categorization of possible pragmatic roles in informal textual discussions, contrary to extraction of question-answers, stance detection or sarcasm identification which are very much role specific tasks. Early attempt was made on a Reddit discussion dataset. We train our model on the same data, and present test results on two different datasets, one from Reddit and one from Facebook. Our proposed model outperformed the previous one in terms of domain independence; without using platform-dependent structural features, our hierarchical LSTM with word relevance attention mechanism achieved F1-scores of 71\% and 66\% respectively to predict discourse roles of comments in Reddit and Facebook discussions. Efficiency of recurrent and convolutional architectures in order to learn discursive representation on the same task has been presented and analyzed, with different word and comment embedding schemes. Our attention mechanism enables us to inquire into relevance ordering oftextsegments according to their roles in discourse. We present a human annotator experiment to unveil important observations about modeling and data annotation. Equipped with ourtext-based discourse identification model, we inquire into how heterogeneous non-textual features like location, time, leaning of information etc. play their roles in charaterizing online discussions on Facebook.","['Subhabrata Dutta', 'Tanmoy Chakraborty', 'Dipankar Das']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.02290,Anomali
Did you take the pill? - Detecting Personal Intake of Medicine from Twitter,"Miningsocial media messages such as tweets, articles, and Facebook posts for health and drug related information has received significant interest in pharmacovigilance research. Social media sites (e.g., Twitter), have been used for monitoring drug abuse, adverse reactions of drug usage and analyzing expression of sentiments related to drugs. Most of these studies are based on aggregated results from a large population rather than specific sets of individuals. In order to conduct studies at an individual level or specific cohorts, identifying posts mentioning intake of medicine by the user is necessary. Towards this objective we develop a classifier for identifying mentions of personal intake of medicine in tweets. We train a stacked ensemble of shallow convolutional neural network (CNN) models on an annotated dataset. We use random search for tuning the hyper-parameters of the CNN models and present an ensemble of best models for the prediction task. Our system produces state-of-the-art result, with a micro-averaged F-score of 0.693. We believe that the developed classifier has direct uses in the areas of psychology, health informatics, pharmacovigilance and affective computing for tracking moods, emotions and sentiments of patients expressing intake of medicine in social media.","['Debanjan Mahata', 'Jasper Friedrichs', 'Rajiv Ratn Shah', 'Jing Jiang']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.02082,Anomali
Automated Extraction of Personal Knowledge from Smartphone Push Notifications,"Personalized services are in need of a rich and powerful personal knowledge base, i.e. a knowledge base containing information about the user. This paper proposes an approach to extracting personal knowledge from smartphone push notifications, which are used by mobile systems and apps to inform users of a rich range of information. Our solution is based on the insight that most notifications are formatted using templates, while knowledge entities can be usually found within the parameters to the templates. As defining all the notification templates and their semantic rules are impractical due to the huge number of notification templates used by potentially millions of apps, we propose an automated approach for personal knowledge extraction from push notifications. We first discover notification templates through patternmining, then use machine learning to understand the template semantics. Based on the templates and their semantics, we are able to translate notificationtextinto knowledge facts automatically. Users' privacy is preserved as we only need to upload the templates to the server for model training, which do not contain any personal information. According to our experiments with about 120 million push notifications from 100,000 smartphone users, our system is able to extract personal knowledge accurately and efficiently.","['Yuanchun Li', 'Ziyue Yang', 'Yao Guo', 'Xiangqun Chen', 'Yuvraj Agarwal', 'Jason Hong']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.02013,Anomali
Evaluating Wikipedia as a source of information for disease understanding,"The increasing availability of biological data is improving our understanding of diseases and providing new insight into their underlying relationships. Thanks to the improvements on bothtextminingtechniques and computational capacity, the combination of biological data with semantic information obtained from medical publications has proven to be a very promising path. However, the limitations in the access to these data and their lack of structure pose challenges to this approach. In this document we propose the use of Wikipedia - the free online encyclopedia - as a source of accessible textual information for disease understanding research. To check its validity, we compare its performance in the determination of relationships between diseases with that of PubMed, one of the most consulted data sources of medicaltexts. The obtained results suggest that the information extracted from Wikipedia is as relevant as that obtained from PubMed abstracts (i.e. the free access portion of its articles), although further research is proposed to verify its reliability for medical studies.","['Eduardo P. Garcia del Valle', 'Gerardo Lagunes Garcia', 'Lucia Prieto Santamaria', 'Massimiliano Zanin', 'Alejandro Rodriguez-Gonzalez', 'Ernestina Menasalvas Ruiz']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.01459,Anomali
How Does Tweet Difficulty Affect Labeling Performance of Annotators?,"Crowdsourcing is a popular means to obtain labeled data at moderate costs, for example for tweets, which can then be used intextminingtasks. To alleviate the problem of low-quality labels in this context, multiple human factors have been analyzed to identify and deal with workers who provide such labels. However, one aspect that has been rarely considered is the inherent difficulty of tweets to be labeled and how this affects the reliability of the labels that annotators assign to such tweets. Therefore, we investigate in this preliminary study this connection using a hierarchical sentiment labeling task on Twitter. We find that there is indeed a relationship between both factors, assuming that annotators have labeled some tweets before: labels assigned to easy tweets are more reliable than those assigned to difficult tweets. Therefore, training predictors on easy tweets enhances the performance by up to 6% in our experiment. This implies potential improvements for active learning techniques and crowdsourcing.","['Stefan Räbiger', 'Yücel Saygın', 'Myra Spiliopoulou']",,arXiv,2018,https://doi.org/10.48550/arXiv.1808.00388,Anomali
MaxMin Linear Initialization for Fuzzy C-Means,"Clustering is an extensive research area in data science. The aim of clustering is to discover groups and to identify interesting patterns in datasets. Crisp (hard) clustering considers that each data point belongs to one and only one cluster. However, it is inadequate as some data points may belong to several clusters, as is the case intextcategorization. Thus, we need more flexible clustering. Fuzzy clustering methods, where each data point can belong to several clusters, are an interesting alternative. Yet, seeding iterative fuzzy algorithms to achieve high quality clustering is an issue. In this paper, we propose a new linear and efficient initialization algorithm MaxMin Linear to deal with this problem. Then, we validate our theoretical results through extensive experiments on a variety of numerical real-world and artificial datasets. We also test several validity indices, including a new validity index that we propose, Transformed Standardized Fuzzy Difference (TSFD).","['Aybükë Oztürk', 'Stéphane Lallich', 'Jérôme Darmont', 'Sylvie Yona Waksman']","IBaI. 14th International Conference on Machine Learning and Data Mining (MLDM 2018), Jul 2018, New York, United States. Springer, Lecture Notes in Artificial Intelligence, 10934-10935, 2018, Machine Learning and Data Mining in Pattern Recognition. http://www.mldm.de",arXiv,2018,https://doi.org/10.48550/arXiv.1808.00197,Anomali
Clustering Prominent People and Organizations in Topic-Specific Text Corpora,"Named entities intextdocuments are the names of people, organization, location or other types of objects in the documents that exist in the real world. A persisting research challenge is to use computational techniques to identify such entities intextdocuments. Once identified, severaltextminingtools and algorithms can be utilized to leverage these discovered named entities and improve NLP applications. In this paper, a method that clusters prominent names of people and organizations based on their semantic similarity in atextcorpus is proposed. The method relies on common named entity recognition techniques and on recent word embeddings models. The semantic similarity scores generated using the word embeddings models for the named entities are used to cluster similar entities of the people and organizations types. Two human judges evaluated ten variations of the method after it was run on a corpus that consists of 4,821 articles on a specific topic. The performance of the method was measured using three quantitative measures. The results of these three metrics demonstrate that the method is effective in clustering semantically similar named entities.","['Abdulkareem Alsudais', 'Hovig Tchalian']",,arXiv,2019,https://doi.org/10.48550/arXiv.1807.10800,Anomali
Learning low dimensional word based linear classifiers using Data Shared Adaptive Bootstrap Aggregated Lasso with application to IMDb data,"In this article we propose a new supervised ensemble learning method called Data Shared Adaptive Bootstrap Aggregated (AdaBag) Lasso for capturing low dimensional useful features for word based sentiment analysis andminingproblems. The literature on ensemble methods is very rich in both statistics and machine learning. The algorithm is a substantial upgrade of the Data Shared Lasso uplift algorithm. The most significant conceptual addition to the existing literature lies in the final selection of bag of predictors through a special bootstrap aggregation scheme. We apply the algorithm to one simulated data and perform dimension reduction in grouped IMDb data (drama, comedy and horror) to extract reduced set of word features for predicting sentiment ratings of movie reviews demonstrating different aspects. We also compare the performance of the present method with the classical Principal Components with associated Linear Discrimination (PCA-LD) as baseline. There are few limitations in the algorithm. Firstly, the algorithm workflow does not incorporate online sequential data acquisition and it does not use sentence based models which are common in ANN algorithms . Our results produce slightly higher error rate compare to the reported state-of-the-art as a consequence.",['Ashutosh K. Maurya'],,arXiv,2018,https://doi.org/10.48550/arXiv.1807.10623,Anomali
Automatic Short Answer Grading and Feedback Using Text Mining Methods,"Automatic grading is not a new approach but the need to adapt the latest technology to automatic grading has become very important. As the technology has rapidly became more powerful on scoring exams and essays, especially from the 1990s onwards, partially or wholly automated grading systems using computational methods have evolved and have become a major area of research. In particular, the demand of scoring of natural language responses has created a need for tools that can be applied to automatically grade these responses. In this paper, we focus on the concept of automatic grading of short answer questions such as are typical in the UK GCSE system, and providing useful feedback on their answers to students. We present experimental results on a dataset provided from the introductory computer science class in the University of North Texas. We first apply standard dataminingtechniques to the corpus of student answers for the purpose of measuring similarity between the student answers and the model answer. This is based on the number of common words. We then evaluate the relation between these similarities and marks awarded by scorers. We then consider an approach that groups student answers into clusters. Each cluster would be awarded the same mark, and the same feedback given to each answer in a cluster. In this manner, we demonstrate that clusters indicate the groups of students who are awarded the same or the similar scores. Words in each cluster are compared to show that clusters are constructed based on how many and which words of the model answer have been used. The main novelty in this paper is that we design a model to predict marks based on the similarities between the student answers and the model answer.","['Neslihan Suzen', 'Alexander Gorban', 'Jeremy Levesley', 'Evgeny Mirkes']","Procedia Computer Science 169 (2020), 726-743",arXiv,2019,https://doi.org/10.48550/arXiv.1807.10543,Anomali
Latent Dirichlet Allocation (LDA) for Topic Modeling of the CFPB Consumer Complaints,"Atextminingapproach is proposed based on latent Dirichlet allocation (LDA) to analyze the Consumer Financial Protection Bureau (CFPB) consumer complaints. The proposed approach aims to extract latent topics in the CFPB complaint narratives, and explores their associated trends over time. The time trends will then be used to evaluate the effectiveness of the CFPB regulations and expectations on financial institutions in creating a consumer oriented culture that treats consumers fairly and prioritizes consumer protection in their decision making processes. The proposed approach can be easily operationalized as a decision support system to automate detection of emerging topics in consumer complaints. Hence, the technology-human partnership between the proposed approach and the CFPB team could certainly improve consumer protections from unfair, deceptive or abusive practices in the financial markets by providing more efficient and effective investigations of consumer complaint narratives.","['Kaveh Bastani', 'Hamed Namavari', 'Jeffry Shaffer']",,arXiv,2018,https://doi.org/10.48550/arXiv.1807.07468,Anomali
Rapid-scan acousto-optical delay line with 34 kHz scan rate and 15 attosecond precision,"An optical fast-scan delay exploiting the near-collinear interaction between a train of ultrashort optical pulses and an acoustic wave propagating in a birefringent crystal is introduced. In combination with a femtosecond Er:fiber laser, the scheme is shown to delay few-fs pulses by up to 6 ps with a precision of 15 as. A resolution of 5 fs is obtained for a single sweep at a repetition rate of 34 kHz. This value can be improved to 39 as for multiple scans at a total rate of 0.3 kHz.","['Olaf Schubert', 'Max Eisele', 'Vincent Crozatier', 'Nicolas Forget', 'Daniel Kaplan', 'Rupert Huber']","Optics Letters Vol. 38, Issue 15, pp. 2907-2910 (2013)",arXiv,2018,https://doi.org/10.48550/arXiv.1807.04752,Anomali
Making Efficient Use of a Domain Expert's Time in Relation Extraction,"Scarcity of labeled data is one of the most frequent problems faced in machine learning. This is particularly true in relation extraction intextmining, where large corpora oftextsexists in many application domains, while labeling oftextdata requires an expert to invest much time to read the documents. Overall, state-of-the art models, like the convolutional neural network used in this paper, achieve great results when trained on large enough amounts of labeled data. However, from a practical point of view the question arises whether this is the most efficient approach when one takes the manual effort of the expert into account. In this paper, we report on an alternative approach where we first construct a relation extraction model using distant supervision, and only later make use of a domain expert to refine the results. Distant supervision provides a mean of labeling data given known relations in a knowledge base, but it suffers from noisy labeling. We introduce an active learning based extension, that allows our neural network to incorporate expert feedback and report on first results on a complex data set.","['Linara Adilova', 'Sven Giesselbach', 'Stefan Rüping']",,arXiv,2018,https://doi.org/10.48550/arXiv.1807.04687,Anomali
RACK: Code Search in the IDE using Crowdsourced Knowledge,"Traditional code search engines often do not perform well with natural language queries since they mostly apply keyword matching. These engines thus require carefully designed queries containing information about programming APIs for code search. Unfortunately, existing studies suggest that preparing an effective query for code search is both challenging and time consuming for the developers. In this paper, we propose a novel code search tool--RACK--that returns relevant source code for a given code search query written in natural languagetext. The tool first translates the query into a list of relevant API classes byminingkeyword-API associations from the crowdsourced knowledge of Stack Overflow, and then applies the reformulated query to GitHub code search API for collecting relevant results. Once a query related to a programming task is submitted, the tool automaticallyminesrelevant code snippets from thousands of open-source projects, and displays them as a ranked list within the context of the developer's programming environment--the IDE.
  Tool page: http://www.usask.ca/~masud.rahman/rack","['Mohammad Masudur Rahman', 'Chanchal K. Roy', 'David Lo']",10.1109/ICSE-C.2017.11,arXiv,2018,https://doi.org/10.48550/arXiv.1807.04479,Anomali
Tracking the Evolution of Words with Time-reflective Text Representations,"More than 80% of today's data is unstructured in nature, and these unstructured datasets evolve over time. A large part of these datasets aretextdocuments generated by media outlets, scholarly articles in digital libraries, findings from scientific and professional communities, and social media. Vector space models were developed to analyzetextdata using dataminingand machine learning algorithms. While ample vector space models exist fortextdata, the evolutionary aspect of ever-changingtextcorpora is still missing in vector-based representations. The advent of word embeddings has enabled us to create a contextual vector space, but the embeddings fail to consider the temporal aspects of the feature space successfully. This paper presents an approach to include temporal aspects in feature spaces. The inclusion of the time aspect in the feature space provides vectors for every natural language element, such as words or entities, at every timestamp. Such temporal word vectors allow us to track how the meaning of a word changes over time, by studying the changes in its neighborhood. Moreover, a time-reflectivetextrepresentation will pave the way to a new set oftextanalytic abilities involving time series fortextcollections. In this paper, we present a time-reflective vector space model for temporaltextdata that is able to capture short and long-term changes in the meaning of words. We compare our approach with the limited literature on dynamic embeddings. We present qualitative and quantitative evaluations using the tracking of semantic evolution as the target application.","['Roberto Camacho Barranco', 'Raimundo F. Dos Santos', 'M. Shahriar Hossain']",,arXiv,2019,https://doi.org/10.48550/arXiv.1807.04441,Anomali
Automated labeling of bugs and tickets using attention-based mechanisms in recurrent neural networks,"We explore solutions for automated labeling of content in bug trackers and customer support systems. In order to do that, we classify content in terms of several criteria, such as priority or product area. In the first part of the paper, we provide an overview of existing methods used fortextclassification. These methods fall into two categories - the ones that rely on neural networks and the ones that don't. We evaluate results of several solutions of both kinds. In the second part of the paper we present our own recurrent neural network solution based on hierarchical attention paradigm. It consists of several Hierarchical Attention network blocks with varying Gated Recurrent Unit cell sizes and a complementary shallow network that goes alongside. Lastly, we evaluate above-mentioned methods when predicting fields from two datasets - Arch Linux bug tracker and Chromium bug tracker. Our contributions include a comprehensive benchmark between a variety of methods on relevant datasets; a novel solution that outperforms previous generation methods; and two new datasets that are made public for further research.","['Volodymyr Lyubinets', 'Taras Boiko', 'Deon Nicholas']",,arXiv,2018,https://doi.org/10.48550/arXiv.1807.02892,Anomali
Natural Language Processing for Music Knowledge Discovery,"Today, a massive amount of musical knowledge is stored in written form, with testimonies dated as far back as several centuries ago. In this work, we present different Natural Language Processing (NLP) approaches to harness the potential of thesetextcollections for automatic music knowledge discovery, covering different phases in a prototypical NLP pipeline, namely corpus compilation,text-mining, information extraction, knowledge graph generation and sentiment analysis. Each of these approaches is presented alongside different use cases (i.e., flamenco, Renaissance and popular music) where large collections of documents are processed, and conclusions stemming from data-driven analyses are presented and discussed.","['Sergio Oramas', 'Luis Espinosa-Anke', 'Francisco Gómez', 'Xavier Serra']",Journal of New Music Research (2018),arXiv,2018,https://doi.org/10.48550/arXiv.1807.02200,Anomali
Introducing non-local correlations into speckles,"Laser speckles have become a fundamental component of the modern optics-research toolbox. Not only are speckle patterns the basis of numerous imaging techniques, but also, they are employed to generate optical potentials for cold atoms and colloidal particles. The ability to manipulate a speckle pattern's spatial intensity correlations, particularly long-range (non-local) ones, is essential in numerous applications. A typical fully-developed speckle pattern, however, only possesses short-ranged (local) intensity correlations which are determined by the spatial field correlations. Here we experimentally demonstrate and theoretically develop a general method for creating fully-developed speckles with strong non-local intensity correlations. The functional form of the spatial intensity correlations can be arbitrarily tailored without altering the field correlations. Our approach provides a versatile and utilitarian framework for enhancing and controlling non-local correlations in speckle patterns.","['Nicholas Bender', 'Hasan Yilmaz', 'Yaron Bromberg', 'Hui Cao']","Opt. Express, Vol. 27, Issue 5, pp. 6057-6067 (2019)",arXiv,2019,https://doi.org/10.48550/arXiv.1807.00671,Anomali
Team assembly mechanisms and the knowledge produced in the Mexico's National Institute of Geriatrics: a network analysis and agent-based modelling approach,"Mexico's National Institute of Geriatrics (INGER) is the national research center of reference for matters related to human aging. INGER scientists perform basic, clinical and demographic research which may imply different scientific cultures working together in the same specialized institution. In this paper, by a combination oftextmining, co-authorship network analysis and agent-based modeling we analyzed and modeled the team assembly practices and the structure of the knowledge produced by scientists from INGER. Our results showed a weak connection between basic and clinical research, and the emergence of a highly connected academic leadership. Importantly, basic and clinical-demographic researchers exhibited different team assembly strategies: Basic researchers tended to form larger teams mainly with external collaborators while clinical and demographic researchers formed smaller teams that very often incorporated internal (INGER) collaborators. We showed how these two different ways to form research teams impacted the organization of knowledge produced at INGER. Following these observations, we modeled, via agent-based modeling, the coexistence of different scientific cultures (basic and clinical research) exhibiting different team assembly strategies in the same institution. Our agent model successfully reproduced the current situation of INGER. Moreover, by modifying the values of homophily we obtain alternative scenarios in which multidisciplinary and interdisciplinary research could be done.","['Carmen García-Peña', 'Luis Miguel Gutiérrez-Robledo', 'Augusto Cabrera-Becerril', 'David Fajardo-Ortiz']","Scientifica, vol. 2019, Article ID 9127657, 7 pages, 2019",arXiv,2018,https://doi.org/10.48550/arXiv.1806.11433,Anomali
Long-term stock index forecasting based on text mining of regulatory disclosures,"Share valuations are known to adjust to new information entering the market, such as regulatory disclosures. We study whether the language of such news items can improve short-term and especially long-term (24 months) forecasts of stock indices. For this purpose, this work utilizes predictive models suited to high-dimensional data and specifically compares techniques for data-driven and knowledge-driven dimensionality reduction in order to avoid overfitting. Our experiments, based on 75,927 ad hoc announcements from 1996-2016, reveal the following results: in the long run,text-based models succeed in reducing forecast errors below baseline predictions from historic lags at a statistically significant level. Our research provides implications to business applications of decision-support in financial markets, especially given the growing prevalence of index ETFs (exchange traded funds).","['Stefan Feuerriegel', 'Julius Gordon']",,arXiv,2018,https://doi.org/10.48550/arXiv.1806.09866,Anomali
Paragraph-based complex networks: application to document classification and authenticity verification,"With the increasing number oftextsmade available on the Internet, many applications have relied ontextminingtools to tackle a diversity of problems. A relevant model to representtextsis the so-called word adjacency (co-occurrence) representation, which is known to capture mainly syntactical features of texts.In this study, we introduce a novel network representation that considers the semantic similarity between paragraphs. Two main properties of paragraph networks are considered: (i) their ability to incorporate characteristics that can discriminate real from artificial, shuffled manuscripts and (ii) their ability to capture syntactical and semantic textual features. Our results revealed that realtextsare organized into communities, which turned out to be an important feature for discriminating them from artificialtexts. Interestingly, we have also found that, differently from traditional co-occurrence networks, the adopted representation is able to capture semantic features. Additionally, the proposed framework was employed to analyze the Voynich manuscript, which was found to be compatible withtextswritten in natural languages. Taken together, our findings suggest that the proposed methodology can be combined with traditional network models to improvetextclassification tasks.","['Henrique F. de Arruda', 'Vanessa Q. Marinho', 'Luciano da F. Costa', 'Diego R. Amancio']","Information Processing & Management 56 (3) 479-494, 2019",arXiv,2018,https://doi.org/10.48550/arXiv.1806.08467,Anomali
A Scalable Machine Learning Approach for Inferring Probabilistic US-LI-RADS Categorization,"We propose a scalable computerized approach for large-scale inference of Liver Imaging Reporting and Data System (LI-RADS) final assessment categories in narrative ultrasound (US) reports. Although our model was trained on reports created using a LI-RADS template, it was also able to infer LI-RADS scoring for unstructured reports that were created before the LI-RADS guidelines were established. No human-labelled data was required in any step of this study; for training, LI-RADS scores were automatically extracted from those reports that contained structured LI-RADS scores, and it translated the derived knowledge to reasoning on unstructured radiology reports. By providing automated LI-RADS categorization, our approach may enable standardizing screening recommendations and treatment planning of patients at risk for hepatocellular carcinoma, and it may facilitate AI-based healthcare research with US images by offering large scaletextminingand data gathering opportunities from standard hospital clinical data repositories.","['Imon Banerjee', 'Hailey H. Choi', 'Terry Desser', 'Daniel L. Rubin']",,arXiv,2018,https://doi.org/10.48550/arXiv.1806.07346,Anomali
On the use of deep neural networks in optical communications,"Information transfer rates in optical communications may be dramatically increased by making use of spatially non-Gaussian states of light. Here we demonstrate the ability of deep neural networks to classify numerically-generated, noisy Laguerre-Gauss modes of up to 100 quanta of orbital angular momentum with near-unity fidelity. The scheme relies only on the intensity profile of the detected modes, allowing for considerable simplification of current measurement schemes required to sort the states containing increasing degrees of orbital angular momentum. We also present results that show the strength of deep neural networks in the classification of experimental superpositions of Laguerre-Gauss modes when the networks are trained solely using simulated images. It is anticipated that these results will allow for an enhancement of current optical communications technologies.","['Sanjaya Lohani', 'Erin M. Knutson', ""Matthew O'Donnell"", 'Sean D. Huver', 'Ryan T. Glasser']",Applied Optics 57(15) 4180-4190 (2018),arXiv,2018,https://doi.org/10.48550/arXiv.1806.06663,Anomali
An Improved Text Sentiment Classification Model Using TF-IDF and Next Word Negation,"With the rapid growth ofTextsentiment analysis, the demand for automatic classification of electronic documents has increased by leaps and bound. The paradigm oftextclassification ortextmininghas been the subject of many research works in recent time. In this paper we propose a technique fortextsentiment classification using term frequency- inverse document frequency (TF-IDF) along with Next Word Negation (NWN). We have also compared the performances of binary bag of words model, TF-IDF model and TF-IDF with next word negation (TF-IDF-NWN) model fortextclassification. Our proposed model is then applied on three differenttextminingalgorithms and we found the Linear Support vector machine (LSVM) is the most appropriate to work with our proposed model. The achieved results show significant increase in accuracy compared to earlier methods.","['Bijoyan Das', 'Sarit Chakraborty']",,arXiv,2018,https://doi.org/10.48550/arXiv.1806.06407,Anomali
Gender Prediction in English-Hindi Code-Mixed Social Media Content : Corpus and Baseline System,"The rapid expansion in the usage of social media networking sites leads to a huge amount of unprocessed user generated data which can be used fortextmining. Author profiling is the problem of automatically determining profiling aspects like the author's gender and age group through atextis gaining much popularity in computational linguistics. Most of the past research in author profiling is concentrated on Englishtexts\cite{1,2}. However many users often change the language while posting on social media which is called code-mixing, and it develops some challenges in the field oftextclassification and author profiling like variations in spelling, non-grammatical structure and transliteration \cite{3}. There are very few English-Hindi code-mixed annotated datasets of social media content present online \cite{4}. In this paper, we analyze the task of author's gender prediction in code-mixed content and present a corpus of English-Hinditextscollected from Twitter which is annotated with author's gender. We also explore language identification of every word in this corpus. We present a supervised classification baseline system which uses various machine learning algorithms to identify the gender of an author using atext, based on character and word level features.","['Ankush Khandelwal', 'Sahil Swami', 'Syed Sarfaraz Akhtar', 'Manish Shrivastava']",,arXiv,2018,https://doi.org/10.48550/arXiv.1806.05600,Anomali
Enabling End-To-End Machine Learning Replicability: A Case Study in Educational Data Mining,"The use of machine learning techniques has expanded in education research, driven by the rich data from digital learning environments and institutional data warehouses. However, replication of machine learned models in the domain of the learning sciences is particularly challenging due to a confluence of experimental, methodological, and data barriers. We discuss the challenges of end-to-end machine learning replication in this context, and present an open-source software toolkit, the MOOC Replication Framework (MORF), to address them. We demonstrate the use of MORF by conducting a replication at scale, and provide a complete executable container, with unique DOIs documenting the configurations of each individual trial, for replication or future extension at https://github.com/educational-technology-collective/fy2015-replication. This work demonstrates an approach to end-to-end machine learning replication which is relevant to any domain with large, complex or multi-format, privacy-protected data with a consistent schema.","['Josh Gardner', 'Yuming Yang', 'Ryan Baker', 'Christopher Brooks']",,arXiv,2018,https://doi.org/10.48550/arXiv.1806.05208,Anomali
Automatic Identification of Research Fields in Scientific Papers,"The TERRE-ISTEX project aims to identify scientific research dealing with specific geographical territories areas based on heterogeneous digital content available in scientific papers. The project is divided into three main work packages: (1) identification of the periods and places of empirical studies, and which reflect the publications resulting from the analyzedtextsamples, (2) identification of the themes which appear in these documents, and (3) development of a web-based geographical information retrieval tool (GIR). The first two actions combine Natural Language Processing patterns withtextminingmethods. The integration of the spatial, thematic and temporal dimensions in a GIR contributes to a better understanding of what kind of research has been carried out, of its topics and its geographical and historical coverage. Another originality of the TERRE-ISTEX project is the heterogeneous character of the corpus, including PhD theses and scientific articles from the ISTEX digital libraries and the CIRAD research center.","['Eric Kergosien', 'Amin Farvardin', 'Maguelonne Teisseire', 'Marie-Noëlle Bessagnet', 'Joachim Schöpfel', 'Stéphane Chaudiron', 'Bernard Jacquemin', 'Annig Le Parc-Lacayrelle', 'Mathieu Roche', 'Christian Sallaberry', 'Jean-Philippe Tonneau']","Proceedings of the Eleventh International Conference on Language Resources and Evaluation, pp.1902-1907, 2018, http://lrec2018.lrec-conf.org",arXiv,2018,https://doi.org/10.48550/arXiv.1806.03144,Anomali
A Simple NLP-based Approach to Support Onboarding and Retention in Open Source Communities,"Successful open source communities are constantly looking for new members and helping them become active developers. A common approach for developer onboarding in open source projects is to let newcomers focus on relevant yet easy-to-solve issues to familiarize themselves with the code and the community. The goal of this research is twofold. First, we aim at automatically identifying issues that newcomers can resolve by analyzing the history of resolved issues by simply using the title and description of issues. Second, we aim at automatically identifying issues, that can be resolved by newcomers who later become active developers. Weminedthe issue trackers of three large open source projects and extracted natural language features from the title and description of resolved issues. In a series of experiments, we optimized and compared the accuracy of four supervised classifiers to address our research goals. Random Forest, achieved up to 91% precision (F1-score 72%) towards the first goal while for the second goal, Decision Tree achieved a precision of 92% (F1-score 91%). A qualitative evaluation gave insights on what information in the issue description is helpful for newcomers. Our approach can be used to automatically identify, label, and recommend issues for newcomers in open source software projects based only on thetextof the issues.","['Christoph Stanik', 'Lloyd Montgomery', 'Daniel Martens', 'Davide Fucci', 'Walid Maalej']",,arXiv,2018,https://doi.org/10.48550/arXiv.1806.02592,Anomali
Superradiant coupling effects in transition-metal dichalcogenides,"Cooperative effects allow for fascinating characteristics in light-matter interacting systems. Here, we study naturally occurring superradiant coupling in a class of quasi-two-dimensional, layered semiconductor systems. We perform optical absorption experiments of the lowest exciton for transition-metal dichalcogenides with different numbers of atomic layers. We examine two representative materials, MoSe$_2$ and WSe$_2$, using incoherent broadband white light. The measured transmission at the A exciton resonance does not saturate for optically thick samples consisting of hundreds of atomic layers, and the transmission varies nonmonotonously with the layer number. A self-consistent microscopic calculation reproduces the experimental observations, clearly identifying superradiant coupling effects as the origin of this unexpected behavior.","['C. E. Stevens', 'T. Stroucken', 'A. V. Stier', 'J. Paul', 'H. Zhang', 'P. Dey', 'S. A. Crooker', 'S. W. Koch', 'D. Karaiskaj']","Optica Vol. 5, Issue 6, pp. 749-755 (2018)",arXiv,2018,https://doi.org/10.48550/arXiv.1806.02540,Anomali
A Visual Quality Index for Fuzzy C-Means,"Cluster analysis is widely used in the areas of machine learning and datamining. Fuzzy clustering is a particular method that considers that a data point can belong to more than one cluster. Fuzzy clustering helps obtain flexible clusters, as needed in such applications astextcategorization. The performance of a clustering algorithm critically depends on the number of clusters, and estimating the optimal number of clusters is a challenging task. Quality indices help estimate the optimal number of clusters. However, there is no quality index that can obtain an accurate number of clusters for different datasets. Thence, in this paper, we propose a new cluster quality index associated with a visual, graph-based solution that helps choose the optimal number of clusters in fuzzy partitions. Moreover, we validate our theoretical results through extensive comparison experiments against state-of-the-art quality indices on a variety of numerical real-world and artificial datasets.","['Aybükë Oztürk', 'Stéphane Lallich', 'Jérôme Darmont']","14th International Conference on Artificial Intelligence Applications and Innovations (AIAI 2018), May 2018, Rhodes, Greece. Springer, IFIP Advances in Information and Communication Technology, 519, pp.546-555, 2018, http://easyconferences.eu/aiai2018/",arXiv,2018,https://doi.org/10.48550/arXiv.1806.01552,Anomali
"OpenTag: Open Attribute Value Extraction from Product Profiles [Deep Learning, Active Learning, Named Entity Recognition]","Extraction of missing attribute values is to find values describing an attribute of interest from a freetextinput. Most past related work on extraction of missing attribute values work with a closed world assumption with the possible set of values known beforehand, or use dictionaries of values and hand-crafted features. How can we discover new attribute values that we have never seen before? Can we do this with limited human annotation or supervision? We study this problem in the context of product catalogs that often have missing values for many attributes of interest.
  In this work, we leverage product profile information such as titles and descriptions to discover missing values of product attributes. We develop a novel deep tagging model OpenTag for this extraction problem with the following contributions: (1) we formalize the problem as a sequence tagging task, and propose a joint model exploiting recurrent neural networks (specifically, bidirectional LSTM) to capture context and semantics, and Conditional Random Fields (CRF) to enforce tagging consistency, (2) we develop a novel attention mechanism to provide interpretable explanation for our model's decisions, (3) we propose a novel sampling strategy exploring active learning to reduce the burden of human annotation. OpenTag does not use any dictionary or hand-crafted features as in prior works. Extensive experiments in real-life datasets in different domains show that OpenTag with our active learning strategy discovers new attribute values from as few as 150 annotated samples (reduction in 3.3x amount of annotation effort) with a high F-score of 83%, outperforming state-of-the-art models.","['Guineng Zheng', 'Subhabrata Mukherjee', 'Xin Luna Dong', 'Feifei Li']",,arXiv,2018,https://doi.org/10.48550/arXiv.1806.01264,Anomali
Text to brain: predicting the spatial distribution of neuroimaging observations from text reports,"Despite the digital nature of magnetic resonance imaging, the resulting observations are most frequently reported and stored intextdocuments. There is a trove of information untapped in medical health records, case reports, and medical publications. In this paper, we propose tominebrain medical publications to learn the spatial distribution associated with anatomical terms. The problem is formulated in terms of minimization of a risk on distributions which leads to a least-deviation cost function. An efficient algorithm in the dual then learns the mapping from documents to brain structures. Empirical results using coordinates extracted from the brain-imaging literature show that i) models must adapt to semantic variation in the terms used to describe a given anatomical structure, ii) voxel-wise parameterization leads to higher likelihood of locations reported in unseen documents, iii) least-deviation cost outperforms least-square. As a proof of concept for our method, we use our model of spatial distributions to predict the distribution of specific neurological conditions fromtext-only reports.","['Jérôme Dockès', 'Demian Wassermann', 'Russell Poldrack', 'Fabian Suchanek', 'Bertrand Thirion', 'Gaël Varoquaux']","MICCAI 2018 - 21st International Conference on Medical Image Computing and Computer Assisted Intervention, Sep 2018, Granada, Spain. pp.1-18, 2018",arXiv,2018,https://doi.org/10.48550/arXiv.1806.01139,Anomali
An unsupervised and customizable misspelling generator for mining noisy health-related text sources,"In this paper, we present a customizable datacentric system that automatically generates common misspellings for complex health-related terms. The spelling variant generator relies on a dense vector model learned from large unlabeledtext, which is used to find semantically close terms to the original/seed keyword, followed by the filtering of terms that are lexically dissimilar beyond a given threshold. The process is executed recursively, converging when no new terms similar (lexically and semantically) to the seed keyword are found. Weighting of intra-word character sequence similarities allows further problem-specific customization of the system. On a dataset prepared for this study, our system outperforms the current state-of-the-art for medication name variant generation with best F1-score of 0.69 and F1/4-score of 0.78. Extrinsic evaluation of the system on a set of cancer-related terms showed an increase of over 67% in retrieval rate from Twitter posts when the generated variants are included. Our proposed spelling variant generator has several advantages over the current state-of-the-art and other types of variant generators-(i) it is capable of filtering out lexically similar but semantically dissimilar terms, (ii) the number of variants generated is low as many low-frequency and ambiguous misspellings are filtered out, and (iii) the system is fully automatic, customizable and easily executable. While the base system is fully unsupervised, we show how supervision maybe employed to adjust weights for task-specific customization. The performance and significant relative simplicity of our proposed approach makes it a much needed misspelling generation resource for health-relatedtextminingfrom noisy sources. The source code for the system has been made publicly available for research purposes.","['Abeed Sarker', 'Graciela Gonzalez-Hernandez']",J Biomed Inform. 2018 Dec;88:98-107. Epub 2018 Nov 13,arXiv,2018,https://doi.org/10.48550/arXiv.1806.00910,Anomali
A Corpus of English-Hindi Code-Mixed Tweets for Sarcasm Detection,Social media platforms like twitter and facebook have be- come two of the largest mediums used by people to express their views to- wards different topics. Generation of such large user data has made NLP tasks like sentiment analysis and opinionminingmuch more important. Using sarcasm intextson social media has become a popular trend lately. Using sarcasm reverses the meaning and polarity of what is implied by thetextwhich poses challenge for many NLP tasks. The task of sarcasm detection intextis gaining more and more importance for both commer- cial and security services. We present the first English-Hindi code-mixed dataset of tweets marked for presence of sarcasm and irony where each token is also annotated with a language tag. We present a baseline su- pervised classification system developed using the same dataset which achieves an average F-score of 78.4 after using random forest classifier and performing 10-fold cross validation.,"['Sahil Swami', 'Ankush Khandelwal', 'Vinay Singh', 'Syed Sarfaraz Akhtar', 'Manish Shrivastava']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.11869,Anomali
Large-Scale Learning from Data Streams with Apache SAMOA,"Apache SAMOA (Scalable Advanced Massive Online Analysis) is an open-source platform forminingbig data streams. Big data is defined as datasets whose size is beyond the ability of typical software tools to capture, store, manage, and analyze, due to the time and memory complexity. Apache SAMOA provides a collection of distributed streaming algorithms for the most common dataminingand machine learning tasks such as classification, clustering, and regression, as well as programming abstractions to develop new algorithms. It features a pluggable architecture that allows it to run on several distributed stream processing engines such as Apache Flink, Apache Storm, and Apache Samza. Apache SAMOA is written in Java and is available at https://samoa.incubator.apache.org under the Apache Software License version 2.0.","['Nicolas Kourtellis', 'Gianmarco De Francisci Morales', 'Albert Bifet']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.11477,Anomali
iLCM - A Virtual Research Infrastructure for Large-Scale Qualitative Data,"The iLCM project pursues the development of an integrated research environment for the analysis of structured and unstructured data in a ""Software as a Service"" architecture (SaaS). The research environment addresses requirements for the quantitative evaluation of large amounts of qualitative data withtextminingmethods as well as requirements for the reproducibility of data-driven research designs in the social sciences. For this, the iLCM research environment comprises two central components. First, the Leipzig Corpus Miner (LCM), a decentralized SaaS application for the analysis of large amounts of newstextsdeveloped in a previous Digital Humanities project. Second, thetextminingtools implemented in the LCM are extended by an ""Open Research Computing"" (ORC) environment for executable script documents, so-called ""notebooks"". This novel integration allows to combine generic, high-performance methods to process large amounts of unstructuredtextdata and with individual program scripts to address specific research requirements in computational social science and digital humanities.","['Andreas Niekler', 'Arnim Bleier', 'Christian Kahmann', 'Lisa Posch', 'Gregor Wiedemann', 'Kenan Erdogan', 'Gerhard Heyer', 'Markus Strohmaier']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.11404,Anomali
Core Conflictual Relationship: Text Mining to Discover What and When,"Following detailed presentation of the Core Conflictual Relationship Theme (CCRT), there is the objective of relevant methods for what has been described as verbalization and visualization of data. Such is also termed dataminingandtextmining, and knowledge discovery in data. The Correspondence Analysis methodology, also termed Geometric Data Analysis, is shown in a case study to be comprehensive and revealing. Computational efficiency depends on how the analysis process is structured. For both illustrative and revealing aspects of the case study here, relatively extensive dream reports are used. This Geometric Data Analysis confirms the validity of CCRT method.","['Fionn Murtagh', 'Giuseppe Iurato']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.11140,Anomali
Effects of Social Bots in the Iran-Debate on Twitter,"2018 started with massive protests in Iran, bringing back the impressions of the so called ""Arab Spring"" and it's revolutionary impact for the Maghreb states, Syria and Egypt. Many reports and scientific examinations considered online social networks (OSN's) such as Twitter or Facebook to play a critical role in the opinion making of people behind those protests. Beside that, there is also evidence for directed manipulation of opinion with the help of social bots and fake accounts. So, it is obvious to ask, if there is an attempt to manipulate the opinion-making process related to the Iranian protest in OSN by employing social bots, and how such manipulations will affect the discourse as a whole. Based on a sample of ca. 900,000 Tweets relating to the topic ""Iran"" we show, that there are Twitter profiles, that have to be considered as social bot accounts. By usingtextminingmethods, we show that these social bots are responsible for negative sentiment in the debate. Thereby, we would like to illustrate a detectable effect of social bots on political discussions on Twitter.","['Andree Thieltges', 'Orestis Papakyriakopoulos', 'Juan Carlos Medina Serrano', 'Simon Hegelich']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.10105,Anomali
EgoCoder: Intelligent Program Synthesis with Hierarchical Sequential Neural Network Model,"Programming has been an important skill for researchers and practitioners in computer science and other related areas. To learn basic programing skills, a long-time systematic training is usually required for beginners. According to a recent market report, the computer software market is expected to continue expanding at an accelerating speed, but the market supply of qualified software developers can hardly meet such a huge demand. In recent years, the surge oftextgeneration research works provides the opportunities to address such a dilemma through automatic program synthesis. In this paper, we propose to make our try to solve the program synthesis problem from a dataminingperspective. To address the problem, a novel generative model, namely EgoCoder, will be introduced in this paper. EgoCoder effectively parses program code into abstract syntax trees (ASTs), where the tree nodes will contain the program code/comment content and the tree structure can capture the program logic flows. Based on a new unit model called Hsu, EgoCoder can effectively capture both the hierarchical and sequential patterns in the program ASTs. Extensive experiments will be done to compare EgoCoder with the state-of-the-arttextgeneration methods, and the experimental results have demonstrated the effectiveness of EgoCoder in addressing the program synthesis problem.","['Jiawei Zhang', 'Limeng Cui', 'Fisher B. Gouza']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.08747,Anomali
Detecting cyber threats through social network analysis: short survey,"This article considers a short survey of basic methods of social networks analysis, which are used for detecting cyber threats. The main types of social network threats are presented. Basic methods of graph theory and datamining, that deals with social networks analysis are described. Typical security tasks of social network analysis, such as community detection in network, detection of leaders in communities, detection experts in networks, clusteringtextinformation and others are considered.","['Lyudmyla Kirichenko', 'Tamara Radivilova', 'Anders Carlsson']","Lyudmyla Kirichenko, Tamara Radivilova, Anders Carlsson. Detecting cyber threats through social network analysis: short survey. SocioEconomic Challenges, Volume 1, Issue 1, 2017. pp.20-34",arXiv,2018,https://doi.org/10.48550/arXiv.1805.06680,Anomali
CLINIQA: A Machine Intelligence Based Clinical Question Answering System,"The recent developments in the field of biomedicine have made large volumes of biomedical literature available to the medical practitioners. Due to the large size and lack of efficient searching strategies, medical practitioners struggle to obtain necessary information available in the biomedical literature. Moreover, the most sophisticated search engines of age are not intelligent enough to interpret the clinicians' questions. These facts reflect the urgent need of an information retrieval system that accepts the queries from medical practitioners' in natural language and returns the answers quickly and efficiently. In this paper, we present an implementation of a machine intelligence based CLINIcal Question Answering system (CLINIQA) to answer medical practitioner's questions. The system was rigorously evaluated on differenttextminingalgorithms and the best components for the system were selected. The system makes use of Unified Medical Language System for semantic analysis of both questions and medical documents. In addition, the system employs supervised machine learning algorithms for classification of the documents, identifying the focus of the question and answer selection. Effective domain-specific heuristics are designed for answer ranking. The performance evaluation on hundred clinical questions shows the effectiveness of our approach.","['M A H Zahid', 'Ankush Mittal', 'R. C. Joshi', 'G. Atluri']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.05927,Anomali
CARL: Content-Aware Representation Learning for Heterogeneous Networks,"Heterogeneous networks not only present a challenge of heterogeneity in the types of nodes and relations, but also the attributes and content associated with the nodes. While recent works have looked at representation learning on homogeneous and heterogeneous networks, there is no work that has collectively addressed the following challenges: (a) the heterogeneous structural information of the network consisting of multiple types of nodes and relations; (b) the unstructured semantic content (e.g.,text) associated with nodes; and (c) online updates due to incoming new nodes in growing network. We address these challenges by developing a Content-Aware Representation Learning model (CARL). CARL performs joint optimization of heterogeneous SkipGram and deep semantic encoding for capturing both heterogeneous structural closeness and unstructured semantic relations among all nodes, as function of node content, that exist in the network. Furthermore, an additional online update module is proposed for efficiently learning representations of incoming nodes. Extensive experiments demonstrate that CARL outperforms state-of-the-art baselines in various heterogeneous networkminingtasks, such as link prediction, document retrieval, node recommendation and relevance search. We also demonstrate the effectiveness of the CARL's online update module through a category visualization study.","['Chuxu Zhang', 'Ananthram Swami', 'Nitesh V. Chawla']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.04983,Anomali
Towards Autonomous Reinforcement Learning: Automatic Setting of Hyper-parameters using Bayesian Optimization,"With the increase of machine learning usage by industries and scientific communities in a variety of tasks such astextmining, image recognition and self-driving cars, automatic setting of hyper-parameter in learning algorithms is a key factor for achieving satisfactory performance regardless of user expertise in the inner workings of the techniques and methodologies. In particular, for a reinforcement learning algorithm, the efficiency of an agent learning a control policy in an uncertain environment is heavily dependent on the hyper-parameters used to balance exploration with exploitation. In this work, an autonomous learning framework that integrates Bayesian optimization with Gaussian process regression to optimize the hyper-parameters of a reinforcement learning algorithm, is proposed. Also, a bandits-based approach to achieve a balance between computational costs and decreasing uncertainty about the Q-values, is presented. A gridworld example is used to highlight how hyper-parameter configurations of a learning algorithm (SARSA) are iteratively improved based on two performance functions.","['Juan Cruz Barsce', 'Jorge A. Palombarini', 'Ernesto C. Martínez']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.04748,Anomali
Argument Harvesting Using Chatbots,"Much research in computational argumentation assumes that arguments and counterarguments can be obtained in some way. Yet, to improve and apply models of argument, we need methods for acquiring them. Current approaches include argumentminingfromtext, hand coding of arguments by researchers, or generating arguments from knowledge bases. In this paper, we propose a new approach, which we call argument harvesting, that uses a chatbot to enter into a dialogue with a participant to get arguments and counterarguments from him or her. Because it is automated, the chatbot can be used repeatedly in many dialogues, and thereby it can generate a large corpus. We describe the architecture of the chatbot, provide methods for managing a corpus of arguments and counterarguments, and an evaluation of our approach in a case study concerning attitudes of women to participation in sport.","['Lisa A. Chalaguine', 'Anthony Hunter', 'Henry W. W. Potts', 'Fiona L. Hamilton']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.04253,Anomali
Text-mining and ontologies: new approaches to knowledge discovery of microbial diversity,"Microbiology research has access to a very large amount of public information on the habitats of microorganisms. Many areas of microbiology research uses this information, primarily in biodiversity studies. However the habitat information is expressed in unstructured natural language form, which hinders its exploitation at large-scale. It is very common for similar habitats to be described by different terms, which makes them hard to compare automatically, e.g. intestine and gut. The use of a common reference to standardize these habitat descriptions as claimed by (Ivana et al., 2010) is a necessity. We propose the ontology called OntoBiotope that we have been developing since 2010. The OntoBiotope ontology is in a formal machine-readable representation that enables indexing of information as well as conceptualization and reasoning.","['Claire Nédellec', 'Robert Bossy', 'Estelle Chaix', 'Louise Deléger']","Proceedings of the 4th International Microbial Diversity Conference. pp. 221-227, ed. Marco Gobetti. Pub. Simtra. ISBN 978-88-943010-0-7, Bari, October 2017",arXiv,2018,https://doi.org/10.48550/arXiv.1805.04107,Anomali
Players Movements and Team Shooting Performance: a Data Mining approach for Basketball,"In the domain of Sport Analytics, Global Positioning Systems devices are intensively used as they permit to retrieve players' movements. Team sports' managers and coaches are interested on the relation between players' patterns of movements and team performance, in order to better manage their team. In this paper we propose a Cluster Analysis and Multidimensional Scaling approach to find and describe separate patterns of players movements. Using real data of multiple professional basketball teams, we find, consistently over different case studies, that in the defensive clusters players are close one to another while the transition cluster are characterized by a large space among them. Moreover, we find the pattern of players' positioning that produce the best shooting performance.",['Rodolfo Metulini'],,arXiv,2018,https://doi.org/10.48550/arXiv.1805.02501,Anomali
Semi-orthogonal Non-negative Matrix Factorization with an Application in Text Mining,"Emergency Department (ED) crowding is a worldwide issue that affects the efficiency of hospital management and the quality of patient care. This occurs when the request for an admit ward-bed to receive a patient is delayed until an admission decision is made by a doctor. To reduce the overcrowding and waiting time of ED, we build a classifier to predict the disposition of patients using manually-typed nurse notes collected during triage, thereby allowing hospital staff to begin necessary preparation beforehand. However, these triage notes involve high dimensional, noisy, and also sparsetextdata which makes model fitting and interpretation difficult. To address this issue, we propose the semi-orthogonal non-negative matrix factorization (SONMF) for both continuous and binary design matrices to first bi-cluster the patients and words into a reduced number of topics. The subjects can then be interpreted as a non-subtractive linear combination of orthogonal basis topic vectors. These generated topic vectors provide the hospital with a direct understanding of the cause of admission. We show that by using a transformation of basis, the classification accuracy can be further increased compared to the conventional bag-of-words model and alternative matrix factorization approaches. Through simulated data experiments, we also demonstrate that the proposed method outperforms other non-negative matrix factorization (NMF) methods in terms of factorization accuracy, rate of convergence, and degree of orthogonality.","['Jack Yutong Li', 'Ruoqing Zhu', 'Annie Qu', 'Han Ye', 'Zhankun Sun']",,arXiv,2019,https://doi.org/10.48550/arXiv.1805.02306,Anomali
Learning Patient Representations from Text,"Miningelectronic health records for patients who satisfy a set of predefined criteria is known in medical informatics as phenotyping. Phenotyping has numerous applications such as outcome prediction, clinical trial recruitment, and retrospective studies. Supervised machine learning for phenotyping typically relies on sparse patient representations such as bag-of-words. We consider an alternative that involves learning patient representations. We develop a neural network model for learning patient representations and show that the learned representations are general enough to obtain state-of-the-art performance on a standard comorbidity detection task.","['Dmitriy Dligach', 'Timothy Miller']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.02096,Anomali
SynTF: Synthetic and Differentially Private Term Frequency Vectors for Privacy-Preserving Text Mining,"Textminingand information retrieval techniques have been developed to assist us with analyzing, organizing and retrieving documents with the help of computers. In many cases, it is desirable that the authors of such documents remain anonymous: Search logs can reveal sensitive details about a user, critical articles or messages about a company or government might have severe or fatal consequences for a critic, and negative feedback in customer surveys might negatively impact business relations if they are identified. Simply removing personally identifying information from a document is, however, insufficient to protect the writer's identity: Given some referencetextsof suspect authors, so-called authorship attribution methods can reidentfy the author from thetextitself.
  One of the most prominent models to represent documents in many commontextminingand information retrieval tasks is the vector space model where each document is represented as a vector, typically containing its term frequencies or related quantities. We therefore propose an automatedtextanonymization approach that produces synthetic term frequency vectors for the input documents that can be used in lieu of the original vectors. We evaluate our method on an exemplarytextclassification task and demonstrate that it only has a low impact on its accuracy. In contrast, we show that our method strongly affects authorship attribution techniques to the level that they become infeasible with a much stronger decline in accuracy. Other than previous authorship obfuscation methods, our approach is the first that fulfills differential privacy and hence comes with a provable plausible deniability guarantee.","['Benjamin Weggenmann', 'Florian Kerschbaum']",,arXiv,2018,https://doi.org/10.48550/arXiv.1805.00904,Anomali
Hyperparameter Optimization for Effort Estimation,"Software analytics has been widely used in software engineering for many tasks such as generating effort estimates for software projects. One of the ""black arts"" of software analytics is tuning the parameters controlling a dataminingalgorithm. Such hyperparameter optimization has been widely studied in other software analytics domains (e.g. defect prediction andtextmining) but, so far, has not been extensively explored for effort estimation. Accordingly, this paper seeks simple, automatic, effective and fast methods for finding good tunings for automatic software effort estimation.
  We introduce a hyperparameter optimization architecture called OIL (Optimized Inductive Learning). We test OIL on a wide range of hyperparameter optimizers using data from 945 software projects. After tuning, large improvements in effort estimation accuracy were observed (measured in terms of standardized accuracy).
  From those results, we recommend using regression trees (CART) tuned by different evolution combine with default analogy-based estimator. This particular combination of learner and optimizers often achieves in a few hours what other optimizers need days to weeks of CPU time to accomplish.
  An important part of this analysis is its reproducibility and refutability. All our scripts and data are on-line. It is hoped that this paper will prompt and enable much more research on better methods to tune software effort estimators.","['Tianpei Xia', 'Rahul Krishna', 'Jianfeng Chen', 'George Mathew', 'Xipeng Shen', 'Tim Menzies']",,arXiv,2019,https://doi.org/10.48550/arXiv.1805.00336,Anomali
Q-Map: Clinical Concept Mining from Clinical Documents,"Over the past decade, there has been a steep rise in the data-driven analysis in major areas of medicine, such as clinical decision support system, survival analysis, patient similarity analysis, image analytics etc. Most of the data in the field are well-structured and available in numerical or categorical formats which can be used for experiments directly. But on the opposite end of the spectrum, there exists a wide expanse of data that is intractable for direct analysis owing to its unstructured nature which can be found in the form of discharge summaries, clinical notes, procedural notes which are in human written narrative format and neither have any relational model nor any standard grammatical structure. An important step in the utilization of thesetextsfor such studies is to transform and process the data to retrieve structured information from the haystack of irrelevant data using information retrieval and dataminingtechniques. To address this problem, the authors present Q-Map in this paper, which is a simple yet robust system that can sift through massive datasets with unregulated formats to retrieve structured information aggressively and efficiently. It is backed by an effectiveminingtechnique which is based on a string matching algorithm that is indexed on curated knowledge sources, that is both fast and configurable. The authors also briefly examine its comparative performance with MetaMap, one of the most reputed tools for medical concepts retrieval and present the advantages the former displays over the latter.","['Sheikh Shams Azam', 'Manoj Raju', 'Venkatesh Pagidimarri', 'Vamsi Kasivajjala']","International Journal of Computer and Information Engineering, 12(9), 2018, 691 - 696",arXiv,2018,https://doi.org/10.48550/arXiv.1804.11149,Anomali
"Can You Explain That, Better? Comprehensible Text Analytics for SE Applications","Textminingmethods are used for a wide range of Software Engineering (SE) tasks. The biggest challenge oftextminingis high dimensional data, i.e., a corpus of documents can contain $10^4$ to $10^6$ unique words. To address this complexity, some very convolutedtextminingmethods have been applied. Is that complexity necessary? Are there simpler ways to quickly generate models that perform as well as the more convoluted methods and also be human-readable?
  To answer these questions, we explore a combination of LDA (Latent Dirichlet Allocation) and FFTs (Fast and Frugal Trees) to classify NASA software bug reports from six different projects. Designed using principles from psychological science, FFTs return very small models that are human-comprehensible. When compared to the commonly usedtextminingmethod and a recent state-of-the-art-system (search-based SE method that automatically tune the control parameters of LDA), these FFT models are very small (a binary tree of depth $d = 4$ that references only 4 topics) and hence easy to understand. They were also faster to generate and produced similar or better severity predictions.
  Hence we can conclude that, at least for datasets explored here, convolutedtextminingmodels can be deprecated in favor of simpler method such as LDA+FFTs. At the very least, we recommend LDA+FFTs (a) when humans need to read, understand, and audit a model or (b) as an initial baseline method for the SE researchers exploringtextartifacts from software projects.","['Amritanshu Agrawal', 'Huy Tu', 'Tim Menzies']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.10657,Anomali
Extracting Parallel Paragraphs from Common Crawl,"Most of the current methods forminingparalleltextsfrom the web assume that web pages of web sites share same structure across languages. We believe that there still exists a non-negligible amount of parallel data spread across sources not satisfying this assumption. We propose an approach based on a combination of bivec (a bilingual extension of word2vec) and locality-sensitive hashing which allows us to efficiently identify pairs of parallel segments located anywhere on pages of a given web domain, regardless their structure. We validate our method on realigning segments from a large parallel corpus. Another experiment with real-world data provided by Common Crawl Foundation confirms that our solution scales to hundreds of terabytes large set of web-crawled data.","['Jakub Kúdela', 'Irena Holubová', 'Ondřej Bojar']","The Prague Bulletin of Mathematical Linguistics, Volume 107, Issue 1, Pages 39-56, ISSN (Online) 1804-0462 (2017)",arXiv,2018,https://doi.org/10.48550/arXiv.1804.10413,Anomali
Integrating Local Context and Global Cohesiveness for Open Information Extraction,"Extracting entities and their relations fromtextis an important task for understanding massivetextcorpora. Open information extraction (IE) systemsminerelation tuples (i.e., entity arguments and a predicate string to describe their relation) from sentences. These relation tuples are not confined to a predefined schema for the relations of interests. However, current Open IE systems focus on modeling local context information in a sentence to extract relation tuples, while ignoring the fact that global statistics in a large corpus can be collectively leveraged to identify high-quality sentence-level extractions. In this paper, we propose a novel Open IE system, called ReMine, which integrates local context signals and global structural signals in a unified, distant-supervision framework. Leveraging facts from external knowledge bases as supervision, the new system can be applied to many different domains to facilitate sentence-level tuple extractions using corpus-level statistics. Our system operates by solving a joint optimization problem to unify (1) segmenting entity/relation phrases in individual sentences based on local context; and (2) measuring the quality of tuples extracted from individual sentences with a translating-based objective. Learning the two subtasks jointly helps correct errors produced in each subtask so that they can mutually enhance each other. Experiments on two real-world corpora from different domains demonstrate the effectiveness, generality, and robustness of ReMine when compared to state-of-the-art open IE systems.","['Qi Zhu', 'Xiang Ren', 'Jingbo Shang', 'Yu Zhang', 'Ahmed El-Kishky', 'Jiawei Han']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.09931,Anomali
Commonsense mining as knowledge base completion? A study on the impact of novelty,"Commonsense knowledge bases such as ConceptNet represent knowledge in the form of relational triples. Inspired by the recent work by Li et al., we analyse if knowledge base completion models can be used tominecommonsense knowledge from rawtext. We propose novelty of predicted triples with respect to the training set as an important factor in interpreting results. We critically analyse the difficulty ofminingnovel commonsense knowledge, and show that a simple baseline method outperforms the previous state of the art on predicting more novel.","['Stanisław Jastrzębski', 'Dzmitry Bahdanau', 'Seyedarian Hosseini', 'Michael Noukhovitch', 'Yoshua Bengio', 'Jackie Chi Kit Cheung']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.09259,Anomali
Label-aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition,"We study the problem of named entity recognition (NER) from electronic medical records, which is one of the most fundamental and critical problems for medicaltextmining. Medical records which are written by clinicians from different specialties usually contain quite different terminologies and writing styles. The difference of specialties and the cost of human annotation makes it particularly difficult to train a universal medical NER system. In this paper, we propose a label-aware double transfer learning framework (La-DTL) for cross-specialty NER, so that a medical NER system designed for one specialty could be conveniently applied to another one with minimal annotation efforts. The transferability is guaranteed by two components: (i) we propose label-aware MMD for feature representation transfer, and (ii) we perform parameter transfer with a theoretical upper bound which is also label aware. We conduct extensive experiments on 12 cross-specialty NER tasks. The experimental results demonstrate that La-DTL provides consistent accuracy improvement over strong baselines. Besides, the promising experimental results on non-medical NER scenarios indicate that La-DTL is potential to be seamlessly adapted to a wide range of NER tasks.","['Zhenghui Wang', 'Yanru Qu', 'Liheng Chen', 'Jian Shen', 'Weinan Zhang', 'Shaodian Zhang', 'Yimei Gao', 'Gen Gu', 'Ken Chen', 'Yong Yu']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.09021,Anomali
Data-driven Summarization of Scientific Articles,"Data-driven approaches to sequence-to-sequence modelling have been successfully applied to shorttextsummarization of news articles. Such models are typically trained on input-summary pairs consisting of only a single or a few sentences, partially due to limited availability of multi-sentence training data. Here, we propose to use scientific articles as a new milestone fortextsummarization: large-scale training data come almost for free with two types of high-quality summaries at different levels - the title and the abstract. We generate two novel multi-sentence summarization datasets from scientific articles and test the suitability of a wide range of existing extractive and abstractive neural network-based summarization approaches. Our analysis demonstrates that scientific papers are suitable for data-driventextsummarization. Our results could serve as valuable benchmarks for scaling sequence-to-sequence models to very long sequences.","['Nikola I. Nikolov', 'Michael Pfeiffer', 'Richard H. R. Hahnloser']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.08875,Anomali
Using Categorical Features in Mining Bug Tracking Systems to Assign Bug Reports,"Most bug assignment approaches utilizetextclassification and information retrieval techniques. These approaches use the textual contents of bug reports to build recommendation models. The textual contents of bug reports are usually of high dimension and noisy source of information. These approaches suffer from low accuracy and high computational needs. In this paper, we investigate whether using categorical fields of bug reports, such as component to which the bug belongs, are appropriate to represent bug reports instead of textual description. We build a classification model by utilizing the categorical features, as a representation, for the bug report. The experimental evaluation is conducted using three projects namely NetBeans, Freedesktop, and Firefox. We compared this approach with two machine learning based bug assignment approaches. The evaluation shows that using the textual contents of bug reports is important. In addition, it shows that the categorical features can improve the classification accuracy.","['Mamdouh Alenezi', 'Shadi Banitaan', 'Mohammad Zarour']","International Journal of Software Engineering & Applications (IJSEA), Vol.9, No.2, March 2018",arXiv,2018,https://doi.org/10.48550/arXiv.1804.07803,Anomali
PMC text mining subset in BioC: 2.3 million full text articles and growing,"Interest in fulltextminingbiomedical research articles is growing. NCBI provides the PMC Open Access and Author Manuscript sets of articles which are available fortextmining. We have made all of these articles available in BioC, an XML and JSON format which is convenient for sharingtext, annotations, and relations. These articles are available both via ftp for bulk download and via a Web API for updates or more focused collection. Availability: https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/","['Donald C. Comeau', 'Chih-Hsuan Wei', 'Rezarta Islamaj Doğan', 'Zhiyong Lu']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.05957,Anomali
Mining actionable information from security forums: the case of malicious IP addresses,"The goal of this work is to systematically extract information from hacker forums, whose information would be in general described as unstructured: thetextof a post is not necessarily following any writing rules. By contrast, many security initiatives and commercial entities are harnessing the readily public information, but they seem to focus on structured sources of information. Here, we focus on the problem of identifying malicious IP addresses, among the IP addresses which are reported in the forums. We develop a method to automate the identification of malicious IP addresses with the design goal of being independent of external sources. A key novelty is that we use a matrix decomposition method to extract latent features of the behavioral information of the users, which we combine with textual information from the related posts. A key design feature of our technique is that it can be readily applied to different language forums, since it does not require a sophisticated Natural Language Processing approach. In particular, our solution only needs a small number of keywords in the new language plus the users behavior captured by specific features. We also develop a tool to automate the data collection from security forums. Using our tool, we collect approximately 600K posts from 3 different forums. Our method exhibits high classification accuracy, while the precision of identifying malicious IP in post is greater than 88% in all three forums. We argue that our method can provide significantly more information: we find up to 3 times more potentially malicious IP address compared to the reference blacklist VirusTotal. As the cyber-wars are becoming more intense, having early accesses to useful information becomes more imperative to remove the hackers first-move advantage, and our work is a solid step towards this direction.","['Joobin Gharibshah', 'Tai Ching Li', 'Andre Castro', 'Konstantinos Pelechrinis', 'Evangelos E. Papalexakis', 'Michalis Faloutsos']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.04800,Anomali
Predicting Good Configurations for GitHub and Stack Overflow Topic Models,"Software repositories contain large amounts of textual data, ranging from source code comments and issue descriptions to questions, answers, and comments on Stack Overflow. To make sense of this textual data, topic modelling is frequently used as atext-miningtool for the discovery of hidden semantic structures intextbodies. Latent Dirichlet allocation (LDA) is a commonly used topic model that aims to explain the structure of a corpus by groupingtexts. LDA requires multiple parameters to work well, and there are only rough and sometimes conflicting guidelines available on how these parameters should be set. In this paper, we contribute (i) a broad study of parameters to arrive at good local optima for GitHub and Stack Overflowtextcorpora, (ii) an a-posteriori characterisation oftextcorpora related to eight programming languages, and (iii) an analysis of corpus feature importance via per-corpus LDA configuration. We find that (1) popular rules of thumb for topic modelling parameter configuration are not applicable to the corpora used in our experiments, (2) corpora sampled from GitHub and Stack Overflow have different characteristics and require different configurations to achieve good model fit, and (3) we can predict good configurations for unseen corpora reliably. These findings support researchers and practitioners in efficiently determining suitable configurations for topic modelling when analysing textual data contained in software repositories.","['Christoph Treude', 'Markus Wagner']",,arXiv,2019,https://doi.org/10.48550/arXiv.1804.04749,Anomali
Deep Attention Model for Triage of Emergency Department Patients,"Optimization of patient throughput and wait time in emergency departments (ED) is an important task for hospital systems. For that reason, Emergency Severity Index (ESI) system for patient triage was introduced to help guide manual estimation of acuity levels, which is used by nurses to rank the patients and organize hospital resources. However, despite improvements that it brought to managing medical resources, such triage system greatly depends on nurse's subjective judgment and is thus prone to human errors. Here, we propose a novel deep model based on the word attention mechanism designed for predicting a number of resources an ED patient would need. Our approach incorporates routinely available continuous and nominal (structured) data with medicaltext(unstructured) data, including patient's chief complaint, past medical history, medication list, and nurse assessment collected for 338,500 ED visits over three years in a large urban hospital. Using both structured and unstructured data, the proposed approach achieves the AUC of $\sim 88\%$ for the task of identifying resource intensive patients (binary classification), and the accuracy of $\sim 44\%$ for predicting exact category of number of resources (multi-class classification task), giving an estimated lift over nurses' performance by 16\% in accuracy. Furthermore, the attention mechanism of the proposed model provides interpretability by assigning attention scores for nurses' notes which is crucial for decision making and implementation of such approaches in the real systems working on human health.","['Djordje Gligorijevic', 'Jelena Stojanovic', 'Wayne Satz', 'Ivan Stojkovic', 'Kathrin Schreyer', 'Daniel Del Portal', 'Zoran Obradovic']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.03240,Anomali
Faster light with competing absorption and gain,"We experimentally investigate the propagation of optical pulses through a fast-light medium with competing absorption and gain. The combination of strong absorption and optical amplification in a potassium-based four-wave mixing process results in pulse peak advancements up to $88\%$ of the input pulse width, more than $35 \times$ that which is achievable without competing absorption. We show that the enhancement occurs even when the total gain of the four-wave mixer is unity, thereby rendering the medium transparent. By varying the pulse width, we observe a transition between fast and slow light, and show that fast light is optimized for large pulse widths.","['Jon D. Swaim', 'Ryan T. Glasser']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.01541,Anomali
Incorporating Word Embeddings into Open Directory Project based Large-scale Classification,"Recently, implicit representation models, such as embedding or deep learning, have been successfully adopted totextclassification task due to their outstanding performance. However, these approaches are limited to small- or moderate-scaletextclassification. Explicit representation models are often used in a large-scaletextclassification, like the Open Directory Project (ODP)-basedtextclassification. However, the performance of these models is limited to the associated knowledge bases. In this paper, we incorporate word embeddings into the ODP-based large-scale classification. To this end, we first generate category vectors, which represent the semantics of ODP categories by jointly modeling word embeddings and the ODP-basedtextclassification. We then propose a novel semantic similarity measure, which utilizes the category and word vectors obtained from the joint model and word embeddings, respectively. The evaluation results clearly show the efficacy of our methodology in large-scaletextclassification. The proposed scheme exhibits significant improvements of 10% and 28% in terms of macro-averaging F1-score and precision at k, respectively, over state-of-the-art techniques.","['Kang-Min Kim', 'Aliyeva Dinara', 'Byung-Ju Choi', 'SangKeun Lee']",,arXiv,2018,https://doi.org/10.48550/arXiv.1804.00828,Anomali
Sensing the Chinese Diaspora: How Mobile Apps Can Provide Insights into Global Migration Flows,"Many countries today have ""country-centric mobile apps"" which are mobile apps that are primarily used by residents of a specific country. Many of these country-centric apps also include a location-based service which takes advantage of the smartphone's API access to the smartphone's current GPS location. In this paper, we investigate how such country-centric apps with location-based services can be employed to study the diaspora associated with ethnic and cultural groups. Our methodology combines GPS hacking, automated task tools for mobile phones, and OCR to generate migration statistics for diaspora. As a case study, we apply our methodology to WeChat, an enormously popular app within China and among ethnic Chinese worldwide. Using WeChat, we collect data about the Chinese diaspora in 32 cities. The combined data provides interesting insights to the modern Chinese diaspora and how it has changed in recent years.","['Minhui Xue', 'Xin Yuan', 'Heather Lee', 'Keith Ross']","IEEE International Conference on Data Mining (ICDM) Workshop, 2019",arXiv,2019,https://doi.org/10.48550/arXiv.1803.08256,Anomali
$ρ$-hot Lexicon Embedding-based Two-level LSTM for Sentiment Analysis,"Sentiment analysis is a key component in varioustextminingapplications. Numerous sentiment classification techniques, including conventional and deep learning-based methods, have been proposed in the literature. In most existing methods, a high-quality training set is assumed to be given. Nevertheless, constructing a high-quality training set that consists of highly accurate labels is challenging in real applications. This difficulty stems from the fact thattextsamples usually contain complex sentiment representations, and their annotation is subjective. We address this challenge in this study by leveraging a new labeling strategy and utilizing a two-level long short-term memory network to construct a sentiment classifier. Lexical cues are useful for sentiment analysis, and they have been utilized in conventional studies. For example, polar and privative words play important roles in sentiment analysis. A new encoding strategy, that is, $ρ$-hot encoding, is proposed to alleviate the drawbacks of one-hot encoding and thus effectively incorporate useful lexical cues. We compile three Chinese data sets on the basis of our label strategy and proposed methodology. Experiments on the three data sets demonstrate that the proposed method outperforms state-of-the-art algorithms.","['Ou Wu', 'Tao Yang', 'Mengyang Li', 'Ming Li']",,arXiv,2018,https://doi.org/10.48550/arXiv.1803.07771,Anomali
SOTorrent: Reconstructing and Analyzing the Evolution of Stack Overflow Posts,"Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-formtexton a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, ortextsurrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individualtextor code blocks. It connects SO posts to other platforms by aggregating URLs fromtextblocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history oftextand code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surroundingtext. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub.","['Sebastian Baltes', 'Lorik Dumani', 'Christoph Treude', 'Stephan Diehl']",,arXiv,2018,https://doi.org/10.48550/arXiv.1803.07311,Anomali
Natural Language or Not (NLoN) - A Package for Software Engineering Text Analysis Pipeline,"The use of natural language processing (NLP) is gaining popularity in software engineering. In order to correctly perform NLP, we must pre-process the textual information to separate natural language from other information, such as log messages, that are often part of the communication in software engineering. We present a simple approach for classifying whether some textual input is natural language or not. Although our NLoN package relies on only 11 language features and character tri-grams, we are able to achieve an area under the ROC curve performances between 0.976-0.987 on three different data sources, with Lasso regression from Glmnet as our learner and two human raters for providing ground truth. Cross-source prediction performance is lower and has more fluctuation with top ROC performances from 0.913 to 0.980. Compared with prior work, our approach offers similar performance but is considerably more lightweight, making it easier to apply in software engineeringtextminingpipelines. Our source code and data are provided as an R-package for further improvements.","['Mika V. Mäntylä', 'Fabio Calefato', 'Maelick Claes']","MSR '18: 15th International Conference on Mining Software Repositories, May 28--29, 2018 Gothenburg, Sweden",arXiv,2018,https://doi.org/10.48550/arXiv.1803.07292,Anomali
Capturing the influence of geopolitical ties from Wikipedia with reduced Google matrix,"Interactions between countries originate from diverse aspects such as geographic proximity, trade, socio-cultural habits, language, religions, etc. Geopolitics studies the influence of a country's geographic space on its political power and its relationships with other countries.
  This work reveals the potential of Wikipediaminingfor geopolitical study. Actually, Wikipedia offers solid knowledge and strong correlations among countries by linking web pages together for different types of information (e.g. economical, historical, political, and many others). The major finding of this paper is to show that meaningful results on the influence of country ties can be extracted from the hyperlinked structure of Wikipedia. We leverage a novel stochastic matrix representation of Markov chains of complex directed networks called the reduced Google matrix theory. For a selected small size set of nodes, the reduced Google matrix concentrates direct and indirect links of the million-node sized Wikipedia network into a small Perron-Frobenius matrix keeping the PageRank probabilities of the global Wikipedia network. We perform a novel sensitivity analysis that leverages this reduced Google matrix to characterize the influence of relationships between countries from the global network. We apply this analysis to two chosen sets of countries (i.e. the set of 27 European Union countries and a set of 40 top worldwide countries). We show that with our sensitivity analysis we can exhibit easily very meaningful information on geopolitics from five different Wikipedia editions (English, Arabic, Russian, French and German).","['Samer El Zant', 'Katia Jaffrès-Runser', 'Dima Shepelyansky']",PLoS ONE 13(8): 2018,arXiv,2018,https://doi.org/10.48550/arXiv.1803.05336,Anomali
How to evaluate sentiment classifiers for Twitter time-ordered data?,"Social media are becoming an increasingly important source of information about the public mood regarding issues such as elections, Brexit, stock market, etc. In this paper we focus on sentiment classification of Twitter data. Construction of sentiment classifiers is a standardtextminingtask, but here we address the question of how to properly evaluate them as there is no settled way to do so. Sentiment classes are ordered and unbalanced, and Twitter produces a stream of time-ordered data. The problem we address concerns the procedures used to obtain reliable estimates of performance measures, and whether the temporal ordering of the training and test data matters. We collected a large set of 1.5 million tweets in 13 European languages. We created 138 sentiment models and out-of-sample datasets, which are used as a gold standard for evaluations. The corresponding 138 in-sample datasets are used to empirically compare six different estimation procedures: three variants of cross-validation, and three variants of sequential validation (where test set always follows the training set). We find no significant difference between the best cross-validation and sequential validation. However, we observe that all cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it. Standard cross-validation with random selection of examples is significantly worse than the blocked cross-validation, and should not be used to evaluate classifiers in time-ordered data scenarios.","['Igor Mozetič', 'Luis Torgo', 'Vitor Cerqueira', 'Jasmina Smailović']","PLoS ONE 13(3): e0194317, 2018",arXiv,2018,https://doi.org/10.48550/arXiv.1803.05160,Anomali
IDEL: In-Database Entity Linking with Neural Embeddings,"We present a novel architecture, In-Database Entity Linking (IDEL), in which we integrate the analytics-optimized RDBMS MonetDB with neuraltextminingabilities. Our system design abstracts core tasks of most neural entity linking systems for MonetDB. To the best of our knowledge, this is the first defacto implemented system integrating entity-linking in a database. We leverage the ability of MonetDB to support in-database-analytics with user defined functions (UDFs) implemented in Python. These functions call machine learning libraries for neuraltextmining, such as TensorFlow. The system achieves zero cost for data shipping and transformation by utilizing MonetDB's ability to embed Python processes in the database kernel and exchange data in NumPy arrays. IDEL representstextand relational data in a joint vector space with neural embeddings and can compensate errors with ambiguous entity representations. For detecting matching entities, we propose a novel similarity function based on joint neural embeddings which are learned via minimizing pairwise contrastive ranking loss. This function utilizes a high dimensional index structures for fast retrieval of matching entities. Our first implementation and experiments using the WebNLG corpus show the effectiveness and the potentials of IDEL.","['Torsten Kilias', 'Alexander Löser', 'Felix A. Gers', 'Richard Koopmanschap', 'Ying Zhang', 'Martin Kersten']",,arXiv,2018,https://doi.org/10.48550/arXiv.1803.04884,Anomali
"Text Data Mining from the Author's Perspective: Whose Text, Whose Mining, and to Whose Benefit?","Given the many technical, social, and policy shifts in access to scholarly content since the early days oftextdatamining, it is time to expand the conversation abouttextdataminingfrom concerns of the researcher wishing tominedata to include concerns of researcher-authors about how their data aremined, by whom, for what purposes, and to whose benefits.",['Christine L. Borgman'],,arXiv,2018,https://doi.org/10.48550/arXiv.1803.04552,Anomali
"Poisson Kernel-Based Clustering on the Sphere: Convergence Properties, Identifiability, and a Method of Sampling","Many applications of interest involve data that can be analyzed as unit vectors on a d-dimensional sphere. Specific examples includetextmining, in particular clustering of documents, biology, astronomy and medicine among others. Previous work has proposed a clustering method using mixtures of Poisson kernel-based distributions (PKBD) on the sphere. We prove identifiability of mixtures of the aforementioned model, convergence of the associated EM-type algorithm and study its operational characteristics. Furthermore, we propose an empirical densities distance plot for estimating the number of clusters in a PKBD model. Finally, we propose a method to simulate data from Poisson kernel-based densities and exemplify our methods via application on real data sets and simulation experiments.","['Mojgan Golzy', 'Marianthi Markatou']",,arXiv,2018,https://doi.org/10.48550/arXiv.1803.04485,Anomali
Automating Reading Comprehension by Generating Question and Answer Pairs,"Neural network-based methods represent the state-of-the-art in question generation fromtext. Existing work focuses on generating only questions fromtextwithout concerning itself with answer generation. Moreover, our analysis shows that handling rare words and generating the most appropriate question given a candidate answer are still challenges facing existing approaches. We present a novel two-stage process to generate question-answer pairs from thetext. For the first stage, we present alternatives for encoding the span of the pivotal answer in the sentence using Pointer Networks. In our second stage, we employ sequence to sequence models for question generation, enhanced with rich linguistic features. Finally, global attention and answer encoding are used for generating the question most relevant to the answer. We motivate and linguistically analyze the role of each component in our framework and consider compositions of these. This analysis is supported by extensive experimental evaluations. Using standard evaluation metrics as well as human evaluations, our experimental results validate the significant improvement in the quality of questions generated by our framework over the state-of-the-art. The technique presented here represents another step towards more automated reading comprehension assessment. We also present a live system \footnote{Demo of the system is available at \url{https://www.cse.iitb.ac.in/~vishwajeet/autoqg.html}.} to demonstrate the effectiveness of our approach.","['Vishwajeet Kumar', 'Kireeti Boorla', 'Yogesh Meena', 'Ganesh Ramakrishnan', 'Yuan-Fang Li']",,arXiv,2018,https://doi.org/10.48550/arXiv.1803.03664,Anomali
Foundations of Temporal Text Networks,"Three fundamental elements to understand human information networks are the individuals (actors) in the network, the information they exchange, that is often observable online astextcontent (emails, social media posts, etc.), and the time when these exchanges happen. An extremely large amount of research has addressed some of these aspects either in isolation or as combinations of two of them. There are also more and more works studying systems where all three elements are present, but typically using ad hoc models and algorithms that cannot be easily transfered to other contexts. To address this heterogeneity, in this article we present a simple, expressive and extensible model for temporaltextnetworks, that we claim can be used as a common ground across different types of networks and analysis tasks, and we show how simple procedures to produce views of the model allow the direct application of analysis methods already developed in other domains, from traditional dataminingto multilayer networkmining.","['Davide Vega', 'Matteo Magnani']",,arXiv,2018,https://doi.org/10.48550/arXiv.1803.02592,Anomali
VIPE: A new interactive classification framework for large sets of short texts - application to opinion mining,"This paper presents a new interactive opinionminingtool that helps users to classify large sets of shorttextsoriginated from Web opinion polls, technical forums or Twitter. From a manual multi-label pre-classification of a very limitedtextsubset, a learning algorithm predicts the labels of the remainingtextsof the corpus and thetextsmost likely associated to a selected label. Using a fast matrix factorization, the algorithm is able to handle large corpora and is well-adapted to interactivity by integrating the corrections proposed by the users on the fly. Experimental results on classical datasets of various sizes and feedbacks of users from marketing services of the telecommunication company Orange confirm the quality of the obtained results.","['Wissam Siblini', 'Frank Meyer', 'Pascale Kuntz']",,arXiv,2018,https://doi.org/10.48550/arXiv.1803.02101,Anomali
"Computational International Relations: What Can Programming, Coding and Internet Research Do for the Discipline?","Computational Social Science emerged as a highly technical and popular discipline in the last few years, owing to the substantial advances in communication technology and daily production of vast quantities of personal data. As per capita data production significantly increased in the last decade, both in terms of its size, bytes, as well as its detail, heart rate monitors, Internet connected appliances, smartphones, social scientists ability to extract meaningful social, political and demographic information from digital data also increased. A vast methodological gap exists in computational international relations, or ComInt, which refers to the use of one or a combination of tools such as datamining, natural language processing, automatedtextanalysis, web scraping, geospatial analysis and machine learning to provide larger and better organized data to test more advanced theories of IR. After providing an overview of the potentials of computational IR and how an IR scholar can establish technical proficiency in computer science, such as starting with Python, R, QGis, ArcGIS or Github, this paper will focus on some of the author's works in providing an idea for IR students on how to think about computational IR. The paper argues that computational methods transcend the methodological schism between qualitative and quantitative approaches and form a solid foundation for building truly multi method research design.",['H. Akin Unver'],,arXiv,2018,https://doi.org/10.48550/arXiv.1803.00105,Anomali
Identifying Sources and Sinks in the Presence of Multiple Agents with Gaussian Process Vector Calculus,"In systems of multiple agents, identifying the cause of observed agent dynamics is challenging. Often, these agents operate in diverse, non-stationary environments, where models rely on hand-crafted environment-specific features to infer influential regions in the system's surroundings. To overcome the limitations of these inflexible models, we present GP-LAPLACE, a technique for locating sources and sinks from trajectories in time-varying fields. Using Gaussian processes, we jointly infer a spatio-temporal vector field, as well as canonical vector calculus operations on that field. Notably, we do this from only agent trajectories without requiring knowledge of the environment, and also obtain a metric for denoting the significance of inferred causal features in the environment by exploiting our probabilistic method. To evaluate our approach, we apply it to both synthetic and real-world GPS data, demonstrating the applicability of our technique in the presence of multiple agents, as well as its superiority over existing methods.","['Adam D. Cobb', 'Richard Everett', 'Andrew Markham', 'Stephen J. Roberts']",,arXiv,2018,https://doi.org/10.48550/arXiv.1802.10446,Anomali
One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation of Text Data,"Due to recent technical and scientific advances, we have a wealth of information hidden in unstructuredtextdata such as offline/online narratives, research articles, and clinical reports. Tominethese data properly, attributable to their innate ambiguity, a Word Sense Disambiguation (WSD) algorithm can avoid numbers of difficulties in Natural Language Processing (NLP) pipeline. However, considering a large number of ambiguous words in one language or technical domain, we may encounter limiting constraints for proper deployment of existing WSD models. This paper attempts to address the problem of one-classifier-per-one-word WSD algorithms by proposing a single Bidirectional Long Short-Term Memory (BLSTM) network which by considering senses and context sequences works on all ambiguous words collectively. Evaluated on SensEval-3 benchmark, we show the result of our model is comparable with top-performing WSD algorithms. We also discuss how applying additional modifications alleviates the model fault and the need for more training data.","['Ahmad Pesaranghader', 'Ali Pesaranghader', 'Stan Matwin', 'Marina Sokolova']",,arXiv,2018,https://doi.org/10.48550/arXiv.1802.09059,Anomali
Sentiment Analysis on Speaker Specific Speech Data,"Sentiment analysis has evolved over past few decades, most of the work in it revolved around textual sentiment analysis withtextminingtechniques. But audio sentiment analysis is still in a nascent stage in the research community. In this proposed research, we perform sentiment analysis on speaker discriminated speech transcripts to detect the emotions of the individual speakers involved in the conversation. We analyzed different techniques to perform speaker discrimination and sentiment analysis to find efficient algorithms to perform this task.","['Maghilnan S', 'Rajesh Kumar M']",,arXiv,2018,https://doi.org/10.48550/arXiv.1802.06209,Anomali
Cross-topic Argument Mining from Heterogeneous Sources Using Attention-based Neural Networks,"Argumentminingis a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches to argumentminingare designed for use only with specifictexttypes and fall short when applied to heterogeneoustexts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Webtexts. We source annotations for over 25,000 instances covering eight controversial topics. The results of cross-topic experiments show that our attention-based neural network generalizes best to unseen topics and outperforms vanilla BiLSTM models by 6% in accuracy and 11% in F-score.","['Christian Stab', 'Tristan Miller', 'Iryna Gurevych']",,arXiv,2018,https://doi.org/10.48550/arXiv.1802.05758,Anomali
500+ Times Faster Than Deep Learning (A Case Study Exploring Faster Methods for Text Mining StackOverflow),"Deep learning methods are useful for high-dimensional data and are becoming widely used in many areas of software engineering. Deep learners utilizes extensive computational power and can take a long time to train-- making it difficult to widely validate and repeat and improve their results. Further, they are not the best solution in all domains. For example, recent results show that for finding related Stack Overflow posts, a tuned SVM performs similarly to a deep learner, but is significantly faster to train. This paper extends that recent result by clustering the dataset, then tuning very learners within each cluster. This approach is over 500 times faster than deep learning (and over 900 times faster if we use all the cores on a standard laptop computer). Significantly, this faster approach generates classifiers nearly as good (within 2\% F1 Score) as the much slower deep learning method. Hence we recommend this faster methods since it is much easier to reproduce and utilizes far fewer CPU resources. More generally, we recommend that before researchers release research results, that they compare their supposedly sophisticated methods against simpler alternatives (e.g applying simpler learners to build local models).","['Suvodeep Majumder', 'Nikhila Balaji', 'Katie Brey', 'Wei Fu', 'Tim Menzies']","MSR '18, Proceedings of the 15th International Conference on Mining Software Repositories, May 2018, Pages 554 to 563",arXiv,2018,https://doi.org/10.48550/arXiv.1802.05319,Anomali
Robust Continuous Co-Clustering,"Clustering consists of grouping together samples giving their similar properties. The problem of modeling simultaneously groups of samples and features is known as Co-Clustering. This paper introduces ROCCO - a Robust Continuous Co-Clustering algorithm. ROCCO is a scalable, hyperparameter-free, easy and ready to use algorithm to address Co-Clustering problems in practice over massive cross-domain datasets. It operates by learning a graph-based two-sided representation of the input matrix. The underlying proposed optimization problem is non-convex, which assures a flexible pool of solutions. Moreover, we prove that it can be solved with a near linear time complexity on the input size. An exhaustive large-scale experimental testbed conducted with both synthetic and real-world datasets demonstrates ROCCO's properties in practice: (i) State-of-the-art performance in cross-domain real-world problems including Biomedicine andTextMining; (ii) very low sensitivity to hyperparameter settings; (iii) robustness to noise and (iv) a linear empirical scalability in practice. These results highlight ROCCO as a powerful general-purpose co-clustering algorithm for cross-domain practitioners, regardless of their technical background.","['Xiao He', 'Luis Moreira-Matias']",,arXiv,2018,https://doi.org/10.48550/arXiv.1802.05036,Anomali
Analyzing the Digital Traces of Political Manipulation: The 2016 Russian Interference Twitter Campaign,"Until recently, social media was seen to promote democratic discourse on social and political issues. However, this powerful communication platform has come under scrutiny for allowing hostile actors to exploit online discussions in an attempt to manipulate public opinion. A case in point is the ongoing U.S. Congress' investigation of Russian interference in the 2016 U.S. election campaign, with Russia accused of using trolls (malicious accounts created to manipulate) and bots to spread misinformation and politically biased information. In this study, we explore the effects of this manipulation campaign, taking a closer look at users who re-shared the posts produced on Twitter by the Russian troll accounts publicly disclosed by U.S. Congress investigation. We collected a dataset with over 43 million election-related posts shared on Twitter between September 16 and October 21, 2016, by about 5.7 million distinct users. This dataset included accounts associated with the identified Russian trolls. We use label propagation to infer the ideology of all users based on the news sources they shared. This method enables us to classify a large number of users as liberal or conservative with precision and recall above 90%. Conservatives retweeted Russian trolls about 31 times more often than liberals and produced 36x more tweets. Additionally, most retweets of troll content originated from two Southern states: Tennessee and Texas. Using state-of-the-art bot detection techniques, we estimated that about 4.9% and 6.2% of liberal and conservative users respectively were bots.Textanalysis on the content shared by trolls reveals that they had a mostly conservative, pro-Trump agenda. Although an ideologically broad swath of Twitter users was exposed to Russian Trolls in the period leading up to the 2016 U.S. Presidential election, it was mainly conservatives who helped amplify their message.","['Adam Badawy', 'Emilio Ferrara', 'Kristina Lerman']","2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), Barcelona, Spain, 2018, pp. 258-265",arXiv,2018,https://doi.org/10.48550/arXiv.1802.04291,Anomali
"Quantum mechanics, ontology, and non-reflexive logics","This is a general philosophical paper where I overview some ideas concerning the non-reflexive foundations of quantum mechanics (NRFQM). By NRFQM I mean formalism and an interpretation of QM that considers an involved ontology of non-individuals as explained in thetext. Thus, I do not endorse a purely instrumentalist view of QM, but believe that it speaks of something, and then I try to show that one of the plausible views of this `something' is as entities devoid of identity conditions. Warning note: This is a revised version of a paper with the same name that was written by invitation to be published in a book titled \textit{The Mammoth Book on Quantum Mechanics Interpretations}, edited by Open Academic Press, Berlin, and having as editor a certain Ulf Edvinsson, who has invited me. The book was announced in the page of OAP and should appear by 2016. This never happened. Later I discovered that OAP is in a list of predatory editorial houses and that ""Ulf Edvinsson"" is (apparently) a fake name. Furthermore, I couldn't contact anyone responding by OAP to retire my name from the announcement of the book and for impeding them to publish the paper. I strongly apologize for such a fault, which is completelymine. Since the subject presented here has been among my preoccupations ever since I met Franscicso Antonio Doria for the first time (in 1987), it is a pleasure to dedicate the stuff to him. And of course I thank the editors for accepting this version of the paper for this book.",['Decio Krause'],,arXiv,2018,https://doi.org/10.48550/arXiv.1802.03805,Anomali
A Study of WhatsApp Usage Patterns and Prediction Models without Message Content,"Internet social networks have become a ubiquitous application allowing people to easily sharetext, pictures, and audio and video files. Popular networks include WhatsApp, Facebook, Reddit and LinkedIn. We present an extensive study of the usage of the WhatsApp social network, an Internet messaging application that is quickly replacing SMS messaging. In order to better understand people's use of the network, we provide an analysis of over 6 million messages from over 100 users, with the objective of building demographic prediction models using activity data. We performed extensive statistical and numerical analysis of the data and found significant differences in WhatsApp usage across people of different genders and ages. We also inputted the data into the Weka dataminingpackage and studied models created from decision tree and Bayesian network algorithms. We found that different genders and age demographics had significantly different usage habits in almost all message and group attributes. We also noted differences in users' group behavior and created prediction models, including the likelihood a given group would have relatively more file attachments, if a group would contain a larger number of participants, a higher frequency of activity, quicker response times and shorter messages. We were successful in quantifying and predicting a user's gender and age demographic. Similarly, we were able to predict different types of group usage. All models were built without analyzing message content. We present a detailed discussion about the specific attributes that were contained in all predictive models and suggest possible applications based on these results.","['Avi Rosenfeld', 'Sigal Sina', 'David Sarne', 'Or Avidov', 'Sarit Kraus']",,arXiv,2018,https://doi.org/10.48550/arXiv.1802.03393,Anomali
Mining Public Opinion about Economic Issues: Twitter and the U.S. Presidential Election,"Opinion polls have been the bridge between public opinion and politicians in elections. However, developing surveys to disclose people's feedback with respect to economic issues is limited, expensive, and time-consuming. In recent years, social media such as Twitter has enabled people to share their opinions regarding elections. Social media has provided a platform for collecting a large amount of social media data. This paper proposes a computational public opinionminingapproach to explore the discussion of economic issues in social media during an election. Current related studies usetextminingmethods independently for election analysis and election prediction; this research combines twotextminingmethods: sentiment analysis and topic modeling. The proposed approach has effectively been deployed on millions of tweets to analyze economic concerns of people during the 2012 US presidential election.","['Amir Karami', 'London S. Bennett', 'Xiaoyun He']",,arXiv,2018,https://doi.org/10.48550/arXiv.1802.01786,Anomali
"Chemical-protein relation extraction with ensembles of SVM, CNN, and RNN models","Textminingthe relations between chemicals and proteins is an increasingly important task. The CHEMPROT track at BioCreative VI aims to promote the development and evaluation of systems that can automatically detect the chemical-protein relations in runningtext(PubMed abstracts). This manuscript describes our submission, which is an ensemble of three systems, including a Support Vector Machine, a Convolutional Neural Network, and a Recurrent Neural Network. Their output is combined using a decision based on majority voting or stacking. Our CHEMPROT system obtained 0.7266 in precision and 0.5735 in recall for an f-score of 0.6410, demonstrating the effectiveness of machine learning-based approaches for automatic relation extraction from biomedical literature. Our submission achieved the highest performance in the task during the 2017 challenge.","['Yifan Peng', 'Anthony Rios', 'Ramakanth Kavuluru', 'Zhiyong Lu']",,arXiv,2018,https://doi.org/10.48550/arXiv.1802.01255,Anomali
Measuring Spark on AWS: A Case Study on Mining Scientific Publications with Annotation Query,"Annotation Query (AQ) is a program that provides the ability to query many different types of NLP annotations on atext, as well as the original content and structure of thetext. The query results may provide new annotations, or they may select subsets of the content and annotations for deeper processing. Like GATE's Mimir, AQ is based on region algebras. Our AQ is implemented to run on a Spark cluster. In this paper we look at how AQ's runtimes are affected by the size of the collection, the number of nodes in the cluster, the type of node, and the characteristics of the queries. Cluster size, of course, makes a large difference in performance so long as skew can be avoided. We find that there is minimal difference in performance when persisting annotations serialized to local SSD drives as opposed to deserialized into local memory. We also find that if the number of nodes is kept constant, then AWS' storage-optimized instance performs the best. But if we factor in total cost, the compute-optimized nodes provides the best performance relative to cost.","['Darin McBeath', 'Ron Daniel Jr']",,arXiv,2018,https://doi.org/10.48550/arXiv.1802.00728,Anomali
Distributed Clustering Algorithm for Spatial Data Mining,"Distributed dataminingtechniques and mainly distributed clustering are widely used in the last decade because they deal with very large and heterogeneous datasets which cannot be gathered centrally. Current distributed clustering approaches are normally generating global models by aggregating local results that are obtained on each site. While this approachminesthe datasets on their locations the aggregation phase is complex, which may produce incorrect and ambiguous global clusters and therefore incorrect knowledge. In this paper we propose a new clustering approach for very large spatial datasets that are heterogeneous and distributed. The approach is based on K-means Algorithm but it generates the number of global clusters dynamically. Moreover, this approach uses an elaborated aggregation phase. The aggregation phase is designed in such a way that the overall process is efficient in time and memory allocation. Preliminary results show that the proposed approach produces high quality results and scales up well. We also compared it to two popular clustering algorithms and show that this approach is much more efficient.","['Malika Bendechache', 'M-Tahar Kechadi']","Spatial Data Mining and Geographical Knowledge Services (ICSDM), 2015 2nd IEEE International Conference on, pages 60--65, 2015",arXiv,2018,https://doi.org/10.48550/arXiv.1802.00304,Anomali
Preparation of Improved Turkish DataSet for Sentiment Analysis in Social Media,"A public dataset, with a variety of properties suitable for sentiment analysis [1], event prediction, trend detection and othertextminingapplications, is needed in order to be able to successfully perform analysis studies. The vast majority of data on social media istext-based and it is not possible to directly apply machine learning processes into these raw data, since several different processes are required to prepare the data before the implementation of the algorithms. For example, different misspellings of same word enlarge the word vector space unnecessarily, thereby it leads to reduce the success of the algorithm and increase the computational power requirement. This paper presents an improved Turkish dataset with an effective spelling correction algorithm based on Hadoop [2]. The collected data is recorded on the Hadoop Distributed File System and thetextbased data is processed by MapReduce programming model. This method is suitable for the storage and processing of large sizedtextbased social media data. In this study, movie reviews have been automatically recorded with Apache ManifoldCF (MCF) [3] and data clusters have been created. Various methods compared such as Levenshtein and Fuzzy String Matching have been proposed to create a public dataset from collected data. Experimental results show that the proposed algorithm, which can be used as an open source dataset in sentiment analysis studies, have been performed successfully to the detection and correction of spelling errors.","['Semiha Makinist', 'Ibrahim Riza Hallac', 'Betul Ay Karakus', 'Galip Aydin']",,arXiv,2018,https://doi.org/10.48550/arXiv.1801.09975,Anomali
Correlations and dynamics of consumption patterns in social-economic networks,"We analyse a coupled dataset collecting the mobile phone communications and bank transactions history of a large number of individuals living in a Latin American country. After mapping the social structure and introducing indicators of socioeconomic status, demographic features, and purchasing habits of individuals we show that typical consumption patterns are strongly correlated with identified socioeconomic classes leading to patterns of stratification in the social structure. In addition we measure correlations between merchant categories and introduce a correlation network, which emerges with a meaningful community structure. We detect multivariate relations between merchant categories and show correlations in purchasing habits of individuals. Finally, by analysing individual consumption histories, we detect dynamical patterns in purchase behaviour and their correlations with the socioeconomic status, demographic characters and the egocentric social network of individuals. Our work provides novel and detailed insight into the relations between social and consuming behaviour with potential applications in resource allocation, marketing, and recommendation system design.","['Yannick Leo', 'Márton Karsai', 'Carlos Sarraute', 'Eric Fleury']",,arXiv,2018,https://doi.org/10.48550/arXiv.1801.08856,Anomali
Incremental Eigenpair Computation for Graph Laplacian Matrices: Theory and Applications,"The smallest eigenvalues and the associated eigenvectors (i.e., eigenpairs) of a graph Laplacian matrix have been widely used in spectral clustering and community detection. However, in real-life applications the number of clusters or communities (say, $K$) is generally unknown a-priori. Consequently, the majority of the existing methods either choose $K$ heuristically or they repeat the clustering method with different choices of $K$ and accept the best clustering result. The first option, more often, yields suboptimal result, while the second option is computationally expensive. In this work, we propose an incremental method for constructing the eigenspectrum of the graph Laplacian matrix. This method leverages the eigenstructure of graph Laplacian matrix to obtain the $K$-th smallest eigenpair of the Laplacian matrix given a collection of all previously computed $K-1$ smallest eigenpairs. Our proposed method adapts the Laplacian matrix such that the batch eigenvalue decomposition problem transforms into an efficient sequential leading eigenpair computation problem. As a practical application, we consider user-guided spectral clustering. Specifically, we demonstrate that users can utilize the proposed incremental method for effective eigenpair computation and for determining the desired number of clusters based on multiple clustering metrics.","['Pin-Yu Chen', 'Baichuan Zhang', 'Mohammad Al Hasan']","Social Network Analysis and Mining, 2018",arXiv,2017,https://doi.org/10.48550/arXiv.1801.08196,Anomali
Data is the Fuel of Organizations: Opportunities and Challenges in Afghanistan,"In this paper, the author at first briefly outlines the value of data in organizations and the opportunities and challenges in Afghanistan. Then the author takes the Kankor (National University Entrance Exam) data, particularly names of participants, locations, high schools and higher education institutions into account and explains how these data, that organizations in Afghanistan do not use for anything, can be useful in several cases and areas. The application of these data is shown through cases such as Auto filling missing values, identifying names of people, locations, and institutions from unstructuredtext, generating fake data to benchmark the database and web application performance and appearance, comparing and matching high school data with Kankor data, producing the top-n male and female names very common in Afghanistan or province-wise, and the dataminingapplication in education and higher education institutions.",['Abdul Rahman Sherzad'],,arXiv,2018,https://doi.org/10.48550/arXiv.1801.07789,Anomali
Entity Retrieval and Text Mining for Online Reputation Monitoring,"Online Reputation Monitoring (ORM) is concerned with the use of computational tools to measure the reputation of entities online, such as politicians or companies. In practice, current ORM methods are constrained to the generation of data analytics reports, which aggregate statistics of popularity and sentiment on social media. We argue that this format is too restrictive as end users often like to have the flexibility to search for entity-centric information that is not available in predefined charts. As such, we propose the inclusion of entity retrieval capabilities as a first step towards the extension of current ORM capabilities. However, an entity's reputation is also influenced by the entity's relationships with other entities. Therefore, we address the problem of Entity-Relationship (E-R) retrieval in which the goal is to search for multiple connected entities. This is a challenging problem which traditional entity search systems cannot cope with. Besides E-R retrieval we also believe ORM would benefit oftext-based entity-centric prediction capabilities, such as predicting entity popularity on social media based on news events or the outcome of political surveys. However, none of these tasks can provide useful results if there is no effective entity disambiguation and sentiment analysis tailored to the context of ORM. Consequently, this thesis address two computational problems in Online Reputation Monitoring: Entity Retrieval andTextMining. We researched and developed methods to extract, retrieve and predict entity-centric information spread across the Web.",['Pedro Saleiro'],,arXiv,2018,https://doi.org/10.48550/arXiv.1801.07743,Anomali
Multi-Source Social Feedback of Online News Feeds,"The profusion of user generated content caused by the rise of social media platforms has enabled a surge in research relating to fields such as information retrieval, recommender systems, dataminingand machine learning. However, the lack of comprehensive baseline data sets to allow a thorough evaluative comparison has become an important issue. In this paper we present a large data set of news items from well-known aggregators such as Google News and Yahoo! News, and their respective social feedback on multiple platforms: Facebook, Google+ and LinkedIn. The data collected relates to a period of 8 months, between November 2015 and July 2016, accounting for about 100,000 news items on four different topics: economy, microsoft, obama and palestine. This data set is tailored for evaluative comparisons in predictive analytics tasks, although allowing for tasks in other research areas such as topic detection and tracking, sentiment analysis in shorttext, first story detection or news recommendation.","['Nuno Moniz', 'Luís Torgo']",,arXiv,2018,https://doi.org/10.48550/arXiv.1801.07055,Anomali
Using Deep Learning for Title-Based Semantic Subject Indexing to Reach Competitive Performance to Full-Text,"For (semi-)automated subject indexing systems in digital libraries, it is often more practical to use metadata such as the title of a publication instead of the full-textor the abstract. Therefore, it is desirable to have goodtextminingandtextclassification algorithms that operate well already on the title of a publication. So far, the classification performance on titles is not competitive with the performance on the full-textsif the same number of training samples is used for training. However, it is much easier to obtain title data in large quantities and to use it for training than full-textdata. In this paper, we investigate the question how models obtained from training on increasing amounts of title training data compare to models from training on a constant number of full-texts. We evaluate this question on a large-scale dataset from the medical domain (PubMed) and from economics (EconBiz). In these datasets, the titles and annotations of millions of publications are available, and they outnumber the available full-textsby a factor of 20 and 15, respectively. To exploit these large amounts of data to their full potential, we develop three strong deep learning classifiers and evaluate their performance on the two datasets. The results are promising. On the EconBiz dataset, all three classifiers outperform their full-textcounterparts by a large margin. The best title-based classifier outperforms the best full-textmethod by 9.4%. On the PubMed dataset, the best title-based method almost reaches the performance of the best full-textclassifier, with a difference of only 2.9%.","['Florian Mai', 'Lukas Galke', 'Ansgar Scherp']","JCDL '18: The 18th ACM/IEEE Joint Conference on Digital Libraries, June 3--7, 2018, Fort Worth, TX, USA",arXiv,2018,https://doi.org/10.48550/arXiv.1801.06717,Anomali
DCDistance: A Supervised Text Document Feature extraction based on class labels,"TextMiningis a field that aims at extracting information from textual data. One of the challenges of such field of study comes from the pre-processing stage in which a vector (and structured) representation should be extracted from unstructured data. The common extraction creates large and sparse vectors representing the importance of each term to a document. As such, this usually leads to the curse-of-dimensionality that plagues most machine learning algorithms. To cope with this issue, in this paper we propose a new supervised feature extraction and reduction algorithm, named DCDistance, that creates features based on the distance between a document to a representative of each class label. As such, the proposed technique can reduce the features set in more than 99% of the original set. Additionally, this algorithm was also capable of improving the classification accuracy over a set of benchmark datasets when compared to traditional and state-of-the-art features selection algorithms.","['Charles Henrique Porto Ferreira', 'Debora Maria Rossi de Medeiros', 'Fabricio Olivetti de França']",,arXiv,2018,https://doi.org/10.48550/arXiv.1801.04554,Anomali
Differentially Private Releasing via Deep Generative Model (Technical Report),"Privacy-preserving releasing of complex data (e.g., image,text, audio) represents a long-standing challenge for the dataminingresearch community. Due to rich semantics of the data and lack of a priori knowledge about the analysis task, excessive sanitization is often necessary to ensure privacy, leading to significant loss of the data utility. In this paper, we present dp-GAN, a general private releasing framework for semantic-rich data. Instead of sanitizing and then releasing the data, the data curator publishes a deep generative model which is trained using the original data in a differentially private manner; with the generative model, the analyst is able to produce an unlimited amount of synthetic data for arbitrary analysis tasks. In contrast of alternative solutions, dp-GAN highlights a set of key features: (i) it provides theoretical privacy guarantee via enforcing the differential privacy principle; (ii) it retains desirable utility in the released model, enabling a variety of otherwise impossible analyses; and (iii) most importantly, it achieves practical training scalability and stability by employing multi-fold optimization strategies. Through extensive empirical evaluation on benchmark datasets and analyses, we validate the efficacy of dp-GAN.","['Xinyang Zhang', 'Shouling Ji', 'Ting Wang']",,arXiv,2018,https://doi.org/10.48550/arXiv.1801.01594,Anomali
How the Taiwanese Do China Studies: Applications of Text Mining,"With the rapid evolution of cross-strait situation, ""Mainland China"" as a subject of social science study has evoked the voice of ""Rethinking China Study"" among intelligentsia recently. This essay tried to apply an automatic content analysis tool (CATAR) to the journal ""Mainland China Studies"" (1998-2015) in order to observe the research trends based on the clustering oftextfrom the title and abstract of each paper in the journal. The results showed that the 473 articles published by the journal were clustered into seven salient topics. From the publication number of each topic over time (including ""volume of publications"", ""percentage of publications""), there are two major topics of this journal while other topics varied over time widely. The contribution of this study includes: 1. We could group each ""independent"" study into a meaningful topic, as a small scale experiment verified that this topic clustering is feasible. 2. This essay reveals the salient research topics and their trends for the Taiwan journal ""Mainland China Studies"". 3. Various topical keywords were identified, providing easy access to the past study. 4. The yearly trends of the identified topics could be viewed as signature of future research directions.","['Hsuan-Lei Shao', 'Sieh-Chuen Huang', 'Yun-Cheng Tsai']","Journal of Data Mining & Digital Humanities, 2018 (May 4, 2018) jdmdh:4178",arXiv,2018,https://doi.org/10.48550/arXiv.1801.00912,Anomali
A Deep Belief Network Based Machine Learning System for Risky Host Detection,"To assure cyber security of an enterprise, typically SIEM (Security Information and Event Management) system is in place to normalize security event from different preventive technologies and flag alerts. Analysts in the security operation center (SOC) investigate the alerts to decide if it is truly malicious or not. However, generally the number of alerts is overwhelming with majority of them being false positive and exceeding the SOC's capacity to handle all alerts. There is a great need to reduce the false positive rate as much as possible. While most previous research focused on network intrusion detection, we focus on risk detection and propose an intelligent Deep Belief Network machine learning system. The system leverages alert information, various security logs and analysts' investigation results in a real enterprise environment to flag hosts that have high likelihood of being compromised.Textminingand graph based method are used to generate targets and create features for machine learning. In the experiment, Deep Belief Network is compared with other machine learning algorithms, including multi-layer neural network, random forest, support vector machine and logistic regression. Results on real enterprise data indicate that the deep belief network machine learning system performs better than other algorithms for our problem and is six times more effective than current rule-based system. We also implement the whole system from data collection, label creation, feature engineering to host score generation in a real enterprise production environment.","['Wangyan Feng', 'Shuning Wu', 'Xiaodan Li', 'Kevin Kunkle']",,arXiv,2017,https://doi.org/10.48550/arXiv.1801.00025,Anomali
Comparative Opinion Mining: A Review,"Opinionminingrefers to the use of natural language processing,textanalysis and computational linguistics to identify and extract subjective information in textual material. Opinionmining, also known as sentiment analysis, has received a lot of attention in recent times, as it provides a number of tools to analyse the public opinion on a number of different topics. Comparative opinionminingis a subfield of opinionminingthat deals with identifying and extracting information that is expressed in a comparative form (e.g.~""paper X is better than the Y""). Comparative opinionminingplays a very important role when ones tries to evaluate something, as it provides a reference point for the comparison. This paper provides a review of the area of comparative opinionmining. It is the first review that cover specifically this topic as all previous reviews dealt mostly with general opinionmining. This survey covers comparative opinionminingfrom two different angles. One from perspective of techniques and the other from perspective of comparative opinion elements. It also incorporates preprocessing tools as well as dataset that were used by the past researchers that can be useful to the future researchers in the field of comparative opinionmining.","['Kasturi Dewi Varathan', 'Anastasia Giachanou', 'Fabio Crestani']","Journal of the Association for Information Science and Technology, 68(4), 2017",arXiv,2017,https://doi.org/10.48550/arXiv.1712.08941,Anomali
$l_1$-$l_2$ Regularization of Split Feasibility Problems,"Numerous problems in signal processing and imaging, statistical learning and datamining, or computer vision can be formulated as optimization problems which consist in minimizing a sum of convex functions, not necessarily differentiable, possibly composed with linear operators and that in turn can be transformed to split feasibility problems (SFP), see for example \cite{ce94}. Each function is typically either a data fidelity term or a regularization term enforcing some properties on the solution, see for example \cite{cpp09} and references therein. In this paper we are interested in Split Feasibility Problems which can be seen as a general form of $Q$-Lasso introduced in \cite{aasnx13} that extended the well-known Lasso of Tibshirani \cite{Tibshirani96}. $Q$ is a closed convex subset of a Euclidean $m$-space, for some integer $m\geq1$, that can be interpreted as the set of errors within given tolerance level when linear measurements are taken to recover a signal/image via the Lasso.
  Inspired by recent works by Lou et al \cite{ly, xcxz12}, we are interested in a nonconvex regularization of SFP and propose three split algorithms for solving this general case. The first one is based on the DC (difference of convex) algorithm (DCA) introduced by Pham Dinh Tao, the second one in nothing else than the celebrate forward-backward algorithm and the third one uses a method introduced byMineand Fukushima. It is worth mentioning that the SFP model a number of applied problems arising from signal/image processing and specially optimization problems for intensity-modulated radiation therapy (IMRT) treatment planning, see for example \cite{cbmt06}.","['Abdellatif Moudafi', 'Aviv Gibali']",,arXiv,2017,https://doi.org/10.48550/arXiv.1712.03822,Anomali
FlagIt: A System for Minimally Supervised Human Trafficking Indicator Mining,"In this paper, we describe and study the indicatorminingproblem in the online sex advertising domain. We present an in-development system, FlagIt (Flexible and adaptive generation of Indicators fromtext), which combines the benefits of both a lightweight expert system and classical semi-supervision (heuristic re-labeling) with recently released state-of-the-art unsupervisedtextembeddings to tag millions of sentences with indicators that are highly correlated with human trafficking. The FlagIt technology stack is open source. On preliminary evaluations involving five indicators, FlagIt illustrates promising performance compared to several alternatives. The system is being actively developed, refined and integrated into a domain-specific search system used by over 200 law enforcement agencies to combat human trafficking, and is being aggressively extended tomineat least six more indicators with minimal programming effort. FlagIt is a good example of a system that operates in limited label settings, and that requires creative combinations of established machine learning techniques to produce outputs that could be used by real-world non-technical analysts.","['Mayank Kejriwal', 'Jiayuan Ding', 'Runqi Shao', 'Anoop Kumar', 'Pedro Szekely']",,arXiv,2017,https://doi.org/10.48550/arXiv.1712.03086,Anomali
A Deep Network Model for Paraphrase Detection in Short Text Messages,"This paper is concerned with paraphrase detection. The ability to detect similar sentences written in natural language is crucial for several applications, such astextmining,textsummarization, plagiarism detection, authorship authentication and question answering. Given two sentences, the objective is to detect whether they are semantically identical. An important insight from this work is that existing paraphrase systems perform well when applied on cleantexts, but they do not necessarily deliver good performance against noisytexts. Challenges with paraphrase detection on user generated shorttexts, such as Twitter, include language irregularity and noise. To cope with these challenges, we propose a novel deep neural network-based approach that relies on coarse-grained sentence modeling using a convolutional neural network and a long short-term memory model, combined with a specific fine-grained word-level similarity matching model. Our experimental results show that the proposed approach outperforms existing state-of-the-art approaches on user-generated noisy social media data, such as Twittertexts, and achieves highly competitive performance on a cleaner corpus.","['Basant Agarwal', 'Heri Ramampiaro', 'Helge Langseth', 'Massimiliano Ruocco']","B Agarwal, H. Ramampiaro, H Langseth, M Ruocco, (2018), ""A Deep Network Model for Paraphrase Detection in Short Text Messages"". In Information Processing & Management Journal (IPM), 54(6), pp. 922-937. Elsevier",arXiv,2017,https://doi.org/10.48550/arXiv.1712.02820,Anomali
An innovative solution for breast cancer textual big data analysis,"The digitalization of stored information in hospitals now allows for the exploitation of medical data intextformat, as electronic health records (EHRs), initially gathered for other purposes than epidemiology. Manual search and analysis operations on such data become tedious. In recent years, the use of natural language processing (NLP) tools was highlighted to automatize the extraction of information contained in EHRs, structure it and perform statistical analysis on this structured information. The main difficulties with the existing approaches is the requirement of synonyms or ontology dictionaries, that are mostly available in English only and do not include local or custom notations. In this work, a team composed of oncologists as domain experts and data scientists develop a custom NLP-based system to process and structure textual clinical reports of patients suffering from breast cancer. The tool relies on the combination of standardtextminingtechniques and an advanced synonym detection method. It allows for a global analysis by retrieval of indicators such as medical history, tumor characteristics, therapeutic responses, recurrences and prognosis. The versatility of the method allows to obtain easily new indicators, thus opening up the way for retrospective studies with a substantial reduction of the amount of manual work. With no need for biomedical annotators or pre-defined ontologies, this language-agnostic method reached an good extraction accuracy for several concepts of interest, according to a comparison with a manually structured file, without requiring any existing corpus with local or new notations.","['Nicolas Thiebaut', 'Antoine Simoulin', 'Karl Neuberger', 'Issam Ibnouhsein', 'Nicolas Bousquet', 'Nathalie Reix', 'Sébastien Molière', 'Carole Mathelin']",,arXiv,2017,https://doi.org/10.48550/arXiv.1712.02259,Anomali
Listening to Chaotic Whispers: A Deep Learning Framework for News-oriented Stock Trend Prediction,"Stock trend prediction plays a critical role in seeking maximized profit from stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of stock market. Exploding information on Internet together with advancing development of natural language processing andtextminingtechniques have enable investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness and comprehensiveness of online content related to stock market varies drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our approach.","['Ziniu Hu', 'Weiqing Liu', 'Jiang Bian', 'Xuanzhe Liu', 'Tie-Yan Liu']",,arXiv,2019,https://doi.org/10.48550/arXiv.1712.02136,Anomali
Sequence Mining and Pattern Analysis in Drilling Reports with Deep Natural Language Processing,"Drilling activities in the oil and gas industry have been reported over decades for thousands of wells on a daily basis, yet the analysis of thistextat large-scale for information retrieval, sequencemining, and pattern analysis is very challenging. Drilling reports contain interpretations written by drillers from noting measurements in downhole sensors and surface equipment, and can be used for operation optimization and accident mitigation. In this initial work, a methodology is proposed for automatic classification of sentences written in drilling reports into three relevant labels (EVENT, SYMPTOM and ACTION) for hundreds of wells in an actual field. Some of the main challenges in thetextcorpus were overcome, which include the high frequency of technical symbols, mistyping/abbreviation of technical terms, and the presence of incomplete sentences in the drilling reports. We obtain state-of-the-art classification accuracy within this technical language and illustrate advanced queries enabled by the tool.","['Júlio Hoffimann', 'Youli Mao', 'Avinash Wesley', 'Aimee Taylor']",,arXiv,2017,https://doi.org/10.48550/arXiv.1712.01476,Anomali
Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals,"Performance appraisal (PA) is an important HR process to periodically measure and evaluate every employee's performance vis-a-vis the goals established by the organization. A PA process involves purposeful multi-step multi-modal communication between employees, their supervisors and their peers, such as self-appraisal, supervisor assessment and peer feedback. Analysis of the structured data andtextproduced in PA is crucial for measuring the quality of appraisals and tracking actual improvements. In this paper, we applytextminingtechniques to produce insights from PAtext. First, we perform sentence classification to identify strengths, weaknesses and suggestions of improvements found in the supervisor assessments and then use clustering to discover broad categories among them. Next we use multi-class multi-label classification techniques to match supervisor assessments to predefined broad perspectives on performance. Finally, we propose a short-textsummarization technique to produce a summary of peer feedback comments for a given employee and compare it with manual summaries. All techniques are illustrated using a real-life dataset of supervisor assessment and peer feedbacktextproduced during the PA of 4528 employees in a large multi-national IT company.","['Girish Keshav Palshikar', 'Sachin Pawar', 'Saheb Chourasia', 'Nitin Ramrakhiyani']",,arXiv,2017,https://doi.org/10.48550/arXiv.1712.00991,Anomali
Exploration of an Interdisciplinary Scientific Landscape,"Patterns of interdisciplinarity in science can be quantified through diverse complementary dimensions. This paper studies as a case study the scientific environment of a generalist journal in Geography, Cybergeo, in order to introduce a novel methodology combining citation network analysis and semantic analysis. We collect a large corpus of around 200,000 articles with their abstracts and the corresponding citation network that provides a first citation classification. Relevant keywords are extracted for each article throughtext-mining, allowing us to construct a semantic classification. We study the qualitative patterns of relations between endogenous disciplines within each classification, and finally show the complementarity of classifications and of their associated interdisciplinarity measures. The tools we develop accordingly are open and reusable for similar large scale studies of scientific environments.",['Juste Raimbault'],"Scientometrics, 119(2), 617-641 (2019)",arXiv,2020,https://doi.org/10.48550/arXiv.1712.00805,Anomali
SHINE: Signed Heterogeneous Information Network Embedding for Sentiment Link Prediction,"In online social networks people often express attitudes towards others, which forms massive sentiment links among users. Predicting the sign of sentiment links is a fundamental task in many areas such as personal advertising and public opinion analysis. Previous works mainly focus on textual sentiment classification, however,textinformation can only disclose the ""tip of the iceberg"" about users' true opinions, of which the most are unobserved but implied by other sources of information such as social relation and users' profile. To address this problem, in this paper we investigate how to predict possibly existing sentiment links in the presence of heterogeneous information. First, due to the lack of explicit sentiment links in mainstream social networks, we establish a labeled heterogeneous sentiment dataset which consists of users' sentiment relation, social relation and profile knowledge by entity-level sentiment extraction method. Then we propose a novel and flexible end-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework to extract users' latent representations from heterogeneous networks and predict the sign of unobserved sentiment links. SHINE utilizes multiple deep autoencoders to map each user into a low-dimension feature space while preserving the network structure. We demonstrate the superiority of SHINE over state-of-the-art baselines on link prediction and node recommendation in two real-world datasets. The experimental results also prove the efficacy of SHINE in cold start scenario.","['Hongwei Wang', 'Fuzheng Zhang', 'Min Hou', 'Xing Xie', 'Minyi Guo', 'Qi Liu']",,arXiv,2017,https://doi.org/10.48550/arXiv.1712.00732,Anomali
Joint Topic-Semantic-aware Social Recommendation for Online Voting,"Online voting is an emerging feature in social networks, in which users can express their attitudes toward various issues and show their unique interest. Online voting imposes new challenges on recommendation, because the propagation of votings heavily depends on the structure of social networks as well as the content of votings. In this paper, we investigate how to utilize these two factors in a comprehensive manner when doing voting recommendation. First, due to the fact that existingtextminingmethods such as topic model and semantic model cannot well process the content of votings that is typically short and ambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to learn word and document representation by jointly considering their topics and semantics. Then we propose our Joint Topic-Semantic-aware social Matrix Factorization (JTS-MF) model for voting recommendation. JTS-MF model calculates similarity among users and votings by combining their TEWE representation and structural information of social networks, and preserves this topic-semantic-social similarity during matrix factorization. To evaluate the performance of TEWE representation and JTS-MF model, we conduct extensive experiments on real online voting dataset. The results prove the efficacy of our approach against several state-of-the-art baselines.","['Hongwei Wang', 'Jia Wang', 'Miao Zhao', 'Jiannong Cao', 'Minyi Guo']",,arXiv,2017,https://doi.org/10.48550/arXiv.1712.00731,Anomali
KIBS Innovative Entrepreneurship Networks on Social Media,"The analysis of the use of social media for innovative entrepreneurship in the context has received little attention in the literature, especially in the context of Knowledge Intensive Business Services (KIBS). Therefore, this paper focuses on bridging this gap by applyingtextminingand sentiment analysis techniques to identify the innovative entrepreneurship reflected by these companies in their social media. Finally, we present and analyze the results of our quantitative analysis of 23.483 posts based on eleven Spanish and Italian consultancy KIBS Twitter Usernames and Keywords using data interpretation techniques such as clustering and topic modeling. This paper suggests that there is a significant gap between the perceived potential of social media and the entrepreneurial behaviors at the social context in business-to-business (B2B) companies.","['José N. Franco-Riquelme', 'Isaac Lemus-Aguilar', 'Joaquín Ordieres-Meré']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.11403,Anomali
"Emerging basic, clinical and translational research fronts in dental biomaterials R&D","The current (2007-2007) structure and content of dental materials research has been investigated by identifying and describing the emergent research fronts which can be related to basic, translational and clinical observation research. By a combination of network analysis andtextminingof the literature on dental materials indexed in the Web of Science, we have identified eleven emerging research fronts. These fronts are related to different dental materials applications which are at different levels in the knowledge translation and biomedical innovation process. We identified fronts related to dominant designs like titanium implants, competing technologies like ceramics and composites applications to prothesis and restauration, and disruptive technologies like nanomaterials and mineral trioxide aggregates. Our results suggest the possible relation between the technological complexity of the dental materials and the level of advance in terms of knowledge translation. This is the first time the structure and content of research on dental materials research is analyzed.","['David Fajardo-Ortiz', 'Pablo Jaramillo', 'Claudia Jaramillo', 'Raul Resendiz', 'Miguel Lara-Flores', 'Victor M. Castano']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.11168,Anomali
The evolution and structure of biomedical knowledge on cytochrome P450,"Cytochrome P450 are fundamental proteins to the metabolism of drugs and other relevant processes. through a combination oftextminingand network analysis of the P450 literature we mapped the emergence and evolution of the biomedical research communities working on this family of proteins. Our results suggest that the historical research communities that worked on P450 emerged and were organized mainly around methodological achievements like the induction of animal liver microsomal P450 by drugs, the use of chemical inhibitors of P450 enzymes in in-vitro metabolism studies and the development of E. coli expression systems. We found clear evidence that P450 research indeed constitutes a material scientific culture, as we discuss in thetext.","['David Fajardo-Ortiz', 'Miguel Lara', 'Victor M. Castano']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.10009,Anomali
Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation,"Thanks to the success of object detection technology, we can retrieve objects of the specified classes even from huge image collections. However, the current state-of-the-art object detectors (such as Faster R-CNN) can only handle pre-specified classes. In addition, large amounts of positive and negative visual samples are required for training. In this paper, we address the problem of open-vocabulary object retrieval and localization, where the target object is specified by a textual query (e.g., a word or phrase). We first propose Query-Adaptive R-CNN, a simple extension of Faster R-CNN adapted to open-vocabulary queries, by transforming thetextembedding vector into an object classifier and localization regressor. Then, for discriminative training, we then propose negative phrase augmentation (NPA) tominehard negative samples which are visually similar to the query and at the same time semantically mutually exclusive of the query. The proposed method can retrieve and localize objects specified by a textual query from one million images in only 0.5 seconds with high precision.","['Ryota Hinami', ""Shin'ichi Satoh""]",,arXiv,2018,https://doi.org/10.48550/arXiv.1711.09509,Anomali
Neural Ranking Models with Multiple Document Fields,"Deep neural networks have recently shown promise in the ad-hoc retrieval task. However, such models have often been based on one field of the document, for example considering document title only or document body only. Since in practice documents typically have multiple fields, and given that non-neural ranking models such as BM25F have been developed to take advantage of document structure, this paper investigates how neural models can deal with multiple document fields. We introduce a model that can consume shorttextfields such as document title and longtextfields such as document body. It can also handle multi-instance fields with variable number of instances, for example where each document has zero or more instances of incoming anchortext. Since fields vary in coverage and quality, we introduce a masking method to handle missing field instances, as well as a field-level dropout method to avoid relying too much on any one field. As in the studies of non-neural field weighting, we find it is better for the ranker to score the whole document jointly, rather than generate a per-field score and aggregate. We find that different document fields may match different aspects of the query and therefore benefit from comparing with separate representations of the querytext. The combination of techniques introduced here leads to a neural ranker that can take advantage of full document structure, including multiple instance and missing instance data, of variable length. The techniques significantly enhance the performance of the ranker, and also outperform a learning to rank baseline with hand-crafted features.","['Hamed Zamani', 'Bhaskar Mitra', 'Xia Song', 'Nick Craswell', 'Saurabh Tiwary']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.09174,Anomali
Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition,"Biomedical named entity recognition (NER) is a fundamental task intextminingof medical documents and has many applications. Deep learning based approaches to this task have been gaining increasing attention in recent years as their parameters can be learned end-to-end without the need for hand-engineered features. However, these approaches rely on high-quality labeled data, which is expensive to obtain. To address this issue, we investigate how to use unlabeledtextdata to improve the performance of NER models. Specifically, we train a bidirectional language model (BiLM) on unlabeled data and transfer its weights to ""pretrain"" an NER model with the same architecture as the BiLM, which results in a better parameter initialization of the NER model. We evaluate our approach on four benchmark datasets for biomedical NER and show that it leads to a substantial improvement in the F1 scores compared with the state-of-the-art approaches. We also show that BiLM weight transfer leads to a faster model training and the pretrained model requires fewer training examples to achieve a particular F1 score.","['Devendra Singh Sachan', 'Pengtao Xie', 'Mrinmaya Sachan', 'Eric P Xing']",,arXiv,2018,https://doi.org/10.48550/arXiv.1711.07908,Anomali
A visual search engine for Bangladeshi laws,"Browsing and finding relevant information for Bangladeshi laws is a challenge faced by all law students and researchers in Bangladesh, and by citizens who want to learn about any legal procedure. Some law archives in Bangladesh are digitized, but lack proper tools to organize the data meaningfully. We present atextvisualization tool that utilizes machine learning techniques to make the searching of laws quicker and easier. Using Doc2Vec to layout law article nodes, linkminingtechniques to visualize relevant citation networks, and named entity recognition to quickly find relevant sections in long law articles, our tool provides a faster and better search experience to the users. Qualitative feedback from law researchers, students, and government officials show promise for visually intuitive search tools in the context of governmental, legal, and constitutional data in developing countries, where digitized data does not necessarily pave the way towards an easy access to information.","['Manash Kumar Mandal', 'Pinku Deb Nath', 'Arpeeta Shams Mizan', 'Nazmus Saquib']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.05233,Anomali
Text Mining Descriptions Of Dreams: aesthetic and clinical efforts,"Dreams are highly valued in both Freudian psychoanalysis and less conservative clinical traditions.Textminingenables the extraction of meaning from writings in powerful and unexpected ways. In this work, we report methods, uses and results obtained byminingdescriptions of dreams. Thetextswere collected as part of a course in Schizoanalysis (Clinical Psychology) from dozens of participants. They were subsequentlyminedusing various techniques for the achievement of poems and summaries, which were then used in clinical sessions by means of music and declamation. The results were found aesthetically appealing and effective to engage the audience. The expansion of the corpus,miningmethods and strategies for using the derivatives for art and therapy are considered for future work.","['Renato Fabbri', 'Fabiane M. Borges']","ISSN 2527-2357, ISBN 978-85-5676-019-7",arXiv,2017,https://doi.org/10.48550/arXiv.1711.04609,Anomali
"Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey","Topic modeling is one of the most powerful techniques intextminingfor datamining, latent data discovery, and finding relationships among data,textdocuments. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modeling, which Latent Dirichlet allocation (LDA) is one of the most popular methods in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper can be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated scholarly articles highly (between 2003 to 2016) related to Topic Modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. Also, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.","['Hamed Jelodar', 'Yongli Wang', 'Chi Yuan', 'Xia Feng', 'Xiahui Jiang', 'Yanchao Li', 'Liang Zhao']",,arXiv,2018,https://doi.org/10.48550/arXiv.1711.04305,Anomali
Feature Enhancement Network: A Refined Scene Text Detector,"In this paper, we propose a refined scenetextdetector with a \textit{novel} Feature Enhancement Network (FEN) for Region Proposal andTextDetection Refinement. Retrospectively, both region proposal with \textit{only} $3\times 3$ sliding-window feature andtextdetection refinement with \textit{single scale} high level feature are insufficient, especially for smaller scenetext. Therefore, we design a new FEN network with \textit{task-specific}, \textit{low} and \textit{high} level semantic features fusion to improve the performance oftextdetection. Besides, since \textit{unitary} position-sensitive RoI pooling in general object detection is unreasonable for variabletextregions, an \textit{adaptively weighted} position-sensitive RoI pooling layer is devised for further enhancing the detecting accuracy. To tackle the \textit{sample-imbalance} problem during the refinement stage, we also propose an effective \textit{positivesmining} strategy for efficiently training our network. Experiments on ICDAR 2011 and 2013 robusttextdetection benchmarks demonstrate that our method can achieve state-of-the-art results, outperforming all reported methods in terms of F-measure.","['Sheng Zhang', 'Yuliang Liu', 'Lianwen Jin', 'Canjie Luo']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.04249,Anomali
Joint Sentiment/Topic Modeling on Text Data Using Boosted Restricted Boltzmann Machine,"Recently by the development of the Internet and the Web, different types of social media such as web blogs become an immense source oftextdata. Through the processing of these data, it is possible to discover practical information about different topics, individuals opinions and a thorough understanding of the society. Therefore, applying models which can automatically extract the subjective information from the documents would be efficient and helpful. Topic modeling methods, also sentiment analysis are the most raised topics in the natural language processing andtextminingfields. In this paper a new structure for joint sentiment-topic modeling based on Restricted Boltzmann Machine (RBM) which is a type of neural networks is proposed. By modifying the structure of RBM as well as appending a layer which is analogous to sentiment oftextdata to it, we propose a generative structure for joint sentiment topic modeling based on neutral networks. The proposed method is supervised and trained by the Contrastive Divergence algorithm. The new attached layer in the proposed model is a layer with the multinomial probability distribution which can be used intextdata sentiment classification or any other supervised application. The proposed model is compared with existing models in the experiments such as evaluating as a generative model, sentiment classification, information retrieval and the corresponding results demonstrate the efficiency of the method.","['Masoud Fatemi', 'Mehran Safayani']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.03736,Anomali
Weakly-supervised Relation Extraction by Pattern-enhanced Embedding Learning,"Extracting relations fromtextcorpora is an important task intextmining. It becomes particularly challenging when focusing on weakly-supervised relation extraction, that is, utilizing a few relation instances (i.e., a pair of entities and their relation) as seeds to extract more instances from corpora. Existing distributional approaches leverage the corpus-level co-occurrence statistics of entities to predict their relations, and require large number of labeled instances to learn effective relation classifiers. Alternatively, pattern-based approaches perform bootstrapping or apply neural networks to model the local contexts, but still rely on large number of labeled instances to build reliable models. In this paper, we study integrating the distributional and pattern-based methods in a weakly-supervised setting, such that the two types of methods can provide complementary supervision for each other to build an effective, unified model. We propose a novel co-training framework with a distributional module and a pattern module. During training, the distributional module helps the pattern module discriminate between the informative patterns and other patterns, and the pattern module generates some highly-confident instances to improve the distributional module. The whole framework can be effectively optimized by iterating between improving the pattern module and updating the distributional module. We conduct experiments on two tasks: knowledge base completion withtextcorpora and corpus-level relation extraction. Experimental results prove the effectiveness of our framework in the weakly-supervised setting.","['Meng Qu', 'Xiang Ren', 'Yu Zhang', 'Jiawei Han']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.03226,Anomali
Quality-Efficiency Trade-offs in Machine Learning for Text Processing,"Datamining, machine learning, and natural language processing are powerful techniques that can be used together to extract information from largetexts. Depending on the task or problem at hand, there are many different approaches that can be used. The methods available are continuously being optimized, but not all these methods have been tested and compared in a set of problems that can be solved using supervised machine learning algorithms. The question is what happens to the quality of the methods if we increase the training data size from, say, 100 MB to over 1 GB? Moreover, are quality gains worth it when the rate of data processing diminishes? Can we trade quality for time efficiency and recover the quality loss by just being able to process more data? We attempt to answer these questions in a general way fortextprocessing tasks, considering the trade-offs involving training data size, learning time, and quality obtained. We propose a performance trade-off framework and apply it to three importanttextprocessing problems: Named Entity Recognition, Sentiment Analysis and Document Classification. These problems were also chosen because they have different levels of object granularity: words, paragraphs, and documents. For each problem, we selected several supervised machine learning algorithms and we evaluated the trade-offs of them on large publicly available data sets (news, reviews, patents). To explore these trade-offs, we use different data subsets of increasing size ranging from 50 MB to several GB. We also consider the impact of the data set and the evaluation technique. We find that the results do not change significantly and that most of the time the best algorithms is the fastest. However, we also show that the results for small data (say less than 100 MB) are different from the results for big data and in those cases the best algorithm is much harder to determine.","['Ricardo Baeza-Yates', 'Zeinab Liaghat']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.02295,Anomali
Optimal Pricing-Based Edge Computing Resource Management in Mobile Blockchain,"As the core issue of blockchain, theminingrequires solving a proof-of-work puzzle, which is resource expensive to implement in mobile devices due to high computing power needed. Thus, the development of blockchain in mobile applications is restricted. In this paper, we consider the edge computing as the network enabler for mobile blockchain. In particular, we study optimal pricing-based edge computing resource management to support mobile blockchain applications where theminingprocess can be offloaded to an Edge computing Service Provider (ESP). We adopt a two-stage Stackelberg game to jointly maximize the profit of the ESP and the individual utilities of different miners. In Stage I, the ESP sets the price of edge computing services. In Stage II, the miners decide on the service demand to purchase based on the observed prices. We apply the backward induction to analyze the sub-game perfect equilibrium in each stage for uniform and discriminatory pricing schemes. Further, the existence and uniqueness of Stackelberg game are validated for both pricing schemes. At last, the performance evaluation shows that the ESP intends to set the maximum possible value as the optimal price for profit maximization under uniform pricing. In addition, the discriminatory pricing helps the ESP encourage higher total service demand from miners and achieve greater profit correspondingly.","['Zehui Xiong', 'Shaohan Feng', 'Dusit Niyato', 'Ping Wang', 'Zhu Han']",,arXiv,2017,https://doi.org/10.48550/arXiv.1711.01049,Anomali
A Comprehensive Low and High-level Feature Analysis for Early Rumor Detection on Twitter,"Recent work have done a good job in modeling rumors and detecting them over microblog streams. However, the performance of their automatic approaches are not relatively high when looking early in the diffusion. A first intuition is that, at early stage, most of the aggregated rumor features (e.g., propagation features) are not mature and distinctive enough. The objective of rumor debunking in microblogs, however, are to detect these misinformation as early as possible. In this work, we leverage neural models in learning the hidden representations of individual rumor-related tweets at the very beginning of a rumor. Our extensive experiments show that the resulting signal improves our classification performance over time, significantly within the first 10 hours. To deepen the understanding of these low and high-level features in contributing to the model performance over time, we conduct an extensive study on a wide range of high impact rumor features for the 48 hours range. The end model that engages these features are shown to be competitive, reaches over 90% accuracy and out-performs strong baselines in our carefully cured dataset.",['Tu Nguyen'],,arXiv,2024,https://doi.org/10.48550/arXiv.1711.00726,Anomali
A Novel Approach to Artistic Textual Visualization via GAN,"While the visualization of statistical data tends to a mature technology, the visualization of textual data is still in its infancy, especially for the artistictext. Due to the fact that visualization of artistictextis valuable and attractive in both art and information science, we attempt to realize this tentative idea in this article. We propose the Generative Adversarial Network based Artistic Textual Visualization (GAN-ATV) which can create paintings after analyzing the semantic content of existing poems. Our GAN-ATV consists of two main sections: natural language analysis section and visual information synthesis section. In natural language analysis section, we use Bag-of-Word (BoW) feature descriptors and a two-layer network tomineand analyze the high-level semantic information from poems. In visual information synthesis section, we design a cross-modal semantic understanding module and integrate it with Generative Adversarial Network (GAN) to create paintings, whose content are corresponding to the original poems. Moreover, in order to train our GAN-ATV and verify its performance, we establish a cross-modal artistic dataset named ""Cross-Art"". In the Cross-Art dataset, there are six topics and each topic has their corresponding paintings and poems. The experimental results on Cross-Art dataset are shown in this article.","['Yichi Ma', 'Muhan Ma']",,arXiv,2017,https://doi.org/10.48550/arXiv.1710.10553,Anomali
Deep Health Care Text Classification,"Health related social mediaminingis a valuable apparatus for the early recognition of the diverse antagonistic medicinal conditions. Mostly, the existing methods are based on machine learning with knowledge-based learning. This working note presents the Recurrent neural network (RNN) and Long short-term memory (LSTM) based embedding for automatic healthtextclassification in the social mediamining. For each task, two systems are built and that classify the tweet at the tweet level. RNN and LSTM are used for extracting features and non-linear activation function at the last layer facilitates to distinguish the tweets of different categories. The experiments are conducted on 2nd Social MediaMiningfor Health Applications Shared Task at AMIA 2017. The experiment results are considerable; however the proposed method is appropriate for the healthtextclassification. This is primarily due to the reason that, it doesn't rely on any feature engineering mechanisms.","['Vinayakumar R', 'Barathi Ganesh HB', 'Anand Kumar M', 'Soman KP']",,arXiv,2017,https://doi.org/10.48550/arXiv.1710.08396,Anomali
MEDOC: a Python wrapper to load MEDLINE into a local MySQL database,"Since the MEDLINE database was released, the number of documents indexed by this entity has risen every year. Several tools have been developed by the National Institutes of Health (NIH) to query this corpus of scientific publications. However, in terms of advances in big data,text-miningand data science, an option to build a local relational database containing all metadata available on MEDLINE would be truly useful to optimally exploit these resources. MEDOC (MEdline DOwnloading Contrivance) is a Python program designed to download data on an FTP and to load all extracted information into a local MySQL database. It took MEDOC 4 days and 17 hours to load the 26 million documents available on this server onto a standard computer. This indexed relational database allows the user to build complex and rapid queries. All fields can thus be searched for desired information, a task that is difficult to accomplish through the PubMed graphical interface. MEDOC is free and publicly available at https://github.com/MrMimic/MEDOC.","['Emeric Dynomant', 'Mathilde Gorieu', 'Helene Perrin', 'Marion Denorme', 'Fabien Pichon', 'Arnaud Desfeux']",,arXiv,2017,https://doi.org/10.48550/arXiv.1710.06590,Anomali
Basic tasks of sentiment analysis,"Subjectivity detection is the task of identifying objective and subjective sentences. Objective sentences are those which do not exhibit any sentiment. So, it is desired for a sentiment analysis engine to find and separate the objective sentences for further analysis, e.g., polarity detection. In subjective sentences, opinions can often be expressed on one or multiple topics. Aspect extraction is a subtask of sentiment analysis that consists in identifying opinion targets in opinionatedtext, i.e., in detecting the specific aspects of a product or service the opinion holder is either praising or complaining about.","['Iti Chaturvedi', 'Soujanya Poria', 'Erik Cambria']","Encyclopedia of Social Network Analysis and Mining, 2017",arXiv,2017,https://doi.org/10.48550/arXiv.1710.06536,Anomali
Classifying Web Exploits with Topic Modeling,"This short empirical paper investigates how well topic modeling and database meta-data characteristics can classify web and other proof-of-concept (PoC) exploits for publicly disclosed software vulnerabilities. By using a dataset comprised of over 36 thousand PoC exploits, near a 0.9 accuracy rate is obtained in the empirical experiment.Textminingand topic modeling are a significant boost factor behind this classification performance. In addition to these empirical results, the paper contributes to the research tradition of enhancing software vulnerability information withtextmining, providing also a few scholarly observations about the potential for semi-automatic classification of exploits in the existing tracking infrastructures.",['Jukka Ruohonen'],,arXiv,2017,https://doi.org/10.48550/arXiv.1710.05561,Anomali
NoReC: The Norwegian Review Corpus,"This paper presents the Norwegian Review Corpus (NoReC), created for training and evaluating models for document-level sentiment analysis. The full-textreviews have been collected from major Norwegian news sources and cover a range of different domains, including literature, movies, video games, restaurants, music and theater, in addition to product reviews across a range of categories. Each review is labeled with a manually assigned score of 1-6, as provided by the rating of the original author. This first release of the corpus comprises more than 35,000 reviews. It is distributed using the CoNLL-U format, pre-processed using UDPipe, along with a rich set of metadata. The work reported in this paper forms part of the SANT initiative (Sentiment Analysis for NorwegianText), a project seeking to provide resources and tools for sentiment analysis and opinionminingfor Norwegian. As resources for sentiment analysis have so far been unavailable for Norwegian, NoReC represents a highly valuable and sought-after addition to Norwegian language technology.","['Erik Velldal', 'Lilja Øvrelid', 'Eivind Alexander Bergem', 'Cathrine Stadsnes', 'Samia Touileb', 'Fredrik Jørgensen']",,arXiv,2017,https://doi.org/10.48550/arXiv.1710.05370,Anomali
Measurement Context Extraction from Text: Discovering Opportunities and Gaps in Earth Science,"We propose Marve, a system for extracting measurement values, units, and related words from natural languagetext. Marve uses conditional random fields (CRF) to identify measurement values and units, followed by a rule-based system to find related entities, descriptors and modifiers within a sentence. Sentence tokens are represented by an undirected graphical model, and rules are based on part-of-speech and word dependency patterns connecting values and units to contextual words. Marve is unique in its focus on measurement context and early experimentation demonstrates Marve's ability to generate high-precision extractions with strong recall. We also discuss Marve's role in refining measurement requirements for NASA's proposed HyspIRI mission, a hyperspectral infrared imaging satellite that will study the world's ecosystems. In general, our work with HyspIRI demonstrates the value of semantic measurement extractions in characterizing quantitative discussion contained in large corpuses of natural languagetext. These extractions accelerate broad, cross-cutting research and expose scientists new algorithmic approaches and experimental nuances. They also facilitate identification of scientific opportunities enabled by HyspIRI leading to more efficient scientific investment and research.","['Kyle Hundman', 'Chris A. Mattmann']","23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Data-Driven Discovery Workshop, Halifax, Canada, August 2017",arXiv,2017,https://doi.org/10.48550/arXiv.1710.04312,Anomali
Decision support from financial disclosures with deep neural networks and transfer learning,"Company disclosures greatly aid in the process of financial decision-making; therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods oftextmining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives.","['Mathias Kraus', 'Stefan Feuerriegel']",,arXiv,2017,https://doi.org/10.48550/arXiv.1710.03954,Anomali
Food for Thought: Analyzing Public Opinion on the Supplemental Nutrition Assistance Program,"This project explores public opinion on the Supplemental Nutrition Assistance Program (SNAP) in news and social media outlets, and tracks elected representatives' voting records on issues relating to SNAP and food insecurity. We used machine learning, sentiment analysis, andtextminingto analyze national and state level coverage of SNAP in order to gauge perceptions of the program over time across these outlets. Results indicate that the majority of news coverage has negative sentiment, more partisan news outlets have more extreme sentiment, and that clustering of negative reporting on SNAP occurs in the Midwest. Our final results and tools will be displayed in an on-line application that the ACFB Advocacy team can use to inform their communication to relevant stakeholders.","['Miriam Chappelka', 'Jihwan Oh', 'Dorris Scott', 'Mizzani Walker-Holmes']",,arXiv,2017,https://doi.org/10.48550/arXiv.1710.02443,Anomali
Predicting Disease-Gene Associations using Cross-Document Graph-based Features,"In the context of personalized medicine,textminingmethods pose an interesting option for identifying disease-gene associations, as they can be used to generate novel links between diseases and genes which may complement knowledge from structured databases. The most straightforward approach to extract such links fromtextis to rely on a simple assumption postulating an association between all genes and diseases that co-occur within the same document. However, this approach (i) tends to yield a number of spurious associations, (ii) does not capture different relevant types of associations, and (iii) is incapable of aggregating knowledge that is spread across documents. Thus, we propose an approach in which disease-gene co-occurrences and gene-gene interactions are represented in an RDF graph. A machine learning-based classifier is trained that incorporates features extracted from the graph to separate disease-gene pairs into valid disease-gene associations and spurious ones. On the manually curated Genetic Testing Registry, our approach yields a 30 points increase in F1 score over a plain co-occurrence baseline.","['Hendrik ter Horst', 'Matthias Hartung', 'Roman Klinger', 'Matthias Zwick', 'Philipp Cimiano']",,arXiv,2017,https://doi.org/10.48550/arXiv.1709.09239,Anomali
"Computational Content Analysis of Negative Tweets for Obesity, Diet, Diabetes, and Exercise","Social media based digital epidemiology has the potential to support faster response and deeper understanding of public health related threats. This study proposes a new framework to analyze unstructured health related textual data via Twitter users' post (tweets) to characterize the negative health sentiments and non-health related concerns in relations to the corpus of negative sentiments, regarding Diet Diabetes Exercise, and Obesity (DDEO). Through the collection of 6 million Tweets for one month, this study identified the prominent topics of users as it relates to the negative sentiments. Our proposed framework uses twotextminingmethods, sentiment analysis and topic modeling, to discover negative topics. The negative sentiments of Twitter users support the literature narratives and the many morbidity issues that are associated with DDEO and the linkage between obesity and diabetes. The framework offers a potential method to understand the publics' opinions and sentiments regarding DDEO. More importantly, this research provides new opportunities for computational social scientists, medical experts, and public health professionals to collectively address DDEO-related issues.","['George Shaw Jr.', 'Amir Karami']",,arXiv,2017,https://doi.org/10.48550/arXiv.1709.07915,Anomali
Analyzing users' sentiment towards popular consumer industries and brands on Twitter,"Social media serves as a unified platform for users to express their thoughts on subjects ranging from their daily lives to their opinion on consumer brands and products. These users wield an enormous influence in shaping the opinions of other consumers and influence brand perception, brand loyalty and brand advocacy. In this paper, we analyze the opinion of 19M Twitter users towards 62 popular industries, encompassing 12,898 enterprise and consumer brands, as well as associated subject matter topics, via sentiment analysis of 330M tweets over a period spanning a month. We find that users tend to be most positive towards manufacturing and most negative towards service industries. In addition, they tend to be more positive or negative when interacting with brands than generally on Twitter. We also find that sentiment towards brands within an industry varies greatly and we demonstrate this using two industries as use cases. In addition, we discover that there is no strong correlation between topic sentiments of different industries, demonstrating that topic sentiments are highly dependent on the context of the industry that they are mentioned in. We demonstrate the value of such an analysis in order to assess the impact of brands on social media. We hope that this initial study will prove valuable for both researchers and companies in understanding users' perception of industries, brands and associated topics and encourage more research in this field.","['Guoning Hu', 'Preeti Bhargava', 'Saul Fuhrmann', 'Sarah Ellinger', 'Nemanja Spasojevic']",2017 IEEE International Conference on Data Mining Workshops (ICDMW 2017),arXiv,2017,https://doi.org/10.48550/arXiv.1709.07434,Anomali
Inducing Distant Supervision in Suggestion Mining through Part-of-Speech Embeddings,"Miningsuggestion expressing sentences from a giventextis a less investigated sentence classification task, and therefore lacks hand labeled benchmark datasets. In this work, we propose and evaluate two approaches for distant supervision in suggestionmining. The distant supervision is obtained through a large silver standard dataset, constructed using thetextfrom wikiHow and Wikipedia. Both the approaches use a LSTM based neural network architecture to learn a classification model for suggestionmining, but vary in their method to use the silver standard dataset. The first approach directly trains the classifier using this dataset, while the second approach only learns word embeddings from this dataset. In the second approach, we also learn POS embeddings, which interestingly gives the best classification accuracy.","['Sapna Negi', 'Paul Buitelaar']",,arXiv,2017,https://doi.org/10.48550/arXiv.1709.07403,Anomali
A textual transform of multivariate time-series for prognostics,"Prognostics or early detection of incipient faults is an important industrial challenge for condition-based and preventive maintenance. Physics-based approaches to modeling fault progression are infeasible due to multiple interacting components, uncontrolled environmental factors and observability constraints. Moreover, such approaches to prognostics do not generalize to new domains. Consequently, domain-agnostic data-driven machine learning approaches to prognostics are desirable. Damage progression is a path-dependent process and explicitly modeling the temporal patterns is critical for accurate estimation of both the current damage state and its progression leading to total failure. In this paper, we present a novel data-driven approach to prognostics that employs a novel textual representation of multivariate temporal sensor observations for predicting the future health state of the monitored equipment early in its life. This representation enables us to utilize well-understood concepts fromtext-miningfor modeling, prediction and understanding distress patterns in a domain agnostic way. The approach has been deployed and successfully tested on large scale multivariate time-series data from commercial aircraft engines. We report experiments on well-known publicly available benchmark datasets and simulation datasets. The proposed approach is shown to be superior in terms of prediction accuracy, lead time to prediction and interpretability.","['Abhay Harpale', 'Abhishek Srivastav']",,arXiv,2017,https://doi.org/10.48550/arXiv.1709.06669,Anomali
A Recorded Debating Dataset,"This paper describes an English audio and textual dataset of debating speeches, a unique resource for the growing research field of computational argumentation and debating technologies. We detail the process of speech recording by professional debaters, the transcription of the speeches with an Automatic Speech Recognition (ASR) system, their consequent automatic processing to produce atextthat is more ""NLP-friendly"", and in parallel -- the manual transcription of the speeches in order to produce gold-standard ""reference"" transcripts. We release 60 speeches on various controversial topics, each in five formats corresponding to the different stages in the production of the data. The intention is to allow utilizing this resource for multiple research purposes, be it the addition of in-domain training data for a debate-specific ASR system, or applying argumentationminingon either noisy or clean debate transcripts. We intend to make further releases of this data in the future.","['Shachar Mirkin', 'Michal Jacovi', 'Tamar Lavee', 'Hong-Kwang Kuo', 'Samuel Thomas', 'Leslie Sager', 'Lili Kotlerman', 'Elad Venezian', 'Noam Slonim']",,arXiv,2018,https://doi.org/10.48550/arXiv.1709.06438,Anomali
Asymptotic Bayesian Generalization Error in Latent Dirichlet Allocation and Stochastic Matrix Factorization,"Latent Dirichlet allocation (LDA) is useful in document analysis, image processing, and many information systems; however, its generalization performance has been left unknown because it is a singular learning machine to which regular statistical theory can not be applied.
  Stochastic matrix factorization (SMF) is a restricted matrix factorization in which matrix factors are stochastic; the column of the matrix is in a simplex. SMF is being applied to image recognition andtextmining. We can understand SMF as a statistical model by which a stochastic matrix of given data is represented by a product of two stochastic matrices, whose generalization performance has also been left unknown because of non-regularity.
  In this paper, by using an algebraic and geometric method, we show the analytic equivalence of LDA and SMF, both of which have the same real log canonical threshold (RLCT), resulting in that they asymptotically have the same Bayesian generalization error and the same log marginal likelihood. Moreover, we derive the upper bound of the RLCT and prove that it is smaller than the dimension of the parameter divided by two, hence the Bayesian generalization errors of them are smaller than those of regular statistical models.","['Naoki Hayashi', 'Sumio Watanabe']","SN Computer Science volume 1, Article number: 69 (2020)",arXiv,2020,https://doi.org/10.48550/arXiv.1709.04212,Anomali
AppTechMiner: Mining Applications and Techniques from Scientific Articles,"This paper presents AppTechMiner, a rule-based information extraction framework that automatically constructs a knowledge base of all application areas and problem solving techniques. Techniques include tools, methods, datasets or evaluation metrics. We also categorize individual research articles based on their application areas and the techniques proposed/improved in the article. Our system achieves high average precision (~82%) and recall (~84%) in knowledge base creation. It also performs well in application and technique assignment to an individual article (average accuracy ~66%). In the end, we further present two use cases presenting a trivial information retrieval system and an extensive temporal analysis of the usage of techniques and application areas. At present, we demonstrate the framework for the domain of computational linguistics but this can be easily generalized to any other field of research.","['Mayank Singh', 'Soham Dan', 'Sanyam Agarwal', 'Pawan Goyal', 'Animesh Mukherjee']",,arXiv,2017,https://doi.org/10.48550/arXiv.1709.03064,Anomali
Fine-grained Visual-textual Representation Learning,"Fine-grained visual categorization is to recognize hundreds of subcategories belonging to the same basic-level category, which is a highly challenging task due to the quite subtle and local visual distinctions among similar subcategories. Most existing methods generally learn part detectors to discover discriminative regions for better categorization performance. However, not all parts are beneficial and indispensable for visual categorization, and the setting of part detector number heavily relies on prior knowledge as well as experimental validation. As is known to all, when we describe the object of an image via textual descriptions, we mainly focus on the pivotal characteristics, and rarely pay attention to common characteristics as well as the background areas. This is an involuntary transfer from human visual attention to textual attention, which leads to the fact that textual attention tells us how many and which parts are discriminative and significant to categorization. So textual attention could help us to discover visual attention in image. Inspired by this, we propose a fine-grained visual-textual representation learning (VTRL) approach, and its main contributions are: (1) Fine-grained visual-textual patternminingdevotes to discovering discriminative visual-textual pairwise information for boosting categorization performance through jointly modeling vision andtextwith generative adversarial networks (GANs), which automatically and adaptively discovers discriminative parts. (2) Visual-textual representation learning jointly combines visual and textual information, which preserves the intra-modality and inter-modality information to generate complementary fine-grained representation, as well as further improves categorization performance.","['Xiangteng He', 'Yuxin Peng']",,arXiv,2019,https://doi.org/10.48550/arXiv.1709.00340,Anomali
Vector Space Model as Cognitive Space for Text Classification,"In this era of digitization, knowing the user's sociolect aspects have become essential features to build the user specific recommendation systems. These sociolect aspects could be found byminingthe user's language sharing in the form oftextin social media and reviews. This paper describes about the experiment that was performed in PAN Author Profiling 2017 shared task. The objective of the task is to find the sociolect aspects of the users from their tweets. The sociolect aspects considered in this experiment are user's gender and native language information. Here user's tweets written in a different language from their native language are represented as Document - Term Matrix with document frequency as the constraint. Further classification is done using the Support Vector Machine by taking gender and native language as target classes. This experiment attains the average accuracy of 73.42% in gender prediction and 76.26% in the native language identification task.","['Barathi Ganesh HB', 'Anand Kumar M', 'Soman KP']",,arXiv,2017,https://doi.org/10.48550/arXiv.1708.06068,Anomali
ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models,"Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification,textmining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs.
  Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack and significantly outperforms existing black-box attacks via substitute models.","['Pin-Yu Chen', 'Huan Zhang', 'Yash Sharma', 'Jinfeng Yi', 'Cho-Jui Hsieh']",,arXiv,2017,https://doi.org/10.48550/arXiv.1708.03999,Anomali
Mining fine-grained opinions on closed captions of YouTube videos with an attention-RNN,"Video reviews are the natural evolution of written product reviews. In this paper we target this phenomenon and introduce the first dataset created from closed captions of YouTube product review videos as well as a new attention-RNN model for aspect extraction and joint aspect extraction and sentiment classification. Our model provides state-of-the-art performance on aspect extraction without requiring the usage of hand-crafted features on the SemEval ABSA corpus, while it outperforms the baseline on the joint task. In our dataset, the attention-RNN model outperforms the baseline for both tasks, but we observe important performance drops for all models in comparison to SemEval. These results, as well as further experiments on domain adaptation for aspect extraction, suggest that differences between speech and writtentext, which have been discussed extensively in the literature, also extend to the domain of product reviews, where they are relevant for fine-grained opinionmining.","['Edison Marrese-Taylor', 'Jorge A. Balazs', 'Yutaka Matsuo']",,arXiv,2017,https://doi.org/10.48550/arXiv.1708.02420,Anomali
Recent Developments and Future Challenges in Medical Mixed Reality,"Mixed Reality (MR) is of increasing interest within technology-driven modern medicine but is not yet used in everyday practice. This situation is changing rapidly, however, and this paper explores the emergence of MR technology and the importance of its utility within medical applications. A classification of medical MR has been obtained by applying an unbiasedtextminingmethod to a database of 1,403 relevant research papers published over the last two decades. The classification results reveal a taxonomy for the development of medical MR research during this period as well as suggesting future trends. We then use the classification to analyse the technology and applications developed in the last five years. Our objective is to aid researchers to focus on the areas where technology advancements in medical MR are most needed, as well as providing medical practitioners with a useful source of reference.","['Long Chen', 'Thomas Day', 'Wen Tang', 'Nigel W. John']",,arXiv,2017,https://doi.org/10.48550/arXiv.1708.01225,Anomali
"A Graph Analytics Framework for Ranking Authors, Papers and Venues","A lot of scientific works are published in different areas of science, technology, engineering and mathematics. It is not easy, even for experts, to judge the quality of authors, papers and venues (conferences and journals). An objective measure to assign scores to these entities and to rank them is very useful. Although, several metrics and indexes have been proposed earlier, they suffer from various problems. In this paper, we propose a graph-based analytics framework to assign scores and to rank authors, papers and venues. Our algorithm considers only the link structures of the underlying graphs. It does not take into account other aspects, such as the associatedtextsand the reputation of these entities. In the limit of large number of iterations, the solution of the iterative equations gives the unique entity scores. This framework can be easily extended to other interdependent networks.","['Arindam Pal', 'Sushmita Ruj']",,arXiv,2017,https://doi.org/10.48550/arXiv.1708.00329,Anomali
Visual Relationship Detection with Internal and External Linguistic Knowledge Distillation,"Understanding visual relationships involves identifying the subject, the object, and a predicate relating them. We leverage the strong correlations between the predicate and the (subj,obj) pair (both semantically and spatially) to predict the predicates conditioned on the subjects and the objects. Modeling the three entities jointly more accurately reflects their relationships, but complicates learning since the semantic space of visual relationships is huge and the training data is limited, especially for the long-tail relationships that have few instances. To overcome this, we use knowledge of linguistic statistics to regularize visual model learning. We obtain linguistic knowledge byminingfrom both training annotations (internal knowledge) and publicly availabletext, e.g., Wikipedia (external knowledge), computing the conditional probability distribution of a predicate given a (subj,obj) pair. Then, we distill the knowledge into a deep model to achieve better generalization. Our experimental results on the Visual Relationship Detection (VRD) and Visual Genome datasets suggest that with this linguistic knowledge distillation, our model outperforms the state-of-the-art methods significantly, especially when predicting unseen relationships (e.g., recall improved from 8.45% to 19.17% on VRD zero-shot testing set).","['Ruichi Yu', 'Ang Li', 'Vlad I. Morariu', 'Larry S. Davis']",,arXiv,2017,https://doi.org/10.48550/arXiv.1707.09423,Anomali
Mining Device-Specific Apps Usage Patterns from Large-Scale Android Users,"When smartphones, applications (a.k.a, apps), and app stores have been widely adopted by the billions, an interesting debate emerges: whether and to what extent do device models influence the behaviors of their users? The answer to this question is critical to almost every stakeholder in the smartphone app ecosystem, including app store operators, developers, end-users, and network providers. To approach this question, we collect a longitudinal data set of app usage through a leading Android app store in China, called Wandoujia. The data set covers the detailed behavioral profiles of 0.7 million (761,262) unique users who use 500 popular types of Android devices and about 0.2 million (228,144) apps, including their app management activities, daily network access time, and network traffic of apps. We present a comprehensive study on investigating how the choices of device models affect user behaviors such as the adoption of app stores, app selection and abandonment, data plan usage, online time length, the tendency to use paid/free apps, and the preferences to choosing competing apps. Some significant correlations between device models and app usage are derived, leading to important findings on the various user behaviors. For example, users owning different device models have a substantial diversity of selecting competing apps, and users owning lower-end devices spend more money to purchase apps and spend more time under cellular network.","['Huoran Li', 'Xuan Lu']",,arXiv,2017,https://doi.org/10.48550/arXiv.1707.09252,Anomali
From Image to Text Classification: A Novel Approach based on Clustering Word Embeddings,"In this paper, we propose a novel approach fortextclassification based on clustering word embeddings, inspired by the bag of visual words model, which is widely used in computer vision. After each word in a collection of documents is represented as word vector using a pre-trained word embeddings model, a k-means algorithm is applied on the word vectors in order to obtain a fixed-size set of clusters. The centroid of each cluster is interpreted as a super word embedding that embodies all the semantically related word vectors in a certain region of the embedding space. Every embedded word in the collection of documents is then assigned to the nearest cluster centroid. In the end, each document is represented as a bag of super word embeddings by computing the frequency of each super word embedding in the respective document. We also diverge from the idea of building a single vocabulary for the entire collection of documents, and propose to build class-specific vocabularies for better performance. Using this kind of representation, we report results on twotextminingtasks, namelytextcategorization by topic and polarity classification. On both tasks, our model yields better performance than the standard bag of words.","['Andrei M. Butnaru', 'Radu Tudor Ionescu']",,arXiv,2017,https://doi.org/10.48550/arXiv.1707.08098,Anomali
Relational Learning and Feature Extraction by Querying over Heterogeneous Information Networks,"Many real world systems need to operate on heterogeneous information networks that consist of numerous interacting components of different types. Examples include systems that perform data analysis on biological information networks; social networks; and information extraction systems processing unstructured data to convert rawtextto knowledge graphs. Many previous works describe specialized approaches to perform specific types of analysis,miningand learning on such networks. In this work, we propose a unified framework consisting of a data model -a graph with a first order schema along with a declarative language for constructing, querying and manipulating such networks in ways that facilitate relational and structured machine learning. In particular, we provide an initial prototype for a relational and graph traversal query language where queries are directly used as relational features for structured machine learning models. Feature extraction is performed by making declarative graph traversal queries. Learning and inference models can directly operate on this relational representation and augment it with new data and knowledge that, in turn, is integrated seamlessly into the relational structure to support new predictions. We demonstrate this system's capabilities by showcasing tasks in natural language processing and computational biology domains.","['Parisa Kordjamshidi', 'Sameer Singh', 'Daniel Khashabi', 'Christos Christodoulopoulos', 'Mark Summons', 'Saurabh Sinha', 'Dan Roth']",,arXiv,2017,https://doi.org/10.48550/arXiv.1707.07794,Anomali
A guide to performing systematic literature reviews in bioinformatics,"Bioinformatics research depends on high-quality databases to provide accurate results. In silico experiments, correctly performed, may prospect novel discoveries and elucidates pathways for biological experiments through data analysis in large scale. However, most biological databases have presented mistakes, such as data incorrectly classified or incomplete information. Also, sometimes, dataminingalgorithms cannot treat these errors, leading to serious problems for the in silico analysis. Manual curation of data extracted from literature is a possible solution for this problem. Systematic Literature Review (SLR), or Systematic Review, is a method to identify, evaluate and summarize the state-of-the-art of a specific theme. Moreover, SLR allows the collection from databases restrictively, which allows an analysis with lower bias than traditional reviews. The SRL approaches have been widely used for decision-making in medical and environmental studies. However, other research areas, such as bioinformatics, do not have a specific step-by-step to guide researchers undertaking the procedures of an SLR. In this study, we propose a guideline, called BiSRL, to perform SLR in bioinformatics. Our procedures cover the most traditional guides to produce SLRs adapted to bioinformatics. To evaluate our method, we propose a case study to detect and summarize SLRs developed for bioinformatics data. We used two databases: PubMed and ScienceDirect. A total of 207 papers were screened in four steps: title, abstract, diagonal and full-textreading. Four evaluators performed the SLR independently to reduce bias risk. A total of 8 papers was included in the SLR case study. The case study demonstrates how to implement the methods of BiSLR to procedure SLR for bioinformatics. BiSLR may guide bioinformaticians to perform systematic reviews reproducible to collect accurate data for higher quality analysis.","['Diego C. B. Mariano', 'Carmelina Leite', 'Lucianna H. S. Santos', 'Rafael E. O. Rocha', 'Raquel C. de Melo-Minardi']",RT.DCC.002/2017,arXiv,2017,https://doi.org/10.48550/arXiv.1707.05813,Anomali
Cooperative Hierarchical Dirichlet Processes: Superposition vs. Maximization,"The cooperative hierarchical structure is a common and significant data structure observed in, or adopted by, many research areas, such as:textmining(author-paper-word) and multi-label classification (label-instance-feature). Renowned Bayesian approaches for cooperative hierarchical structure modeling are mostly based on topic models. However, these approaches suffer from a serious issue in that the number of hidden topics/factors needs to be fixed in advance and an inappropriate number may lead to overfitting or underfitting. One elegant way to resolve this issue is Bayesian nonparametric learning, but existing work in this area still cannot be applied to cooperative hierarchical structure modeling.
  In this paper, we propose a cooperative hierarchical Dirichlet process (CHDP) to fill this gap. Each node in a cooperative hierarchical structure is assigned a Dirichlet process to model its weights on the infinite hidden factors/topics. Together with measure inheritance from hierarchical Dirichlet process, two kinds of measure cooperation, i.e., superposition and maximization, are defined to capture the many-to-many relationships in the cooperative hierarchical structure. Furthermore, two constructive representations for CHDP, i.e., stick-breaking and international restaurant process, are designed to facilitate the model inference. Experiments on synthetic and real-world data with cooperative hierarchical structures demonstrate the properties and the ability of CHDP for cooperative hierarchical structure modeling and its potential for practical application scenarios.","['Junyu Xuan', 'Jie Lu', 'Guangquan Zhang', 'Richard Yi Da Xu']",,arXiv,2017,https://doi.org/10.48550/arXiv.1707.05420,Anomali
Leipzig Corpus Miner - A Text Mining Infrastructure for Qualitative Data Analysis,"This paper presents the ""Leipzig Corpus Miner"", a technical infrastructure for supporting qualitative and quantitative content analysis. The infrastructure aims at the integration of 'close reading' procedures on individual documents with procedures of 'distant reading', e.g. lexical characteristics of large document collections. Therefore information retrieval systems, lexicometric statistics and machine learning procedures are combined in a coherent framework which enables qualitative data analysts to make use of state-of-the-art Natural Language Processing techniques on very large document collections. Applicability of the framework ranges from social sciences to media studies and market research. As an example we introduce the usage of the framework in a political science study on post-democracy and neoliberalism.","['Andreas Niekler', 'Gregor Wiedemann', 'Gerhard Heyer']",,arXiv,2017,https://doi.org/10.48550/arXiv.1707.03253,Anomali
"A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques","The amount oftextthat is generated every day is increasing dramatically. This tremendous volume of mostly unstructuredtextcannot be simply processed and perceived by computers. Therefore, efficient and effective techniques and algorithms are required to discover useful patterns.Textminingis the task of extracting meaningful information fromtext, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamentaltextminingtasks and techniques includingtextpre-processing, classification and clustering. Additionally, we briefly explaintextminingin biomedical and health care domains.","['Mehdi Allahyari', 'Seyedamin Pouriyeh', 'Mehdi Assefi', 'Saied Safaei', 'Elizabeth D. Trippe', 'Juan B. Gutierrez', 'Krys Kochut']",,arXiv,2017,https://doi.org/10.48550/arXiv.1707.02919,Anomali
Determining sentiment in citation text and analyzing its impact on the proposed ranking index,"Whenever human beings interact with each other, they exchange or express opinions, emotions, and sentiments. These opinions can be expressed intext, speech or images. Analysis of these sentiments is one of the popular research areas of present day researchers. Sentiment analysis, also known as opinionminingtries to identify or classify these sentiments or opinions into two broad categories - positive and negative. In recent years, the scientific community has taken a lot of interest in analyzing sentiment in textual data available in various social media platforms. Much work has been done on social media conversations, blog posts, newspaper articles and various narrativetexts. However, when it comes to identifying emotions from scientific papers, researchers have faced some difficulties due to the implicit and hidden nature of opinion. By default, citation instances are considered inherently positive in emotion. Popular ranking and indexing paradigms often neglect the opinion present while citing. In this paper, we have tried to achieve three objectives. First, we try to identify the major sentiment in the citationtextand assign a score to the instance. We have used a statistical classifier for this purpose. Secondly, we have proposed a new index (we shall refer to it hereafter as M-index) which takes into account both the quantitative and qualitative factors while scoring a paper. Thirdly, we developed a ranking of research papers based on the M-index. We also try to explain how the M-index impacts the ranking of scientific papers.","['Souvick Ghosh', 'Dipankar Das', 'Tanmoy Chakraborty']",,arXiv,2017,https://doi.org/10.48550/arXiv.1707.01425,Anomali
JaTeCS an open-source JAva TExt Categorization System,"JaTeCS is an open source Java library that supports research on automatictextcategorization and other related problems, such as ordinal regression and quantification, which are of special interest in opinionminingapplications. It covers all the steps of an experimental activity, from reading the corpus to the evaluation of the experimental results. As JaTeCS is focused ontextas the main input data, it provides the user with manytext-dedicated tools, e.g.: data readers for many formats, including the most commonly usedtextcorpora and lexical resources, natural language processing tools, multi-language support, methods for feature selection and weighting, the implementation of many machine learning algorithms as well as wrappers for well-known external software (e.g., SVM_light) which enable their full control from code. JaTeCS support its expansion by abstracting through interfaces many of the typical tools and procedures used intextprocessing tasks. The library also provides a number of ""template"" implementations of typical experimental setups (e.g., train-test, k-fold validation, grid-search optimization, randomized runs) which enable fast realization of experiments just by connecting the templates with data readers, learning algorithms and evaluation measures.","['Andrea Esuli', 'Tiziano Fagni', 'Alejandro Moreo Fernandez']",,arXiv,2017,https://doi.org/10.48550/arXiv.1706.06802,Anomali
Identifying Condition-Action Statements in Medical Guidelines Using Domain-Independent Features,"This paper advances the state of the art intextunderstanding of medical guidelines by releasing two new annotated clinical guidelines datasets, and establishing baselines for using machine learning to extract condition-action pairs. In contrast to prior work that relies on manually created rules, we report experiment with several supervised machine learning techniques to classify sentences as to whether they express conditions and actions. We show the limitations and possible extensions of this work ontextminingof medical guidelines.","['Hossein Hematialam', 'Wlodek Zadrozny']",,arXiv,2017,https://doi.org/10.48550/arXiv.1706.04206,Anomali
Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2017),"The large scale of scholarly publications poses a challenge for scholars in information seeking and sensemaking. Bibliometrics, information retrieval (IR),textminingand NLP techniques could help in these search and look-up activities, but are not yet widely used. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometrics,textminingand recommendation techniques that can advance the state-of-the-art in scholarly document understanding, analysis, and retrieval at scale. The BIRNDL workshop at SIGIR 2017 will incorporate an invited talk, paper sessions and the third edition of the Computational Linguistics (CL) Scientific Summarization Shared Task.","['Muthu Kumar Chandrasekaran', 'Kokil Jaidka', 'Philipp Mayr']",,arXiv,2017,https://doi.org/10.48550/arXiv.1706.02509,Anomali
Mining Process Model Descriptions of Daily Life through Event Abstraction,"Processminingtechniques focus on extracting insight in processes from event logs. Processmininghas the potential to provide valuable insights in (un)healthy habits and to contribute to ambient assisted living solutions when applied on data from smart home environments. However, events recorded in smart home environments are on the level of sensor triggers, at which process discovery algorithms produce overgeneralizing process models that allow for too much behavior and that are difficult to interpret for human experts. We show that abstracting the events to a higher-level interpretation can enable discovery of more precise and more comprehensible models. We present a framework for the extraction of features that can be used for abstraction with supervised learning methods that is based on the XES IEEE standard for event logs. This framework can automatically abstract sensor-level events to their interpretation at the human activity level, after training it on training data for which both the sensor and human activity events are known. We demonstrate our abstraction framework on three real-life smart home event logs and show that the process models that can be discovered after abstraction are more precise indeed.","['Niek Tax', 'Natalia Sidorova', 'Reinder Haakma', 'Wil M. P. van der Aalst']","Studies in Computational Intelligence, 751 (2017) 83-104",arXiv,2017,https://doi.org/10.48550/arXiv.1705.10202,Anomali
Max-Cosine Matching Based Neural Models for Recognizing Textual Entailment,"Recognizing textual entailment is a fundamental task in a variety oftextminingor natural language processing applications. This paper proposes a simple neural model for RTE problem. It first matches each word in the hypothesis with its most-similar word in the premise, producing an augmented representation of the hypothesis conditioned on the premise as a sequence of word pairs. The LSTM model is then used to model this augmented sequence, and the final output from the LSTM is fed into a softmax layer to make the prediction. Besides the base model, in order to enhance its performance, we also proposed three techniques: the integration of multiple word-embedding library, bi-way integration, and ensemble based on model averaging. Experimental results on the SNLI dataset have shown that the three techniques are effective in boosting the predicative accuracy and that our method outperforms several state-of-the-state ones.","['Zhipeng Xie', 'Junfeng Hu']",DASFAA (1) 2017: 295-308,arXiv,2017,https://doi.org/10.48550/arXiv.1705.09054,Anomali
Self-supervised learning of visual features through embedding images into text topic spaces,"End-to-end training from scratch of current deep architectures for new computer vision problems would require Imagenet-scale datasets, and this is not always possible. In this paper we present a method that is able to take advantage of freely available multi-modal content to train computer vision algorithms without human supervision. We put forward the idea of performing self-supervised learning of visual features bymininga large scale corpus of multi-modal (textand image) documents. We show that discriminative visual features can be learnt efficiently by training a CNN to predict the semantic context in which a particular image is more probable to appear as an illustration. For this we leverage the hidden semantic structures discovered in thetextcorpus with a well-known topic modeling technique. Our experiments demonstrate state of the art performance in image classification, object detection, and multi-modal retrieval compared to recent self-supervised or natural-supervised approaches.","['Lluis Gomez', 'Yash Patel', 'Marçal Rusiñol', 'Dimosthenis Karatzas', 'C. V. Jawahar']",,arXiv,2017,https://doi.org/10.48550/arXiv.1705.08631,Anomali
Towards Interrogating Discriminative Machine Learning Models,"It is oftentimes impossible to understand how machine learning models reach a decision. While recent research has proposed various technical approaches to provide some clues as to how a learning model makes individual decisions, they cannot provide users with ability to inspect a learning model as a complete entity. In this work, we propose a new technical approach that augments a Bayesian regression mixture model with multiple elastic nets. Using the enhanced mixture model, we extract explanations for a target model through global approximation. To demonstrate the utility of our approach, we evaluate it on different learning models covering the tasks oftextminingand image recognition. Our results indicate that the proposed approach not only outperforms the state-of-the-art technique in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of a learning model.","['Wenbo Guo', 'Kaixuan Zhang', 'Lin Lin', 'Sui Huang', 'Xinyu Xing']",,arXiv,2017,https://doi.org/10.48550/arXiv.1705.08564,Anomali
Learning Convolutional Text Representations for Visual Question Answering,"Visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images andtexts. In deep learning, images are typically modeled through convolutional neural networks, andtextsare typically modeled through recurrent neural networks. While the requirement for modeling images is similar to traditional computer vision tasks, such as object recognition and image classification, visual question answering raises a different need for textual representation as compared to other natural language processing tasks. In this work, we perform a detailed analysis on natural language questions in visual question answering. Based on the analysis, we propose to rely on convolutional neural networks for learning textual representations. By exploring the various properties of convolutional neural networks specialized fortextdata, such as width and depth, we present our ""CNN Inception + Gate"" model. We show that our model improves question representations and thus the overall accuracy of visual question answering models. We also show that thetextrepresentation requirement in visual question answering is more complicated and comprehensive than that in conventional natural language processing tasks, making it a better task to evaluate textual representation methods. Shallow models like fastText, which can obtain comparable results with deep learning models in tasks liketextclassification, are not suitable in visual question answering.","['Zhengyang Wang', 'Shuiwang Ji']",In proceedings of the 2018 SIAM International Conference on Data Mining (pp. 594-602). 2018,arXiv,2018,https://doi.org/10.48550/arXiv.1705.06824,Anomali
Social Media-based Substance Use Prediction,"In this paper, we demonstrate how the state-of-the-art machine learning andtextminingtechniques can be used to build effective social media-based substance use detection systems. Since a substance use ground truth is difficult to obtain on a large scale, to maximize system performance, we explore different feature learning methods to take advantage of a large amount of unsupervised social media data. We also demonstrate the benefit of using multi-view unsupervised feature learning to combine heterogeneous user information such as Facebook `""likes"" and ""status updates"" to enhance system performance. Based on our evaluation, our best models achieved 86% AUC for predicting tobacco use, 81% for alcohol use and 84% for drug use, all of which significantly outperformed existing methods. Our investigation has also uncovered interesting relations between a user's social media behavior (e.g., word usage) and substance use.","['Tao Ding', 'Warren K. Bickel', 'Shimei Pan']",,arXiv,2017,https://doi.org/10.48550/arXiv.1705.05633,Anomali
ResumeVis: A Visual Analytics System to Discover Semantic Information in Semi-structured Resume Data,"Massive public resume data emerging on the WWW indicates individual-related characteristics in terms of profile and career experiences. Resume Analysis (RA) provides opportunities for many applications, such as talent seeking and evaluation. Existing RA studies based on statistical analyzing have primarily focused on talent recruitment by identifying explicit attributes. However, they failed to discover the implicit semantic information, i.e., individual career progress patterns and social-relations, which are vital to comprehensive understanding of career development. Besides, how to visualize them for better human cognition is also challenging. To tackle these issues, we propose a visual analytics system ResumeVis tomineand visualize resume data. Firstly, atext-miningbased approach is presented to extract semantic information. Then, a set of visualizations are devised to represent the semantic information in multiple perspectives. By interactive exploration on ResumeVis performed by domain experts, the following tasks can be accomplished: to trace individual career evolving trajectory; tominelatent social-relations among individuals; and to hold the full picture of massive resumes' collective mobility. Case studies with over 2500 online officer resumes demonstrate the effectiveness of our system. We provide a demonstration video.","['Chen Zhang', 'Hao Wang', 'Yingcai Wu']",,arXiv,2017,https://doi.org/10.48550/arXiv.1705.05206,Anomali
Learning Semantic Correspondences in Technical Documentation,"We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation. We focus specifically on the problem of learning translational correspondences betweentextdescriptions and grounded representations in the target documentation, such as formal representation of functions or code templates. Our approach exploits the parallel nature of such documentation, or the tight coupling between high-leveltextand the low-level representations we aim to learn. Data is collected byminingtechnical documents for such paralleltext-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals.","['Kyle Richardson', 'Jonas Kuhn']",,arXiv,2017,https://doi.org/10.48550/arXiv.1705.04815,Anomali
Clustering Airbnb Reviews,"In the last decade, online customer reviews increasingly exert influence on consumers' decision when booking accommodation online. The renewal importance to the concept of word-of mouth is reflected in the growing interests in investigating consumers' experience by analyzing their online reviews through the process oftextminingand sentiment analysis. A clustering approach is developed for Boston Airbnb reviews submitted in the English language and collected from 2009 to 2016. This approach is based on a mixture of latent variable models, which provides an appealing framework for handling clustered binary data. We address here the problem of discovering meaningful segments of consumers that are coherent from both the underlying topics and the sentiment behind the reviews. A penalized mixture of latent traits approach is developed to reduce the number of parameters and identify variables that are not informative for clustering. The introduction of component-specific rate parameters avoids the over-penalization that can occur when inferring a shared rate parameter on clustered data. We divided the guests into four groups -- property driven guests, host driven guests, guests with recent overall negative stay and guests with some negative experiences.","['Yang Tang', 'Paul D. McNicholas']",,arXiv,2019,https://doi.org/10.48550/arXiv.1705.03134,Anomali
On Using Active Learning and Self-Training when Mining Performance Discussions on Stack Overflow,"Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator's work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for futuretextminers aspiring to use AL and self-training.","['Markus Borg', 'Iben Lennerstad', 'Rasmus Ros', 'Elizabeth Bjarnason']",,arXiv,2017,https://doi.org/10.48550/arXiv.1705.02395,Anomali
ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases,"The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals' Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems.
  In this paper, we present a new chest X-ray database, namely ""ChestX-ray8"", which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with thetext-minedeight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based ""reading chest X-rays"" (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems. Data download link: https://nihcc.app.box.com/v/ChestXray-NIHCC","['Xiaosong Wang', 'Yifan Peng', 'Le Lu', 'Zhiyong Lu', 'Mohammadhadi Bagheri', 'Ronald M. Summers']","IEEE CVPR 2017, pp. 2097-2106 (2017)",arXiv,2017,https://doi.org/10.48550/arXiv.1705.02315,Anomali
Crowdsourcing Argumentation Structures in Chinese Hotel Reviews,"Argumentationminingaims at automatically extracting the premises-claim discourse structures in natural languagetexts. There is a great demand for argumentation corpora for customer reviews. However, due to the controversial nature of the argumentation annotation task, there exist very few large-scale argumentation corpora for customer reviews. In this work, we novelly use the crowdsourcing technique to collect argumentation annotations in Chinese hotel reviews. As the first Chinese argumentation dataset, our corpus includes 4814 argument component annotations and 411 argument relation annotations, and its annotations qualities are comparable to some widely used argumentation corpora in other languages.","['Mengxue Li', 'Shiqiang Geng', 'Yang Gao', 'Haijing Liu', 'Hao Wang']",,arXiv,2017,https://doi.org/10.48550/arXiv.1705.02077,Anomali
Virtual Machine Introspection Based Malware Behavior Profiling and Family Grouping,"The proliferation of malwares have been attributed to the alternations of a handful of original malware source codes. The malwares alternated from the same origin share some intrinsic behaviors and form a malware family. Expediently, identifying its malware family when a malware is first seen on the Internet can provide useful clues to mitigate the threat. In this paper, a malware profiler (VMP) is proposed to profile the execution behaviors of a malware by leveraging virtual machine introspection (VMI) technique. The VMP inserts plug-ins inside the virtual machine monitor (VMM) to record the invoked API calls with their input parameters and return values as the profile of malware. In this paper, a popular similarity measurement Jaccard distance and a phylogenetic tree construction method are adopted to discover malware families. The studies of malware profiles show the malwares from a malware family are very similar to each others and distinct from other malware families as well as benign software. This paper also examines VMP against existing anti-malware detection engines and some well-known malware grouping methods to compare the goodness in their malware family constructions. A peer voting approach is proposed and the results show VMP is better than almost all of the compared anti-malware engines, and compatible with the fine tunedtext-miningapproach and high order N-gram approaches. We also establish a malware profiling website based on VMP for malware research.","['Shun-Wen Hsiao', 'Yeali S. Sun', 'Meng Chang Chen']",,arXiv,2017,https://doi.org/10.48550/arXiv.1705.01697,Anomali
A Methodology of Guiding Web Content Mining and Knowledge Discovery in Evidence-based Software Engineering,"Systematic Literature Review (SLR) is a rigorous methodology applied for Evidence-Based Software Engineering (EBSE) that identify, assess and synthesize the relevant evidence for answering specific research questions. Benefiting from the booming online materials in the era of Web 2.0, the technical Web content starts acting as alternative sources for EBSE. Web knowledge has been investigated and derived from Web contentminingand knowledge discovery techniques, however they are still significantly different from reviewing academic literature. Thus the direct adoption of Web knowledge in EBSE lacks of systematic guidelines. In this paper, we propose to make an SLR adaptation to bridge the aforementioned gap along two stages. Firstly, we follow the general logic and procedure of SLR to regulate Webminingactivities. Secondly, we substitute and enhance particular SLR processes with Web-mining-friendly methods and approaches. At the second stage, we mainly focus on adapting Conducting Review by integrating a set of automated components ranging from programmatic searching to varioustextminingtechniques.","['Zheng Li', 'Yan Liu']",,arXiv,2017,https://doi.org/10.48550/arXiv.1704.07551,Anomali
Dependency resolution and semantic mining using Tree Adjoining Grammars for Tamil Language,"Tree adjoining grammars (TAGs) provide an ample tool to capture syntax of many Indian languages. Tamil represents a special challenge to computational formalisms as it has extensive agglutinative morphology and a comparatively difficult argument structure. Modelling Tamil syntax and morphology using TAG is an interesting problem which has not been in focus even though TAGs are over 4 decades old, since its inception. Our research with Tamil TAGs have shown us that we can not only represent syntax of the language, but to an extentmineout semantics through dependency resolution of the sentence. But in order to demonstrate this phenomenal property, we need to parse Tamil language sentences using TAGs we have built and through parsing obtain a derivation we could use to resolve dependencies, thus proving the semantic property. We use an in-house developed pseudo lexical TAG chart parser; algorithm given by Schabes and Joshi (1988), for generating derivations of sentences. We do not use any statistics to rank out ambiguous derivations but rather use all of them to understand the mentioned semantic relation with in TAGs for Tamil. We shall also present a brief parser analysis for the completeness of our discussions.","['Vijay Krishna Menon', 'S Rajendran', 'M Anandkumar', 'K P Soman']",,arXiv,2017,https://doi.org/10.48550/arXiv.1704.05611,Anomali
Predicting Role Relevance with Minimal Domain Expertise in a Financial Domain,"Word embeddings have made enormous inroads in recent years in a wide variety oftextminingapplications. In this paper, we explore a word embedding-based architecture for predicting the relevance of a role between two financial entities within the context of natural language sentences. In this extended abstract, we propose a pooled approach that uses a collection of sentences to train word embeddings using the skip-gram word2vec architecture. We use the word embeddings to obtain context vectors that are assigned one or more labels based on manual annotations. We train a machine learning classifier using the labeled context vectors, and use the trained classifier to predict contextual role relevance on test data. Our approach serves as a good minimal-expertise baseline for the task as it is simple and intuitive, uses open-source modules, requires little feature crafting effort and performs well across roles.",['Mayank Kejriwal'],,arXiv,2017,https://doi.org/10.48550/arXiv.1704.05571,Anomali
MRA - Proof of Concept of a Multilingual Report Annotator Web Application,MRA (Multilingual Report Annotator) is a web application that translates Radiologytextand annotates it with RadLex terms. Its goal is to explore the solution of translating non-English Radiology reports as a way to solve the problem of most of theTextMiningtools being developed for English. In this brief paper we explain the language barrier problem and shortly describe the application. MRA can be found at https://github.com/lasigeBioTM/MRA .,"['Luís Campos', 'Francisco Couto']",,arXiv,2017,https://doi.org/10.48550/arXiv.1704.01748,Anomali
Review on Requirements Modeling and Analysis for Self-Adaptive Systems: A Ten-Year Perspective,"Context: Over the last decade, software researchers and engineers have developed a vast body of methodologies and technologies in requirements engineering for self-adaptive systems. Although existing studies have explored various aspects of this field, no systematic study has been performed on summarizing modeling methods and corresponding requirements activities. Objective: This study summarizes the state-of-the-art research trends, details the modeling methods and corresponding requirements activities, identifies relevant quality attributes and application domains and assesses the quality of each study. Method: We perform a systematic literature review underpinned by a rigorously established and reviewed protocol. To ensure the quality of the study, we choose 21 highly regarded publication venues and 8 popular digital libraries. In addition, we applytextminingto derive search strings and use Kappa coefficient to mitigate disagreements of researchers. Results: We selected 109 papers during the period of 2003-2013 and presented the research distributions over various kinds of factors. We extracted 29 modeling methods which are classified into 8 categories and identified 14 requirements activities which are classified into 4 requirements timelines. We captured 8 concerned software quality attributes based on the ISO 9126 standard and 12 application domains. Conclusion: The frequency of application of modeling methods varies greatly. Enterprise models were more widely used while behavior models were more rigorously evaluated. Requirements-driven runtime adaptation was the most frequently studied requirements activity. Activities at runtime were conveyed with more details. Finally, we draw other conclusions by discussing how well modeling dimensions were considered in these modeling methods and how well assurance dimensions were conveyed in requirements activities.","['Zhuoqun Yang', 'Zhi Li', 'Zhi Jin', 'He Zhang']",,arXiv,2017,https://doi.org/10.48550/arXiv.1704.00421,Anomali
Opinion Mining on Non-English Short Text,"As the type and the number of such venues increase, automated analysis of sentiment on textual resources has become an essential dataminingtask. In this paper, we investigate the problem ofminingopinions on the collection of informal shorttexts. Both positive and negative sentiment strength oftextsare detected. We focus on a non-English language that has few resources fortextmining. This approach would help enhance the sentiment analysis in languages where a list of opinionated words does not exist. We propose a new method projects thetextinto dense and low dimensional feature vectors according to the sentiment strength of the words. We detect the mixture of positive and negative sentiments on a multi-variant scale. Empirical evaluation of the proposed framework on Turkish tweets shows that our approach gets good results for opinionmining.",['Esra Akbas'],,arXiv,2017,https://doi.org/10.48550/arXiv.1704.00016,Anomali
Semi-supervised Embedding in Attributed Networks with Outliers,"In this paper, we propose a novel framework, called Semi-supervised Embedding in Attributed Networks with Outliers (SEANO), to learn a low-dimensional vector representation that systematically captures the topological proximity, attribute affinity and label similarity of vertices in a partially labeled attributed network (PLAN). Our method is designed to work in both transductive and inductive settings while explicitly alleviating noise effects from outliers. Experimental results on various datasets drawn from the web,textand image domains demonstrate the advantages of SEANO over state-of-the-art methods in semi-supervised classification under transductive as well as inductive settings. We also show that a subset of parameters in SEANO is interpretable as outlier score and can significantly outperform baseline methods when applied for detecting network outliers. Finally, we present the use of SEANO in a challenging real-world setting -- flood mapping of satellite images and show that it is able to outperform modern remote sensing algorithms for this task.","['Jiongqian Liang', 'Peter Jacobs', 'Jiankai Sun', 'Srinivasan Parthasarathy']",,arXiv,2018,https://doi.org/10.48550/arXiv.1703.08100,Anomali
"OncoScore: a novel, Internet-based tool to assess the oncogenic potential of genes","The complicated, evolving landscape of cancer mutations poses a formidable challenge to identify cancer genes among the large lists of mutations typically generated in NGS experiments. The ability to prioritize these variants is therefore of paramount importance. To address this issue we developed OncoScore, atext-miningtool that ranks genes according to their association with cancer, based on available biomedical literature. Receiver operating characteristic curve and the area under the curve (AUC) metrics on manually curated datasets confirmed the excellent discriminating capability of OncoScore (OncoScore cut-off threshold = 21.09; AUC = 90.3%, 95% CI: 88.1-92.5%), indicating that OncoScore provides useful results in cases where an efficient prioritization of cancer-associated genes is needed.","['Rocco Piazza', 'Daniele Ramazzotti', 'Roberta Spinelli', 'Alessandra Pirola', 'Luca De Sano', 'Pierangelo Ferrari', 'Vera Magistroni', 'Nicoletta Cordani', 'Nitesh Sharma', 'Carlo Gambacorti-Passerini']",,arXiv,2017,https://doi.org/10.48550/arXiv.1703.05692,Anomali
MetaPAD: Meta Pattern Discovery from Massive Text Corpora,"Miningtextual patterns in news, tweets, papers, and many other kinds oftextcorpora has been an active theme intextminingand NLP research. Previous studies adopt a dependency parsing-based pattern discovery approach. However, the parsing results lose rich context around entities in the patterns, and the process is costly for a corpus of large scale. In this study, we propose a novel typed textual pattern structure, called meta pattern, which is extended to a frequent, informative, and precise subsequence pattern in certain context. We propose an efficient framework, called MetaPAD, which discovers meta patterns from massive corpora with three techniques: (1) it develops a context-aware segmentation method to carefully determine the boundaries of patterns with a learnt pattern quality assessment function, which avoids costly dependency parsing and generates high-quality patterns; (2) it identifies and groups synonymous meta patterns from multiple facets---their types, contexts, and extractions; and (3) it examines type distributions of entities in the instances extracted by each group of patterns, and looks for appropriate type levels to make discovered patterns precise. Experiments demonstrate that our proposed framework discovers high-quality typed textual patterns efficiently from different genres of massive corpora and facilitates information extraction.","['Meng Jiang', 'Jingbo Shang', 'Taylor Cassidy', 'Xiang Ren', 'Lance M. Kaplan', 'Timothy P. Hanratty', 'Jiawei Han']",,arXiv,2017,https://doi.org/10.48550/arXiv.1703.04213,Anomali
Introduction to Formal Concept Analysis and Its Applications in Information Retrieval and Related Fields,"This paper is a tutorial on Formal Concept Analysis (FCA) and its applications. FCA is an applied branch of Lattice Theory, a mathematical discipline which enables formalisation of concepts as basic units of human thinking and analysing data in the object-attribute form. Originated in early 80s, during the last three decades, it became a popular human-centred tool for knowledge representation and data analysis with numerous applications. Since the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics include Information Retrieval with a focus on visualisation aspects, Machine Learning, DataMiningand Knowledge Discovery,TextMiningand several others.",['Dmitry I. Ignatov'],"RuSSIR 2014, Nizhniy Novgorod, Russia, CCIS vol. 505, Springer 42-141",arXiv,2017,https://doi.org/10.48550/arXiv.1703.02819,Anomali
Studying Positive Speech on Twitter,"We present results of empirical studies on positive speech on Twitter. By positive speech we understand speech that works for the betterment of a given situation, in this case relations between different communities in a conflict-prone country. We worked with four Twitter data sets. Through semi-manual opinionmining, we found that positive speech accounted for < 1% of the data . In fully automated studies, we tested two approaches: unsupervised statistical analysis, and supervisedtextclassification based on distributed word representation. We discuss benefits and challenges of those approaches and report empirical evidence obtained in the study.","['Marina Sokolova', 'Vera Sazonova', 'Kanyi Huang', 'Rudraneel Chakraboty', 'Stan Matwin']",,arXiv,2017,https://doi.org/10.48550/arXiv.1702.08866,Anomali
A Selfie is Worth a Thousand Words: Mining Personal Patterns behind User Selfie-posting Behaviours,"Selfies have become increasingly fashionable in the social media era. People are willing to share their selfies in various social media platforms such as Facebook, Instagram and Flicker. The popularity of selfie have caught researchers' attention, especially psychologists. In computer vision and machine learning areas, little attention has been paid to this phenomenon as a valuable data source. In this paper, we focus on exploring the deeper personal patterns behind people's different kinds of selfie-posting behaviours. We develop this work based on a dataset of WeChat, one of the most extensively used instant messaging platform in China. In particular, we first propose an unsupervised approach to classify the images posted by users. Based on the classification result, we construct three types of user-level features that reflect user preference, activity and posting habit. Based on these features, for a series of selfie related tasks, we build classifiers that can accurately predict two sets of users with opposite selfie-posting behaviours. We have found that people's interest, activity and posting habit have a great influence on their selfie-posting behaviours. For example, the classification accuracy between selfie-posting addict and nonaddict reaches 89.36%. We also prove that using user's image information to predict these behaviours achieve better performance than usingtextinformation. More importantly, for each set of users with a specific selfie-posting behaviour, we extract and visualize significant personal patterns about them. In addition, we cluster users and extract their high-level attributes, revealing the correlation between these attributes and users' selfie-posting behaviours. In the end, we demonstrate that users' selfie-posting behaviour, as a good predictor, could predict their different preferences toward these high-level attributes accurately.","['Tianlang Chen', 'Yuxiao Chen', 'Jiebo Luo']",,arXiv,2017,https://doi.org/10.48550/arXiv.1702.08097,Anomali
Simulation of Patient Flow in Multiple Healthcare Units using Process and Data Mining Techniques for Model Identification,"Introduction: An approach to building a hybrid simulation of patient flow is introduced with a combination of data-driven methods for automation of model identification. The approach is described with a conceptual framework and basic methods for combination of different techniques. The implementation of the proposed approach for simulation of acute coronary syndrome (ACS) was developed and used within an experimental study. Methods: Combination of data,text, and processminingtechniques and machine learning approaches for analysis of electronic health records (EHRs) with discrete-event simulation (DES) and queueing theory for simulation of patient flow was proposed. The performed analysis of EHRs for ACS patients enable identification of several classes of clinical pathways (CPs) which were used to implement a more realistic simulation of the patient flow. The developed solution was implemented using Python libraries (SimPy, SciPy, and others). Results: The proposed approach enables more realistic and detailed simulation of the patient flow within a group of related departments. Experimental study shows that the improved simulation of patient length of stay for ACS patient flow obtained from EHRs in Federal Almazov North-west Medical Research Centre in Saint Petersburg, Russia. Conclusion: The proposed approach, methods, and solutions provide a conceptual, methodological, and programming framework for implementation of simulation of complex and diverse scenarios within a flow of patients for different purposes: decision making, training, management optimization, and others.","['Sergey V. Kovalchuk', 'Anastasia A. Funkner', 'Oleg G. Metsker', 'Aleksey N. Yakovlev']",,arXiv,2018,https://doi.org/10.48550/arXiv.1702.07733,Anomali
LTSG: Latent Topical Skip-Gram for Mutually Learning Topic Model and Vector Representations,"Topic models have been widely used in discovering latent topics which are shared across documents intextmining. Vector representations, word embeddings and topic embeddings, map words and topics into a low-dimensional and dense real-value vector space, which have obtained high performance in NLP tasks. However, most of the existing models assume the result trained by one of them are perfect correct and used as prior knowledge for improving the other model. Some other models use the information trained from external large corpus to help improving smaller corpus. In this paper, we aim to build such an algorithm framework that makes topic models and vector representations mutually improve each other within the same corpus. An EM-style algorithm framework is employed to iteratively optimize both topic model and vector representations. Experimental results show that our model outperforms state-of-art methods on various NLP tasks.","['Jarvan Law', 'Hankz Hankui Zhuo', 'Junhua He', 'Erhu Rong']",,arXiv,2017,https://doi.org/10.48550/arXiv.1702.07117,Anomali
Reinforcement Learning Based Argument Component Detection,"Argument component detection (ACD) is an important sub-task in argumentationmining. ACD aims at detecting and classifying different argument components in natural languagetexts. Historical annotations (HAs) are important features the human annotators consider when they manually perform the ACD task. However, HAs are largely ignored by existing automatic ACD techniques. Reinforcement learning (RL) has proven to be an effective method for using HAs in some natural language processing tasks. In this work, we propose a RL-based ACD technique, and evaluate its performance on two well-annotated corpora. Results suggest that, in terms of classification accuracy, HAs-augmented RL outperforms plain RL by at most 17.85%, and outperforms the state-of-the-art supervised learning algorithm by at most 11.94%.","['Yang Gao', 'Hao Wang', 'Chen Zhang', 'Wei Wang']",,arXiv,2017,https://doi.org/10.48550/arXiv.1702.06239,Anomali
Automated Phrase Mining from Massive Text Corpora,"As one of the fundamental tasks intextanalysis, phraseminingaims at extracting quality phrases from atextcorpus. Phraseminingis important in various tasks such as information extraction/retrieval, taxonomy construction, and topic modeling. Most existing methods rely on complex, trained linguistic analyzers, and thus likely have unsatisfactory performance ontextcorpora of new domains and genres without extra but expensive adaption. Recently, a few data-driven methods have been developed successfully for extraction of phrases from massive domain-specifictext. However, none of the state-of-the-art models is fully automated because they require human experts for designing rules or labeling phrases.
  Since one can easily obtain many quality phrases from public knowledge bases to a scale that is much larger than that produced by human experts, in this paper, we propose a novel framework for automated phrasemining, AutoPhrase, which leverages this large amount of high-quality phrases in an effective way and achieves better performance compared to limited human labeled phrases. In addition, we develop a POS-guided phrasal segmentation model, which incorporates the shallow syntactic information in part-of-speech (POS) tags to further enhance the performance, when a POS tagger is available. Note that, AutoPhrase can support any language as long as a general knowledge base (e.g., Wikipedia) in that language is available, while benefiting from, but not requiring, a POS tagger. Compared to the state-of-the-art methods, the new method has shown significant improvements in effectiveness on five real-world datasets across different domains and languages.","['Jingbo Shang', 'Jialu Liu', 'Meng Jiang', 'Xiang Ren', 'Clare R Voss', 'Jiawei Han']",,arXiv,2017,https://doi.org/10.48550/arXiv.1702.04457,Anomali
A Technical Report: Entity Extraction using Both Character-based and Token-based Similarity,"Entity extraction is fundamental to manytextminingtasks such as organisation name recognition. A popular approach to entity extraction is based on matching sub-string candidates in a document against a dictionary of entities. To handle spelling errors and name variations of entities, usually the matching is approximate and edit or Jaccard distance is used to measure dissimilarity between sub-string candidates and the entities. For approximate entity extraction from freetext, existing work considers solely character-based or solely token-based similarity and hence cannot simultaneously deal with minor variations at token level and typos. In this paper, we address this problem by considering both character-based similarity and token-based similarity (i.e. two-level similarity). Measuring one-level (e.g. character-based) similarity is computationally expensive, and measuring two-level similarity is dramatically more expensive. By exploiting the properties of the two-level similarity and the weights of tokens, we develop novel techniques to significantly reduce the number of sub-string candidates that require computation of two-level similarity against the dictionary of entities. A comprehensive experimental study on real world datasets show that our algorithm can efficiently extract entities from documents and produce a high F1 score in the range of [0.91, 0.97].","['Zeyi Wen', 'Dong Deng', 'Rui Zhang', 'Kotagiri Ramamohanarao']",,arXiv,2017,https://doi.org/10.48550/arXiv.1702.03519,Anomali
Learning Concept Embeddings for Efficient Bag-of-Concepts Densification,"Explicit concept space models have proven efficacy fortextrepresentation in many natural language andtextminingapplications. The idea is to embed textual structures into a semantic space of concepts which captures the main ideas, objects, and the characteristics of these structures. The so called Bag of Concepts (BoC) representation suffers from data sparsity causing low similarity scores between similartextsdue to low concept overlap. To address this problem, we propose two neural embedding models to learn continuous concept vectors. Once they are learned, we propose an efficient vector aggregation method to generate fully continuous BoC representations. We evaluate our concept embedding models on three tasks: 1) measuring entity semantic relatedness and ranking where we achieve 1.6% improvement in correlation scores, 2) dataless concept categorization where we achieve state-of-the-art performance and reduce the categorization error rate by more than 5% compared to five prior word and entity embedding models, and 3) dataless document classification where our models outperform the sparse BoC representations. In addition, by exploiting our efficient linear time vector aggregation method, we achieve better accuracy scores with much less concept dimensions compared to previous BoC densification methods which operate in polynomial time and require hundreds of dimensions in the BoC representation.","['Walid Shalaby', 'Wlodek Zadrozny']",,arXiv,2018,https://doi.org/10.48550/arXiv.1702.03342,Anomali
Exact heat kernel on a hypersphere and its applications in kernel SVM,"Many contemporary statistical learning methods assume a Euclidean feature space. This paper presents a method for defining similarity based on hyperspherical geometry and shows that it often improves the performance of support vector machine compared to other competing similarity measures. Specifically, the idea of using heat diffusion on a hypersphere to measure similarity has been previously proposed, demonstrating promising results based on a heuristic heat kernel obtained from the zeroth order parametrix expansion; however, how well this heuristic kernel agrees with the exact hyperspherical heat kernel remains unknown. This paper presents a higher order parametrix expansion of the heat kernel on a unit hypersphere and discusses several problems associated with this expansion method. We then compare the heuristic kernel with an exact form of the heat kernel expressed in terms of a uniformly and absolutely convergent series in high-dimensional angular momentum eigenmodes. Being a natural measure of similarity between sample points dwelling on a hypersphere, the exact kernel often shows superior performance in kernel SVM classifications applied totextmining, tumor somatic mutation imputation, and stock market analysis.","['Chenchao Zhao', 'Jun S. Song']",,arXiv,2017,https://doi.org/10.48550/arXiv.1702.01373,Anomali
XML Warehousing and OLAP,"The aim of this article is to present an overview of the major XML warehousing approaches from the literature, as well as the existing approaches for performing OLAP analyses over XML data (which is termed XML-OLAP or XOLAP; Wang et al., 2005). We also discuss the issues and future trends in this area and illustrate this topic by presenting the design of a unified, XML data warehouse architecture and a set of XOLAP operators expressed in an XML algebra.","['Hadj Mahboubi', 'Marouane Hachicha', 'Jérôme Darmont']","Encyclopedia of Data Warehousing and Mining, Second Edition, IV, IGI Publishing, pp.2109-2116, 2009",arXiv,2017,https://doi.org/10.48550/arXiv.1701.08612,Anomali
Data Warehouse Benchmarking with DWEB,"Performance evaluation is a key issue for designers and users of Database Management Systems (DBMSs). Performance is generally assessed with software benchmarks that help, e.g., test architectural choices, compare different technologies or tune a system. In the particular context of data warehousing and On-Line Analytical Processing (OLAP), although the Transaction Processing Performance Council (TPC) aims at issuing standard decision-support benchmarks, few benchmarks do actually exist. We present in this chapter the Data Warehouse Engineering Benchmark (DWEB), which allows generating various ad-hoc synthetic data warehouses and workloads. DWEB is fully parameterized to fulfill various data warehouse design needs. However, two levels of parameterization keep it relatively easy to tune. We also expand on our previous work on DWEB by presenting its new Extract, Transform, and Load (ETL) feature as well as its new execution protocol. A Java implementation of DWEB is freely available on-line, which can be interfaced with most existing relational DMBSs. To the best of our knowledge, DWEB is the only easily available, up-to-date benchmark for data warehouses.",['Jérôme Darmont'],"Advances in Data Warehousing and Mining, 3, IGI Publishing, pp.302-323, 2009, Progressive Methods in Data Warehousing and Business Intelligence: Concepts and Competitive Analytics",arXiv,2017,https://doi.org/10.48550/arXiv.1701.08053,Anomali
Unsupervised Joint Mining of Deep Features and Image Labels for Large-scale Radiology Image Categorization and Scene Recognition,"The recent rapid and tremendous success of deep convolutional neural networks (CNN) on many challenging computer vision tasks largely derives from the accessibility of the well-annotated ImageNet and PASCAL VOC datasets. Nevertheless, unsupervised image categorization (i.e., without the ground-truth labeling) is much less investigated, yet critically important and difficult when annotations are extremely hard to obtain in the conventional way of ""Google Search"" and crowd sourcing. We address this problem by presenting a looped deep pseudo-task optimization (LDPO) framework for jointminingof deep CNN features and image labels. Our method is conceptually simple and rests upon the hypothesized ""convergence"" of better labels leading to better trained CNN models which in turn feed more discriminative image representations to facilitate more meaningful clusters/labels. Our proposed method is validated in tackling two important applications: 1) Large-scale medical image annotation has always been a prohibitively expensive and easily-biased task even for well-trained radiologists. Significantly better image categorization results are achieved via our proposed approach compared to the previous state-of-the-art method. 2) Unsupervised scene recognition on representative and publicly available datasets with our proposed technique is examined. The LDPO achieves excellent quantitative scene classification results. On the MIT indoor scene dataset, it attains a clustering accuracy of 75.3%, compared to the state-of-the-art supervised classification accuracy of 81.0% (when both are based on the VGG-VD model).","['Xiaosong Wang', 'Le Lu', 'Hoo-chang Shin', 'Lauren Kim', 'Mohammadhadi Bagheri', 'Isabella Nogues', 'Jianhua Yao', 'Ronald M. Summers']",,arXiv,2017,https://doi.org/10.48550/arXiv.1701.06599,Anomali
On Practical Accuracy of Edit Distance Approximation Algorithms,"The edit distance is a basic string similarity measure used in many applications such astextmining, signal processing, bioinformatics, and so on. However, the computational cost can be a problem when we repeat many distance calculations as seen in real-life searching situations. A promising solution to cope with the problem is to approximate the edit distance by another distance with a lower computational cost. There are, indeed, many distances have been proposed for approximating the edit distance. However, their approximation accuracies are evaluated only theoretically: many of them are evaluated only with big-oh (asymptotic) notations, and without experimental analysis. Therefore, it is beneficial to know their actual performance in real applications. In this study we compared existing six approximation distances in two approaches: (i) we refined their theoretical approximation accuracy by calculating up to the constant coefficients, and (ii) we conducted some experiments, in one artificial and two real-life data sets, to reveal under which situations they perform best. As a result we obtained the following results: [Batu 2006] is the best theoretically and [Andoni 2010] experimentally. Theoretical considerations show that [Batu 2006] is the best if the string length n is large enough (n >= 300). [Andoni 2010] is experimentally the best for most data sets and theoretically the second best. [Bar-Yossef 2004], [Charikar 2006] and [Sokolov 2007], despite their middle-level theoretical performance, are experimentally as good as [Andoni 2010] for pairs of strings with large alphabet size.","['Hiroyuki Hanada', 'Mineichi Kudo', 'Atsuyoshi Nakamura']",,arXiv,2017,https://doi.org/10.48550/arXiv.1701.06134,Anomali
A Joint Framework for Argumentative Text Analysis Incorporating Domain Knowledge,"For argumentationmining, there are several sub-tasks such as argumentation component type classification, relation classification. Existing research tends to solve such sub-tasks separately, but ignore the close relation between them. In this paper, we present a joint framework incorporating logical relation between sub-tasks to improve the performance of argumentation structure generation. We design an objective function to combine the predictions from individual models for each sub-task and solve the problem with some constraints constructed from background knowledge. We evaluate our proposed model on two public corpora and the experiment results show that our model can outperform the baseline that uses a separate model significantly for each sub-task. Our model also shows advantages on component-related sub-tasks compared to a state-of-the-art joint model based on the evidence graph.","['Zhongyu Wei', 'Chen Li', 'Yang Liu']",,arXiv,2017,https://doi.org/10.48550/arXiv.1701.05343,Anomali
A Data-Oriented Model of Literary Language,"We consider the task of predicting how literary atextis, with a gold standard from human ratings. Aside from a standard bigram baseline, we apply rich syntactic tree fragments,minedfrom the training set, and a series of hand-picked features. Our model is the first to distinguish degrees of highly and less literary novels using a variety of lexical and syntactic features, and explains 76.0 % of the variation in literary ratings.","['Andreas van Cranenburgh', 'Rens Bod']","Proceedings of EACL 2017, pp. 1228-1238",arXiv,2017,https://doi.org/10.48550/arXiv.1701.03329,Anomali
Parallel mining of time-faded heavy hitters,"We present PFDCMSS, a novel message-passing based parallel algorithm forminingtime-faded heavy hitters. The algorithm is a parallel version of the recently published FDCMSS sequential algorithm. We formally prove its correctness by showing that the underlying data structure, a sketch augmented with a Space Saving stream summary holding exactly two counters, is mergeable. Whilst mergeability of traditional sketches derives immediately from theory, we show that merging our augmented sketch is non trivial. Nonetheless, the resulting parallel algorithm is fast and simple to implement. To the best of our knowledge, PFDCMSS is the first parallel algorithm solving the problem ofminingtime-faded heavy hitters on message-passing parallel architectures. Extensive experimental results confirm that PFDCMSS retains the extreme accuracy and error bound provided by FDCMSS whilst providing excellent parallel scalability.","['Massimo Cafaro', 'Marco Pulimeno', 'Italo Epicoco']",,arXiv,2017,https://doi.org/10.48550/arXiv.1701.03004,Anomali
Outlier Detection for Text Data : An Extended Version,"The problem of outlier detection is extremely challenging in many domains such astext, in which the attribute values are typically non-negative, and most values are zero. In such cases, it often becomes difficult to separate the outliers from the natural variations in the patterns in the underlying data. In this paper, we present a matrix factorization method, which is naturally able to distinguish the anomalies with the use of low rank approximations of the underlying data. Our iterative algorithm TONMF is based on block coordinate descent (BCD) framework. We define blocks over the term-document matrix such that the function becomes solvable. Given most recently updated values of other matrix blocks, we always update one block at a time to its optimal. Our approach has significant advantages over traditional methods fortextoutlier detection. Finally, we present experimental results illustrating the effectiveness of our method over competing methods.","['Ramakrishnan Kannan', 'Hyenkyun Woo', 'Charu C. Aggarwal', 'Haesun Park']",,arXiv,2017,https://doi.org/10.48550/arXiv.1701.01325,Anomali
Fuzzy Based Implicit Sentiment Analysis on Quantitative Sentences,"With the rapid growth of social media on the web, emotional polarity computation has become a flourishing frontier in thetextminingcommunity. However, it is challenging to understand the latest trends and summarize the state or general opinions about products due to the big diversity and size of social media data and this creates the need of automated and real time opinion extraction andmining. On the other hand, the bulk of current research has been devoted to study the subjective sentences which contain opinion keywords and limited work has been reported for objective statements that imply sentiment. In this paper, fuzzy based knowledge engineering model has been developed for sentiment classification of special group of such sentences including the change or deviation from desired range or value. Drug reviews are the rich source of such statements. Therefore, in this research, some experiments were carried out on patient's reviews on several different cholesterol lowering drugs to determine their sentiment polarity. The main conclusion through this study is, in order to increase the accuracy level of existing drug opinionminingsystems, objective sentences which imply opinion should be taken into account. Our experimental results demonstrate that our proposed model obtains over 72 percent F1 value.","['Amir Hossein Yazdavar', 'Monireh Ebrahimi', 'Naomie Salim']",,arXiv,2017,https://doi.org/10.48550/arXiv.1701.00798,Anomali
The leveled approach. Using and evaluating text mining tools AVResearcherXL and Texcavator for historical research on public perceptions of drugs,"We introduce our explorative historical leveled approach that we use to understand drug debates in the Royal Dutch Library's digital newspaper archive. In this approach we alternate between distant reading and close reading. Furthermore, we use this approach to evaluate twotextminingtools: AVResearcherXL and Texcavator.","['Berrie van der Molen', 'Lars Buitinck', 'Toine Pieters']",,arXiv,2017,https://doi.org/10.48550/arXiv.1701.00487,Anomali
Benchmarking data warehouses,"Data warehouse architectural choices and optimization techniques are critical to decision support query performance. To facilitate these choices, the performance of the designed data warehouse must be assessed, usually with benchmarks. These tools can either help system users comparing the performances of different systems, or help system engineers testing the effect of various design choices. While the Transaction Processing Performance Council's standard benchmarks address the first point, they are not tunable enough to address the second one and fail to model different data warehouse schemas. By contrast, our Data Warehouse Engineering Benchmark (DWEB) allows generating various ad-hoc synthetic data warehouses and workloads. DWEB is implemented as a Java free software that can be interfaced with most existing relational database management systems. The full specifications of DWEB, as well as experiments we performed to illustrate how our benchmark may be used, are provided in this paper.","['Jérôme Darmont', 'Fadila Bentayeb', 'Omar Boussaïd']","International Journal of Business Intelligence and Data Mining, Inderscience, 2007, 2 (1), pp.79-104",arXiv,2017,https://doi.org/10.48550/arXiv.1701.00399,Anomali
PAMPO: using pattern matching and pos-tagging for effective Named Entities recognition in Portuguese,"This paper deals with the entity extraction task (named entity recognition) of atextminingprocess that aims at unveiling non-trivial semantic structures, such as relationships and interaction between entities or communities. In this paper we present a simple and efficient named entity extraction algorithm. The method, named PAMPO (PAttern Matching and POs tagging based algorithm for NER), relies on flexible pattern matching, part-of-speech tagging and lexical-based rules. It was developed to processtextswritten in Portuguese, however it is potentially applicable to other languages as well.
  We compare our approach with current alternatives that support Named Entity Recognition (NER) for content written in Portuguese. These are Alchemy, Zemanta and Rembrandt. Evaluation of the efficacy of the entity extraction method on severaltextswritten in Portuguese indicates a considerable improvement on $recall$ and $F_1$ measures.","['Conceição Rocha', 'Alípio Jorge', 'Roberta Sionara', 'Paula Brito', 'Carlos Pimenta', 'Solange Rezende']",,arXiv,2016,https://doi.org/10.48550/arXiv.1612.09535,Anomali
Here's My Point: Joint Pointer Architecture for Argument Mining,"One of the major goals in automated argumentationminingis to uncover the argument structure present in argumentativetext. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentationmining, focusing on the two tasks of extracting links between argument components, and classifying types of argument components. In order to solve this problem, we propose to use a joint model that is based on a Pointer Network architecture. A Pointer Network is appealing for this task for the following reasons: 1) It takes into account the sequential nature of argument components; 2) By construction, it enforces certain properties of the tree structure present in argument relations; 3) The hidden representations can be applied to auxiliary tasks. In order to extend the contribution of the original Pointer Network model, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks, and adding a fully-connected layer prior to recurrent neural network input, is crucial for high performance.","['Peter Potash', 'Alexey Romanov', 'Anna Rumshisky']",,arXiv,2017,https://doi.org/10.48550/arXiv.1612.08994,Anomali
Evaluating Marijuana-Related Tweets On Twitter,"This paper studies marijuana-related tweets in social network Twitter. We collected more than 300,000 marijuana related tweets during November 2016 in our study. Ourtext-miningbased algorithms and data analysis unveil some interesting patterns including: (i) users' attitudes (e.g., positive or negative) can be characterized by the existence of outer links in a tweet; (ii) 67% users use their mobile phones to post their messages while many users publish their messages using third-party automatic posting services; and (3) the number of tweets during weekends is much higher than during weekdays. Our data also showed the impact of the political events such as the U.S. presidential election or state marijuana legalization votes on the marijuana-related tweeting frequencies.","['Anh Nguyen', 'Quang Hoang', 'Hung Nguyen', 'Dong Nguyen', 'Tuan Tran']",,arXiv,2016,https://doi.org/10.48550/arXiv.1612.08913,Anomali
Multi-cultural Wikipedia mining of geopolitics interactions leveraging reduced Google matrix analysis,"Geopolitics focuses on political power in relation to geographic space. Interactions among world countries have been widely studied at various scales, observing economic exchanges, world history or international politics among others. This work exhibits the potential of Wikipediaminingfor such studies. Indeed, Wikipedia stores valuable fine-grained dependencies among countries by linking webpages together for diverse types of interactions (not only related to economical, political or historical facts). Wemineherein the Wikipedia networks of several language editions using the recently proposed method of reduced Google matrix analysis. This approach allows to establish direct and hidden links between a subset of nodes that belong to a much larger directed network. Our study concentrates on 40 major countries chosen worldwide. Our aim is to offer a multicultural perspective on their interactions by comparing networks extracted from five different Wikipedia language editions, emphasizing English, Russian and Arabic ones. We demonstrate that this approach allows to recover meaningful direct and hidden links among the 40 countries of interest.","['Klaus M. Frahm', 'Samer El Zant', 'Katia Jaffrès-Runser', 'Dima L. Shepelyansky']",Physics Letters A 381 (2017) 2677-2685,arXiv,2017,https://doi.org/10.48550/arXiv.1612.07920,Anomali
"""420 Friendly"": Revealing Marijuana Use via Craigslist Rental Ads","Recent studies have shown that informationminedfrom Craigslist can be used for informing public health policy or monitoring risk behavior. This paper presents atext-miningmethod for conducting public health surveillance of marijuana use concerns in the U.S. using online classified ads in Craigslist. We collected more than 200 thousands of rental ads in the housing categories in Craigslist and devisedtext-miningmethods for efficiently and accurately extract rental ads associated with concerns about the uses of marijuana in different states across the U.S. We linked the extracted ads to their geographic locations and computed summary statistics of the ads having marijuana use concerns. Our data is then compared with the State Marijuana Laws Map published by the U.S. government and marijuana related keywords search in Google to verify our collected data with respect to the demographics of marijuana use concerns. Our data not only indicates strong correlations between Craigslist ads, Google search and the State Marijuana Laws Map in states where marijuana uses are legal, but also reveals some hidden world of marijuana use concerns in other states where marijuana use is illegal. Our approach can be utilized as a marijuana surveillance tool for policy makers to develop public health policy and regulations.","['Anh Nguyen', 'Long Nguyen', 'Dong Nguyen', 'Uyen Le', 'Tuan Tran']",,arXiv,2016,https://doi.org/10.48550/arXiv.1612.07630,Anomali
A Scalable Document-based Architecture for Text Analysis,"Analyzing textual data is a very challenging task because of the huge volume of data generated daily. Fundamental issues intextanalysis include the lack of structure in document datasets, the need for various preprocessing steps %(e.g., stem or lemma extraction, part-of-speech tagging, named entities recognition...), and performance and scaling issues. Existingtextanalysis architectures partly solve these issues, providing restrictive data schemas, addressing only one aspect oftextpreprocessing and focusing on one single task when dealing with performance optimization. %As a result, no definite solution is currently available. Thus, we propose in this paper a new generictextanalysis architecture, where document structure is flexible, many preprocessing techniques are integrated and textual datasets are indexed for efficient access. We implement our conceptual architecture using both a relational and a document-oriented database. Our experiments demonstrate the feasibility of our approach and the superiority of the document-oriented logical and physical implementation.","['Ciprian-Octavian Truică', 'Jérôme Darmont', 'Julien Velcin']","12th International Conference on Advanced Data Mining and Applications (ADMA 2016), Dec 2016, Gold Coast, Australia. Springer, 10086, pp.481-494, 2016, Lecture Notes in Artificial Intelligence",arXiv,2016,https://doi.org/10.48550/arXiv.1612.06195,Anomali
A Simple Approach to Multilingual Polarity Classification in Twitter,"Recently, sentiment analysis has received a lot of attention due to the interest inminingopinions of social media users. Sentiment analysis consists in determining the polarity of a giventext, i.e., its degree of positiveness or negativeness. Traditionally, Sentiment Analysis algorithms have been tailored to a specific language given the complexity of having a number of lexical variations and errors introduced by the people generating content. In this contribution, our aim is to provide a simple to implement and easy to use multilingual framework, that can serve as a baseline for sentiment analysis contests, and as starting point to build new sentiment analysis systems. We compare our approach in eight different languages, three of them have important international contests, namely, SemEval (English), TASS (Spanish), and SENTIPOLC (Italian). Within the competitions our approach reaches from medium to high positions in the rankings; whereas in the remaining languages our approach outperforms the reported results.","['Eric S. Tellez', 'Sabino Miranda Jiménez', 'Mario Graff', 'Daniela Moctezuma', 'Ranyart R. Suárez', 'Oscar S. Siordia']",,arXiv,2016,https://doi.org/10.48550/arXiv.1612.05270,Anomali
Upper Bound of Bayesian Generalization Error in Non-negative Matrix Factorization,"Non-negative matrix factorization (NMF) is a new knowledge discovery method that is used fortextmining, signal processing, bioinformatics, and consumer analysis. However, its basic property as a learning machine is not yet clarified, as it is not a regular statistical model, resulting that theoretical optimization method of NMF has not yet established. In this paper, we study the real log canonical threshold of NMF and give an upper bound of the generalization error in Bayesian learning. The results show that the generalization error of the matrix factorization can be made smaller than regular statistical models if Bayesian learning is applied.","['Naoki Hayashi', 'Sumio Watanabe']","Neurocomputing, Volume 266C, 29 November 2017, pp.21-28",arXiv,2017,https://doi.org/10.48550/arXiv.1612.04112,Anomali
A New Spectral Method for Latent Variable Models,"This paper presents an algorithm for the unsupervised learning of latent variable models from unlabeled sets of data. We base our technique on spectral decomposition, providing a technique that proves to be robust both in theory and in practice. We also describe how to use this algorithm to learn the parameters of two well knowntextminingmodels: single topic model and Latent Dirichlet Allocation, providing in both cases an efficient technique to retrieve the parameters to feed the algorithm. We compare the results of our algorithm with those of existing algorithms on synthetic data, and we provide examples of applications to real worldtextcorpora for both single topic model and LDA, obtaining meaningful results.","['Matteo Ruffini', 'Marta Casanellas', 'Ricard Gavaldà']",,arXiv,2017,https://doi.org/10.48550/arXiv.1612.03409,Anomali
Data mining when each data point is a network,"We discuss the problem of extending dataminingapproaches to cases in which data points arise in the form of individual graphs. Being able to find the intrinsic low-dimensionality in ensembles of graphs can be useful in a variety of modeling contexts, especially when coarse-graining the detailed graph information is of interest. One of the main challenges inmininggraph data is the definition of a suitable pairwise similarity metric in the space of graphs. We explore two practical solutions to solving this problem: one based on finding subgraph densities, and one using spectral information. The approach is illustrated on three test data sets (ensembles of graphs); two of these are obtained from standard graph generating algorithms, while the graphs in the third example are sampled as dynamic snapshots from an evolving network simulation. We further incorporate these approaches with equation free techniques, demonstrating how such dataminingapproaches can enhance scientific computation of network evolution dynamics.","['Karthikeyan Rajendran', 'Assimakis A. Kattis', 'Alexander Holiday', 'Risi Kondor', 'Ioannis G. Kevrekidis']",,arXiv,2016,https://doi.org/10.48550/arXiv.1612.02908,Anomali
"The Evolution of Sentiment Analysis - A Review of Research Topics, Venues, and Top Cited Papers","Sentiment analysis is one of the fastest growing research areas in computer science, making it challenging to keep track of all the activities in the area. We present a computer-assisted literature review, where we utilize bothtextminingand qualitative coding, and analyze 6,996 papers from Scopus. We find that the roots of sentiment analysis are in the studies on public opinion analysis at the beginning of 20th century and in thetextsubjectivity analysis performed by the computational linguistics community in 1990's. However, the outbreak of computer-based sentiment analysis only occurred with the availability of subjectivetextson the Web. Consequently, 99% of the papers have been published after 2004. Sentiment analysis papers are scattered to multiple publication venues, and the combined number of papers in the top-15 venues only represent ca. 30% of the papers in total. We present the top-20 cited papers from Google Scholar and Scopus and a taxonomy of research topics. In recent years, sentiment analysis has shifted from analyzing online product reviews to social mediatextsfrom Twitter and Facebook. Many topics beyond product reviews like stock markets, elections, disasters, medicine, software engineering and cyberbullying extend the utilization of sentiment analysis","['Mika Viking Mäntylä', 'Daniel Graziotin', 'Miikka Kuutila']","Computer Science Review, Volume 27, February 2018, Pages 16-32",arXiv,2017,https://doi.org/10.48550/arXiv.1612.01556,Anomali
Mining Spatio-temporal Data on Industrialization from Historical Registries,"Despite the growing availability of big data in many fields, historical data on socioevironmental phenomena are often not available due to a lack of automated and scalable approaches for collecting, digitizing, and assembling them. We have developed a data-miningmethod for extracting tabulated, geocoded data from printed directories. While scanning and optical character recognition (OCR) can digitize printedtext, these methods alone do not capture the structure of the underlying data. Our pipeline integrates both page layout analysis and OCR to extract tabular, geocoded data from structuredtext. We demonstrate the utility of this method by applying it to scanned manufacturing registries from Rhode Island that record 41 years of industrial land use. The resulting spatio-temporal data can be used for socioenvironmental analyses of industrialization at a resolution that was not previously possible. In particular, we find strong evidence for the dispersion of manufacturing from the urban core of Providence, the state's capital, along the Interstate 95 corridor to the north and south.","['David Berenbaum', 'Dwyer Deighan', 'Thomas Marlow', 'Ashley Lee', 'Scott Frickel', 'Mark Howison']",Journal of Environmental Informatics 34(1): 28-34 (2019),arXiv,2016,https://doi.org/10.48550/arXiv.1612.00992,Anomali
Fine-grained Mining of Illicit Drug Use Patterns Using Social Multimedia Data from Instagram,"According to NSDUH (National Survey on Drug Use and Health), 20 million Americans consumed drugs in the past few 30 days. Combating illicit drug use is of great interest to public health and law enforcement agencies. Despite of the importance, most of the existing studies on drug uses rely on surveys. Surveys on sensitive topics such as drug use may not be answered truthfully by the people taking them. Selecting a representative sample to survey is another major challenge. In this paper, we explore the possibility of using big multimedia data, including both images andtext, from social media in order to discover drug use patterns at fine granularity with respect to demographics. Instagram posts are searched and collected by drug related terms by analyzing the hashtags supplied with each post. A large and dynamic dictionary of frequent drug related slangs is used to find these posts. User demographics are extracted using robust face image analysis algorithms. These posts are thenminedto find common trends with regard to the time and location they are posted, and further in terms of age and gender of the drug users. Furthermore, by studying the accounts followed by the users of drug related posts, we extract common interests shared by drug users.","['Yiheng Zhou', 'Numair Sani', 'Jiebo Luo']","Special Session on Intelligent Data Mining, IEEE Big Data Conference, Washington, DC, December 2016",arXiv,2016,https://doi.org/10.48550/arXiv.1611.08351,Anomali
The emergence and evolution of the research fronts in HIV/AIDS research,"In this paper, we have identified and analyzed the emergence, structure and dynamics of the paradigmatic research fronts that established the fundamentals of the biomedical knowledge on HIV/AIDS. A search of papers with the identifiers ""HIV/AIDS"", ""Human Immunodeficiency Virus"", ""HIV-1"" and ""Acquired Immunodeficiency Syndrome"" in the Web of Science (Thomson Reuters), was carried out. A citation network of those papers was constructed. Then, a sub-network of the papers with the highest number of inter-citations (with a minimal in-degree of 28) was selected to perform a combination of network clustering andtextminingto identify the paradigmatic research fronts and analyze their dynamics. Thirteen research fronts were identified in this sub-network. The biggest and oldest front is related to the clinical knowledge on the disease in the patient. Nine of the fronts are related to the study of specific molecular structures and mechanisms and two of these fronts are related to the development of drugs. The rest of the fronts are related to the study of the disease at the cellular level. Interestingly, the emergence of these fronts occurred in successive ""waves"" over the time which suggest a transition in the paradigmatic focus. The emergence and evolution of the biomedical fronts in HIV/AIDS research is explained not just by the partition of the problem in elements and interactions leading to increasingly specialized communities, but also by changes in the technological context of this health problem and the dramatic changes in the epidemiological reality of HIV/AIDS that occurred between 1993 and 1995.","['David Fajardo-Ortiz', 'Malaquias Lopez-Cervantes', 'Luis Duran', 'Michel Dumontier', 'Miguel Lara', 'Hector Ochoa', 'Victor M Castano']","PLoS ONE, 2017 12(5): e0178293",arXiv,2017,https://doi.org/10.48550/arXiv.1611.05204,Anomali
SimDoc: Topic Sequence Alignment based Document Similarity Framework,"Document similarity is the problem of estimating the degree to which a given pair of documents has similar semantic content. An accurate document similarity measure can improve several enterprise relevant tasks such as document clustering,textmining, and question-answering. In this paper, we show that a document's thematic flow, which is often disregarded by bag-of-word techniques, is pivotal in estimating their similarity. To this end, we propose a novel semantic document similarity framework, called SimDoc. We model documents as topic-sequences, where topics represent latent generative clusters of related words. Then, we use a sequence alignment algorithm to estimate their semantic similarity. We further conceptualize a novel mechanism to compute topic-topic similarity to fine tune our system. In our experiments, we show that SimDoc outperforms many contemporary bag-of-words techniques in accurately computing document similarity, and on practical applications such as document clustering.","['Gaurav Maheshwari', 'Priyansh Trivedi', 'Harshita Sahijwani', 'Kunal Jha', 'Sourish Dasgupta', 'Jens Lehmann']",,arXiv,2017,https://doi.org/10.48550/arXiv.1611.04822,Anomali
Using text mining and machine learning for detection of child abuse,"Abuse in any form is a grave threat to a child's health. Public health institutions in the Netherlands try to identify and prevent different kinds of abuse, and building a decision support system can help such institutions achieve this goal. Such decision support relies on the analysis of relevant child health data. A significant part of the medical data that the institutions have on children is unstructured, and in the form of freetextnotes. In this research, we employ machine learning andtextminingtechniques to detect patterns of possible child abuse in the data. The resulting model achieves a high score in classifying cases of possible abuse. We then describe our implementation of the decision support API at a municipality in the Netherlands.","['Chintan Amrit', 'Tim Paauw', 'Robin Aly', 'Miha Lavric']",,arXiv,2016,https://doi.org/10.48550/arXiv.1611.03660,Anomali
Optimal Extraction and Taxation of Strategic Natural Resources: A Differential Game Approach,"This paper studies the optimal extraction and taxation of nonrenewable natural resources. It is well known that the market values of the main strategic resources such as oil, natural gas, uranium, copper,..., etc, fluctuate randomly following global and seasonal macroeconomic parameters, these values are modeled using Markov switching Lévy processes. We formulate this problem as a differential game. The two players of this differential game are theminingcompany whose aim is to maximize the revenues generated from its extracting activities and the government agency in charge of regulating and taxing natural resources. We prove the existence of a Nash equilibrium. The corresponding Hamilton Jacobi Isaacs equations are completely solved and the value functions as well as the optimal extraction and taxation rates are derived in closed-form. A Numerical example is presented to illustrate our findings.",['Moustapha Pemy'],,arXiv,2018,https://doi.org/10.48550/arXiv.1611.02547,Anomali
Distributed Coordinate Descent for Generalized Linear Models with Regularization,"Generalized linear model with $L_1$ and $L_2$ regularization is a widely used technique for solving classification, class probability estimation and regression problems. With the numbers of both features and examples growing rapidly in the fields liketextminingand clickstream data analysis parallelization and the use of cluster architectures becomes important. We present a novel algorithm for fitting regularized generalized linear models in the distributed environment. The algorithm splits data between nodes by features, uses coordinate descent on each node and line search to merge results globally. Convergence proof is provided. A modifications of the algorithm addresses slow node problem. For an important particular case of logistic regression we empirically compare our program with several state-of-the art approaches that rely on different algorithmic and data spitting methods. Experiments demonstrate that our approach is scalable and superior when training on large and sparse datasets.","['Ilya Trofimov', 'Alexander Genkin']",,arXiv,2017,https://doi.org/10.48550/arXiv.1611.02101,Anomali
"Analysis of Link Formation, Persistence and Dissolution in NetSense Data","We study a unique behavioral network data set (based on periodic surveys and on electronic logs of dyadic contact via smartphones) collected at the University of Notre Dame.The participants are a sample of members of the entering class of freshmen in the fall of 2011 whose opinions on a wide variety of political and social issues and activities on campus were regularly recorded - at the beginning and end of each semester - for the first three years of their residence on campus. We create a communication activity network implied by call andtextdata, and a friendship network based on surveys. Both networks are limited to students participating in the NetSense surveys. We aim at finding student traits and activities on which agreements correlate well with formation and persistence of links while disagreements are highly correlated with non-existence or dissolution of links in the two social networks that we created. Using statistical analysis and machine learning, we observe several traits and activities displaying such correlations, thus being of potential use to predict social network evolution.","['Ashwin Bahulkar', 'Boleslaw K. Szymanski', 'Omar Lizardo', 'Yuxiao Dong', 'Yang Yang', 'Nitesh V. Chawla']","Proc. 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), San Francisco, CA, August 18-21, 2016, pp. 1197-1204",arXiv,2016,https://doi.org/10.48550/arXiv.1611.00568,Anomali
Towards Sub-Word Level Compositions for Sentiment Analysis of Hindi-English Code Mixed Text,"Sentiment analysis (SA) using code-mixed data from social media has several applications in opinionminingranging from customer satisfaction to social campaign analysis in multilingual societies. Advances in this area are impeded by the lack of a suitable annotated dataset. We introduce a Hindi-English (Hi-En) code-mixed dataset for sentiment analysis and perform empirical analysis comparing the suitability and performance of various state-of-the-art SA methods in social media.
  In this paper, we introduce learning sub-word level representations in LSTM (Subword-LSTM) architecture instead of character-level or word-level representations. This linguistic prior in our architecture enables us to learn the information about sentiment value of important morphemes. This also seems to work well in highly noisytextcontaining misspellings as shown in our experiments which is demonstrated in morpheme-level feature maps learned by our model. Also, we hypothesize that encoding this linguistic prior in the Subword-LSTM architecture leads to the superior performance. Our system attains accuracy 4-5% greater than traditional approaches on our dataset, and also outperforms the available system for sentiment analysis in Hi-En code-mixedtextby 18%.","['Ameya Prabhu', 'Aditya Joshi', 'Manish Shrivastava', 'Vasudeva Varma']",,arXiv,2016,https://doi.org/10.48550/arXiv.1611.00472,Anomali
Rapid Prototyping of a Text Mining Application for Cryptocurrency Market Intelligence,"Blockchain represents a technology for establishing a shared, immutable version of the truth between a network of participants that do not trust one another, and therefore has the potential to disrupt any financial or other industries that rely on third-parties to establish trust. Recent trends in computing including: prevalence of Free and Open Source Software (FOSS); easy access to High Performance Computing (HPC i.e. 'The Cloud'); and increasingly advanced analytics capabilities such as Natural Language Processing (NLP) and Machine Learning (ML) allow for rapidly prototyping applications for analysis of trends in the emergence of Blockchain technology. A scaleable proof-of-concept pipeline that lays the groundwork for analysis of multiple streams of semi-structured data posted on social media is demonstrated. Preliminary analysis and performance metrics are presented and discussed. Future work is described that will scale the system to cloud-based, real-time, analysis of multiple data streams, with Information Extraction (IE) (ex. sentiment analysis) and Machine Learning capability.","['Marek Laskowski', 'Henry M. Kim']",,arXiv,2016,https://doi.org/10.48550/arXiv.1611.00315,Anomali
Embedding Deep Metric for Person Re-identication A Study Against Large Variations,"Person re-identification is challenging due to the large variations of pose, illumination, occlusion and camera view. Owing to these variations, the pedestrian data is distributed as highly-curved manifolds in the feature space, despite the current convolutional neural networks (CNN)'s capability of feature extraction. However, the distribution is unknown, so it is difficult to use the geodesic distance when comparing two samples. In practice, the current deep embedding methods use the Euclidean distance for the training and test. On the other hand, the manifold learning methods suggest to use the Euclidean distance in the local range, combining with the graphical relationship between samples, for approximating the geodesic distance. From this point of view, selecting suitable positive i.e. intra-class) training samples within a local range is critical for training the CNN embedding, especially when the data has large intra-class variations. In this paper, we propose a novel moderate positive sampleminingmethod to train robust CNN for person re-identification, dealing with the problem of large variation. In addition, we improve the learning by a metric weight constraint, so that the learned metric has a better generalization ability. Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification, and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification. Therefore, the study presented in this paper may be useful in inspiring new designs of deep models for person re-identification.","['Hailin Shi', 'Yang Yang', 'Xiangyu Zhu', 'Shengcai Liao', 'Zhen Lei', 'Weishi Zheng', 'Stan Z. Li']",,arXiv,2016,https://doi.org/10.48550/arXiv.1611.00137,Anomali
Scientific Literature Text Mining and the Case for Open Access,"""Open access"" has become a central theme of journal reform in academic publishing. In this article, I examine the relationship between open access publishing and an important infrastructural element of a modern research enterprise, scientific literaturetextmining, or the use of data analytic techniques to conduct meta-analyses and investigations into the scientific corpus. I give a brief history of the open access movement, discuss novel journalistic practices, and an overview of data-driven investigation of the scientific corpus. I argue that particularly in an era where the veracity of many research studies has been called into question, scientific literaturetextminingshould be one of the key motivations for open access publishing, not only in the basic sciences, but in the engineering and applied sciences as well. The enormous benefits of unrestricted access to the research literature should prompt scholars from all disciplines to lend their vocal support to enabling legal, wholesale access to the scientific literature as part of a data science pipeline.",['Gopal P. Sarma'],Sarma G. Scientific Literature Text Mining and the Case for Open Access. The Journal of Open Engineering [Internet]. 2017 Dec 8; Available from: https://www.tjoe.org/pub/scientific-literature-text-mining-and-the-case-for-open-access,arXiv,2018,https://doi.org/10.48550/arXiv.1611.00097,Anomali
Mining Social Media for Open Innovation in Transportation Systems,"This work proposes a novel framework for the development of new products and services in transportation through an open innovation approach based on automatic content analysis of social media data. The framework is able to extract users comments from Online Social Networks (OSN), to process and analyzetextthrough information extraction and sentiment analysis techniques to obtain relevant information about product reception on the market. A use case was developed using the mobile application Uber, which is today one of the fastest growing technology companies in the world. We measured how a controversial, highly diffused event influences the volume of tweets about Uber and the perception of its users. While there is no change in the image of Uber, a large increase in the number of tweets mentioning the company is observed, which meant a free and important diffusion of its product.","['Daniela Ulloa', 'Pedro Saleiro', 'Rosaldo J. F. Rossetti', 'Elis Regina Silva']",,arXiv,2016,https://doi.org/10.48550/arXiv.1610.09894,Anomali
Three-dimensional organic Dirac-line materials due to nonsymmorphic symmetry: a data mining approach,"A dataminingstudy of electronic Kohn-Sham band structures was performed to identify Dirac materials within the Organic Materials Database (OMDB). Out of that, the 3-dimensional organic crystal 5,6-bis(trifluoromethyl)-2-methoxy-1$H$-1,3-diazepine was found to host different Dirac line-nodes within the band structure. From a group theoretical analysis, it is possible to distinguish between Dirac line-nodes occurring due to 2-fold degenerate energy levels protected by the monoclinic crystalline symmetry and 2-fold degenerate accidental crossings protected by the topology of the electronic band structure. The obtained results can be generalized to all materials having the space group $P2_1/c$ (No. 14, $C^5_{2h}$) by introducing three distinct topological classes.","['R. Matthias Geilhufe', 'Adrien Bouhon', 'Stanislav S. Borysov', 'Alexander V. Balatsky']","Phys. Rev. B 95, 041103 (2017)",arXiv,2017,https://doi.org/10.48550/arXiv.1610.07815,Anomali
KGEval: Estimating Accuracy of Automatically Constructed Knowledge Graphs,"Automatic construction of large knowledge graphs (KG) byminingweb-scaletextdatasets has received considerable attention recently. Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity. This important problem has largely been ignored in prior research we fill this gap and propose KGEval. KGEval binds facts of a KG using coupling constraints and crowdsources the facts that infer correctness of large parts of the KG. We demonstrate that the objective optimized by KGEval is submodular and NP-hard, allowing guarantees for our approximation algorithm. Through extensive experiments on real-world datasets, we demonstrate that KGEval is able to estimate KG accuracy more accurately compared to other competitive baselines, while requiring significantly lesser number of human evaluations.","['Prakhar Ojha', 'Partha Talukdar']",,arXiv,2016,https://doi.org/10.48550/arXiv.1610.06912,Anomali
Clinical Text Prediction with Numerically Grounded Conditional Language Models,"Assistedtextinput techniques can save time and effort and improvetextquality. In this paper, we investigate how grounded and conditional extensions to standard neural language models can bring improvements in the tasks of word prediction and completion. These extensions incorporate a structured knowledge base and numerical values from thetextinto the context used to predict the next word. Our automated evaluation on a clinical dataset shows extended models significantly outperform standard models. Our best system uses both conditioning and grounding, because of their orthogonal benefits. For word prediction with a list of 5 suggestions, it improves recall from 25.03% to 71.28% and for word completion it improves keystroke savings from 34.35% to 44.81%, where theoretical bound for this dataset is 58.78%. We also perform a qualitative investigation of how models with lower perplexity occasionally fare better at the tasks. We found that at test time numbers have more influence on the document level than on individual word probabilities.","['Georgios P. Spithourakis', 'Steffen E. Petersen', 'Sebastian Riedel']",,arXiv,2016,https://doi.org/10.48550/arXiv.1610.06370,Anomali
SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods,"In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis that assumes a single entity per document and targeted sentiment analysis that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units oftextoften mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform in this case a QA platform, is used for fine-grained opinionmining.Textcoming from QA platforms is far less constrained compared totextfrom review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks.","['Marzieh Saeidi', 'Guillaume Bouchard', 'Maria Liakata', 'Sebastian Riedel']",,arXiv,2016,https://doi.org/10.48550/arXiv.1610.03771,Anomali
Scalable Construction of Text Indexes,"The suffix array is the key to efficient solutions for myriads of string processing problems in different applications domains, like data compression, datamining, or Bioinformatics. With the rapid growth of available data, suffix array construction algorithms had to be adapted to advanced computational models such as external memory and distributed computing. In this article, we present five suffix array construction algorithms utilizing the new algorithmic big data batch processing framework Thrill, which allows us to process input sizes in orders of magnitude that have not been considered before.","['Timo Bingmann', 'Simon Gog', 'Florian Kurpicz']",,arXiv,2016,https://doi.org/10.48550/arXiv.1610.03007,Anomali
A New Data Representation Based on Training Data Characteristics to Extract Drug Named-Entity in Medical Text,"One essential task in information extraction from the medical corpus is drug name recognition. Compared withtextsources come from other domains, the medicaltextis special and has unique characteristics. In addition, the medicaltextminingposes more challenges, e.g., more unstructuredtext, the fast growing of new terms addition, a wide range of name variation for the same drug. Theminingis even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645.","['Sadikin Mujiono', 'Mohamad Ivan Fanany', 'Chan Basaruddin']",3483528,arXiv,2016,https://doi.org/10.48550/arXiv.1610.01891,Anomali
MPI-FAUN: An MPI-Based Framework for Alternating-Updating Nonnegative Matrix Factorization,"Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such that $A \approx W H$. NMF is a useful tool for many applications in different domains such as topic modeling intextmining, background separation in video analysis, and community detection in social networks. Despite its popularity in the dataminingcommunity, there is a lack of efficient parallel algorithms to solve the problem for big data sets.
  The main contribution of this work is a new, high-performance parallel computational framework for a broad class of NMF algorithms that iteratively solves alternating non-negative least squares (NLS) subproblems for $W$ and $H$. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). The framework is flexible and able to leverage a variety of NMF and NLS algorithms, including Multiplicative Update, Hierarchical Alternating Least Squares, and Block Principal Pivoting. Our implementation allows us to benchmark and compare different algorithms on massive dense and sparse data matrices of size that spans for few hundreds of millions to billions. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements. The code and the datasets used for conducting the experiments are available online.","['Ramakrishnan Kannan', 'Grey Ballard', 'Haesun Park']",,arXiv,2016,https://doi.org/10.48550/arXiv.1609.09154,Anomali
Psychologically Motivated Text Mining,"Natural language processing techniques are increasingly applied to identify social trends and predict behavior based on largetextcollections. Existing methods typically rely on surface lexical and syntactic information. Yet, research in psychology shows that patterns of human conceptualisation, such as metaphorical framing, are reliable predictors of human expectations and decisions. In this paper, we present a method to learn patterns of metaphorical framing from largetextcollections, using statistical techniques. We apply the method to data in three different languages and evaluate the identified patterns, demonstrating their psychological validity.","['Ekaterina Shutova', 'Patricia Lichtenstein']",,arXiv,2016,https://doi.org/10.48550/arXiv.1609.09019,Anomali
Aligning Coordinated Text Streams through Burst Information Network Construction and Decipherment,"Aligning coordinatedtextstreams from multiple sources and multiple languages has opened many new research venues on cross-lingual knowledge discovery. In this paper we aim to advance state-of-the-art by: (1). extending coarse-grained topic-level knowledgeminingto fine-grained information units such as entities and events; (2). following a novel Data-to-Network-to-Knowledge (D2N2K) paradigm to construct and utilize network structures to capture and propagate reliable evidence. We introduce a novel Burst Information Network (BINet) representation that can display the most important information and illustrate the connections among bursty entities, events and keywords in the corpus. We propose an effective approach to construct and decipher BINets, incorporating novel criteria based on multi-dimensional clues from pronunciation, translation, burst, neighbor and graph topological structure. The experimental results on Chinese and English coordinatedtextstreams show that our approach can accurately decipher the nodes with high confidence in the BINets and that the algorithm can be efficiently run in parallel, which makes it possible to apply it to huge amounts of streaming data for never-ending language and information decipherment.","['Tao Ge', 'Qing Dou', 'Xiaoman Pan', 'Heng Ji', 'Lei Cui', 'Baobao Chang', 'Zhifang Sui', 'Ming Zhou']",,arXiv,2016,https://doi.org/10.48550/arXiv.1609.08237,Anomali
An Investigation of Recurrent Neural Architectures for Drug Name Recognition,"Drug name recognition (DNR) is an essential step in the Pharmacovigilance (PV) pipeline. DNR aims to find drug name mentions in unstructured biomedicaltextsand classify them into predefined categories. State-of-the-art DNR approaches heavily rely on hand crafted features and domain specific resources which are difficult to collect and tune. For this reason, this paper investigates the effectiveness of contemporary recurrent neural architectures - the Elman and Jordan networks and the bidirectional LSTM with CRF decoding - at performing DNR straight from thetext. The experimental results achieved on the authoritative SemEval-2013 Task 9.1 benchmarks show that the bidirectional LSTM-CRF ranks closely to highly-dedicated, hand-crafted systems.","['Raghavendra Chalapathy', 'Ehsan Zare Borzeshi', 'Massimo Piccardi']",,arXiv,2016,https://doi.org/10.48550/arXiv.1609.07585,Anomali
Building accurate HAV exploiting User Profiling and Sentiment Analysis,"Social Engineering (SE) is one of the most dangerous aspect an attacker can use against a given entity (private citizen, industry, government, ...). In order to perform SE attacks, it is necessary to collect as much information as possible about the target (or victim(s)). The aim of this paper is to report the details of an activity which took to the development of an automatic tool that extracts, categorizes and summarizes the target interests, thus possible weaknesses with respect to specific topics. Data is collected from the user's activity on social networks, parsed and analyzed usingtextminingtechniques. The main contribution of the proposed tool consists in delivering some reports that allow the citizen, institutions as well as private bodies the screening of their exposure to SE attacks, with a strong awareness potential that will be reflected in a decrease of the risks and a good opportunity to save money.","['Alan Ferrari', 'Angelo Consoli']",,arXiv,2016,https://doi.org/10.48550/arXiv.1609.07302,Anomali
Quantifying the informativeness for biomedical literature summarization: An itemset mining method,"Objective: Automatictextsummarization tools can help users in the biomedical domain to access information efficiently from a large volume of scientific literature and other sources oftextdocuments. In this paper, we propose a summarization method that combines itemsetminingand domain knowledge to construct a concept-based model and to extract the main subtopics from an input document. Our summarizer quantifies the informativeness of each sentence using the support values of itemsets appearing in the sentence. Methods: To address the concept-level analysis oftext, our method initially maps the original document to biomedical concepts using the UMLS. Then, it discovers the essential subtopics of thetextusing a dataminingtechnique, namely itemsetmining, and constructs the summarization model. The employed itemsetminingalgorithm extracts a set of frequent itemsets containing correlated and recurrent concepts of the input document. The summarizer selects the most related and informative sentences and generates the final summary. Results: We evaluate the performance of our itemset-based summarizer using the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics, performing a set of experiments. The results show that the itemset-based summarizer performs better than the compared methods. The itemset-based summarizer achieves the best scores for all the assessed ROUGE metrics . Conclusion: Compared to the statistical, similarity, and word frequency methods, the proposed method demonstrates that the summarization model obtained from the concept extraction and itemsetminingprovides the summarizer with an effective metric for measuring the informative content of sentences. This can lead to an improvement in the performance of biomedical literature summarization.","['Milad Moradi', 'Nasser Ghadiri']",,arXiv,2017,https://doi.org/10.48550/arXiv.1609.03067,Anomali
Image and Video Mining through Online Learning,"Within the field of image and video recognition, the traditional approach is a dataset split into fixed training and test partitions. However, the labelling of the training set is time-consuming, especially as datasets grow in size and complexity. Furthermore, this approach is not applicable to the home user, who wants to intuitively group their media without tirelessly labelling the content. Our interactive approach is able to iteratively cluster classes of images and video. Our approach is based around the concept of an image signature which, unlike a standard bag of words model, can express co-occurrence statistics as well as symbol frequency. We efficiently compute metric distances between signatures despite their inherent high dimensionality and provide discriminative feature selection, to allow common and distinctive elements to be identified from a small set of user labelled examples. These elements are then accentuated in the image signature to increase similarity between examples and pull correct classes together. By repeating this process in an online learning framework, the accuracy of similarity increases dramatically despite labelling only a few training examples. To demonstrate that the approach is agnostic to media type and features used, we evaluate on three image datasets (15 scene, Caltech101 and FG-NET), a mixedtextand image dataset (ImageTag), a dataset used in active learning (Iris) and on three action recognition datasets (UCF11, KTH and Hollywood2). On the UCF11 video dataset, the accuracy is 86.7% despite using only 90 labelled examples from a dataset of over 1200 videos, instead of the standard 1122 training videos. The approach is both scalable and efficient, with a single iteration over the full UCF11 dataset of around 1200 videos taking approximately 1 minute on a standard desktop machine.","['Andrew Gilbert', 'Richard Bowden']",,arXiv,2016,https://doi.org/10.48550/arXiv.1609.02770,Anomali
Mining Half a Billion Topical Experts Across Multiple Social Networks,"Miningtopical experts on social media is a problem that has gained significant attention due to its wide-ranging applications. Here we present the first study that combines data from four major social networks -- Twitter, Facebook, Google+ and LinkedIn, along with the Wikipedia graph and internet webpagetextand metadata, to rank topical experts across the global population of users. We perform an in-depth analysis of 37 features derived from various data sources such as messagetext, user lists, webpages, social graphs and wikipedia. This large-scale study includes more than 12 billion messages over a 90-day sliding window and 58 billion social graph edges. Comparison reveals that features derived from Twitter Lists, Wikipedia, internet webpages and Twitter Followers are especially good indicators of expertise. We train an expertise ranking model using these features on a large ground truth dataset containing almost 90,000 labels. This model is applied within a production system that ranks over 650 million experts in more than 9,000 topical domains on a daily basis. We provide results and examples on the effectiveness of our expert ranking system, along with empirical validation. Finally, we make the topical expertise data available through open REST APIs for wider use.","['Nemanja Spasojevic', 'Prantik Bhattacharyya', 'Adithya Rao']",,arXiv,2016,https://doi.org/10.48550/arXiv.1608.09002,Anomali
What is Wrong with Topic Modeling? (and How to Fix it Using Search-based Software Engineering),"Context: Topic modeling finds human-readable structures in unstructured textual data. A widely used topic modeler is Latent Dirichlet allocation. When run on different datasets, LDA suffers from ""order effects"" i.e. different topics are generated if the order of training data is shuffled. Such order effects introduce a systematic error for any study. This error can relate to misleading results;specifically, inaccurate topic descriptions and a reduction in the efficacy oftextminingclassification results. Objective: To provide a method in which distributions generated by LDA are more stable and can be used for further analysis. Method: We use LDADE, a search-based software engineering tool that tunes LDA's parameters using DE (Differential Evolution). LDADE is evaluated on data from a programmer information exchange site (Stackoverflow), title and abstracttextof thousands ofSoftware Engineering (SE) papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using Gibbs sampling). Results were scored via topic stability andtextminingclassification accuracy. Results: In all treatments: (i) standard LDA exhibits very large topic instability; (ii) LDADE's tunings dramatically reduce cluster instability; (iii) LDADE also leads to improved performances for supervised as well as unsupervised learning. Conclusion: Due to topic instability, using standard LDA with its ""off-the-shelf"" settings should now be depreciated. Also, in future, we should require SE papers that use LDA to test and (if needed) mitigate LDA topic instability. Finally, LDADE is a candidate technology for effectively and efficiently reducing that instability.","['Amritanshu Agrawal', 'Wei Fu', 'Tim Menzies']","Information and Software Technology Journal, 2018",arXiv,2018,https://doi.org/10.48550/arXiv.1608.08176,Anomali
Finding Trends in Software Research,"This paper explores the structure of research papers in software engineering. Usingtextmining, we study 35,391 software engineering (SE) papers from 34 leading SE venues over the last 25 years. These venues were divided, nearly evenly, between conferences and journals. An important aspect of this analysis is that it is fully automated and repeatable. To achieve that automation, we used a stable topic modeling technique called LDADE that fully automates parameter tuning in LDA. Using LDADE, wemine11 topics that represent much of the structure of contemporary SE. The 11 topics presented here should not be ""set in stone"" as the only topics worthy of study in SE. Rather our goal is to report that (a)textminingmethods can detect large scale trends within our community; (b) those topic change with time; so (c) it is important to have automatic agents that can update our understanding of our community whenever new data arrives.","['George Mathew', 'Amritanshu Agrawal', 'Tim Menzies']",,arXiv,2018,https://doi.org/10.48550/arXiv.1608.08100,Anomali
Using Semantic Similarity for Input Topic Identification in Crawling-based Web Application Testing,"To automatically test web applications, crawling-based techniques are usually adopted tominethe behavior models, explore the state spaces or detect the violated invariants of the applications. However, in existing crawlers, rules for identifying the topics of inputtextfields, such as login ids, passwords, emails, dates and phone numbers, have to be manually configured. Moreover, the rules for one application are very often not suitable for another. In addition, when several rules conflict and match an inputtextfield to more than one topics, it can be difficult to determine which rule suggests a better match. This paper presents a natural-language approach to automatically identify the topics of encountered input fields during crawling by semantically comparing their similarities with the input fields in labeled corpus. In our evaluation with 100 real-world forms, the proposed approach demonstrated comparable performance to the rule-based one. Our experiments also show that the accuracy of the rule-based approach can be improved by up to 19% when integrated with our approach.","['Jun-Wei Lin', 'Farn Wang']",,arXiv,2016,https://doi.org/10.48550/arXiv.1608.06549,Anomali
An intelligent classification model for phishing email detection,"Phishing attacks are one of the trending cyber attacks that apply socially engineered messages that are communicated to people from professional hackers aiming at fooling users to reveal their sensitive information, the most popular communication channel to those messages is through users emails. This paper presents an intelligent classification model for detecting phishing emails using knowledge discovery, dataminingandtextprocessing techniques. This paper introduces the concept of phishing terms weighting which evaluates the weight of phishing terms in each email. The pre processing phase is enhanced by applyingtextstemming and WordNet ontology to enrich the model with word synonyms. The model applied the knowledge discovery procedures using five popular classification algorithms and achieved a notable enhancement in classification accuracy, 0.991 accuracy was achieved using the Random Forest algorithm and 0.984 using J48, which is to our knowledge the highest accuracy rate for an accredited data set. This paper also presents a comparative study with similar proposed classification techniques.","['Adwan Yasin', 'Abdelmunem Abuhasan']",,arXiv,2016,https://doi.org/10.48550/arXiv.1608.02196,Anomali
Lévy NMF for robust nonnegative source separation,"Source separation, which consists in decomposing data into meaningful structured components, is an active research topic in many areas, such as music and image signal processing, applied physics andtextmining. In this paper, we introduce the Positive $α$-stable (P$α$S) distributions to model the latent sources, which are a subclass of the stable distributions family. They notably permit us to model random variables that are both nonnegative and impulsive. Considering the Lévy distribution, the only P$α$S distribution whose density is tractable, we propose a mixture model called Lévy Nonnegative Matrix Factorization (Lévy NMF). This model accounts for low-rank structures in nonnegative data that possibly has high variability or is corrupted by very adverse noise. The model parameters are estimated in a maximum-likelihood sense. We also derive an estimator of the sources given the parameters, which extends the validity of the generalized Wiener filtering to the P$α$S case. Experiments on synthetic data show that Lévy NMF compares favorably with state-of-the art techniques in terms of robustness to impulsive noise. The analysis of two types of realistic signals is also considered: musical spectrograms and fluorescence spectra of chemical species. The results highlight the potential of the Lévy NMF model for decomposing nonnegative data.","['Paul Magron', 'Roland Badeau', 'Antoine Liutkus']",,arXiv,2016,https://doi.org/10.48550/arXiv.1608.01844,Anomali
Leveraging Unstructured Data to Detect Emerging Reliability Issues,"Unstructured data refers to information that does not have a predefined data model or is not organized in a pre-defined manner. Loosely speaking, unstructured data refers totextdata that is generated by humans. In after-sales service businesses, there are two main sources of unstructured data: customer complaints, which generally describe symptoms, and technician comments, which outline diagnostics and treatment information. A legitimate customer complaint can eventually be tracked to a failure or a claim. However, there is a delay between the time of a customer complaint and the time of a failure or a claim. A proactive strategy aimed at analyzing customer complaints for symptoms can help service providers detect reliability problems in advance and initiate corrective actions such as recalls. This paper introduces essentialtextminingconcepts in the context of reliability analysis and a method to detect emerging reliability issues. The application of the method is illustrated using a case study.","['Deovrat Kakde', 'Arin Chaudhuri']",,arXiv,2016,https://doi.org/10.48550/arXiv.1607.07745,Anomali
Opinion Mining in Online Reviews About Distance Education Programs,"The popularity of distance education programs is increasing at a fast pace. En par with this development, online communication in fora, social media and reviewing platforms between students is increasing as well. Exploiting this information to support fellow students or institutions requires to extract the relevant opinions in order to automatically generate reports providing an overview of pros and cons of different distance education programs. We report on an experiment involving distance education experts with the goal to develop a dataset of reviews annotated with relevant categories and aspects in each category discussed in the specific review together with an indication of the sentiment.
  Based on this experiment, we present an approach to extract general categories and specific aspects under discussion in a review together with their sentiment. We frame this task as a multi-label hierarchicaltextclassification problem and empirically investigate the performance of different classification architectures to couple the prediction of a category with the prediction of particular aspects in this category. We evaluate different architectures and show that a hierarchical approach leads to superior results in comparison to a flat model which makes decisions independently.","['Janik Jaskolski', 'Fabian Siegberg', 'Thomas Tibroni', 'Philipp Cimiano', 'Roman Klinger']",,arXiv,2016,https://doi.org/10.48550/arXiv.1607.06299,Anomali
Analysis of opinionated text for opinion mining,"In sentiment analysis, the polarities of the opinions expressed on an object/feature are determined to assess the sentiment of a sentence or document whether it is positive/negative/neutral. Naturally, the object/feature is a noun representation which refers to a product or a component of a product, let us say, the ""lens"" in a camera and opinions emanating on it are captured in adjectives, verbs, adverbs and noun words themselves. Apart from such words, other meta-information and diverse effective features are also going to play an important role in influencing the sentiment polarity and contribute significantly to the performance of the system. In this paper, some of the associated information/meta-data are explored and investigated in the sentimenttext. Based on the analysis results presented here, there is scope for further assessment and utilization of the meta-information as features intextcategorization, rankingtextdocument, identification of spam documents and polarity classification problems.","['K Paramesha', 'K C Ravishankar']","Machine Learning and Applications: An International Journal (MLAIJ) Vol.3, No.2, June 2016",arXiv,2016,https://doi.org/10.48550/arXiv.1607.02576,Anomali
Lexical Based Semantic Orientation of Online Customer Reviews and Blogs,"Rapid increase in internet users along with growing power of online review sites and social media has given birth to sentiment analysis or opinionmining, which aims at determining what other people think and comment. Sentiments or Opinions contain public generated content about products, services, policies and politics. People are usually interested to seek positive and negative opinions containing likes and dislikes, shared by users for features of particular product or service. This paper proposed sentence-level lexical based domain independent sentiment classification method for different types of data such as reviews and blogs. The proposed method is based on general lexicons i.e. WordNet, SentiWordNet and user defined lexical dictionaries for semantic orientation. The relations and glosses of these dictionaries provide solution to the domain portability problem. The method performs better than word andtextlevel corpus based machine learning methods for semantic orientation. The results show the proposed method performs better as it shows precision of 87% and83% at document and sentence levels respectively for online comments.","['Aurangzeb khan', 'Khairullah khan', 'Shakeel Ahmad', 'Fazal Masood Kundi', 'Irum Tareen', 'Muhammad Zubair Asghar']",Journal of American Science 2014;10(8) http://www.jofamericanscience.org,arXiv,2016,https://doi.org/10.48550/arXiv.1607.02355,Anomali
Representation learning for very short texts using weighted word embedding aggregation,"Shorttextmessages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of suchtexts, which is important in applications such as event detection, opinionmining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for shorttextsdesigned to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in thetext, and is applicable out-of-the-box.","['Cedric De Boom', 'Steven Van Canneyt', 'Thomas Demeester', 'Bart Dhoedt']",,arXiv,2016,https://doi.org/10.48550/arXiv.1607.00570,Anomali
Representation of texts as complex networks: a mesoscopic approach,"Statistical techniques that analyzetexts, referred to astextanalytics, have departed from the use of simple word count statistics towards a new paradigm.Textminingnow hinges on a more sophisticated set of methods, including the representations in terms of complex networks. While well-established word-adjacency (co-occurrence) methods successfully grasp syntactical features of writtentexts, they are unable to represent important aspects of textual data, such as its topical structure, i.e. the sequence of subjects developing at a mesoscopic level along thetext. Such aspects are often overlooked by current methodologies. In order to grasp the mesoscopic characteristics of semantical content in writtentexts, we devised a network model which is able to analyze documents in a multi-scale fashion. In the proposed model, a limited amount of adjacent paragraphs are represented as nodes, which are connected whenever they share a minimum semantical content. To illustrate the capabilities of our model, we present, as a case example, a qualitative analysis of ""Alice's Adventures in Wonderland"". We show that the mesoscopic structure of a document, modeled as a network, reveals many semantic traits oftexts. Such an approach paves the way to a myriad of semantic-based applications. In addition, our approach is illustrated in a machine learning context, in whichtextsare classified among realtextsand randomized instances.","['Henrique F. de Arruda', 'Filipi N. Silva', 'Vanessa Q. Marinho', 'Diego R. Amancio', 'Luciano da F. Costa']","Journal of Complex Networks 6(1), 125-144, 2018",arXiv,2017,https://doi.org/10.48550/arXiv.1606.09636,Anomali
Satellite Images Analysis with Symbolic Time Series: A Case Study of the Algerian Zone,"Satellite Image Time Series (SITS) are an important source of information for studying land occupation and its evolution. Indeed, the very large volumes of digital data stored, usually are not ready to a direct analysis. In order to both reduce the dimensionality and information extraction, time series datamininggenerally gives rise to change of time series representation. In an objective of information intelligibility extracted from the representation change, we may use symbolic representations of time series. Many high level representations of time series have been proposed for datamining, including Fourier transforms, wavelets, piecewise polynomial models, etc. Many researchers have also considered symbolic representations of time series, noting that such representations would potentiality allow researchers to avail of the wealth of data structures and algorithms from thetextprocessing and bioinformatics communities. We present in this work, one of the main symbolic representation methods ""SAX""(Symbolic Aggregate Approximation) and we experience this method to symbolize and reduce the dimensionality of a Satellite Image Times Series acquired over a period of 5 years by characterizing the evolution of a vegetation index (NDVI).","['Dalila Attaf', 'Djamila Hamdadou', 'Sidahmed Benabderrahmane', 'Aicha Lafrid']",,arXiv,2016,https://doi.org/10.48550/arXiv.1606.07784,Anomali
Is a Picture Worth Ten Thousand Words in a Review Dataset?,"While textual reviews have become prominent in many recommendation-based systems, automated frameworks to provide relevant visual cues againsttextreviews where pictures are not available is a new form of task confronted by dataminingand machine learning researchers. Suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review. We propose a deep learning-based framework to automatically: (1) tag the images available in a review dataset, (2) generate a caption for each image that does not have one, and (3) enhance each review by recommending relevant images that might not be uploaded by the corresponding reviewer. We evaluate the proposed framework using the Yelp Challenge Dataset. While a subset of the images in this particular dataset are correctly captioned, the majority of the pictures do not have any associatedtext. Moreover, there is no mapping between reviews and images. Each image has a corresponding business-tag where the picture was taken, though. The overall data setting and unavailability of crucial pieces required for a mapping make the problem of recommending images for reviews a major challenge. Qualitative and quantitative evaluations indicate that our proposed framework provides high quality enhancements through automatic captioning, tagging, and recommendation for mapping reviews and images.","['Roberto Camacho Barranco', 'Laura M. Rodriguez', 'Rebecca Urbina', 'M. Shahriar Hossain']",,arXiv,2016,https://doi.org/10.48550/arXiv.1606.07496,Anomali
Gender and Interest Targeting for Sponsored Post Advertising at Tumblr,"As one of the leading platforms for creative content, Tumblr offers advertisers a unique way of creating brand identity. Advertisers can tell their story through images, animation,text, music, video, and more, and promote that content by sponsoring it to appear as an advertisement in the streams of Tumblr users. In this paper we present a framework that enabled one of the key targeted advertising components for Tumblr, specifically gender and interest targeting. We describe the main challenges involved in development of the framework, which include creating the ground truth for training gender prediction models, as well as mapping Tumblr content to an interest taxonomy. For purposes of inferring user interests we propose a novel semi-supervised neural language model for categorization of Tumblr content (i.e., post tags and post keywords). The model was trained on a large-scale data set consisting of 6.8 billion user posts, with very limited amount of categorized keywords, and was shown to have superior performance over the bag-of-words model. We successfully deployed gender and interest targeting capability in Yahoo production systems, delivering inference for users that cover more than 90% of daily activities at Tumblr. Online performance results indicate advantages of the proposed approach, where we observed 20% lift in user engagement with sponsored posts as compared to untargeted campaigns.","['Mihajlo Grbovic', 'Vladan Radosavljevic', 'Nemanja Djuric', 'Narayan Bhamidipati', 'Ananth Nagarajan']","Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2015), Sydney, Australia",arXiv,2016,https://doi.org/10.48550/arXiv.1606.07189,Anomali
FSMJ: Feature Selection with Maximum Jensen-Shannon Divergence for Text Categorization,"In this paper, we present a new wrapper feature selection approach based on Jensen-Shannon (JS) divergence, termed feature selection with maximum JS-divergence (FSMJ), fortextcategorization. Unlike most existing feature selection approaches, the proposed FSMJ approach is based on real-valued features which provide more information for discrimination than binary-valued features used in conventional approaches. We show that the FSMJ is a greedy approach and the JS-divergence monotonically increases when more features are selected. We conduct several experiments on real-life data sets, compared with the state-of-the-art feature selection approaches fortextcategorization. The superior performance of the proposed FSMJ approach demonstrates its effectiveness and further indicates its wide potential applications on datamining.","['Bo Tang', 'Haibo He']",,arXiv,2016,https://doi.org/10.48550/arXiv.1606.06366,Anomali
How marketing vocabulary was evolving from 2005 to 2014? An illustrative application of statistical methods on text mining,"Here a collection of 1169 abstracts, which corresponds to articles that the Journal of Marketing Research has published from 2005 to 2014, are analysed under a novel approach. We apply several statistical methods, such as Principal Components Analysis and Correspondence Analysis to identify the way Marketing vocabulary is evolving. Similarly those articles that introduce new vocabulary are identified and the preferred words by authors are also detected. In order to provide an easy-to-understand explanation, we provide our results graphically. A word-cloud with the most frequent words is given first. Secondly abstracts-words are represented on the factorial plane. Finally one representation of word-years allows us to detect changes on the vocabulary through the passing of time.","['Igor Barahona', 'Daria Micaela Hernandez', 'Hector Hugo Perez-Villarreal']","JSM Proceedings (2015) Section on Statistics in Marketing. Alexandria, VA. American Statistical Association. 1121-1131",arXiv,2016,https://doi.org/10.48550/arXiv.1606.03963,Anomali
Models Coupling Urban Growth and Transportation Network Growth : An Algorithmic Systematic Review Approach,"A broad bibliographical study suggests a scarcity of quantitative models of simulation integrating both network and urban growth. This absence may be due to diverging interests of concerned disciplines, resulting in a lack of communication. We propose to proceed to an algorithmic systematic review to give quantitative elements of answer to this question. A formal iterative algorithm to retrieve corpuses of references from initial keywords, based ontext-mining, is developed and implemented. We study its convergence properties and do a sensitivity analysis. We then apply it on queries representative of the specific question, for which results tend to confirm the assumption of disciplines compartmentalisation.",['Juste Raimbault'],,arXiv,2016,https://doi.org/10.48550/arXiv.1605.08888,Anomali
SS4MCT: A Statistical Stemmer for Morphologically Complex Texts,"There have been multiple attempts to resolve various inflection matching problems in information retrieval. Stemming is a common approach to this end. Among many techniques for stemming, statistical stemming has been shown to be effective in a number of languages, particularly highly inflected languages. In this paper we propose a method for finding affixes in different positions of a word. Common statistical techniques heavily rely on string similarity in terms of prefix and suffix matching. Since infixes are common in irregular/informal inflections in morphologically complextexts, it is required to find infixes for stemming. In this paper we propose a method whose aim is to find statistical inflectional rules based on minimum edit distance table of word pairs and the likelihoods of the rules in a language. These rules are used to statistically stem words and can be used in differenttextminingtasks. Experimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks indicate that the proposed method significantly outperforms all the baselines in terms of MAP.","['Javid Dadashkarimi', 'Hossein Nasr Esfahani', 'Heshaam Faili', 'Azadeh Shakery']",,arXiv,2016,https://doi.org/10.48550/arXiv.1605.07852,Anomali
As Cool as a Cucumber: Towards a Corpus of Contemporary Similes in Serbian,"Similes are natural language expressions used to compare unlikely things, where the comparison is not taken literally. They are often used in everyday communication and are an important part of cultural heritage. Having an up-to-date corpus of similes is challenging, as they are constantly coined and/or adapted to the contemporary times. In this paper we present a methodology for semi-automated collection of similes from the world wide web usingtextminingtechniques. We expanded an existing corpus of traditional similes (containing 333 similes) by collecting 446 additional expressions. We, also, explore how crowdsourcing can be used to extract and curate new similes.","['Nikola Milosevic', 'Goran Nenadic']",,arXiv,2016,https://doi.org/10.48550/arXiv.1605.06319,Anomali
Matching Handwritten Document Images,"We address the problem of predicting similarity between a pair of handwritten document images written by different individuals. This has applications related to matching andminingin image collections containing handwritten content. A similarity score is computed by detecting patterns oftextre-usages between document images irrespective of the minor variations in word morphology, word ordering, layout and paraphrasing of the content. Our method does not depend on an accurate segmentation of words and lines. We formulate the document matching problem as a structured comparison of the word distributions across two document images. To match two word images, we propose a convolutional neural network (CNN) based feature descriptor. Performance of this representation surpasses the state-of-the-art on handwritten word spotting. Finally, we demonstrate the applicability of our method on a practical problem of matching handwritten assignments.","['Praveen Krishnan', 'C. V. Jawahar']",,arXiv,2016,https://doi.org/10.48550/arXiv.1605.05923,Anomali
From Query to Usable Code: An Analysis of Stack Overflow Code Snippets,"Enriched by natural languagetexts, Stack Overflow code snippets are an invaluable code-centric knowledge base of small units of source code. Besides being useful for software developers, these annotated snippets can potentially serve as the basis for automated tools that provide working code solutions to specific natural language queries.
  With the goal of developing automated tools with the Stack Overflow snippets and surroundingtext, this paper investigates the following questions: (1) How usable are the Stack Overflow code snippets? and (2) When usingtextsearch engines for matching on the natural language questions and answers around the snippets, what percentage of the top results contain usable code snippets?
  A total of 3M code snippets are analyzed across four languages: C\#, Java, JavaScript, and Python. Python and JavaScript proved to be the languages for which the most code snippets are usable. Conversely, Java and C\# proved to be the languages with the lowest usability rate. Further qualitative analysis on usable Python snippets shows the characteristics of the answers that solve the original question. Finally, we use Google search to investigate the alignment of usability and the natural language annotations around code snippets, and explore how to make snippets in Stack Overflow an adequate base for future automatic program generation.","['Di Yang', 'Aftab Hussain', 'Cristina Lopes']",,arXiv,2016,https://doi.org/10.48550/arXiv.1605.04464,Anomali
Online Optimization Methods for the Quantification Problem,"The estimation of class prevalence, i.e., the fraction of a population that belongs to a certain class, is a very useful tool in data analytics and learning, and finds applications in many domains such as sentiment analysis, epidemiology, etc. For example, in sentiment analysis, the objective is often not to estimate whether a specifictextconveys a positive or a negative sentiment, but rather estimate the overall distribution of positive and negative sentiments during an event window. A popular way of performing the above task, often dubbed quantification, is to use supervised learning to train a prevalence estimator from labeled data.
  Contemporary literature cites several performance measures used to measure the success of such prevalence estimators. In this paper we propose the first online stochastic algorithms for directly optimizing these quantification-specific performance measures. We also provide algorithms that optimize hybrid performance measures that seek to balance quantification and classification performance. Our algorithms present a significant advancement in the theory of multivariate optimization and we show, by a rigorous theoretical analysis, that they exhibit optimal convergence. We also report extensive experiments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization techniques used for these performance measures.","['Purushottam Kar', 'Shuai Li', 'Harikrishna Narasimhan', 'Sanjay Chawla', 'Fabrizio Sebastiani']","Final version published in Proceedings of the 22nd ACM Conference on Knowledge Discovery and Data Mining (KDD 2016), San Francisco, US, 2016, pp. 1625-1634",arXiv,2016,https://doi.org/10.48550/arXiv.1605.04135,Anomali
Data mining : past present and future - a typical survey on data streams,"Data StreamMiningis one of the area gaining lot of practical significance and is progressing at a brisk pace with new methods, methodologies and findings in various applications related to medicine, computer science, bioinformatics and stock market prediction, weather forecast,text, audio and video processing to name a few. Data happens to be the key concern in datamining. With the huge online data generated from several sensors, Internet Relay Chats, Twitter, Face book, Online Bank or ATM Transactions, the concept of dynamically changing data is becoming a key challenge, what we call as data streams. In this paper, we give the algorithm for finding frequent patterns from data streams with a case study and identify the research issues in handling data streams.","['M. S. B. PhridviRaja', 'C. V. GuruRao']",,arXiv,2016,https://doi.org/10.48550/arXiv.1605.01429,Anomali
Text-mining the NeuroSynth corpus using Deep Boltzmann Machines,"Large-scale automated meta-analysis of neuroimaging data has recently established itself as an important tool in advancing our understanding of human brain function. This research has been pioneered by NeuroSynth, a database collecting both brain activation coordinates and associatedtextacross a large cohort of neuroimaging research papers. One of the fundamental aspects of such meta-analysis istext-mining. To date, word counts and more sophisticated methods such as Latent Dirichlet Allocation have been proposed. In this work we present an unsupervised study of the NeuroSynthtextcorpus using Deep Boltzmann Machines (DBMs). The use of DBMs yields several advantages over the aforementioned methods, principal among which is the fact that it yields both word and document embeddings in a high-dimensional vector space. Such embeddings serve to facilitate the use of traditional machine learning techniques on thetextcorpus. The proposed DBM model is shown to learn embeddings with a clear semantic structure.","['Ricardo Pio Monti', 'Romy Lorenz', 'Robert Leech', 'Christoforos Anagnostopoulos', 'Giovanni Montana']",,arXiv,2016,https://doi.org/10.48550/arXiv.1605.00223,Anomali
Liposomes versus metallic nanostructures: differences in the process of knowledge translation in cancer,"This research maps the knowledge translation process for two different types of nanotechnologies applied to cancer: liposomes and metallic nanostructures (MNs). We performed a structural analysis of citation networks andtextminingsupported in controlled vocabularies. In the case of liposomes, our results identify subnetworks (invisible colleges) associated with different therapeutic strategies: nanopharmacology, hyperthermia, and gene therapy. Only in the pharmacological strategy was an organized knowledge translation process identified, which, however, is monopolized by the liposomal doxorubicins. In the case of MNs, subnetworks are not differentiated by the type of therapeutic strategy, and the content of the documents is still basic research. Research on MNs is highly focused on developing a combination of molecular imaging and photothermal therapy.","['David Fajardo-Ortiz', 'Luis Duran', 'Laura Moreno', 'Hector Ochoa', 'Victor-M Castano']",International Journal of Nanomedicine. 2014; 9: 2627-2634,arXiv,2016,https://doi.org/10.48550/arXiv.1604.04641,Anomali
Accessing accurate documents by mining auxiliary document information,"Earlier techniques oftextminingincluded algorithms like k-means, Naive Bayes, SVM which classify and cluster thetextdocument forminingrelevant information about the documents. The need for improving theminingtechniques has us searching for techniques using the available algorithms. This paper proposes one technique which uses the auxiliary information that is present inside thetextdocuments to improve themining. This auxiliary information can be a description to the content. This information can be either useful or completely useless formining. The user should assess the worth of the auxiliary information before considering this technique fortextmining. In this paper, a combination of classical clustering algorithms is used tominethe datasets. The algorithm runs in two stages which carry outminingat different levels of abstraction. The clustered documents would then be classified based on the necessary groups. The proposed technique is aimed at improved results of document clustering.","['Jinju Joby', 'Jyothi Korra']",,arXiv,2016,https://doi.org/10.48550/arXiv.1604.04558,Anomali
Multilevel Weighted Support Vector Machine for Classification on Healthcare Data with Missing Values,"This work is motivated by the needs of predictive analytics on healthcare data as represented by Electronic Medical Records. Such data is invariably problematic: noisy, with missing entries, with imbalance in classes of interests, leading to serious bias in predictive modeling. Since standard dataminingmethods often produce poor performance measures, we argue for development of specialized techniques of data-preprocessing and classification. In this paper, we propose a new method to simultaneously classify large datasets and reduce the effects of missing values. It is based on a multilevel framework of the cost-sensitive SVM and the expected maximization imputation method for missing values, which relies on iterated regression analyses. We compare classification results of multilevel SVM-based algorithms on public benchmark datasets with imbalanced classes and missing values as well as real data in health applications, and show that our multilevel SVM-based method produces fast, and more accurate and robust classification results.","['Talayeh Razzaghi', 'Oleg Roderick', 'Ilya Safro', 'Nicholas Marko']",,arXiv,2016,https://doi.org/10.48550/arXiv.1604.02123,Anomali
Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text,"This paper investigates how linguistic knowledgeminedfrom largetextcorpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on largetextcorpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.","['Subhashini Venugopalan', 'Lisa Anne Hendricks', 'Raymond Mooney', 'Kate Saenko']",Proc.EMNLP (2016) pg.1961-1966,arXiv,2016,https://doi.org/10.48550/arXiv.1604.01729,Anomali
Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation,"Despite the recent advances in automatically describing image contents, their applications have been mostly limited to image caption datasets containing natural images (e.g., Flickr 30k, MSCOCO). In this paper, we present a deep learning model to efficiently detect a disease from an image and annotate its contexts (e.g., location, severity and the affected organs). We employ a publicly available radiology dataset of chest x-rays and their reports, and use its image annotations tominedisease names to train convolutional neural networks (CNNs). In doing so, we adopt various regularization techniques to circumvent the large normal-vs-diseased cases bias. Recurrent neural networks (RNNs) are then trained to describe the contexts of a detected disease, based on the deep CNN features. Moreover, we introduce a novel approach to use the weights of the already trained pair of CNN/RNN on the domain-specific image/textdataset, to infer the joint image/textcontexts for composite image labeling. Significantly improved image annotation results are demonstrated using the recurrent neural cascade model by taking the joint image/textcontexts into account.","['Hoo-Chang Shin', 'Kirk Roberts', 'Le Lu', 'Dina Demner-Fushman', 'Jianhua Yao', 'Ronald M Summers']",,arXiv,2016,https://doi.org/10.48550/arXiv.1603.08486,Anomali
Semantic Properties of Customer Sentiment in Tweets,"An increasing number of people are using online social networking services (SNSs), and a significant amount of information related to experiences in consumption is shared in this new media form.Textminingis an emerging technique formininguseful information from the web. We aim at discovering in particular tweets semantic patterns in consumers' discussions on social media. Specifically, the purposes of this study are twofold: 1) finding similarity and dissimilarity between two sets of textual documents that include consumers' sentiment polarities, two forms of positive vs. negative opinions and 2) driving actual content from the textual data that has a semantic trend. The considered tweets include consumers opinions on US retail companies (e.g., Amazon, Walmart). Cosine similarity and K-means clustering methods are used to achieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic modeling algorithm, is used for the latter purpose. This is the first study which discover semantic properties of textual data in consumption context beyond sentiment analysis. In addition to major findings, we apply LDA (Latent Dirichlet Allocations) to the same data and drew latent topics that represent consumers' positive opinions and negative opinions on social media.","['Eun Hee Ko', 'Diego Klabjan']",,arXiv,2016,https://doi.org/10.48550/arXiv.1603.07624,Anomali
The Anatomy of a Search and Mining System for Digital Archives,"Samtla (Search AndMiningTools with Linguistic Analysis) is a digital humanities system designed in collaboration with historians and linguists to assist them with their research work in quantifying the content of any textual corpora through approximate phrase search and document comparison. The retrieval engine uses a character-based n-gram language model rather than the conventional word-based one so as to achieve great flexibility in language agnostic query processing.
  The index is implemented as a space-optimised character-based suffix tree with an accompanying database of document content and metadata. A number oftextminingtools are integrated into the system to allow researchers to discover textual patterns, perform comparative analysis, and find out what is currently popular in the research community.
  Herein we describe the system architecture, user interface, models and algorithms, and data storage of the Samtla system. We also present several case studies of its usage in practice together with an evaluation of the systems' ranking performance through crowdsourcing.","['Martyn Harris', 'Mark Levene', 'Dell Zhang', 'Dan Levene']",,arXiv,2016,https://doi.org/10.48550/arXiv.1603.07150,Anomali
"Mining Valence, Arousal, and Dominance - Possibilities for Detecting Burnout and Productivity?","Similar to other industries, the software engineering domain is plagued by psychological diseases such as burnout, which lead developers to lose interest, exhibit lower activity and/or feel powerless. Prevention is essential for such diseases, which in turn requires early identification of symptoms. The emotional dimensions of Valence, Arousal and Dominance (VAD) are able to derive a person's interest (attraction), level of activation and perceived level of control for a particular situation from textual communication, such as emails. As an initial step towards identifying symptoms of productivity loss in software engineering, this paper explores the VAD metrics and their properties on 700,000 Jira issue reports containing over 2,000,000 comments, since issue reports keep track of a developer's progress on addressing bugs or new features. Using a general-purpose lexicon of 14,000 English words with known VAD scores, our results show that issue reports of different type (e.g., Feature Request vs. Bug) have a fair variation of Valence, while increase in issue priority (e.g., from Minor to Critical) typically increases Arousal. Furthermore, we show that as an issue's resolution time increases, so does the arousal of the individual the issue is assigned to. Finally, the resolution of an issue increases valence, especially for the issue Reporter and for quickly addressed issues. The existence of such relations between VAD and issue report activities shows promise thattextminingin the future could offer an alternative way for work health assessment surveys.","['Mika Mäntylä', 'Bram Adams', 'Giuseppe Destefanis', 'Daniel Graziotin', 'Marco Ortu']",MSR '16 Proceedings of the 13th International Workshop on Mining Software Repositories Pages 247-258. 2016,arXiv,2016,https://doi.org/10.48550/arXiv.1603.04287,Anomali
Intrusion Detection A Text Mining Based Approach,"Intrusion Detection is one of major threats for organization. The approach of intrusion detection usingtextprocessing has been one of research interests which is gaining significant importance from researchers. Intextminingbased approach for intrusion detection, system calls serve as source forminingand predicting possibility of intrusion or attack. When an application runs, there might be several system calls which are initiated in the background. These system calls form the strong basis and the deciding factor for intrusion detection. In this paper, we mainly discuss the approach for intrusion detection by designing a distance measure which is designed by taking into consideration the conventional Gaussian function and modified to suit the need for similarity function. A Framework for intrusion detection is also discussed as part of this research.","['Gunupudi RajeshKumar', 'N Mangathayaru', 'G Narsimha']",,arXiv,2016,https://doi.org/10.48550/arXiv.1603.03837,Anomali
Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning,"In this paper we consider two sequence tagging tasks for medieval Latin: part-of-speech tagging and lemmatization. These are both basic, yet foundational preprocessing steps in applications such astextre-use detection. Nevertheless, they are generally complicated by the considerable orthographic variation which is typical of medieval Latin. In Digital Classics, these tasks are traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion. For example, a lexicon is used to generate all the potential lemma-tag pairs for a token, and next, a context-aware PoS-tagger is used to select the most appropriate tag-lemma pair. Apart from the problems with out-of-lexicon items, error percolation is a major downside of such approaches. In this paper we explore the possibility to elegantly solve these tasks using a single, integrated approach. For this, we make use of a layered neural network architecture from the field of deep representation learning.","['Mike Kestemont', 'Jeroen De Gussem']","Journal of Data Mining & Digital Humanities, Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages, Towards a Digital Ecosystem: NLP. Corpus infrastructure. Methods for Retrieving Texts and Computing Text Similarities (August 6, 2017) jdmdh:1398",arXiv,2017,https://doi.org/10.48550/arXiv.1603.01597,Anomali
Event Search and Analytics: Detecting Events in Semantically Annotated Corpora for Search and Analytics,"In this article, I present the questions that I seek to answer in my PhD research. I posit to analyze natural languagetextwith the help of semantic annotations andmineimportant events for navigating largetextcorpora. Semantic annotations such as named entities, geographic locations, and temporal expressions can help usmineevents from the given corpora. These events thus provide us with useful means to discover the locked knowledge in them. I pose three problems that can help unlock this knowledge vault in semantically annotatedtextcorpora: i. identifying important events; ii. semantic search; and iii. event analytics.",['Dhruv Gupta'],,arXiv,2016,https://doi.org/10.48550/arXiv.1603.00260,Anomali
Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus,"We propose a method for efficiently finding all parallel passages in a large corpus, even if the passages are not quite identical due to rephrasing and orthographic variation. The key ideas are the representation of each word in the corpus by its two most infrequent letters, finding matched pairs of strings of four or five words that differ by at most one word and then identifying clusters of such matched pairs. Using this method, over 4600 parallel pairs of passages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of over 1.8 million words, in just over 30 seconds. Empirical comparisons on sample data indicate that the coverage obtained by our method is essentially the same as that obtained using slow exhaustive methods.","['Avi Shmidman', 'Moshe Koppel', 'Ely Porat']","Journal of Data Mining & Digital Humanities, Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages, Towards a Digital Ecosystem: NLP. Corpus infrastructure. Methods for Retrieving Texts and Computing Text Similarities (March 11, 2018) jdmdh:1388",arXiv,2017,https://doi.org/10.48550/arXiv.1602.08715,Anomali
QuotationFinder - Searching for Quotations and Allusions in Greek and Latin Texts and Establishing the Degree to Which a Quotation or Allusion Matches Its Source,"The software programs generally used with the TLG (Thesaurus Linguae Graecae) and the CLCLT (CETEDOC Library of Christian LatinTexts) CD-ROMs are not well suited for finding quotations and allusions. QuotationFinder uses more sophisticated criteria as it ranks search results based on how closely they match the sourcetext, listing search results with literal quotations first and loose verbal parallels last.",['Luc Herren'],"Journal of Data Mining & Digital Humanities, Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages, Project presentations (August 2, 2017) jdmdh:1389",arXiv,2017,https://doi.org/10.48550/arXiv.1602.08657,Anomali
"2-Bit Random Projections, NonLinear Estimators, and Approximate Near Neighbor Search","The method of random projections has become a standard tool for machine learning, datamining, and search with massive data at Web scale. The effective use of random projections requires efficient coding schemes for quantizing (real-valued) projected data into integers. In this paper, we focus on a simple 2-bit coding scheme. In particular, we develop accurate nonlinear estimators of data similarity based on the 2-bit strategy. This work will have important practical applications. For example, in the task of near neighbor search, a crucial step (often called re-ranking) is to compute or estimate data similarities once a set of candidate data points have been identified by hash table techniques. This re-ranking step can take advantage of the proposed coding scheme and estimator.
  As a related task, in this paper, we also study a simple uniform quantization scheme for the purpose of building hash tables with projected data. Our analysis shows that typically only a small number of bits are needed. For example, when the target similarity level is high, 2 or 3 bits might be sufficient. When the target similarity level is not so high, it is preferable to use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good choice for the task of sublinear time approximate near neighbor search via hash tables.
  Combining these results, we conclude that 2-bit random projections should be recommended for approximate near neighbor search and similarity estimation. Extensive experimental results are provided.","['Ping Li', 'Michael Mitzenmacher', 'Anshumali Shrivastava']",,arXiv,2016,https://doi.org/10.48550/arXiv.1602.06577,Anomali
Corpus analysis without prior linguistic knowledge - unsupervised mining of phrases and subphrase structure,"When looking at the structure of natural language, ""phrases"" and ""words"" are central notions. We consider the problem of identifying such ""meaningful subparts"" of language of any length and underlying composition principles in a completely corpus-based and language-independent way without using any kind of prior linguistic knowledge. Unsupervised methods for identifying ""phrases"",miningsubphrase structure and finding words in a fully automated way are described. This can be considered as a step towards automatically computing a ""general dictionary and grammar of the corpus"". We hope that in the long run variants of our approach turn out to be useful for other kind of sequence data as well, such as, e.g., speech, genom sequences, or music annotation. Even if we are not primarily interested in immediate applications, results obtained for a variety of languages show that our methods are interesting for many practical tasks intextmining, terminology extraction and lexicography, search engine technology, and related fields.","['Stefan Gerdjikov', 'Klaus U. Schulz']",,arXiv,2016,https://doi.org/10.48550/arXiv.1602.05772,Anomali
Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data,"Data represented as strings abounds in biology, linguistics, documentmining, web search and many other fields. Such data often have a hierarchical structure, either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings. We propose a framework, referred to as ""Lexis"", that produces an optimized hierarchical representation of a given set of ""target"" strings. The resulting hierarchy, ""Lexis-DAG"", shows how to construct each target through the concatenation of intermediate substrings, minimizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-Hardness for two cost formulations, we propose an efficient greedy algorithm for the construction of Lexis-DAGs. We also consider the problem of identifying the set of intermediate nodes (substrings) that collectively form the ""core"" of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse applications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-basedtextcompression, and feature extraction from a set of documents.","['Payam Siyari', 'Bistra Dilkina', 'Constantine Dovrolis']",,arXiv,2016,https://doi.org/10.48550/arXiv.1602.05561,Anomali
Scalable Text Mining with Sparse Generative Models,"The information age has brought a deluge of data. Much of this is intextform, insurmountable in scope for humans and incomprehensible in structure for computers.Textminingis an expanding field of research that seeks to utilize the information contained in vast document collections. General dataminingmethods based on machine learning face challenges with the scale oftextdata, posing a need for scalabletextminingmethods.
  This thesis proposes a solution to scalabletextmining: generative models combined with sparse computation. A unifying formalization for generativetextmodels is defined, bringing together research traditions that have used formally equivalent models, but ignored parallel developments. This framework allows the use of methods developed in different processing tasks such as retrieval and classification, yielding effective solutions across differenttextminingtasks. Sparse computation using inverted indices is proposed for inference on probabilistic models. This reduces the computational complexity of the commontextminingoperations according to sparsity, yielding probabilistic models with the scalability of modern search engines.
  The proposed combination provides sparse generative models: a solution fortextminingthat is general, effective, and scalable. Extensive experimentation ontextclassification and ranked retrieval datasets are conducted, showing that the proposed solution matches or outperforms the leading task-specific methods in effectiveness, with a order of magnitude decrease in classification times for Wikipedia article categorization with a million classes. The developed methods were further applied in two 2014 Kaggle dataminingprize competitions with over a hundred competing teams, earning first and second places.",['Antti Puurula'],,arXiv,2016,https://doi.org/10.48550/arXiv.1602.02332,Anomali
Mining Software Quality from Software Reviews: Research Trends and Open Issues,Software reviewtextfragments have considerably valuable information about users experience. It includes a huge set of properties including the software quality. Opinionminingor sentiment analysis is concerned with analyzing textual user judgments. The application of sentiment analysis on software reviews can find a quantitative value that represents software quality. Although many software quality methods are proposed they are considered difficult to customize and many of them are limited. This article investigates the application of opinionminingas an approach to extract software quality properties. We found that the major issues of software reviewsminingusing sentiment analysis are due to software lifecycle and the diverse users and teams.,"['Issa Atoum', 'Ahmed Otoom']","International Journal of Computer Trends and Technology,Vol. 31, No. 2, Jan 2016",arXiv,2016,https://doi.org/10.48550/arXiv.1602.02133,Anomali
Marvin: Semantic annotation using multiple knowledge sources,"People are producing more written material then anytime in the history. The increase is so high that professionals from the various fields are no more able to cope with this amount of publications.Textminingtools can offer tools to help them and one of the tools that can aid information retrieval and information extraction is semantictextannotation. In this report we present Marvin, atextannotator written in Java, which can be used as a command line tool and as a Java library. Marvin is able to annotatetextusing multiple sources, including WordNet, MetaMap, DBPedia and thesauri represented as SKOS.",['Nikola Milosevic'],,arXiv,2016,https://doi.org/10.48550/arXiv.1602.00515,Anomali
Detecting and Extracting Events from Text Documents,"Events of various kinds are mentioned and discussed intextdocuments, whether they are books, news articles, blogs or microblog feeds. The paper starts by giving an overview of how events are treated in linguistics and philosophy. We follow this discussion by surveying how events and associated information are handled in computationally. In particular, we look at how textual documents can beminedto extract events and ancillary information. These days, it is mostly through the application of various machine learning techniques. We also discuss applications of event detection and extraction systems, particularly in summarization, in the medical domain and in the context of Twitter posts. We end the paper with a discussion of challenges and future directions.",['Jugal Kalita'],,arXiv,2016,https://doi.org/10.48550/arXiv.1601.04012,Anomali
De novo visual proteomics in single cells through pattern mining,"Cryo-electron tomography enables 3D visualization of cells in a near native state at molecular resolution. The produced cellular tomograms contain detailed information about all macromolecular complexes, their structures, their abundances and their specific spatial locations in the cell. However, extracting this information is very challenging and current methods usually rely on templates of known structure. Here, we formulate a template-free visual proteomics analysis as a de novo patternminingproblem and propose a new framework called ""Multi Pattern Pursuit"" for supporting proteome-scale de novo discovery of macromolecular complexes in cellular tomograms without using templates of known structures. Our tests on simulated and experimental tomograms show that our method is a promising tool for template-free visual proteomics analysis.","['Min Xu', 'Elitza I Tocheva', 'Yi-Wei Chang', 'Grant J Jensen', 'Frank Alber']",Structure. 2019 Apr 2;27(4):679-691.e14,arXiv,2016,https://doi.org/10.48550/arXiv.1512.09347,Anomali
Mining Massive Hierarchical Data Using a Scalable Probabilistic Graphical Model,"Probabilistic Graphical Models (PGM) are very useful in the fields of machine learning and datamining. The crucial limitation of those models,however, is the scalability. The Bayesian Network, which is one of the most common PGMs used in machine learning and datamining, demonstrates this limitation when the training data consists of random variables, each of them has a large set of possible values. In the big data era, one would expect new extensions to the existing PGMs to handle the massive amount of data produced these days by computers, sensors and other electronic devices. With hierarchical data - data that is arranged in a treelike structure with several levels - one would expect to see hundreds of thousands or millions of values distributed over even just a small number of levels. When modeling this kind of hierarchical data across large data sets, Bayesian Networks become infeasible for representing the probability distributions. In this paper we introduce an extension to Bayesian Networks to handle massive sets of hierarchical data in a reasonable amount of time and space. The proposed model achieves perfect precision of 1.0 and high recall of 0.93 when it is used as multi-label classifier for the annotation of mass spectrometry data. On another data set of 1.5 billion search logs provided by CareerBuilder.com the model was able to predict latent semantic relationships between search keywords with accuracy up to 0.80.","['Khalifeh AlJadda', 'Mohammed Korayem', 'Camilo Ortiz', 'Trey Grainger', 'John A. Miller', 'Khaled Rasheed', 'Krys J. Kochut', 'William S. York', 'Rene Ranzinger', 'Melody Porterfield']",,arXiv,2015,https://doi.org/10.48550/arXiv.1512.08525,Anomali
Mined Semantic Analysis: A New Concept Space Model for Semantic Representation of Textual Data,"MinedSemantic Analysis (MSA) is a novel concept space model which employs unsupervised learning to generate semantic representations oftext. MSA represents textual structures (terms, phrases, documents) as a Bag of Concepts (BoC) where concepts are derived from concept rich encyclopedic corpora. Traditional concept space models exploit only target corpus content to construct the concept space. MSA, alternatively, uncovers implicit relations between concepts byminingfor their associations (e.g.,miningWikipedia's ""See also"" link graph). We evaluate MSA's performance on benchmark datasets for measuring semantic relatedness of words and sentences. Empirical results show competitive performance of MSA compared to prior state-of-the-art methods. Additionally, we introduce the first analytical study to examine statistical significance of results reported by different semantic relatedness methods. Our study shows that, the nuances of results across top performing methods could be statistically insignificant. The study positions MSA as one of state-of-the-art methods for measuring semantic relatedness, besides the inherent interpretability and simplicity of the generated semantic representation.","['Walid Shalaby', 'Wlodek Zadrozny']",,arXiv,2017,https://doi.org/10.48550/arXiv.1512.03465,Anomali
Unsupervised comparable corpora preparation and exploration for bi-lingual translation equivalents,"The multilingual nature of the world makes translation a crucial requirement today. Parallel dictionaries constructed by humans are a widely-available resource, but they are limited and do not provide enough coverage for good quality translation purposes, due to out-of-vocabulary words and neologisms. This motivates the use of statistical translation systems, which are unfortunately dependent on the quantity and quality of training data. Such systems have a very limited availability especially for some languages and very narrowtextdomains. In this research we present our improvements to current comparable corporaminingmethodologies by re- implementation of the comparison algorithms (using Needleman-Wunch algorithm), introduction of a tuning script and computation time improvement by GPU acceleration. Experiments are carried out on bilingual data extracted from the Wikipedia, on various domains. For the Wikipedia itself, additional cross-lingual comparison heuristics were introduced. The modifications made a positive impact on the quality and quantity ofmineddata and on the translation quality.","['Krzysztof Wołk', 'Krzysztof Marasek']","Proceedings of the 12th IWSLT, Da Nang, Vietnam, December 3-4, 2015, p.118-125",arXiv,2015,https://doi.org/10.48550/arXiv.1512.01641,Anomali
Machine Learning Sentiment Prediction based on Hybrid Document Representation,"Automated sentiment analysis and opinionminingis a complex process concerning the extraction of useful subjective information fromtext. The explosion of user generated content on the Web, especially the fact that millions of users, on a daily basis, express their opinions on products and services to blogs, wikis, social networks, message boards, etc., render the reliable, automated export of sentiments and opinions from unstructuredtextcrucial for several commercial applications. In this paper, we present a novel hybrid vectorization approach for textual resources that combines a weighted variant of the popular Word2Vec representation (based on Term Frequency-Inverse Document Frequency) representation and with a Bag- of-Words representation and a vector of lexicon-based sentiment values. The proposedtextrepresentation approach is assessed through the application of several machine learning classification algorithms on a dataset that is used extensively in literature for sentiment detection. The classification accuracy derived through the proposed hybrid vectorization approach is higher than when its individual components are used fortextrepresenation, and comparable with state-of-the-art sentiment detection methodologies.","['Panagiotis Stalidis', 'Maria Giatsoglou', 'Konstantinos Diamantaras', 'George Sarigiannidis', 'Konstantinos Ch. Chatzisavvas']",,arXiv,2015,https://doi.org/10.48550/arXiv.1511.09107,Anomali
POPmine: Tracking Political Opinion on the Web,"The automatic content analysis of mass media in the social sciences has become necessary and possible with the raise of social media and computational power. One particularly promising avenue of research concerns the use of opinionmining. We design and implement the POPmine system which is able to collecttextsfrom web-based conventional media (news items in mainstream media sites) and social media (blogs and Twitter) and to process thosetexts, recognizing topics and political actors, analyzing relevant linguistic units, and generating indicators of both frequency of mention and polarity (positivity/negativity) of mentions to political actors across sources, types of sources, and across time.","['Pedro Saleiro', 'Sílvio Amir', 'Mário J. Silva', 'Carlos Soares']",,arXiv,2015,https://doi.org/10.48550/arXiv.1511.09101,Anomali
Harvesting comparable corpora and mining them for equivalent bilingual sentences using statistical classification and analogy- based heuristics,"Parallel sentences are a relatively scarce but extremely useful resource for many applications including cross-lingual retrieval and statistical machine translation. This research explores our new methodologies forminingsuch data from previously obtained comparable corpora. The task is highly practical since non-parallel multilingual data exist in far greater quantities than parallel corpora, but parallel sentences are a much more useful resource. Here we propose a web crawling method for building subject-aligned comparable corpora from e.g. Wikipedia dumps and Euronews web page. The improvements in machine translation are shown on Polish-English language pair for varioustextdomains. We also tested another method of building parallel corpora based on comparable corpora data. It lets automatically broad existing corpus of sentences from subject of corpora based on analogies between them.","['Krzysztof Wołk', 'Emilia Rejmund', 'Krzysztof Marasek']",,arXiv,2015,https://doi.org/10.48550/arXiv.1511.06285,Anomali
Using Text Mining To Analyze Real Estate Classifieds,"Many brokers have adapted their operation to exploit the potential of the web. Despite the importance of the real estate classifieds, there has been little work in analyzing such data. In this paper we propose a two-stage regression model that exploits the textual data in real estate classifieds. We show how our model can be used to predict the price of a real estate classified. We also show how our model can be used to highlight keywords that affect the price positively or negatively. To assess our contributions, we analyze four real world data sets, which we gathered from three different property websites. The analysis shows that our model (which exploits textual features) achieves significantly lower root mean squared error across the different data sets and against variety of regression models.",['Sherief Abdallah'],,arXiv,2015,https://doi.org/10.48550/arXiv.1511.04674,Anomali
Bayesian Analysis of Dynamic Linear Topic Models,"In dynamic topic modeling, the proportional contribution of a topic to a document depends on the temporal dynamics of that topic's overall prevalence in the corpus. We extend the Dynamic Topic Model of Blei and Lafferty (2006) by explicitly modeling document level topic proportions with covariates and dynamic structure that includes polynomial trends and periodicity. A Markov Chain Monte Carlo (MCMC) algorithm that utilizes Polya-Gamma data augmentation is developed for posterior inference. Conditional independencies in the model and sampling are made explicit, and our MCMC algorithm is parallelized where possible to allow for inference in large corpora. To address computational bottlenecks associated with Polya-Gamma sampling, we appeal to the Central Limit Theorem to develop a Gaussian approximation to the Polya-Gamma random variable. This approximation is fast and reliable for parameter values relevant in thetextminingdomain. Our model and inference algorithm are validated with multiple simulation examples, and we consider the application of modeling trends in PubMed abstracts. We demonstrate that sharing information across documents is critical for accurately estimating document-specific topic proportions. We also show that explicitly modeling polynomial and periodic behavior improves our ability to predict topic prevalence at future time points.","['Chris Glynn', 'Surya T. Tokdar', 'David L. Banks', 'Brian Howard']",,arXiv,2015,https://doi.org/10.48550/arXiv.1511.03947,Anomali
Mining Local Gazetteers of Literary Chinese with CRF and Pattern based Methods for Biographical Information in Chinese History,"Person names and location names are essential building blocks for identifying events and social networks in historical documents that were written in literary Chinese. We take the lead to explore the research on algorithmically recognizing named entities in literary Chinese for historical studies with language-model based and conditional-random-field based methods, and extend our work tominingthe document structures in historical documents. Practical evaluations were conducted withtextsthat were extracted from more than 220 volumes of local gazetteers (Difangzhi). Difangzhi is a huge and the single most important collection that contains information about officers who served in local government in Chinese history. Our methods performed very well on these realistic tests. Thousands of names and addresses were identified from thetexts. A good portion of the extracted names match the biographical information currently recorded in the China Biographical Database (CBDB) of Harvard University, and many others can be verified by historians and will become as new additions to CBDB.","['Chao-Lin Liu', 'Chih-Kai Huang', 'Hongsu Wang', 'Peter K. Bol']",,arXiv,2015,https://doi.org/10.48550/arXiv.1511.01556,Anomali
Study of a bias in the offline evaluation of a recommendation algorithm,"Recommendation systems have been integrated into the majority of large online systems to filter and rank information according to user profiles. It thus influences the way users interact with the system and, as a consequence, bias the evaluation of the performance of a recommendation algorithm computed using historical data (via offline evaluation). This paper describes this bias and discuss the relevance of a weighted offline evaluation to reduce this bias for different classes of recommendation algorithms.","['Arnaud De Myttenaere', 'Boris Golden', 'Bénédicte Le Grand', 'Fabrice Rossi']","Petra Perner. 11th Industrial Conference on Data Mining, ICDM 2015, Jul 2015, Hamburg, Germany. Ibai Publishing, pp.57-70, 2015, Advances in Data Mining",arXiv,2015,https://doi.org/10.48550/arXiv.1511.01280,Anomali
Social Media Analysis for Product Safety using Text Mining and Sentiment Analysis,"The growing incidents of counterfeiting and associated economic and health consequences necessitate the development of active surveillance systems capable of producing timely and reliable information for all stake holders in the anti-counterfeiting fight. User generated content from social media platforms can provide early clues about product allergies, adverse events and product counterfeiting. This paper reports a work in progresswith contributions including: the development of a framework for gathering and analyzing the views and experiences of users of drug and cosmetic products using machine learning,textminingand sentiment analysis, the application of the proposed framework on Facebook comments and data from Twitter for brand analysis, and the description of how to develop a product safety lexicon and training data for modeling a machine learning classifier for drug and cosmetic product sentiment prediction. The initial brand and product comparison results signify the usefulness oftextminingand sentiment analysis on social media data while the use of machine learning classifier for predicting the sentiment orientation provides a useful tool for users, product manufacturers, regulatory and enforcement agencies to monitor brand or product sentiment trends in order to act in the event of sudden or significant rise in negative sentiment.","['Haruna Isah', 'Daniel Neagu', 'Paul Trundle']",,arXiv,2015,https://doi.org/10.48550/arXiv.1510.05301,Anomali
A Novel Approach to Document Classification using WordNet,Content based Document Classification is one of the biggest challenges in the context of freetextmining. Current algorithms on document classifications mostly rely on cluster analysis based on bag-of-words approach. However that method is still being applied to many modern scientific dilemmas. It has established a strong presence in fields like economics and social science to merit serious attention from the researchers. In this paper we would like to propose and explore an alternative grounded more securely on the dictionary classification and correlatedness of words and phrases. It is expected that application of our existing knowledge about the underlying classification structure may lead to improvement of the classifier's performance.,"['Koushiki Sarkar', 'Ritwika Law']",,arXiv,2015,https://doi.org/10.48550/arXiv.1510.02755,Anomali
Automatic Taxonomy Extraction from Query Logs with no Additional Sources of Information,"Search engine logs store detailed information on Web users interactions. Thus, as more and more people use search engines on a daily basis, important trails of users common knowledge are being recorded in those files. Previous research has shown that it is possible to extract concept taxonomies from fulltextdocuments, while other scholars have proposed methods to obtain similar queries from query logs. We propose a mixture of both lines of research, that is,miningquery logs not to find related queries nor query hierarchies, but actual term taxonomies that could be used to improve search engine effectiveness and efficiency. As a result, in this study we have developed a method that combines lexical heuristics with a supervised classification model to successfully extract hyponymy relations from specialization search patterns revealed from log missions, with no additional sources of information, and in a language independent way.","['Miguel Fernandez-Fernandez', 'Daniel Gayo-Avello']",,arXiv,2015,https://doi.org/10.48550/arXiv.1510.00618,Anomali
Using consumer behavior data to reduce energy consumption in smart homes,"This paper discusses how usage patterns and preferences of inhabitants can be learned efficiently to allow smart homes to autonomously achieve energy savings. We propose a frequent sequential patternminingalgorithm suitable for real-life smart home event data. The performance of the proposed algorithm is compared to existing algorithms regarding completeness/correctness of the results, run times as well as memory consumption and elaborates on the shortcomings of the different solutions. We also present a recommender system based on the developed algorithm that provides recommendations to the users to reduce their energy consumption. The recommender system was deployed to a set of test homes. The test participants rated the impact of the recommendations on their comfort. We used this feedback to adjust the system parameters and make it more accurate during a second test phase.","['Daniel Schweizer', 'Michael Zehnder', 'Holger Wache', 'Hans-Friedrich Witschel', 'Danilo Zanatta', 'Miguel Rodriguez']",,arXiv,2015,https://doi.org/10.48550/arXiv.1510.00165,Anomali
A High-Performance Parallel Algorithm for Nonnegative Matrix Factorization,"Non-negative matrix factorization (NMF) is the problem of determining two non-negative low rank factors $W$ and $H$, for the given input matrix $A$, such that $A \approx W H$. NMF is a useful tool for many applications in different domains such as topic modeling intextmining, background separation in video analysis, and community detection in social networks. Despite its popularity in the dataminingcommunity, there is a lack of efficient parallel software to solve the problem for big datasets. Existing distributed-memory algorithms are limited in terms of performance and applicability, as they are implemented using Hadoop and are designed only for sparse matrices.
  We propose a distributed-memory parallel algorithm that computes the factorization by iteratively solving alternating non-negative least squares (NLS) subproblems for $W$ and $H$. To our knowledge, our algorithm is the first high-performance parallel algorithm for NMF. It maintains the data and factor matrices in memory (distributed across processors), uses MPI for interprocessor communication, and, in the dense case, provably minimizes communication costs (under mild assumptions). As opposed to previous implementations, our algorithm is also flexible: (1) it performs well for dense and sparse matrices, and (2) it allows the user to choose from among multiple algorithms for solving local NLS subproblems within the alternating iterations. We demonstrate the scalability of our algorithm and compare it with baseline implementations, showing significant performance improvements.","['Ramakrishnan Kannan', 'Grey Ballard', 'Haesun Park']",,arXiv,2015,https://doi.org/10.48550/arXiv.1509.09313,Anomali
Tuned and GPU-accelerated parallel data mining from comparable corpora,"The multilingual nature of the world makes translation a crucial requirement today. Parallel dictionaries constructed by humans are a widely-available resource, but they are limited and do not provide enough coverage for good quality translation purposes, due to out-of-vocabulary words and neologisms. This motivates the use of statistical translation systems, which are unfortunately dependent on the quantity and quality of training data. Such has a very limited availability especially for some languages and very narrowtextdomains. Is this research we present our improvements to Yalignminingmethodology by reimplementing the comparison algorithm, introducing a tuning scripts and by improving performance using GPU computing acceleration. The experiments are conducted on varioustextdomains and bi-data is extracted from the Wikipedia dumps.","['Krzysztof Wołk', 'Krzysztof Marasek']","Lecture Notes in Artificial Intelligence, p. 32-40, ISBN: 978-3-319-24032-9, Springer, 2015",arXiv,2015,https://doi.org/10.48550/arXiv.1509.08639,Anomali
LEWIS: Latent Embeddings for Word Images and their Semantics,"The goal of this work is to bring semantics into the tasks oftextrecognition and retrieval in natural images. Althoughtextrecognition and retrieval have received a lot of attention in recent years, previous works have focused on recognizing or retrieving exactly the same word used as a query, without taking the semantics into consideration.
  In this paper, we ask the following question: \emph{can we predict semantic concepts directly from a word image, without explicitly trying to transcribe the word image or its characters at any point?} For this goal we propose a convolutional neural network (CNN) with a weighted ranking loss objective that ensures that the concepts relevant to the query image are ranked ahead of those that are not relevant. This can also be interpreted as learning a Euclidean space where word images and concepts are jointly embedded. This model is learned in an end-to-end manner, from image pixels to semantic concepts, using a dataset of synthetically generated word images and conceptsminedfrom a lexical database (WordNet). Our results show that, despite the complexity of the task, word images and concepts can indeed be associated with a high degree of accuracy","['Albert Gordo', 'Jon Almazan', 'Naila Murray', 'Florent Perronnin']",,arXiv,2015,https://doi.org/10.48550/arXiv.1509.06243,Anomali
Discovery of Web Usage Profiles Using Various Clustering Techniques,"The explosive growth of World Wide Web (WWW) has necessitated the development of Web personalization systems in order to understand the user preferences to dynamically serve customized content to individual users. To reveal information about user preferences from Web usage data, Web UsageMining(WUM) techniques are extensively being applied to the Web log data. Clustering techniques are widely used in WUM to capture similar interests and trends among users accessing a Web site. Clustering aims to divide a data set into groups or clusters where inter-cluster similarities are minimized while the intra cluster similarities are maximized. This paper reviews four of the popularly used clustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniques are implemented and tested against the Web user navigational data. Performance and validity results of each technique are presented and compared.","['Zahid Ansari', 'Waseem Ahmed', 'M. F. Azeem', 'A. Vinaya Babu']","International Journal of Computer Information Systems, pp. 18-27 Vol. 1, No. 3, July 2011. (ISSN 2229-5208, Silicon Valley Publishers, United Kingdom)",arXiv,2015,https://doi.org/10.48550/arXiv.1509.00692,Anomali
Non-standard Nonstandard Analysis and the computational content of standard mathematics,"The aim of this paper is to highlight a hitherto unknown computational aspect of Nonstandard Analysis. Recently, a number of nonstandard versions of Goedel's system T have been introduced ([2,9,12]), and it was shown in [26] that the systems from [2] play a pivotal role in extracting computational information from proofs in Nonstandard Analysis. It is a natural question if similar techniques may be used to extract computational information from proofs not involving Nonstandard Analysis. In this paper, we provide a positive answer to this question using the nonstandard system from [9]. This system validates so-called non-standard uniform boundedness principles which are central to Kohlenbach's approach to proofmining([14]). In particular, we show that from classical and ineffective existence proofs (not involving Nonstandard Analysis but using weak Koenig's lemma), one can `automatically' extract approximations to the objects claimed to exist.",['Sam Sanders'],,arXiv,2015,https://doi.org/10.48550/arXiv.1509.00282,Anomali
Simple Text Mining for Sentiment Analysis of Political Figure Using Naive Bayes Classifier Method,Textminingcan be applied to many fields. One of the application is usingtextminingin digital newspaper to do politic sentiment analysis. In this paper sentiment analysis is applied to get information from digital news articles about its positive or negative sentiment regarding particular politician. This paper suggests a simple model to analyze digital newspaper sentiment polarity using naive Bayes classifier method. The model uses a set of initial data to begin with which will be updated when new information appears. The model showed promising result when tested and can be implemented to some other sentiment analysis problems.,"['Yustinus Eko Soelistio', 'Martinus Raditia Sigit Surendra']",,arXiv,2015,https://doi.org/10.48550/arXiv.1508.05163,Anomali
Social Influence in the Concurrent Diffusion of Information and Behaviors in Online Social Networks,"The emergence of online social networks has greatly facilitated the diffusion of information and behaviors. While the two diffusion processes are often intertwined, ""talking the talk"" does not necessarily mean ""walking the talk""--those who share information about an action may not actually participate in it. We do not know if the diffusion of information and behaviors are similar, or if social influence plays an equally important role in these processes. Integratingtextmining, social network analyses, and survival analysis, this research examines the concurrent spread of information and behaviors related to the Ice Bucket Challenge on Twitter. We show that the two processes follow different patterns. Unilateral social influence contributes to the diffusion of information, but not to the diffusion of behaviors; bilateral influence conveyed via the communication process is a significant and positive predictor of the diffusion of behaviors, but not of information. These results have implications for theories of social influence, social networks, and contagion.","['Kang Zhao', 'Shiyao Wang', 'Ion B. Vasi', 'Qi Zhang']",,arXiv,2018,https://doi.org/10.48550/arXiv.1508.04417,Anomali
"Probabilistic, statistical and algorithmic aspects of the similarity of texts and application to Gospels comparison","The fundamental problem of similarity studies, in the frame of data-mining, is to examine and detect similar items in articles, papers, books, with huge sizes. In this paper, we are interested in the probabilistic, and the statistical and the algorithmic aspects in studies oftexts. We will be using the approach of $k$\textit{-shinglings}, a $k$\textit{-shingling} being defined as a sequence of $k$ consecutive characters that are extracted from atext($k\geq 1$ ). The main stake in this field is to find accurate and quick algorithms to compute the similarity in short times. This will be achieved in using approximation methods. The first approximation method is statistical and, is based on the theorem of Glivenko-Cantelli. The second is the banding technique. And the third concerns a modification of the algorithm proposed by Rajaraman and al (% \cite{AnandJeffrey}), denoted here as (RUM). The Jaccard index is the one used in this paper. We finally illustrate these results of the paper on the four Gospels. The results are very conclusive.","['Gane Samb Lo', 'Soumaila Dembele']",,arXiv,2015,https://doi.org/10.48550/arXiv.1508.03772,Anomali
Space-efficient detection of unusual words,"Detecting all the strings that occur in atextmore frequently or less frequently than expected according to an IID or a Markov model is a basic problem in stringmining, yet current algorithms are based on data structures that are either space-inefficient or incur large slowdowns, and current implementations cannot scale to genomes or metagenomes in practice. In this paper we engineer an algorithm based on the suffix tree of a string to use just a small data structure built on the Burrows-Wheeler transform, and a stack of $O(σ^2\log^2 n)$ bits, where $n$ is the length of the string and $σ$ is the size of the alphabet. The size of the stack is $o(n)$ except for very large values of $σ$. We further improve the algorithm by removing its time dependency on $σ$, by reporting only a subset of the maximal repeats and of the minimal rare words of the string, and by detecting and scoring candidate under-represented strings that $\textit{do not occur}$ in the string. Our algorithms are practical and work directly on the BWT, thus they can be immediately applied to a number of existing datasets that are available in this form, returning this stringminingproblem to a manageable scale.","['Djamal Belazzougui', 'Fabio Cunial']",,arXiv,2015,https://doi.org/10.48550/arXiv.1508.02968,Anomali
Improving Decision Analytics with Deep Learning: The Case of Financial Disclosures,"Decision analytics commonly focuses on thetextminingof financial news sources in order to provide managerial decision support and to predict stock market movements. Existing predictive frameworks almost exclusively apply traditional machine learning methods, whereas recent research indicates that traditional machine learning methods are not sufficiently capable of extracting suitable features and capturing the non-linear nature of complex tasks. As a remedy, novel deep learning models aim to overcome this issue by extending traditional neural network models with additional hidden layers. Indeed, deep learning has been shown to outperform traditional methods in terms of predictive performance. In this paper, we adapt the novel deep learning technique to financial decision support. In this instance, we aim to predict the direction of stock movements following financial disclosures. As a result, we show how deep learning can outperform the accuracy of random forests as a benchmark for machine learning by 5.66%.","['Stefan Feuerriegel', 'Ralph Fehrer']","Twenty-Fourth European Conference on Information Systems (ECIS 2016), Istanbul, Turkey, 2016",arXiv,2018,https://doi.org/10.48550/arXiv.1508.01993,Anomali
Tag-Weighted Topic Model For Large-scale Semi-Structured Documents,"To date, there have been massive Semi-Structured Documents (SSDs) during the evolution of the Internet. These SSDs contain both unstructured features (e.g., plaintext) and metadata (e.g., tags). Most previous works focused on modeling the unstructuredtext, and recently, some other methods have been proposed to model the unstructuredtextwith specific tags. To build a general model for SSDs remains an important problem in terms of both model fitness and efficiency. We propose a novel method to model the SSDs by a so-called Tag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the tags and words information, not only to learn the document-topic and topic-word distributions, but also to infer the tag-topic distributions fortextminingtasks. We present an efficient variational inference method with an EM algorithm for estimating the model parameters. Meanwhile, we propose three large-scale solutions for our model under the MapReduce distributed computing platform for modeling large-scale SSDs. The experimental results show the effectiveness, efficiency and the robustness by comparing our model with the state-of-the-art methods in document modeling, tags prediction andtextclassification. We also show the performance of the three distributed solutions in terms of time and accuracy on document modeling.","['Shuangyin Li', 'Jiefei Li', 'Guan Huang', 'Ruiyang Tan', 'Rong Pan']",,arXiv,2015,https://doi.org/10.48550/arXiv.1507.08396,Anomali
Data Mining of Causal Relations from Text: Analysing Maritime Accident Investigation Reports,"Textminingis a process of extracting information of interest fromtext. Such a method includes techniques from various areas such as Information Retrieval (IR), Natural Language Processing (NLP), and Information Extraction (IE). In this study,textminingmethods are applied to extract causal relations from maritime accident investigation reports collected from the Marine Accident Investigation Branch (MAIB). These causal relations provide information on various mechanisms behind accidents, including human and organizational factors relating to the accident. The objective of this study is to facilitate the analysis of the maritime accident investigation reports, by means of extracting contributory causes with more feasibility. A careful investigation of contributory causes from the reports provide opportunity to improve safety in future.
  Two methods have been employed in this study to extract the causal relations. They are 1) Pattern classification method and 2) Connectives method. The earlier one uses naive Bayes and Support Vector Machines (SVM) as classifiers. The latter simply searches for the words connecting cause and effect in sentences.
  The causal patterns extracted using these two methods are compared to the manual (human expert) extraction. The pattern classification method showed a fair and sensible performance with F-measure(average) = 65% when compared to connectives method with F-measure(average) = 58%. This study is an evidence, thattextminingmethods could be employed in extracting causal relations from marine accident investigation reports.",['Santosh Tirunagari'],,arXiv,2015,https://doi.org/10.48550/arXiv.1507.02447,Anomali
Correspondence Factor Analysis of Big Data Sets: A Case Study of 30 Million Words; and Contrasting Analytics using Apache Solr and Correspondence Analysis in R,"We consider a large number oftextdata sets. These are cooking recipes. Term distribution and other distributional properties of the data are investigated. Our aim is to look at various analytical approaches which allow forminingof information on both high and low detail scales. Metric space embedding is fundamental to our interest in the semantic properties of this data. We consider the projection of all data into analyses of aggregated versions of the data. We contrast that with projection of aggregated versions of the data into analyses of all the data. Analogously for the term set, we look at analysis of selected terms. We also look at inherent term associations such as between singular and plural. In addition to our use of Correspondence Analysis in R, for latent semantic space mapping, we also use Apache Solr. Setting up the Solr server and carrying out querying is described. A further novelty is that querying is supported in Solr based on the principal factor plane mapping of all the data. This uses a bounding box query, based on factor projections.",['Fionn Murtagh'],,arXiv,2015,https://doi.org/10.48550/arXiv.1507.01529,Anomali
How to improve robustness in Kohonen maps and display additional information in Factorial Analysis: application to text mining,"This article is an extended version of a paper presented in the WSOM'2012 conference [1]. We display a combination of factorial projections, SOM algorithm and graph techniques applied to atextminingproblem. The corpus contains 8 medieval manuscripts which were used to teach arithmetic techniques to merchants. Among the techniques for Data Analysis, those used for Lexicometry (such as Factorial Analysis) highlight the discrepancies between manuscripts. The reason for this is that they focus on the deviation from the independence between words and manuscripts. Still, we also want to discover and characterize the common vocabulary among the whole corpus. Using the properties of stochastic Kohonen maps, which define neighborhood between inputs in a non-deterministic way, we highlight the words which seem to play a special role in the vocabulary. We call them fickle and use them to improve both Kohonen map robustness and significance of FCA visualization. Finally we use graph algorithmic to exploit this fickleness for classification of words.","['Nicolas Bourgeois', 'Marie Cottrell', 'Benjamin Déruelle', 'Stéphane Lamassé', 'Patrick Letrémy']","Neurocomputing, Elsevier, 2014, 147, pp.120-135",arXiv,2015,https://doi.org/10.48550/arXiv.1506.07732,Anomali
Scientific Discovery by Machine Intelligence: A New Avenue for Drug Research,"The majority of big data is unstructured and of this majority the largest chunk istext. While dataminingtechniques are well developed and standardized for structured, numerical data, the realm of unstructured data is still largely unexplored. The general focus lies on information extraction, which attempts to retrieve known information fromtext. The Holy Grail, however is knowledge discovery, where machines are expected to unearth entirely new facts and relations that were not previously known by any human expert. Indeed, understanding the meaning oftextis often considered as one of the main characteristics of human intelligence. The ultimate goal of semantic artificial intelligence is to devise software that can understand the meaning of freetext, at least in the practical sense of providing new, actionable information condensed out of a body of documents. As a stepping stone on the road to this vision I will introduce a totally new approach to drug research, namely that of identifying relevant information by employing a self-organizing semantic engine totextminelarge repositories of biomedical research papers, a technique pioneered by Merck with the InfoCodex software. I will describe the methodology and a first successful experiment for the discovery of new biomarkers and phenotypes for diabetes and obesity on the basis of PubMed abstracts, public clinical trials and Merck internal documents. The reported approach shows much promise and has potential to impact fundamentally pharmaceutical research as a way to shorten time-to-market of novel drugs, and for early recognition of dead ends.",['Carlo A. Trugenberger'],,arXiv,2015,https://doi.org/10.48550/arXiv.1506.07116,Anomali
Extreme Extraction: Only One Hour per Relation,"Information Extraction (IE) aims to automatically generate a large knowledge base from natural languagetext, but progress remains slow. Supervised learning requires copious human annotation, while unsupervised and weakly supervised approaches do not deliver competitive accuracy. As a result, most fielded applications of IE, as well as the leading TAC-KBP systems, rely on significant amounts of manual engineering. Even ""Extreme"" methods, such as those reported in Freedman et al. 2011, require about 10 hours of expert labor per relation.
  This paper shows how to reduce that effort by an order of magnitude. We present a novel system, InstaRead, that streamlines authoring with an ensemble of methods: 1) encoding extraction rules in an expressive and compositional representation, 2) guiding the user to promising rules based on corpus statistics andminedresources, and 3) introducing a new interactive development cycle that provides immediate feedback --- even on large datasets. Experiments show that experts can create quality extractors in under an hour and even NLP novices can author good extractors. These extractors equal or outperform ones obtained by comparably supervised and state-of-the-art distantly supervised approaches.","['Raphael Hoffmann', 'Luke Zettlemoyer', 'Daniel S. Weld']",,arXiv,2015,https://doi.org/10.48550/arXiv.1506.06418,Anomali
Editorial for the First Workshop on Mining Scientific Papers: Computational Linguistics and Bibliometrics,"The workshop ""MiningScientific Papers: Computational Linguistics and Bibliometrics"" (CLBib 2015), co-located with the 15th International Society of Scientometrics and Informetrics Conference (ISSI 2015), brought together researchers in Bibliometrics and Computational Linguistics in order to study the ways Bibliometrics can benefit from large-scaletextanalytics and senseminingof scientific papers, thus exploring the interdisciplinarity of Bibliometrics and Natural Language Processing (NLP). The goals of the workshop were to answer questions like: How can we enhance author network analysis and Bibliometrics using data obtained bytextanalytics? What insights can NLP provide on the structure of scientific writing, on citation networks, and on in-textcitation analysis? This workshop is the first step to foster the reflection on the interdisciplinarity and the benefits that the two disciplines Bibliometrics and Natural Language Processing can drive from it.","['Iana Atanassova', 'Marc Bertin', 'Philipp Mayr']",,arXiv,2015,https://doi.org/10.48550/arXiv.1506.05402,Anomali
"Early Predictions of Movie Success: the Who, What, and When of Profitability","This paper proposes a decision support system to aid movie investment decisions at the early stage of movie productions. The system predicts the success of a movie based on its profitability by leveraging historical data from various sources. Using social network analysis andtextminingtechniques, the system automatically extracts several groups of features, including ""who"" are on the cast, ""what"" a movie is about, ""when"" a movie will be released, as well as ""hybrid"" features that match ""who"" with ""what"", and ""when"" with ""what"". Experiment results with movies during an 11-year period showed that the system outperforms benchmark methods by a large margin in predicting movie profitability. Novel features we proposed also made great contributions to the prediction. In addition to designing a decision support system with practical utilities, our analysis of key factors for movie profitability may also have implications for theoretical research on team performance and the success of creative work.","['Michael T. Lash', 'Kang Zhao']",,arXiv,2016,https://doi.org/10.48550/arXiv.1506.05382,Anomali
A community role approach to assess social capitalists visibility in the Twitter network,"In the context of Twitter, social capitalists are specific users trying to increase their number of followers and interactions by any means. These users are not healthy for the service, because they are either spammers or real users flawing the notions of influence and visibility. Studying their behavior and understanding their position in Twit-ter is thus of important interest. It is also necessary to analyze how these methods effectively affect user visibility. Based on a recently proposed method allowing to identify social capitalists, we tackle both points by studying how they are organized, and how their links spread across the Twitter follower-followee network. To that aim, we consider their position in the network w.r.t. its community structure. We use the concept of community role of a node, which describes its position in a network depending on its connectiv-ity at the community level. However, the topological measures originally defined to characterize these roles consider only certain aspects of the community-related connectivity, and rely on a set of empirically fixed thresholds. We first show the limitations of these measures, before extending and generalizing them. Moreover, we use an unsupervised approach to identify the roles, in order to provide more flexibility relatively to the studied system. We then apply our method to the case of social capitalists and show they are highly visible on Twitter, due to the specific roles they hold.","['Nicolas Dugué', 'Vincent Labatut', 'Anthony Perez']","Social Network Analysis and Mining, Springer, 2015, 5, pp. 26",arXiv,2015,https://doi.org/10.48550/arXiv.1506.04571,Anomali
Idioms-Proverbs Lexicon for Modern Standard Arabic and Colloquial Sentiment Analysis,"Although, the fair amount of works in sentiment analysis (SA) and opinionmining(OM) systems in the last decade and with respect to the performance of these systems, but it still not desired performance, especially for morphologically-Rich Language (MRL) such as Arabic, due to the complexities and challenges exist in the nature of the languages itself. One of these challenges is the detection of idioms or proverbs phrases within the writertextor comment. An idiom or proverb is a form of speech or an expression that is peculiar to itself. Grammatically, it cannot be understood from the individual meanings of its elements and can yield different sentiment when treats as separate words. Consequently, In order to facilitate the task of detection and classification of lexical phrases for automated SA systems, this paper presents AIPSeLEX a novel idioms/ proverbs sentiment lexicon for modern standard Arabic (MSA) and colloquial. AIPSeLEX is manually collected and annotated at sentence level with semantic orientation (positive or negative). The efforts of manually building and annotating the lexicon are reported. Moreover, we build a classifier that extracts idioms and proverbs, phrases fromtextusing n-gram and similarity measure methods. Finally, several experiments were carried out on various data, including Arabic tweets and Arabic microblogs (hotel reservation, product reviews, and TV program comments) from publicly available Arabic online reviews websites (social media, blogs, forums, e-commerce web sites) to evaluate the coverage and accuracy of AIPSeLEX.","['Hossam S. Ibrahim', 'Sherif M. Abdou', 'Mervat Gheith']","International Journal of Computer Applications 118(11):26-31, May 2015",arXiv,2015,https://doi.org/10.48550/arXiv.1506.01906,Anomali
Colors $-$Messengers of Concepts: Visual Design Mining for Learning Color Semantics,"This paper studies the concept of color semantics by modeling a dataset of magazine cover designs, evaluating the model via crowdsourcing, and demonstrating several prototypes that facilitate color-related design tasks. We investigate a probabilistic generative modeling framework that expresses semantic concepts as a combination of color and word distributions $-$color-word topics. We adopt an extension to Latent Dirichlet Allocation (LDA) topic modeling called LDA-dual to infer a set of color-word topics over a corpus of 2,654 magazine covers spanning 71 distinct titles and 12 genres. While LDA modelstextdocuments as distributions over word topics, we model magazine covers as distributions over color-word topics. The results of our crowdsourced experiments confirm that the model is able to successfully discover the associations between colors and linguistic concepts. Finally, we demonstrate several simple prototypes that apply the learned model to color palette recommendation, design example retrieval, image retrieval, image color selection, and image recoloring.","['Ali Jahanian', 'S. V. N. Vishwanathan', 'Jan P. Allebach']",,arXiv,2015,https://doi.org/10.48550/arXiv.1505.06532,Anomali
Mining Scientific Papers for Bibliometrics: a (very) Brief Survey of Methods and Tools,"The Open Access movement in scientific publishing and search engines like Google Scholar have made scientific articles more broadly accessible. During the last decade, the availability of scientific papers in fulltexthas become more and more widespread thanks to the growing number of publications on online platforms such as ArXiv and CiteSeer. The efforts to provide articles in machine-readable formats and the rise of Open Access publishing have resulted in a number of standardized formats for scientific papers (such as NLM-JATS, TEI, DocBook). Our aim is to stimulate research at the intersection of Bibliometrics and Computational Linguistics in order to study the ways Bibliometrics can benefit from large-scaletextanalytics and senseminingof scientific papers, thus exploring the interdisciplinarity of Bibliometrics and Natural Language Processing.","['Iana Atanassova', 'Marc Bertin', 'Philipp Mayr']",,arXiv,2015,https://doi.org/10.48550/arXiv.1505.01393,Anomali
Mining Measured Information from Text,"We present an approach to extract measured information fromtext(e.g., a 1370 degrees C melting point, a BMI greater than 29.9 kg/m^2 ). Such extractions are critically important across a wide range of domains - especially those involving search and exploration of scientific and technical documents. We first propose a rule-based entity extractor tominemeasured quantities (i.e., a numeric value paired with a measurement unit), which supports a vast and comprehensive set of both common and obscure measurement units. Our method is highly robust and can correctly recover valid measured quantities even when significant errors are introduced through the process of converting document formats like PDF to plaintext. Next, we describe an approach to extracting the properties being measured (e.g., the property ""pixel pitch"" in the phrase ""a pixel pitch as high as 352 μm""). Finally, we present MQSearch: the realization of a search engine with full support for measured information.","['Arun S. Maiya', 'Dale Visser', 'Andrew Wan']",,arXiv,2015,https://doi.org/10.48550/arXiv.1505.01072,Anomali
Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation,"Despite tremendous progress in computer vision, there has not been an attempt for machine learning on very large-scale medical image databases. We present an interleavedtext/image deep learning system to extract andminethe semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, weminea collection of representative ~216K two-dimensional key images selected by clinicians for diagnostic reference, and match the images with their descriptions in an automated manner. Our system interleaves between unsupervised learning and supervised learning on document- and sentence-leveltextcollections, to generate semantic labels and to predict them given an image. Given an image of a patient scan, semantic topics in radiology levels are predicted, and associated key-words are generated. Also, a number of frequent disease types are detected as present or absent, to provide more specific interpretation of a patient scan. This shows the potential of large-scale learning and prediction in electronic patient records available in most modern clinical institutions.","['Hoo-Chang Shin', 'Le Lu', 'Lauren Kim', 'Ari Seff', 'Jianhua Yao', 'Ronald M. Summers']",,arXiv,2015,https://doi.org/10.48550/arXiv.1505.00670,Anomali
Open Data Platform for Knowledge Access in Plant Health Domain : VESPA Mining,"Important data are locked in ancient literature. It would be uneconomic to produce these data again and today or to extract them without the help oftextminingtechnologies. Vespa is atextminingproject whose aim is to extract data on pest and crops interactions, to model and predict attacks on crops, and to reduce the use of pesticides. A few attempts proposed an agricultural information access. Another originality of our work is to parse documents with a dependency of the document architecture.","['Nicolas Turenne', 'Mathieu Andro', 'Roselyne Corbière', 'Tien T. Phan']",,arXiv,2015,https://doi.org/10.48550/arXiv.1504.06077,Anomali
A new generation of science overlay maps with an application to the history of biosystematics,"The paper proposes atext-miningbased analytical framework aiming at the cognitive organization of complex scientific discourses. The approach is based on models recently developed in science mapping, being a generalization of the so-called Science Overlay Mapping methodology, referred to as Topic Overlay Mapping (TOM). It is shown that via applications of TOM in visualization, document clustering, time series analysis etc. the in-depth exploration and even the measurement of cognitive complexity and its dynamics is feasible for scientific domains. As a use case, an empirical study is presented into the discovery of a long-standing complex, interdisciplinary discourse, the debate on the species concept in biosystematics.",['Sandor Soos'],,arXiv,2015,https://doi.org/10.48550/arXiv.1504.05816,Anomali
Mining and discovering biographical information in Difangzhi with a language-model-based approach,"We present results of expanding the contents of the China Biographical Database bytextmininghistorical local gazetteers, difangzhi. The goal of the database is to see how people are connected together, through kinship, social connections, and the places and offices in which they served. The gazetteers are the single most important collection of names and offices covering the Song through Qing periods. Although we begin with local officials we shall eventually include lists of local examination candidates, people from the locality who served in government, and notable local figures with biographies. The more data we collect the more connections emerge. The value of doing systematictextminingwork is that we can identify relevant connections that are either directly informative or can become useful without deep historical research. Academia Sinica is developing a name database for officials in the central governments of the Ming and Qing dynasties.","['Peter K. Bol', 'Chao-Lin Liu', 'Hongsu Wang']",,arXiv,2015,https://doi.org/10.48550/arXiv.1504.02148,Anomali
Infinite Author Topic Model based on Mixed Gamma-Negative Binomial Process,"Incorporating the side information oftextcorpus, i.e., authors, time stamps, and emotional tags, into the traditionaltextminingmodels has gained significant interests in the area of information retrieval, statistical natural language processing, and machine learning. One branch of these works is the so-called Author Topic Model (ATM), which incorporates the authors's interests as side information into the classical topic model. However, the existing ATM needs to predefine the number of topics, which is difficult and inappropriate in many real-world settings. In this paper, we propose an Infinite Author Topic (IAT) model to resolve this issue. Instead of assigning a discrete probability on fixed number of topics, we use a stochastic process to determine the number of topics from the data itself. To be specific, we extend a gamma-negative binomial process to three levels in order to capture the author-document-keyword hierarchical structure. Furthermore, each document is assigned a mixed gamma process that accounts for the multi-author's contribution towards this document. An efficient Gibbs sampling inference algorithm with each conditional distribution being closed-form is developed for the IAT model. Experiments on several real-world datasets show the capabilities of our IAT model to learn the hidden topics, authors' interests on these topics and the number of topics simultaneously.","['Junyu Xuan', 'Jie Lu', 'Guangquan Zhang', 'Richard Yi Da Xu', 'Xiangfeng Luo']",,arXiv,2015,https://doi.org/10.48550/arXiv.1503.08535,Anomali
Measuring robustness of community structure in complex networks,"The theory of community structure is a powerful tool for real networks, which can simplify their topological and functional analysis considerably. However, since community detection methods have random factors and real social networks obtained from complex systems always contain error edges, evaluating the robustness of community structure is an urgent and important task. In this letter, we employ the critical threshold of resolution parameter in Hamiltonian function, $γ_C$, to measure the robustness of a network. According to spectral theory, a rigorous proof shows that the index we proposed is inversely proportional to robustness of community structure. Furthermore, by utilizing the co-evolution model, we provides a new efficient method for computing the value of $γ_C$. The research can be applied to broad clustering problems in network analysis and dataminingdue to its solid mathematical basis and experimental effects.","['Hui-Jia Li', 'Hao Wang', 'Luonan Chen']","EPL (Europhysics Letters),108(6),2015",arXiv,2015,https://doi.org/10.48550/arXiv.1503.08012,Anomali
Construction of FuzzyFind Dictionary using Golay Coding Transformation for Searching Applications,"Searching through a large volume of data is very critical for companies, scientists, and searching engines applications due to time complexity and memory complexity. In this paper, a new technique of generating FuzzyFind Dictionary fortextminingwas introduced. We simply mapped the 23 bits of the English alphabet into a FuzzyFind Dictionary or more than 23 bits by using more FuzzyFind Dictionary, and reflecting the presence or absence of particular letters. This representation preserves closeness of word distortions in terms of closeness of the created binary vectors within Hamming distance of 2 deviations. This paper talks about the Golay Coding Transformation Hash Table and how it can be used on a FuzzyFind Dictionary as a new technology for using in searching through big data. This method is introduced by linear time complexity for generating the dictionary and constant time complexity to access the data and update by new data sets, also updating for new data sets is linear time depends on new data points. This technique is based on searching only for letters of English that each segment has 23 bits, and also we have more than 23-bit and also it could work with more segments as reference table.","['Kamran Kowsari', 'Maryam Yammahi', 'Nima Bari', 'Roman Vichr', 'Faisal Alsaby', 'Simon Y. Berkovich']",,arXiv,2015,https://doi.org/10.48550/arXiv.1503.06483,Anomali
Interpretable Aircraft Engine Diagnostic via Expert Indicator Aggregation,"Detecting early signs of failures (anomalies) in complex systems is one of the main goal of preventive maintenance. It allows in particular to avoid actual failures by (re)scheduling maintenance operations in a way that optimizes maintenance costs. Aircraft engine health monitoring is one representative example of a field in which anomaly detection is crucial. Manufacturers collect large amount of engine related data during flights which are used, among other applications, to detect anomalies.  This article introduces and studies a generic methodology that allows one to build automatic early signs of anomaly detection in a way that builds upon human expertise and that remains understandable by human operators who make the final maintenance decision. The main idea of the method is to generate a very large number of binary indicators based on parametric anomaly scores designed by experts, complemented by simple aggregations of those scores. A feature selection method is used to keep only the most discriminant indicators which are used as inputs of a Naive Bayes classifier. This give an interpretable classifier based on interpretable anomaly detectors whose parameters have been optimized indirectly by the selection process. The proposed methodology is evaluated on simulated data designed to reproduce some of the anomaly types observed in real world engines.","['Tsirizo Rabenoro', 'Jérôme Lacaille', 'Marie Cottrell', 'Fabrice Rossi']","Transactions on Machine Learning and Data Mining, 2014, 7 (2), pp.39-64",arXiv,2015,https://doi.org/10.48550/arXiv.1503.05526,Anomali
Fusing Text and Image for Event Detection in Twitter,"In this contribution, we develop an accurate and effective event detection method to detect events from a Twitter stream, which uses visual and textual information to improve the performance of theminingprocess. The method monitors a Twitter stream to pick up tweets havingtextsand images and stores them into a database. This is followed by applying aminingalgorithm to detect an event. The procedure starts with detecting events based ontextonly by using the feature of the bag-of-words which is calculated using the term frequency-inverse document frequency (TF-IDF) method. Then it detects the event based on image only by using visual features including histogram of oriented gradients (HOG) descriptors, grey-level cooccurrence matrix (GLCM), and color histogram. K nearest neighbours (Knn) classification is used in the detection. The final decision of the event detection is made based on the reliabilities oftextonly detection and image only detection. The experiment result showed that the proposed method achieved high accuracy of 0.94, comparing with 0.89 withtextsonly, and 0.86 with images only.","['Samar M. Alqhtani', 'Suhuai Luo', 'Brian Regan']",,arXiv,2015,https://doi.org/10.48550/arXiv.1503.03920,Anomali
Experimental Estimation of Number of Clusters Based on Cluster Quality,"TextClustering is atextminingtechnique which divides the given set oftextdocuments into significant clusters. It is used for organizing a huge number oftextdocuments into a well-organized form. In the majority of the clustering algorithms, the number of clusters must be specified apriori, which is a drawback of these algorithms. The aim of this paper is to show experimentally how to determine the number of clusters based on cluster quality. Since partitional clustering algorithms are well-suited for clustering large document datasets, we have confined our analysis to a partitional clustering algorithm.","['G. Hannah Grace', 'Kalyani Desikan']","Journal of mathematics and computer science, Vol12 (2014), 304-315",arXiv,2015,https://doi.org/10.48550/arXiv.1503.03168,Anomali
Syntax-based Deep Matching of Short Texts,"Many tasks in natural language processing, ranging from machine translation to question answering, can be reduced to the problem of matching two sentences or more generally two shorttexts. We propose a new approach to the problem, called Deep Match Tree (DeepMatch$_{tree}$), under a general setting. The approach consists of two components, 1) aminingalgorithm to discover patterns for matching two short-texts, defined in the product space of dependency trees, and 2) a deep neural network for matching shorttextsusing theminedpatterns, as well as a learning algorithm to build the network having a sparse structure. We test our algorithm on the problem of matching a tweet and a response in social media, a hard matching problem proposed in [Wang et al., 2013], and show that DeepMatch$_{tree}$ can outperform a number of competitor models including one without using dependency trees and one based on word-embedding, all with large margins","['Mingxuan Wang', 'Zhengdong Lu', 'Hang Li', 'Qun Liu']",,arXiv,2015,https://doi.org/10.48550/arXiv.1503.02427,Anomali
A Review of Relational Machine Learning for Knowledge Graphs,"Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be ""trained"" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based onminingobservable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined withtext-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.","['Maximilian Nickel', 'Kevin Murphy', 'Volker Tresp', 'Evgeniy Gabrilovich']",,arXiv,2015,https://doi.org/10.48550/arXiv.1503.00759,Anomali
Novel Metaknowledge-based Processing Technique for Multimedia Big Data clustering challenges,"Past research has challenged us with the task of showing relational patterns betweentext-based data and then clustering for predictive analysis using Golay Code technique. We focus on a novel approach to extract metaknowledge in multimedia datasets. Our collaboration has been an on-going task of studying the relational patterns between datapoints based on metafeatures extracted from metaknowledge in multimedia datasets. Those selected are significant to suit theminingtechnique we applied, Golay Code algorithm. In this research paper we summarize findings in optimization of metaknowledge representation for 23-bit representation of structured and unstructured multimedia data in order to","['Nima Bari', 'Roman Vichr', 'Kamran Kowsari', 'Simon Y. Berkovich']",,arXiv,2015,https://doi.org/10.48550/arXiv.1503.00245,Anomali
A framework to discover potential ideas of new product development from crowdsourcing application,"In this paper, we study ideaminingfrom crowdsourcing applications which encourage a group of people, who are usually undefined and very large sized, to generate ideas for new product development (NPD). In order to isolate the relatively small number of potential ones among ideas from crowd, decision makers not only have to identify the key textual information representing the ideas, but they also need to consider online opinions of people who gave comments and votes on the ideas. Due to the extremely large size oftextdata generated by people on the Internet, identifying textual information has been carried out in manual ways, and has been considered very time consuming and costly. To overcome the ineffectiveness, this paper introduces a novel framework that can help decision makers discover ideas having the potential to be used in an NPD process. To achieve this, a semi-automatictextminingtechnique that retrieves usefultextpatterns from ideas posted on crowdsourcing application is proposed. Then, we provide an online learning algorithm to evaluate whether the idea is potential or not. Finally to verify the effectiveness of our algorithm, we conducted experiments on the data, which are collected from an existing crowd sourcing website.","['Thanh-Cong Dinh', 'Hyerim Bae', 'Jaehun Park', 'Joonsoo Bae']",,arXiv,2015,https://doi.org/10.48550/arXiv.1502.07015,Anomali
Unified vector space mapping for knowledge representation systems,"One of the most significant problems which inhibits further developments in the areas of Knowledge Representation and Artificial Intelligence is a problem of semantic alignment or knowledge mapping. The progress in its solution will be greatly beneficial for further advances of information retrieval, ontology alignment, relevance calculation,textmining, natural language processing etc. In the paper the concept of multidimensional global knowledge map, elaborated through unsupervised extraction of dependencies from large documents corpus, is proposed. In addition, the problem of direct Human - Knowledge Representation System interface is addressed and a concept of adaptive decoder proposed for the purpose of interaction with previously described unified mapping model. In combination these two approaches are suggested as basis for a development of a new generation of knowledge representation systems.","['Dmytro Filatov', 'Taras Filatov']",,arXiv,2015,https://doi.org/10.48550/arXiv.1502.06124,Anomali
An Approach For Transforming of Relational Databases to OWL Ontology,"Rapid growth of documents, web pages, and other types oftextcontent is a huge challenge for the modern content management systems. One of the problems in the areas of information storage and retrieval is the lacking of semantic data. Ontologies can present knowledge in sharable and repeatedly usable manner and provide an effective way to reduce the data volume overhead by encoding the structure of a particular domain. Metadata in relational databases can be used to extract ontology from database in a special domain. According to solve the problem of sharing and reusing of data, approaches based on transforming relational database to ontology are proposed. In this paper we propose a method for automatic ontology construction based on relational database.Miningand obtaining further components from relational database leads to obtain knowledge with high semantic power and more expressiveness. Triggers are one of the database components which could be transformed to the ontology model and increase the amount of power and expressiveness of knowledge by presenting part of the knowledge dynamically","['Mona Dadjoo', 'Esmaeil Kheirkhah']",,arXiv,2015,https://doi.org/10.48550/arXiv.1502.05844,Anomali
Evolutionary algorithm based adaptive navigation in information retrieval interfaces,"In computer interfaces in general, especially in information retrieval tasks, it is important to be able to quickly find and retrieve information. State of the art approach, used, for example, in search engines, is not effective as it introduces losses of meanings due to context to keywords back and forth translation. Authors argue it increases the time and reduces the accuracy of information retrieval compared to what it could be in the system that employs modern information retrieval andtextminingmethods while presenting results in an adaptive human- computer interface where system effectively learns what operator needs through iterative interaction. In current work, a combination of adaptive navigational interface and real time collaborative feedback analysis for documents relevance weighting is proposed as an viable alternative to prevailing ""telegraphic"" approach in information retrieval systems. Adaptive navigation is provided through a dynamic links panel controlled by an evolutionary algorithm. Documents relevance is initially established with standard information retrieval techniques and is further refined in real time through interaction of users with the system. Introduced concepts of multidimensional Knowledge Map and Weighted Point of Interest allow finding relevant documents and users with common interests through a trivial calculation. Browsing search approach, the ability of the algorithm to adapt navigation to users interests, collaborative refinement and the self-organising features of the system are the main factors making such architecture effective in various fields where non-structured knowledge shall be represented to the users.","['Dmytro Filatov', 'Taras Filatov']",,arXiv,2015,https://doi.org/10.48550/arXiv.1502.05535,Anomali
Optimizing Text Quantifiers for Multivariate Loss Functions,"We address the problem of \emph{quantification}, a supervised learning task whose goal is, given a class, to estimate the relative frequency (or \emph{prevalence}) of the class in a dataset of unlabelled items. Quantification has several applications in data andtextmining, such as estimating the prevalence of positive reviews in a set of reviews of a given product, or estimating the prevalence of a given support issue in a dataset of transcripts of phone calls to tech support. So far, quantification has been addressed by learning a general-purpose classifier, counting the unlabelled items which have been assigned the class, and tuning the obtained counts according to some heuristics. In this paper we depart from the tradition of using general-purpose classifiers, and use instead a supervised learning model for \emph{structured prediction}, capable of generating classifiers directly optimized for the (multivariate and non-linear) function used for evaluating quantification accuracy. The experiments that we have run on 5500 binary high-dimensional datasets (averaging more than 14,000 documents each) show that this method is more accurate, more stable, and more efficient than existing, state-of-the-art quantification methods.","['Andrea Esuli', 'Fabrizio Sebastiani']","Final version published in ACM Transactions on Knowledge Discovery from Data, 9(4):Article 27, 2015",arXiv,2015,https://doi.org/10.48550/arXiv.1502.05491,Anomali
The Evolution of Popular Music: USA 1960-2010,"In modern societies, cultural change seems ceaseless. The flux of fashion is especially obvious for popular music. While much has been written about the origin and evolution of pop, most claims about its history are anecdotal rather than scientific in nature. To rectify this we investigate the US Billboard Hot 100 between 1960 and 2010. Using Music Information Retrieval (MIR) andtext-miningtools we analyse the musical properties of ~17,000 recordings that appeared in the charts and demonstrate quantitative trends in their harmonic and timbral properties. We then use these properties to produce an audio-based classification of musical styles and study the evolution of musical diversity and disparity, testing, and rejecting, several classical theories of cultural change. Finally, we investigate whether pop musical evolution has been gradual or punctuated. We show that, although pop music has evolved continuously, it did so with particular rapidity during three stylistic ""revolutions"" around 1964, 1983 and 1991. We conclude by discussing how our study points the way to a quantitative science of cultural change.","['Matthias Mauch', 'Robert M. MacCallum', 'Mark Levy', 'Armand M. Leroi']",R. Soc. open sci. 2015 2 150081,arXiv,2015,https://doi.org/10.48550/arXiv.1502.05417,Anomali
Count-Min-Log sketch: Approximately counting with approximate counters,"Count-Min Sketch is a widely adopted algorithm for approximate event counting in large scale processing. However, the original version of the Count-Min-Sketch (CMS) suffers of some deficiences, especially if one is interested by the low-frequency items, such as intext-miningrelated tasks. Several variants of CMS have been proposed to compensate for the high relative error for low-frequency events, but the proposed solutions tend to correct the errors instead of preventing them. In this paper, we propose the Count-Min-Log sketch, which uses logarithm-based, approximate counters instead of linear counters to improve the average relative error of CMS at constant memory footprint.","['Guillaume Pitel', 'Geoffroy Fouquier']",,arXiv,2015,https://doi.org/10.48550/arXiv.1502.04885,Anomali
"Improving Access to Digitized Historical Newspapers with Text Mining, Coordinated Models, and Formative User Interface Design","Most tools for accessing digitized historical newspapers emphasize relatively simple search; but, as increasing numbers of digitized historical newspapers and other historical resources become available we can consider much richer modes of interaction with these collections. For instance, users might use exploratory search for looking at larger issues and events such as elections and campaigns or to get a sense of ""the texture of the city"" or ""what the city was thinking"". To take full advantage of rich interface tools, the content of the newspapers needs to be described systematically and accurately. Moreover, collections of multiple newspapers need to be richly cross-indexed across titles and even with historical resources beyond the newspapers.",['Robert B. Allen'],,arXiv,2015,https://doi.org/10.48550/arXiv.1502.03943,Anomali
"Cryoelectron Microscopy as a Functional Instrument for Systems Biology, Structural Analysis & Experimental Manipulations with Living Cells. A comprehensive review of the current works","The aim of this paper is to give an introductory review of the cryoelectron microscopy as a complex data source for the most of the system biology branches, including the most perspective non-local approaches known as ""localomics"" and ""dynamomics"". A brief summary of various cryoelectron mi-croscopy methods and corresponding system biological ap-proaches is given in thetext. The above classification can be considered as a useful framework for the primary comprehen-sions about cryoelectron microscopy aims and instrumental tools. We do not discuss any of these concepts in details, but merely point out that their methodological complexity follows only from the structure-functional complexity of biological systems which are investigated in this manner. We also postu-late that one can employ some of the cryoelectron microscopic techniques not only for observation, but also for modification and structural refunctionalization of some biological and similar soft matter objects and microscopic samples. In other worlds, we start with the cryoelectron microscopy as a tool for the sys-tem biology and progress to its applying as an instrument for system biology and functional biomimetics; i.e. ""system cryobi-ology"" goes over into ""synthetic cryobiology"" or ""cryogenic biomimetics"". All these conclusions can be deduced from the most recent works of the latest years, including just submitted foreign papers. This article provides an up-to-date description of the conceptual basis for the novel view on the computational cryoelectron microscopy (in silico) approaches and the dataminingprinciples which lie at the very foundation of modern structural analysis and reconstruction.","['Oleg V. Gradov', 'Margaret A. Gradova']","Problems of Cryobiology and Cryomedicine, Vol. 24, Issue 3, pp. 193-210 (2014)",arXiv,2015,https://doi.org/10.48550/arXiv.1501.04337,Anomali
Deep Belief Nets for Topic Modeling,"Applying traditional collaborative filtering to digital publishing is challenging because user data is very sparse due to the high volume of documents relative to the number of users. Content based approaches, on the other hand, is attractive because textual content is often very informative. In this paper we describe large-scale content based collaborative filtering for digital publishing. To solve the digital publishing recommender problem we compare two approaches: latent Dirichlet allocation (LDA) and deep belief nets (DBN) that both find low-dimensional latent representations for documents. Efficient retrieval can be carried out in the latent representation. We work both on public benchmarks and digital media content provided by Issuu, an online publishing platform. This article also comes with a newly developed deep belief nets toolbox for topic modeling tailored towards performance evaluation of the DBN model and comparisons to the LDA model.","['Lars Maaloe', 'Morten Arngren', 'Ole Winther']",,arXiv,2015,https://doi.org/10.48550/arXiv.1501.04325,Anomali
The potential and challenges of Big data - Recommendation systems next level application,"The continuous increase of data generated provides enormous possibilities of both public and private companies. The management of this mass of data or big data will play a crucial role in the society of the future, as it finds applications in different fields. There are so much potential and extremely useful insights hidden in the huge volume of data. The advanced analysis techniques available including predictive analytics,textmining, semantic analysis are needed to enable organizations to create a competitive advantage through data analyzed with different levels of sophistication, speed and accuracy previously unavailable. Therefore, is it still possible to have that level of sophistication with the ubiquitous numeric ocean that accompanies use every day via connected devices that invade our lives? However, development of big data requires a good understanding of the issues associated with it. And this is the purpose of this paper, which focuses on giving a close-up view of big data analysis, opportunities and challenges.","['Fatima El Jamiy', 'Abderrahmane Daif', 'Mohamed Azouazi', 'Abdelaziz Marzak']",,arXiv,2015,https://doi.org/10.48550/arXiv.1501.03424,Anomali
Improving Persian Document Classification Using Semantic Relations between Words,"With the increase of information, document classification as one of the methods oftextmining, plays vital role in many management and organizing information. Document classification is the process of assigning a document to one or more predefined category labels. Document classification includes different parts such astextprocessing, term selection, term weighting and final classification. The accuracy of document classification is very important. Thus improvement in each part of classification should lead to better results and higher precision. Term weighting has a great impact on the accuracy of the classification. Most of the existing weighting methods exploit the statistical information of terms in documents and do not consider semantic relations between words. In this paper, an automated document classification system is presented that uses a novel term weighting method based on semantic relations between terms. To evaluate the proposed method, three standard Persian corpuses are used. Experiment results show 2 to 4 percent improvement in classification accuracy compared with the best previous designed system for Persian documents.","['Saeed Parseh', 'Ahmad Baraani']",,arXiv,2014,https://doi.org/10.48550/arXiv.1412.8147,Anomali
Cauchy Principal Component Analysis,"Principal Component Analysis (PCA) has wide applications in machine learning,textminingand computer vision. Classical PCA based on a Gaussian noise model is fragile to noise of large magnitude. Laplace noise assumption based PCA methods cannot deal with dense noise effectively. In this paper, we propose Cauchy Principal Component Analysis (Cauchy PCA), a very simple yet effective PCA method which is robust to various types of noise. We utilize Cauchy distribution to model noise and derive Cauchy PCA under the maximum likelihood estimation (MLE) framework with low rank constraint. Our method can robustly estimate the low rank matrix regardless of whether noise is large or small, dense or sparse. We analyze the robustness of Cauchy PCA from a robust statistics view and present an efficient singular value projection optimization method. Experimental results on both simulated data and real applications demonstrate the robustness of Cauchy PCA to various noise patterns.","['Pengtao Xie', 'Eric Xing']",,arXiv,2014,https://doi.org/10.48550/arXiv.1412.6506,Anomali
Integer-Programming Ensemble of Temporal-Relations Classifiers,"The extraction and understanding of temporal events and their relations are major challenges in natural language processing. Processingtexton a sentence-by-sentence or expression-by-expression basis often fails, in part due to the challenge of capturing the global consistency of thetext. We present an ensemble method, which reconciles the outputs of multiple classifiers of temporal expressions across thetextusing integer programming. Computational experiments show that the ensemble improves upon the best individual results from two recent challenges, SemEval-2013 TempEval-3 (Temporal Annotation) and SemEval-2016 Task 12 (Clinical TempEval).","['Catherine Kerr', 'Terri Hoare', 'Paula Carroll', 'Jakub Marecek']","Data Mining and Knowledge Discovery, 2020",arXiv,2018,https://doi.org/10.48550/arXiv.1412.1866,Anomali
Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from the Literature,"Drug-drug interaction (DDI) is a major cause of morbidity and mortality and a subject of intense scientific interest. Biomedical literatureminingcan aid DDI research by extracting evidence for large numbers of potential interactions from published literature and clinical databases. Though DDI is investigated in domains ranging in scale from intracellular biochemistry to human populations, literaturemininghas not been used to extract specific types of experimental evidence, which are reported differently for distinct experimental goals. We focus on pharmacokinetic evidence for DDI, essential for identifying causal mechanisms of putative interactions and as input for further pharmacological and pharmaco-epidemiology investigations. We used manually curated corpora of PubMed abstracts and annotated sentences to evaluate the efficacy of literatureminingon two tasks: first, identifying PubMed abstracts containing pharmacokinetic evidence of DDIs; second, extracting sentences containing such evidence from abstracts. We implemented atextminingpipeline and evaluated it using several linear classifiers and a variety of feature transforms. The most important textual features in the abstract and sentence classification tasks were analyzed. We also investigated the performance benefits of using features derived from PubMed metadata fields, various publicly available named entity recognizers, and pharmacokinetic dictionaries. Several classifiers performed very well in distinguishing relevant and irrelevant abstracts (reaching F1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65, iAUC~=0.83). We found that word bigram features were important for achieving optimal classifier performance and that features derived from Medical Subject Headings (MeSH) terms significantly improved abstract classification. ...","['Artemy Kolchinsky', 'Anália Lourenço', 'Heng-Yi Wu', 'Lang Li', 'Luis M. Rocha']",,arXiv,2015,https://doi.org/10.48550/arXiv.1412.0744,Anomali
Fast Algorithms for the Maximum Clique Problem on Massive Graphs with Applications to Overlapping Community Detection,"The maximum clique problem is a well known NP-Hard problem with applications in datamining, network analysis, information retrieval and many other areas related to the World Wide Web. There exist several algorithms for the problem with acceptable runtimes for certain classes of graphs, but many of them are infeasible for massive graphs. We present a new exact algorithm that employs novel pruning techniques and is able to find maximum cliques in very large, sparse graphs quickly. Extensive experiments on different kinds of synthetic and real-world graphs show that our new algorithm can be orders of magnitude faster than existing algorithms. We also present a heuristic that runs orders of magnitude faster than the exact algorithm while providing optimal or near-optimal solutions. We illustrate a simple application of the algorithms in developing methods for detection of overlapping communities in networks.","['Bharath Pattabiraman', 'Md. Mostofa Ali Patwary', 'Assefaw H. Gebremedhin', 'Wei-keng Liao', 'Alok Choudhary']","Internet Mathematics 2014, Special Issue (WAW'13)",arXiv,2014,https://doi.org/10.48550/arXiv.1411.7460,Anomali
A Joint Probabilistic Classification Model of Relevant and Irrelevant Sentences in Mathematical Word Problems,"Estimating the difficulty level of math word problems is an important task for many educational applications. Identification of relevant and irrelevant sentences in math word problems is an important step for calculating the difficulty levels of such problems. This paper addresses a novel application oftextcategorization to identify two types of sentences in mathematical word problems, namely relevant and irrelevant sentences. A novel joint probabilistic classification model is proposed to estimate the joint probability of classification decisions for all sentences of a math word problem by utilizing the correlation among all sentences along with the correlation between the question sentence and other sentences, and sentencetext. The proposed model is compared with i) a SVM classifier which makes independent classification decisions for individual sentences by only using the sentencetextand ii) a novel SVM classifier that considers the correlation between the question sentence and other sentences along with the sentencetext. An extensive set of experiments demonstrates the effectiveness of the joint probabilistic classification model for identifying relevant and irrelevant sentences as well as the novel SVM classifier that utilizes the correlation between the question sentence and other sentences. Furthermore, empirical results and analysis show that i) it is highly beneficial not to remove stopwords and ii) utilizing part of speech tagging does not make a significant improvement although it has been shown to be effective for the related task of math word problem type classification.","['Suleyman Cetintas', 'Luo Si', 'Yan Ping Xin', 'Dake Zhang', 'Joo Young Park', 'Ron Tzur']",,arXiv,2014,https://doi.org/10.48550/arXiv.1411.5732,Anomali
Opinion mining of text documents written in Macedonian language,"The ability to extract public opinion from web portals such as review sites, social networks and blogs will enable companies and individuals to form a view, an attitude and make decisions without having to do lengthy and costly researches and surveys. In this paper machine learning techniques are used for determining the polarity of forum posts on kajgana which are written in Macedonian language. The posts are classified as being positive, negative or neutral. We test different feature metrics and classifiers and provide detailed evaluation of their participation in improving the overall performance on a manually generated dataset. By achieving 92% accuracy, we show that the performance of systems for automated opinionminingis comparable to a human evaluator, thus making it a viable option fortextdata analysis. Finally, we present a few statistics derived from the forum posts using the developed system.","['Andrej Gajduk', 'Ljupco Kocarev']",,arXiv,2014,https://doi.org/10.48550/arXiv.1411.4472,Anomali
"Analysis of the human diseasome reveals phenotype modules across common, genetic, and infectious diseases","Phenotypes are the observable characteristics of an organism arising from its response to the environment. Phenotypes associated with engineered and natural genetic variation are widely recorded using phenotype ontologies in model organisms, as are signs and symptoms of human Mendelian diseases in databases such as OMIM and Orphanet. Exploiting these resources, several computational methods have been developed for integration and analysis of phenotype data to identify the genetic etiology of diseases or suggest plausible interventions. A similar resource would be highly useful not only for rare and Mendelian diseases, but also for common, complex and infectious diseases. We apply a semantictext-miningapproach to identify the phenotypes (signs and symptoms) associated with over 8,000 diseases. We demonstrate that our method generates phenotypes that correctly identify known disease-associated genes in mice and humans with high accuracy. Using a phenotypic similarity measure, we generate a human disease network in which diseases that share signs and symptoms cluster together, and we use this network to identify phenotypic disease modules.","['Robert Hoehndorf', 'Paul N Schofield', 'Georgios V Gkoutos']",,arXiv,2014,https://doi.org/10.48550/arXiv.1411.0450,Anomali
Experiments to Improve Named Entity Recognition on Turkish Tweets,"Social mediatextsare significant information sources for several application areas including trend analysis, event monitoring, and opinionmining. Unfortunately, existing solutions for tasks such as named entity recognition that perform well on formaltextsusually perform poorly when applied to social mediatexts. In this paper, we report on experiments that have the purpose of improving named entity recognition on Turkish tweets, using two different annotated data sets. In these experiments, starting with a baseline named entity recognition system, we adapt its recognition rules and resources to better fit Twitter language by relaxing its capitalization constraint and by diacritics-based expansion of its lexical resources, and we employ a simplistic normalization scheme on tweets to observe the effects of these on the overall named entity recognition performance on Turkish tweets. The evaluation results of the system with these different settings are provided with discussions of these results.","['Dilek Küçük', 'Ralf Steinberger']",,arXiv,2014,https://doi.org/10.48550/arXiv.1410.8668,Anomali
Analysis of Named Entity Recognition and Linking for Tweets,"Applying natural language processing forminingand intelligent information access to tweets (a form of microblog) is a challenging, emerging research area. Unlike carefully authored newstextand other longer content, tweets pose a number of new challenges, due to their short, noisy, context-dependent, and dynamic nature. Information extraction from tweets is typically performed in a pipeline, comprising consecutive stages of language identification, tokenisation, part-of-speech tagging, named entity recognition and entity disambiguation (e.g. with respect to DBpedia). In this work, we describe a new Twitter entity disambiguation dataset, and conduct an empirical analysis of named entity recognition and disambiguation, investigating how robust a number of state-of-the-art systems are on such noisytexts, what the main sources of error are, and which problems should be further investigated to improve the state of the art.","['Leon Derczynski', 'Diana Maynard', 'Giuseppe Rizzo', 'Marieke van Erp', 'Genevieve Gorrell', 'Raphaël Troncy', 'Johann Petrak', 'Kalina Bontcheva']","Information Processing & Management 51 (2), 32-49, 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1410.7182,Anomali
Hybrid Mobility Prediction of 802.11 Infrastructure Nodes by Location Tracking and Data Mining,"In an IEEE 802.11 Infrastructure network, as the mobile node is moving from one access point to another, the resource allocation and smooth hand off may be a problem. If some reliable prediction is done on mobile nodes next move, then resources can be allocated optimally as the mobile node moves around. This would increase the performance throughput of wireless network. We plan to investigate on a hybrid mobility prediction scheme that uses location tracking and dataminingto predict the future path of the mobile node. We also propose a secure version of the same scheme. Through simulation and analysis, we present the prediction accuracy of our proposal.","['B. Issac', 'K. Hamid', 'C. E. Tan']",,arXiv,2014,https://doi.org/10.48550/arXiv.1410.4377,Anomali
"A Scalable, Lexicon Based Technique for Sentiment Analysis","Rapid increase in the volume of sentiment rich social media on the web has resulted in an increased interest among researchers regarding Sentimental Analysis and opinionmining. However, with so much social media available on the web, sentiment analysis is now considered as a big data task. Hence the conventional sentiment analysis approaches fails to efficiently handle the vast amount of sentiment data available now a days. The main focus of the research was to find such a technique that can efficiently perform sentiment analysis on big data sets. A technique that can categorize thetextas positive, negative and neutral in a fast and accurate manner. In the research, sentiment analysis was performed on a large data set of tweets using Hadoop and the performance of the technique was measured in form of speed and accuracy. The experimental results shows that the technique exhibits very good efficiency in handling big sentiment data sets.","['Chetan Kaushik', 'Atul Mishra']","International Journal in Foundations of Computer Science & Technology (IJFCST), Vol.4, No.5, September 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1410.2265,Anomali
Term-Weighting Learning via Genetic Programming for Text Classification,"This paper describes a novel approach to learning term-weighting schemes (TWSs) in the context oftextclassification. Intextmininga TWS determines the way in which documents will be represented in a vector space model, before applying a classifier. Whereas acceptable performance has been obtained with standard TWSs (e.g., Boolean and term-frequency schemes), the definition of TWSs has been traditionally an art. Further, it is still a difficult task to determine what is the best TWS for a particular problem and it is not clear yet, whether better schemes, than those currently available, can be generated by combining known TWS. We propose in this article a genetic program that aims at learning effective TWSs that can improve the performance of current schemes intextclassification. The genetic program learns how to combine a set of basic units to give rise to discriminative TWSs. We report an extensive experimental study comprising data sets from thematic and non-thematictextclassification as well as from image classification. Our study shows the validity of the proposed method; in fact, we show that TWSs learned with the genetic program outperform traditional schemes and other TWSs proposed in recent works. Further, we show that TWSs learned from a specific domain can be effectively used for other tasks.","['Hugo Jair Escalante', 'Mauricio A. García-Limón', 'Alicia Morales-Reyes', 'Mario Graff', 'Manuel Montes-y-Gómez', 'Eduardo F. Morales']",,arXiv,2014,https://doi.org/10.48550/arXiv.1410.0640,Anomali
Heterogeneous Metric Learning with Content-based Regularization for Software Artifact Retrieval,"The problem of software artifact retrieval has the goal to effectively locate software artifacts, such as a piece of source code, in a large code repository. This problem has been traditionally addressed through the textual query. In other words, information retrieval techniques will be exploited based on the textual similarity between queries and textual representation of software artifacts, which is generated by collecting words from comments, identifiers, and descriptions of programs. However, in addition to these semantic information, there are rich information embedded in source codes themselves. These source codes, if analyzed properly, can be a rich source for enhancing the efforts of software artifact retrieval. To this end, in this paper, we develop a feature extraction method on source codes. Specifically, this method can capture both the inherent information in the source codes and the semantic information hidden in the comments, descriptions, and identifiers of the source codes. Moreover, we design a heterogeneous metric learning approach, which allows to integrate code features andtextfeatures into the same latent semantic space. This, in turn, can help to measure the artifact similarity by exploiting the joint power of both code andtextfeatures. Finally, extensive experiments on real-world data show that the proposed method can help to improve the performances of software artifact retrieval with a significant margin.","['Liang Wu', 'Hui Xiong', 'Liang Du', 'Bo Liu', 'Guandong Xu', 'Yong Ge', 'Yanjie Fu', 'Yuanchun Zhou', 'Jianhui Li']",,arXiv,2014,https://doi.org/10.48550/arXiv.1409.7165,Anomali
Lurking in Social Networks: Topology-based Analysis and Ranking Methods,"The massive presence of silent members in online communities, the so-called lurkers, has long attracted the attention of researchers in social science, cognitive psychology, and computer-human interaction. However, the study of lurking phenomena represents an unexplored opportunity of research in datamining, information retrieval and related fields. In this paper, we take a first step towards the formal specification and analysis of lurking in social networks. We address the new problem of lurker ranking and propose the first centrality methods specifically conceived for ranking lurkers in social networks. Our approach utilizes only the network topology without probing intotextcontents or user relationships related to media. Using Twitter, Flickr, FriendFeed and GooglePlus as cases in point, our methods' performance was evaluated against data-driven rankings as well as existing centrality methods, including the classic PageRank and alpha-centrality. Empirical evidence has shown the significance of our lurker ranking approach, and its uniqueness in effectively identifying and ranking lurkers in an online social network.","['Andrea Tagarelli', 'Roberto Interdonato']","Social Network Analysis and Mining. August 2014, 4:230",arXiv,2014,https://doi.org/10.48550/arXiv.1409.4695,Anomali
Polarity detection movie reviews in hindi language,"Nowadays peoples are actively involved in giving comments and reviews on social networking websites and other websites like shopping websites, news websites etc. large number of people everyday share their opinion on the web, results is a large number of user data is collected .users also find it trivial task to read all the reviews and then reached into the decision. It would be better if these reviews are classified into some category so that the user finds it easier to read. OpinionMiningor Sentiment Analysis is a natural language processing task thatminesinformation from varioustextforms such as reviews, news, and blogs and classify them on the basis of their polarity as positive, negative or neutral. But, from the last few years, user content in Hindi language is also increasing at a rapid rate on the Web. So it is very important to perform opinionminingin Hindi language as well. In this paper a Hindi language opinionminingsystem is proposed. The system classifies the reviews as positive, negative and neutral for Hindi language. Negation is also handled in the proposed system. Experimental results using reviews of movies show the effectiveness of the system","['Richa Sharma', 'Shweta Nigam', 'Rekha Jain']",,arXiv,2014,https://doi.org/10.48550/arXiv.1409.3942,Anomali
An NLP Assistant for Clide,"This report describes an NLP assistant for the collaborative development environment Clide, that supports the development of NLP applications by providing easy access to some common NLP data structures. The assistant visualizestextfragments and their dependencies by displaying the semantic graph of a sentence, the coreference chain of a paragraph andminedtriples that are extracted from a paragraph's semantic graphs and linked using its coreference chain. Using this information and a logic programming library, we create an NLP database which is used by a series of queries tominethe triples. The algorithm is tested by translating a natural languagetextdescribing a graph to an actual graph that is shown as an annotation in thetexteditor.",['Tobias Kortkamp'],,arXiv,2014,https://doi.org/10.48550/arXiv.1409.2073,Anomali
Testing parametric models in linear-directional regression,"This paper presents a goodness-of-fit test for parametric regression models with scalar response and directional predictor, that is, a vector on a sphere of arbitrary dimension. The testing procedure is based on the weighted squared distance between a smooth and a parametric regression estimator, where the smooth regression estimator is obtained by a projected local approach. Asymptotic behavior of the test statistic under the null hypothesis and local alternatives is provided, jointly with a consistent bootstrap algorithm for application in practice. A simulation study illustrates the performance of the test in finite samples. The procedure is applied to test a linear model intextmining.","['Eduardo García-Portugués', 'Ingrid Van Keilegom', 'Rosa M. Crujeiras', 'Wenceslao González-Manteiga']","Scandinavian Journal of Statistics, 43(4):1178-1191, 2016",arXiv,2020,https://doi.org/10.48550/arXiv.1409.0506,Anomali
A Case Study in Text Mining: Interpreting Twitter Data From World Cup Tweets,"Cluster analysis is a field of data analysis that extracts underlying patterns in data. One application of cluster analysis is intext-mining, the analysis of large collections oftextto find similarities between documents. We used a collection of about 30,000 tweets extracted from Twitter just before the World Cup started. A common problem with real worldtextdata is the presence of linguistic noise. In our case it would be extraneous tweets that are unrelated to dominant themes. To combat this problem, we created an algorithm that combined the DBSCAN algorithm and a consensus matrix. This way we are left with the tweets that are related to those dominant themes. We then used cluster analysis to find those topics that the tweets describe. We clustered the tweets using k-means, a commonly used clustering algorithm, and Non-Negative Matrix Factorization (NMF) and compared the results. The two algorithms gave similar results, but NMF proved to be faster and provided more easily interpreted results. We explored our results using two visualization tools, Gephi and Wordle.","['Daniel Godfrey', 'Caley Johns', 'Carl Meyer', 'Shaina Race', 'Carol Sadek']",,arXiv,2014,https://doi.org/10.48550/arXiv.1408.5427,Anomali
Opinion mining of movie reviews at document level,"The whole world is changed rapidly and using the current technologies Internet becomes an essential need for everyone. Web is used in every field. Most of the people use web for a common purpose like online shopping, chatting etc. During an online shopping large number of reviews/opinions are given by the users that reflect whether the product is good or bad. These reviews need to be explored, analyse and organized for better decision making. OpinionMiningis a natural language processing task that deals with finding orientation of opinion in a piece oftextwith respect to a topic. In this paper a document based opinionminingsystem is proposed that classify the documents as positive, negative and neutral. Negation is also handled in the proposed system. Experimental results using reviews of movies show the effectiveness of the system.","['Richa Sharma', 'Shweta Nigam', 'Rekha Jain']",,arXiv,2014,https://doi.org/10.48550/arXiv.1408.3829,Anomali
Convex Biclustering,"In the biclustering problem, we seek to simultaneously group observations and features. While biclustering has applications in a wide array of domains, ranging fromtextminingto collaborative filtering, the problem of identifying structure in high dimensional genomic data motivates this work. In this context, biclustering enables us to identify subsets of genes that are co-expressed only within a subset of experimental conditions. We present a convex formulation of the biclustering problem that possesses a unique global minimizer and an iterative algorithm, COBRA, that is guaranteed to identify it. Our approach generates an entire solution path of possible biclusters as a single tuning parameter is varied. We also show how to reduce the problem of selecting this tuning parameter to solving a trivial modification of the convex biclustering problem. The key contributions of our work are its simplicity, interpretability, and algorithmic guarantees - features that arguably are lacking in the current alternative algorithms. We demonstrate the advantages of our approach, which includes stably and reproducibly identifying biclusterings, on simulated and real microarray data.","['Eric C. Chi', 'Genevera I. Allen', 'Richard G. Baraniuk']","Biometrics 73 (1):10-19, 2017",arXiv,2016,https://doi.org/10.48550/arXiv.1408.0856,Anomali
"Text Classification Using Association Rules, Dependency Pruning and Hyperonymization","We present new methods for pruning and enhancing item- sets fortextclassification via association rulemining. Pruning methods are based on dependency syntax and enhancing methods are based on replacing words by their hyperonyms of various orders. We discuss the impact of these methods, compared to pruning based on tfidf rank of words.","['Yannis Haralambous', 'Philippe Lenca']",,arXiv,2014,https://doi.org/10.48550/arXiv.1407.7357,Anomali
Benchmarking Named Entity Disambiguation approaches for Streaming Graphs,"Named Entity Disambiaguation (NED) is a central task for applications dealing with natural languagetext. Assume that we have a graph based knowledge base (subsequently referred as Knowledge Graph) where nodes represent various real world entities such as people, location, organization and concepts. Given data sources such as social media streams and web pages Entity Linking is the task of mapping named entities that are extracted from the data to those present in the Knowledge Graph. This is an inherently difficult task due to several reasons. Almost all these data sources are generated without any formal ontology; the unstructured nature of the input, limited context and the ambiguity involved when multiple entities are mapped to the same name make this a hard task. This report looks at two state of the art systems employing two distinctive approaches: graph based Accurate Online Disambiguation of Entities (AIDA) andMinedEvidence Named Entity Disambiguation (MENED), which employs a statistical inference approach. We compare both approaches using the data set and queries provided by the Knowledge Base Population (KBP) track at 2011 NISTTextAnalytics Conference (TAC). This report begins with an overview of the respective approaches, followed by detailed description of the experimental setup. It concludes with our findings from the benchmarking exercise.","['Sutanay Choudhury', 'Chase Dowling']","PNNL-23455 2014-05, Pacific Northwest National Laboratory, Richland, WA",arXiv,2014,https://doi.org/10.48550/arXiv.1407.3751,Anomali
Future Influence Ranking of Scientific Literature,"Researchers or students entering a emerging research area are particularly interested in what newly published papers will be most cited and which young researchers will become influential in the future, so that they can catch the most recent advances and find valuable research directions. However, predicting the future importance of scientific articles and authors is challenging due to the dynamic nature of literature networks and evolving research topics. Different from most previous studies aiming to rank the current importance of literatures and authors, we focus on \emph{ranking the future popularity of new publications and young researchers} by proposing a unified ranking model to combine various available information. Specifically, we first propose to extract two kinds oftextfeatures, words and words co-occurrence to characterize innovative papers and authors. Then, instead of using static and un-weighted graphs, we construct time-aware weighted graphs to distinguish the various importance of links established at different time. Finally, by leveraging both the constructedtextfeatures and graphs, we propose a mutual reinforcement ranking framework called \emph{MRFRank} to rank the future importance of papers and authors simultaneously. Experimental results on the ArnetMiner dataset show that the proposed approach significantly outperforms the baselines on the metric \emph{recommendation intensity}.","['Senzhang Wang', 'Sihong Xie', 'Xiaoming Zhang', 'Zhoujun Li', 'Philip S. Yu', 'Xinyu Shu']",,arXiv,2014,https://doi.org/10.48550/arXiv.1407.1772,Anomali
KNET: A General Framework for Learning Word Embedding using Morphological Knowledge,"Neural network techniques are widely applied to obtain high-quality distributed representations of words, i.e., word embeddings, to addresstextmining, information retrieval, and natural language processing tasks. Recently, efficient methods have been proposed to learn word embeddings from context that captures both semantic and syntactic relationships between words. However, it is challenging to handle unseen words or rare words with insufficient context. In this paper, inspired by the study on word recognition process in cognitive psychology, we propose to take advantage of seemingly less obvious but essentially important morphological knowledge to address these challenges. In particular, we introduce a novel neural network architecture called KNET that leverages both contextual information and morphological word similarity built based on morphological knowledge to learn word embeddings. Meanwhile, the learning architecture is also able to refine the pre-defined morphological knowledge and obtain more accurate word similarity. Experiments on an analogical reasoning task and a word similarity task both demonstrate that the proposed KNET framework can greatly enhance the effectiveness of word embeddings.","['Qing Cui', 'Bin Gao', 'Jiang Bian', 'Siyu Qiu', 'Tie-Yan Liu']",,arXiv,2014,https://doi.org/10.48550/arXiv.1407.1687,Anomali
Stock Market Prediction from WSJ: Text Mining via Sparse Matrix Factorization,"We revisit the problem of predicting directional movements of stock prices based on news articles: here our algorithm uses daily articles from The Wall Street Journal to predict the closing stock prices on the same day. We propose a unified latent space model to characterize the ""co-movements"" between stock prices and news articles. Unlike many existing approaches, our new model is able to simultaneously leverage the correlations: (a) among stock prices, (b) among news articles, and (c) between stock prices and news articles. Thus, our model is able to make daily predictions on more than 500 stocks (most of which are not even mentioned in any news article) while having low complexity. We carry out extensive backtesting on trading strategies based on our algorithm. The result shows that our model has substantially better accuracy rate (55.7%) compared to many widely used algorithms. The return (56%) and Sharpe ratio due to a trading strategy based on our model are also much higher than baseline indices.","['Felix Ming Fai Wong', 'Zhenming Liu', 'Mung Chiang']",,arXiv,2014,https://doi.org/10.48550/arXiv.1406.7330,Anomali
Scalable Topical Phrase Mining from Text Corpora,"While most topic modeling algorithms modeltextcorpora with unigrams, human interpretation often relies on inherent grouping of terms into phrases. As such, we consider the problem of discovering topical phrases of mixed lengths. Existing work either performs post processing to the inference results of unigram-based topic models, or utilizes complex n-gram-discovery topic models. These methods generally produce low-quality topical phrases or suffer from poor scalability on even moderately-sized datasets. We propose a different approach that is both computationally efficient and effective. Our solution combines a novel phraseminingframework to segment a document into single and multi-word phrases, and a new topic model that operates on the induced document partition. Our approach discovers high quality topical phrases with negligible extra cost to the bag-of-words topic model in a variety of datasets including research publication titles, abstracts, reviews, and news articles.","['Ahmed El-Kishky', 'Yanglei Song', 'Chi Wang', 'Clare Voss', 'Jiawei Han']","Proceedings of the VLDB Endowment, Vol. 8(3), pp. 305 - 316, 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1406.6312,Anomali
A survey on phrase structure learning methods for text classification,"Textclassification is a task of automatic classification oftextinto one of the predefined categories. The problem oftextclassification has been widely studied in different communities like natural language processing, dataminingand information retrieval.Textclassification is an important constituent in many information management tasks like topic identification, spam filtering, email routing, language identification, genre classification, readability assessment etc. The performance oftextclassification improves notably when phrase patterns are used. The use of phrase patterns helps in capturing non-local behaviours and thus helps in the improvement oftextclassification task. Phrase structure extraction is the first step to continue with the phrase pattern identification. In this survey, detailed study of phrase structure learning methods have been carried out. This will enable future work in several NLP tasks, which uses syntactic information from phrase structure like grammar checkers, question answering, information extraction, machine translation,textclassification. The paper also provides different levels of classification and detailed comparison of the phrase structure learning methods.","['Reshma Prasad', 'Mary Priya Sebastian']",,arXiv,2014,https://doi.org/10.48550/arXiv.1406.5598,Anomali
Predicting Motivations of Actions by Leveraging Text,"Understanding human actions is a key problem in computer vision. However, recognizing actions is only the first step of understanding what a person is doing. In this paper, we introduce the problem of predicting why a person has performed an action in images. This problem has many applications in human activity understanding, such as anticipating or explaining an action. To study this problem, we introduce a new dataset of people performing actions annotated with likely motivations. However, the information in an image alone may not be sufficient to automatically solve this task. Since humans can rely on their lifetime of experiences to infer motivation, we propose to give computer vision systems access to some of these experiences by using recently developed natural language models tomineknowledge stored in massive amounts oftext. While we are still far away from fully understanding motivation, our results suggest that transferring knowledge from language into vision can help machines understand why people in images might be performing an action.","['Carl Vondrick', 'Deniz Oktay', 'Hamed Pirsiavash', 'Antonio Torralba']",,arXiv,2016,https://doi.org/10.48550/arXiv.1406.5472,Anomali
Mining of product reviews at aspect level,"Todays world is a world of Internet, almost all work can be done with the help of it, from simple mobile phone recharge to biggest business deals can be done with the help of this technology. People spent their most of the times on surfing on the Web it becomes a new source of entertainment, education, communication, shopping etc. Users not only use these websites but also give their feedback and suggestions that will be useful for other users. In this way a large amount of reviews of users are collected on the Web that needs to be explored, analyse and organized for better decision making. OpinionMiningor Sentiment Analysis is a Natural Language Processing and Information Extraction task that identifies the users views or opinions explained in the form of positive, negative or neutral comments and quotes underlying thetext. Aspect based opinionminingis one of the level of Opinionminingthat determines the aspect of the given reviews and classify the review for each feature. In this paper an aspect based opinionminingsystem is proposed to classify the reviews as positive, negative and neutral for each feature. Negation is also handled in the proposed system. Experimental results using reviews of products show the effectiveness of the system.","['Richa Sharma', 'Shweta Nigam', 'Rekha Jain']","International Journal in Foundations of Computer Science & Technology (IJFCST), Vol.4, No.3, May 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1406.3714,Anomali
A Semantic VSM-Based Recommender System,"Online forums enable users to discuss together around various topics. One of the serious problems of these environments is high volume of discussions and thus information overload problem. Unfortunately without considering the users interests, traditional Information Retrieval (IR) techniques are not able to solve the problem. Therefore, employment of a Recommender System (RS) that could suggest favorite's topics of users according to their tastes could increases the dynamism of forum and prevent the users from duplicate posts. In addition, consideration of semantics can be useful for increasing the performance of IR based RS. Our goal is study of impact of ontology and dataminingtechniques on improving of content-based RS. For this purpose, at first, three type of ontologies will be constructed from the domain corpus with utilization oftextmining, Natural Language Processing (NLP) and Wordnet and then they will be used as an input in two kind of RS: one, fully ontology-based and one with enriching the user profile vector with ontology in vector space model (VSM) (proposed method). Afterward the results will be compared with the simple VSM based RS. Given results show that the proposed RS presents the highest performance.","['Hadi Fanaee-T', 'Mehran Yazdi']","International Journal of Computer Theory and Engineering vol. 5, no. 2, pp. 331-336, 2013",arXiv,2014,https://doi.org/10.48550/arXiv.1406.3277,Anomali
A Multiplicative Model for Learning Distributed Text-Based Attribute Representations,"In this paper we propose a general framework for learning distributed representations of attributes: characteristics oftextwhose representations can be jointly learned with word embeddings. Attributes can correspond to document indicators (to learn sentence vectors), language indicators (to learn distributed language representations), meta-data and side information (such as the age, gender and industry of a blogger) or representations of authors. We describe a third-order model where word context and attribute vectors interact multiplicatively to predict the next word in a sequence. This leads to the notion of conditional word similarity: how meanings of words change when conditioned on different attributes. We perform several experimental tasks including sentiment classification, cross-lingual document classification, and blog authorship attribution. We also qualitatively evaluate conditional word neighbours and attribute-conditionedtextgeneration.","['Ryan Kiros', 'Richard S. Zemel', 'Ruslan Salakhutdinov']",,arXiv,2014,https://doi.org/10.48550/arXiv.1406.2710,Anomali
Text Mining System for Non-Expert Miners,"Service oriented architecture integrated withtextminingallows services to extract information in a well defined manner. In this paper, it is proposed to design a knowledge extracting system for the Ocean Information Data System. Deployed ARGO floating sensors of INCOIS (Indian National Council for Ocean Information Systems) organization reflects the characteristics of ocean. This is forwarded to the OIDS (Ocean Information Data System). For the data received from OIDS, pre-processing techniques are applied. Pre-processing involves the header retrieval and data separation. Header information is used to identify the region of sensor, whereas data is used in the analysis process of Ocean Information System. Analyzed data is segmented based on the region, by the header value.Miningtechnique and composition principle is applied on the segments for further analysis. Index Terms-- Service oriented architecture;TextMining; ARGO floating sensor; INCOIS; OIDS; Pre-processing.","['P. Ramya', 'S. Sasirekha']",,arXiv,2014,https://doi.org/10.48550/arXiv.1406.1855,Anomali
Machine learning approach for text and document mining,"TextCategorization (TC), also known asTextClassification, is the task of automatically classifying a set oftextdocuments into different categories from a predefined set. If a document belongs to exactly one of the categories, it is a single-label classification task; otherwise, it is a multi-label classification task. TC uses several tools from Information Retrieval (IR) and Machine Learning (ML) and has received much attention in the last years from both researchers in the academia and industry developers. In this paper, we first categorize the documents using KNN based machine learning approach and then return the most relevant documents.","['Vishwanath Bijalwan', 'Pinki Kumari', 'Jordan Pascual', 'Vijay Bhaskar Semwal']",,arXiv,2014,https://doi.org/10.48550/arXiv.1406.1580,Anomali
Isometry on Interval-valued Fuzzy Graphs,"Especially in research areas of computer science such as datamining, image segmentation, clustering image capturing and networking. The interval-valued fuzzy graphs are more flexible and compatible than fuzzy graphs due to the fact that they allowed the degree of membership of a vertex to an edge to be represented by interval valued in [0,1] rather than the crisp real values between 0 and 1.","['Hossein Rashmanlou', 'Madhumangal Pal']","International Journal of Fuzzy Mathematical Archive, vol. 3, (2013) 28-35",arXiv,2014,https://doi.org/10.48550/arXiv.1405.6003,Anomali
New Perspectives in Sinographic Language Processing Through the Use of Character Structure,"Chinese characters have a complex and hierarchical graphical structure carrying both semantic and phonetic information. We use this structure to enhance thetextmodel and obtain better results in standard NLP operations. First of all, to tackle the problem of graphical variation we define allographic classes of characters. Next, the relation of inclusion of a subcharacter in a characters, provides us with a directed graph of allographic classes. We provide this graph with two weights: semanticity (semantic relation between subcharacter and character) and phoneticity (phonetic relation) and calculate ""most semantic subcharacter paths"" for each character. Finally, adding the information contained in these paths to unigrams we claim to increase the efficiency oftextminingmethods. We evaluate our method on atextclassification task on two corpora (Chinese and Japanese) of a total of 18 million characters and get an improvement of 3% on an already high baseline of 89.6% precision, obtained by a linear SVM classifier. Other possible applications and perspectives of the system are discussed.",['Yannis Haralambous'],"Lecture Notes in Computer Science 7816 (2013), pp. 201--217",arXiv,2014,https://doi.org/10.48550/arXiv.1405.5474,Anomali
Student Dropout Risk Assessment in Undergraduate Course at Residential University,"Student dropout prediction is an indispensable for numerous intelligent systems to measure the education system and success rate of any university as well as throughout the university in the world. Therefore, it becomes essential to develop efficient methods for prediction of the students at risk of dropping out, enabling the adoption of proactive process to minimize the situation. Thus, this research work propose a prototype machine learning tool which can automatically recognize whether the student will continue their study or drop their study using classification technique based on decision tree and extract hidden information from large data about what factors are responsible for dropout student. Further the contribution of factors responsible for dropout risk was studied using discriminant analysis and to extract interesting correlations, frequent patterns, associations or casual structures among significant datasets, Association ruleminingwas applied. In this study, the descriptive statistics analysis was carried out to measure the quality of data using SPSS 20.0 statistical software and application of decision tree and association rule were carried out by using WEKA dataminingtool.",['Sweta Rai'],,arXiv,2014,https://doi.org/10.48550/arXiv.1405.3727,Anomali
Sentiment Analysis: A Survey,"Sentiment analysis (also known as opinionmining) refers to the use of natural language processing,textanalysis and computational linguistics to identify and extract subjective information in source materials.Miningopinions expressed in the user generated content is a challenging yet practically very useful problem. This survey would cover various approaches and methodology used in Sentiment Analysis and OpinionMiningin general. The focus would be on Internettextlike, Product review, tweets and other social media.",['Rahul Tejwani'],,arXiv,2014,https://doi.org/10.48550/arXiv.1405.2584,Anomali
Web Content Classification: A Survey,"As the information contained within the web is increasing day by day, organizing this information could be a necessary requirement.The dataminingprocess is to extract information from a data set and transform it into an understandable structure for further use. Classification of web page content is essential to many tasks in web information retrieval such as maintaining web directories and focused crawling.The uncontrolled type of nature of web content presents additional challenges to web page classification as compared to the traditionaltextclassification, but the interconnected nature of hypertext also provides features that can assist the process. In this paper the web classification is discussed in detail and its importance in field of dataminingis explored.",['Prabhjot Kaur'],"International Journal of Computer Trends and Technology (IJCTT) V10(2):97-101, Apr 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1405.0580,Anomali
Opinion Mining In Hindi Language: A Survey,"Opinions are very important in the life of human beings. These Opinions helped the humans to carry out the decisions. As the impact of the Web is increasing day by day, Web documents can be seen as a new source of opinion for human beings. Web contains a huge amount of information generated by the users through blogs, forum entries, and social networking websites and so on To analyze this large amount of information it is required to develop a method that automatically classifies the information available on the Web. This domain is called Sentiment Analysis and OpinionMining. OpinionMiningor Sentiment Analysis is a natural language processing task thatmineinformation from varioustextforms such as reviews, news, and blogs and classify them on the basis of their polarity as positive, negative or neutral. But, from the last few years, enormous increase has been seen in Hindi language on the Web. Research in opinionminingmostly carried out in English language but it is very important to perform the opinionminingin Hindi language also as large amount of information in Hindi is also available on the Web. This paper gives an overview of the work that has been done Hindi language.","['Richa Sharma', 'Shweta Nigam', 'Rekha Jain']","International Journal in Foundations of Computer Science & Technology (IJFCST) International Journal in Foundations of Computer Science & Technology (IJFCST), Vol.4, No.2, March 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1404.4935,Anomali
Overview of Stemming Algorithms for Indian and Non-Indian Languages,"Stemming is a pre-processing step inTextMiningapplications as well as a very common requirement of Natural Language processing functions. Stemming is the process for reducing inflected words to their stem. The main purpose of stemming is to reduce different grammatical forms / word forms of a word like its noun, adjective, verb, adverb etc. to its root form. Stemming is widely uses in Information Retrieval system and reduces the size of index files. We can say that the goal of stemming is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. In this paper we have discussed different stemming algorithm for non-Indian and Indian language, methods of stemming, accuracy and errors.","['Dalwadi Bijal', 'Suthar Sanket']","International Journal of Computer Science and Information Technologies, Vol. 5 (2) , 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1404.2878,Anomali
Aspect-Based Opinion Extraction from Customer reviews,"Textis the main method of communicating information in the digital age. Messages, blogs, news articles, reviews, and opinionated information abound on the Internet. People commonly purchase products online and post their opinions about purchased items. This feedback is displayed publicly to assist others with their purchasing decisions, creating the need for a mechanism with which to extract and summarize useful information for enhancing the decision-making process. Our contribution is to improve the accuracy of extraction by combining different techniques from three major areas, named DataMining, Natural Language Processing techniques and Ontologies. The proposed framework sequentiallyminesproducts aspects and users opinions, groups representative aspects by similarity, and generates an output summary. This paper focuses on the task of extracting product aspects and users opinions by extracting all possible aspects and opinions from reviews using natural language, ontology, and frequent (tag) sets. The proposed framework, when compared with an existing baseline model, yielded promising results.","['Amani K Samha', 'Yuefeng Li', 'Jinglan Zhang']",,arXiv,2014,https://doi.org/10.48550/arXiv.1404.1982,Anomali
Relevant Feature Selection Model Using Data Mining for Intrusion Detection System,"Network intrusions have become a significant threat in recent years as a result of the increased demand of computer networks for critical systems. Intrusion detection system (IDS) has been widely deployed as a defense measure for computer networks. Features extracted from network traffic can be used as sign to detect anomalies. However with the huge amount of network traffic, collected data contains irrelevant and redundant features that affect the detection rate of the IDS, consumes high amount of system resources, and slowdown the training and testing process of the IDS. In this paper, a new feature selection model is proposed; this model can effectively select the most relevant features for intrusion detection. Our goal is to build a lightweight intrusion detection system by using a reduced features set. Deleting irrelevant and redundant features helps to build a faster training and testing process, to have less resource consumption as well as to maintain high detection rates. The effectiveness and the feasibility of our feature selection model were verified by several experiments on KDD intrusion detection dataset. The experimental results strongly showed that our model is not only able to yield high detection rates but also to speed up the detection process.","['Ayman I. Madbouly', 'Amr M. Gody', 'Tamer M. Barakat']","International Journal of Engineering Trends and Technology (IJETT), V9(10),501-512 March 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1403.7726,Anomali
Mining Attribute-Based Access Control Policies from Logs,"Attribute-based access control (ABAC) provides a high level of flexibility that promotes security and information sharing. ABAC policyminingalgorithms have potential to significantly reduce the cost of migration to ABAC, by partially automating the development of an ABAC policy from information about the existing access-control policy and attribute data. This paper presents an algorithm forminingABAC policies from operation logs and attribute data. To the best of our knowledge, it is the first algorithm for this problem.","['Zhongyuan Xu', 'Scott D. Stoller']",,arXiv,2018,https://doi.org/10.48550/arXiv.1403.5715,Anomali
k-Nearest Neighbor Classification over Semantically Secure Encrypted Relational Data,"DataMininghas wide applications in many areas such as banking, medicine, scientific research and among government agencies. Classification is one of the commonly used tasks in dataminingapplications. For the past decade, due to the rise of various privacy issues, many theoretical and practical solutions to the classification problem have been proposed under different security models. However, with the recent popularity of cloud computing, users now have the opportunity to outsource their data, in encrypted form, as well as the dataminingtasks to the cloud. Since the data on the cloud is in encrypted form, existing privacy preserving classification techniques are not applicable. In this paper, we focus on solving the classification problem over encrypted data. In particular, we propose a secure k-NN classifier over encrypted data in the cloud. The proposed k-NN protocol protects the confidentiality of the data, user's input query, and data access patterns. To the best of our knowledge, our work is the first to develop a secure k-NN classifier over encrypted data under the semi-honest model. Also, we empirically analyze the efficiency of our solution through various experiments.","['Bharath K. Samanthula', 'Yousef Elmehdwi', 'Wei Jiang']",,arXiv,2014,https://doi.org/10.48550/arXiv.1403.5001,Anomali
Time Series Analysis on Stock Market for Text Mining Correlation of Economy News,"This paper proposes an information retrieval method for the economy news. The effect of economy news, are researched in the word level and stock market values are considered as the ground proof. The correlation between stock market prices and economy news is an already addressed problem for most of the countries. The most well-known approach is applying thetextminingapproaches to the news and some time series analysis techniques over stock market closing values in order to apply classification or clustering algorithms over the features extracted. This study goes further and tries to ask the question what are the available time series analysis techniques for the stock market closing values and which one is the most suitable? In this study, the news and their dates are collected into a database andtextminingis applied over the news, thetextminingpart has been kept simple with only term frequency-inverse document frequency method. For the time series analysis part, we have studied 10 different methods such as random walk, moving average, acceleration, Bollinger band, price rate of change, periodic average, difference, momentum or relative strength index and their variation. In this study we have also explained these techniques in a comparative way and we have applied the methods over Turkish Stock Market closing values for more than a 2 year period. On the other hand, we have applied the term frequency-inverse document frequency method on the economy news of one of the high-circulating newspapers in Turkey.","['Sadi Evren Seker', 'Cihan Mert', 'Khaled Al-Naami', 'Nuri Ozalp', 'Ugur Ayan']","International Journal of Social Sciences and Humanity Studies Vol 6, No 1, 2014 ISSN: 1309-8063 (Online)",arXiv,2014,https://doi.org/10.48550/arXiv.1403.2002,Anomali
Performance Evaluation of Machine Learning Classifiers in Sentiment Mining,"In recent years, the use of machine learning classifiers is of great value in solving a variety of problems intextclassification. Sentimentminingis a kind oftextclassification in which, messages are classified according to sentiment orientation such as positive or negative. This paper extends the idea of evaluating the performance of various classifiers to show their effectiveness in sentimentminingof online product reviews. The product reviews are collected from Amazon reviews. To evaluate the performance of classifiers various evaluation methods like random sampling, linear sampling and bootstrap sampling are used. Our results shows that support vector machine with bootstrap sampling method outperforms others classifiers and sampling methods in terms of misclassification rate.",['Vinodhini G Chandrasekaran RM'],,arXiv,2014,https://doi.org/10.48550/arXiv.1402.3891,Anomali
Efficient Analysis of Pattern and Association Rule Mining Approaches,"The process of dataminingproduces various patterns from a given data source. The most recognized dataminingtasks are the process of discovering frequent itemsets, frequent sequential patterns, frequent sequential rules and frequent association rules. Numerous efficient algorithms have been proposed to do the above processes. Frequent patternmininghas been a focused topic in dataminingresearch with a good number of references in literature and for that reason an important progress has been made, varying from performant algorithms for frequent itemsetminingin transaction databases to complex algorithms, such as sequential patternmining, structured patternmining, correlationmining. Association Rulemining(ARM) is one of the utmost current dataminingtechniques designed to group objects together from large databases aiming to extract the interesting correlation and relation among huge amount of data. In this article, we provide a brief review and analysis of the current status of frequent patternminingand discuss some promising research directions. Additionally, this paper includes a comparative study between the performance of the described approaches.","['Thabet Slimani', 'Amor Lazzez']","International Journal of Information Technology and Computer Science (IJITCS), vol.6, no.3, pp.70-81, 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1402.2892,Anomali
High-dimensional tests for spherical location and spiked covariance,"Rotationally symmetric distributions on the p-dimensional unit hypersphere, extremely popular in directional statistics, involve a location parameter theta that indicates the direction of the symmetry axis. The most classical way of addressing the spherical location problem H_0:theta=theta_0, with theta_0 a fixed location, is the so-called Watson test, which is based on the sample mean of the observations. This test enjoys many desirable properties, but its implementation requires the sample size n to be large compared to the dimension p. This is a severe limitation, since more and more problems nowadays involve high-dimensional directional data (e.g., in genetics ortextmining). In this work, we therefore introduce a modified Watson statistic that can cope with high-dimensionality. We derive its asymptotic null distribution as both n and p go to infinity. This is achieved in a universal asymptotic framework that allows p to go to infinity arbitrarily fast (or slowly) as a function of n. We further show that our results also provide high-dimensional tests for a problem that has recently attracted much attention, namely that of testing that the covariance matrix of a multinormal distribution has a ""theta_0-spiked"" structure. Finally, a Monte Carlo simulation study corroborates our asymptotic results.","['Christophe Ley', 'Davy Paindaveine', 'Thomas Verdebout']",,arXiv,2014,https://doi.org/10.48550/arXiv.1402.2823,Anomali
Enhancing Human Aspect of Software Engineering using Bayesian Classifier,"IT industries in current scenario have to struggle effectively in terms of cost, quality, service or innovation for their subsistence in the global market. Due to the swift transformation of technology, software industries owe to manage a large set of data having precious information hidden. Dataminingtechnique enables one to effectively cope with this hidden information where it can be applied to code optimization, fault prediction and other domains which modulates the success nature of software projects. Additionally, the efficiency of the product developed further depends upon the quality of the project personnel. The position of the paper therefore is to explore potentials of project personnel in terms of their competency and skill set and its influence on quality of project. The above mentioned objective is accomplished using a Bayesian classifier in order to capture the pattern of human performance. By this means, the hidden and valuable knowledge discovered in the related databases will be summarized in the statistical structure. This mode of predictive study enables the project managers to reduce the failure ratio to a significant level and improve the performance of the project using the right choice of project personnel.","['Sangita Gupta', 'Suma V']","International Journal of Cognitive Science, Engineering, and Technology Volume 1, Issue 1, November 2013 ISSN 2347 8047",arXiv,2014,https://doi.org/10.48550/arXiv.1402.2379,Anomali
Empirical Study on Selection of Team Members for Software Projects - Data Mining Approach,"One of the essential requisites of any software industry is the development of customer satisfied products. However, accomplishing the aforesaid business objective depends upon the depth of quality of product that is engineered in the organization. Thus, generation of high quality depends upon process, which is in turn depends upon the people. Existing scenario in IT industries demands a requirement for deploying the right personnel for achieving desirable quality in the product through the existing process. The goal of this paper is to identify the criteria which will be used in industrial practice to select members of a software project team, and to look for relationships between these criteria and project success. Using semi-structured interviews and qualitative methods for data analysis and synthesis, a set of team building criteria was identified from project managers in industry. The findings show that the consistent use of the set of criteria correlated significantly with project success, and the criteria related to human factors present strong correlations with software quality and thereby project success. This knowledge enables decision making for project managers in allocation of right personnel to realize desired level.","['Sangita Gupta', 'Suma. V']","International Journal of Computer Science and Informatics, ISSN PRINT: 2231 5292, Volume 3, Issue 2, 2013",arXiv,2014,https://doi.org/10.48550/arXiv.1402.2377,Anomali
Mining Images in Biomedical Publications: Detection and Analysis of Gel Diagrams,"Authors of biomedical publications use gel images to report experimental results such as protein-protein interactions or protein expressions under different conditions. Gel images offer a concise way to communicate such findings, not all of which need to be explicitly discussed in the articletext. This fact together with the abundance of gel images and their shared common patterns makes them prime candidates for automated imageminingand parsing. We introduce an approach for the detection of gel images, and present a workflow to analyze them. We are able to detect gel segments and panels at high accuracy, and present preliminary results for the identification of gene names in these images. While we cannot provide a complete solution at this point, we present evidence that this kind of imageminingis feasible.","['Tobias Kuhn', 'Mate Levente Nagy', 'ThaiBinh Luong', 'Michael Krauthammer']","Journal of Biomedical Semantics 2014, 5:10",arXiv,2014,https://doi.org/10.48550/arXiv.1402.2073,Anomali
Generating Extractive Summaries of Scientific Paradigms,"Researchers and scientists increasingly find themselves in the position of having to quickly understand large amounts of technical material. Our goal is to effectively serve this need by using bibliometrictextminingand summarization techniques to generate summaries of scientific literature.  We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose C-LexRank, a model for summarizing single scientific articles based on citations, which employs community detection and extracts salient information-rich sentences. Next, we further extend our experiments  to summarize a set of papers, which cover the same scientific topic. We generate extractive summaries of a set of Question Answering (QA) and Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show that citations have unique information amenable to creating a summary.","['Vahed Qazvinian', 'Dragomir R. Radev', 'Saif M. Mohammad', 'Bonnie Dorr', 'David Zajic', 'Michael Whidby', 'Taesun Moon']","Journal Of Artificial Intelligence Research, Volume 46, pages 165-201, 2013",arXiv,2014,https://doi.org/10.48550/arXiv.1402.0556,Anomali
A Novel String Distance Function based on Most Frequent K Characters,"This study aims to publish a novel similarity metric to increase the speed of comparison operations. Also the new metric is suitable for distance-based operations among strings. Most of the simple calculation methods, such as string length are fast to calculate but does not represent the string correctly. On the other hand the methods like keeping the histogram over all characters in the string are slower but good to represent the string characteristics in some areas, like natural language. We propose a new metric, easy to calculate and satisfactory for string comparison. Method is built on a hash function, which gets a string at any size and outputs the most frequent K characters with their frequencies. The outputs are open for comparison and our studies showed that the success rate is quite satisfactory for thetextminingoperations.","['Sadi Evren Seker', 'Oguz Altun', 'Uğur Ayan', 'Cihan Mert']","International Journal of Machine Learning and Computation (IJMLC), Issn : 2010-3700, vol.4, is.2, pp.177-183, 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1401.6596,Anomali
The Readability of Tweets and their Geographic Correlation with Education,"Twitter has rapidly emerged as one of the largest worldwide venues for written communication. Thanks to the ease with which vast quantities of tweets can bemined, Twitter has also become a source for studying modern linguistic style. The readability oftexthas long provided a simple method to characterize the complexity of language and ease that documents may be understood by readers. In this note we use a modified version of the Flesch Reading Ease formula, applied to a corpus of 17.4 million tweets. We find tweets have characteristically more difficult readability scores compared to other short format communication, such as SMS or chat. This linguistic difference is insensitive to the presence of ""hashtags"" within tweets. By utilizing geographic data provided by 2% of users, joined with ""ZIP Code Tabulation Area"" (ZCTA) level education data from the U.S. Census, we find an intriguing correlation between the average readability and the college graduation rate within a ZCTA. This points towards a difference in either the underlying language, or a change in the type of content being tweeted in these areas","['James R. A. Davenport', 'Robert DeLine']",,arXiv,2014,https://doi.org/10.48550/arXiv.1401.6058,Anomali
Modulated Hawking radiation and a nonviolent channel for information release,"Unitarization of black hole evaporation requires that quantum information escapes a black hole; an important question is to identify the mechanism or channel by which it does so. Accurate counting of black hole states via the Bekenstein-Hawking entropy would indicate this information should be encoded in radiation with average energy flux matching Hawking's. Information can be encoded with no change in net flux via fine-grained modulation of the Hawking radiation. In an approximate effective field theory description, couplings to the stress tensor of the black hole atmosphere that depend on the internal state of the black hole are a promising alternative for inducing such modulation. These can be picturesquely thought of as due to state-dependent metric fluctuations in the vicinity of the horizon. Such couplings offer the prospect of emitting information without extra energy flux, and can be shown to do so at linear order in the couplings, with motivation given for possible extension of this result to higher orders. The potential advantages of such couplings to the stress tensor thus extend beyond their universality, which is helpful in addressing constraints from black holemining.",['Steven B. Giddings'],,arXiv,2014,https://doi.org/10.48550/arXiv.1401.5804,Anomali
Data Mining Cultural Aspects of Social Media Marketing,"For marketing to function in a globalized world it must respect a diverse set of local cultures. With marketing efforts extending to social media platforms, the crossing of cultural boundaries can happen in an instant. In this paper we examine how culture influences the popularity of marketing messages in social media platforms.Textmining, automated translation and sentiment analysis contribute largely to our research. From our analysis of 400 posts on the localized Google+ pages of German car brands in Germany and the US, we conclude that posting time and emotions are important predictors for reshare counts.","['Ronald Hochreiter', 'Christoph Waldhauser']",Lecture Notes in Computer Science 8557: 130-143. 2014,arXiv,2014,https://doi.org/10.48550/arXiv.1401.5726,Anomali
A new keyphrases extraction method based on suffix tree data structure for arabic documents clustering,"Document Clustering is a branch of a larger area of scientific study known as datamining.which is an unsupervised classification using to find a structure in a collection of unlabeled data. The useful information in the documents can be accompanied by a large amount of noise words when using FullTextRepresentation, and therefore will affect negatively the result of the clustering process. So it is with great need to eliminate the noise words and keeping just the useful information in order to enhance the quality of the clustering results. This problem occurs with different degree for any language such as English, European, Hindi, Chinese, and Arabic Language. To overcome this problem, in this paper, we propose a new and efficient Keyphrases extraction method based on the Suffix Tree data structure (KpST), the extracted Keyphrases are then used in the clustering process instead of FullTextRepresentation. The proposed method for Keyphrases extraction is language independent and therefore it may be applied to any language. In this investigation, we are interested to deal with the Arabic language which is one of the most complex languages. To evaluate our method, we conduct an experimental study on Arabic Documents using the most popular Clustering approach of Hierarchical algorithms: Agglomerative Hierarchical algorithm with seven linkage techniques and a variety of distance functions and similarity measures to perform Arabic Document Clustering task. The obtained results show that our method for extracting Keyphrases increases the quality of the clustering results. We propose also to study the effect of using the stemming for the testing dataset to cluster it with the same documents clustering techniques and similarity/distance measures.","['Issam Sahmoudi', 'Hanane Froud', 'Abdelmonaime Lachkar']","International Journal of Database Management Systems ( IJDMS ) Vol.5, No.6, December 2013",arXiv,2014,https://doi.org/10.48550/arXiv.1401.5644,Anomali
The Why and How of Nonnegative Matrix Factorization,"Nonnegative matrix factorization (NMF) has become a widely used tool for the analysis of high-dimensional data as it automatically extracts sparse and meaningful features from a set of nonnegative data vectors. We first illustrate this property of NMF on three applications, in image processing,textminingand hyperspectral imaging --this is the why. Then we address the problem of solving NMF, which is NP-hard in general. We review some standard NMF algorithms, and also present a recent subclass of NMF problems, referred to as near-separable NMF, that can be solved efficiently (that is, in polynomial time), even in the presence of noise --this is the how. Finally, we briefly describe some problems in mathematics and computer science closely related to NMF via the nonnegative rank.",['Nicolas Gillis'],"In: ""Regularization, Optimization, Kernels, and Support Vector Machines"", J.A.K. Suykens, M. Signoretto and A. Argyriou (eds), Chapman & Hall/CRC, Machine Learning and Pattern Recognition Series, pp. 257-291, 2014",arXiv,2014,https://doi.org/10.48550/arXiv.1401.5226,Anomali
Fully Online Grammar Compression in Constant Space,"We present novel variants of fully online LCA (FOLCA), a fully online grammar compression that builds a straight line program (SLP) and directly encodes it into a succinct representation in an online manner. FOLCA enables a direct encoding of an SLP into a succinct representation that is asymptotically equivalent to an information theoretic lower bound for representing an SLP (Maruyama et al., SPIRE'13). The compression of FOLCA takes linear time proportional to the length of an inputtextand its working space depends only on the size of the SLP, which enables us to apply FOLCA to large-scale repetitivetexts. Recent repetitivetexts, however, include some noise. For example, current sequencing technology has significant error rates, which embeds noise into genome sequences. For such noisy repetitivetexts, FOLCA working in the SLP size consumes a large amount of memory. We present two variants of FOLCA working in constant space by leveraging the idea behind streamminingtechniques. Experiments using 100 human genomes corresponding to about 300GB from the 1000 human genomes project revealed the applicability of our method to large-scale, noisy repetitivetexts.","['Shirou Maruyama', 'Yasuo Tabei']",,arXiv,2014,https://doi.org/10.48550/arXiv.1401.5143,Anomali
Learning Language from a Large (Unannotated) Corpus,"A novel approach to the fully automated, unsupervised extraction of dependency grammars and associated syntax-to-semantic-relationship mappings from largetextcorpora is described. The suggested approach builds on the authors' prior work with the Link Grammar, RelEx and OpenCog systems, as well as on a number of prior papers and approaches from the statistical language learning literature. If successful, this approach would enable theminingof all the information needed to power a natural language comprehension and generation system, directly from a large, unannotated corpus.","['Linas Vepstas', 'Ben Goertzel']",,arXiv,2014,https://doi.org/10.48550/arXiv.1401.3372,Anomali
A survey of methods to ease the development of highly multilingual text mining applications,"Multilingualtextprocessing is useful because the information content found in different languages is complementary, both regarding facts and opinions. While Information Extraction and othertextminingsoftware can, in principle, be developed for many languages, mosttextanalysis tools have only been applied to small sets of languages because the development effort per language is large. Self-training tools obviously alleviate the problem, but even the effort of providing training data and of manually tuning the results is usually considerable. In this paper, we gather insights by various multilingual system developers on how to minimise the effort of developing natural language processing applications for many languages. We also explain the main guidelines underlying our own effort to develop complextextminingsoftware for tens of languages. While these guidelines - most of all: extreme simplicity - can be very restrictive and limiting, we believe to have shown the feasibility of the approach through the development of the Europe Media Monitor (EMM) family of applications (http://emm.newsbrief.eu/overview.html). EMM is a set of complex media monitoring tools that process and analyse up to 100,000 online news articles per day in between twenty and fifty languages. We will also touch upon the kind of language resources that would make it easier for all to develop highly multilingualtextminingapplications. We will argue that - to achieve this - the most needed resources would be freely available, simple, parallel and uniform multilingual dictionaries, corpora and software tools.",['Ralf Steinberger'],"Language Resources and Evaluation, Volume 46, Issue 2, pp 155-176, June 2012",arXiv,2014,https://doi.org/10.48550/arXiv.1401.2937,Anomali
Statistical Analysis based Hypothesis Testing Method in Biological Knowledge Discovery,"The correlation and interactions among different biological entities comprise the biological system. Although already revealed interactions contribute to the understanding of different existing systems, researchers face many questions everyday regarding inter-relationships among entities. Their queries have potential role in exploring new relations which may open up a new area of investigation. In this paper, we introduce atextminingbased method for answering the biological queries in terms of statistical computation such that researchers can come up with new knowledge discovery. It facilitates user to submit their query in natural linguistic form which can be treated as hypothesis. Our proposed approach analyzes the hypothesis and measures the p-value of the hypothesis with respect to the existing literature. Based on the measured value, the system either accepts or rejects the hypothesis from statistical point of view. Moreover, even it does not find any direct relationship among the entities of the hypothesis, it presents a network to give an integral overview of all the entities through which the entities might be related. This is also congenial for the researchers to widen their view and thus think of new hypothesis for further investigation. It assists researcher to get a quantitative evaluation of their assumptions such that they can reach a logical conclusion and thus aids in relevant re-searches of biological knowledge discovery. The system also provides the researchers a graphical interactive interface to submit their hypothesis for assessment in a more convenient way.","['Md. Naseef-Ur-Rahman Chowdhury', 'Suvankar Paul', 'Kazi Zakia Sultana']",,arXiv,2014,https://doi.org/10.48550/arXiv.1401.2851,Anomali
Dictionary-Based Concept Mining: An Application for Turkish,"In this study, a dictionary-based method is used to extract expressive concepts from documents. So far, there have been many studies concerning conceptminingin English, but this area of study for Turkish, an agglutinative language, is still immature. We used dictionary instead of WordNet, a lexical database grouping words into synsets that is widely used for concept extraction. The dictionaries are rarely used in the domain of conceptmining, but taking into account that dictionary entries have synonyms, hypernyms, hyponyms and other relationships in their meaningtexts, the success rate has been high for determining concepts. This concept extraction method is implemented on documents, that are collected from different corpora.","['Cem Rıfkı Aydın', 'Ali Erkan', 'Tunga Güngör', 'Hidayet Takçı']",,arXiv,2014,https://doi.org/10.48550/arXiv.1401.2663,Anomali
Towards Connected Enterprises: The Business Network System,"The discovery, representation and reconstruction of Business Networks (BN) from NetworkMining(NM) raw data is a difficult problem for enterprises. This is due to huge amounts of complex business processes within and across enterprise boundaries, heterogeneous technology stacks, and fragmented data. To remain competitive, visibility into the enterprise and partner networks on different, interrelated abstraction levels is desirable. We present a novel data discovery,miningand network inference system, called Business Network System (BNS), that reconstructs the BN--integration and business process networks--from raw data, hidden in the enterprises' landscapes. BNS provides a new, declarative foundation for gathering information, defining a network model, inferring the network and check its conformance to the real-world ""as-is"" network. The paper covers both the foundation and the key features of BNS, including its underlying technologies, its overall system architecture, and its most interesting capabilities.",['Daniel Ritter'],,arXiv,2013,https://doi.org/10.48550/arXiv.1312.7542,Anomali
Subjectivity Classification using Machine Learning Techniques for Mining Feature-Opinion Pairs from Web Opinion Sources,"Due to flourish of the Web 2.0, web opinion sources are rapidly emerging containing precious information useful for both customers and manufactures. Recently, feature based opinionminingtechniques are gaining momentum in which customer reviews are processed automatically forminingproduct features and user opinions expressed over them. However, customer reviews may contain both opinionated and factual sentences. Distillations of factual contents improveminingperformance by preventing noisy and irrelevant extraction. In this paper, combination of both supervised machine learning and rule-based approaches are proposed forminingfeasible feature-opinion pairs from subjective review sentences. In the first phase of the proposed approach, a supervised machine learning technique is applied for classifying subjective and objective sentences from customer reviews. In the next phase, a rule based method is implemented which applies linguistic and semantic analysis oftextstominefeasible feature-opinion pairs from subjective sentences retained after the first phase. The effectiveness of the proposed methods is established through experimentation over customer reviews on different electronic products.",['Ahmad Kamal'],"International Journal of Computer Science Issues (IJCSI), Volume 10 Issue 5, 2013, pp 191-200",arXiv,2013,https://doi.org/10.48550/arXiv.1312.6962,Anomali
A Robust Missing Value Imputation Method MifImpute For Incomplete Molecular Descriptor Data And Comparative Analysis With Other Missing Value Imputation Methods,"Missing data imputation is an important research topic in datamining. Large-scale Molecular descriptor data may contains missing values (MVs). However, some methods for downstream analyses, including some prediction tools, require a complete descriptor data matrix. We propose and evaluate an iterative imputation method MiFoImpute based on a random forest. By averaging over many unpruned regression trees, random forest intrinsically constitutes a multiple imputation scheme. Using the NRMSE and NMAE estimates of random forest, we are able to estimate the imputation error. Evaluation is performed on two molecular descriptor datasets generated from a diverse selection of pharmaceutical fields with artificially introduced missing values ranging from 10% to 30%. The experimental result demonstrates that missing values has a great impact on the effectiveness of imputation techniques and our method MiFoImpute is more robust to missing value than the other ten imputation methods used as benchmark. Additionally, MiFoImpute exhibits attractive computational efficiency and can cope with high-dimensional data.","['Doreswamy', 'Chanabasayya . M. Vastrad']","Published International Journal on Computational Sciences & Applications (IJCSA) Vol.3, No4, August 2013",arXiv,2013,https://doi.org/10.48550/arXiv.1312.2859,Anomali
Novel text categorization by amalgamation of augmented k-nearest neighborhood classification and k-medoids clustering,"Machine learning fortextclassification is the underpinning of document cataloging, news filtering, document steering and exemplification. Intextminingrealm, effective feature selection is significant to make the learning task more accurate and competent. One of the traditional lazytextclassifier k-Nearest Neighborhood (kNN) has a major pitfall in calculating the similarity between all the objects in training and testing sets, there by leads to exaggeration of both computational complexity of the algorithm and massive consumption of main memory. To diminish these shortcomings in viewpoint of a data-miningpractitioner an amalgamative technique is proposed in this paper using a novel restructured version of kNN called AugmentedkNN(AkNN) and k-Medoids(kMdd) clustering.The proposed work comprises preprocesses on the initial training set by imposing attribute feature selection for reduction of high dimensionality, also it detects and excludes the high-fliers samples in the initial training set and restructures a constrictedtraining set. The kMdd clustering algorithm generates the cluster centers (as interior objects) for each category and restructures the constricted training set with centroids. This technique is amalgamated with AkNNclassifier that was prearranged withtextminingsimilarity measures. Eventually, significantweights and ranks were assigned to each object in the new training set based upon their accessory towards the object in testing set. Experiments conducted on Reuters-21578 a UCI benchmarktextminingdata set, and comparisons with traditional kNNclassifier designates the referredmethod yieldspreeminentrecitalin both clustering and classification.","['RamachandraRao Kurada', 'Dr. K Karteeka Pavan']","International Journal of Computational Science & Information Technology(IJCSITY)Vol.1,No.4,Nov2013,ISSN: 2320-7442",arXiv,2013,https://doi.org/10.48550/arXiv.1312.2375,Anomali
Request and notification Pattern for an internet banking System,"The quality of software is enhanced by using the design patterns. The design patterns are the reusable component used in the development of the software, which delivers improved quality software to the end users. The researchers have developed design patterns for user interface, e-commerce applications, mobile applications,textclassification and so on. There are no design patterns for internet banking applications, but there is analysis pattern for banking. This motivated tominethe design patterns for internet banking application. It can beminedfrom the document of Business Process Management (BPM). In this paper the request and notification are two patterns, that have been presented, which have beenminedfrom internet banking.","['A. Meiappane', 'Dr. V. Prasanna Venkataesan']",,arXiv,2013,https://doi.org/10.48550/arXiv.1312.2344,Anomali
Implementation of CRISP Methodology for ERP Systems,"ERP systems contain huge amounts of data related to the actual execution of business processes. These systems have a particular way of recording activities which results in an unclear display of business processes in event logs. Several works have been conducted on ERP systems, most of them focusing on the development of new algorithms for the automatic discovery of business processes. We focused on addressing issues like, how can organizations with ERP systems apply processminingfor analyzing their business processes in order to improve them. The data handling aspect of ERP systems contrasts with those of BPMS or workflow based systems, whose systematical storage of events facilitates the application of processminingtechniques. CRISP-DM has emerged as the de facto standard for developing dataminingand knowledge discovery projects. Successful dataminingrequires three families of analytical capabilities namely reporting, classification and forecasting. A data miner uses more than one analytical method to get the best results. The objective of this paper is to improve the usability and understandability of processminingtechniques, by implementing CRISP-DM methodology for their application in ERP contexts, detailed in terms of specific implementation tools and step by step coordination. Our study confirms that data discovery from ERP system improves strategic and operational decision making.","['S. Hanumanth Sastry', 'Prof. M. S. Prasada Babu']",,arXiv,2013,https://doi.org/10.48550/arXiv.1312.2065,Anomali
Evolutionary Dynamics of Information Diffusion over Social Networks,"Current social networks are of extremely large-scale generating tremendous information flows at every moment. How information diffuse over social networks has attracted much attention from both industry and academics. Most of the existing works on information diffusion analysis are based on machine learning methods focusing on social network structure analysis and empirical datamining. However, the dynamics of information diffusion, which are heavily influenced by network users' decisions, actions and their socio-economic interactions, is generally ignored by most of existing works. In this paper, we propose an evolutionary game theoretic framework to model the dynamic information diffusion process in social networks. Specifically, we derive the information diffusion dynamics in complete networks, uniform degree and non-uniform degree networks, with the highlight of two special networks, Erdős-Rényi random network and the Barabási-Albert scale-free network. We find that the dynamics of information diffusion over these three kinds of networks are scale-free and the same with each other when the network scale is sufficiently large. To verify our theoretical analysis, we perform simulations for the information diffusion over synthetic networks and real-world Facebook networks. Moreover, we also conduct experiment on Twitter hashtags dataset, which shows that the proposed game theoretic model can well fit and predict the information diffusion over real social networks.","['Chunxiao Jiang', 'Yan Chen', 'K. J. Ray Liu']",,arXiv,2013,https://doi.org/10.48550/arXiv.1312.0317,Anomali
Web Mining Techniques in E-Commerce Applications,"Today web is the best medium of communication in modern business. Many companies are redefining their business strategies to improve the business output. Business over internet provides the opportunity to customers and partners where their products and specific business can be found. Nowadays online business breaks the barrier of time and space as compared to the physical office. Big companies around the world are realizing that e-commerce is not just buying and selling over Internet, rather it improves the efficiency to compete with other giants in the market. For this purpose dataminingsometimes called as knowledge discovery is used. Webminingis dataminingtechnique that is applied to the WWW. There are vast quantities of information available over the Internet.","['Ahmad Tasnim Siddiqui', 'Sultan Aljahdali']","International Journal of Computer Applications, Volume 69 No.8, May 2013",arXiv,2013,https://doi.org/10.48550/arXiv.1311.7388,Anomali
A Hybrid Web Recommendation System based on the Improved Association Rule Mining Algorithm,"As the growing interest of web recommendation systems those are applied to deliver customized data for their users, we started working on this system. Generally the recommendation systems are divided into two major categories such as collaborative recommendation system and content based recommendation system. In case of collaborative recommen-dation systems, these try to seek out users who share same tastes that of given user as well as recommends the websites according to the liking given user. Whereas the content based recommendation systems tries to recommend web sites similar to those web sites the user has liked. In the recent research we found that the efficient technique based on asso-ciation ruleminingalgorithm is proposed in order to solve the problem of web page recommendation. Major problem of the same is that the web pages are given equal importance. Here the importance of pages changes according to the fre-quency of visiting the web page as well as amount of time user spends on that page. Also recommendation of newly added web pages or the pages those are not yet visited by users are not included in the recommendation set. To over-come this problem, we have used the web usage log in the adaptive association rule based webminingwhere the asso-ciation rules were applied to personalization. This algorithm was purely based on the Apriori dataminingalgorithm in order to generate the association rules. However this method also suffers from some unavoidable drawbacks. In this paper we are presenting and investigating the new approach based on weighted Association RuleMiningAlgorithm andtextmining. This is improved algorithm which adds semantic knowledge to the results, has more efficiency and hence gives better quality and performances as compared to existing approaches.","['Ujwala Wanaskar', 'Sheetal Vij', 'Debajyoti Mukhopadhyay']",,arXiv,2013,https://doi.org/10.48550/arXiv.1311.7204,Anomali
Clustering and Relational Ambiguity: from Text Data to Natural Data,"Textdata is often seen as ""take-away"" materials with little noise and easy to process information. Main questions are how to get data and transform them into a good document format. But data can be sensitive to noise oftenly called ambiguities. Ambiguities are aware from a long time, mainly because polysemy is obvious in language and context is required to remove uncertainty. I claim in this paper that syntactic context is not suffisant to improve interpretation. In this paper I try to explain that firstly noise can come from natural data themselves, even involving high technology, secondlytexts, seen as verified but meaningless, can spoil content of a corpus; it may lead to contradictions and background noise.",['Nicolas Turenne'],"Journal of Data Mining & Digital Humanities, 2014 (June 24, 2014) jdmdh:4",arXiv,2014,https://doi.org/10.48550/arXiv.1311.5401,Anomali
Data Mining Model for the Data Retrieval from Central Server Configuration,"A server, which is to keep track of heavy document traffic, is unable to filter the documents that are most relevant and updated for continuoustextsearch queries. This paper focuses on handling continuoustextextraction sustaining high document traffic. The main objective is to retrieve recent updated documents that are most relevant to the query by applying sliding window technique. Our solution indexes the streamed documents in the main memory with structure based on the principles of inverted file, and processes document arrival and expiration events with incremental threshold-based method. It also ensures elimination of duplicate document retrieval using unsupervised duplicate detection. The documents are ranked based on user feedback and given higher priority for retrieval.","['Srivatsan Sridharan', 'Kausal Malladi', 'Yamini Muralitharan']","International Journal of Computer Science & Information Technology (IJCSIT) Vol 5, No 5, October 2013",arXiv,2013,https://doi.org/10.48550/arXiv.1311.5013,Anomali
Automatic ontology generation for data mining using fca and clustering,"Motivated by the increased need for formalized representations of the domain of DataMining, the success of using Formal Concept Analysis (FCA) and Ontology in several Computer Science fields, we present in this paper a new approach for automatic generation of Fuzzy Ontology of DataMining(FODM), through the fusion of conceptual clustering, fuzzy logic, and FCA. In our approach, we propose to generate ontology taking in consideration another degree of granularity into the process of generation. Indeed, we suggest to define an ontology between classes resulting from a preliminary classification on the data. We prove that this approach optimize the definition of the ontology, offered a better interpretation of the data and optimized both the space memory and the execution time for exploiting this data.","['Amel Grissa Touzi', 'Hela Ben Massoud', 'Alaya Ayadi']",,arXiv,2013,https://doi.org/10.48550/arXiv.1311.1764,Anomali
Using Robust PCA to estimate regional characteristics of language use from geo-tagged Twitter messages,"Principal component analysis (PCA) and related techniques have been successfully employed in natural language processing.Textminingapplications in the age of the online social media (OSM) face new challenges due to properties specific to these use cases (e.g. spelling issues specific totextsposted by users, the presence of spammers and bots, service announcements, etc.). In this paper, we employ a Robust PCA technique to separate typical outliers and highly localized topics from the low-dimensional structure present in language use in online social networks. Our focus is on identifying geospatial features among the messages posted by the users of the Twitter microblogging service. Using a dataset which consists of over 200 million geolocated tweets collected over the course of a year, we investigate whether the information present in word usage frequencies can be used to identify regional features of language use and topics of interest. Using the PCA pursuit method, we are able to identify important low-dimensional features, which constitute smoothly varying functions of the geographic location.","['Dániel Kondor', 'István Csabai', 'László Dobos', 'János Szüle', 'Norbert Barankai', 'Tamás Hanyecz', 'Tamás Sebők', 'Zsófia Kallus', 'Gábor Vattay']",,arXiv,2013,https://doi.org/10.48550/arXiv.1311.1169,Anomali
A language independent web data extraction using vision based page segmentation algorithm,"Web usageminingis a process of extracting useful information from server logs i.e. users history. Web usageminingis a process of finding out what users are looking for on the internet. Some users might be looking at only textual data, where as some others might be interested in multimedia data. One would retrieve the data by copying it and pasting it to the relevant document. But this is tedious and time consuming as well as difficult when the data to be retrieved is plenty. Extracting structured data from a web page is challenging problem due to complicated structured pages. Earlier they were used web page programming language dependent; the main problem is to analyze the html source code. In earlier they were considered the scripts such as java scripts and cascade styles in the html files. When it makes different for existing solutions to infer the regularity of the structure of the Web Pages only by analyzing the tag structures. To overcome this problem we are using a new algorithm called VIPS algorithm i.e. independent language. This approach primary utilizes the visual features on the webpage to implement web data extraction.","['P YesuRaju', 'P KiranSree']","IJRET APR 2013,Volume: 2 Issue: 4,635 - 639,ISSN: 2319 - 1163",arXiv,2013,https://doi.org/10.48550/arXiv.1310.6637,Anomali
Towards Applying Text Mining Techniques on Software Quality Standards and Models,"Many of quality approaches are described in hundreds of textual pages. Manual processing of information consumes plenty of resources. In this report we present atextminingapproach applied on CMMI, one well known and widely known quality approach. Thetextmininganalysis can provide a quick overview on the scope of a quality approaches. The result of the analysis could accelerate the understanding and the selection of quality approaches.","['Zádor Dániel Kelemen', 'Rob Kusters', 'Jos Trienekens', 'Katalin Balla']",TR_201302,arXiv,2013,https://doi.org/10.48550/arXiv.1310.5988,Anomali
IntelligentWeb Agent for Search Engines,"In this paper we review studies of the growth of the Internet and technologies that are useful for information search and retrieval on the Web. Search engines are retrieve the efficient information. We collected data on the Internet from several different sources, e.g., current as well as projected number of users, hosts, and Web sites. The trends cited by the sources are consistent and point to exponential growth in the past and in the coming decade. Hence it is not surprising that about 85% of Internet users surveyed claim using search engines and search services to find specific information and users are not satisfied with the performance of the current generation of search engines; the slow retrieval speed, communication delays, and poor quality of retrieved results. Web agents, programs acting autonomously on some task, are already present in the form of spiders, crawler, and robots. Agents offer substantial benefits and hazards, and because of this, their development must involve attention to technical details. This paper illustrates the different types of agents,crawlers, robots,etc forminingthe contents of web in a methodical, automated manner, also discusses the use of crawler to gather specific types of information from Web pages, such as harvesting e-mail addresses","['Avinash N Bhute', 'B. B. Meshram']",,arXiv,2013,https://doi.org/10.48550/arXiv.1310.4774,Anomali
Reading Stockholm Riots 2013 in social media by text-mining,"The riots in Stockholm in May 2013 were an event that reverberated in the world media for its dimension of violence that had spread through the Swedish capital. In this study we have investigated the role of social media in creating media phenomena viatextminingand natural language processing. We have focused on two channels of communication for our analysis: Twitter and Poloniainfo.se (Forum of Polish community in Sweden). Our preliminary results show some hot topics driving discussion related mostly to Swedish Police and Swedish Politics by counting word usage. Typical features for media intervention are presented. We have built networks of most popular phrases, clustered by categories (geography, media institution, etc.). Sentiment analysis shows negative connotation with Police. The aim of this preliminary exploratory quantitative study was to generate questions and hypotheses, which we could carefully follow by deeper more qualitative methods.","['Andrzej Jarynowski', 'Amir Rostami']",,arXiv,2013,https://doi.org/10.48550/arXiv.1310.1249,Anomali
Predicate Logic as a Modeling Language: Modeling and Solving some Machine Learning and Data Mining Problems with IDP3,"This paper provides a gentle introduction to problem solving with the IDP3 system. The core of IDP3 is a finite model generator that supports first order logic enriched with types, inductive definitions, aggregates and partial functions. It offers its users a modeling language that is a slight extension of predicate logic and allows them to solve a wide range of search problems. Apart from a small introductory example, applications are selected from problems that arose within machine learning and dataminingresearch. These research areas have recently shown a strong interest in declarative modeling and constraint solving as opposed to algorithmic approaches. The paper illustrates that the IDP3 system can be a valuable tool for researchers with such an interest.
  The first problem is in the domain of stemmatology, a domain of philology concerned with the relationship between surviving variant versions oftext. The second problem is about a somewhat related problem within biology where phylogenetic trees are used to represent the evolution of species. The third and final problem concerns the classical problem of learning a minimal automaton consistent with a given set of strings. For this last problem, we show that the performance of our solution comes very close to that of a state-of-the art solution. For each of these applications, we analyze the problem, illustrate the development of a logic-based model and explore how alternatives can affect the performance.","['Maurice Bruynooghe', 'Hendrik Blockeel', 'Bart Bogaerts', 'Broes De Cat', 'Stef De Pooter', 'Joachim Jansen', 'Anthony Labarre', 'Jan Ramon', 'Marc Denecker', 'Sicco Verwer']",Theory and Practice of Logic Programming 15 (2014) 783-817,arXiv,2014,https://doi.org/10.48550/arXiv.1309.6883,Anomali
Sentiment Analysis in the News,"Recent years have brought a significant growth in the volume of research in sentiment analysis, mostly on highly subjectivetexttypes (movie or product reviews). The main difference thesetextshave with news articles is that their target is clearly defined and unique across thetext. Following different annotation efforts and the analysis of the issues encountered, we realised that news opinionminingis different from that of othertexttypes. We identified three subtasks that need to be addressed: definition of the target; separation of the good and bad news content from the good and bad sentiment expressed on the target; and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. Furthermore, we distinguish three different possible views on newspaper articles - author, reader andtext, which have to be addressed differently at the time of analysing sentiment. Given these definitions, we present work onminingopinions about entities in English language news, in which (a) we test the relative suitability of various sentiment dictionaries and (b) we attempt to separate positive or negative opinion from good or bad news. In the experiments described here, we tested whether or not subject domain-defining vocabulary should be ignored. Results showed that this idea is more appropriate in the context of news opinionminingand that the approaches taking this into consideration produce a better performance.","['Alexandra Balahur', 'Ralf Steinberger', 'Mijail Kabadjov', 'Vanni Zavarella', 'Erik van der Goot', 'Matina Halkia', 'Bruno Pouliquen', 'Jenya Belyaeva']","Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC'2010), pp. 2216-2220. Valletta, Malta, 19-21 May 2010",arXiv,2013,https://doi.org/10.48550/arXiv.1309.6202,Anomali
Performance Investigation of Feature Selection Methods,"Sentiment analysis or opinionmininghas become an open research domain after proliferation of Internet and Web 2.0 social media. People express their attitudes and opinions on social media including blogs, discussion forums, tweets, etc. and, sentiment analysis concerns about detecting and extracting sentiment or opinion from onlinetext. Sentiment basedtextclassification is different from topicaltextclassification since it involves discrimination based on expressed opinion on a topic. Feature selection is significant for sentiment analysis as the opinionatedtextmay have high dimensions, which can adversely affect the performance of sentiment analysis classifier. This paper explores applicability of feature selection methods for sentiment analysis and investigates their performance for classification in term of recall, precision and accuracy. Five feature selection methods (Document Frequency, Information Gain, Gain Ratio, Chi Squared, and Relief-F) and three popular sentiment feature lexicons (HM, GI and Opinion Lexicon) are investigated on movie reviews corpus with a size of 2000 documents. The experimental results show that Information Gain gave consistent results and Gain Ratio performs overall best for sentimental feature selection while sentiment lexicons gave poor performance. Furthermore, we found that performance of the classifier depends on appropriate number of representative feature selected fromtext.","['Anuj sharma', 'Shubhamoy Dey']",,arXiv,2013,https://doi.org/10.48550/arXiv.1309.3949,Anomali
Using Self-Organizing Maps for Sentiment Analysis,"Web 2.0 services have enabled people to express their opinions, experience and feelings in the form of user-generated content. Sentiment analysis or opinionmininginvolves identifying, classifying and aggregating opinions as per their positive or negative polarity. This paper investigates the efficacy of different implementations of Self-Organizing Maps (SOM) for sentiment based visualization and classification of online reviews. Specifically, this paper implements the SOM algorithm for both supervised and unsupervised learning fromtextdocuments. The unsupervised SOM algorithm is implemented for sentiment based visualization and classification tasks. For supervised sentiment analysis, a competitive learning algorithm known as Learning Vector Quantization is used. Both algorithms are also compared with their respective multi-pass implementations where a quick rough ordering pass is followed by a fine tuning pass. The experimental results on the online movie review data set show that SOMs are well suited for sentiment based classification and sentiment polarity visualization.","['Anuj Sharma', 'Shubhamoy Dey']",,arXiv,2013,https://doi.org/10.48550/arXiv.1309.3946,Anomali
General Purpose Textual Sentiment Analysis and Emotion Detection Tools,"Textual sentiment analysis and emotion detection consists in retrieving the sentiment or emotion carried by atextor document. This task can be useful in many domains: opinionmining, prediction, feedbacks, etc. However, building a general purpose tool for doing sentiment analysis and emotion detection raises a number of issues, theoretical issues like the dependence to the domain or to the language but also pratical issues like the emotion representation for interoperability. In this paper we present our sentiment/emotion analysis tools, the way we propose to circumvent the di culties and the applications they are used for.","['Alexandre Denis', 'Samuel Cruz-Lara', 'Nadia Bellalem']",,arXiv,2013,https://doi.org/10.48550/arXiv.1309.2853,Anomali
Incorporating Text Analysis into Evolution of Social Groups in Blogosphere,Data reflecting social and business relations has often form of network of connections between entities (called social network). In such network important and influential users can be identified as well as groups of strongly connected users. Finding such groups and observing their evolution becomes an increasingly important research problem. One of the significant problems is to develop method incorporating not only information about connections between entities but also information obtained fromtextwritten by the users. Method presented in this paper combine social network analysis andtextminingin order to understand groups evolution.,"['Bogdan Gliwa', 'Anna Zygmunt', 'Stanisław Podgórski']",,arXiv,2013,https://doi.org/10.48550/arXiv.1308.4999,Anomali
Decentralized Online Big Data Classification - a Bandit Framework,"Distributed, online dataminingsystems have emerged as a result of applications requiring analysis of large amounts of correlated and high-dimensional data produced by multiple distributed data sources. We propose a distributed online data classification framework where data is gathered by distributed data sources and processed by a heterogeneous set of distributed learners which learn online, at run-time, how to classify the different data streams either by using their locally available classification functions or by helping each other by classifying each other's data. Importantly, since the data is gathered at different locations, sending the data to another learner to process incurs additional costs such as delays, and hence this will be only beneficial if the benefits obtained from a better classification will exceed the costs. We assume that the classification functions available to each processing element are fixed, but their prediction accuracy for various types of incoming data are unknown and can change dynamically over time, and thus they need to be learned online. We model the problem of joint classification by the distributed and heterogeneous learners from multiple data sources as a distributed contextual bandit problem where each data is characterized by a specific context. We develop distributed online learning algorithms for which we can prove that they have sublinear regret. Compared to prior work in distributed online datamining, our work is the first to provide analytic regret results characterizing the performance of the proposed algorithms.","['Cem Tekin', 'Mihaela van der Schaar']",,arXiv,2013,https://doi.org/10.48550/arXiv.1308.4565,Anomali
Ontology Enrichment by Extracting Hidden Assertional Knowledge from Text,"In this position paper we present a new approach for discovering some special classes of assertional knowledge in thetextby using large RDF repositories, resulting in the extraction of new non-taxonomic ontological relations. Also we use inductive reasoning beside our approach to make it outperform. Then, we prepare a case study by applying our approach on sample data and illustrate the soundness of our proposed approach. Moreover in our point of view current LOD cloud is not a suitable base for our proposal in all informational domains. Therefore we figure out some directions based on prior works to enrich datasets of Linked Data by using webmining. The result of such enrichment can be reused for further relation extraction and ontology enrichment from unstructured freetextdocuments.","['Meisam Booshehri', 'Abbas Malekpour', 'Peter Luksch', 'Kamran Zamanifar', 'Shahdad Shariatmadari']","IJCSIS, 11(5), 64-72",arXiv,2013,https://doi.org/10.48550/arXiv.1308.0701,Anomali
An Approach Finding Frequent Items In Text Or Transactional Data Base By Using BST To Improve The Efficiency Of Apriori Algorithm,"Dataminingtechniques have been widely used in various applications. Binary search tree based frequent items is an effective method for automatically recognize the most frequent items, least frequent items and average frequent items. This paper presents a new approach in order to find out frequent items. The word frequent item refers to how many times the item appeared in the given input. This approach is used to find out item sets in any order using familiar approach binary search tree. The method adapted here is in order to find out frequent items by comparing and incrementing the counter variable in existing transactional data base ortextdata. We are also representing different approaches in frequent item sets and also propose an algorithmic approach for the problem solving",['P Vasanth Sena'],,arXiv,2013,https://doi.org/10.48550/arXiv.1307.7513,Anomali
Designing a Network Based System for Delivery of Remote Mine Services,"There is a great body of work in the areas of tele-assistance/tele-collaboration offering novel and effective ways to improve collaboration between personnel located at a remoteminesite and off-site personnel located in major metropolitan areas. Much of this work involves the use of high-bandwidth communications or targeted sensory experiences using large format displays. There are also existing remote access technologies but these suffer from limited functionality (providingtext, voice, video or one-way desktop sharing), are often poorly supported in the security-conscious corporate environment and require complicated set up processes. There is currently no singular piece of remote collaboration technology that is suitable for the delivery of high-quality planning and scheduling services to clients at aminingsite from a remote operating centre. In response to this issue, as part of a research and technology development effort between CSIRO and aminingengineering firm, we have developed a concept of remoteminingengineer (RME) and conducted a functional requirements analysis for deliveringminingengineering services tominesites remotely. Based on the obtained requirements, a further study was performed to characterise existing technologies and to identify the scope for future work in designing and prototyping a network based system for RME. We report on the method and findings of this study in this paper.","['Craig James', 'Weidong Huang', 'Kazys Stepanas', 'Eleonora Widzyk-Capehart', 'Leila Alem', 'Chris Gunn', 'Matt Adcock', 'Kerstin Haustein']",,arXiv,2013,https://doi.org/10.48550/arXiv.1307.3015,Anomali
Text Categorization via Similarity Search: An Efficient and Effective Novel Algorithm,"We present a supervised learning algorithm fortextcategorization which has brought the team of authors the 2nd place in thetextcategorization division of the 2012 Cybersecurity DataMiningCompetition (CDMC'2012) and a 3rd prize overall. The algorithm is quite different from existing approaches in that it is based on similarity search in the metric space of measure distributions on the dictionary. At the preprocessing stage, given a labeled learning sample oftexts, we associate to every class label (document category) a point in the space of question. Unlike it is usual in clustering, this point is not a centroid of the category but rather an outlier, a uniform measure distribution on a selection of domain-specific words. At the execution stage, an unlabeledtextis assigned atextcategory as defined by the closest labeled neighbour to the point representing the frequency distribution of the words in thetext. The algorithm is both effective and efficient, as further confirmed by experiments on the Reuters 21578 dataset.","['Hubert Haoyang Duan', 'Vladimir Pestov', 'Varun Singla']",,arXiv,2013,https://doi.org/10.48550/arXiv.1307.2669,Anomali
The Application of a Data Mining Framework to Energy Usage Profiling in Domestic Residences using UK data,This paper describes a method for defining representative load profiles for domestic electricity users in the UK. It considers bottom up and clustering methods and then details the research plans for implementing and improving existing framework approaches based on the overall usage profile. The work focuses on adapting and applying analysis framework approaches to UK energy data in order to determine the effectiveness of creating a few (single figures) archetypical users with the intention of improving on the current methods of determining usage profiles. The work is currently in progress and the paper details initial results using data collected in Milton Keynes around 1990. Various possible enhancements to the work are considered including a split based on temperature to reflect the varying UK weather conditions.,"['Ian Dent', 'Uwe Aickelin', 'Tom Rodden']",,arXiv,2013,https://doi.org/10.48550/arXiv.1307.1380,Anomali
Overview of Web Content Mining Tools,"Nowadays, the Web has become one of the most widespread platforms for information change and retrieval. As it becomes easier to publish documents, as the number of users, and thus publishers, increases and as the number of documents grows, searching for information is turning into a cumbersome and time-consuming operation. Due to heterogeneity and unstructured nature of the data available on the WWW, Webmininguses various dataminingtechniques to discover useful knowledge from Web hyperlinks, page content and usage log. The main uses of web contentminingare to gather, categorize, organize and provide the best possible information available on the Web to the user requesting the information. Theminingtools are imperative to scanning the many HTML documents, images, andtext. Then, the result is used by the search engines. In this paper, we first introduce the concepts related to webmining; we then present an overview of different Web ContentMiningtools. We conclude by presenting a comparative table of these tools based on some pertinent criteria.","['Abdelhakim Herrouz', 'Chabane Khentout', 'Mahieddine Djoudi']","The International Journal of Engineering And Science (IJES), Vol.2, Issue 6, June 2013, pp. 106-110, 2013",arXiv,2013,https://doi.org/10.48550/arXiv.1307.1024,Anomali
Arabizi Detection and Conversion to Arabic,"Arabizi is Arabictextthat is written using Latin characters. Arabizi is used to present both Modern Standard Arabic (MSA) or Arabic dialects. It is commonly used in informal settings such as social networking sites and is often with mixed with English. In this paper we address the problems of: identifying Arabizi intextand converting it to Arabic characters. We used word and sequence-level features to identify Arabizi that is mixed with English. We achieved an identification accuracy of 98.5%. As for conversion, we used transliterationminingwith language modeling to generate equivalent Arabictext. We achieved 88.7% conversion accuracy, with roughly a third of errors being spelling and morphological variants of the forms in ground truth.",['Kareem Darwish'],,arXiv,2013,https://doi.org/10.48550/arXiv.1306.6755,Anomali
Clinical Relationships Extraction Techniques from Patient Narratives,"The Clinical E-Science Framework (CLEF) project was used to extract important information from medicaltextsby building a system for the purpose of clinical research, evidence-based healthcare and genotype-meets-phenotype informatics. The system is divided into two parts, one part concerns with the identification of relationships between clinically important entities in thetext. The full parses and domain-specific grammars had been used to apply many approaches to extract the relationship. In the second part of the system, statistical machine learning (ML) approaches are applied to extract relationship. A corpus of oncology narratives that hand annotated with clinical relationships can be used to train and test a system that has been designed and implemented by supervised machine learning (ML) approaches. Many features can be extracted from thesetextsthat are used to build a model by the classifier. Multiple supervised machine learning algorithms can be applied for relationship extraction. Effects of adding the features, changing the size of the corpus, and changing the type of the algorithm on relationship extraction are examined. Keywords:Textmining; information extraction; NLP; entities; and relations.","['Wafaa Tawfik Abdel-moneim', 'Mohamed Hashem Abdel-Aziz', 'Mohamed Monier Hassan']","IJCSI International Journal of Computer Science Issues, Vol.10, Issue 1, January 2013",arXiv,2013,https://doi.org/10.48550/arXiv.1306.5170,Anomali
A Fuzzy Based Approach to Text Mining and Document Clustering,"Fuzzy logic deals with degrees of truth. In this paper, we have shown how to apply fuzzy logic intextminingin order to perform document clustering. We took an example of document clustering where the documents had to be clustered into two categories. The method involved cleaning up thetextand stemming of words. Then, we chose m number of features which differ significantly in their word frequencies (WF), normalized by document length, between documents belonging to these two clusters. The documents to be clustered were represented as a collection of m normalized WF values. Fuzzy c-means (FCM) algorithm was used to cluster these documents into two clusters. After the FCM execution finished, the documents in the two clusters were analysed for the values of their respective m features. It was known that documents belonging to a document type, say X, tend to have higher WF values for some particular features. If the documents belonging to a cluster had higher WF values for those same features, then that cluster was said to represent X. By fuzzy logic, we not only get the cluster name, but also the degree to which a document belongs to a cluster.","['Sumit Goswami', 'Mayank Singh Shishodia']",,arXiv,2013,https://doi.org/10.48550/arXiv.1306.4633,Anomali
Proceedings Fourth International Workshop on Computational Models for Cell Processes,"The fourth international workshop on Computational Models for Cell Processes (CompMod 2013) took place on June 11, 2013 at the Åbo Akademi University, Turku, Finland, in conjunction with iFM 2013. The first edition of the workshop (2008) took place in Turku, Finland, in conjunction with Formal Methods 2008, the second edition (2009) took place in Eindhoven, the Netherlands, as well in conjunction with Formal Methods 2009, and the third one took place in Aachen, Germany, in conjunction with CONCUR 2013. This volume contains the final versions of all contributions accepted for presentation at the workshop.
  The goal of the CompMod workshop series is to bring together researchers in Computer Science and Mathematics (both discrete and continuous), interested in the opportunities and the challenges of Systems Biology. The Program Committee of CompMod 2013 selected 3 papers for presentation at the workshop. In addition, we had two invited talks and five informal presentations. 
  The scientific program of the workshop spans an interesting mix of approaches to systems and even synthetic biology, encompassing several different modeling approaches, ranging from quantitative to qualitative techniques, from continuous to discrete mathematics, and from deterministic to stochastic methods. We thank our invited speakers Daniela Besozzi (Universita degli Studi di Milano, Milano, Italy) and Juho Rousu (Aalto University, Finland) for accepting our invitation and for presenting some of their recent results at CompMod 2013. 
  The technical contributions address the mathematical modeling of the PDGF signalling pathway, the canonical labelling of site graphs, rule-based modeling of polymerization reactions, rule-based modeling as a platform for the analysis of synthetic self-assembled nano-systems, robustness analysis of stochastic systems, an algebraic approach to gene assembly in ciliates, and large-scaletextminingof biomedical literature.",['Ion Petre'],"EPTCS 116, 2013",arXiv,2013,https://doi.org/10.48550/arXiv.1306.2019,Anomali
Mining top-k granular association rules for recommendation,"Recommender systems are important for e-commerce companies as well as researchers. Recently, granular association rules have been proposed for cold-start recommendation. However, existing approaches reserve only globally strong rules; therefore some users may receive no recommendation at all. In this paper, we propose tominethe top-k granular association rules for each user. First we define three measures of granular association rules. These are the source coverage which measures the user granule size, the target coverage which measures the item granule size, and the confidence which measures the strength of the association. With the confidence measure, rules can be ranked according to their strength. Then we propose algorithms for training the recommender and suggesting items to each user. Experimental are undertaken on a publicly available data set MovieLens. Results indicate that the appropriate setting of granule can avoid over-fitting and at the same time, help obtaining high recommending accuracy.","['Fan Min', 'William Zhu']",,arXiv,2013,https://doi.org/10.48550/arXiv.1305.4801,Anomali
A Mining-Based Compression Approach for Constraint Satisfaction Problems,"In this paper, we propose an extension of ourMiningfor SAT framework to Constraint satisfaction Problem (CSP). We consider n-ary extensional constraints (table constraints). Our approach aims to reduce the size of the CSP by exploiting the structure of the constraints graph and of its associated microstructure. More precisely, we apply itemsetminingtechniques to search for closed frequent itemsets on these two representation. Using Tseitin extension, we rewrite the whole CSP to another compressed CSP equivalent with respect to satisfiability. Our approach contrast with previous proposed approach by Katsirelos and Walsh, as we do not change the structure of the constraints.","['Said Jabbour', 'Lakhdar Sais', 'Yakoub Salhi']",,arXiv,2013,https://doi.org/10.48550/arXiv.1305.3321,Anomali
Somoclu: An Efficient Parallel Library for Self-Organizing Maps,"Somoclu is a massively parallel tool for training self-organizing maps on large data sets written in C++. It builds on OpenMP for multicore execution, and on MPI for distributing the workload across the nodes in a cluster. It is also able to boost training by using CUDA if graphics processing units are available. A sparse kernel is included, which is useful for high-dimensional but sparse data, such as the vector spaces common intextminingworkflows. Python, R and MATLAB interfaces facilitate interactive use. Apart from fast execution, memory use is highly optimized, enabling training large emergent maps even on a single computer.","['Peter Wittek', 'Shi Chao Gao', 'Ik Soo Lim', 'Li Zhao']","Journal of Statistical Software, 78(9), 1-21 (2017)",arXiv,2017,https://doi.org/10.48550/arXiv.1305.1422,Anomali
Web Services Dependency Networks Analysis,"Along with a continuously growing number of publicly available Web services (WS), we are witnessing a rapid development in semantic-related web technologies, which lead to the apparition of semantically described WS. In this work, we perform a comparative analysis of the syntactic and semantic approaches used to describe WS, from a complex network perspective. First, we extract syntactic and semantic WS dependency networks from a collection of publicly available WS descriptions. Then, we take advantage of tools from the complex network field to analyze them and determine their topological properties. We show WS dependency networks exhibit some of the typical characteristics observed in real-world networks, such as small world and scale free properties, as well as community structure. By comparing syntactic and semantic networks through their topological properties, we show the introduction of semantics in WS description allows modeling more accurately the dependencies between parameters, which in turn could lead to improved compositionminingmethods.","['Chantal Cherifi1', 'Vincent Labatut', 'Jean-François Santucci']","International Conference of New Media and Interactivity (NMI), pp. 115-120 (2010)",arXiv,2013,https://doi.org/10.48550/arXiv.1305.0261,Anomali
"Spaces, Trees and Colors: The Algorithmic Landscape of Document Retrieval on Sequences","Document retrieval is one of the best established information retrieval activities since the sixties, pervading all search engines. Its aim is to obtain, from a collection oftextdocuments, those most relevant to a pattern query. Current technology is mostly oriented to ""natural language""textcollections, where inverted indices are the preferred solution. As successful as this paradigm has been, it fails to properly handle some East Asian languages and other scenarios where the ""natural language"" assumptions do not hold. In this survey we cover the recent research in extending the document retrieval techniques to a broader class of sequence collections, which has applications bioinformatics, data and Webmining, chemoinformatics, software engineering, multimedia information retrieval, and many others. We focus on the algorithmic aspects of the techniques, uncovering a rich world of relations between document retrieval challenges and fundamental problems on trees, strings, range queries, discrete geometry, and others.",['Gonzalo Navarro'],,arXiv,2013,https://doi.org/10.48550/arXiv.1304.6023,Anomali
Analytic Feature Selection for Support Vector Machines,"Support vector machines (SVMs) rely on the inherent geometry of a data set to classify training data. Because of this, we believe SVMs are an excellent candidate to guide the development of an analytic feature selection algorithm, as opposed to the more commonly used heuristic methods. We propose a filter-based feature selection algorithm based on the inherent geometry of a feature set. Through observation, we identified six geometric properties that differ between optimal and suboptimal feature sets, and have statistically significant correlations to classifier performance. Our algorithm is based on logistic and linear regression models using these six geometric properties as predictor variables. The proposed algorithm achieves excellent results on high dimensionaltextdata sets, with features that can be organized into a handful of feature types; for example, unigrams, bigrams or semantic structural features. We believe this algorithm is a novel and effective approach to solving the feature selection problem for linear SVMs.","['Carly Stambaugh', 'Hui Yang', 'Felix Breuer']",,arXiv,2013,https://doi.org/10.48550/arXiv.1304.5678,Anomali
Sentiment Analysis : A Literature Survey,"Our day-to-day life has always been influenced by what people think. Ideas and opinions of others have always affected our own opinions. The explosion of Web 2.0 has led to increased activity in Podcasting, Blogging, Tagging, Contributing to RSS, Social Bookmarking, and Social Networking. As a result there has been an eruption of interest in people tominethese vast resources of data for opinions. Sentiment Analysis or OpinionMiningis the computational treatment of opinions, sentiments and subjectivity oftext. In this report, we take a look at the various challenges and applications of Sentiment Analysis. We will discuss in details various approaches to perform a computational treatment of sentiments and opinions. Various supervised or data-driven techniques to SA like Naïve Byes, Maximum Entropy, SVM, and Voted Perceptrons will be discussed and their strengths and drawbacks will be touched upon. We will also see a new dimension of analyzing sentiments by Cognitive Psychology mainly through the work of Janyce Wiebe, where we will see ways to detect subjectivity, perspective in narrative and understanding the discourse structure. We will also study some specific topics in Sentiment Analysis and the contemporary works in those areas.","['Subhabrata Mukherjee', 'Pushpak Bhattacharyya']",,arXiv,2013,https://doi.org/10.48550/arXiv.1304.4520,Anomali
Towards more accurate clustering method by using dynamic time warping,"An intrinsic problem of classifiers based on machine learning (ML) methods is that their learning time grows as the size and complexity of the training dataset increases. For this reason, it is important to have efficient computational methods and algorithms that can be applied on large datasets, such that it is still possible to complete the machine learning tasks in reasonable time. In this context, we present in this paper a more accurate simple process to speed up ML methods. An unsupervised clustering algorithm is combined with Expectation, Maximization (EM) algorithm to develop an efficient Hidden Markov Model (HMM) training. The idea of the proposed process consists of two steps. In the first step, training instances with similar inputs are clustered and a weight factor which represents the frequency of these instances is assigned to each representative cluster. Dynamic Time Warping technique is used as a dissimilarity function to cluster similar examples. In the second step, all formulas in the classical HMM training algorithm (EM) associated with the number of training instances are modified to include the weight factor in appropriate terms. This process significantly accelerates HMM training while maintaining the same initial, transition and emission probabilities matrixes as those obtained with the classical HMM training algorithm. Accordingly, the classification accuracy is preserved. Depending on the size of the training set, speedups of up to 2200 times is possible when the size is about 100.000 instances. The proposed approach is not limited to training HMMs, but it can be employed for a large variety of MLs methods.",['Khadoudja Ghanem'],"International Journal of Data Mining & Knowledge Management Process (IJDKP) Vol.3, No.2, March 2013",arXiv,2013,https://doi.org/10.48550/arXiv.1304.3745,Anomali
"Data, text and web mining for business intelligence: a survey","The Information and Communication Technologies revolution brought a digital world with huge amounts of data available. Enterprises useminingtechnologies to search vast amounts of data for vital insight and knowledge.Miningtools such as datamining,textmining, and webminingare used to find hidden knowledge in large databases or the Internet.",['Abdul-Aziz Rashid Al-Azmi'],"International Journal of Data Mining & Knowledge Management Process (IJDKP) Vol.3, No.2, March 2013",arXiv,2013,https://doi.org/10.48550/arXiv.1304.3563,Anomali
Scalable Text and Link Analysis with Mixed-Topic Link Models,"Many data sets contain rich information about objects, as well as pairwise relations between them. For instance, in networks of websites, scientific papers, and other documents, each node has content consisting of a collection of words, as well as hyperlinks or citations to other nodes. In order to perform inference on such data sets, and make predictions and recommendations, it is useful to have models that are able to capture the processes which generate thetextat each node and the links between them. In this paper, we combine classic ideas in topic modeling with a variant of the mixed-membership block model recently developed in the statistical physics community. The resulting model has the advantage that its parameters, including the mixture of topics of each document and the resulting overlapping communities, can be inferred with a simple and scalable expectation-maximization algorithm. We test our model on three data sets, performing unsupervised topic classification and link prediction. For both tasks, our model outperforms several existing state-of-the-art methods, achieving higher accuracy with significantly less computation, analyzing a data set with 1.3 million words and 44 thousand links in a few minutes.","['Yaojia Zhu', 'Xiaoran Yan', 'Lise Getoor', 'Cristopher Moore']","Proc. 19th SIGKDD Conference on Knowledge Discovery and Data Mining (KDD) 2013, 473-481",arXiv,2013,https://doi.org/10.48550/arXiv.1303.7264,Anomali
An improved semantic similarity measure for document clustering based on topic maps,"A major computational burden, while performing document clustering, is the calculation of similarity measure between a pair of documents. Similarity measure is a function that assigns a real number between 0 and 1 to a pair of documents, depending upon the degree of similarity between them. A value of zero means that the documents are completely dissimilar whereas a value of one indicates that the documents are practically identical. Traditionally, vector-based models have been used for computing the document similarity. The vector-based models represent several features present in documents. These approaches to similarity measures, in general, cannot account for the semantics of the document. Documents written in human languages contain contexts and the words used to describe these contexts are generally semantically related. Motivated by this fact, many researchers have proposed seman-tic-based similarity measures by utilizingtextannotation through external thesauruses like WordNet (a lexical database). In this paper, we define a semantic similarity measure based on documents represented in topic maps. Topic maps are rapidly becoming an industrial standard for knowledge representation with a focus for later search and extraction. The documents are transformed into a topic map based coded knowledge and the similarity between a pair of documents is represented as a correlation between the common patterns (sub-trees). The experimental studies on thetextminingdatasets reveal that this new similarity measure is more effective as compared to commonly used similarity measures intextclustering.","['Muhammad Rafi', 'Mohammad Shahid Shaikh']",,arXiv,2013,https://doi.org/10.48550/arXiv.1303.4087,Anomali
A Hybrid Approach to Extract Keyphrases from Medical Documents,"Keyphrases are the phrases, consisting of one or more words, representing the important concepts in the articles. Keyphrases are useful for a variety of tasks such astextsummarization, automatic indexing, clustering/classification,textminingetc. This paper presents a hybrid approach to keyphrase extraction from medical documents. The keyphrase extraction approach presented in this paper is an amalgamation of two methods: the first one assigns weights to candidate keyphrases based on an effective combination of features such as position, term frequency, inverse document frequency and the second one assign weights to candidate keyphrases using some knowledge about their similarities to the structure and characteristics of keyphrases available in the memory (stored list of keyphrases). An efficient candidate keyphrase identification method as the first component of the proposed keyphrase extraction system has also been introduced in this paper. The experimental results show that the proposed hybrid approach performs better than some state-of-the art keyphrase extraction approaches.",['Kamal Sarkar'],"International Journal of Computer Applications 63(18):14-19, February 2013. Published by Foundation of Computer Science, New York, USA",arXiv,2014,https://doi.org/10.48550/arXiv.1303.1441,Anomali
A Semantic approach for effective document clustering using WordNet,"Now a days, thetextdocument is spontaneously increasing over the internet, e-mail and web pages and they are stored in the electronic database format. To arrange and browse the document it becomes difficult. To overcome such problem the document preprocessing, term selection, attribute reduction and maintaining the relationship between the important terms using background knowledge, WordNet, becomes an important parameters in datamining. In these paper the different stages are formed, firstly the document preprocessing is done by removing stop words, stemming is performed using porter stemmer algorithm, word net thesaurus is applied for maintaining relationship between the important terms, global unique words, and frequent word sets get generated, Secondly, data matrix is formed, and thirdly terms are extracted from the documents by using term selection approaches tf-idf, tf-df, and tf2 based on their minimum threshold value. Further each and every document terms gets preprocessed, where the frequency of each term within the document is counted for representation. The purpose of this approach is to reduce the attributes and find the effective term selection method using WordNet for better clustering accuracy. Experiments are evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and ship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group (Hardware), 20 News group (Computer Graphics) etc.","['Leena H. Patil', 'Mohammed Atique']",,arXiv,2013,https://doi.org/10.48550/arXiv.1303.0489,Anomali
Onomastics 2.0 - The Power of Social Co-Occurrences,"Onomastics is ""the science or study of the origin and forms of proper names of persons or places."" [""Onomastics"". Merriam-Webster.com, 2013. http://www.merriam-webster.com (11 February 2013)]. Especially personal names play an important role in daily life, as all over the world future parents are facing the task of finding a suitable given name for their child. This choice is influenced by different factors, such as the social context, language, cultural background and, in particular, personal taste.
  With the rise of the Social Web and its applications, users more and more interact digitally and participate in the creation of heterogeneous, distributed, collaborative data collections. These sources of data also reflect current and new naming trends as well as new emerging interrelations among names.
  The present work shows, how basic approaches from the field of social network analysis and information retrieval can be applied for discovering relations among names, thus extending Onomastics by dataminingtechniques. The considered approach starts with building co-occurrence graphs relative to data from the Social Web, respectively for given names and city names. As a main result, correlations between semantically grounded similarities among names (e.g., geographical distance for city names) and structural graph based similarities are observed.
  The discovered relations among given names are the foundation of ""nameling"" [http://nameling.net], a search engine and academic research platform for given names which attracted more than 30,000 users within four months, underpinningthe relevance of the proposed methodology.","['Folke Mitzlaff', 'Gerd Stumme']",,arXiv,2013,https://doi.org/10.48550/arXiv.1303.0484,Anomali
Detecting and resolving spatial ambiguity in text using named entity extraction and self learning fuzzy logic techniques,Information extraction identifies useful and relevanttextin a document and converts unstructuredtextinto a form that can be loaded into a database table. Named entity extraction is a main task in the process of information extraction and is a classification problem in which words are assigned to one or more semantic classes or to a default non-entity class. A word which can belong to one or more classes and which has a level of uncertainty in it can be best handled by a self learning Fuzzy Logic Technique. This paper proposes a method for detecting the presence of spatial uncertainty in thetextand dealing with spatial ambiguity using named entity extraction techniques coupled with self learning fuzzy logic techniques,"['Kanagavalli V R', 'Raja. K']",ISBN 978-81-909042-5-4 P.no.71-76,arXiv,2013,https://doi.org/10.48550/arXiv.1303.0445,Anomali
A Fuzzy Logic based Method for Efficient Retrieval of Vague and Uncertain Spatial Expressions in Text Exploiting the Granulation of the Spatial Event Queries,"The arrangement of things in n-dimensional space is specified as Spatial. Spatial data consists of values that denote the location and shape of objects and areas on the earths surface. Spatial information includes facts such as location of features, the relationship of geographic features and measurements of geographic features. The spatial cognition is a primal area of study in various other fields such as Robotics, Psychology, Geosciences, Geography, Political Sciences, Geographic Economy, Environmental,Miningand Petroleum Engineering, Natural Resources, Epidemiology, Demography etc., Anytextdocument which contains physical location specifications such as place names, geographic coordinates, landmarks, country names etc., are supposed to contain the spatial information. The spatial information may also be represented using vague or fuzzy descriptions involving linguistic terms such as near to, far from, to the east of, very close. Given a query involving events, the aim of this ongoing research work is to extract the relevant information from multipletextdocuments, resolve the uncertainty and vagueness and translate them in to locations in a map. The input to the system would be atextCorpus and a Spatial Query event. The output of the system is a map showing the most possible, disambiguated location of the event queried. The author proposes Fuzzy Logic Techniques for resolving the uncertainty in the spatial expressions.","['Kanagavalli. V. R', 'Raja. K']",,arXiv,2013,https://doi.org/10.48550/arXiv.1302.6340,Anomali
Alternating proximal gradient method for sparse nonnegative Tucker decomposition,"Multi-way data arises in many applications such as electroencephalography (EEG) classification, face recognition,textminingand hyperspectral data analysis. Tensor decomposition has been commonly used to find the hidden factors and elicit the intrinsic structures of the multi-way data. This paper considers sparse nonnegative Tucker decomposition (NTD), which is to decompose a given tensor into the product of a core tensor and several factor matrices with sparsity and nonnegativity constraints. An alternating proximal gradient method (APG) is applied to solve the problem. The algorithm is then modified to sparse NTD with missing values. Per-iteration cost of the algorithm is estimated scalable about the data size, and global convergence is established under fairly loose conditions. Numerical experiments on both synthetic and real world data demonstrate its superiority over a few state-of-the-art methods for (sparse) NTD from partial and/or full observations. The MATLAB code along with demos are accessible from the author's homepage.",['Yangyang Xu'],,arXiv,2014,https://doi.org/10.48550/arXiv.1302.2559,Anomali
Arabic text summarization based on latent semantic analysis to enhance arabic documents clustering,"Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (IR) systems especially with the rapid growth of the number of online documents present in Arabic language. Documents clustering aim to automatically group similar documents in one cluster using different similarity/distance measures. This task is often affected by the documents length, useful information on the documents is often accompanied by a large amount of noise, and therefore it is necessary to eliminate this noise while keeping useful information to boost the performance of Documents clustering. In this paper, we propose to evaluate the impact oftextsummarization using the Latent Semantic Analysis Model on Arabic Documents Clustering in order to solve problems cited above, using five similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler Divergence, for two times: without and with stemming. Our experimental results indicate that our proposed approach effectively solves the problems of noisy information and documents length, and thus significantly improve the clustering performance.","['Hanane Froud', 'Abdelmonaime Lachkar', 'Said Alaoui Ouatik']",International Journal of Data Mining & Knowledge Management Process (IJDKP)- 2013,arXiv,2013,https://doi.org/10.48550/arXiv.1302.1612,Anomali
An alternative text representation to TF-IDF and Bag-of-Words,"Intextmining, information retrieval, and machine learning,textdocuments are commonly represented through variants of sparse Bag of Words (sBoW) vectors (e.g. TF-IDF). Although simple and intuitive, sBoW style representations suffer from their inherent over-sparsity and fail to capture word-level synonymy and polysemy. Especially when labeled data is limited (e.g. in document classification), or thetextdocuments are short (e.g. emails or abstracts), many features are rarely observed within the training corpus. This leads to overfitting and reduced generalization accuracy. In this paper we propose Dense Cohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoW document features. dCoT explicitly models absent words by removing and reconstructing random sub-sets of words in the unlabeled corpus. With this approach, dCoT learns to reconstruct frequent words from co-occurring infrequent words and maps the high dimensional sparse sBoW vectors into a low-dimensional dense representation. We show that the feature removal can be marginalized out and that the reconstruction can be solved for in closed-form. We demonstrate empirically, on several benchmark datasets, that dCoT features significantly improve the classification accuracy across several document classification tasks.","['Zhixiang', 'Xu', 'Minmin Chen', 'Kilian Q. Weinberger', 'Fei Sha']",,arXiv,2013,https://doi.org/10.48550/arXiv.1301.6770,Anomali
Transfer Topic Modeling with Ease and Scalability,"The increasing volume of shorttextsgenerated on social media sites, such as Twitter or Facebook, creates a great demand for effective and efficient topic modeling approaches. While latent Dirichlet allocation (LDA) can be applied, it is not optimal due to its weakness in handling shorttextswith fast-changing topics and scalability concerns. In this paper, we propose a transfer learning approach that utilizes abundant labeled documents from other domains (such as Yahoo! News or Wikipedia) to improve topic modeling, with better model fitting and result interpretation. Specifically, we develop Transfer Hierarchical LDA (thLDA) model, which incorporates the label information from other domains via informative priors. In addition, we develop a parallel implementation of our model for large-scale applications. We demonstrate the effectiveness of our thLDA model on both a microblogging dataset and standardtextcollections including AP and RCV1 datasets.","['Jeon-Hyung Kang', 'Jun Ma', 'Yan Liu']",,arXiv,2013,https://doi.org/10.48550/arXiv.1301.5686,Anomali
The Diagonalized Newton Algorithm for Nonnegative Matrix Factorization,"Non-negative matrix factorization (NMF) has become a popular machine learning approach to many problems intextmining, speech and image processing, bio-informatics and seismic data analysis to name a few. In NMF, a matrix of non-negative data is approximated by the low-rank product of two matrices with non-negative entries. In this paper, the approximation quality is measured by the Kullback-Leibler divergence between the data and its low-rank reconstruction. The existence of the simple multiplicative update (MU) algorithm for computing the matrix factors has contributed to the success of NMF. Despite the availability of algorithms showing faster convergence, MU remains popular due to its simplicity. In this paper, a diagonalized Newton algorithm (DNA) is proposed showing faster convergence while the implementation remains simple and suitable for high-rank problems. The DNA algorithm is applied to various publicly available data sets, showing a substantial speed-up on modern hardware.",['Hugo Van hamme'],,arXiv,2013,https://doi.org/10.48550/arXiv.1301.3389,Anomali
Matrix Approximation under Local Low-Rank Assumption,"Matrix approximation is a common tool in machine learning for building accurate prediction models for recommendation systems,textmining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is only locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy in recommendation tasks.","['Joonseok Lee', 'Seungyeon Kim', 'Guy Lebanon', 'Yoram Singer']",,arXiv,2013,https://doi.org/10.48550/arXiv.1301.3192,Anomali
A comparative study of root-based and stem-based approaches for measuring the similarity between arabic words for arabic text mining applications,"Representation of semantic information contained in the words is needed for any ArabicTextMiningapplications. More precisely, the purpose is to better take into account the semantic dependencies between words expressed by the co-occurrence frequencies of these words. There have been many proposals to compute similarities between words based on their distributions in contexts. In this paper, we compare and contrast the effect of two preprocessing techniques applied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming) approaches for measuring the similarity between Arabic words with the well known abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of distance functions and similarity measures, such as the Euclidean Distance, Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation Coefficient. The obtained results show that, on the one hand, the variety of the corpus produces more accurate results; on the other hand, the Stem-based approach outperformed the Root-based one because this latter affects the words meanings.","['Hanane Froud', 'Abdelmonaim Lachkar', 'Said Alaoui Ouatik']","Advanced Computing An International Journal (ACIJ), November 2012, Volume 3, Number 6",arXiv,2012,https://doi.org/10.48550/arXiv.1212.3634,Anomali
A Comparative Study of Discretization Approaches for Granular Association Rule Mining,"Granular association ruleminingis a new relational dataminingapproach to reveal patterns hidden in multiple tables. The current research of granular association ruleminingconsiders only nominal data. In this paper, we study the impact of discretization approaches onminingsemantically richer and stronger rules from numeric data. Specifically, the Equal Width approach and the Equal Frequency approach are adopted and compared. The setting of interval numbers is a key issue in discretization approaches, so we compare different settings through experiments on a well-known real life data set. Experimental results show that: 1) discretization is an effective preprocessing technique inminingstronger rules; 2) the Equal Frequency approach helps generating more rules than the Equal Width approach; 3) with certain settings of interval numbers, we can obtain much more rules than others.","['Xu He', 'Fan Min', 'William Zhu']",,arXiv,2012,https://doi.org/10.48550/arXiv.1212.0190,Anomali
An Approach of Improving Students Academic Performance by using k means clustering algorithm and Decision tree,"Improving students academic performance is not an easy task for the academic community of higher learning. The academic performance of engineering and science students during their first year at university is a turning point in their educational path and usually encroaches on their General Point Average,GPA in a decisive manner. The students evaluation factors like class quizzes mid and final exam assignment lab work are studied. It is recommended that all these correlated information should be conveyed to the class teacher before the conduction of final exam. This study will help the teachers to reduce the drop out ratio to a significant level and improve the performance of students. In this paper, we present a hybrid procedure based on Decision Tree of Dataminingmethod and Data Clustering that enables academicians to predict students GPA and based on that instructor can take necessary step to improve student academic performance.","['Md. Hedayetul Islam Shovon', 'Mahfuza Haque']","International Journal of Advanced Computer Science and Applications(IJACSA),Vol. 3, No. 8, Page no. 146-149, 2012",arXiv,2012,https://doi.org/10.48550/arXiv.1211.6340,Anomali
Finding influential users of an online health community: a new metric based on sentiment influence,"What characterizes influential users in online health communities (OHCs)? We hypothesize that (1) the emotional support received by OHC members can be assessed from their sentiment ex-pressed in online interactions, and (2) such assessments can help to identify influential OHC members. Throughtextminingand sentiment analysis of users' online interactions, we propose a novel metric that directly measures a user's ability to affect the sentiment of others. Using dataset from an OHC, we demonstrate that this metric is highly effective in identifying influential users. In addition, combining the metric with other traditional measures further improves the identification of influential users. This study can facilitate online community management and advance our understanding of social influence in OHCs.","['Kang Zhao', 'Greta Greer', 'Baojun Qiu', 'Prasenjit Mitra', 'Kenneth Portier', 'John Yen']",,arXiv,2012,https://doi.org/10.48550/arXiv.1211.6086,Anomali
A Brief Review of Data Mining Application Involving Protein Sequence Classification,"Dataminingtechniques have been used by researchers for analyzing protein sequences. In protein analysis, especially in protein sequence classification, selection of feature is most important. Popular protein sequence classification techniques involve extraction of specific features from the sequences. Researchers apply some well-known classification techniques like neural networks, Genetic algorithm, Fuzzy ARTMAP, Rough Set Classifier etc for accurate classification. This paper presents a review is with three different classification models such as neural network model, fuzzy ARTMAP model and Rough set classifier model. A new technique for classifying protein sequences have been proposed in the end. The proposed technique tries to reduce the computational overheads encountered by earlier approaches and increase the accuracy of classification.","['Suprativ Saha', 'Rituparna Chaki']",,arXiv,2012,https://doi.org/10.48550/arXiv.1211.4866,Anomali
Inference of the Russian drug community from one of the largest social networks in the Russian Federation,"The criminal nature of narcotics complicates the direct assessment of a drug community, while having a good understanding of the type of people drawn or currently using drugs is vital for finding effective intervening strategies. Especially for the Russian Federation this is of immediate concern given the dramatic increase it has seen in drug abuse since the fall of the Soviet Union in the early nineties. Using unique data from the Russian social network 'LiveJournal' with over 39 million registered users worldwide, we were able for the first time to identify the on-line drug community by context sensitivetextminingof the users' blogs using a dictionary of known drug-related official and 'slang' terminology. By comparing the interests of the users that most actively spread information on narcotics over the network with the interests of the individuals outside the on-line drug community, we found that the 'average' drug user in the Russian Federation is generally mostly interested in topics such as Russian rock, non-traditional medicine, UFOs, Buddhism, yoga and the occult. We identify three distinct scale-free sub-networks of users which can be uniquely classified as being either 'infectious', 'susceptible' or 'immune'.","['L. J. Dijkstra', 'A. V. Yakushev', 'P. A. C. Duijn', 'A. V. Boukhanovsky', 'P. M. A. Sloot']",,arXiv,2013,https://doi.org/10.48550/arXiv.1211.4783,Anomali
A New Similarity Measure for Taxonomy Based on Edge Counting,"This paper introduces a new similarity measure based on edge counting in a taxonomy like WorldNet or Ontology. Measurement of similarity betweentextsegments or concepts is very useful for many applications like information retrieval, ontology matching,textmining, and question answering and so on. Several measures have been developed for measuring similarity between two concepts: out of these we see that the measure given by Wu and Palmer [1] is simple, and gives good performance. Our measure is based on their measure but strengthens it. Wu and Palmer [1] measure has a disadvantage that it does not consider how far the concepts are semantically. In our measure we include the shortest path between the concepts and the depth of whole taxonomy together with the distances used in Wu and Palmer [1]. Also the measure has following disadvantage i.e. in some situations, the similarity of two elements of an IS-A ontology contained in the neighborhood exceeds the similarity value of two elements contained in the same hierarchy. Our measure introduces a penalization factor for this case based upon shortest length between the concepts and depth of whole taxonomy.","['Manjula Shenoy. K', 'K. C. Shet', 'U. Dinesh Acharya']",,arXiv,2012,https://doi.org/10.48550/arXiv.1211.4709,Anomali
An Effective Method for Fingerprint Classification,"This paper presents an effective method for fingerprint classification using dataminingapproach. Initially, it generates a numeric code sequence for each fingerprint image based on the ridge flow patterns. Then for each class, a seed is selected by using a frequent itemsets generation technique. These seeds are subsequently used for clustering the fingerprint images. The proposed method was tested and evaluated in terms of several real-life datasets and a significant improvement in reducing the misclassification errors has been noticed in comparison to its other counterparts.","['Monowar H. Bhuyan', 'Sarat Saharia', 'Dhruba Kr Bhattacharyya']","International A. Journal of e-Technology, Vol. 1, No. 3, pp. 89-97, January, 2010",arXiv,2012,https://doi.org/10.48550/arXiv.1211.4658,Anomali
Automating Legal Research through Data Mining,"The term legal research generally refers to the process of identifying and retrieving appropriate information necessary to support legal decision making from past case records. At present, the process is mostly manual, but some traditional technologies such as keyword searching are commonly used to speed the process up. But a keyword search is not a comprehensive search to cater to the requirements of legal research as the search result includes too many false hits in terms of irrelevant case records. Hence the present generic tools cannot be used to automate legal research.
  This paper presents a framework which was developed by combining severalTextMiningtechniques to automate the process overcoming the difficulties in the existing methods. Further, the research also identifies the possible enhancements that could be done to enhance the effectiveness of the framework.",['Mohamed Firdhous'],"International Journal of Advanced Computer Science and Applications (IJACSA) Vol. 1, No 6, December 2010, pp. 9-16",arXiv,2012,https://doi.org/10.48550/arXiv.1211.1861,Anomali
Link Prediction in Complex Networks by Multi Degree Preferential-Attachment Indices,"In principle, the rules of links formation of a network model can be considered as a kind of link prediction algorithm. By revisiting the preferential attachment mechanism for generating a scale-free network, here we propose a class of preferential attachment indices which are different from the previous one. Traditionally, the preferential attachment index is defined by the product of the related nodes degrees, while the new indices will define the similarity score of a pair of nodes by either the maximum in the two nodes degrees or the summarization of their degrees. Extensive experiments are carried out on fourteen real-world networks. Compared with the traditional preferential attachment index, the new ones, especially the degree-summarization similarity index, can provide more accurate prediction on most of the networks. Due to the improved prediction accuracy and low computational complexity, these proposed preferential attachment indices may be of help to provide an instruction forminingunknown links in incomplete networks.","['Ke Hu', 'Ju Xiang', 'Wanchun Yang', 'Xiaoke Xu', 'Yi Tang']",,arXiv,2012,https://doi.org/10.48550/arXiv.1211.1790,Anomali
An Adaptive parameter free data mining approach for healthcare application,"In today's world, healthcare is the most important factor affecting human life. Due to heavy work load it is not possible for personal healthcare. The proposed system acts as a preventive measure for determining whether a person is fit or unfit based on person's historical and real time data by applying clustering algorithms like K-means and D-stream. The Density-based clustering algorithm i.e. the D-stream algorithm overcomes drawbacks of K-Means algorithm. By calculating their performance measures we finally find out effectiveness and efficiency of both the algorithms. Both clustering algorithms are applied on patient's bio-medical historical database. To check the correctness of both the algorithms, we apply them on patient's current bio-medical data.","['Dipti Patil', 'Dr. Vijay M. Wadhai', 'Mayuri Gund', 'Richa Biyani', 'Snehal Andhalkar', 'Bhagyashree Agrawal']",,arXiv,2012,https://doi.org/10.48550/arXiv.1211.1788,Anomali
Illustrating a neural model of logic computations: The case of Sherlock Holmes' old maxim,"Natural languages can express some logical propositions that humans are able to understand. We illustrate this fact with a famoustextthat Conan Doyle attributed to Holmes: 'It is an old maxim ofminethat when you have excluded the impossible, whatever remains, however improbable, must be the truth'. This is a subtle logical statement usually felt as an evident truth. The problem we are trying to solve is the cognitive reason for such a feeling. We postulate here that we accept Holmes' maxim as true because our adult brains are equipped with neural modules that naturally perform modal logical computations.",['Eduardo Mizraji'],THEORIA 31/1 (2016): 7-25,arXiv,2016,https://doi.org/10.48550/arXiv.1210.7495,Anomali
Deterministic algorithms for skewed matrix products,"Recently, Pagh presented a randomized approximation algorithm for the multiplication of real-valued matrices building upon work for detecting the most frequent items in data streams. We continue this line of research and present new {\em deterministic} matrix multiplication algorithms.
  Motivated by applications in datamining, we first consider the case of real-valued, nonnegative $n$-by-$n$ input matrices $A$ and $B$, and show how to obtain a deterministic approximation of the weights of individual entries, as well as the entrywise $p$-norm, of the product $AB$. The algorithm is simple, space efficient and runs in one pass over the input matrices. For a user defined $b \in (0, n^2)$ the algorithm runs in time$O(nb + n\cdot\text{Sort}(n))$and space $O(n + b)$ and returns an approximation of the entries of $AB$ within an additive factor of $\|AB\|_{E1}/b$, where $\|C\|_{E1} = \sum_{i, j} |C_{ij}|$ is the entrywise 1-norm of a matrix $C$ and$\text{Sort}(n)$is the time required to sort $n$ real numbers in linear space. Building upon a result by Berinde et al. we show that for skewed matrix products (a common situation in many real-life applications) the algorithm is more efficient and achieves better approximation guarantees than previously known randomized algorithms.
  When the input matrices are not restricted to nonnegative entries, we present a new deterministic group testing algorithm detecting nonzero entries in the matrix product with large absolute value. The algorithm is clearly outperformed by randomized matrix multiplication algorithms, but as a byproduct we obtain the first $O(n^{2 + \varepsilon})$-time deterministic algorithm for matrix products with $O(\sqrt{n})$ nonzero entries.",['Konstantin Kutzkov'],,arXiv,2012,https://doi.org/10.48550/arXiv.1209.4508,Anomali
Hybrid Data Mining Technique for Knowledge Discovery from Engineering Materials' Data sets,Studying materials informatics from a dataminingperspective can be beneficial for manufacturing and other industrial engineering applications. Predictive dataminingtechnique and machine learning algorithm are combined to design a knowledge discovery system for the selection of engineering materials that meet the design specifications. Predictive method-Naive Bayesian classifier and Machine learning Algorithm - Pearson correlation coefficient method were implemented respectively for materials classification and selection. The knowledge extracted from the engineering materials data sets is proposed for effective decision making in advanced engineering materials design applications.,"['Doreswamy', 'Hemanth K. S']",,arXiv,2012,https://doi.org/10.48550/arXiv.1209.4169,Anomali
Cultural Algorithm Toolkit for Multi-objective Rule Mining,"Cultural algorithm is a kind of evolutionary algorithm inspired from societal evolution and is composed of a belief space, a population space and a protocol that enables exchange of knowledge between these sources. Knowledge created in the population space is accepted into the belief space while this collective knowledge from these sources is combined to influence the decisions of the individual agents in solving problems. Classification rules comes under descriptive knowledge discovery in dataminingand are the most sought out by users since they represent highly comprehensible form of knowledge. The rules have certain properties which make them useful forms of actionable knowledge to users. The rules are evaluated using these properties namely the rule metrics. In the current study a Cultural Algorithm Toolkit for Classification RuleMining(CAT-CRM) is proposed which allows the user to control three different set of parameters namely the evolutionary parameters, the rule parameters as well as agent parameters and hence can be used for experimenting with an evolutionary system, a ruleminingsystem or an agent based social system. Results of experiments conducted to observe the effect of different number and type of metrics on the performance of the algorithm on bench mark data sets is reported.","['Sujatha Srinivasan', 'Sivakumar Ramakrishnan']","International Journal on Computational Sciences & Applications (IJCSA) Vo2, No.4, 2012, 9-23",arXiv,2012,https://doi.org/10.48550/arXiv.1209.2948,Anomali
Image Mining from Gel Diagrams in Biomedical Publications,"Authors of biomedical publications often use gel images to report experimental results such as protein-protein interactions or protein expressions under different conditions. Gel images offer a way to concisely communicate such findings, not all of which need to be explicitly discussed in the articletext. This fact together with the abundance of gel images and their shared common patterns makes them prime candidates for imageminingendeavors. We introduce an approach for the detection of gel images, and present an automatic workflow to analyze them. We are able to detect gel segments and panels at high accuracy, and present first results for the identification of gene names in these images. While we cannot provide a complete solution at this point, we present evidence that this kind of imageminingis feasible.","['Tobias Kuhn', 'Michael Krauthammer']",Proceedings of the 5th International Symposium on Semantic Mining in Biomedicine (SMBM 2012),arXiv,2012,https://doi.org/10.48550/arXiv.1209.1481,Anomali
Business Intelligence: A Rapidly Growing Option through Web Mining,"The World Wide Web is a popular and interactive medium to distribute information in this scenario. The web is huge, diverse, ever changing, widely disseminated global information service center. We are familiar with terms like e-commerce, e-governance, e-market, e-finance, e-learning, e-banking etc. for an organization it is new challenge to maintain direct contact with customers because of the rapid growth in e-commerce, e-publishing and electronic service delivery. To deal with this there is need of intelligent marketing strategies and CRM (customer relationship management) i.e. the effective way of integrating enterprise applications in real time. Webminingis the vast field that helps to understand various concepts of different fields. Web usageminingtechniques are attempted to reason about different materialized issues of Business Intelligence which include marketing expertise as domain knowledge and are specifically designed for electronic commerce purposes. To this end, the chapter provides an introduction to the field of Webminingand examines existing as well as potential Webminingapplications applicable for different business function, like marketing, human resources, and fiscal administration. Suggestions for improving information technology infrastructure are made, which can help businesses interested in Webmininghit the ground running.",['Priyanka Rahi'],,arXiv,2012,https://doi.org/10.48550/arXiv.1208.5875,Anomali
Content-based Text Categorization using Wikitology,"A major computational burden, while performing document clustering, is the calculation of similarity measure between a pair of documents. Similarity measure is a function that assign a real number between 0 and 1 to a pair of documents, depending upon the degree of similarity between them. A value of zero means that the documents are completely dissimilar whereas a value of one indicates that the documents are practically identical. Traditionally, vector-based models have been used for computing the document similarity. The vector-based models represent several features present in documents. These approaches to similarity measures, in general, cannot account for the semantics of the document. Documents written in human languages contain contexts and the words used to describe these contexts are generally semantically related. Motivated by this fact, many researchers have proposed semantic-based similarity measures by utilizingtextannotation through external thesauruses like WordNet (a lexical database). In this paper, we define a semantic similarity measure based on documents represented in topic maps. Topic maps are rapidly becoming an industrial standard for knowledge representation with a focus for later search and extraction. The documents are transformed into a topic map based coded knowledge and the similarity between a pair of documents is represented as a correlation between the common patterns. The experimental studies on thetextminingdatasets reveal that this new similarity measure is more effective as compared to commonly used similarity measures intextclustering.","['Muhammad Rafi', 'Sundus Hassan', 'Mohammad Shahid Shaikh']",,arXiv,2012,https://doi.org/10.48550/arXiv.1208.3623,Anomali
Tracing scientist's research trends realtimely,"In this research, we propose a method to trace scientists' research trends realtimely. By monitoring the downloads of scientific articles in the journal of Scientometrics for 744 hours, namely one month, we investigate the download statistics. Then we aggregate the keywords in these downloaded research papers, and analyze the trends of article downloading and keyword downloading. Furthermore, taking both the download of keywords and articles into consideration, we design a method to detect the emerging research trends. We find that in scientometrics field, social media, new indices to quantify scientific productivity (g-index), webometrics, semantic,textmining, open access are emerging fields that scientometrics researchers are focusing on.","['Xianwen Wang', 'Zhi Wang', 'Shenmeng Xu']","Scientometrics. 2013, 95(2): 717-729",arXiv,2012,https://doi.org/10.48550/arXiv.1208.1349,Anomali
Semantic Web Requirements through Web Mining Techniques,"In recent years, Semantic web has become a topic of active research in several fields of computer science and has applied in a wide range of domains such as bioinformatics, life sciences, and knowledge management. The two fast-developing research areas semantic web and webminingcan complement each other and their different techniques can be used jointly or separately to solve the issues in both areas. In addition, since shifting from current web to semantic web mainly depends on the enhancement of knowledge, webminingcan play a key role in facing numerous challenges of this changing. In this paper, we analyze and classify the application of divers webminingtechniques in different challenges of the semantic web in form of an analytical framework.","['Hamed Hassanzadeh', 'Mohammad Reza Keyvanpour']","International Journal of Computer Theory and Engineering vol. 4, no. 4, pp. 616-620, 2012",arXiv,2012,https://doi.org/10.48550/arXiv.1208.0690,Anomali
Achieving Approximate Soft Clustering in Data Streams,"In recent years, data streaming has gained prominence due to advances in technologies that enable many applications to generate continuous flows of data. This increases the need to develop algorithms that are able to efficiently process data streams. Additionally, real-time requirements and evolving nature of data streams make streamminingproblems, including clustering, challenging research problems.
  In this paper, we propose a one-pass streaming soft clustering (membership of a point in a cluster is described by a distribution) algorithm which approximates the ""soft"" version of the k-means objective function. Soft clustering has applications in various aspects of databases and machine learning including density estimation and learning mixture models. We first achieve a simple pseudo-approximation in terms of the ""hard"" k-means algorithm, where the algorithm is allowed to output more than $k$ centers. We convert this batch algorithm to a streaming one (using an extension of the k-means++ algorithm recently proposed) in the ""cash register"" model. We also extend this algorithm when the clustering is done over a moving window in the data stream.","['Vaneet Aggarwal', 'Shankar Krishnan']",,arXiv,2012,https://doi.org/10.48550/arXiv.1207.6199,Anomali
Data Mining on Educational Domain,"Educational datamining(EDM) is defined as the area of scientific inquiry centered around the development of methods for making discoveries within the unique kinds of data that come from educational settings, and using those methods to better understand students and the settings which they learn in. Dataminingenables organizations to use their current reporting capabilities to uncover and understand hidden patterns in vast databases. As a result of this insight, institutions are able to allocate resources and staff more effectively. In this paper, we present a real-world experiment conducted in Shree Rayeshwar Institute of Engineering and Information Technology (SRIEIT) in Goa, India. Here we found the relevant subjects in an undergraduate syllabus and the strength of their relationship. We have also focused on classification of students into different categories such as good, average, poor depending on their marks scored by them by obtaining a decision tree which will predict the performance of the students and accordingly help the weaker section of students to improve in their academics. We have also found clusters of students for helping in analyzing student's performance and also improvising the subject teaching in that particular subject.","['Prof Rudresh Shirwaikar', 'Nikhil Rajadhyax']",,arXiv,2012,https://doi.org/10.48550/arXiv.1207.1535,Anomali
Mining Associated Text and Images with Dual-Wing Harmoniums,"We propose a multi-wing harmonium model forminingmultimedia data that extends and improves on earlier models based on two-layer random fields, which capture bidirectional dependencies between hidden topic aspects and observed inputs. This model can be viewed as an undirected counterpart of the two-layer directed models such as LDA for similar tasks, but bears significant difference in inference/learning cost tradeoffs, latent topic representations, and topic mixing mechanisms. In particular, our model facilitates efficient inference and robust topic mixing, and potentially provides high flexibilities in modeling the latent topic spaces. A contrastive divergence and a variational algorithm are derived for learning. We specialized our model to a dual-wing harmonium for captioned images, incorporating a multivariate Poisson for word-counts and a multivariate Gaussian for color histogram. We present empirical results on the applications of this model to classification, retrieval and image annotation on news video collections, and we report an extensive comparison with various extant models.","['Eric P. Xing', 'Rong Yan', 'Alexander G. Hauptmann']",UAI-P-2005-PG-633-641,arXiv,2012,https://doi.org/10.48550/arXiv.1207.1423,Anomali
INSTRUCT: Space-Efficient Structure for Indexing and Complete Query Management of String Databases,"The tremendous expanse of search engines, dictionary and thesaurus storage, and othertextminingapplications, combined with the popularity of readily available scanning devices and optical character recognition tools, has necessitated efficient storage, retrieval and management of massivetextdatabases for various modern applications. For such applications, we propose a novel data structure, INSTRUCT, for efficient storage and management of sequence databases. Our structure uses bit vectors for reusing the storage space for common triplets, and hence, has a very low memory requirement. INSTRUCT efficiently handles prefix and suffix search queries in addition to the exact string search operation by iteratively checking the presence of triplets. We also propose an extension of the structure to handle substring search efficiently, albeit with an increase in the space requirements. This extension is important in the context of trie-based solutions which are unable to handle such queries efficiently. We perform several experiments portraying that INSTRUCT outperforms the existing structures by nearly a factor of two in terms of space requirements, while the query times are better. The ability to handle insertion and deletion of strings in addition to supporting all kinds of queries including exact search, prefix/suffix search and substring search makes INSTRUCT a complete data structure.","['Sourav Dutta', 'Arnab Bhattacharya']",,arXiv,2012,https://doi.org/10.48550/arXiv.1207.0361,Anomali
Mining Statistically Significant Substrings using the Chi-Square Statistic,"The problem of identification of statistically significant patterns in a sequence of data has been applied to many domains such as intrusion detection systems, financial models, web-click records, automated monitoring systems, computational biology, cryptology, andtextanalysis. An observed pattern of events is deemed to be statistically significant if it is unlikely to have occurred due to randomness or chance alone. We use the chi-square statistic as a quantitative measure of statistical significance. Given a string of characters generated from a memoryless Bernoulli model, the problem is to identify the substring for which the empirical distribution of single letters deviates the most from the distribution expected from the generative Bernoulli model. This deviation is captured using the chi-square measure. The most significant substring (MSS) of a string is thus defined as the substring having the highest chi-square value. Till date, to the best of our knowledge, there does not exist any algorithm to find the MSS in better than O(n^2) time, where n denotes the length of the string. In this paper, we propose an algorithm to find the most significant substring, whose running time is O(n^{3/2}) with high probability. We also study some variants of this problem such as finding the top-t set, finding all substrings having chi-square greater than a fixed threshold and finding the MSS among substrings greater than a given length. We experimentally demonstrate the asymptotic behavior of the MSS on varying the string size and alphabet size. We also describe some applications of our algorithm on cryptology and real world data from finance and sports. Finally, we compare our technique with the existing heuristics for finding the MSS.","['Mayank Sachan', 'Arnab Bhattacharya']","Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 10, pp. 1052-1063 (2012)",arXiv,2012,https://doi.org/10.48550/arXiv.1207.0144,Anomali
Discrete Elastic Inner Vector Spaces with Application in Time Series and Sequence Mining,"This paper proposes a framework dedicated to the construction of what we call discrete elastic inner product allowing one to embed sets of non-uniformly sampled multivariate time series or sequences of varying lengths into inner product space structures. This framework is based on a recursive definition that covers the case of multiple embedded time elastic dimensions. We prove that such inner products exist in our general framework and show how a simple instance of this inner product class operates on some prospective applications, while generalizing the Euclidean inner product. Classification experimentations on time series and symbolic sequences datasets demonstrate the benefits that we can expect by embedding time series or sequences into elastic inner spaces rather than into classical Euclidean spaces. These experiments show good accuracy when compared to the euclidean distance or even dynamic programming algorithms while maintaining a linear algorithmic complexity at exploitation stage, although a quadratic indexing phase beforehand is required.","['Pierre-François Marteau', 'Nicolas Bonnel', 'Gilbas Ménier']",IEEE Transactions on Knowledge and Data Engineering (2012) pp 1-14,arXiv,2012,https://doi.org/10.48550/arXiv.1206.6196,Anomali
Measuring political sentiment on Twitter: factor-optimal design for multinomial inverse regression,"This article presents a short case study intextanalysis: the scoring of Twitter posts for positive, negative, or neutral sentiment directed towards particular US politicians. The study requires selection of a sub-sample of representative posts for sentiment scoring, a common and costly aspect of sentimentmining. As a general contribution, our application is preceded by a proposed algorithm for maximizing sampling efficiency. In particular, we outline and illustrate greedy selection of documents to build designs that are D-optimal in a topic-factor decomposition of the originaltext. The strategy is applied to our motivating dataset of political posts, and we outline a new technique for predicting both generic and subject-specific document sentiment through use of variable interactions in multinomial inverse regression. Results are presented for analysis of 2.1 million Twitter posts around February 2012.",['Matt Taddy'],,arXiv,2013,https://doi.org/10.48550/arXiv.1206.3776,Anomali
Mining Educational Data Using Classification to Decrease Dropout Rate of Students,"In the last two decades, number of Higher Education Institutions (HEI) grows rapidly in India. Since most of the institutions are opened in private mode therefore, a cut throat competition rises among these institutions while attracting the student to got admission. This is the reason for institutions to focus on the strength of students not on the quality of education. This paper presents a dataminingapplication to generate predictive models for engineering student's dropout management. Given new records of incoming students, the predictive model can produce short accurate prediction list identifying students who tend to need the support from the student dropout program most. The results show that the machine learning algorithm is able to establish effective predictive model from the existing student dropout data.",['Saurabh Pal'],"International journal of multidisciplinary sciences and engineering, vol. 3, no. 5, May 2012, 35-39",arXiv,2012,https://doi.org/10.48550/arXiv.1206.3078,Anomali
Memory-Efficient Topic Modeling,"As one of the simplest probabilistic topic modeling techniques, latent Dirichlet allocation (LDA) has found many important applications intextmining, computer vision and computational biology. Recent training algorithms for LDA can be interpreted within a unified message passing framework. However, message passing requires storing previous messages with a large amount of memory space, increasing linearly with the number of documents or the number of topics. Therefore, the high memory usage is often a major problem for topic modeling of massive corpora containing a large number of topics. To reduce the space complexity, we propose a novel algorithm without storing previous messages for training LDA: tiny belief propagation (TBP). The basic idea of TBP relates the message passing algorithms with the non-negative matrix factorization (NMF) algorithms, which absorb the message updating into the message passing process, and thus avoid storing previous messages. Experimental results on four large data sets confirm that TBP performs comparably well or even better than current state-of-the-art training algorithms for LDA but with a much less memory consumption. TBP can do topic modeling when massive corpora cannot fit in the computer memory, for example, extracting thematic topics from 7 GB PUBMED corpora on a common desktop computer with 2GB memory.","['Jia Zeng', 'Zhi-Qiang Liu', 'Xiao-Qin Cao']",,arXiv,2012,https://doi.org/10.48550/arXiv.1206.1147,Anomali
A Machine Learning Approach For Opinion Holder Extraction In Arabic Language,"Opinionminingaims at extracting useful subjective information from reliable amounts oftext. Opinionminingholder recognition is a task that has not been considered yet in Arabic Language. This task essentially requires deep understanding of clauses structures. Unfortunately, the lack of a robust, publicly available, Arabic parser further complicates the research. This paper presents a leading research for the opinion holder extraction in Arabic news independent from any lexical parsers. We investigate constructing a comprehensive feature set to compensate the lack of parsing structural outcomes. The proposed feature set is tuned from English previous works coupled with our proposed semantic field and named entities features. Our feature analysis is based on Conditional Random Fields (CRF) and semi-supervised pattern recognition techniques. Different research models are evaluated via cross-validation experiments achieving 54.03 F-measure. We publicly release our own research outcome corpus and lexicon for opinionminingcommunity to encourage further research.","['Mohamed Elarnaoty', 'Samir AbdelRahman', 'Aly Fahmy']","Mohamed Elarnaoty, Samir AbdelRahman and Aly Fahmy. ""A Machine Learning Approach for Opinion Holder Extraction in Arabic Language"", ISSN:0976-2191, vol 3, March 2012",arXiv,2012,https://doi.org/10.48550/arXiv.1206.1011,Anomali
Effective Listings of Function Stop words for Twitter,"Many words in documents recur very frequently but are essentially meaningless as they are used to join words together in a sentence. It is commonly understood that stop words do not contribute to the context or content of textual documents. Due to their high frequency of occurrence, their presence intextminingpresents an obstacle to the understanding of the content in the documents. To eliminate the bias effects, mosttextminingsoftware or approaches make use of stop words list to identify and remove those words. However, the development of such top words list is difficult and inconsistent between textual sources. This problem is further aggravated by sources such as Twitter which are highly repetitive or similar in nature. In this paper, we will be examining the original work using term frequency, inverse document frequency and term adjacency for developing a stop words list for the Twitter data source. We propose a new technique using combinatorial values as an alternative measure to effectively list out stop words.",['Murphy Choy'],,arXiv,2012,https://doi.org/10.48550/arXiv.1205.6396,Anomali
"EHRs Connect Research and Practice: Where Predictive Modeling, Artificial Intelligence, and Clinical Decision Support Intersect","Objectives: Electronic health records (EHRs) are only a first step in capturing and utilizing health-related data - the challenge is turning that data into useful information. Furthermore, EHRs are increasingly likely to include data relating to patient outcomes, functionality such as clinical decision support, and genetic information as well, and, as such, can be seen as repositories of increasingly valuable information about patients' health conditions and responses to treatment over time. Methods: We describe a case study of 423 patients treated by Centerstone within Tennessee and Indiana in which we utilized electronic health record data to generate predictive algorithms of individual patient treatment response. Multiple models were constructed using predictor variables derived from clinical, financial and geographic data. Results: For the 423 patients, 101 deteriorated, 223 improved and in 99 there was no change in clinical condition. Based on modeling of various clinical indicators at baseline, the highest accuracy in predicting individual patient response ranged from 70-72% within the models tested. In terms of individual predictors, the Centerstone Assessment of Recovery Level - Adult (CARLA) baseline score was most significant in predicting outcome over time (odds ratio 4.1 + 2.27). Other variables with consistently significant impact on outcome included payer, diagnostic category, location and provision of case management services. Conclusions: This approach represents a promising avenue toward reducing the current gap between research and practice across healthcare, developing data-driven clinical decision support based on real-world populations, and serving as a component of embedded clinical artificial intelligences that ""learn"" over time.","['Casey Bennett', 'Tom Doub', 'Rebecca Selove']",Health Policy and Technology 1(2): 105-114 (2012),arXiv,2012,https://doi.org/10.48550/arXiv.1204.4927,Anomali
"Proceedings of the first International Workshop On Open Data, WOD-2012","WOD-2012 aims at facilitating new trends and ideas from a broad range of topics concerned within the widely-spread Open Data movement, from the viewpoint of computer science research.
  While being most commonly known from the recent Linked Open Data movement, the concept of publishing data explicitly as Open Data has meanwhile developed many variants and facets that go beyond publishing large and highly structured RDF/S repositories. Open Data comprisestextand semi-structured data, but also open multi-modal contents, including music, images, and videos. With the increasing amount of data that is published by governments (see, e.g., data.gov, data.gov.uk or data.gouv.fr), by international organizations (data.worldbank.org or data.undp.org) and by scientific communities (tdar.org, cds.u-strasbg.fr, GenBank, IRIS or KNB) explicitly under an Open Data policy, new challenges arise not only due to the scale at which this data becomes available.
  A number of community-based conferences accommodate tracks or workshops which are dedicated to Open Data. However, WOD aims to be a premier venue to gather researchers and practitioners who are contributing to and interested in the emerging field of managing Open Data from a computer science perspective. Hence, it is a unique opportunity to find in a single place up-to-date scientific works on Web-scale Open Data issues that have so far only partially been addressed by different research communities such as Databases, DataMiningand Knowledge Management, Distributed Systems, Data Privacy, and Data Visualization.","['Guillaume Raschia', 'Martin Theobald', 'Ioana Manolescu']",,arXiv,2012,https://doi.org/10.48550/arXiv.1204.3726,Anomali
Event based classification of Web 2.0 text streams,"Web 2.0 applications like Twitter or Facebook create a continuous stream of information. This demands new ways of analysis in order to offer insight into this stream right at the moment of the creation of the information, because lots of this data is only relevant within a short period of time. To address this problem real time search engines have recently received increased attention. They take into account the continuous flow of information differently than traditional web search by incorporating temporal and social features, that describe the context of the information during its creation. Standard approaches where data first get stored and then is processed from a peristent storage suffer from latency. We want to address the fluent and rapid nature oftextstream by providing an event based approach that analyses directly the stream of information. In a first step we want to define the difference between real time search and traditional search to clarify the demands in moderntextfiltering. In a second step we want to show how event based features can be used to support the tasks of real time search engines. Using the example of Twitter we present in this paper a way how to combine an event based approach withtextminingand information filtering concepts in order to classify incoming information based on stream features. We calculate stream dependant features and feed them into a neural network in order to classify thetextstreams. We show the separative capabilities of event based features as the foundation for a real time search engine.","['Andreas Bauer', 'Christian Wolff']",,arXiv,2013,https://doi.org/10.48550/arXiv.1204.3362,Anomali
A Fuzzy Similarity Based Concept Mining Model for Text Classification,"TextClassification is a challenging and a red hot field in the current scenario and has great importance intextcategorization applications. A lot of research work has been done in this field but there is a need to categorize a collection oftextdocuments into mutually exclusive categories by extracting the concepts or features using supervised learning paradigm and different classification algorithms. In this paper, a new Fuzzy Similarity Based ConceptMiningModel (FSCMM) is proposed to classify a set oftextdocuments into pre - defined Category Groups (CG) by providing them training and preparing on the sentence, document and integrated corpora levels along with feature reduction, ambiguity removal on each level to achieve high system performance. Fuzzy Feature Category Similarity Analyzer (FFCSA) is used to analyze each extracted feature of Integrated Corpora Feature Vector (ICFV) with the corresponding categories or classes. This model uses Support Vector Machine Classifier (SVMC) to classify correctly the training data patterns into two groups; i. e., + 1 and - 1, thereby producing accurate and correct results. The proposed model works efficiently and effectively with great performance and high - accuracy results.",['Shalini Puri'],"Volume 2, Number 11, pp. 115 - 121, November, 2011",arXiv,2012,https://doi.org/10.48550/arXiv.1204.2061,Anomali
A technical study and analysis on fuzzy similarity based models for text classification,"In this new and current era of technology, advancements and techniques, efficient and effectivetextdocument classification is becoming a challenging and highly required area to capably categorizetextdocuments into mutually exclusive categories. Fuzzy similarity provides a way to find the similarity of features among various documents. In this paper, a technical review on various fuzzy similarity based models is given. These models are discussed and compared to frame out their use and necessity. A tour of different methodologies is provided which is based upon fuzzy similarity related concerns. It shows that howtextand web documents are categorized efficiently into different categories. Various experimental results of these models are also discussed. The technical comparisons among each model's parameters are shown in the form of a 3-D chart. Such study and technical review provide a strong base of research work done on fuzzy similarity basedtextdocument categorization.","['Shalini Puri', 'Sona Kaushik']","International Journal of Data Mining & Knowledge Management Process (IJDKP) March, 2012, Vol. 2, Number 2,pp. 1-15",arXiv,2012,https://doi.org/10.48550/arXiv.1204.2058,Anomali
Semi-Automatically Extracting FAQs to Improve Accessibility of Software Development Knowledge,"Frequently asked questions (FAQs) are a popular way to document software development knowledge. As creating such documents is expensive, this paper presents an approach for automatically extracting FAQs from sources of software development discussion, such as mailing lists and Internet forums, by combining techniques oftextminingand natural language processing. We apply the approach to popular mailing lists and carry out a survey among software developers to show that it is able to extract high-quality FAQs that may be further improved by experts.","['Stefan Henß', 'Martin Monperrus', 'Mira Mezini']","ICSE - 34th International Conference on Software Engineering, 2012",arXiv,2012,https://doi.org/10.48550/arXiv.1203.5188,Anomali
Data Mining: A Prediction for Performance Improvement of Engineering Students using Classification,"Now-a-days the amount of data stored in educational database increasing rapidly. These databases contain hidden information for improvement of students' performance. Educational dataminingis used to study the data available in the educational field and bring out the hidden knowledge from it. Classification methods like decision trees, Bayesian network etc can be applied on the educational data for predicting the student's performance in examination. This prediction will help to identify the weak students and help them to score better marks. The C4.5, ID3 and CART decision tree algorithms are applied on engineering student's data to predict their performance in the final exam. The outcome of the decision tree predicted the number of students who are likely to pass, fail or promoted to next year. The results provide steps to improve the performance of the students who were predicted to fail or promoted. After the declaration of the results in the final examination the marks obtained by the students are fed into the system and the results were analyzed for the next session. The comparative analysis of the results states that the prediction has helped the weaker students to improve and brought out betterment in the result.","['Surjeet Kumar Yadav', 'Saurabh Pal']","World of Computer Science and Information Technology Journal WCSIT, Vol. 2, No. 2, 51-56, 2012",arXiv,2012,https://doi.org/10.48550/arXiv.1203.3832,Anomali
Literature-based knowledge discovery: the state of the art,"Literature-based knowledge discovery method was introduced by Dr. Swanson in 1986. He hypothesized a connection between Raynaud's phenomenon and dietary fish oil, the field of literature-based discovery (LBD) was born from then on. During the subsequent two decades, LBD's research attracts some scientists including information science, computer science, and biomedical science, etc.. It has been a part of knowledge discovery andtextmining. This paper summarizes the development of recent years about LBD and presents two parts, methodology research and applied research. Lastly, some problems are pointed as future research directions.","['Xiaoyong Liu', 'Hui Fu']",,arXiv,2012,https://doi.org/10.48550/arXiv.1203.3611,Anomali
Mining Education Data to Predict Student's Retention: A comparative Study,"The main objective of higher education is to provide quality education to students. One way to achieve highest level of quality in higher education system is by discovering knowledge for prediction regarding enrolment of students in a course. This paper presents a dataminingproject to generate predictive models for student retention management. Given new records of incoming students, these predictive models can produce short accurate prediction lists identifying students who tend to need the support from the student retention program most. This paper examines the quality of the predictive models generated by the machine learning algorithms. The results show that some of the machines learning algorithms are able to establish effective predictive models from the existing student retention data.","['Surjeet Kumar Yadav', 'Brijesh Bharadwaj', 'Saurabh Pal']","(IJCSIS) International Journal of Computer Science and Information Security, Vol. 10, No. 2, 2012, pp113-117",arXiv,2012,https://doi.org/10.48550/arXiv.1203.2987,Anomali
Data Mining Applications: A comparative Study for Predicting Student's performance,Knowledge Discovery and DataMining(KDD) is a multidisciplinary area focusing upon methodologies for extracting useful knowledge from data and there are several useful KDD tools to extracting the knowledge. This knowledge can be used to increase the quality of education. But educational institution does not use any knowledge discovery process approach on these data. Dataminingcan be used for decision making in educational system. A decision tree classifier is one of the most widely used supervised learning methods used for data exploration based on divide & conquer technique. This paper discusses use of decision trees in educational datamining. Decision tree algorithms are applied on students' past performance data to generate the model and this model can be used to predict the students' performance. It helps earlier in identifying the dropouts and students who need special attention and allow the teacher to provide appropriate advising/counseling.,"['Surjeet Kumar Yadav', 'Brijesh Bharadwaj', 'Saurabh Pal']","International Journal of Innovative Technology and Creative Engineering, Vol.1 No.12 (2011) 13-19",arXiv,2012,https://doi.org/10.48550/arXiv.1202.4815,Anomali
Speeding-up $q$-gram mining on grammar-based compressed texts,"We present an efficient algorithm for calculating $q$-gram frequencies on strings represented in compressed form, namely, as a straight line program (SLP). Given an SLP $\mathcal{T}$ of size $n$ that represents string $T$, the algorithm computes the occurrence frequencies of all $q$-grams in $T$, by reducing the problem to the weighted $q$-gram frequencies problem on a trie-like structure of size $m = |T|-\mathit{dup}(q,\mathcal{T})$, where $\mathit{dup}(q,\mathcal{T})$ is a quantity that represents the amount of redundancy that the SLP captures with respect to $q$-grams. The reduced problem can be solved in linear time. Since $m = O(qn)$, the running time of our algorithm is $O(\min\{|T|-\mathit{dup}(q,\mathcal{T}),qn\})$, improving our previous $O(qn)$ algorithm when $q = Ω(|T|/n)$.","['Keisuke Goto', 'Hideo Bannai', 'Shunsuke Inenaga', 'Masayuki Takeda']",,arXiv,2012,https://doi.org/10.48550/arXiv.1202.3311,Anomali
Data quality measurement on categorical data using genetic algorithm,"Data quality on categorical attribute is a difficult problem that has not received as much attention as numerical counterpart. Our basic idea is to employ association rule for the purpose of data quality measurement. Strong rule generation is an important area of datamining. Association ruleminingproblems can be considered as a multi objective problem rather than as a single objective one. The main area of concentration was the rules generated by association ruleminingusing genetic algorithm. The advantage of using genetic algorithm is to discover high level prediction rules is that they perform a global search and cope better with attribute interaction than the greedy rule induction algorithm often used in datamining. Genetic algorithm based approach utilizes the linkage between association rule and feature selection. In this paper, we put forward a Multi objective genetic algorithm approach for data quality on categorical attributes. The result shows that our approach is outperformed by the objectives like accuracy, completeness, comprehensibility and interestingness.","['J. Malar Vizhi', 'T. Bhuvaneswari']","International Journal of Data Mining & Knowledge Management Process (IJDKP) Vol.2, No.1, January 2012",arXiv,2012,https://doi.org/10.48550/arXiv.1202.3215,Anomali
Concept Relation Discovery and Innovation Enabling Technology (CORDIET),"Concept Relation Discovery and Innovation Enabling Technology (CORDIET), is a toolbox for gaining new knowledge from unstructuredtextdata. At the core of CORDIET is the C-K theory which captures the essential elements of innovation. The tool uses Formal Concept Analysis (FCA), Emergent Self Organizing Maps (ESOM) and Hidden Markov Models (HMM) as main artifacts in the analysis process. The user can define temporal,textminingand compound attributes. Thetextminingattributes are used to analyze the unstructuredtextin documents, the temporal attributes use these document's timestamps for analysis. The compound attributes are XML rules based ontextminingand temporal attributes. The user can cluster objects with object-cluster rules and can chop the data in pieces with segmentation rules. The artifacts are optimized for efficient data analysis; object labels in the FCA lattice and ESOM map contain an URL on which the user can click to open the selected document.","['Jonas Poelmans', 'Paul Elzinga', 'Alexey Neznanov', 'Stijn Viaene', 'Sergei O. Kuznetsov', 'Dmitry Ignatov', 'Guido Dedene']","In CEUR Workshop proceedings Vol-757, CDUD'11 - Concept Discovery in Unstructured Data, pp. 53-62, 2011",arXiv,2012,https://doi.org/10.48550/arXiv.1202.2895,Anomali
Construction of Learning Path Using Ant Colony Optimization from a Frequent Pattern Graph,"In an e-Learning system a learner may come across multiple unknown terms, which are generally hyperlinked, while reading atextdefinition or theory on any topic. It becomes even harder when one tries to understand those unknown terms through further such links and they again find some new terms that have new links. As a consequence they get confused where to initiate from and what are the prerequisites. So it is very obvious for the learner to make a choice of what should be learnt before what. In this paper we have taken the dataminingbased frequent pattern graph model to define the association and sequencing between the words and then adopted the Ant Colony Optimization, an artificial intelligence approach, to derive a searching technique to obtain an efficient and optimized learning path to reach to a unknown term.","['Souvik Sengupta', 'Sandipan Sahu', 'Ranjan Dasgupta']",,arXiv,2012,https://doi.org/10.48550/arXiv.1201.3976,Anomali
Data Mining: A prediction for performance improvement using classification,"Now-a-days the amount of data stored in educational database increasing rapidly. These databases contain hidden information for improvement of students' performance. The performance in higher education in India is a turning point in the academics for all students. This academic performance is influenced by many factors, therefore it is essential to develop predictive dataminingmodel for students' performance so as to identify the difference between high learners and slow learners student. In the present investigation, an experimental methodology was adopted to generate a database. The raw data was preprocessed in terms of filling up missing values, transforming values in one form into another and relevant attribute/ variable selection. As a result, we had 300 student records, which were used for by Byes classification prediction model construction. Keywords- DataMining, Educational DataMining, Predictive Model, Classification.","['Brijesh Kumar Bhardwaj', 'Saurabh Pal']","(IJCSIS) International Journal of Computer Science and Information Security, Vol. 9, No. 4, April 2011, pp 136-140",arXiv,2012,https://doi.org/10.48550/arXiv.1201.3418,Anomali
Mining Educational Data to Analyze Students' Performance,"The main objective of higher education institutions is to provide quality education to its students. One way to achieve highest level of quality in higher education system is by discovering knowledge for prediction regarding enrolment of students in a particular course, alienation of traditional classroom teaching model, detection of unfair means used in online examination, detection of abnormal values in the result sheets of the students, prediction about students' performance and so on. The knowledge is hidden among the educational data set and it is extractable through dataminingtechniques. Present paper is designed to justify the capabilities of dataminingtechniques in context of higher education by offering a dataminingmodel for higher education system in the university. In this research, the classification task is used to evaluate student's performance and as there are many approaches that are used for data classification, the decision tree method is used here. By this task we extract knowledge that describes students' performance in end semester examination. It helps earlier in identifying the dropouts and students who need special attention and allow the teacher to provide appropriate advising/counseling. Keywords-Educational DataMining(EDM); Classification; Knowledge Discovery in Database (KDD); ID3 Algorithm.","['Brijesh Kumar Baradwaj', 'Saurabh Pal']","IJACSA Vol. 2, No. 6, 2011, pp 63-69",arXiv,2012,https://doi.org/10.48550/arXiv.1201.3417,Anomali
Pbm: A new dataset for blog mining,"Textminingis becoming vital as Web 2.0 offers collaborative content creation and sharing. Now Researchers have growing interest intextminingmethods for discovering knowledge.Textminingresearchers come from variety of areas like: Natural Language Processing, Computational Linguistic, Machine Learning, and Statistics. A typicaltextminingapplication involves preprocessing oftext, stemming and lemmatization, tagging and annotation, deriving knowledge patterns, evaluating and interpreting the results. There are numerous approaches for performingtextminingtasks, like: clustering, categorization, sentimental analysis, and summarization. There is a growing need to standardize the evaluation of these tasks. One major component of establishing standardization is to provide standard datasets for these tasks. Although there are various standard datasets available for traditionaltextminingtasks, but there are very few and expensive datasets for blog-miningtask. Blogs, a new genre in web 2.0 is a digital diary of web user, which has chronological entries and contains a lot of useful knowledge, thus offers a lot of challenges and opportunities fortextmining. In this paper, we report a new indigenous dataset for Pakistani Political Blogosphere. The paper describes the process of data collection, organization, and standardization. We have used this dataset for carrying out varioustextminingtasks for blogosphere, like: blog-search, political sentiments analysis and tracking, identification of influential blogger, and clustering of the blog-posts. We wish to offer this dataset free for others who aspire to pursue further in this domain.","['Mehwish Aziz', 'Muhammad Rafi']",,arXiv,2012,https://doi.org/10.48550/arXiv.1201.2073,Anomali
A Topic Modeling Toolbox Using Belief Propagation,"Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications intextmining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. TMBP toolbox is implemented by MEX C++/Matlab/Octave for either Windows 7 or Linux. Compared with existing topic modeling packages, the novelty of this toolbox lies in the BP algorithms for learning LDA-based topic models. The current version includes BP algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more BP-based algorithms for various topic models will be added in the near future. Interested users may also extend BP algorithms for learning more complicated topic models. The source codes are freely available under the GNU General Public Licence, Version 1.0 at https://mloss.org/software/view/399/.",['Jia Zeng'],"Journal of Machine Learning Research (13) 2233-2236, 2012",arXiv,2012,https://doi.org/10.48550/arXiv.1201.0838,Anomali
Alternating proximal gradient method for nonnegative matrix factorization,"Nonnegative matrix factorization has been widely applied in face recognition,textmining, as well as spectral analysis. This paper proposes an alternating proximal gradient method for solving this problem. With a uniformly positive lower bound assumption on the iterates, any limit point can be proved to satisfy the first-order optimality conditions. A Nesterov-type extrapolation technique is then applied to accelerate the algorithm. Though this technique is at first used for convex program, it turns out to work very well for the non-convex nonnegative matrix factorization problem. Extensive numerical experiments illustrate the efficiency of the alternating proximal gradient method and the accleration technique. Especially for real data tests, the accelerated method reveals high superiority to state-of-the-art algorithms in speed with comparable solution qualities.",['Yangyang Xu'],,arXiv,2013,https://doi.org/10.48550/arXiv.1112.5407,Anomali
Learning Context for Text Categorization,This paper describes our work which is based on discovering context fortextdocument categorization. The document categorization approach is derived from a combination of a learning paradigm known as relation extraction and an technique known as context discovery. We demonstrate the effectiveness of our categorization approach using reuters 21578 dataset and synthetic real world data from sports domain. Our experimental results indicate that the learned context greatly improves the categorization performance as compared to traditional categorization approaches.,"['Y. V. Haribhakta', 'Dr. Parag Kulkarni']",,arXiv,2011,https://doi.org/10.48550/arXiv.1112.2031,Anomali
Document Classification Using Expectation Maximization with Semi Supervised Learning,"As the amount of online document increases, the demand for document classification to aid the analysis and management of document is increasing.Textis cheap, but information, in the form of knowing what classes a document belongs to, is expensive. The main purpose of this paper is to explain the expectation maximization technique of dataminingto classify the document and to learn how to improve the accuracy while using semi-supervised approach. Expectation maximization algorithm is applied with both supervised and semi-supervised approach. It is found that semi-supervised approach is more accurate and effective. The main advantage of semi supervised approach is ""Dynamically Generation of New Class"". The algorithm first trains a classifier using the labeled document and probabilistically classifies the unlabeled documents. The car dataset for the evaluation purpose is collected from UCI repository dataset in which some changes have been done from our side.","['Bhawna Nigam', 'Poorvi Ahirwal', 'Sonal Salve', 'Swati Vamney']",,arXiv,2011,https://doi.org/10.48550/arXiv.1112.2028,Anomali
A more appropriate Protein Classification using Data Mining,"Research in bioinformatics is a complex phenomenon as it overlaps two knowledge domains, namely, biological and computer sciences. This paper has tried to introduce an efficient dataminingapproach for classifying proteins into some useful groups by representing them in hierarchy tree structure. There are several techniques used to classify proteins but most of them had few drawbacks on their grouping. Among them the most efficient grouping technique is used by PSIMAP. Even though PSIMAP (Protein Structural Interactome Map) technique was successful to incorporate most of the protein but it fails to classify the scale free property proteins. Our technique overcomes this drawback and successfully maps all the protein in different groups, including the scale free property proteins failed to group by PSIMAP. Our approach selects the six major attributes of protein: a) Structure comparison b) Sequence Comparison c) Connectivity d) Cluster Index e) Interactivity f) Taxonomic to group the protein from the databank by generating a hierarchal tree structure. The proposed approach calculates the degree (probability) of similarity of each protein newly entered in the system against of existing proteins in the system by using probability theorem on each six properties of proteins.","['Muhammad Mahbubur Rahman', 'Arif Ul Alam', 'Abdullah-Al-Mamun', 'Tamnun E Mursalin']","Journal of Theoretical and Applied Information Technology(JATIT), pp. 33-43, 2010",arXiv,2011,https://doi.org/10.48550/arXiv.1111.2514,Anomali
Dynamical Classes of Collective Attention in Twitter,"Micro-blogging systems such as Twitter expose digital traces of social discourse with an unprecedented degree of resolution of individual behaviors. They offer an opportunity to investigate how a large-scale social system responds to exogenous or endogenous stimuli, and to disentangle the temporal, spatial and topical aspects of users' activity. Here we focus on spikes of collective attention in Twitter, and specifically on peaks in the popularity of hashtags. Users employ hashtags as a form of social annotation, to define a shared context for a specific event, topic, or meme. We analyze a large-scale record of Twitter activity and find that the evolution of hastag popularity over time defines discrete classes of hashtags. We link these dynamical classes to the events the hashtags represent and usetextminingtechniques to provide a semantic characterization of the hastag classes. Moreover, we track the propagation of hashtags in the Twitter social network and find that epidemic spreading plays a minor role in hastag popularity, which is mostly driven by exogenous factors.","['Janette Lehmann', 'Bruno Gonçalves', 'José J. Ramasco', 'Ciro Cattuto']","WWW'12, 251 (2012)",arXiv,2012,https://doi.org/10.48550/arXiv.1111.1896,Anomali
Syndromic classification of Twitter messages,Recent studies have shown strong correlation between social networking data and national influenza rates. We expanded upon this success to develop an automatedtextminingsystem that classifies Twitter messages in real time into six syndromic categories based on key terms from a public health ontology. 10-fold cross validation tests were used to compare Naive Bayes (NB) and Support Vector Machine (SVM) models on a corpus of 7431 Twitter messages. SVM performed better than NB on 4 out of 6 syndromes. The best performing classifiers showed moderately strong F1 scores: respiratory = 86.2 (NB); gastrointestinal = 85.4 (SVM polynomial kernel degree 2); neurological = 88.6 (SVM polynomial kernel degree 1); rash = 86.0 (SVM polynomial kernel degree 1); constitutional = 89.3 (SVM polynomial kernel degree 1); hemorrhagic = 89.9 (NB). The resulting classifiers were deployed together with an EARS C2 aberration detection algorithm in an experimental online system.,"['Nigel Collier', 'Son Doan']",,arXiv,2011,https://doi.org/10.48550/arXiv.1110.3094,Anomali
What's unusual in online disease outbreak news?,"Background: Accurate and timely detection of public health events of international concern is necessary to help support risk assessment and response and save lives. Novel event-based methods that use the World Wide Web as a signal source offer potential to extend health surveillance into areas where traditional indicator networks are lacking. In this paper we address the issue of systematically evaluating online health news to support automatic alerting using daily disease-country countstextminedfrom real world data using BioCaster. For 18 data sets produced by BioCaster, we compare 5 aberration detection algorithms (EARS C2, C3, W2, F-statistic and EWMA) for performance against expert moderated ProMED-mail postings. Results: We report sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV), mean alerts/100 days and F1, at 95% confidence interval (CI) for 287 ProMED-mail postings on 18 outbreaks across 14 countries over a 366 day period. Results indicate that W2 had the best F1 with a slight benefit for day of week effect over C2. In drill down analysis we indicate issues arising from the granular choice of country-level modeling, sudden drops in reporting due to day of week effects and reporting bias. Automatic alerting has been implemented in BioCaster available from http://born.nii.ac.jp. Conclusions: Online health news alerts have the potential to enhance manual analytical methods by increasing throughput, timeliness and detection rates. Systematic evaluation of health news aberrations is necessary to push forward our understanding of the complex relationship between news report volumes and case numbers and to select the best performing features and algorithms.",['Nigel Collier'],"Journal of Biomedical Semantics 2010, 1:2",arXiv,2011,https://doi.org/10.48550/arXiv.1110.3091,Anomali
Towards cross-lingual alerting for bursty epidemic events,"Background: Online news reports are increasingly becoming a source for event based early warning systems that detect natural disasters. Harnessing the massive volume of information available from multilingual newswire presents as many challenges as opportunities due to the patterns of reporting complex spatiotemporal events. Results: In this article we study the problem of utilising correlated event reports across languages. We track the evolution of 16 disease outbreaks using 5 temporal aberration detection algorithms ontext-minedevents classified according to disease and outbreak country. Using ProMED reports as a silver standard, comparative analysis of news data for 13 languages over a 129 day trial period showed improved sensitivity, F1 and timeliness across most models using cross-lingual events. We report a detailed case study analysis for Cholera in Angola 2010 which highlights the challenges faced in correlating news events with the silver standard. Conclusions: The results show that automated health surveillance using multilingualtextmininghas the potential to turn low value news into high value alerts if informed choices are used to govern the selection of models and data sources. An implementation of the C2 alerting algorithm using multilingual news is available at the BioCaster portal http://born.nii.ac.jp/?page=globalroundup.",['Nigel Collier'],"Journal of Biomedical Semantics 2011, 2(Suppl 5):S10",arXiv,2011,https://doi.org/10.48550/arXiv.1110.3088,Anomali
Queries mining for efficient routing in P2P communities,"Peer-to-peer (P2P) computing is currently attracting enormous attention. In P2P systems a very large number of autonomous computing nodes (the peers) pool together their resources and rely on each other for data and services. Peer-to-peer (P2P) Data-sharing systems now generate a significant portion of Internet traffic. Examples include P2P systems for network storage, web caching, searching and indexing of relevant documents and distributed network-threat analysis. Requirements for widely distributed information systems supporting virtual organizations have given rise to a new category of P2P systems called schema-based. In such systems each peer exposes its own schema and the main objective is the efficient search across the P2P network by processing each incoming query without overly consuming bandwidth. The usability of these systems depends on effective techniques to find and retrieve data; however, efficient and effective routing of content-based queries is a challenging problem in P2P networks. This work was attended as an attempt to motivate the use ofminingalgorithms and hypergraphs context to develop two different methods that improve significantly the efficiency of P2P communications. The proposed query routing methods direct the query to a set of relevant peers in such way as to avoid network traffic and bandwidth consumption. We compare the performance of the two proposed methods with the baseline one and our experimental results prove that our proposed methods generate impressive levels of performance and scalability.","['Anis Ismail', 'Mohamed Quafafou', 'Nicolas Durand', 'Gilles Nachouki', 'Mohammad Hajjar']",,arXiv,2011,https://doi.org/10.48550/arXiv.1109.5679,Anomali
Higher-Order Markov Tag-Topic Models for Tagged Documents and Images,"This paper studies the topic modeling problem of tagged documents and images. Higher-order relations among tagged documents and images are major and ubiquitous characteristics, and play positive roles in extracting reliable and interpretable topics. In this paper, we propose the tag-topic models (TTM) to depict such higher-order topic structural dependencies within the Markov random field (MRF) framework. First, we use the novel factor graph representation of latent Dirichlet allocation (LDA)-based topic models from the MRF perspective, and present an efficient loopy belief propagation (BP) algorithm for approximate inference and parameter estimation. Second, we propose the factor hypergraph representation of TTM, and focus on both pairwise and higher-order relation modeling among tagged documents and images. Efficient loopy BP algorithm is developed to learn TTM, which encourages the topic labeling smoothness among tagged documents and images. Extensive experimental results confirm the incorporation of higher-order relations to be effective in enhancing the overall topic modeling performance, when compared with current state-of-the-art topic models, in manytextand imageminingtasks of broad interests such as word and link prediction, document classification, and tag recommendation.","['Jia Zeng', 'Wei Feng', 'William K. Cheung', 'Chun-Hung Li']",,arXiv,2011,https://doi.org/10.48550/arXiv.1109.5370,Anomali
VOGCLUSTERS: an example of DAME web application,"We present the alpha release of the VOGCLUSTERS web application, specialized for data andtextminingon globular clusters. It is one of the web2.0 technology based services of DataMining& Exploration (DAME) Program, devoted tomineand explore heterogeneous information related to globular clusters data.","['Marco Castellani', 'Massimo Brescia', 'Ettore Mancini', 'Luca Pellecchia', 'Giuseppe Longo']",,arXiv,2011,https://doi.org/10.48550/arXiv.1109.4104,Anomali
Learning Topic Models by Belief Propagation,"Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications intextmining, computer vision and computational biology. This paper represents LDA as a factor graph within the Markov random field (MRF) framework, which enables the classic loopy belief propagation (BP) algorithm for approximate inference and parameter estimation. Although two commonly-used approximate inference methods, such as variational Bayes (VB) and collapsed Gibbs sampling (GS), have gained great successes in learning LDA, the proposed BP is competitive in both speed and accuracy as validated by encouraging experimental results on four large-scale document data sets. Furthermore, the BP algorithm has the potential to become a generic learning scheme for variants of LDA-based topic models. To this end, we show how to learn two typical variants of LDA-based topic models, such as author-topic models (ATM) and relational topic models (RTM), using BP based on the factor graph representation.","['Jia Zeng', 'William K. Cheung', 'Jiming Liu']","IEEE Transactions on Pattern Analysis and Machine Intelligence, Volume 33, Number 5, Pages 1121-1134, 2013",arXiv,2012,https://doi.org/10.48550/arXiv.1109.3437,Anomali
Text mining and visualization using VOSviewer,"VOSviewer is a computer program for creating, visualizing, and exploring bibliometric maps of science. In this report, the newtextminingfunctionality of VOSviewer is presented. A number of examples are given of applications in which VOSviewer is used for analyzing large amounts oftextdata.","['Nees Jan van Eck', 'Ludo Waltman']",,arXiv,2011,https://doi.org/10.48550/arXiv.1109.2058,Anomali
A Survey on Web Multimedia Mining,"Modern developments in digital media technologies has made transmitting and storing large amounts of multi/rich media data (e.g.text, images, music, video and their combination) more feasible and affordable than ever before. However, the state of the art techniques to process,miningand manage those rich media are still in their infancy. Advances developments in multimedia acquisition and storage technology the rapid progress has led to the fast growing incredible amount of data stored in databases. Useful information to users can be revealed if these multimedia files are analyzed. Multimediaminingdeals with the extraction of implicit knowledge, multimedia data relationships, or other patterns not explicitly stored in multimedia files. Also in retrieval, indexing and classification of multimedia data with efficient information fusion of the different modalities is essential for the system's overall performance. The purpose of this paper is to provide a systematic overview of multimediamining. This article is also represents the issues in the application process component for multimediaminingfollowed by the multimediaminingmodels.","['Pravin M. Kamde', 'Dr. Siddu. P. Algur']",,arXiv,2011,https://doi.org/10.48550/arXiv.1109.1145,Anomali
Decision Support for e-Governance: A Text Mining Approach,"Information and communication technology has the capability to improve the process by which governments involve citizens in formulating public policy and public projects. Even though much of government regulations may now be in digital form (and often available online), due to their complexity and diversity, identifying the ones relevant to a particular context is a non-trivial task. Similarly, with the advent of a number of electronic online forums, social networking sites and blogs, the opportunity of gathering citizens' petitions and stakeholders' views on government policy and proposals has increased greatly, but the volume and the complexity of analyzing unstructured data makes this difficult. On the other hand,textmininghas come a long way from simple keyword search, and matured into a discipline capable of dealing with much more complex tasks. In this paper we discuss howtext-miningtechniques can help in retrieval of information and relationships from textual data sources, thereby assisting policy makers in discovering associations between policies and citizens' opinions expressed in electronic public forums and blogs etc. We also present here, an integratedtextminingbased architecture for e-governance decision support along with a discussion on the Indian scenario.","['G. Koteswara Rao', 'Shubhamoy Dey']","International Journal of Managing Information Technology (IJMIT) Vol.3, No.3, 2011, 73-91",arXiv,2011,https://doi.org/10.48550/arXiv.1108.6198,Anomali
Typesafe Modeling in Text Mining,"Based on the concept of annotation-based agents, this report introduces tools and a formal notation for defining and runningtextminingexperiments using a statically typed domain-specific language embedded in Scala. Using machine learning for classification as an example, the framework is used to develop and documenttextminingexperiments, and to show how the concept of generic, typesafe annotation corresponds to a general information model that goes beyondtextprocessing.",['Fabian Steeg'],,arXiv,2011,https://doi.org/10.48550/arXiv.1108.0363,Anomali
Accelerated Multiplicative Updates and Hierarchical ALS Algorithms for Nonnegative Matrix Factorization,"Nonnegative matrix factorization (NMF) is a data analysis technique used in a great variety of applications such astextmining, image processing, hyperspectral data analysis, computational biology, and clustering. In this paper, we consider two well-known algorithms designed to solve NMF problems, namely the multiplicative updates of Lee and Seung and the hierarchical alternating least squares of Cichocki et al. We propose a simple way to significantly accelerate these schemes, based on a careful analysis of the computational cost needed at each iteration, while preserving their convergence properties. This acceleration technique can also be applied to other algorithms, which we illustrate on the projected gradient method of Lin. The efficiency of the accelerated algorithms is empirically demonstrated on image andtextdatasets, and compares favorably with a state-of-the-art alternating nonnegative least squares algorithm.","['Nicolas Gillis', 'François Glineur']","Neural Computation 24 (4), pp. 1085-1105, 2012",arXiv,2011,https://doi.org/10.48550/arXiv.1107.5194,Anomali
Fence - An Efficient Parser with Ambiguity Support for Model-Driven Language Specification,"Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration,textmining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities. In this paper, we propose Fence, an efficient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables the use of model-based language specification in practice.","['Luis Quesada', 'Fernando Berzal', 'Francisco J. Cortijo']",,arXiv,2011,https://doi.org/10.48550/arXiv.1107.4687,Anomali
Computing q-gram Non-overlapping Frequencies on SLP Compressed Texts,"Length-$q$ substrings, or $q$-grams, can represent important characteristics oftextdata, and determining the frequencies of all $q$-grams contained in the data is an important problem with many applications in the field of dataminingand machine learning. In this paper, we consider the problem of calculating the {\em non-overlapping frequencies} of all $q$-grams in atextgiven in compressed form, namely, as a straight line program (SLP). We show that the problem can be solved in $O(q^2n)$ time and $O(qn)$ space where $n$ is the size of the SLP. This generalizes and greatly improves previous work (Inenaga & Bannai, 2009) which solved the problem only for $q=2$ in $O(n^4\log n)$ time and $O(n^3)$ space.","['Keisuke Goto', 'Hideo Bannai', 'Shunsuke Inenaga', 'Masayuki Takeda']",,arXiv,2011,https://doi.org/10.48550/arXiv.1107.3022,Anomali
Tuffy: Scaling up Statistical Inference in Markov Logic Networks using an RDBMS,"Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems including information extraction, entity resolution, andtextmining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their wide-spread adoption. We present Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that allows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outperforms state-of-the-art implementations in both quality and speed on several publicly available datasets.","['Feng Niu', 'Christopher Ré', 'AnHai Doan', 'Jude Shavlik']","Proceedings of the VLDB Endowment (PVLDB), Vol. 4, No. 6, pp. 373-384 (2011)",arXiv,2011,https://doi.org/10.48550/arXiv.1104.3216,Anomali
Fast $q$-gram Mining on SLP Compressed Strings,"We present simple and efficient algorithms for calculating $q$-gram frequencies on strings represented in compressed form, namely, as a straight line program (SLP). Given an SLP of size $n$ that represents string $T$, we present an $O(qn)$ time and space algorithm that computes the occurrence frequencies of $q$-grams in $T$. Computational experiments show that our algorithm and its variation are practical for small $q$, actually running faster on various real string data, compared to algorithms that work on the uncompressedtext. We also discuss applications in dataminingand classification of string data, for which our algorithms can be useful.","['Keisuke Goto', 'Hideo Bannai', 'Shunsuke Inenaga', 'Masayuki Takeda']",,arXiv,2011,https://doi.org/10.48550/arXiv.1103.3114,Anomali
Compressed String Dictionaries,"The problem of storing a set of strings --- a string dictionary --- in compact form appears naturally in many cases. While classically it has represented a small part of the whole data to be processed (e.g., for Natural Language processing or for indexingtextcollections), more recent applications in Web engines, Webmining, RDF graphs, Internet routing, Bioinformatics, and many others, make use of very large string dictionaries, whose size is a significant fraction of the whole data. Thus novel approaches to compress them efficiently are necessary. In this paper we experimentally compare time and space performance of some existing alternatives, as well as new ones we propose. We show that space reductions of up to 20% of the original size of the strings is possible while supporting fast dictionary searches.","['Nieves R. Brisaboa', 'Rodrigo Cánovas', 'Miguel A. Martínez-Prieto', 'Gonzalo Navarro']",,arXiv,2011,https://doi.org/10.48550/arXiv.1101.5506,Anomali
A Concept Annotation System for Clinical Records,"Unstructured information comprises a valuable source of data in clinical records. Fortextminingin clinical records, concept extraction is the first step in finding assertions and relationships. This study presents a system developed for the annotation of medical concepts, including medical problems, tests, and treatments, mentioned in clinical records. The system combines six publicly available named entity recognition system into one framework, and uses a simple voting scheme that allows to tune precision and recall of the system to specific needs. The system provides both a web service interface and a UIMA interface which can be easily used by other systems. The system was tested in the fourth i2b2 challenge and achieved an F-score of 82.1% for the concept exact match task, a score which is among the top-ranking systems. To our knowledge, this is the first publicly available clinical record concept annotation system.","['Ning Kang', 'Rogier Barendse', 'Zubair Afzal', 'Bharat Singh', 'Martijn J. Schuemie', 'Erik M. van Mulligen', 'Jan A. Kors']",SWAT4LS 2010,arXiv,2010,https://doi.org/10.48550/arXiv.1012.1663,Anomali
The CALBC RDF Triple Store: retrieval over large literature content,"Integration of the scientific literature into a biomedical research infrastructure requires the processing of the literature, identification of the contained named entities (NEs) and concepts, and to represent the content in a standardised way. The CALBC project partners (PPs) have produced a large-scale annotated biomedical corpus with four different semantic groups through the harmonisation of annotations from automatictextminingsolutions (Silver Standard Corpus, SSC). The four semantic groups were chemical entities and drugs (CHED), genes and proteins (PRGE), diseases and disorders (DISO) and species (SPE). The content of the SSC has been fully integrated into RDF Triple Store (4,568,678 triples) and has been aligned with content from the GeneAtlas (182,840 triples), UniProtKb (12,552,239 triples for human) and the lexical resource LexEBI (BioLexicon). RDF Triple Store enables querying the scientific literature and bioinformatics resources at the same time for evidence of genetic causes, such as drug targets and disease involvement.","['Samuel Croset', 'Christoph Grabmüller', 'Chen Li', 'Silvestras Kavaliauskas', 'Dietrich Rebholz-Schuhmann']",SWAT4LS 2010,arXiv,2010,https://doi.org/10.48550/arXiv.1012.1650,Anomali
A study on the relation between linguistics-oriented and domain-specific semantics,"In this paper we dealt with the comparison and linking between lexical resources with domain knowledge provided by ontologies. It is one of the issues for the combination of the Semantic Web Ontologies andTextMining. We investigated the relations between the linguistics oriented and domain-specific semantics, by associating the GO biological process concepts to the FrameNet semantic frames. The result shows the gaps between the linguistics-oriented and domain-specific semantics on the classification of events and the grouping of target words. The result provides valuable information for the improvement of domain ontologies supporting fortextminingsystems. And also, it will result in benefits to language understanding technology.",['He Tan'],SWAT4LS 2010,arXiv,2010,https://doi.org/10.48550/arXiv.1012.1635,Anomali
Predicting Bugs' Components via Mining Bug Reports,"The number of bug reports in complex software increases dramatically. Now bugs are triaged manually, bug triage or assignment is a labor-intensive and time-consuming task. Without knowledge about the structure of the software, testers often specify the component of a new bug wrongly. Meanwhile, it is difficult for triagers to determine the component of the bug only by its description. We dig out the components of 28,829 bugs in Eclipse bug project have been specified wrongly and modified at least once. It results in these bugs have to be reassigned and delays the process of bug fixing. The average time of fixing wrongly-specified bugs is longer than that of correctly-specified ones. In order to solve the problem automatically, we use historical fixed bug reports as training corpus and build classifiers based on support vector machines and Naïve Bayes to predict the component of a new bug. The best prediction accuracy reaches up to 81.21% on our validation corpus of Eclipse project. Averagely our predictive model can save about 54.3 days for triagers and developers to repair a bug. Keywords: bug reports; bug triage;textclassification; predictive model","['Deqing Wang', 'Hui Zhang', 'Rui Liu', 'Mengxiang Lin', 'Wenjun Wu', 'Hongping Hu']","Journal of Software, Vol 7, No 5, 2012",arXiv,2011,https://doi.org/10.48550/arXiv.1010.4092,Anomali
Text Classification using Data Mining,"Textclassification is the process of classifying documents into predefined categories based on their content. It is the automated assignment of natural languagetextsto predefined categories.Textclassification is the primary requirement oftextretrieval systems, which retrievetextsin response to a user query, andtextunderstanding systems, which transformtextin some way such as producing summaries, answering questions or extracting data. Existing supervised learning algorithms to automatically classifytextneed sufficient documents to learn accurately. This paper presents a new algorithm fortextclassification using dataminingthat requires fewer documents for training. Instead of using words, word relation i.e. association rules from these words is used to derive feature set from pre-classifiedtextdocuments. The concept of Naive Bayes classifier is then used on derived features and finally only a single concept of Genetic Algorithm has been added for final classification. A system based on the proposed algorithm has been implemented and tested. The experimental results show that the proposed system works as a successfultextclassifier.","['S. M. Kamruzzaman', 'Farhana Haider', 'Ahmed Ryadh Hasan']","Proc. International Conference on Information and Communication Technology in Management (ICTM-2005), Multimedia University, Malaysia, May 2005",arXiv,2010,https://doi.org/10.48550/arXiv.1009.4987,Anomali
Text Classification using the Concept of Association Rule of Data Mining,"As the amount of onlinetextincreases, the demand fortextclassification to aid the analysis and management oftextis increasing.Textis cheap, but information, in the form of knowing what classes atextbelongs to, is expensive. Automatic classification oftextcan provide this information at low cost, but the classifiers themselves must be built with expensive human effort, or trained fromtextswhich have themselves been manually classified. In this paper we will discuss a procedure of classifyingtextusing the concept of association rule of datamining. Association ruleminingtechnique has been used to derive feature set from pre-classifiedtextdocuments. Naive Bayes classifier is then used on derived features for final classification.","['Chowdhury Mofizur Rahman', 'Ferdous Ahmed Sohel', 'Parvez Naushad', 'S. M. Kamruzzaman']","Proc. International Conference on Information Technology, Kathmandu, Nepal, pp. 234-241, May. 2003",arXiv,2010,https://doi.org/10.48550/arXiv.1009.4582,Anomali
A Multilevel Approach For Nonnegative Matrix Factorization,"Nonnegative Matrix Factorization (NMF) is the problem of approximating a nonnegative matrix with the product of two low-rank nonnegative matrices and has been shown to be particularly useful in many applications, e.g., intextmining, image processing, computational biology, etc. In this paper, we explain how algorithms for NMF can be embedded into the framework of multilevel methods in order to accelerate their convergence. This technique can be applied in situations where data admit a good approximate representation in a lower dimensional space through linear transformations preserving nonnegativity. A simple multilevel strategy is described and is experimentally shown to speed up significantly three popular NMF algorithms (alternating nonnegative least squares, multiplicative updates and hierarchical alternating least squares) on several standard image datasets.","['Nicolas Gillis', 'François Glineur']","Journal of Computational and Applied Mathematics 236 (7), pp. 1708-1723, 2012",arXiv,2011,https://doi.org/10.48550/arXiv.1009.0881,Anomali
Clustering Unstructured Data (Flat Files) - An Implementation in Text Mining Tool,"With the advancement of technology and reduced storage costs, individuals and organizations are tending towards the usage of electronic media for storing textual information and documents. It is time consuming for readers to retrieve relevant information from unstructured document collection. It is easier and less time consuming to find documents from a large collection when the collection is ordered or classified by group or category. The problem of finding best such grouping is still there. This paper discusses the implementation of k-Means clustering algorithm for clustering unstructuredtextdocuments that we implemented, beginning with the representation of unstructuredtextand reaching the resulting set of clusters. Based on the analysis of resulting clusters for a sample set of documents, we have also proposed a technique to represent documents that can further improve the clustering result.","['Yasir Safeer', 'Atika Mustafa', 'Anis Noor Ali']","(IJCSIS) International Journal of Computer Science and Information Security, Vol. 8, No. 2, MAY 2010, pp. 174-180",arXiv,2010,https://doi.org/10.48550/arXiv.1007.4324,Anomali
Clustering of Content Supporting Computer Mediated Courseware Development,"Computer Mediated Courseware (CMC) has been developed so far for individual courses considering single or multipletextbooks. A group of courseware can be developed by using multipletextbooks and in this case, it is a requirement to cluster the contents of different books to form a generalized clustered content. No work has been found to develop courseware applying generalized clustered content. We have proposed a clustering of content supporting computer mediated courseware development based on dataminingtechniques to construct a hierarchical general structure of a group of courseware combining the individual structure of a set of books. The clustering will help the courseware developer to dynamically allocate contents to develop different courses using a group of books. The authors have applied this methodology for different level of courses on database. The methodology is generalized and can be applied to any other courses.","['G. M. M. Bashir', 'M. J. Hossain', 'M. R. Karim']","Journal of Telecommunications, Volume 2, Issue 1, p30-35, April 2010",arXiv,2010,https://doi.org/10.48550/arXiv.1004.4595,Anomali
A New Approach to Keyphrase Extraction Using Neural Networks,"Keyphrases provide a simple way of describing a document, giving the reader some clues about its contents. Keyphrases can be useful in a various applications such as retrieval engines, browsing interfaces, thesaurus construction,textminingetc.. There are also other tasks for which keyphrases are useful, as we discuss in this paper. This paper describes a neural network based approach to keyphrase extraction from scientific articles. Our results show that the proposed method performs better than some state-of-the art keyphrase extraction approaches.","['Kamal Sarkar', 'Mita Nasipuri', 'Suranjan Ghose']","IJCSI, Volume 7, Issue 2, March 2010",arXiv,2010,https://doi.org/10.48550/arXiv.1004.3274,Anomali
Probabilistic Semantic Web Mining Using Artificial Neural Analysis,"Most of the web user's requirements are search or navigation time and getting correctly matched result. These constrains can be satisfied with some additional modules attached to the existing search engines and web servers. This paper proposes that powerful architecture for search engines with the title of Probabilistic Semantic WebMiningnamed from the methods used. With the increase of larger and larger collection of various data resources on the World Wide Web (WWW), WebMininghas become one of the most important requirements for the web users. Web servers will store various formats of data includingtext, image, audio, video etc., but servers can not identify the contents of the data. These search techniques can be improved by adding some special techniques including semantic webminingand probabilistic analysis to get more accurate results. Semantic webminingtechnique can provide meaningful search of data resources by eliminating useless information withminingprocess. In this technique web servers will maintain Meta information of each and every data resources available in that particular web server. This will help the search engine to retrieve information that is relevant to user given input string. This paper proposing the idea of combing these two techniques Semantic webminingand Probabilistic analysis for efficient and accurate search results of webmining. SPF can be calculated by considering both semantic accuracy and syntactic accuracy of data with the input string. This will be the deciding factor for producing results.","['T. Krishna Kishore', 'T. Sasi Vardhan', 'N. Lakshmi Narayana']","IJCSIS, Vol. 7 No. 3, March 2010, 294-304",arXiv,2010,https://doi.org/10.48550/arXiv.1004.1794,Anomali
On Generation of Firewall Log Status Reporter (SRr) Using Perl,Computer System Administration and Network Administration are few such areas where Practical Extraction Reporting Language (Perl) has robust utilization these days apart from Bioinformatics. The key role of a System/Network Administrator is to monitor log files. Log file are updated every day. To scan the summary of large log files and to quickly determine if there is anything wrong with the server or network we develop a Firewall Log Status Reporter (SRr). SRr helps to generate the reports based on the parameters of interest. SRr provides the facility to admin to generate the individual firewall report or all reports in one go. By scrutinizing the results of the reports admin can trace how many times a particular request has been made from which source to which destination and can track the errors easily. Perl scripts can be seen as the UNIX script replacement in future arena and SRr is one development with the same hope that we can believe in. SRr is a generalized and customizable utility completely written in Perl and may be used fortextminingand dataminingapplication in Bioinformatics research and development too.,"['Sugam Sharma', 'Hari Cohly', 'Tzusheng Pei']",International Journal of Network Security & Its Applications 1.2 (2009) 90-99,arXiv,2010,https://doi.org/10.48550/arXiv.1004.0604,Anomali
Mining Statistically Significant Substrings Based on the Chi-Square Measure,"Given the vast reservoirs of data stored worldwide, efficientminingof data from a large information store has emerged as a great challenge. Many databases like that of intrusion detection systems, web-click records, player statistics,texts, proteins etc., store strings or sequences. Searching for an unusual pattern within such long strings of data has emerged as a requirement for diverse applications. Given a string, the problem then is to identify the substrings that differs the most from the expected or normal behavior, i.e., the substrings that are statistically significant. In other words, these substrings are less likely to occur due to chance alone and may point to some interesting information or phenomenon that warrants further exploration. To this end, we use the chi-square measure. We propose two heuristics for retrieving the top-k substrings with the largest chi-square measure. We show that the algorithms outperform other competing algorithms in the runtime, while maintaining a high approximation ratio of more than 0.96.","['Sourav Dutta', 'Arnab Bhattacharya']",,arXiv,2010,https://doi.org/10.48550/arXiv.1002.4315,Anomali
Finding Sequential Patterns from Large Sequence Data,"Dataminingis the task of discovering interesting patterns from large amounts of data. There are many dataminingtasks, such as classification, clustering, association rulemining, and sequential patternmining. Sequential patternminingfinds sets of data items that occur together frequently in some sequences. Sequential patternmining, which extracts frequent subsequences from a sequence database, has attracted a great deal of interest during the recent dataminingresearch because it is the basis of many applications, such as: web user analysis, stock trend prediction, DNA sequence analysis, finding language or linguistic patterns from natural languagetexts, and using the history of symptoms to predict certain kind of disease. The diversity of the applications may not be possible to apply a single sequential pattern model to all these problems. Each application may require a unique model and solution. A number of research projects were established in recent years to develop meaningful sequential pattern models and efficient algorithms forminingthese patterns. In this paper, we theoretically provided a brief overview three types of sequential patterns model.","['Mahdi Esmaeili', 'Fazekas Gabor']","International Journal of Computer Science Issues, IJCSI, Vol. 7, Issue 1, No. 1, January 2010, http://ijcsi.org/articles/Finding-Sequential-Patterns-from-Large-Sequence-Data.php",arXiv,2010,https://doi.org/10.48550/arXiv.1002.1150,Anomali
Towards Effective Sentence Simplification for Automatic Processing of Biomedical Text,"The complexity of sentences characteristic to biomedical articles poses a challenge to natural language parsers, which are typically trained on large-scale corpora of non-technicaltext. We propose atextsimplification process, bioSimplify, that seeks to reduce the complexity of sentences in biomedical abstracts in order to improve the performance of syntactic parsers on the processed sentences. Syntactic parsing is typically one of the first steps in atextminingpipeline. Thus, any improvement in performance would have a ripple effect over all processing steps. We evaluated our method using a corpus of biomedical sentences annotated with syntactic links. Our empirical results show an improvement of 2.90% for the Charniak-McClosky parser and of 4.23% for the Link Grammar parser when processing simplified sentences rather than the original sentences in the corpus.","['Siddhartha Jonnalagadda', 'Luis Tari', 'Jorg Hakenberg', 'Chitta Baral', 'Graciela Gonzalez']","Proc. of the NAACL-HLT 2009, Boulder, USA, June 2009",arXiv,2010,https://doi.org/10.48550/arXiv.1001.4277,Anomali
On Utilization and Importance of Perl Status Reporter (SRr) in Text Mining,"In Bioinformatics,textminingandtextdataminingsometimes interchangeably used is a process to derive high-quality information fromtext. Perl Status Reporter (SRr) is a data fetching tool from a flattextfile and in this research paper we illustrate the use of SRr intextor datamining. SRr needs a flattextinput file where theminingprocess to be performed. SRr reads input file and derives the high quality information from it. Typicallytextminingtasks aretextcategorization,textclustering, concept and entity extraction, and document summarization. SRr can be utilized for any of these tasks with little or none customizing efforts. In our implementation we performtextcategorizationminingoperation on input file. The input file has two parameters of interest (firstKey and secondKey). The composition of these two parameters describes the uniqueness of entries in that file in the similar manner as done by composite key in database. SRr reads the input file line by line and extracts the parameters of interest and form a composite key by joining them together. It subsequently generates an output file consisting of the name as firstKey secondKey. SRr reads the input file and tracks the composite key. It further stores all that data lines, having the same composite key, in output file generated by SRr based on that composite key.","['Sugam Sharma', 'Tzusheng Pei', 'Hari Cohly']","International Journal of Computer Science and Information Security, IJCSIS, Vol. 6, No. 3, pp. 253-259, December 2009, USA",arXiv,2010,https://doi.org/10.48550/arXiv.1001.3277,Anomali
Practical Algorithmic Techniques for Several String Processing Problems,"The domains of dataminingand knowledge discovery make use of large amounts of textual data, which need to be handled efficiently. Specific problems, like finding the maximum weight ordered common subset of a set of ordered sets or searching for specific patterns withintexts, occur frequently in this context. In this paper we present several novel and practical algorithmic techniques for processing textual data (strings) in order to efficiently solve multiple problems. Our techniques make use of efficient string algorithms and data structures, like KMP, suffix arrays, tries and deterministic finite automata.","['Mugurel Ionut Andreica', 'Nicolae Tapus']","Proceedings of the 8th RoEduNet International Conference, pp. 136-141, Galati, Romania, 3-4 December, 2009. (ISBN: 978-606-8085-15-9)",arXiv,2009,https://doi.org/10.48550/arXiv.0912.0807,Anomali
Metric and Kernel Learning using a Linear Transformation,"Metric and kernel learning are important in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study metric learning as a problem of learning a linear transformation of the input data. We show that for high-dimensional data, a particular framework for learning a linear transformation of the data based on the LogDet divergence can be efficiently kernelized to learn a metric (or equivalently, a kernel function) over an arbitrarily high dimensional space. We further demonstrate that a wide class of convex loss functions for learning linear transformations can similarly be kernelized, thereby considerably expanding the potential applications of metric learning. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision andtextmining.","['Prateek Jain', 'Brian Kulis', 'Jason V. Davis', 'Inderjit S. Dhillon']",,arXiv,2009,https://doi.org/10.48550/arXiv.0910.5932,Anomali
An Enhanced Static Data Compression Scheme Of Bengali Short Message,"This paper concerns a modified approach of compressing Short BengaliTextMessage for small devices. The prime objective of this research technique is to establish a low complexity compression scheme suitable for small devices having small memory and relatively lower processing speed. The basic aim is not to compresstextof any size up to its maximum level without having any constraint on space and time, rather than the main target is to compress short messages up to an optimal level which needs minimum space, consume less time and the processor requirement is lower. We have implemented Character Masking, Dictionary Matching, Associative rule of dataminingand Hyphenation algorithm for syllable based compression in hierarchical steps to achieve low complexity lossless compression oftextmessage for any mobile devices. The scheme to choose the diagrams are performed on the basis of extensive statistical model and the static Huffman coding is done through the same context.","['Abu Shamim Mohammad Arif', 'Asif Mahamud', 'Rashedul Islam']",ISSN 1947 5500,arXiv,2009,https://doi.org/10.48550/arXiv.0909.0247,Anomali
Visualization of Mined Pattern and Its Human Aspects,"Researchers got success inminingthe Web usage data effectively and efficiently. But representation of theminedpatterns is often not in a form suitable for direct human consumption. Hence mechanisms and tools that can representminedpatterns in easily understandable format are utilized. Different techniques are used for pattern analysis, one of them is visualization. Visualization can provide valuable assistance for data analysis and decision making tasks. In the data visualization process, technical representations of web pages are replaced by user attractivetextinterpretations. Experiments with the real world problems showed that the visualization can significantly increase the quality and usefulness of web logminingresults. However, how decision makers perceive and interact with a visual representation can strongly influence their understanding of the data as well as the usefulness of the visual presentation. Human factors therefore contribute significantly to the visualization process and should play an important role in the design and evaluation of visualization tools. This electronic document is a live template. The various components of your paper, title,text, heads, etc., are already defined on the style sheet, as illustrated by the portions given in this document.","['Ratnesh Kumar Jain', 'Dr. Suresh Jain', 'Dr. R. S. Kasana']",ISSN 1947 5500,arXiv,2009,https://doi.org/10.48550/arXiv.0908.4374,Anomali
Data Mining and Machine Learning in Astronomy,"We review the current state of dataminingand machine learning in astronomy. 'DataMining' can have a somewhat mixed connotation from the point of view of a researcher in this field. If used correctly, it can be a powerful approach, holding the potential to fully exploit the exponentially increasing amount of available data, promising great scientific advance. However, if misused, it can be little more than the black-box application of complex computing algorithms that may give little physical insight, and provide questionable results. Here, we give an overview of the entire dataminingprocess, from data collection through to the interpretation of results. We cover common machine learning algorithms, such as artificial neural networks and support vector machines, applications from a broad range of astronomy, emphasizing those where dataminingtechniques directly resulted in improved science, and important current and future directions, including probability density functions, parallel algorithms, petascale computing, and the time domain. We conclude that, so long as one carefully selects an appropriate algorithm, and is guided by the astronomical problem at hand, dataminingcan be very much the powerful tool, and not the questionable black box.","['Nicholas M. Ball', 'Robert J. Brunner']","Int.J.Mod.Phys.D19:1049-1106,2010",arXiv,2010,https://doi.org/10.48550/arXiv.0906.2173,Anomali
Using Genetic Algorithms for Texts Classification Problems,"The avalanche quantity of the information developed by mankind has led to concept of automation of knowledge extraction - DataMining([1]). This direction is connected with a wide spectrum of problems - from recognition of the fuzzy set to creation of search machines. Important component of DataMiningis processing of thetextinformation. Such problems lean on concept of classification and clustering ([2]). Classification consists in definition of an accessory of some element (text) to one of in advance created classes. Clustering means splitting a set of elements (texts) on clusters which quantity are defined by localization of elements of the given set in vicinities of these some natural centers of these clusters. Realization of a problem of classification initially should lean on the given postulates, basic of which - the aprioristic information on primary set oftextsand a measure of affinity of elements and classes.","['A. A. Shumeyko', 'S. L. Sotnik']","Ann. Univ. Tibiscus Comp. Sci. Series VII(2009),325-340",arXiv,2009,https://doi.org/10.48550/arXiv.0906.0861,Anomali
Using Underapproximations for Sparse Nonnegative Matrix Factorization,"Nonnegative Matrix Factorization consists in (approximately) factorizing a nonnegative data matrix by the product of two low-rank nonnegative matrices. It has been successfully applied as a data analysis technique in numerous domains, e.g.,textmining, image processing, microarray data analysis, collaborative filtering, etc.
  We introduce a novel approach to solve NMF problems, based on the use of an underapproximation technique, and show its effectiveness to obtain sparse solutions. This approach, based on Lagrangian relaxation, allows the resolution of NMF problems in a recursive fashion. We also prove that the underapproximation problem is NP-hard for any fixed factorization rank, using a reduction of the maximum edge biclique problem in bipartite graphs.
  We test two variants of our underapproximation approach on several standard image datasets and show that they provide sparse part-based representations with low reconstruction error. Our results are comparable and sometimes superior to those obtained by two standard Sparse Nonnegative Matrix Factorization techniques.","['Nicolas Gillis', 'François Glineur']","Pattern Recognition 43 (4), pp. 1676-1687, 2010",arXiv,2009,https://doi.org/10.48550/arXiv.0902.1060,Anomali
Adaptive Spam Detection Inspired by a Cross-Regulation Model of Immune Dynamics: A Study of Concept Drift,"This paper proposes a novel solution to spam detection inspired by a model of the adaptive immune system known as the crossregulation model. We report on the testing of a preliminary algorithm on six e-mail corpora. We also compare our results statically and dynamically with those obtained by the Naive Bayes classifier and another binary classification method we developed previously for biomedicaltext-miningapplications. We show that the cross-regulation model is competitive against those and thus promising as a bio-inspired algorithm for spam detection in particular, and binary classification in general.","['Alaa Abi-Haidar', 'Luis M. Rocha']","Artificial Immune Systems: 7th International Conference, (ICARIS 2008). Bentley, Peter; Lee, Doheon; Jung, Sungwon (Eds.) Lecture Notes in Computer Science. Springer-Verlag, 5132: 36-47",arXiv,2008,https://doi.org/10.48550/arXiv.0812.1014,Anomali
Text Data Mining: Theory and Methods,"This paper provides the reader with a very brief introduction to some of the theory and methods oftextdatamining. The intent of this article is to introduce the reader to some of the current methodologies that are employed within this discipline area while at the same time making the reader aware of some of the interesting challenges that remain to be solved within the area. Finally, the articles serves as a very rudimentary tutorial on some of techniques while also providing the reader with a list of references for additional study.",['Jeffrey Solka'],IMS-SS-SS_2007_16,arXiv,2008,this http URL,Anomali
The structure of verbal sequences analyzed with unsupervised learning techniques,"Dataminingallows the exploration of sequences of phenomena, whereas one usually tends to focus on isolated phenomena or on the relation between two phenomena. It offers invaluable tools for theoretical analyses and exploration of the structure of sentences,texts, dialogues, and speech. We report here the results of an attempt at using it for inspecting sequences of verbs from French accounts of road accidents. This analysis comes from an original approach of unsupervised training allowing the discovery of the structure of sequential data. The entries of the analyzer were only made of the verbs appearing in the sentences. It provided a classification of the links between two successive verbs into four distinct clusters, allowing thustextsegmentation. We give here an interpretation of these clusters by applying a statistical analysis to independent semantic annotations.","['Catherine Recanati', 'Nicoleta Rogovschi', 'Younès Bennani']","Dans Proceedings - The 3rd Language & Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics, Poznan : Pologne (2007)",arXiv,2007,https://doi.org/10.48550/arXiv.0710.2446,Anomali
Reconstruction of Protein-Protein Interaction Pathways by Mining Subject-Verb-Objects Intermediates,"The exponential increase in publication rate of new articles is limiting access of researchers to relevant literature. This has prompted the use oftextminingtools to extract key biological information. Previous studies have reported extensive modification of existing generictextprocessors to process biologicaltext. However, this requirement for modification had not been examined. In this study, we have constructed Muscorian, using MontyLingua, a generictextprocessor. It uses a two-layered generalization-specialization paradigm previously proposed wheretextwas generically processed to a suitable intermediate format before domain-specific data extraction techniques are applied at the specialization layer. Evaluation using a corpus and experts indicated 86-90% precision and approximately 30% recall in extracting protein-protein interactions, which was comparable to previous studies using either specialized biologicaltextprocessing tools or modified existing tools. Our study had also demonstrated the flexibility of the two-layered generalization-specialization paradigm by using the same generalization layer for two specialized information extraction tasks.","['Maurice HT Ling', 'Christophe Lefevre', 'Kevin R. Nicholas', 'Feng Lin']","Ling, Maurice HT, Lefevre, Christophe, Nicholas, Kevin R, Lin, Feng. 2007. In J.C. Ragapakse, B. Schmidt, and G. Volkert (Eds.), PRIB 2007. Lecture Notes in Bioinformatics 4774: 286-299. Springer-Verlag.",arXiv,2007,https://doi.org/10.48550/arXiv.0708.0694,Anomali
Web data modeling for integration in data warehouses,"In a data warehousing process, the data preparation phase is crucial. Mastering this phase allows substantial gains in terms of time and performance when performing a multidimensional analysis or using dataminingalgorithms. Furthermore, a data warehouse can require external data. The web is a prevalent data source in this context, but the data broadcasted on this medium are very heterogeneous. We propose in this paper a UML conceptual model for a complex object representing a superclass of any useful data source (databases, plaintexts, HTML and XML documents, images, sounds, video clips...). The translation into a logical model is achieved with XML, which helps integrating all these diverse, heterogeneous data into a unified format, and whose schema definition provides first-rate metadata in our data warehousing context. Moreover, we benefit from XML's flexibility, extensibility and from the richness of the semi-structured data model, but we are still able to later map XML documents into a database if more structuring is needed.","['Sami Miniaoui', 'Jérôme Darmont', 'Omar Boussaïd']",First International Workshop on Multimedia Data and Document Engineering (MDDE 01) (07/2001) 88-97,arXiv,2007,https://doi.org/10.48550/arXiv.0705.1457,Anomali
A Methodological Framework for Socio-Cognitive Analyses of Collaborative Design of Open Source Software,"Open Source Software (OSS) development challenges traditional software engineering practices. In particular, OSS projects are managed by a large number of volunteers, working freely on the tasks they choose to undertake. OSS projects also rarely rely on explicit system-level design, or on project plans or schedules. Moreover, OSS developers work in arbitrary locations and collaborate almost exclusively over the Internet, using simple tools such as email and software code tracking databases (e.g. CVS). All the characteristics above make OSS development akin to weaving a tapestry of heterogeneous components. The OSS design process relies on various types of actors: people with prescribed roles, but also elements coming from a variety of information spaces (such as email and software code). The objective of our research is to understand the specific hybrid weaving accomplished by the actors of this distributed, collective design process. This, in turn, challenges traditional methodologies used to understand distributed software engineering: OSS development is simply too ""fibrous"" to lend itself well to analysis under a single methodological lens. In this paper, we describe the methodological framework we articulated to analyze collaborative design in the Open Source world. Our framework focuses on the links between the heterogeneous components of a project's hybrid network. We combine ethnography,textmining, and socio-technical network analysis and visualization to understand OSS development in its totality. This way, we are able to simultaneously consider the social, technical, and cognitive aspects of OSS development. We describe our methodology in detail, and discuss its implications for future research on distributed collective practices.","['Warren Sack', 'Françoise Détienne', 'Nicholas Ducheneaut', 'Jean-Marie Burkhardt', 'Dilan Mahendran', 'Flore Barcellini']","Computer Supported Cooperative Work (CSCW), the Journal of Collaborative Computing 15, 2-3 (2006) 229-250",arXiv,2007,https://doi.org/10.48550/arXiv.cs/0703009,Anomali
Acronym-Meaning Extraction from Corpora Using Multi-Tape Weighted Finite-State Machines,"The automatic extraction of acronyms and their meaning from corpora is an important sub-task oftextmining. It can be seen as a special case of string alignment, where atextchunk is aligned with an acronym. Alternative alignments have different cost, and ideally the least costly one should give the correct meaning of the acronym. We show how this approach can be implemented by means of a 3-tape weighted finite-state machine (3-WFSM) which reads atextchunk on tape 1 and an acronym on tape 2, and generates all alternative alignments on tape 3. The 3-WFSM can be automatically generated from a simple regular expression. No additional algorithms are required at any stage. Our 3-WFSM has a size of 27 states and 64 transitions, and finds the best analysis of an acronym in a few milliseconds.",['André Kempe'],"2006/019 (at Xerox Research Centre Europe, France)",arXiv,2006,https://doi.org/10.48550/arXiv.cs/0612033,Anomali
Expressing Implicit Semantic Relations without Supervision,"We present an unsupervised learning algorithm thatmineslargetextcorpora for patterns that express implicit semantic relations. For a given input word pair X:Y with some unspecified semantic relations, the corresponding output list of patterns <P1,...,Pm> is ranked according to how well each pattern Pi expresses the relations between X and Y. For example, given X=ostrich and Y=bird, the two highest ranking output patterns are ""X is the largest Y"" and ""Y such as the X"". The output patterns are intended to be useful for finding further pairs with the same relations, to support the construction of lexicons, ontologies, and semantic networks. The patterns are sorted by pertinence, where the pertinence of a pattern Pi for a word pair X:Y is the expected relational similarity between the given pair and typical pairs for Pi. The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs. On both tasks, the algorithm achieves state-of-the-art results, performing significantly better than several alternative pattern ranking algorithms, based on tf-idf.",['Peter D. Turney'],NRC-48761,arXiv,2006,https://doi.org/10.48550/arXiv.cs/0607120,Anomali
Ultrametric embedding: application to data fingerprinting and to fast data clustering,"We begin with pervasive ultrametricity due to high dimensionality and/or spatial sparsity. How extent or degree of ultrametricity can be quantified leads us to the discussion of varied practical cases when ultrametricity can be partially or locally present in data. We show how the ultrametricity can be assessed intextor document collections, and in time series signals. An aspect of importance here is that to draw benefit from this perspective the data may need to be recoded. Such data recoding can also be powerful in proximity searching, as we will show, where the data is embedded globally and not locally in an ultrametric space.",['Fionn Murtagh'],"P.M. Pardalos and P. Hansen, Eds., Data Mining and Mathematical Programming, CRM Proceedings & Lecture Notes Vol. 45, American Mathematical Society, 199-209, 2008",arXiv,2007,https://doi.org/10.48550/arXiv.math/0605555,Anomali
Exploring term-document matrices from matrix models in text mining,"We explore a matrix-space model, that is a natural extension to the vector space model for Information Retrieval. Each document can be represented by a matrix that is based on document extracts (e.g. sentences, paragraphs, sections). We focus on the performance of this model for the specific case in which documents are originally represented as term-by-sentence matrices. We use the singular value decomposition to approximate the term-by-sentence matrices and assemble these results to form the pseudo-``term-document'' matrix that forms the basis of atextminingmethod alternative to traditional VSM and LSI. We investigate the singular values of this matrix and provide experimental evidence suggesting that the method can be particularly effective in terms of accuracy fortextcollections with multi-topic documents, such as web pages with news.","['Ioannis Antonellis', 'Efstratios Gallopoulos']",03/02-06,arXiv,2006,https://doi.org/10.48550/arXiv.cs/0602076,Anomali
Preference Learning in Terminology Extraction: A ROC-based approach,"A key data preparation step inTextMining, Term Extraction selects the terms, or collocation of words, attached to specific concepts. In this paper, the task of extracting relevant collocations is achieved through a supervised learning algorithm, exploiting a few collocations manually labelled as relevant/irrelevant. The candidate terms are described along 13 standard statistical criteria measures. From these examples, an evolutionary learning algorithm termed Roger, based on the optimization of the Area under the ROC curve criterion, extracts an order on the candidate terms. The robustness of the approach is demonstrated on two real-world domain applications, considering different domains (biology and human resources) and different languages (English and French).","['Jérôme Azé', 'Mathieu Roche', 'Yves Kodratoff', 'Michèle Sebag']",Proceeedings of Applied Stochastic Models and Data Analysis (2005) 209-219,arXiv,2005,https://doi.org/10.48550/arXiv.cs/0512050,Anomali
Transitive Text Mining for Information Extraction and Hypothesis Generation,"Transitivetextmining- also named Swanson Linking (SL) after its primary and principal researcher - tries to establish meaningful links between literature sets which are virtually disjoint in the sense that each does not mention the main concept of the other. If successful, SL may give rise to the development of new hypotheses. In this communication we describe our approach to transitivetextminingwhich employs co-occurrence analysis of the medical subject headings (MeSH), the descriptors assigned to papers indexed in PubMed. In addition, we will outline the current state of our web-based information system which will enable our users to perform literature-driven hypothesis building on their own.","['Johannes Stegmann', 'Guenter Grohmann']",,arXiv,2005,https://doi.org/10.48550/arXiv.cs/0509020,Anomali
Learning from Web: Review of Approaches,"Knowledge discovery is defined as non-trivial extraction of implicit, previously unknown and potentially useful information from given data. Knowledge extraction from web documents deals with unstructured, free-format documents whose number is enormous and rapidly growing. The artificial neural networks are well suitable to solve a problem of knowledge discovery from web documents because trained networks are able more accurately and easily to classify the learning and testing examples those represent thetextminingdomain. However, the neural networks that consist of large number of weighted connections and activation units often generate the incomprehensible and hard-to-understand models oftextclassification. This problem may be also addressed to most powerful recurrent neural networks that employ the feedback links from hidden or output units to their input units. Due to feedback links, recurrent neural networks are able take into account of a context in document. To be useful for datamining, self-organizing neural network techniques of knowledge extraction have been explored and developed. Self-organization principles were used to create an adequate neural-network structure and reduce a dimensionality of features used to describetextdocuments. The use of these principles seems interesting because ones are able to reduce a neural-network redundancy and considerably facilitate the knowledge representation.",['Vitaly Schetinin'],,arXiv,2005,https://doi.org/10.48550/arXiv.cs/0504054,Anomali
"State of the Art, Evaluation and Recommendations regarding ""Document Processing and Visualization Techniques""","Several Networks of Excellence have been set up in the framework of the European FP5 research program. Among these Networks of Excellence, the NEMIS project focuses on the field ofTextMining.
  Within this field, document processing and visualization was identified as one of the key topics and the WG1 working group was created in the NEMIS project, to carry out a detailed survey of techniques associated with thetextminingprocess and to identify the relevant research topics in related research areas.
  In this document we present the results of this comprehensive survey. The report includes a description of the current state-of-the-art and practice, a roadmap for follow-up research in the identified areas, and recommendations for anticipated technological development in the domain oftextmining.","['Martin Rajman', 'Martin Vesely', 'Pierre Andrews']",,arXiv,2004,https://doi.org/10.48550/arXiv.cs/0412114,Anomali
The Revolution In Database System Architecture,"Database system architectures are undergoing revolutionary changes. Algorithms and data are being unified by integrating programming languages with the database system. This gives an extensible object-relational system where non-procedural relational operators manipulate object sets. Coupled with this, each DBMS is now a web service. This has huge implications for how we structure applications. DBMSs are now object containers. Queues are the first objects to be added. These queues are the basis for transaction processing and workflow applica-tions. Future workflow systems are likely to be built on this core. Data cubes and online analytic processing are now baked into most DBMSs. Beyond that, DBMSs have a framework for dataminingand machine learning algorithms. Decision trees, Bayes nets, clustering, and time series analysis are built in; new algorithms can be added.Text, temporal, and spatial data access methods, along with their probabilistic reasoning have been added to database systems. Allowing approximate and probabilistic answers is essential for many applications. Many believe that XML and xQuery will be the main data structure and access pattern. Database systems must accommodate that perspective.These changes mandate a much more dynamic query optimization strategy. Intelligence is moving to the periphery of the network. Each disk and each sensor will be a competent database machine. Relational algebra is a convenient way to program these systems. Database systems are now expected to be self-managing, self-healing, and always-up.",['Jim Gray'],MSR-TR-2004-31,arXiv,2004,https://doi.org/10.48550/arXiv.cs/0408030,Anomali
Word Sense Disambiguation by Web Mining for Word Co-occurrence Probabilities,"This paper describes the National Research Council (NRC) Word Sense Disambiguation (WSD) system, as applied to the English Lexical Sample (ELS) task in Senseval-3. The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill's rule-based part-of-speech tagger. Head words are represented as feature vectors with several hundred features. Approximately half of the features are syntactic and the other half are semantic. The main novelty in the system is the method for generating the semantic features, based on word \hbox{co-occurrence} probabilities. The probabilities are estimated using the Waterloo MultiText System with a corpus of about one terabyte of unlabeledtext, collected by a web crawler.",['Peter D. Turney'],"Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3), (2004), Barcelona, Spain, 239-242",arXiv,2004,https://doi.org/10.48550/arXiv.cs/0407065,Anomali
"""In vivo"" spam filtering: A challenge problem for data mining","Spam, also known as Unsolicited Commercial Email (UCE), is the bane of email communication. Many dataminingresearchers have addressed the problem of detecting spam, generally by treating it as a statictextclassification problem. True in vivo spam filtering has characteristics that make it a rich and challenging domain for datamining. Indeed, real-world datasets with these characteristics are typically difficult to acquire and to share. This paper demonstrates some of these characteristics and argues that researchers should pursue in vivo spam filtering as an accessible domain for investigating them.",['Tom Fawcett'],"KDD Explorations vol.5 no.2, Dec 2003. pp.140-148",arXiv,2004,https://doi.org/10.48550/arXiv.cs/0405007,Anomali
Embedding Web-based Statistical Translation Models in Cross-Language Information Retrieval,"Although more and more language pairs are covered by machine translation services, there are still many pairs that lack translation resources. Cross-language information retrieval (CLIR) is an application which needs translation functionality of a relatively low level of sophistication since current models for information retrieval (IR) are still based on a bag-of-words. The Web provides a vast resource for the automatic construction of parallel corpora which can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this paper, we will investigate the problem of automaticallyminingparalleltextsfrom the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost.","['Wessel Kraaij', 'Jian-Yun Nie', 'Michel Simard']",Computational Linguistics 29(3) september 2003,arXiv,2003,https://doi.org/10.48550/arXiv.cs/0312008,Anomali
The Lowell Database Research Self Assessment,"A group of senior database researchers gathers every few years to assess the state of database research and to point out problem areas that deserve additional focus. This report summarizes the discussion and conclusions of the sixth ad-hoc meeting held May 4-6, 2003 in Lowell, Mass. It observes that information management continues to be a critical component of most complex software systems. It recommends that database researchers increase focus on: integration oftext, data, code, and streams; fusion of information from heterogeneous data sources; reasoning about uncertain data; unsupervised dataminingfor interesting correlations; information privacy; and self-adaptation and repair.","['Serge Abiteboul', 'Rakesh Agrawal', 'Phil Bernstein', 'Mike Carey', 'Stefano Ceri', 'Bruce Croft', 'David DeWitt', 'Mike Franklin', 'Hector Garcia Molina', 'Dieter Gawlick', 'Jim Gray', 'Laura Haas', 'Alon Halevy', 'Joe Hellerstein', 'Yannis Ioannidis', 'Martin Kersten', 'Michael Pazzani', 'Mike Lesk', 'David Maier', 'Jeff Naughton', 'Hans Schek', 'Timos Sellis', 'Avi Silberschatz', 'Mike Stonebraker', 'Rick Snodgrass']",,arXiv,2003,https://doi.org/10.48550/arXiv.cs/0310006,Anomali
Coherent Keyphrase Extraction via Web Mining,"Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within thetextof a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. A limitation of previous keyphrase extraction algorithms is that the selected keyphrases are occasionally incoherent. That is, the majority of the output keyphrases may fit together well, but there may be a minority that appear to be outliers, with no clear semantic relation to the majority or to each other. This paper presents enhancements to the Kea keyphrase extraction algorithm that are designed to increase the coherence of the extracted keyphrases. The approach is to use the degree of statistical association among candidate keyphrases as evidence that they may be semantically related. The statistical association is measured using webmining. Experiments demonstrate that the enhancements improve the quality of the extracted keyphrases. Furthermore, the enhancements are not domain-specific: the algorithm generalizes well when it is trained on one domain (computer science documents) and tested on another (physics documents).",['Peter D. Turney'],NRC-46496,arXiv,2003,https://doi.org/10.48550/arXiv.cs/0308033,Anomali
Evaluation of text data mining for database curation: lessons learned from the KDD Challenge Cup,"MOTIVATION: The biological literature is a major repository of knowledge. Many biological databases draw much of their content from a careful curation of this literature. However, as the volume of literature increases, the burden of curation increases.Textminingmay provide useful tools to assist in the curation process. To date, the lack of standards has made it impossible to determine whethertextminingtechniques are sufficiently mature to be useful.
  RESULTS: We report on a Challenge Evaluation task that we created for the Knowledge Discovery and DataMining(KDD) Challenge Cup. We provided a training corpus of 862 articles consisting of journal articles curated in FlyBase, along with the associated lists of genes and gene products, as well as the relevant data fields from FlyBase. For the test, we provided a corpus of 213 new (`blind') articles; the 18 participating groups provided systems that flagged articles for curation, based on whether the article contained experimental evidence for gene expression products. We report on the the evaluation results and describe the techniques used by the top performing groups.
  CONTACT: asy@mitre.org
  KEYWORDS:textmining, evaluation, curation, genomics, data management","['Alexander S. Yeh', 'Lynette Hirschman', 'Alexander A. Morgan']","Bioinformatics Vol. 19 Suppl. 1 2003, pages i331-i339",arXiv,2003,https://doi.org/10.48550/arXiv.cs/0308032,Anomali
Mining the Web for Lexical Knowledge to Improve Keyphrase Extraction: Learning from Labeled and Unlabeled Data,"Keyphrases are useful for a variety of purposes, including summarizing, indexing, labeling, categorizing, clustering, highlighting, browsing, and searching. The task of automatic keyphrase extraction is to select keyphrases from within thetextof a given document. Automatic keyphrase extraction makes it feasible to generate keyphrases for the huge number of documents that do not have manually assigned keyphrases. Good performance on this task has been obtained by approaching it as a supervised learning problem. An input document is treated as a set of candidate phrases that must be classified as either keyphrases or non-keyphrases. To classify a candidate phrase as a keyphrase, the most important features (attributes) appear to be the frequency and location of the candidate phrase in the document. Recent work has demonstrated that it is also useful to know the frequency of the candidate phrase as a manually assigned keyphrase for other documents in the same domain as the given document (e.g., the domain of computer science). Unfortunately, this keyphrase-frequency feature is domain-specific (the learning process must be repeated for each new domain) and training-intensive (good performance requires a relatively large number of training documents in the given domain, with manually assigned keyphrases). The aim of the work described here is to remove these limitations. In this paper, I introduce new features that are derived bymininglexical knowledge from a very large collection of unlabeled data, consisting of approximately 350 million Web pages without manually assigned keyphrases. I present experiments that show that the new features result in improved keyphrase extraction, although they are neither domain-specific nor training-intensive.",['Peter D. Turney'],NRC-44947,arXiv,2002,https://doi.org/10.48550/arXiv.cs/0212011,Anomali
Phenomenological approach to profile impact of scientific research: Citation Mining,"In this paper we present a phenomenological approach to describe a complex system: scientific research impact through CitationMining. The novel concept of CitationMining, a combination of citation bibliometrics andtextmining, is used for the phenomenological description. CitationMiningstarts with a group of core papers whose impact is to be examined, retrieves the papers that cite these core papers, and then analyzes the technical infrastructure (authors, jorunals, institutions) of the citing papers as well as their thematic characteristics.","['J. A del Rio', 'R. N. Kostoff', 'E. O. Garcia', 'A. M. Ramirez', 'J. A. Humenik']","Adv. Complex Systems 5, 19-42 (2002)",arXiv,2001,https://doi.org/10.48550/arXiv.physics/0112047,Anomali
Bipartite graph partitioning and data clustering,"Many data types arising from dataminingapplications can be modeled as bipartite graphs, examples include terms and documents in atextcorpus, customers and purchasing items in market basket analysis and reviewers and movies in a movie recommender system. In this paper, we propose a new data clustering method based on partitioning the underlying bipartite graph. The partition is constructed by minimizing a normalized sum of edge weights between unmatched pairs of vertices of the bipartite graph. We show that an approximate solution to the minimization problem can be obtained by computing a partial singular value decomposition (SVD) of the associated edge weight matrix of the bipartite graph. We point out the connection of our clustering algorithm to correspondence analysis used in multivariate analysis. We also briefly discuss the issue of assigning data objects to multiple clusters. In the experimental results, we apply our clustering algorithm to the problem of document clustering to illustrate its effectiveness and efficiency.","['H. Zha', 'X. He', 'C. Ding', 'M. Gu', 'H. Simon']",,arXiv,2001,https://doi.org/10.48550/arXiv.cs/0108018,Anomali
Data Mining to Measure and Improve the Success of Web Sites,"For many companies, competitiveness in e-commerce requires a successful presence on the web. Web sites are used to establish the company's image, to promote and sell goods and to provide customer support. The success of a web site affects and reflects directly the success of the company in the electronic market. In this study, we propose a methodology to improve the ``success'' of web sites, based on the exploitation of navigation pattern discovery. In particular, we present a theory, in which success is modelled on the basis of the navigation behaviour of the site's users. We then exploit WUM, a navigation pattern discovery miner, to study how the success of a site is reflected in the users' behaviour. With WUM we measure the success of a site's components and obtain concrete indications of how the site should be improved. We report on our first experiments with an online catalog, the success of which we have studied. Ourmininganalysis has shown very promising results, on the basis of which the site is currently undergoing concrete improvements.","['Myra Spiliopoulou', 'Carsten Pohle']",,arXiv,2000,https://doi.org/10.48550/arXiv.cs/0008009,Anomali
Using compression to identify acronyms in text,"Textminingis about looking for patterns in natural languagetext, and may be defined as the process of analyzingtextto extract information from it for particular purposes. In previous work, we claimed that compression is a key technology fortextmining, and backed this up with a study that showed how particular kinds of lexical tokens---names, dates, locations, etc.---can be identified and located in runningtext, using compression models to provide the leverage necessary to distinguish different token types (Witten et al., 1999)","['Stuart Yeates', 'David Bainbridge', 'Ian H. Witten']",Working Paper 00/01,arXiv,2000,https://doi.org/10.48550/arXiv.cs/0007003,Anomali
Parallel Strands: A Preliminary Investigation into Mining the Web for Bilingual Text,"Parallel corpora are a valuable resource for machine translation, but at present their availability and utility is limited by genre- and domain-specificity, licensing restrictions, and the basic difficulty of locating paralleltextsin all but the most dominant of the world's languages. A parallel corpus resource not yet explored is the World Wide Web, which hosts an abundance of pages in parallel translation, offering a potential solution to some of these problems and unique opportunities of its own. This paper presents the necessary first step in that exploration: a method for automatically finding parallel translated documents on the Web. The technique is conceptually simple, fully language independent, and scalable, and preliminary evaluation results indicate that the method may be accurate enough to apply without human intervention.",['Philip Resnik'],UMIACS TR 98-41,arXiv,1998,https://doi.org/10.48550/arXiv.cmp-lg/9808003,Anomali
Manual Annotation of Translational Equivalence: The Blinker Project,"Bilingual annotators were paid to link roughly sixteen thousand corresponding words between on-line versions of the Bible in modern French and modern English. These annotations are freely available to the research community from http://www.cis.upenn.edu/~melamed . The annotations can be used for several purposes. First, they can be used as a standard data set for developing and testing translation lexicons and statistical translation models. Second, researchers in lexical semantics will be able tominethe annotations for insights about cross-linguistic lexicalization patterns. Third, the annotations can be used in research into certain recently proposed methods for monolingual word-sense disambiguation. This paper describes the annotatedtexts, the specially-designed annotation tool, and the strategies employed to increase the consistency of the annotations. The annotation process was repeated five times by different annotators. Inter-annotator agreement rates indicate that the annotations are reasonably reliable and that the method is easy to replicate.",['I. Dan Melamed'],IRCS TR #98-07,arXiv,1998,https://doi.org/10.48550/arXiv.cmp-lg/9805005,Anomali
